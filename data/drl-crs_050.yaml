- en: Quiz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ‹éªŒ
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit3/quiz](https://huggingface.co/learn/deep-rl-course/unit3/quiz)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/learn/deep-rl-course/unit3/quiz](https://huggingface.co/learn/deep-rl-course/unit3/quiz)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)
    **is to test yourself.** This will help you to find **where you need to reinforce
    your knowledge**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ å’Œ[é¿å…è‡ªä»¥ä¸ºæ˜¯çš„é”™è§‰](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)çš„æœ€ä½³æ–¹æ³•**æ˜¯æµ‹è¯•è‡ªå·±**ã€‚è¿™å°†å¸®åŠ©æ‚¨æ‰¾åˆ°**éœ€è¦åŠ å¼ºçŸ¥è¯†çš„åœ°æ–¹**ã€‚
- en: 'Q1: We mentioned Q Learning is a tabular method. What are tabular methods?'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q1: æˆ‘ä»¬æåˆ° Q å­¦ä¹ æ˜¯ä¸€ç§è¡¨æ ¼æ–¹æ³•ã€‚ä»€ä¹ˆæ˜¯è¡¨æ ¼æ–¹æ³•ï¼Ÿ'
- en: <details data-svelte-h="svelte-gsebop"><summary>Solution</summary>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-gsebop"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: '*Tabular methods* is a type of problem in which the state and actions spaces
    are small enough to approximate value functions to be **represented as arrays
    and tables**. For instance, **Q-Learning is a tabular method** since we use a
    table to represent the state, and action value pairs.</details>'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¡¨æ ¼æ–¹æ³•*æ˜¯ä¸€ç§é—®é¢˜ç±»å‹ï¼Œå…¶ä¸­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´è¶³å¤Ÿå°ï¼Œå¯ä»¥å°†å€¼å‡½æ•°è¿‘ä¼¼ä¸º**æ•°ç»„å’Œè¡¨**ã€‚ä¾‹å¦‚ï¼Œ**Q å­¦ä¹ æ˜¯ä¸€ç§è¡¨æ ¼æ–¹æ³•**ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨è¡¨æ¥è¡¨ç¤ºçŠ¶æ€å’ŒåŠ¨ä½œå€¼å¯¹ã€‚</details>'
- en: 'Q2: Why canâ€™t we use a classical Q-Learning to solve an Atari Game?'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q2: ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ç»å…¸çš„ Q å­¦ä¹ æ¥è§£å†³ Atari æ¸¸æˆï¼Ÿ'
- en: 'Q3: Why do we stack four frames together when we use frames as input in Deep
    Q-Learning?'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q3: å½“æˆ‘ä»¬åœ¨æ·±åº¦ Q å­¦ä¹ ä¸­å°†å››ä¸ªå¸§å †å åœ¨ä¸€èµ·æ—¶ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è¿™æ ·åšï¼Ÿ'
- en: <details data-svelte-h="svelte-nzbq54"><summary>Solution</summary>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-nzbq54"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: 'We stack frames together because it helps us **handle the problem of temporal
    limitation**: one frame is not enough to capture temporal information. For instance,
    in pong, our agent **will be unable to know the ball direction if it gets only
    one frame**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¸§å †å åœ¨ä¸€èµ·ï¼Œå› ä¸ºè¿™æœ‰åŠ©äºæˆ‘ä»¬**å¤„ç†æ—¶é—´é™åˆ¶çš„é—®é¢˜**ï¼šä¸€ä¸ªå¸§ä¸è¶³ä»¥æ•æ‰æ—¶é—´ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¹’ä¹“çƒä¸­ï¼Œå¦‚æœæˆ‘ä»¬åªæœ‰ä¸€ä¸ªå¸§ï¼Œæˆ‘ä»¬çš„ä»£ç†**å°†æ— æ³•çŸ¥é“çƒçš„æ–¹å‘**ã€‚
- en: '![Temporal limitation](../Images/916225d18ad696514245f8c4e88a5a56.png) ![Temporal
    limitation](../Images/e35a3e3cfeefe6f7a16b681ab91dfa7b.png)</details>'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![æ—¶é—´é™åˆ¶](../Images/916225d18ad696514245f8c4e88a5a56.png) ![æ—¶é—´é™åˆ¶](../Images/e35a3e3cfeefe6f7a16b681ab91dfa7b.png)</details>'
- en: 'Q4: What are the two phases of Deep Q-Learning?'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q4: æ·±åº¦ Q å­¦ä¹ çš„ä¸¤ä¸ªé˜¶æ®µæ˜¯ä»€ä¹ˆï¼Ÿ'
- en: 'Q5: Why do we create a replay memory in Deep Q-Learning?'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q5: ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨æ·±åº¦ Q å­¦ä¹ ä¸­åˆ›å»ºé‡æ”¾å†…å­˜ï¼Ÿ'
- en: <details data-svelte-h="svelte-10rxkt3"><summary>Solution</summary>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-10rxkt3"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: '**1\. Make more efficient use of the experiences during the training**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. åœ¨è®­ç»ƒæœŸé—´æ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç»éªŒ**'
- en: Usually, in online reinforcement learning, the agent interacts in the environment,
    gets experiences (state, action, reward, and next state), learns from them (updates
    the neural network), and discards them. This is not efficient. But, with experience
    replay, **we create a replay buffer that saves experience samples that we can
    reuse during the training**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œåœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»£ç†ä¸ç¯å¢ƒäº’åŠ¨ï¼Œè·å¾—ç»éªŒï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ï¼‰ï¼Œä»ä¸­å­¦ä¹ ï¼ˆæ›´æ–°ç¥ç»ç½‘ç»œï¼‰ï¼Œç„¶åä¸¢å¼ƒå®ƒä»¬ã€‚è¿™å¹¶ä¸é«˜æ•ˆã€‚ä½†æ˜¯ï¼Œé€šè¿‡ç»éªŒé‡æ”¾ï¼Œ**æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä¿å­˜ç»éªŒæ ·æœ¬çš„é‡æ”¾ç¼“å†²åŒºï¼Œå¯ä»¥åœ¨è®­ç»ƒæœŸé—´é‡å¤ä½¿ç”¨**ã€‚
- en: '**2\. Avoid forgetting previous experiences and reduce the correlation between
    experiences**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. é¿å…å¿˜è®°å…ˆå‰çš„ç»éªŒå¹¶å‡å°‘ç»éªŒä¹‹é—´çš„ç›¸å…³æ€§**'
- en: The problem we get if we give sequential samples of experiences to our neural
    network is that it **tends to forget the previous experiences as it overwrites
    new experiences**. For instance, if we are in the first level and then the second,
    which is different, our agent can forget how to behave and play in the first level.</details>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†ç»éªŒçš„é¡ºåºæ ·æœ¬æä¾›ç»™æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼Œé—®é¢˜å°±ä¼šå‡ºç°ï¼Œå› ä¸ºå®ƒ**å€¾å‘äºå¿˜è®°å…ˆå‰çš„ç»éªŒï¼Œå› ä¸ºå®ƒä¼šè¦†ç›–æ–°çš„ç»éªŒ**ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬åœ¨ç¬¬ä¸€å…³ç„¶åæ˜¯ä¸åŒçš„ç¬¬äºŒå…³ï¼Œæˆ‘ä»¬çš„ä»£ç†å¯èƒ½ä¼šå¿˜è®°å¦‚ä½•åœ¨ç¬¬ä¸€å…³ä¸­è¡Œä¸ºå’Œç©è€ã€‚</details>
- en: 'Q6: How do we use Double Deep Q-Learning?'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q6: æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨åŒé‡æ·±åº¦ Q å­¦ä¹ ï¼Ÿ'
- en: <details data-svelte-h="svelte-1u03gxl"><summary>Solution</summary>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-1u03gxl"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: 'When we compute the Q target, we use two networks to decouple the action selection
    from the target Q value generation. We:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è®¡ç®— Q ç›®æ ‡æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªç½‘ç»œæ¥è§£è€¦åŠ¨ä½œé€‰æ‹©å’Œç›®æ ‡ Q å€¼ç”Ÿæˆã€‚æˆ‘ä»¬ï¼š
- en: Use our *DQN network* to **select the best action to take for the next state**
    (the action with the highest Q value).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬çš„*DQN ç½‘ç»œ*æ¥**é€‰æ‹©ä¸‹ä¸€ä¸ªçŠ¶æ€è¦é‡‡å–çš„æœ€ä½³è¡ŒåŠ¨**ï¼ˆå…·æœ‰æœ€é«˜ Q å€¼çš„è¡ŒåŠ¨ï¼‰ã€‚
- en: Use our *Target network* to calculate **the target Q value of taking that action
    at the next state**.</details>
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬çš„*ç›®æ ‡ç½‘ç»œ*æ¥è®¡ç®—**åœ¨ä¸‹ä¸€ä¸ªçŠ¶æ€é‡‡å–è¯¥åŠ¨ä½œçš„ç›®æ ‡ Q å€¼**ã€‚</details>
- en: Congrats on finishing this Quiz ğŸ¥³, if you missed some elements, take time to
    read again the chapter to reinforce (ğŸ˜) your knowledge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œæ‚¨å®Œæˆäº†è¿™ä¸ªæµ‹éªŒğŸ¥³ï¼Œå¦‚æœæ‚¨é”™è¿‡äº†ä¸€äº›å…ƒç´ ï¼Œè¯·èŠ±æ—¶é—´å†æ¬¡é˜…è¯»ç« èŠ‚ï¼Œä»¥åŠ å¼ºï¼ˆğŸ˜ï¼‰æ‚¨çš„çŸ¥è¯†ã€‚
