- en: Pre-tokenizers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预分词器
- en: 'Original text: [https://huggingface.co/docs/tokenizers/api/pre-tokenizers](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/tokenizers/api/pre-tokenizers](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)
- en: PythonRustNode
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: BertPreTokenizer
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BertPreTokenizer
- en: '### `class tokenizers.pre_tokenizers.BertPreTokenizer`'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.BertPreTokenizer`'
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: BertPreTokenizer
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: BertPreTokenizer
- en: This pre-tokenizer splits tokens on spaces, and also on punctuation. Each occurence
    of a punctuation character will be treated separately.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此预分词器在空格上分割标记，也在标点上分割。每个标点字符的出现将被单独处理。
- en: ByteLevel
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ByteLevel
- en: '### `class tokenizers.pre_tokenizers.ByteLevel`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.ByteLevel`'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `True`) — Whether to add
    a space to the first word if there isn’t already one. This lets us treat *hello*
    exactly like *say hello*.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *可选*, 默认为 `True`) — 如果第一个单词前没有空格，则是否添加一个空格。这样我们可以将
    *hello* 和 *say hello* 完全相同对待。'
- en: '`use_regex` (`bool`, *optional*, defaults to `True`) — Set this to `False`
    to prevent this *pre_tokenizer* from using the GPT2 specific regexp for spliting
    on whitespace.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_regex` (`bool`, *可选*, 默认为 `True`) — 将此 *pre_tokenizer* 设置为 `False`，以防止其使用
    GPT2 特定的正则表达式在空格上分割。'
- en: ByteLevel PreTokenizer
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ByteLevel 预分词器
- en: This pre-tokenizer takes care of replacing all bytes of the given string with
    a corresponding representation, as well as splitting into words.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此预分词器负责用相应表示替换给定字符串的所有字节，以及分割成单词。
- en: '#### `alphabet`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `alphabet`'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Returns
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[str]`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: A list of characters that compose the alphabet
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由字母表组成的字符列表
- en: Returns the alphabet used by this PreTokenizer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 返回此 PreTokenizer 使用的字母表。
- en: Since the ByteLevel works as its name suggests, at the byte level, it encodes
    each byte value to a unique visible character. This means that there is a total
    of 256 different characters composing this alphabet.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ByteLevel 的工作方式如其名称所示，在字节级别上，它将每个字节值编码为唯一的可见字符。这意味着总共有 256 个不同的字符组成此字母表。
- en: CharDelimiterSplit
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CharDelimiterSplit
- en: '### `class tokenizers.pre_tokenizers.CharDelimiterSplit`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.CharDelimiterSplit`'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This pre-tokenizer simply splits on the provided char. Works like `.split(delimiter)`
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此预分词器简单地在提供的字符上分割。类似于 `.split(delimiter)`
- en: Digits
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数字
- en: '### `class tokenizers.pre_tokenizers.Digits`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.Digits`'
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`individual_digits` (`bool`, *optional*, defaults to `False`) —'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`individual_digits` (`bool`, *可选*, 默认为 `False`) —'
- en: This pre-tokenizer simply splits using the digits in separate tokens
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此预分词器简单地使用数字在单独的标记中分割
- en: 'If set to True, digits will each be separated as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置为 True，数字将按以下方式分隔：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If set to False, digits will grouped as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置为 False，数字将按以下方式分组：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Metaspace
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Metaspace
- en: '### `class tokenizers.pre_tokenizers.Metaspace`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.Metaspace`'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`replacement` (`str`, *optional*, defaults to `▁`) — The replacement character.
    Must be exactly one character. By default we use the *▁* (U+2581) meta symbol
    (Same as in SentencePiece).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replacement` (`str`, *可选*, 默认为 `▁`) — 替换字符。必须正好是一个字符。默认情况下，我们使用 *▁* (U+2581)
    元符号（与 SentencePiece 中相同）。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `True`) — Whether to add
    a space to the first word if there isn’t already one. This lets us treat *hello*
    exactly like *say hello*.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *可选*, 默认为 `True`) — 如果第一个单词前没有空格，则是否添加一个空格。这样我们可以将
    *hello* 和 *say hello* 完全相同对待。'
- en: Metaspace pre-tokenizer
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Metaspace 预分词器
- en: This pre-tokenizer replaces any whitespace by the provided replacement character.
    It then tries to split on these spaces.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此预分词器将任何空格替换为提供的替换字符。然后尝试在这些空格上分割。
- en: PreTokenizer
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PreTokenizer
- en: '### `class tokenizers.pre_tokenizers.PreTokenizer`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.PreTokenizer`'
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Base class for all pre-tokenizers
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所有预分词器的基类
- en: This class is not supposed to be instantiated directly. Instead, any implementation
    of a PreTokenizer will return an instance of this class when instantiated.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不应直接实例化此类。相反，任何 PreTokenizer 的实现在实例化时将返回此类的实例。
- en: '#### `pre_tokenize`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pre_tokenize`'
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretok` (`~tokenizers.PreTokenizedString) -- The pre-tokenized string on which
    to apply this :class:`~tokenizers.pre_tokenizers.PreTokenizer`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretok` (`~tokenizers.PreTokenizedString) -- 要应用此 :class:`~tokenizers.pre_tokenizers.PreTokenizer`
    的预分词字符串'
- en: Pre-tokenize a `~tokenizers.PyPreTokenizedString` in-place
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在原地对 `~tokenizers.PyPreTokenizedString` 进行预分词
- en: This method allows to modify a `PreTokenizedString` to keep track of the pre-tokenization,
    and leverage the capabilities of the `PreTokenizedString`. If you just want to
    see the result of the pre-tokenization of a raw string, you can use `pre_tokenize_str()`
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法允许修改 `PreTokenizedString` 以跟踪预分词，并利用 `PreTokenizedString` 的功能。如果只想查看原始字符串的预分词结果，可以使用
    `pre_tokenize_str()`
- en: '#### `pre_tokenize_str`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pre_tokenize_str`'
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequence` (`str`) — A string to pre-tokeize'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence` (`str`) — 要预分词的字符串'
- en: Returns
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Tuple[str, Offsets]]`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Tuple[str, Offsets]]`'
- en: A list of tuple with the pre-tokenized parts and their offsets
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 包含预分词部分及其偏移量的元组列表
- en: Pre tokenize the given string
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定字符串进行预分词
- en: This method provides a way to visualize the effect of a [PreTokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer)
    but it does not keep track of the alignment, nor does it provide all the capabilities
    of the `PreTokenizedString`. If you need some of these, you can use `pre_tokenize()`
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法提供了一种可视化 [PreTokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer)
    效果的方式，但它不会跟踪对齐，也不提供 `PreTokenizedString` 的所有功能。如果需要其中一些功能，可以使用 `pre_tokenize()`
- en: Punctuation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标点
- en: '### `class tokenizers.pre_tokenizers.Punctuation`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.Punctuation`'
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`behavior` (`SplitDelimiterBehavior`) — The behavior to use when splitting.
    Choices: “removed”, “isolated” (default), “merged_with_previous”, “merged_with_next”,
    “contiguous”'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`behavior` (`SplitDelimiterBehavior`) — 分割时要使用的行为。选择: “removed”, “isolated”
    (默认), “merged_with_previous”, “merged_with_next”, “contiguous”'
- en: This pre-tokenizer simply splits on punctuation as individual characters.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此预分词器简单地在标点上分割为单独的字符。
- en: Sequence
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sequence
- en: '### `class tokenizers.pre_tokenizers.Sequence`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.Sequence`'
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This pre-tokenizer composes other pre_tokenizers and applies them in sequence
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预分词器组合其他预分词器，并按顺序应用它们
- en: Split
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割
- en: '### `class tokenizers.pre_tokenizers.Split`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.Split`'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pattern` (`str` or `Regex`) — A pattern used to split the string. Usually
    a string or a a regex built with *tokenizers.Regex*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pattern` (`str` 或 `Regex`) — 用于分割字符串的模式。通常是一个字符串或使用*tokenizers.Regex*构建的正则表达式'
- en: '`behavior` (`SplitDelimiterBehavior`) — The behavior to use when splitting.
    Choices: “removed”, “isolated”, “merged_with_previous”, “merged_with_next”, “contiguous”'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`behavior` (`SplitDelimiterBehavior`) — 分割时要使用的行为。选择：“removed”, “isolated”,
    “merged_with_previous”, “merged_with_next”, “contiguous”'
- en: '`invert` (`bool`, *optional*, defaults to `False`) — Whether to invert the
    pattern.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`invert` (`bool`, *可选*，默认为`False`) — 是否反转模式。'
- en: Split PreTokenizer
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 分割预分词器
- en: This versatile pre-tokenizer splits using the provided pattern and according
    to the provided behavior. The pattern can be inverted by making use of the invert
    flag.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多功能的预分词器使用提供的模式进行分割，并根据提供的行为进行操作。通过使用反转标志，可以反转模式。
- en: UnicodeScripts
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UnicodeScripts
- en: '### `class tokenizers.pre_tokenizers.UnicodeScripts`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.UnicodeScripts`'
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This pre-tokenizer splits on characters that belong to different language family
    It roughly follows [https://github.com/google/sentencepiece/blob/master/data/Scripts.txt](https://github.com/google/sentencepiece/blob/master/data/Scripts.txt)
    Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too. This
    mimicks SentencePiece Unigram implementation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预分词器根据属于不同语言家族的字符进行分割，它大致遵循[https://github.com/google/sentencepiece/blob/master/data/Scripts.txt](https://github.com/google/sentencepiece/blob/master/data/Scripts.txt)实际上，平假名和片假名与汉字融合在一起，0x30FC也是汉字。这模仿了SentencePiece
    Unigram的实现。
- en: Whitespace
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Whitespace
- en: '### `class tokenizers.pre_tokenizers.Whitespace`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.Whitespace`'
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This pre-tokenizer simply splits using the following regex: `\w+|[^\w\s]+`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预分词器简单地使用以下正则表达式进行分割：`\w+|[^\w\s]+`
- en: WhitespaceSplit
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhitespaceSplit
- en: '### `class tokenizers.pre_tokenizers.WhitespaceSplit`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class tokenizers.pre_tokenizers.WhitespaceSplit`'
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This pre-tokenizer simply splits on the whitespace. Works like `.split()`
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预分词器简单地在空格上分割。工作方式类似于`.split()`
