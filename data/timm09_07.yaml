- en: Scripts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脚本
- en: 'Original text: [https://huggingface.co/docs/timm/training_script](https://huggingface.co/docs/timm/training_script)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/timm/training_script](https://huggingface.co/docs/timm/training_script)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: A train, validation, inference, and checkpoint cleaning script included in the
    github root folder. Scripts are not currently packaged in the pip release.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在github根文件夹中包含了训练、验证、推理和检查点清理脚本。这些脚本目前尚未打包在pip发布中。
- en: The training and validation scripts evolved from early versions of the [PyTorch
    Imagenet Examples](https://github.com/pytorch/examples). I have added significant
    functionality over time, including CUDA specific performance enhancements based
    on [NVIDIA’s APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证脚本是从早期版本的[PyTorch Imagenet Examples](https://github.com/pytorch/examples)演变而来。随着时间的推移，我添加了重要的功能，包括基于[NVIDIA的APEX示例](https://github.com/NVIDIA/apex/tree/master/examples)的CUDA特定性能增强。
- en: Training Script
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练脚本
- en: The variety of training args is large and not all combinations of options (or
    even options) have been fully tested. For the training dataset folder, specify
    the folder to the base that contains a `train` and `validation` folder.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 训练参数的种类很多，并且并非所有选项的组合（甚至选项）都经过充分测试。对于训练数据集文件夹，请指定包含`train`和`validation`文件夹的基础文件夹。
- en: 'To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process
    per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要在ImageNet上训练一个SE-ResNet34，本地分布式，4个GPU，每个GPU一个进程，采用余弦调度，随机擦除概率为50%，每像素随机值：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead
    of APEX AMP. --amp defaults to native AMP as of timm ver 0.4.3\. --apex-amp will
    force use of APEX components if they are installed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用PyTorch 1.9+与PyTorch本机AMP和DDP，而不是APEX AMP。--amp默认为timmm ver 0.4.3中的本机AMP。--apex-amp将强制使用APEX组件（如果已安装）。
- en: Validation / Inference Scripts
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证/推理脚本
- en: Validation and inference scripts are similar in usage. One outputs metrics on
    a validation set and the other outputs topk class ids in a csv. Specify the folder
    containing validation images, not the base as in training script.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 验证和推理脚本的使用方式类似。一个输出验证集上的指标，另一个输出csv中的topk类别ID。请指定包含验证图像的文件夹，而不是像训练脚本中那样指定基础文件夹。
- en: 'To validate with the model’s pretrained weights (if they exist):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用模型的预训练权重进行验证（如果存在）：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To run inference from a checkpoint:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要从检查点运行推理：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Training Examples
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练示例
- en: EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5
- en: 'These params are for dual Titan RTX cards with NVIDIA Apex installed:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数适用于安装了NVIDIA Apex的双Titan RTX卡：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5
- en: 'This params are for dual Titan RTX cards with NVIDIA Apex installed:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数适用于安装了NVIDIA Apex的双Titan RTX卡：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: SE-ResNeXt-26-D and SE-ResNeXt-26-T
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SE-ResNeXt-26-D和SE-ResNeXt-26-T
- en: 'These hparams (or similar) work well for a wide range of ResNet architecture,
    generally a good idea to increase the epoch # as the model size increases… ie
    approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR
    proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti
    cards:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数（或类似的）适用于各种ResNet架构，通常建议随着模型大小的增加增加epoch数量...例如，对于ResNe(X)t50，大约为180-200，对于更大的模型则为220+。对于性能更好的GPU或启用AMP，增加批量大小和LR。这些参数适用于两张1080Ti卡：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5
- en: The training of this model started with the same command line as EfficientNet-B2
    w/ RA above. After almost three weeks of training the process crashed. The results
    weren’t looking amazing so I resumed the training several times with tweaks to
    a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing
    looked great. I ended up averaging the best checkpoints from all restarts. The
    result is mediocre at default res/crop but oddly performs much better with a full
    image test crop of 1.0.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的训练从与上面的EfficientNet-B2 w/ RA相同的命令行开始。经过将近三周的训练，进程崩溃了。结果看起来并不令人惊讶，所以我多次恢复训练，并对一些参数进行了调整（增加RE概率，减少rand-aug，增加ema-decay）。没有什么看起来很好。我最终对所有重新启动的最佳检查点进行了平均。结果在默认的res/crop下一般，但在全图像测试裁剪为1.0时表现得更好。
- en: EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5
- en: '[Michael Klachko](https://github.com/michaelklachko) achieved these results
    with the command line for B2 adapted for larger batch size, with the recommended
    B0 dropout rate of 0.2.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[Michael Klachko](https://github.com/michaelklachko) 使用了适用于更大批量大小的B2命令行，并采用了推荐的B0丢失率为0.2。'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39
    top-5
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ResNet50 with JSD loss and RandAugment（干净+2x RA增强）- 79.04 top-1, 94.39 top-5
- en: Trained on two older 1080Ti cards, this took a while. Only slightly, non statistically
    better ImageNet validation result than my first good AugMix training of 78.99\.
    However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch,
    etc. Unlike my first AugMix runs, I’ve enabled SplitBatchNorm, disabled random
    erasing on the clean split, and cranked up random erasing prob on the 2 augmented
    paths.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在两张较旧的1080Ti卡上训练，这花了一些时间。与我第一次良好的AugMix训练相比，ImageNet验证结果仅略有提高，达到了78.99。然而，这些权重在ImageNetV2、ImageNet-Sketch等测试中更加稳健。与我的第一次AugMix运行不同，我启用了SplitBatchNorm，关闭了干净分割上的随机擦除，并增加了两个增强路径上的随机擦除概率。
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EfficientNet-ES（EdgeTPU-Small）with RandAugment - 78.066 top-1, 93.926 top-5
- en: Trained by [Andrew Lavin](https://github.com/andravin) with 8 V100 cards. Model
    EMA was not used, final checkpoint is the average of 8 best checkpoints during
    training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Andrew Lavin](https://github.com/andravin)使用8张V100卡进行训练。未使用模型EMA，最终检查点是在训练期间8个最佳检查点的平均值。
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5
- en: These params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely
    101\. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using
    a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd
    line below are tuned for 8 GPU training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数也适用于SE-ResNeXt-50和SK-ResNeXt-50，可能也适用于101。我用它们来训练SK-ResNeXt-50 32x4d，使用2个GPU，每个有效批次大小稍高的学习率（lr=0.18，b=192每个GPU）。下面的命令行针对8个GPU训练进行了调整。
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
