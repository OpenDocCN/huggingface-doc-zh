- en: Text-to-video
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到视频
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/text_to_video](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/text_to_video](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🧪 This pipeline is for research purposes only.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🧪 这个流程仅用于研究目的。
- en: '[ModelScope Text-to-Video Technical Report](https://arxiv.org/abs/2308.06571)
    is by Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, Shiwei
    Zhang.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[ModelScope文本到视频技术报告](https://arxiv.org/abs/2308.06571)由王久牛、袁航杰、陈大有、张颖雅、王翔、张世伟撰写。'
- en: 'The abstract from the paper is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*This paper introduces ModelScopeT2V, a text-to-video synthesis model that
    evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V
    incorporates spatio-temporal blocks to ensure consistent frame generation and
    smooth movement transitions. The model could adapt to varying frame numbers during
    training and inference, rendering it suitable for both image-text and video-text
    datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text
    encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in
    which 0.5 billion parameters are dedicated to temporal capabilities. The model
    demonstrates superior performance over state-of-the-art methods across three evaluation
    metrics. The code and an online demo are available at [https://modelscope.cn/models/damo/text-to-video-synthesis/summary](https://modelscope.cn/models/damo/text-to-video-synthesis/summary).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文介绍了ModelScopeT2V，这是一个从文本到图像合成模型（即稳定扩散）发展而来的文本到视频合成模型。ModelScopeT2V结合了时空块，以确保一致的帧生成和平滑的运动过渡。该模型可以适应训练和推断过程中不同的帧数，适用于图像文本和视频文本数据集。ModelScopeT2V汇集了三个组件（即VQGAN、文本编码器和去噪UNet），总共包含17亿个参数，其中5亿个参数专门用于时间能力。该模型在三个评估指标上表现优异。代码和在线演示可在[https://modelscope.cn/models/damo/text-to-video-synthesis/summary](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)找到。*'
- en: You can find additional information about Text-to-Video on the [project page](https://modelscope.cn/models/damo/text-to-video-synthesis/summary),
    [original codebase](https://github.com/modelscope/modelscope/), and try it out
    in a [demo](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis).
    Official checkpoints can be found at [damo-vilab](https://huggingface.co/damo-vilab)
    and [cerspense](https://huggingface.co/cerspense).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[项目页面](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)、[原始代码库](https://github.com/modelscope/modelscope/)和[演示](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis)中找到有关文本到视频的其他信息。官方检查点可以在[damo-vilab](https://huggingface.co/damo-vilab)和[cerspense](https://huggingface.co/cerspense)找到。
- en: Usage example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示例
- en: text-to-video-ms-1.7b
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: text-to-video-ms-1.7b
- en: 'Let’s start by generating a short video with the default length of 16 frames
    (2s at 8 fps):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从生成默认长度为16帧（8 fps下的2秒）的短视频开始：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Diffusers supports different optimization techniques to improve the latency
    and memory footprint of a pipeline. Since videos are often more memory-heavy than
    images, we can enable CPU offloading and VAE slicing to keep the memory footprint
    at bay.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Diffusers支持不同的优化技术，以改善流程的延迟和内存占用。由于视频通常比图像更占用内存，我们可以启用CPU卸载和VAE切片来控制内存占用。
- en: 'Let’s generate a video of 8 seconds (64 frames) on the same GPU using CPU offloading
    and VAE slicing:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在同一台GPU上使用CPU卸载和VAE切片生成8秒（64帧）的视频：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It just takes **7 GBs of GPU memory** to generate the 64 video frames using
    PyTorch 2.0, “fp16” precision and the techniques mentioned above.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch 2.0，“fp16”精度和上述技术生成64个视频帧仅需要**7 GB的GPU内存**。
- en: 'We can also use a different scheduler easily, using the same method we’d use
    for Stable Diffusion:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以轻松地使用不同的调度器，使用与稳定扩散相同的方法：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here are some sample outputs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例输出：
- en: '| An astronaut riding a horse. ![An astronaut riding a horse.](../Images/7624b329ee96da305cc8cda11bf80fae.png)
    | Darth vader surfing in waves. ![Darth vader surfing in waves.](../Images/fa05339350fb9cb455c843e7f524e44d.png)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 一名宇航员骑马。![一名宇航员骑马。](../Images/7624b329ee96da305cc8cda11bf80fae.png) | 达斯维达在波浪中冲浪。![达斯维达在波浪中冲浪。](../Images/fa05339350fb9cb455c843e7f524e44d.png)
    |'
- en: cerspense/zeroscope_v2_576w & cerspense/zeroscope_v2_XL
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cerspense/zeroscope_v2_576w和cerspense/zeroscope_v2_XL
- en: Zeroscope are watermark-free model and have been trained on specific sizes such
    as `576x320` and `1024x576`. One should first generate a video using the lower
    resolution checkpoint [`cerspense/zeroscope_v2_576w`](https://huggingface.co/cerspense/zeroscope_v2_576w)
    with [TextToVideoSDPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.TextToVideoSDPipeline),
    which can then be upscaled using [VideoToVideoSDPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.VideoToVideoSDPipeline)
    and [`cerspense/zeroscope_v2_XL`](https://huggingface.co/cerspense/zeroscope_v2_XL).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Zeroscope是无水印模型，已经在特定尺寸（如`576x320`和`1024x576`）上进行了训练。首先应使用较低分辨率检查点[`cerspense/zeroscope_v2_576w`](https://huggingface.co/cerspense/zeroscope_v2_576w)和[TextToVideoSDPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.TextToVideoSDPipeline)生成视频，然后可以使用[VideoToVideoSDPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.VideoToVideoSDPipeline)和[`cerspense/zeroscope_v2_XL`](https://huggingface.co/cerspense/zeroscope_v2_XL)进行放大。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now the video can be upscaled:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在视频可以进行放大：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are some sample outputs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例输出：
- en: '| Darth vader surfing in waves. ![Darth vader surfing in waves.](../Images/aab508b5ff9217f4f86b14a309983b1d.png)
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 达斯维达在波浪中冲浪。![达斯维达在波浪中冲浪。](../Images/aab508b5ff9217f4f86b14a309983b1d.png)
    |'
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看调度器[指南](../../using-diffusers/schedulers)以了解如何探索调度器速度和质量之间的权衡，并查看[在流程之间重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个流程中。
- en: TextToVideoSDPipeline
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TextToVideoSDPipeline
- en: '### `class diffusers.TextToVideoSDPipeline`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.TextToVideoSDPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L84)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L84)'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — 用于对文本进行标记化的[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` ([UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel))
    — A [UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)
    to denoise the encoded video latents.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel))
    — 用于去噪编码视频潜在表示的[UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 与`unet`结合使用的调度器，用于去噪编码图像潜在表示。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: Pipeline for text-to-video generation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到视频生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道还继承了以下加载方法：
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    用于加载文本反演嵌入'
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    用于加载 LoRA 权重'
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    用于保存 LoRA 权重'
- en: '#### `__call__`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L527)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L527)'
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated video.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成视频的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated video.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成视频的像素宽度。'
- en: '`num_frames` (`int`, *optional*, defaults to 16) — The number of video frames
    that are generated. Defaults to 16 frames which at 8 frames per seconds amounts
    to 2 seconds of video.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_frames` (`int`, *可选*, 默认为16) — 生成的视频帧数。默认为16帧，每秒8帧，相当于2秒的视频。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality videos at the expense
    of slower inference.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的视频，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成中不包含的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。当不使用引导时（`guidance_scale
    < 1`）将被忽略。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *可选*, 默认为1) — 每个提示生成的图像数量。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *可选*，默认为0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta
    (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`或`List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for video generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`. Latents
    should be of shape `(batch_size, num_channel, num_frames, height, width)`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作视频生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。潜变量应该具有形状`(batch_size,
    num_channel, num_frames, height, width)`。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。'
- en: '`output_type` (`str`, *optional*, defaults to `"np"`) — The output format of
    the generated video. Choose between `torch.FloatTensor` or `np.array`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*，默认为`"np"`) — 生成视频的输出格式。选择`torch.FloatTensor`或`np.array`之间。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    instead of a plain tuple.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*，默认为`True`) — 是否返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)而不是普通的tuple。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *可选*) — 在推断过程中每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *可选*，默认为1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调函数。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给`AttentionProcessor`的kwargs字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从CLIP跳过的层数。值为1意味着将使用预最终层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    or `tuple`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated frames.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成帧的列表。
- en: The call function to the pipeline for generation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `disable_freeu`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L523)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L523)'
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，则禁用FreeU机制。
- en: '#### `disable_vae_slicing`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L141)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L141)'
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片的VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `disable_vae_tiling`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L158)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L158)'
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用平铺式VAE解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。
- en: '#### `enable_freeu`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L500)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L500)'
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1` (`float`) — 第1阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2` (`float`) — 第2阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1` (`float`) — 第1阶段的缩放因子，用于放大骨干特征的贡献。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2` (`float`) — 第2阶段的缩放因子，用于放大骨干特征的贡献。'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后缀表示它们被应用的阶段。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如Stable Diffusion
    v1、v2和Stable Diffusion XL）的值组合。
- en: '#### `enable_vae_slicing`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L133)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L133)'
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片式VAE解码。启用此选项时，VAE将将输入张量分割成片段，以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。
- en: '#### `enable_vae_tiling`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L149)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L149)'
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 启用平铺式VAE解码。启用此选项时，VAE将将输入张量分割成多个瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。
- en: '#### `encode_prompt`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L199)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L199)'
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示 设备 — (`torch.device`): torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 每个提示应生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *可选*) — 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 从CLIP中跳过的层数，用于计算提示嵌入。值为1意味着将使用预最终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: VideoToVideoSDPipeline
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoToVideoSDPipeline
- en: '### `class diffusers.VideoToVideoSDPipeline`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.VideoToVideoSDPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L160)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L160)'
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode videos to and from
    latent representations.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）—
    变分自动编码器（VAE）模型，用于将视频编码和解码为潜在表示。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`CLIPTextModel`）— 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`CLIPTokenizer`）— 用于对文本进行标记化的[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` ([UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel))
    — A [UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)
    to denoise the encoded video latents.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)）—
    用于去噪编码视频潜在特征的[UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）—
    用于与`unet`结合使用以去噪编码图像潜在特征的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: Pipeline for text-guided video-to-video generation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本引导的视频到视频生成的流水线。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该流水线还继承了以下加载方法：
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    用于加载文本反演嵌入'
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    用于加载 LoRA 权重'
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    用于保存 LoRA 权重'
- en: '#### `__call__`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L632)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L632)'
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）— 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`video` (`List[np.ndarray]` or `torch.FloatTensor`) — `video` frames or tensor
    representing a video batch to be used as the starting point for the process. Can
    also accept video latents as `image`, if passing latents directly, it will not
    be encoded again.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`video`（`List[np.ndarray]`或`torch.FloatTensor`）— `video`帧或表示视频批次的张量，用作过程的起点。还可以接受视频潜在特征作为`image`，如果直接传递潜在特征，则不会再次编码。'
- en: '`strength` (`float`, *optional*, defaults to 0.8) — Indicates extent to transform
    the reference `video`. Must be between 0 and 1\. `video` is used as a starting
    point, adding more noise to it the larger the `strength`. The number of denoising
    steps depends on the amount of noise initially added. When `strength` is 1, added
    noise is maximum and the denoising process runs for the full number of iterations
    specified in `num_inference_steps`. A value of 1 essentially ignores `video`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength`（`float`，*可选*，默认为0.8）— 表示转换参考`video`的程度。必须介于0和1之间。`video`用作起点，添加的噪音越多，`strength`越大。去噪步骤的数量取决于最初添加的噪音量。当`strength`为1时，添加的噪音最大，去噪过程将运行指定的`num_inference_steps`的全部迭代次数。值为1基本上忽略`video`。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality videos at the expense
    of slower inference.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`（`int`，*可选*，默认为50）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的视频，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`（`float`，*可选*，默认为7.5）— 更高的指导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用指导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in video generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`（`str`或`List[str]`，*可选*）— 用于指导在视频生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时被忽略（`guidance_scale
    < 1`）。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *可选*, 默认为0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta
    (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for video generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`. Latents
    should be of shape `(batch_size, num_channel, num_frames, height, width)`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作视频生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。潜变量应该具有形状`(batch_size,
    num_channel, num_frames, height, width)`。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。'
- en: '`output_type` (`str`, *optional*, defaults to `"np"`) — The output format of
    the generated video. Choose between `torch.FloatTensor` or `np.array`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"np"`) — 生成视频的输出格式。选择`torch.FloatTensor`或`np.array`之间。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    instead of a plain tuple.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)而不是普通的tuple。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *可选*) — 一个在推断过程中每`callback_steps`步调用的函数。该函数接受以下参数：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *可选*, 默认为1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 一个kwargs字典，如果指定了，将传递给[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的CLIP层数。值为1表示将使用预终层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    or `tuple`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)或`tuple`'
- en: If `return_dict` is `True`, [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated frames.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，将返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)，否则将返回一个`tuple`，其中第一个元素是生成的帧的列表。
- en: The call function to the pipeline for generation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#### `disable_freeu`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L628)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L628)'
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，将禁用FreeU机制。
- en: '#### `disable_vae_slicing`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L217)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L217)'
- en: '[PRE19]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `disable_vae_tiling`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L234)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L234)'
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用平铺 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。
- en: '#### `enable_freeu`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L605)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L605)'
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1` (`float`) — 用于减弱跳过特征贡献的第 1 阶段的缩放因子。这样做是为了在增强去噪过程中减轻“过度平滑效应”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2` (`float`) — 用于减弱跳过特征贡献的第 2 阶段的缩放因子。这样做是为了在增强去噪过程中减轻“过度平滑效应”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1` (`float`) — 用于放大第 1 阶段的骨干特征贡献的缩放因子。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2` (`float`) — 用于放大第 2 阶段的骨干特征贡献的缩放因子。'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 FreeU 机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后缀表示应用它们的阶段。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如 Stable Diffusion
    v1、v2 和 Stable Diffusion XL）的值组合。
- en: '#### `enable_vae_slicing`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L209)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L209)'
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片 VAE 解码。当启用此选项时，VAE 将将输入张量切片以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。
- en: '#### `enable_vae_tiling`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L225)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L225)'
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 启用平铺 VAE 解码。当启用此选项时，VAE 将将输入张量分割成多个瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。
- en: '#### `encode_prompt`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L275)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L275)'
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`):
    torch 设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 应该为每个提示生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 不用于指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *optional*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA
    比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *optional*) — 从 CLIP 中跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: TextToVideoSDPipelineOutput
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TextToVideoSDPipelineOutput
- en: '### `class diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_output.py#L12)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_output.py#L12)'
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`frames` (`List[np.ndarray]` or `torch.FloatTensor`) — List of denoised frames
    (essentially images) as NumPy arrays of shape `(height, width, num_channels)`
    or as a `torch` tensor. The length of the list denotes the video length (the number
    of frames).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frames` (`List[np.ndarray]` 或 `torch.FloatTensor`) — 一系列去噪帧（基本上是图像），作为形状为`(height,
    width, num_channels)`的NumPy数组或`torch`张量。列表的长度表示视频长度（帧数）。'
- en: Output class for text-to-video pipelines.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本到视频流水线的输出类。
