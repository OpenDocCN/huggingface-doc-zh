- en: Pop2Piano
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pop2Piano
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano)
- en: '[![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/sweetcocoa/pop2piano)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/sweetcocoa/pop2piano)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover
    Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Pop2Pianoæ¨¡å‹ç”±Jongho Choiå’ŒKyogu Leeåœ¨[Pop2Pianoï¼šåŸºäºæµè¡ŒéŸ³é¢‘çš„é’¢ç´ç¿»å¥ç”Ÿæˆ](https://arxiv.org/abs/2211.00895)ä¸­æå‡ºã€‚
- en: Piano covers of pop music are widely enjoyed, but generating them from music
    is not a trivial task. It requires great expertise with playing piano as well
    as knowing different characteristics and melodies of a song. With Pop2Piano you
    can directly generate a cover from a songâ€™s audio waveform. It is the first model
    to directly generate a piano cover from pop audio without melody and chord extraction
    modules.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æµè¡ŒéŸ³ä¹çš„é’¢ç´ç¿»å¥å¹¿å—æ¬¢è¿ï¼Œä½†ä»éŸ³ä¹ä¸­ç”Ÿæˆå®ƒä»¬å¹¶ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ã€‚è¿™éœ€è¦å¯¹å¼¹å¥é’¢ç´æœ‰å¾ˆé«˜çš„ä¸“ä¸šçŸ¥è¯†ï¼ŒåŒæ—¶è¿˜è¦äº†è§£æ­Œæ›²çš„ä¸åŒç‰¹å¾å’Œæ—‹å¾‹ã€‚é€šè¿‡Pop2Pianoï¼Œæ‚¨å¯ä»¥ç›´æ¥ä»æ­Œæ›²çš„éŸ³é¢‘æ³¢å½¢ç”Ÿæˆç¿»å¥ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç›´æ¥ä»æµè¡ŒéŸ³é¢‘ç”Ÿæˆé’¢ç´ç¿»å¥çš„æ¨¡å‹ï¼Œè€Œæ— éœ€æ—‹å¾‹å’Œå’Œå¼¦æå–æ¨¡å—ã€‚
- en: 'Pop2Piano is an encoder-decoder Transformer model based on [T5](https://arxiv.org/pdf/1910.10683.pdf).
    The input audio is transformed to its waveform and passed to the encoder, which
    transforms it to a latent representation. The decoder uses these latent representations
    to generate token ids in an autoregressive way. Each token id corresponds to one
    of four different token types: time, velocity, note and â€˜specialâ€™. The token ids
    are then decoded to their equivalent MIDI file.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Pop2Pianoæ˜¯åŸºäº[T5](https://arxiv.org/pdf/1910.10683.pdf)çš„ç¼–ç å™¨-è§£ç å™¨Transformeræ¨¡å‹ã€‚è¾“å…¥éŸ³é¢‘è¢«è½¬æ¢ä¸ºå…¶æ³¢å½¢å¹¶ä¼ é€’ç»™ç¼–ç å™¨ï¼Œç¼–ç å™¨å°†å…¶è½¬æ¢ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚è§£ç å™¨ä½¿ç”¨è¿™äº›æ½œåœ¨è¡¨ç¤ºä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆä»¤ç‰Œidã€‚æ¯ä¸ªä»¤ç‰Œidå¯¹åº”äºå››ç§ä¸åŒçš„ä»¤ç‰Œç±»å‹ä¹‹ä¸€ï¼šæ—¶é—´ã€é€Ÿåº¦ã€éŸ³ç¬¦å’Œâ€œç‰¹æ®Šâ€ã€‚ç„¶åå°†ä»¤ç‰Œidè§£ç ä¸ºå…¶ç­‰æ•ˆçš„MIDIæ–‡ä»¶ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Piano covers of pop music are enjoyed by many people. However, the task of
    automatically generating piano covers of pop music is still understudied. This
    is partly due to the lack of synchronized {Pop, Piano Cover} data pairs, which
    made it challenging to apply the latest data-intensive deep learning-based methods.
    To leverage the power of the data-driven approach, we make a large amount of paired
    and synchronized {Pop, Piano Cover} data using an automated pipeline. In this
    paper, we present Pop2Piano, a Transformer network that generates piano covers
    given waveforms of pop music. To the best of our knowledge, this is the first
    model to generate a piano cover directly from pop audio without using melody and
    chord extraction modules. We show that Pop2Piano, trained with our dataset, is
    capable of producing plausible piano covers.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*è®¸å¤šäººå–œæ¬¢æµè¡ŒéŸ³ä¹çš„é’¢ç´ç¿»å¥ã€‚ç„¶è€Œï¼Œè‡ªåŠ¨ç”Ÿæˆæµè¡ŒéŸ³ä¹çš„é’¢ç´ç¿»å¥çš„ä»»åŠ¡ä»ç„¶æœªè¢«å……åˆ†ç ”ç©¶ã€‚éƒ¨åˆ†åŸå› æ˜¯ç¼ºä¹åŒæ­¥çš„{æµè¡ŒéŸ³ä¹ï¼Œé’¢ç´ç¿»å¥}æ•°æ®å¯¹ï¼Œè¿™ä½¿å¾—åº”ç”¨æœ€æ–°çš„æ•°æ®å¯†é›†å‹åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åˆ©ç”¨æ•°æ®é©±åŠ¨æ–¹æ³•çš„åŠ›é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿åˆ¶ä½œäº†å¤§é‡é…å¯¹å’ŒåŒæ­¥çš„{æµè¡ŒéŸ³ä¹ï¼Œé’¢ç´ç¿»å¥}æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Pop2Pianoï¼Œè¿™æ˜¯ä¸€ä¸ªTransformerç½‘ç»œï¼Œå¯ä»¥æ ¹æ®æµè¡ŒéŸ³ä¹çš„æ³¢å½¢ç”Ÿæˆé’¢ç´ç¿»å¥ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥ç›´æ¥ä»æµè¡ŒéŸ³é¢‘ç”Ÿæˆé’¢ç´ç¿»å¥çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä½¿ç”¨æ—‹å¾‹å’Œå’Œå¼¦æå–æ¨¡å—ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†è®­ç»ƒçš„Pop2Pianoèƒ½å¤Ÿç”Ÿæˆåˆç†çš„é’¢ç´ç¿»å¥ã€‚*'
- en: This model was contributed by [Susnato Dhar](https://huggingface.co/susnato).
    The original code can be found [here](https://github.com/sweetcocoa/pop2piano).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[Susnato Dhar](https://huggingface.co/susnato)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/sweetcocoa/pop2piano)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: 'To use Pop2Piano, you will need to install the ğŸ¤— Transformers library, as well
    as the following third party modules:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨Pop2Pianoï¼Œæ‚¨éœ€è¦å®‰è£…ğŸ¤— Transformersåº“ï¼Œä»¥åŠä»¥ä¸‹ç¬¬ä¸‰æ–¹æ¨¡å—ï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Please note that you may need to restart your runtime after installation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨å®‰è£…åé‡æ–°å¯åŠ¨è¿è¡Œæ—¶ã€‚
- en: Pop2Piano is an Encoder-Decoder based model like T5.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pop2Pianoæ˜¯ä¸€ç§åŸºäºç¼–ç å™¨-è§£ç å™¨çš„æ¨¡å‹ï¼Œç±»ä¼¼äºT5ã€‚
- en: Pop2Piano can be used to generate midi-audio files for a given audio sequence.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pop2Pianoå¯ç”¨äºä¸ºç»™å®šéŸ³é¢‘åºåˆ—ç”ŸæˆmidiéŸ³é¢‘æ–‡ä»¶ã€‚
- en: Choosing different composers in `Pop2PianoForConditionalGeneration.generate()`
    can lead to variety of different results.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`Pop2PianoForConditionalGeneration.generate()`ä¸­é€‰æ‹©ä¸åŒçš„ä½œæ›²å®¶å¯ä»¥äº§ç”Ÿä¸åŒç»“æœçš„å¤šæ ·æ€§ã€‚
- en: Setting the sampling rate to 44.1 kHz when loading the audio file can give good
    performance.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½éŸ³é¢‘æ–‡ä»¶æ—¶å°†é‡‡æ ·ç‡è®¾ç½®ä¸º44.1 kHzå¯ä»¥è·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚
- en: Though Pop2Piano was mainly trained on Korean Pop music, it also does pretty
    well on other Western Pop or Hip Hop songs.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°½ç®¡Pop2Pianoä¸»è¦æ˜¯åœ¨éŸ©å›½æµè¡ŒéŸ³ä¹ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†å®ƒåœ¨å…¶ä»–è¥¿æ–¹æµè¡ŒéŸ³ä¹æˆ–å˜»å“ˆæ­Œæ›²ä¸Šä¹Ÿè¡¨ç°ä¸é”™ã€‚
- en: Examples
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹
- en: 'Example using HuggingFace Dataset:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨HuggingFaceæ•°æ®é›†çš„ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Example using your own audio file:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨è‡ªå·±çš„éŸ³é¢‘æ–‡ä»¶ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Example of processing multiple audio files in batch:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶çš„ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor`
    and `Pop2PianoTokenizer`):'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶çš„ç¤ºä¾‹ï¼ˆä½¿ç”¨`Pop2PianoFeatureExtractor`å’Œ`Pop2PianoTokenizer`ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pop2PianoConfig
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pop2PianoConfig
- en: '### `class transformers.Pop2PianoConfig`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pop2PianoConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/configuration_pop2piano.py#L29)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/configuration_pop2piano.py#L29)'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 2400) â€” Vocabulary size of the
    `Pop2PianoForConditionalGeneration` model. Defines the number of different tokens
    that can be represented by the `inputs_ids` passed when calling [Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º2400ï¼‰- `Pop2PianoForConditionalGeneration`æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒä»¤ç‰Œæ•°é‡ã€‚'
- en: '`composer_vocab_size` (`int`, *optional*, defaults to 21) â€” Denotes the number
    of composers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`composer_vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º21ï¼‰- è¡¨ç¤ºä½œæ›²å®¶çš„æ•°é‡ã€‚'
- en: '`d_model` (`int`, *optional*, defaults to 512) â€” Size of the encoder layers
    and the pooler layer.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„å¤§å°ã€‚'
- en: '`d_kv` (`int`, *optional*, defaults to 64) â€” Size of the key, query, value
    projections per attention head. The `inner_dim` of the projection layer will be
    defined as `num_heads * d_kv`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_kv` (`int`, *å¯é€‰*, é»˜è®¤ä¸º64) â€” æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸­é”®ã€æŸ¥è¯¢ã€å€¼æŠ•å½±çš„å¤§å°ã€‚æŠ•å½±å±‚çš„`inner_dim`å°†å®šä¹‰ä¸º`num_heads
    * d_kv`ã€‚'
- en: '`d_ff` (`int`, *optional*, defaults to 2048) â€” Size of the intermediate feed
    forward layer in each `Pop2PianoBlock`.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_ff` (`int`, *å¯é€‰*, é»˜è®¤ä¸º2048) â€” æ¯ä¸ª`Pop2PianoBlock`ä¸­é—´çº§å‰é¦ˆå±‚çš„å¤§å°ã€‚'
- en: '`num_layers` (`int`, *optional*, defaults to 6) â€” Number of hidden layers in
    the Transformer encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º6) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_decoder_layers` (`int`, *optional*) â€” Number of hidden layers in the Transformer
    decoder. Will use the same value as `num_layers` if not set.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_decoder_layers` (`int`, *å¯é€‰*) â€” Transformerè§£ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ä¸`num_layers`ç›¸åŒçš„å€¼ã€‚'
- en: '`num_heads` (`int`, *optional*, defaults to 8) â€” Number of attention heads
    for each attention layer in the Transformer encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º8) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚'
- en: '`relative_attention_num_buckets` (`int`, *optional*, defaults to 32) â€” The
    number of buckets to use for each attention layer.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_num_buckets` (`int`, *å¯é€‰*, é»˜è®¤ä¸º32) â€” æ¯ä¸ªæ³¨æ„åŠ›å±‚ä½¿ç”¨çš„æ¡¶æ•°é‡ã€‚'
- en: '`relative_attention_max_distance` (`int`, *optional*, defaults to 128) â€” The
    maximum distance of the longer sequences for the bucket separation.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_max_distance` (`int`, *å¯é€‰*, é»˜è®¤ä¸º128) â€” ç”¨äºæ¡¶åˆ†ç¦»çš„è¾ƒé•¿åºåˆ—çš„æœ€å¤§è·ç¦»ã€‚'
- en: '`dropout_rate` (`float`, *optional*, defaults to 0.1) â€” The ratio for all dropout
    layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” æ‰€æœ‰dropoutå±‚çš„æ¯”ç‡ã€‚'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-6) â€” The epsilon
    used by the layer normalization layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1e-6) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) â€” A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1.0) â€” åˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„å› å­ï¼ˆåº”ä¿æŒä¸º1.0ï¼Œç”¨äºå†…éƒ¨åˆå§‹åŒ–æµ‹è¯•ï¼‰ã€‚'
- en: '`feed_forward_proj` (`string`, *optional*, defaults to `"gated-gelu"`) â€” Type
    of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feed_forward_proj` (`string`, *å¯é€‰*, é»˜è®¤ä¸º`"gated-gelu"`) â€” è¦ä½¿ç”¨çš„å‰é¦ˆå±‚ç±»å‹ã€‚åº”ä¸º`"relu"`æˆ–`"gated-gelu"`ä¹‹ä¸€ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚'
- en: '`dense_act_fn` (`string`, *optional*, defaults to `"relu"`) â€” Type of Activation
    Function to be used in `Pop2PianoDenseActDense` and in `Pop2PianoDenseGatedActDense`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dense_act_fn` (`string`, *å¯é€‰*, é»˜è®¤ä¸º`"relu"`) â€” ç”¨äº`Pop2PianoDenseActDense`å’Œ`Pop2PianoDenseGatedActDense`ä¸­çš„æ¿€æ´»å‡½æ•°ç±»å‹ã€‚'
- en: This is the configuration class to store the configuration of a [Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration).
    It is used to instantiate a Pop2PianoForConditionalGeneration model according
    to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the Pop2Piano
    [sweetcocoa/pop2piano](https://huggingface.co/sweetcocoa/pop2piano) architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªPop2PianoForConditionalGenerationæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºPop2Piano
    [sweetcocoa/pop2piano](https://huggingface.co/sweetcocoa/pop2piano)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: Pop2PianoFeatureExtractor
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pop2PianoFeatureExtractor
- en: '### `class transformers.Pop2PianoFeatureExtractor`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pop2PianoFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L5)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L5)'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `__call__`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Call self as a function.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è‡ªèº«ä½œä¸ºå‡½æ•°è°ƒç”¨ã€‚
- en: Pop2PianoForConditionalGeneration
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pop2PianoForConditionalGeneration
- en: '### `class transformers.Pop2PianoForConditionalGeneration`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pop2PianoForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1009)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1009)'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Pop2Piano Model with a `language modeling` head on top. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Pop2Pianoæ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´ã€‚è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `å‰å‘`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1109)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1109)'
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Pop2Piano is a model with
    relative position embeddings so you should be able to pad the inputs on both the
    right and the left. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for detail. [What are input IDs?](../glossary#input-ids) To know more on how to
    prepare `input_ids` for pretraining take a look a [Pop2Pianp Training](./Pop2Piano#training).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚Pop2Pianoæ˜¯ä¸€ä¸ªå¸¦æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§å¡«å……è¾“å…¥ã€‚å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æŸ¥çœ‹[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[Pop2Pianoè®­ç»ƒ](./Pop2Piano#training)ã€‚'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº`æœªå±è”½`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºæ ‡è®°ä¸º`å±è”½`çš„æ ‡è®°ã€‚[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary. Indices
    can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids) Pop2Piano
    uses the `pad_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`). To know more on how to prepare'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æŸ¥çœ‹[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)Pop2Pianoä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªéœ€é€‰æ‹©æœ€åçš„`decoder_input_ids`è¾“å…¥ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚è¦äº†è§£å¦‚ä½•å‡†å¤‡'
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚é»˜è®¤æƒ…å†µä¸‹è¿˜å°†ä½¿ç”¨å› æœæ©ç ã€‚'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules in
    the encoder. Mask values selected in `[0, 1]`:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿ç¼–ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`å±è”½`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`å±è”½`ã€‚
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è§£ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`å±è”½`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`å±è”½`ã€‚
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the cross-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`å±è”½`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`å±è”½`ã€‚
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, `optional`: *hidden_states*, `optional`: *attentions*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` is a
    sequence of hidden states at the output of the last layer of the encoder. Used
    in the cross-attention of the decoder.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œå¯é€‰ï¼š*hidden_states*ï¼Œå¯é€‰ï¼š*attentions*ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) â€” Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding. If `past_key_values`
    are used, the user can optionally input only the last `decoder_input_ids` (those
    that donâ€™t have their past key value states given to this model) of shape `(batch_size,
    1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œé•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å½¢çŠ¶ä¸º`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`çš„4ä¸ªå¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*)
    â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœè¦æ›´å¥½åœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å¾ˆæœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Does the same task as `inputs_embeds`. If `inputs_embeds`
    is not present but `input_features` is present then `input_features` will be considered
    as `inputs_embeds`.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*)
    â€” æ‰§è¡Œä¸`inputs_embeds`ç›¸åŒçš„ä»»åŠ¡ã€‚å¦‚æœä¸å­˜åœ¨`inputs_embeds`ä½†å­˜åœ¨`input_features`ï¼Œåˆ™å°†`input_features`è§†ä¸º`inputs_embeds`ã€‚'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix. If `decoder_input_ids` and `decoder_inputs_embeds` are
    both unset, `decoder_inputs_embeds` takes the value of `inputs_embeds`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length,
    hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100,
    0, ..., config.vocab_size - 1]`å†…ã€‚æ‰€æœ‰æ ‡ç­¾è®¾ç½®ä¸º`-100`çš„å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`å†…çš„ã€‚'
- en: Returns
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig))
    and inputs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*optional*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›
    â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: '#### `generate`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1217)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1217)'
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” This is the featurized version of audio generated
    by `Pop2PianoFeatureExtractor`. attention_mask â€” For batched generation `input_features`
    are padded to have the same shape across all examples. `attention_mask` helps
    to determine which areas were padded and which were not.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*)
    â€” è¿™æ˜¯ç”± `Pop2PianoFeatureExtractor` ç”Ÿæˆçš„éŸ³é¢‘çš„ç‰¹å¾åŒ–ç‰ˆæœ¬ã€‚attention_mask â€” å¯¹äºæ‰¹é‡ç”Ÿæˆï¼Œ`input_features`
    è¢«å¡«å……ä»¥ä½¿æ‰€æœ‰ç¤ºä¾‹å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚`attention_mask` æœ‰åŠ©äºç¡®å®šå“ªäº›åŒºåŸŸè¢«å¡«å……ï¼Œå“ªäº›æ²¡æœ‰è¢«å¡«å……ã€‚'
- en: 1 for tokens that are `not padded`,
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 ç”¨äº `æœªå¡«å……` çš„æ ‡è®°ã€‚
- en: 0 for tokens that are `padded`.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 ç”¨äº `å¡«å……` çš„æ ‡è®°ã€‚
- en: '`composer` (`str`, *optional*, defaults to `"composer1"`) â€” This value is passed
    to `Pop2PianoConcatEmbeddingToMel` to generate different embeddings for each `"composer"`.
    Please make sure that the composet value is present in `composer_to_feature_token`
    in `generation_config`. For an example please see [https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json](https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json)
    .'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`composer` (`str`, *optional*, defaults to `"composer1"`) â€” ä¼ é€’ç»™ `Pop2PianoConcatEmbeddingToMel`
    çš„å€¼ï¼Œç”¨äºä¸ºæ¯ä¸ª `"composer"` ç”Ÿæˆä¸åŒçš„åµŒå…¥ã€‚è¯·ç¡®ä¿ `composer` å€¼åœ¨ `generation_config` çš„ `composer_to_feature_token`
    ä¸­å­˜åœ¨ã€‚ä¾‹å¦‚ï¼Œè¯·å‚é˜… [https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json](https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json)ã€‚'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) â€” The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)â€™s
    default values, whose documentation should be checked to parameterize generation.
    kwargs â€” Ad hoc parametrization of `generate_config` and/or additional model-specific
    kwargs that will be forwarded to the `forward` function of the model. If the model
    is an encoder-decoder model, encoder specific kwargs should not be prefixed and
    decoder specific kwargs should be prefixed with *decoder_*.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`~generation.GenerationConfig`, *optional*) â€” ç”¨ä½œç”Ÿæˆè°ƒç”¨çš„åŸºæœ¬å‚æ•°åŒ–çš„ç”Ÿæˆé…ç½®ã€‚ä¼ é€’ç»™
    generate çš„ `**kwargs` åŒ¹é… `generation_config` çš„å±æ€§å°†è¦†ç›–å®ƒä»¬ã€‚å¦‚æœæœªæä¾› `generation_config`ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ï¼Œå…¶åŠ è½½ä¼˜å…ˆçº§å¦‚ä¸‹ï¼š1)
    ä» `generation_config.json` æ¨¡å‹æ–‡ä»¶ä¸­ï¼Œå¦‚æœå­˜åœ¨ï¼›2) ä»æ¨¡å‹é…ç½®ä¸­ã€‚è¯·æ³¨æ„ï¼ŒæœªæŒ‡å®šçš„å‚æ•°å°†ç»§æ‰¿ [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    çš„é»˜è®¤å€¼ï¼Œå…¶æ–‡æ¡£åº”è¯¥è¢«æ£€æŸ¥ä»¥å‚æ•°åŒ–ç”Ÿæˆã€‚kwargs â€” `generate_config` çš„ç‰¹å®šå‚æ•°åŒ–å’Œ/æˆ–å°†è½¬å‘åˆ°æ¨¡å‹çš„ `forward` å‡½æ•°çš„å…¶ä»–æ¨¡å‹ç‰¹å®š
    kwargsã€‚å¦‚æœæ¨¡å‹æ˜¯ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œåˆ™ç¼–ç å™¨ç‰¹å®šçš„ kwargs ä¸åº”è¯¥æœ‰å‰ç¼€ï¼Œè§£ç å™¨ç‰¹å®šçš„ kwargs åº”è¯¥ä»¥ *decoder_* ä¸ºå‰ç¼€ã€‚'
- en: Returns
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    æˆ– `torch.LongTensor`'
- en: 'A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`. Since Pop2Piano is an encoder-decoder model (`model.config.is_encoder_decoder=True`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)ï¼ˆå¦‚æœ
    `return_dict_in_generate=True` æˆ–å½“ `config.return_dict_in_generate=True` æ—¶ï¼‰æˆ–ä¸€ä¸ª
    `torch.FloatTensor`ã€‚ç”±äº Pop2Piano æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼ˆ`model.config.is_encoder_decoder=True`ï¼‰ï¼Œå¯èƒ½çš„
    [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    ç±»å‹ä¸ºï¼š
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
- en: Generates token ids for midi outputs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸º MIDI è¾“å‡ºç”Ÿæˆæ ‡è®° IDã€‚
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the modelâ€™s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`. For an overview
    of generation strategies and code examples, check out the [following guide](./generation_strategies).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æ§åˆ¶ç”Ÿæˆçš„å‚æ•°éƒ½åœ¨ `generation_config` ä¸­è®¾ç½®ï¼Œå¦‚æœæœªä¼ é€’ï¼Œåˆ™å°†è®¾ç½®ä¸ºæ¨¡å‹çš„é»˜è®¤ç”Ÿæˆé…ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡ä¼ é€’ç›¸åº”çš„å‚æ•°åˆ° generate()
    æ¥è¦†ç›–ä»»ä½• `generation_config`ï¼Œä¾‹å¦‚ `.generate(inputs, num_beams=4, do_sample=True)`ã€‚æœ‰å…³ç”Ÿæˆç­–ç•¥å’Œä»£ç ç¤ºä¾‹çš„æ¦‚è¿°ï¼Œè¯·æŸ¥çœ‹[ä»¥ä¸‹æŒ‡å—](./generation_strategies)ã€‚
- en: Pop2PianoTokenizer
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pop2PianoTokenizer
- en: '### `class transformers.Pop2PianoTokenizer`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pop2PianoTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L12)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L12)'
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `__call__`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Call self as a function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è‡ªèº«ä½œä¸ºå‡½æ•°è°ƒç”¨ã€‚
- en: Pop2PianoProcessor
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pop2PianoProcessor
- en: '### `class transformers.Pop2PianoProcessor`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pop2PianoProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L19)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L19)'
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#### `__call__`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Call self as a function.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è‡ªèº«ä½œä¸ºå‡½æ•°è°ƒç”¨ã€‚
