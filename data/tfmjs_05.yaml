- en: Use custom models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/transformers.js/custom_usage](https://huggingface.co/docs/transformers.js/custom_usage)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers.js/custom_usage](https://huggingface.co/docs/transformers.js/custom_usage)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Transformers.js uses [hosted pretrained models](https://huggingface.co/models?library=transformers.js)
    and [precompiled WASM binaries](https://cdn.jsdelivr.net/npm/@xenova/transformers@2.15.0/dist/),
    which should work out-of-the-box. You can customize this as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼ŒTransformers.jsä½¿ç”¨[æ‰˜ç®¡çš„é¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/models?library=transformers.js)å’Œ[é¢„ç¼–è¯‘çš„WASMäºŒè¿›åˆ¶æ–‡ä»¶](https://cdn.jsdelivr.net/npm/@xenova/transformers@2.15.0/dist/)ï¼Œåº”è¯¥å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚æ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è‡ªå®šä¹‰ï¼š
- en: Settings
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾ç½®
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For a full list of available settings, check out the [API Reference](./api/env).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨è®¾ç½®çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·æŸ¥çœ‹[APIå‚è€ƒ](./api/env)ã€‚
- en: Convert your models to ONNX
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„æ¨¡å‹è½¬æ¢ä¸ºONNX
- en: We recommend using our [conversion script](https://github.com/xenova/transformers.js/blob/main/scripts/convert.py)
    to convert your PyTorch, TensorFlow, or JAX models to ONNX in a single command.
    Behind the scenes, it uses [ğŸ¤— Optimum](https://huggingface.co/docs/optimum) to
    perform conversion and quantization of your model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®ä½¿ç”¨æˆ‘ä»¬çš„[è½¬æ¢è„šæœ¬](https://github.com/xenova/transformers.js/blob/main/scripts/convert.py)æ¥å°†æ‚¨çš„PyTorchã€TensorFlowæˆ–JAXæ¨¡å‹ä¸€æ¬¡æ€§è½¬æ¢ä¸ºONNXã€‚åœ¨å¹•åï¼Œå®ƒä½¿ç”¨[ğŸ¤—
    Optimum](https://huggingface.co/docs/optimum)æ¥æ‰§è¡Œæ¨¡å‹çš„è½¬æ¢å’Œé‡åŒ–ã€‚
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example, convert and quantize [bert-base-uncased](https://huggingface.co/bert-base-uncased)
    using:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è½¬æ¢å’Œé‡åŒ–[bert-base-uncased](https://huggingface.co/bert-base-uncased)ï¼š
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will save the following files to `./models/`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ä¿å­˜ä»¥ä¸‹æ–‡ä»¶åˆ°`./models/`ï¼š
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For the full list of supported architectures, see the [Optimum documentation](https://huggingface.co/docs/optimum/main/en/exporters/onnx/overview).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹æ”¯æŒçš„æ‰€æœ‰æ¶æ„çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚é˜…[Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/main/en/exporters/onnx/overview)ã€‚
