- en: A Q-Learning example
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个Q-Learning示例
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example](https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example](https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand Q-Learning, let’s take a simple example:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解Q-Learning，让我们来看一个简单的例子：
- en: '![Maze-Example](../Images/7bdcf448ce834a314efde42768c303f7.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/7bdcf448ce834a314efde42768c303f7.png)'
- en: You’re a mouse in this tiny maze. You always **start at the same starting point.**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是这个小迷宫中的一只老鼠。你总是**从同一个起点开始。**
- en: The goal is **to eat the big pile of cheese at the bottom right-hand corner** and
    avoid the poison. After all, who doesn’t like cheese?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是**在右下角吃掉大堆奶酪**并避免毒药。毕竟，谁不喜欢奶酪呢？
- en: The episode ends if we eat the poison, **eat the big pile of cheese**, or if
    we take more than five steps.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们吃了毒药、**吃了大堆奶酪**或者走了超过五步，情节就会结束。
- en: The learning rate is 0.1
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率为0.1
- en: The discount rate (gamma) is 0.99
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣率（gamma）为0.99
- en: '![Maze-Example](../Images/c17db319504a73cec7f8f89c466d8000.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/c17db319504a73cec7f8f89c466d8000.png)'
- en: 'The reward function goes like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数如下：
- en: '**+0:** Going to a state with no cheese in it.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**+0：** 去一个没有奶酪的状态。'
- en: '**+1:** Going to a state with a small cheese in it.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**+1：** 去一个有一小块奶酪的状态。'
- en: '**+10:** Going to the state with the big pile of cheese.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**+10：** 去一个有大堆奶酪的状态。'
- en: '**-10:** Going to the state with the poison and thus dying.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**-10：** 去一个有毒药的状态，从而死亡。'
- en: '**+0** If we take more than five steps.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**+0** 如果我们走了超过五步。'
- en: '![Maze-Example](../Images/f8fc089c4b1640dfdcc484c4f5916025.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/f8fc089c4b1640dfdcc484c4f5916025.png)'
- en: To train our agent to have an optimal policy (so a policy that goes right, right,
    down), **we will use the Q-Learning algorithm**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的智能体具有最优策略（即向右、向右、向下的策略），**我们将使用Q-Learning算法**。
- en: 'Step 1: Initialize the Q-table'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：初始化Q表
- en: '![Maze-Example](../Images/36ec86ed2fe8ef5a648fca96e67b8e6a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/36ec86ed2fe8ef5a648fca96e67b8e6a.png)'
- en: So, for now, **our Q-table is useless**; we need **to train our Q-function using
    the Q-Learning algorithm.**
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**目前我们的Q表是无用的**；我们需要**使用Q-Learning算法训练我们的Q函数。**
- en: 'Let’s do it for 2 training timesteps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行2个训练时间步：
- en: 'Training timestep 1:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间步1：
- en: 'Step 2: Choose an action using the Epsilon Greedy Strategy'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：使用Epsilon Greedy策略选择动作
- en: Because epsilon is big (= 1.0), I take a random action. In this case, I go right.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为epsilon很大（= 1.0），我采取了一个随机动作。在这种情况下，我向右走。
- en: '![Maze-Example](../Images/3b703676d561e8e0bcd1fb87c2b3aef2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/3b703676d561e8e0bcd1fb87c2b3aef2.png)'
- en: 'Step 3: Perform action At, get Rt+1 and St+1'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：执行动作At，获取Rt+1和St+1
- en: By going right, I get a small cheese, so<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">R_{t+1} = 1</annotation></semantics></math>Rt+1​=1
    and I’m in a new state.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 向右走，我得到了一小块奶酪，所以Rt+1 = 1，我进入了一个新的状态。
- en: '![Maze-Example](../Images/59cd579ddd1d870f7a3df5e80ebc2b03.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/59cd579ddd1d870f7a3df5e80ebc2b03.png)'
- en: 'Step 4: Update Q(St, At)'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：更新Q(St, At)
- en: We can now update<math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">Q(S_t, A_t)</annotation></semantics></math>Q(St​,At​)
    using our formula.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用我们的公式更新Q(St, At)。
- en: '![Maze-Example](../Images/039d07cc30eaeda29cabcae129111e00.png) ![Maze-Example](../Images/a6cf10447e88cd0d903730da115cb468.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/039d07cc30eaeda29cabcae129111e00.png) ![迷宫示例](../Images/a6cf10447e88cd0d903730da115cb468.png)'
- en: 'Training timestep 2:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间步2：
- en: 'Step 2: Choose an action using the Epsilon Greedy Strategy'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：使用Epsilon Greedy策略选择动作
- en: '**I take a random action again, since epsilon=0.99 is big**. (Notice we decay
    epsilon a little bit because, as the training progress, we want less and less
    exploration).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**我再次采取随机动作，因为epsilon=0.99很大**。（请注意，我们稍微降低epsilon，因为随着训练的进行，我们希望探索越来越少）。'
- en: I took the action ‘down’. **This is not a good action since it leads me to the
    poison.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了动作“向下”。**这不是一个好的动作，因为它导致我吃到了毒药。**
- en: '![Maze-Example](../Images/8a555fe2a851cda0cd8432c383e56d7f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/8a555fe2a851cda0cd8432c383e56d7f.png)'
- en: 'Step 3: Perform action At, get Rt+1 and St+1'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：执行动作At，获取Rt+1和St+1
- en: Because I ate poison, **I get<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mo>−</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">R_{t+1} = -10</annotation></semantics></math>Rt+1​=−10,
    and I die.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我吃了毒药，**我得到Rt+1 = -10，然后我死了。**
- en: '![Maze-Example](../Images/44ccf2ebfc7c5cc6d2383ac1d60d8f4a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/44ccf2ebfc7c5cc6d2383ac1d60d8f4a.png)'
- en: 'Step 4: Update Q(St, At)'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：更新Q(St, At)
- en: '![Maze-Example](../Images/b633bdc1c80d53eef7dad7daab4e8e1e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![迷宫示例](../Images/b633bdc1c80d53eef7dad7daab4e8e1e.png)'
- en: Because we’re dead, we start a new episode. But what we see here is that, **with
    two explorations steps, my agent became smarter.**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经死了，我们开始一个新的情节。但我们在这里看到的是，**通过两次探索步骤，我的智能体变得更聪明了。**
- en: As we continue exploring and exploiting the environment and updating Q-values
    using the TD target, the **Q-table will give us a better and better approximation.
    At the end of the training, we’ll get an estimate of the optimal Q-function.**
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续探索和利用环境，并使用TD目标更新Q值，**Q表将给我们一个越来越好的近似值。在训练结束时，我们将得到最优Q函数的估计。**
