- en: Quickstart
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速入门
- en: 'Original text: [https://huggingface.co/docs/trl/quickstart](https://huggingface.co/docs/trl/quickstart)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/trl/quickstart](https://huggingface.co/docs/trl/quickstart)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'Fine-tuning a language model via PPO consists of roughly three steps:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过PPO微调语言模型大致包括三个步骤：
- en: '**Rollout**: The language model generates a response or continuation based
    on a query which could be the start of a sentence.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**滚动**: 语言模型根据可能是句子开头的查询生成响应或延续。'
- en: '**Evaluation**: The query and response are evaluated with a function, model,
    human feedback, or some combination of them. The important thing is that this
    process should yield a scalar value for each query/response pair. The optimization
    will aim at maximizing this value.'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估**: 查询和响应通过函数、模型、人类反馈或它们的某种组合进行评估。重要的是，这个过程应该为每个查询/响应对产生一个标量值。优化将旨在最大化这个值。'
- en: '**Optimization**: This is the most complex part. In the optimisation step the
    query/response pairs are used to calculate the log-probabilities of the tokens
    in the sequences. This is done with the model that is trained and a reference
    model, which is usually the pre-trained model before fine-tuning. The KL-divergence
    between the two outputs is used as an additional reward signal to make sure the
    generated responses don’t deviate too far from the reference language model. The
    active language model is then trained with PPO.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化**: 这是最复杂的部分。在优化步骤中，查询/响应对用于计算序列中令牌的对数概率。这是通过经过训练的模型和一个参考模型来完成的，通常是微调之前的预训练模型。两个输出之间的KL散度被用作额外的奖励信号，以确保生成的响应不会偏离太远的参考语言模型。然后使用PPO对主动语言模型进行训练。'
- en: 'The full process is illustrated in the following figure:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程如下图所示：
- en: '![](../Images/00db2b272395f1f726efb5b3cd2a73ca.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00db2b272395f1f726efb5b3cd2a73ca.png)'
- en: Minimal example
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小示例
- en: The following code illustrates the steps above.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了上述步骤。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In general, you would run steps 3-6 in a for-loop and run it on many diverse
    queries. You can find more realistic examples in the examples section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您会在for循环中运行步骤3-6，并在许多不同的查询上运行它。您可以在示例部分找到更多现实示例。
- en: How to use a trained model
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用训练好的模型
- en: After training a `AutoModelForCausalLMWithValueHead`, you can directly use the
    model in `transformers`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完`AutoModelForCausalLMWithValueHead`后，您可以直接在`transformers`中使用该模型。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can also load your model with `AutoModelForCausalLMWithValueHead` if you
    want to use the value head, for example to continue training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用值头部，也可以使用`AutoModelForCausalLMWithValueHead`加载您的模型，例如继续训练。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
