- en: RLHF
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLHF
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf](https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf](https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback (RLHF) is a **methodology for integrating
    human data labels into a RL-based optimization process**. It is motivated by the
    **challenge of modeling human preferences**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类反馈中学习强化学习（RLHF）是一种**将人类数据标签整合到基于RL的优化过程中的方法论**。它受到**建模人类偏好挑战**的启发。
- en: For many questions, even if you could try and write down an equation for one
    ideal, humans differ on their preferences.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多问题，即使你可以尝试写下一个理想的方程，人们在他们的偏好上也存在差异。
- en: Updating models **based on measured data is an avenue to try and alleviate these
    inherently human ML problems**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 基于测量数据更新模型**是一种尝试缓解这些固有的人类ML问题的途径**。
- en: Start Learning about RLHF
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始学习关于RLHF的知识
- en: 'To start learning about RLHF:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 开始学习关于RLHF的知识：
- en: 'Read this introduction: [Illustrating Reinforcement Learning from Human Feedback
    (RLHF)](https://huggingface.co/blog/rlhf).'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读这篇介绍：[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)。
- en: Watch the recorded live we did some weeks ago, where Nathan covered the basics
    of Reinforcement Learning from Human Feedback (RLHF) and how this technology is
    being used to enable state-of-the-art ML tools like ChatGPT. Most of the talk
    is an overview of the interconnected ML models. It covers the basics of Natural
    Language Processing and RL and how RLHF is used on large language models. We then
    conclude with open questions in RLHF.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观看我们几周前录制的现场直播，Nathan在其中介绍了从人类反馈中学习强化学习（RLHF）的基础知识，以及这项技术如何被用来实现像ChatGPT这样的最先进ML工具。大部分讨论是关于相互连接的ML模型的概述。它涵盖了自然语言处理和RL的基础知识，以及RLHF如何在大型语言模型上使用。最后，我们提出了RLHF中的开放问题。
- en: '[https://www.youtube-nocookie.com/embed/2MBJOuVq380](https://www.youtube-nocookie.com/embed/2MBJOuVq380)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/2MBJOuVq380](https://www.youtube-nocookie.com/embed/2MBJOuVq380)'
- en: 'Read other blogs on this topic, such as [Closed-API vs Open-source continues:
    RLHF, ChatGPT, data moats](https://robotic.substack.com/p/rlhf-chatgpt-data-moats).
    Let us know if there are more you like!'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读其他关于这个主题的博客，比如[封闭API vs 开源持续：RLHF，ChatGPT，数据壕沟](https://robotic.substack.com/p/rlhf-chatgpt-data-moats)。如果你喜欢的话，告诉我们还有更多！
- en: Additional readings
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外阅读
- en: '*Note, this is copied from the Illustrating RLHF blog post above*. Here is
    a list of the most prevalent papers on RLHF to date. The field was recently popularized
    with the emergence of DeepRL (around 2017) and has grown into a broader study
    of the applications of LLMs from many large technology companies. Here are some
    papers on RLHF that pre-date the LM focus:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是迄今为止关于RLHF的最流行论文列表。该领域最近因DeepRL的出现（大约在2017年）而变得流行，并已发展成为许多大型技术公司LLMs应用的广泛研究。以下是一些关于RLHF的早期论文，先于LM焦点：
- en: '[TAMER: Training an Agent Manually via Evaluative Reinforcement](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf)
    (Knox and Stone 2008): Proposed a learned agent where humans provided scores on
    the actions taken iteratively to learn a reward model.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TAMER：通过评估强化手动训练代理](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf)（Knox和Stone，2008年）：提出了一个学习代理，其中人类提供了对迭代学习奖励模型的行为评分。'
- en: '[Interactive Learning from Policy-Dependent Human Feedback](http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf)
    (MacGlashan et al. 2017): Proposed an actor-critic algorithm, COACH, where human
    feedback (both positive and negative) is used to tune the advantage function.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于政策相关人类反馈的交互式学习](http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf)（MacGlashan等人，2017年）：提出了一个演员-评论家算法COACH，其中人类反馈（积极和消极）用于调整优势函数。'
- en: '[Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)
    (Christiano et al. 2017): RLHF applied on preferences between Atari trajectories.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)（Christiano等人，2017年）：在Atari轨迹之间应用RLHF。'
- en: '[Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces](https://ojs.aaai.org/index.php/AAAI/article/view/11485)
    (Warnell et al. 2018): Extends the TAMER framework where a deep neural network
    is used to model the reward prediction.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Deep TAMER：在高维状态空间中的交互式代理塑造](https://ojs.aaai.org/index.php/AAAI/article/view/11485)（Warnell等人，2018年）：扩展了TAMER框架，其中使用深度神经网络来建模奖励预测。'
- en: 'And here is a snapshot of the growing set of papers that show RLHF’s performance
    for LMs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一组不断增长的论文快照，展示了RLHF在LM中的表现：
- en: '[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)
    (Zieglar et al. 2019): An early paper that studies the impact of reward learning
    on four specific tasks.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于人类偏好微调语言模型](https://arxiv.org/abs/1909.08593)（Zieglar等人，2019年）：一篇早期论文，研究了奖励学习对四个特定任务的影响。'
- en: '[Learning to summarize with human feedback](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html)
    (Stiennon et al., 2020): RLHF applied to the task of summarizing text. Also, [Recursively
    Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862) (OpenAI
    Alignment Team 2021), follow on work summarizing books.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习使用人类反馈进行摘要](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html)（Stiennon等人，2020年）：将RLHF应用于文本摘要任务。此外，[使用人类反馈递归总结书籍](https://arxiv.org/abs/2109.10862)（OpenAI
    Alignment Team 2021），后续工作总结书籍。'
- en: '[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)
    (OpenAI, 2021): Using RLHF to train an agent to navigate the web.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WebGPT：使用人类反馈进行浏览器辅助问答](https://arxiv.org/abs/2112.09332)（OpenAI，2021年）：使用RLHF训练一个代理来浏览网络。'
- en: 'InstructGPT: [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
    (OpenAI Alignment Team 2022): RLHF applied to a general language model [[Blog
    post](https://openai.com/blog/instruction-following/) on InstructGPT].'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'InstructGPT: [训练语言模型遵循人类反馈的指令](https://arxiv.org/abs/2203.02155) (OpenAI Alignment
    Team 2022年): 将RLHF应用于通用语言模型[[关于InstructGPT的博文](https://openai.com/blog/instruction-following/)]'
- en: 'GopherCite: [Teaching language models to support answers with verified quotes](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes)
    (Menick et al. 2022): Train a LM with RLHF to return answers with specific citations.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GopherCite: [教导语言模型支持带有验证引用的答案](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes)
    (Menick等人，2022年): 使用RLHF训练LM以返回带有特定引用的答案'
- en: 'Sparrow: [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/abs/2209.14375)
    (Glaese et al. 2022): Fine-tuning a dialogue agent with RLHF'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sparrow: [通过有针对性的人类判断改善对话代理的对齐](https://arxiv.org/abs/2209.14375) (Glaese等人，2022年):
    使用RLHF对对话代理进行微调'
- en: '[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)
    (OpenAI 2022): Training a LM with RLHF for suitable use as an all-purpose chat
    bot.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT: 优化语言模型用于对话](https://openai.com/blog/chatgpt/) (OpenAI 2022年): 使用RLHF训练LM，以适合作为通用聊天机器人使用'
- en: '[Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760)
    (Gao et al. 2022): studies the scaling properties of the learned preference model
    in RLHF.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[奖励模型过度优化的扩展定律](https://arxiv.org/abs/2210.10760) (Gao等人，2022年): 研究RLHF中学习偏好模型的扩展特性'
- en: '[Training a Helpful and Harmless Assistant with Reinforcement Learning from
    Human Feedback](https://arxiv.org/abs/2204.05862) (Anthropic, 2022): A detailed
    documentation of training a LM assistant with RLHF.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用人类反馈的强化学习训练一个有用且无害的助手](https://arxiv.org/abs/2204.05862) (Anthropic, 2022年):
    对使用RLHF训练LM助手的详细文档'
- en: '[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and
    Lessons Learned](https://arxiv.org/abs/2209.07858) (Ganguli et al. 2022): A detailed
    documentation of efforts to “discover, measure, and attempt to reduce [language
    models] potentially harmful outputs.”'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对抗性团队语言模型以减少伤害: 方法、扩展行为和经验教训](https://arxiv.org/abs/2209.07858) (Ganguli等人，2022年):
    关于“发现、衡量和尝试减少[语言模型]潜在有害输出”的详细文档'
- en: '[Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning](https://arxiv.org/abs/2208.02294)
    (Cohen at al. 2022): Using RL to enhance the conversational skill of an open-ended
    dialogue agent.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用强化学习进行开放式对话的动态规划](https://arxiv.org/abs/2208.02294) (Cohen等人，2022年): 使用RL增强开放式对话代理的会话技能'
- en: '[Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks,
    Baselines, and Building Blocks for Natural Language Policy Optimization](https://arxiv.org/abs/2210.01241)
    (Ramamurthy and Ammanabrolu et al. 2022): Discusses the design space of open-source
    tools in RLHF and proposes a new algorithm NLPO (Natural Language Policy Optimization)
    as an alternative to PPO.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习是否适用于自然语言处理？: 自然语言政策优化的基准、基线和构建块](https://arxiv.org/abs/2210.01241) (Ramamurthy和Ammanabrolu等人，2022年):
    讨论RLHF中开源工具的设计空间，并提出一种新算法NLPO（自然语言政策优化）作为PPO的替代方案。'
- en: Author
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者
- en: This section was written by [Nathan Lambert](https://twitter.com/natolambert)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是由[Nathan Lambert](https://twitter.com/natolambert)撰写的
