- en: LUKE
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卢克
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/luke](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/luke)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/luke](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/luke)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The LUKE model was proposed in [LUKE: Deep Contextualized Entity Representations
    with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada,
    Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto. It is based on
    RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism,
    which helps improve performance on various downstream tasks involving reasoning
    about entities such as named entity recognition, extractive and cloze-style question
    answering, entity typing, and relation classification.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'LUKE模型是由Ikuya Yamada、Akari Asai、Hiroyuki Shindo、Hideaki Takeda和Yuji Matsumoto在[LUKE:
    Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057)中提出的。它基于RoBERTa，并添加了实体嵌入以及实体感知自注意机制，有助于提高在涉及推理实体的各种下游任务上的性能，如命名实体识别、提取式和填空式问答、实体类型划分和关系分类。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是论文摘要：
- en: '*Entity representations are useful in natural language tasks involving entities.
    In this paper, we propose new pretrained contextualized representations of words
    and entities based on the bidirectional transformer. The proposed model treats
    words and entities in a given text as independent tokens, and outputs contextualized
    representations of them. Our model is trained using a new pretraining task based
    on the masked language model of BERT. The task involves predicting randomly masked
    words and entities in a large entity-annotated corpus retrieved from Wikipedia.
    We also propose an entity-aware self-attention mechanism that is an extension
    of the self-attention mechanism of the transformer, and considers the types of
    tokens (words or entities) when computing attention scores. The proposed model
    achieves impressive empirical performance on a wide range of entity-related tasks.
    In particular, it obtains state-of-the-art results on five well-known datasets:
    Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named
    entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive
    question answering).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*实体表示在涉及实体的自然语言任务中非常有用。在本文中，我们提出了基于双向变换器的新的预训练上下文化单词和实体表示。所提出的模型将给定文本中的单词和实体视为独立标记，并输出它们的上下文化表示。我们的模型使用基于BERT的遮蔽语言模型的新预训练任务进行训练。该任务涉及在从维基百科检索的大型实体注释语料库中预测随机屏蔽的单词和实体。我们还提出了一种实体感知自注意机制，它是变换器自注意机制的扩展，并在计算注意力分数时考虑标记类型（单词或实体）。所提出的模型在广泛的与实体相关任务上取得了令人印象深刻的实证性表现。特别是，在五个知名数据集上取得了最新成果：Open
    Entity（实体类型划分）、TACRED（关系分类）、CoNLL-2003（命名实体识别）、ReCoRD（填空式问答）和SQuAD 1.1（提取式问答）。'
- en: This model was contributed by [ikuyamada](https://huggingface.co/ikuyamada)
    and [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/studio-ousia/luke).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[ikuyamada](https://huggingface.co/ikuyamada)和[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/studio-ousia/luke)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: This implementation is the same as [RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel)
    with the addition of entity embeddings as well as an entity-aware self-attention
    mechanism, which improves performance on tasks involving reasoning about entities.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此实现与[RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel)相同，只是添加了实体嵌入以及实体感知自注意机制，从而提高了在涉及推理实体的任务中的性能。
- en: LUKE treats entities as input tokens; therefore, it takes `entity_ids`, `entity_attention_mask`,
    `entity_token_type_ids` and `entity_position_ids` as extra input. You can obtain
    those using [LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LUKE将实体视为输入标记；因此，它接受`entity_ids`、`entity_attention_mask`、`entity_token_type_ids`和`entity_position_ids`作为额外输入。您可以使用[LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer)来获取这些信息。
- en: '[LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer)
    takes `entities` and `entity_spans` (character-based start and end positions of
    the entities in the input text) as extra input. `entities` typically consist of
    [MASK] entities or Wikipedia entities. The brief description when inputting these
    entities are as follows:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer)接受`entities`和`entity_spans`（实体在输入文本中基于字符的起始和结束位置）作为额外输入。`entities`通常包括[MASK]实体或维基百科实体。输入这些实体时的简要描述如下：'
- en: '*Inputting [MASK] entities to compute entity representations*: The [MASK] entity
    is used to mask entities to be predicted during pretraining. When LUKE receives
    the [MASK] entity, it tries to predict the original entity by gathering the information
    about the entity from the input text. Therefore, the [MASK] entity can be used
    to address downstream tasks requiring the information of entities in text such
    as entity typing, relation classification, and named entity recognition.'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入[MASK]实体以计算实体表示*：[MASK]实体用于在预训练期间屏蔽待预测的实体。当LUKE接收到[MASK]实体时，它会尝试通过从输入文本中收集有关实体的信息来预测原始实体。因此，[MASK]实体可用于处理需要文本中实体信息的下游任务，如实体类型划分、关系分类和命名实体识别。'
- en: '*Inputting Wikipedia entities to compute knowledge-enhanced token representations*:
    LUKE learns rich information (or knowledge) about Wikipedia entities during pretraining
    and stores the information in its entity embedding. By using Wikipedia entities
    as input tokens, LUKE outputs token representations enriched by the information
    stored in the embeddings of these entities. This is particularly effective for
    tasks requiring real-world knowledge, such as question answering.'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入维基百科实体以计算知识增强的令牌表示*：LUKE在预训练期间学习关于维基百科实体的丰富信息（或知识），并将信息存储在其实体嵌入中。通过使用维基百科实体作为输入令牌，LUKE输出由这些实体嵌入中存储的信息丰富的令牌表示。这对于需要现实世界知识的任务（如问答）特别有效。'
- en: 'There are three head models for the former use case:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前一用例有三个头模型：
- en: '[LukeForEntityClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityClassification),
    for tasks to classify a single entity in an input text such as entity typing,
    e.g. the [Open Entity dataset](https://www.cs.utexas.edu/~eunsol/html_pages/open_entity.html).
    This model places a linear head on top of the output entity representation.'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于实体分类的LukeForEntityClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityClassification)，用于对输入文本中的单个实体进行分类，例如实体类型标注，例如[Open
    Entity数据集](https://www.cs.utexas.edu/~eunsol/html_pages/open_entity.html)。该模型在输出实体表示上放置了一个线性头。'
- en: '[LukeForEntityPairClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityPairClassification),
    for tasks to classify the relationship between two entities such as relation classification,
    e.g. the [TACRED dataset](https://nlp.stanford.edu/projects/tacred/). This model
    places a linear head on top of the concatenated output representation of the pair
    of given entities.'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于实体对分类的LukeForEntityPairClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityPairClassification)，用于对两个实体之间的关系进行分类，例如关系分类，例如[TACRED数据集](https://nlp.stanford.edu/projects/tacred/)。该模型在给定实体对的连接输出表示上放置了一个线性头。'
- en: '[LukeForEntitySpanClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntitySpanClassification),
    for tasks to classify the sequence of entity spans, such as named entity recognition
    (NER). This model places a linear head on top of the output entity representations.
    You can address NER using this model by inputting all possible entity spans in
    the text to the model.'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于实体跨度分类的LukeForEntitySpanClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntitySpanClassification)，用于对实体跨度序列进行分类，例如命名实体识别（NER）。该模型在输出实体表示上放置了一个线性头。您可以通过将文本中的所有可能实体跨度输入到模型中来使用此模型来处理NER。'
- en: '[LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer)
    has a `task` argument, which enables you to easily create an input to these head
    models by specifying `task="entity_classification"`, `task="entity_pair_classification"`,
    or `task="entity_span_classification"`. Please refer to the example code of each
    head models.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer)有一个`task`参数，通过指定`task="entity_classification"`、`task="entity_pair_classification"`或`task="entity_span_classification"`，可以轻松地创建这些头模型的输入。请参考每个头模型的示例代码。'
- en: 'Usage example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用法示例：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[A demo notebook on how to fine-tune [LukeForEntityPairClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityPairClassification)
    for relation classification](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LUKE)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于如何微调[LukeForEntityPairClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityPairClassification)用于关系分类的演示笔记本](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LUKE)'
- en: '[Notebooks showcasing how you to reproduce the results as reported in the paper
    with the HuggingFace implementation of LUKE](https://github.com/studio-ousia/luke/tree/master/notebooks)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[展示如何使用HuggingFace LUKE实现重现论文中结果的笔记本](https://github.com/studio-ousia/luke/tree/master/notebooks)'
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[令牌分类任务指南](../tasks/token_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question_answering)'
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掩码语言建模任务指南](../tasks/masked_language_modeling)'
- en: '[Multiple choice task guide](../tasks/multiple_choice)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多项选择任务指南](../tasks/multiple_choice)'
- en: LukeConfig
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeConfig
- en: '### `class transformers.LukeConfig`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/configuration_luke.py#L29)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/configuration_luke.py#L29)'
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 50267) — Vocabulary size of the
    LUKE model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *可选*, 默认为50267) — LUKE模型的词汇表大小。定义了在调用[LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel)时可以表示的不同令牌数量。'
- en: '`entity_vocab_size` (`int`, *optional*, defaults to 500000) — Entity vocabulary
    size of the LUKE model. Defines the number of different entities that can be represented
    by the `entity_ids` passed when calling [LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_vocab_size` (`int`, *可选*, 默认为500000) — LUKE模型的实体词汇表大小。定义了在调用[LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel)时可以表示的不同实体数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化层的维度。'
- en: '`entity_emb_size` (`int`, *optional*, defaults to 256) — The number of dimensions
    of the entity embedding.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_emb_size` (`int`, *可选*, 默认为256) — 实体嵌入的维度数。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, 默认为3072) — Transformer编码器中“中间”（通常称为前馈）层的维度。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`Callable`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, 默认为0.1) — 注意力概率的dropout比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, 默认为512) — 该模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024或2048）。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling [LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *optional*, 默认为2) — 在调用[LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel)时传递的`token_type_ids`的词汇大小。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`use_entity_aware_attention` (`bool`, *optional*, defaults to `True`) — Whether
    or not the model should use the entity-aware self-attention mechanism proposed
    in [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention
    (Yamada et al.)](https://arxiv.org/abs/2010.01057).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_entity_aware_attention` (`bool`, *optional*, 默认为`True`) — 模型是否应该使用[LUKE:
    Deep Contextualized Entity Representations with Entity-aware Self-attention (Yamada
    et al.)](https://arxiv.org/abs/2010.01057)中提出的实体感知自注意机制。'
- en: '`classifier_dropout` (`float`, *optional*) — The dropout ratio for the classification
    head.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout` (`float`, *optional*) — 分类头的dropout比率。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 1) — Padding token id.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, 默认为1) — 填充标记id。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 0) — Beginning of stream token
    id.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*, 默认为0) — 流的起始标记id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — End of stream token id.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, 默认为2) — 流的结束标记id。'
- en: This is the configuration class to store the configuration of a [LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel).
    It is used to instantiate a LUKE model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the LUKE [studio-ousia/luke-base](https://huggingface.co/studio-ousia/luke-base)
    architecture.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel)配置的配置类。根据指定的参数实例化LUKE模型，定义模型架构。使用默认值实例化配置将产生类似于LUKE
    [studio-ousia/luke-base](https://huggingface.co/studio-ousia/luke-base)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: LukeTokenizer
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeTokenizer
- en: '### `class transformers.LukeTokenizer`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/tokenization_luke.py#L193)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/tokenization_luke.py#L193)'
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件的路径。'
- en: '`entity_vocab_file` (`str`) — Path to the entity vocabulary file.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_vocab_file` (`str`) — 实体词汇文件的路径。'
- en: '`task` (`str`, *optional*) — Task for which you want to prepare sequences.
    One of `"entity_classification"`, `"entity_pair_classification"`, or `"entity_span_classification"`.
    If you specify this argument, the entity sequence is automatically created based
    on the given entity span(s).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, *optional*) — 您想要准备序列的任务。其中之一是`"entity_classification"`、`"entity_pair_classification"`或`"entity_span_classification"`。如果指定此参数，实体序列将根据给定的实体跨度自动创建。'
- en: '`max_entity_length` (`int`, *optional*, defaults to 32) — The maximum length
    of `entity_ids`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_entity_length` (`int`, *optional*, 默认为32) — `entity_ids`的最大长度。'
- en: '`max_mention_length` (`int`, *optional*, defaults to 30) — The maximum number
    of tokens inside an entity span.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_mention_length` (`int`, *optional*, 默认为30) — 实体跨度内的最大标记数。'
- en: '`entity_token_1` (`str`, *optional*, defaults to `<ent>`) — The special token
    used to represent an entity span in a word token sequence. This token is only
    used when `task` is set to `"entity_classification"` or `"entity_pair_classification"`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_1` (`str`, *optional*, 默认为`<ent>`) — 用于表示单词标记序列中实体跨度的特殊标记。仅当`task`设置为`"entity_classification"`或`"entity_pair_classification"`时才使用此标记。'
- en: '`entity_token_2` (`str`, *optional*, defaults to `<ent2>`) — The special token
    used to represent an entity span in a word token sequence. This token is only
    used when `task` is set to `"entity_pair_classification"`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_2`（`str`，*可选*，默认为`<ent2>`）— 用于表示单词标记序列中实体跨度的特殊标记。仅当`task`设置为`"entity_pair_classification"`时才使用此标记。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors`（`str`，*可选*，默认为`"replace"`）— 解码字节为UTF-8时要遵循的范例。有关更多信息，请参阅[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开头标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*，默认为`"</s>"`）— 序列结尾标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`（`str`，*可选*，默认为`"</s>"`）— 分隔符标记，在从多个序列构建序列时使用，例如，用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token`（`str`，*可选*，默认为`"<s>"`）— 在进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。在使用特殊标记构建时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为`"<pad>"`）— 用于填充的标记，例如，当批处理不同长度的序列时。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`（`str`，*可选*，默认为`"<mask>"`）— 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (LUKE tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space`（`bool`，*可选*，默认为`False`）— 是否在输入前添加一个初始空格。这允许将前导单词视为任何其他单词。（LUKE分词器通过前面的空格检测单词的开头）。'
- en: Constructs a LUKE tokenizer, derived from the GPT-2 tokenizer, using byte-level
    Byte-Pair-Encoding.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个LUKE分词器，从GPT-2分词器派生，使用字节级字节对编码。
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器已经训练成将空格视为标记的一部分（有点像sentencepiece），因此一个单词将
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子开头（无空格）或不是时，将以不同方式编码：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在实例化此分词器时或在对某些文本调用它时传递`add_prefix_space=True`来避免这种行为，但由于该模型不是以这种方式进行预训练的，因此可能会导致性能下降。
- en: When used with `is_split_into_words=True`, this tokenizer will add a space before
    each word (even the first one).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当与`is_split_into_words=True`一起使用时，此分词器将在每个单词之前添加一个空格（即使是第一个单词）。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods. It also creates entity sequences,
    namely `entity_ids`, `entity_attention_mask`, `entity_token_type_ids`, and `entity_position_ids`
    to be used by the LUKE model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考这个超类以获取有关这些方法的更多信息。它还创建实体序列，即`entity_ids`、`entity_attention_mask`、`entity_token_type_ids`和`entity_position_ids`，供LUKE模型使用。
- en: '#### `__call__`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/tokenization_luke.py#L577)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/tokenization_luke.py#L577)'
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence must be a string. Note that this tokenizer does not
    support tokenization based on pretokenized strings.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`（`str`，`List[str]`，`List[List[str]]`）— 要编码的序列或序列批次。每个序列必须是一个字符串。请注意，此分词器不支持基于预分词字符串的分词。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch
    of sequences to be encoded. Each sequence must be a string. Note that this tokenizer
    does not support tokenization based on pretokenized strings.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`（`str`，`List[str]`，`List[List[str]]`）— 要编码的序列或批次。每个序列必须是一个字符串。请注意，此分词器不支持基于预分词字符串的分词。'
- en: '`entity_spans` (`List[Tuple[int, int]]`, `List[List[Tuple[int, int]]]`, *optional*)
    — The sequence or batch of sequences of entity spans to be encoded. Each sequence
    consists of tuples each with two integers denoting character-based start and end
    positions of entities. If you specify `"entity_classification"` or `"entity_pair_classification"`
    as the `task` argument in the constructor, the length of each sequence must be
    1 or 2, respectively. If you specify `entities`, the length of each sequence must
    be equal to the length of each sequence of `entities`.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_spans`（`List[Tuple[int, int]]`，`List[List[Tuple[int, int]]]`，*可选*）—
    要编码的实体跨度序列或批次。每个序列由元组组成，每个元组包含两个整数，表示实体的基于字符的起始和结束位置。如果在构造函数中将 `task` 参数指定为 `"entity_classification"`
    或 `"entity_pair_classification"`，则每个序列的长度必须分别为 1 或 2。如果指定了 `entities`，则每个序列的长度必须等于
    `entities` 的每个序列的长度。'
- en: '`entity_spans_pair` (`List[Tuple[int, int]]`, `List[List[Tuple[int, int]]]`,
    *optional*) — The sequence or batch of sequences of entity spans to be encoded.
    Each sequence consists of tuples each with two integers denoting character-based
    start and end positions of entities. If you specify the `task` argument in the
    constructor, this argument is ignored. If you specify `entities_pair`, the length
    of each sequence must be equal to the length of each sequence of `entities_pair`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_spans_pair`（`List[Tuple[int, int]]`，`List[List[Tuple[int, int]]]`，*可选*）—
    要编码的实体跨度序列或批次。每个序列由元组组成，每个元组包含两个整数，表示实体的基于字符的起始和结束位置。如果在构造函数中指定了 `task` 参数，则将忽略此参数。如果指定了
    `entities_pair`，则每个序列的长度必须等于 `entities_pair` 的每个序列的长度。'
- en: '`entities` (`List[str]`, `List[List[str]]`, *optional*) — The sequence or batch
    of sequences of entities to be encoded. Each sequence consists of strings representing
    entities, i.e., special entities (e.g., [MASK]) or entity titles of Wikipedia
    (e.g., Los Angeles). This argument is ignored if you specify the `task` argument
    in the constructor. The length of each sequence must be equal to the length of
    each sequence of `entity_spans`. If you specify `entity_spans` without specifying
    this argument, the entity sequence or the batch of entity sequences is automatically
    constructed by filling it with the [MASK] entity.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entities`（`List[str]`，`List[List[str]]`，*可选*）— 要编码的实体序列或批次。每个序列由表示实体的字符串组成，即特殊实体（例如
    [MASK]）或维基百科的实体标题（例如洛杉矶）。如果在构造函数中指定了 `task` 参数，则将忽略此参数。每个序列的长度必须等于 `entity_spans`
    的每个序列的长度。如果指定了 `entity_spans` 而没有指定此参数，则实体序列或实体序列批次将通过填充 [MASK] 实体来自动构建。'
- en: '`entities_pair` (`List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences of entities to be encoded. Each sequence consists of strings
    representing entities, i.e., special entities (e.g., [MASK]) or entity titles
    of Wikipedia (e.g., Los Angeles). This argument is ignored if you specify the
    `task` argument in the constructor. The length of each sequence must be equal
    to the length of each sequence of `entity_spans_pair`. If you specify `entity_spans_pair`
    without specifying this argument, the entity sequence or the batch of entity sequences
    is automatically constructed by filling it with the [MASK] entity.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entities_pair`（`List[str]`，`List[List[str]]`，*可选*）— 要编码的实体序列或批次。每个序列由表示实体的字符串组成，即特殊实体（例如
    [MASK]）或维基百科的实体标题（例如洛杉矶）。如果在构造函数中指定了 `task` 参数，则将忽略此参数。每个序列的长度必须等于 `entity_spans_pair`
    的每个序列的长度。如果指定了 `entity_spans_pair` 而没有指定此参数，则实体序列或实体序列批次将通过填充 [MASK] 实体来自动构建。'
- en: '`max_entity_length` (`int`, *optional*) — The maximum length of `entity_ids`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_entity_length`（`int`，*可选*）— `entity_ids` 的最大长度。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`（`bool`，*可选*，默认为 `True`）— 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens`
    函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加 `bos` 或 `eos` 标记，这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为
    `False`）— 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度（使用 `max_length` 参数）或者填充到模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：不进行填充（即可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，`str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为
    `False`）— 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供一对序列（或一批序列），则逐标记截断，从一对序列中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供一对序列（或一批序列），则仅截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供一对序列（或一批序列），则仅截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）: 不截断（即，可以输出长度大于模型最大可接受输入大小的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则将使用预定义的模型最大长度，如果截断/填充参数需要最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，当 `return_overflowing_tokens=True`
    时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预分词（例如，已经分成单词）。如果设置为
    `True`，分词器会假定输入已经分成单词（例如，通过空格分割），然后进行分词。这对于命名实体识别或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活 `padding`。这对于在具有计算能力
    `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *可选*) — 是否返回 token 类型 ID。如果保持默认设置，将根据特定分词器的默认设置返回
    token 类型 ID，由 `return_outputs` 属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是 token 类型 ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认设置返回注意力掩码，由
    `return_outputs` 属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *可选*, 默认为 `False`) — 是否返回溢出的标记序列。如果提供一对输入
    ID 序列（或一批序列），并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出的标记。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *可选*, 默认为 `False`) — 是否返回特殊标记掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *可选*, 默认为 `False`) — 是否返回每个标记的 `(char_start,
    char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length`（`bool`，*可选*，默认为`False`） — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`（`bool`，*可选*，默认为`True`） — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法'
- en: Returns
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要馈送到模型的标记ID列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要馈送到模型的标记类型ID列表（当`return_token_type_ids=True`或者*“token_type_ids”*在`self.model_input_names`中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定模型应该关注哪些标记的索引列表（当`return_attention_mask=True`或者*“attention_mask”*在`self.model_input_names`中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`entity_ids` — List of entity ids to be fed to a model.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids` — 要馈送到模型的实体ID列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`entity_position_ids` — List of entity positions in the input sequence to be
    fed to a model.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids` — 输入序列中实体位置的列表，将被馈送到模型中。'
- en: '`entity_token_type_ids` — List of entity token type ids to be fed to a model
    (when `return_token_type_ids=True` or if *“entity_token_type_ids”* is in `self.model_input_names`).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids` — 要馈送到模型的实体标记类型ID列表（当`return_token_type_ids=True`或者*“entity_token_type_ids”*在`self.model_input_names`中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`entity_attention_mask` — List of indices specifying which entities should
    be attended to by the model (when `return_attention_mask=True` or if *“entity_attention_mask”*
    is in `self.model_input_names`).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask` — 指定模型应该关注哪些实体的索引列表（当`return_attention_mask=True`或者*“entity_attention_mask”*在`self.model_input_names`中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`entity_start_positions` — List of the start positions of entities in the word
    token sequence (when `task="entity_span_classification"`).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_start_positions` — 单词标记序列中实体的开始位置列表（当`task="entity_span_classification"`时）。'
- en: '`entity_end_positions` — List of the end positions of entities in the word
    token sequence (when `task="entity_span_classification"`).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_end_positions` — 单词标记序列中实体的结束位置列表（当`task="entity_span_classification"`时）。'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出标记序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 被截断的标记数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。'
- en: '`length` — The length of the inputs (when `return_length=True`)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当`return_length=True`时）'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences, depending on the task you want to prepare
    them for.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为模型准备一个或多个序列或一个或多个序列对的主要方法，具体取决于您要为其准备的任务。
- en: '#### `save_vocabulary`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/tokenization_luke.py#L1692)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/tokenization_luke.py#L1692)'
- en: '[PRE6]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: LukeModel
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeModel
- en: '### `class transformers.LukeModel`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1009)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1009)'
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare LUKE model transformer outputting raw hidden-states for both word tokens
    and entities without any specific head on top.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 裸LUKE模型变压器输出原始隐藏状态，既适用于单词标记，也适用于实体，没有特定的头部。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1043)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1043)'
- en: '[PRE8]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    序列输入标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 索引可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获得。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]` 之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示 `未被掩码` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示 `被掩码` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引选择在 `[0, 1]` 之间：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings -
    1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    实体词汇中实体标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 索引可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获得。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — 用于避免在填充实体标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]` 之间：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示 `未被掩码` 的实体标记，
- en: 0 for entity tokens that are `masked`.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示 `被掩码` 的实体标记。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — 段标记索引，用于指示实体标记输入的第一部分和第二部分。索引选择在 `[0, 1]` 之间：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *部分 A* 实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *部分 B* 实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — 每个输入实体在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings
    - 1]`。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids`
    索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*)
    — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。更多细节请参见返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。更多细节请参见返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.BaseLukeModelOutputWithPooling` or
    `tuple(torch.FloatTensor)`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.BaseLukeModelOutputWithPooling`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.BaseLukeModelOutputWithPooling` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.BaseLukeModelOutputWithPooling`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入而异的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`entity_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, entity_length,
    hidden_size)`) — Sequence of entity hidden-states at the output of the last layer
    of the model.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, entity_length,
    hidden_size)`) — 模型最后一层的实体隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    further processed by a Linear layer and a Tanh activation function.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和Tanh激活函数进一步处理。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（嵌入输出的一个+每层输出的一个）。模型在每一层的隐藏状态加上初始嵌入输出。'
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, entity_length, hidden_size)`的`torch.FloatTensor`元组（嵌入输出的一个+每层输出的一个）。模型在每一层的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length + entity_length,
    sequence_length + entity_length)`. Attentions weights after the attention softmax,
    used to compute the weighted average in the self-attention heads.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length + entity_length, sequence_length
    + entity_length)`的`torch.FloatTensor`元组（每层一个）。自注意力头部中的注意力权重softmax后的值，用于计算自注意力头部中的加权平均值。'
- en: The [LukeModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel)
    forward method, overrides the `__call__` special method.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeyModel](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在之后调用`Module`实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: LukeForMaskedLM
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForMaskedLM
- en: '### `class transformers.LukeForMaskedLM`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeForMaskedLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1262)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1262)'
- en: '[PRE10]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）—
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The LUKE model with a language modeling head and entity prediction head on top
    for masked language modeling and masked entity prediction.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE模型具有语言建模头和顶部的实体预测头，用于掩码语言建模和掩码实体预测。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1295)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1295)'
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表`未被掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表`被掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于一个*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于一个*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`）— 实体词汇表中实体标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask`（形状为`(batch_size, entity_length)`的`torch.FloatTensor`，*可选*）—
    避免在填充实体标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表`未被掩码`的实体标记，
- en: 0 for entity tokens that are `masked`.
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表`被掩码`的实体标记。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示实体标记输入的第一部分和第二部分。索引选择在`[0, 1]`之间：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于一个*部分A*实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于一个*部分B*实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — 每个输入实体在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings
    - 1]`中选择。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。选择的掩码值在`[0, 1]`中。'
- en: 1 indicates the head is `not masked`,
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`not masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`中的标记。'
- en: '`entity_labels` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Labels for computing the masked language modeling loss. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_labels` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`中的标记。'
- en: Returns
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.LukeMaskedLMOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.LukeMaskedLMOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.LukeMaskedLMOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.LukeMaskedLMOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包含根据配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — The sum of masked language modeling (MLM) loss and entity prediction
    loss.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — 掩码语言建模（MLM）损失和实体预测损失的总和。'
- en: '`mlm_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when
    `labels` is provided) — Masked language modeling (MLM) loss.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlm_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when
    `labels` is provided) — 掩码语言建模（MLM）损失。'
- en: '`mep_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when
    `labels` is provided) — Masked entity prediction (MEP) loss.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mep_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when
    `labels` is provided) — 掩码实体预测（MEP）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`entity_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.vocab_size)`) — Prediction scores of the entity prediction head (scores
    for each entity vocabulary token before SoftMax).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.vocab_size)`) — 实体预测头的预测分数（SoftMax之前每个实体词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — `torch.FloatTensor`元组（一个用于嵌入输出，一个用于每个层的输出）的形状为`(batch_size,
    sequence_length, hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型输出的隐藏状态加上初始嵌入输出。
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, entity_length, hidden_size)`的`torch.FloatTensor`元组。模型在每一层的输出（嵌入的输出和每一层的输出）的实体隐藏状态。每一层的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [LukeForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[LuKeForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForMaskedLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: LukeForEntityClassification
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForEntityClassification
- en: '### `class transformers.LukeForEntityClassification`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeForEntityClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1397)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1397)'
- en: '[PRE12]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The LUKE model with a classification head on top (a linear layer on top of the
    hidden state of the first entity token) for entity classification tasks, such
    as Open Entity.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE模型在顶部有一个分类头（在第一个实体标记的隐藏状态上有一个线性层）用于实体分类任务，如Open Entity。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1417)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1417)'
- en: '[PRE13]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表未被“掩盖”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表被“掩盖”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*句子A*标记。
- en: 1 corresponds to a *sentence B* token.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    实体词汇表中实体标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — 避免在填充实体标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的实体标记，
- en: 0 for entity tokens that are `masked`.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`masked`的实体标记。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — 段标记索引，指示实体标记输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*部分A*实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*部分B*实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — 每个输入实体在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings
    - 1]`。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中的选定头部失效的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)` or `(batch_size, num_labels)`,
    *optional*) — Labels for computing the classification loss. If the shape is `(batch_size,)`,
    the cross entropy loss is used for the single-label classification. In this case,
    labels should contain the indices that should be in `[0, ..., config.num_labels
    - 1]`. If the shape is `(batch_size, num_labels)`, the binary cross entropy loss
    is used for the multi-label classification. In this case, labels should only contain
    `[0, 1]`, where 0 and 1 indicate false and true, respectively.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)` or `(batch_size, num_labels)`,
    *optional*) — 用于计算分类损失的标签。如果形状为`(batch_size,)`，则单标签分类使用交叉熵损失。在这种情况下，标签应包含应在`[0,
    ..., config.num_labels - 1]`中的索引。如果形状为`(batch_size, num_labels)`，则多标签分类使用二元交叉熵损失。在这种情况下，标签应只包含`[0,
    1]`，其中0和1分别表示false和true。'
- en: Returns
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.EntityClassificationOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.EntityClassificationOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.EntityClassificationOutput` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.EntityClassificationOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入而异的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification scores (before SoftMax).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。模型在每个层的输出的隐藏状态加上初始嵌入输出。'
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, entity_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。模型在每个层的输出的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。'
- en: The [LukeForEntityClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeForEntityClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: LukeForEntityPairClassification
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForEntityPairClassification
- en: '### `class transformers.LukeForEntityPairClassification`'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeForEntityPairClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1512)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1512)'
- en: '[PRE15]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The LUKE model with a classification head on top (a linear layer on top of the
    hidden states of the two entity tokens) for entity pair classification tasks,
    such as TACRED.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE模型在顶部具有分类头（两个实体标记的隐藏状态上的线性层）用于实体对分类任务，如TACRED。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1532)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1532)'
- en: '[PRE16]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被屏蔽的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被屏蔽的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    指示输入的第一部分和第二部分的段标记索引。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`）- 实体词汇中实体标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask`（形状为`(batch_size, entity_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充实体标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被屏蔽的实体标记，
- en: 0 for entity tokens that are `masked`.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被屏蔽的实体标记。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`，*可选*）-
    指示实体标记输入的第一部分和第二部分的段标记索引。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*部分A*实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*部分B*实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids`（形状为`(batch_size, entity_length, max_mention_length)`的`torch.LongTensor`，*可选*）-
    每个输入实体在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被屏蔽。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)` or `(batch_size, num_labels)`,
    *optional*) — Labels for computing the classification loss. If the shape is `(batch_size,)`,
    the cross entropy loss is used for the single-label classification. In this case,
    labels should contain the indices that should be in `[0, ..., config.num_labels
    - 1]`. If the shape is `(batch_size, num_labels)`, the binary cross entropy loss
    is used for the multi-label classification. In this case, labels should only contain
    `[0, 1]`, where 0 and 1 indicate false and true, respectively.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`或`(batch_size, num_labels)`，*optional*)
    — 用于计算分类损失的标签。如果形状为`(batch_size,)`，则用于单标签分类的交叉熵损失。在这种情况下，标签应包含应在`[0, ..., config.num_labels
    - 1]`中的索引。如果形状为`(batch_size, num_labels)`，则用于多标签分类的二元交叉熵损失。在这种情况下，标签应只包含`[0, 1]`，其中0和1分别表示false和true。'
- en: Returns
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.EntityPairClassificationOutput` or
    `tuple(torch.FloatTensor)`'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.EntityPairClassificationOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.EntityPairClassificationOutput` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.EntityPairClassificationOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification scores (before SoftMax).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — SoftMax之前的分类分数。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。每层输出的模型隐藏状态加上初始嵌入输出。'
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, entity_length, hidden_size)`的`torch.FloatTensor`元组。每层输出的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。'
- en: The [LukeForEntityPairClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityPairClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeyForEntityPairClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntityPairClassification)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默忽略它们。
- en: 'Examples:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: LukeForEntitySpanClassification
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForEntitySpanClassification
- en: '### `class transformers.LukeForEntitySpanClassification`'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeForEntitySpanClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1632)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1632)'
- en: '[PRE18]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The LUKE model with a span classification head on top (a linear layer on top
    of the hidden states output) for tasks such as named entity recognition.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE模型，顶部带有一个跨度分类头（隐藏状态输出的线性层），用于命名实体识别等任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1652)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1652)'
- en: '[PRE19]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被掩码的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被掩码的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    指示输入的第一部分和第二部分的段标记索引。索引选在`[0, 1]`之间：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应一个*句子A*标记。
- en: 1 corresponds to a *sentence B* token.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应一个*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选在范围`[0, config.max_position_embeddings - 1]`内。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`）— 实体词汇表中实体标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask`（形状为`(batch_size, entity_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充实体标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被掩码的实体标记，
- en: 0 for entity tokens that are `masked`.
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被掩码的实体标记。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`，*可选*）—
    指示实体标记输入的第一部分和第二部分的段标记索引。索引选在`[0, 1]`之间：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应一个*部分A*实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应一个*部分B*实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids`（形状为`(batch_size, entity_length, max_mention_length)`的`torch.LongTensor`，*可选*）—
    每个输入实体在位置嵌入中的位置索引。选在范围`[0, config.max_position_embeddings - 1]`内。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*）
    — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在 `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩盖。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*） — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`entity_start_positions` (`torch.LongTensor`) — The start positions of entities
    in the word token sequence.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_start_positions` (`torch.LongTensor`) — 实体在单词标记序列中的起始位置。'
- en: '`entity_end_positions` (`torch.LongTensor`) — The end positions of entities
    in the word token sequence.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_end_positions` (`torch.LongTensor`) — 实体在单词标记序列中的结束位置。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, entity_length)` or `(batch_size,
    entity_length, num_labels)`, *optional*) — Labels for computing the classification
    loss. If the shape is `(batch_size, entity_length)`, the cross entropy loss is
    used for the single-label classification. In this case, labels should contain
    the indices that should be in `[0, ..., config.num_labels - 1]`. If the shape
    is `(batch_size, entity_length, num_labels)`, the binary cross entropy loss is
    used for the multi-label classification. In this case, labels should only contain
    `[0, 1]`, where 0 and 1 indicate false and true, respectively.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为 `(batch_size, entity_length)` 或 `(batch_size,
    entity_length, num_labels)`，*可选*） — 用于计算分类损失的标签。如果形状为 `(batch_size, entity_length)`，则使用交叉熵损失进行单标签分类。在这种情况下，标签应包含应在
    `[0, ..., config.num_labels - 1]` 中的索引。如果形状为 `(batch_size, entity_length, num_labels)`，则使用二元交叉熵损失进行多标签分类。在这种情况下，标签应只包含
    `[0, 1]`，其中 0 和 1 分别表示 false 和 true。'
- en: Returns
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.EntitySpanClassificationOutput` or
    `tuple(torch.FloatTensor)`'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.EntitySpanClassificationOutput` 或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.EntitySpanClassificationOutput` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.luke.modeling_luke.EntitySpanClassificationOutput` 或一个
    `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时），包括根据配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供了 `labels` 时返回） — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, entity_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为 `(batch_size, entity_length, config.num_labels)`)
    — 分类分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每个层的输出的隐藏状态加上初始嵌入输出。'
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, entity_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每个层的输出的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每个层一个）。注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。'
- en: The [LukeForEntitySpanClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntitySpanClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeyForEntitySpanClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForEntitySpanClassification)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: LukeForSequenceClassification
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForSequenceClassification
- en: '### `class transformers.LukeForSequenceClassification`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1778)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1778)'
- en: '[PRE21]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）—
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The LUKE Model transformer with a sequence classification/regression head on
    top (a linear layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE模型变压器，顶部带有一个序列分类/回归头（在池化输出的顶部有一个线性层），例如用于GLUE任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1798)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1798)'
- en: '[PRE22]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“masked”掩盖的标记，将其设为1，
- en: 0 for tokens that are `masked`.
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“masked”掩盖的标记，将其设为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`）— 实体词汇中实体标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask`（形状为`(batch_size, entity_length)`的`torch.FloatTensor`，*可选*）—
    避免在填充实体标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“masked”掩盖的实体标记，将其设为1，
- en: 0 for entity tokens that are `masked`.
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“masked”掩盖的实体标记，将其设为0。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — 段标记索引，用于指示实体标记输入的第一部分和第二部分。索引选在`[0, 1]`范围内：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*部分A*实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*部分B*实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — 每个输入实体在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings
    - 1]`。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`范围内。'
- en: 1 indicates the head is `not masked`,
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.LukeSequenceClassifierOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.LukeSequenceClassifierOutput` 或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.LukeSequenceClassifierOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.LukeSequenceClassifierOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — 分类（或回归，如果config.num_labels==1）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    分类（或回归，如果config.num_labels==1）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — 形状为`(batch_size, sequence_length,
    hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型每层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — 形状为`(batch_size, entity_length, hidden_size)`的`torch.FloatTensor`元组（嵌入输出的输出加上每层的输出）。模型每层输出的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — 形状为`(batch_size, num_heads,
    sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [LukeForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example of single-label classification:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签分类的示例：
- en: '[PRE23]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Example of multi-label classification:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类的示例：
- en: '[PRE24]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: LukeForMultipleChoice
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForMultipleChoice
- en: '### `class transformers.LukeForMultipleChoice`'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeForMultipleChoice`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L2107)'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L2107)'
- en: '[PRE25]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “config”（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。
- en: The LUKE Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE模型在顶部具有多选分类头（在池化输出的顶部和softmax上的线性层），例如用于RocStories/SWAG任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L2127)'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L2127)'
- en: '[PRE26]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “input_ids”（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`）-
    词汇表中输入序列标记的索引。
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “attention_mask”（形状为`(batch_size, num_choices, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：
- en: 1 for tokens that are `not masked`,
  id: totrans-511
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于未被“掩码”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于被“掩码”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “token_type_ids”（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`，*可选*）-
    段标记索引，用于指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：
- en: 0 corresponds to a *sentence A* token,
  id: totrans-515
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于“句子A”标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-516
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于“句子B”标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “position_ids”（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`，*可选*）-
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “entity_ids”（形状为`(batch_size, entity_length)`的`torch.LongTensor`）- 实体词汇表中实体标记的索引。
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask`（形状为`(batch_size, entity_length)`的`torch.FloatTensor`，*可选*）
    - 避免在填充实体标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的实体标记，
- en: 0 for entity tokens that are `masked`.
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的实体标记为0。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`，*可选*）
    - 指示实体标记输入的第一部分和第二部分的段标记索引。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-526
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*部分A*实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-527
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*部分B*实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids`（形状为`(batch_size, entity_length, max_mention_length)`的`torch.LongTensor`，*可选*）
    - 每个输入实体在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, num_choices, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）
    - 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）
    - 用于使自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-531
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-532
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*） - 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*） - 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*） - 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices-1]` where `num_choices` is the size of the second dimension of
    the input tensors. (See `input_ids` above)'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*） - 用于计算多选分类损失的标签。索引应在`[0,
    ..., num_choices-1]`中，其中`num_choices`是输入张量的第二维的大小。（参见上面的`input_ids`）'
- en: Returns
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.LukeMultipleChoiceModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.LukeMultipleChoiceModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.LukeMultipleChoiceModelOutput` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.LukeMultipleChoiceModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括各种元素，取决于配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为*(1,)*的`torch.FloatTensor`，*可选*，当提供`labels`时返回） - 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_choices)`的`torch.FloatTensor`） - *num_choices*是输入张量的第二维。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SoftMax之前的分类分数。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的隐藏状态加上可选的初始嵌入输出。
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, entity_length, hidden_size)`
    的 `torch.FloatTensor` 元组。模型在每一层输出的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [LukeForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeyForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForMultipleChoice)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE27]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: LukeForTokenClassification
  id: totrans-552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForTokenClassification
- en: '### `class transformers.LukeForTokenClassification`'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.LukeForTokenClassification` 类'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1891)'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1891)'
- en: '[PRE28]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The LUKE Model with a token classification head on top (a linear layer on top
    of the hidden-states output). To solve Named-Entity Recognition (NER) task using
    LUKE, `LukeForEntitySpanClassification` is more suitable than this class.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE 模型在顶部具有一个标记分类头（隐藏状态输出的线性层）。为了使用 LUKE 解决命名实体识别（NER）任务，比起这个类，更适合使用 `LukeForEntitySpanClassification`。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1913)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1913)'
- en: '[PRE29]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-566
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]` 中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-569
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被 `masked` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-570
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `masked` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 段标记索引，用于指示输入的第一部分和第二部分。索引选择在 `[0, 1]` 中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-573
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-574
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令牌类型ID是什么？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “position_ids”（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    每个输入序列令牌在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置ID是什么？
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “entity_ids”（形状为`(batch_size, entity_length)`的`torch.LongTensor`）- 实体词汇中实体令牌的索引。
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “entity_attention_mask”（形状为`(batch_size, entity_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充实体令牌索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：
- en: 1 for entity tokens that are `not masked`,
  id: totrans-581
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“掩盖”的实体令牌，ID为1，
- en: 0 for entity tokens that are `masked`.
  id: totrans-582
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩盖”的实体令牌，ID为0。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “entity_token_type_ids”（形状为`(batch_size, entity_length)`的`torch.LongTensor`，*可选*）-
    段令牌索引，用于指示实体令牌输入的第一部分和第二部分。索引在`[0, 1]`中选择：
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-584
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*部分A*实体令牌，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-585
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*部分B*实体令牌。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “entity_position_ids”（形状为`(batch_size, entity_length, max_mention_length)`的`torch.LongTensor`，*可选*）-
    每个输入实体在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “inputs_embeds”（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “head_mask”（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：
- en: 1 indicates the head is `not masked`,
  id: totrans-589
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-590
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “output_attentions”（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “output_hidden_states”（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “return_dict”（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices-1]` where `num_choices` is the size of the second dimension of
    the input tensors. (See `input_ids` above)'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “labels”（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）- 用于计算多项选择分类损失的标签。索引应在`[0,
    ..., num_choices-1]`中，其中`num_choices`是输入张量第二维的大小。（请参见上面的`input_ids`）
- en: Returns
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.LukeTokenClassifierOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.LukeTokenClassifierOutput`或`元组(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.LukeTokenClassifierOutput` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.LukeTokenClassifierOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “loss”（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）- 分类损失。
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “logits”（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）-
    分类得分（SoftMax之前）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, entity_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。模型在每层输出的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-604
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均的注意力权重在注意力softmax之后。
- en: The [LukeForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE30]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: LukeForQuestionAnswering
  id: totrans-609
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LukeForQuestionAnswering
- en: '### `class transformers.LukeForQuestionAnswering`'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LukeForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1988)'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L1988)'
- en: '[PRE31]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The LUKE Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: LUKE模型在顶部具有一个跨度分类头，用于提取式问答任务，如SQuAD（在隐藏状态输出的线性层上计算`span start logits`和`span
    end logits`）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L2007)'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/luke/modeling_luke.py#L2007)'
- en: '[PRE32]'
  id: totrans-620
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-626
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于未被“掩盖”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-627
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于被“掩盖”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-631
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`entity_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`) —
    Indices of entity tokens in the entity vocabulary.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`）— 实体词汇中实体标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`entity_attention_mask` (`torch.FloatTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Mask to avoid performing attention on padding entity token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask`（形状为`(batch_size, entity_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充实体标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for entity tokens that are `not masked`,
  id: totrans-638
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于未被“掩盖”的实体标记，
- en: 0 for entity tokens that are `masked`.
  id: totrans-639
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于被“掩盖”的实体标记。
- en: '`entity_token_type_ids` (`torch.LongTensor` of shape `(batch_size, entity_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    entity token inputs. Indices are selected in `[0, 1]`:'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids`（形状为`(batch_size, entity_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示实体标记输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *portion A* entity token,
  id: totrans-641
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*部分A*实体标记，
- en: 1 corresponds to a *portion B* entity token.
  id: totrans-642
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*部分B*实体标记。
- en: '`entity_position_ids` (`torch.LongTensor` of shape `(batch_size, entity_length,
    max_mention_length)`, *optional*) — Indices of positions of each input entity
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids`（形状为`(batch_size, entity_length, max_mention_length)`的`torch.LongTensor`，*可选*）—
    每个输入实体在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-646
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-647
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被“掩盖”。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算标记范围开始位置的位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会用于计算损失。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`torch.LongTensor`形状为`(batch_size,)`, *optional*) — 用于计算标记跨度结束位置的位置（索引）的标签。位置被夹紧到序列的长度(`sequence_length`)。序列外的位置不会被考虑在内计算损失。'
- en: Returns
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.luke.modeling_luke.LukeQuestionAnsweringModelOutput` or
    `tuple(torch.FloatTensor)`'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.luke.modeling_luke.LukeQuestionAnsweringModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.luke.modeling_luke.LukeQuestionAnsweringModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig))
    and inputs.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.luke.modeling_luke.LukeQuestionAnsweringModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[LukeConfig](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`形状为`(1,)`, *optional*, 当提供`labels`时返回) — 总跨度提取损失是起始位置和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor`形状为`(batch_size, sequence_length)`) — 跨度起始得分（SoftMax之前）。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor`形状为`(batch_size, sequence_length)`) — 跨度结束得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, entity_length, hidden_size)`.
    Entity hidden-states of the model at the output of each layer plus the initial
    entity embedding outputs.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（嵌入输出+每一层的输出）的形状为`(batch_size, entity_length, hidden_size)`。模型在每一层输出的实体隐藏状态加上初始实体嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [LukeForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '[LukeForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE33]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
