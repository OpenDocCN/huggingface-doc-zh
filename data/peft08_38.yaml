- en: LoHa
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoHa
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/loha](https://huggingface.co/docs/peft/package_reference/loha)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/package_reference/loha](https://huggingface.co/docs/peft/package_reference/loha)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Low-Rank Hadamard Product ([LoHa](https://huggingface.co/papers/2108.06098)),
    is similar to LoRA except it approximates the large weight matrix with more low-rank
    matrices and combines them with the Hadamard product. This method is even more
    parameter-efficient than LoRA and achieves comparable performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩哈达玛积（[LoHa](https://huggingface.co/papers/2108.06098)），类似于 LoRA，但它使用更多低秩矩阵来近似大权重矩阵，并将它们与哈达玛积结合。这种方法比
    LoRA 更加参数高效，并实现了可比较的性能。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要：
- en: '*In this work, we propose a communication-efficient parameterization, FedPara,
    for federated learning (FL) to overcome the burdens on frequent model uploads
    and downloads. Our method re-parameterizes weight parameters of layers using low-rank
    weights followed by the Hadamard product. Compared to the conventional low-rank
    parameterization, our FedPara method is not restricted to low-rank constraints,
    and thereby it has a far larger capacity. This property enables to achieve comparable
    performance while requiring 3 to 10 times lower communication costs than the model
    with the original layers, which is not achievable by the traditional low-rank
    methods. The efficiency of our method can be further improved by combining with
    other efficient FL optimizers. In addition, we extend our method to a personalized
    FL application, pFedPara, which separates parameters into global and local ones.
    We show that pFedPara outperforms competing personalized FL methods with more
    than three times fewer parameters*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这项工作中，我们提出了一种通信高效的参数化方法 FedPara，用于克服频繁模型上传和下载的负担。我们的方法重新参数化层的权重参数，使用低秩权重后跟哈达玛积。与传统的低秩参数化相比，我们的
    FedPara 方法不受低秩约束，因此具有更大的容量。这种特性使得在需要 3 到 10 倍较低的通信成本的情况下实现可比较的性能成为可能，而这是传统低秩方法无法实现的。我们的方法的效率可以通过与其他高效的
    FL 优化器结合来进一步提高。此外，我们将我们的方法扩展到个性化 FL 应用 pFedPara，将参数分为全局和本地参数。我们展示 pFedPara 在比其他个性化
    FL 方法少三倍以上参数的情况下表现更好*。'
- en: LoHaConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoHaConfig
- en: '### `class peft.LoHaConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.LoHaConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/config.py#L22)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/config.py#L22)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`r` (`int`) — LoHa rank.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r` (`int`) — LoHa 秩。'
- en: '`alpha` (`int`) — The alpha parameter for LoHa scaling.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha` (`int`) — LoHa 缩放的 alpha 参数。'
- en: '`rank_dropout` (`float`) — The dropout probability for rank dimension during
    training.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank_dropout` (`float`) — 在训练期间对秩维度的丢弃概率。'
- en: '`module_dropout` (`float`) — The dropout probability for disabling LoHa modules
    during training.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module_dropout` (`float`) — 在训练期间禁用 LoHa 模块的丢弃概率。'
- en: '`use_effective_conv2d` (`bool`) — Use parameter effective decomposition for
    Conv2d with ksize > 1 (“Proposition 3” from FedPara paper).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_effective_conv2d` (`bool`) — 使用参数有效分解对于 ksize > 1 的 Conv2d（来自 FedPara
    论文的“Proposition 3”）。'
- en: '`target_modules` (`Optional[Union[List[str], str]]`) — The names of the modules
    to apply the adapter to. If this is specified, only the modules with the specified
    names will be replaced. When passing a string, a regex match will be performed.
    When passing a list of strings, either an exact match will be performed or it
    is checked if the name of the module ends with any of the passed strings. If this
    is specified as ‘all-linear’, then all linear/Conv1D modules are chosen, excluding
    the output layer. If this is not specified, modules will be chosen according to
    the model architecture. If the architecture is not known, an error will be raised
    — in this case, you should specify the target modules manually.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了这个，只有指定名称的模块将被替换。当传递一个字符串时，将执行正则表达式匹配。当传递一个字符串列表时，将执行精确匹配或检查模块名称是否以任何传递的字符串结尾。如果指定为
    ''all-linear''，则选择所有线性/Conv1D 模块，不包括输出层。如果未指定，将根据模型架构选择模块。如果不知道架构，将引发错误 — 在这种情况下，您应该手动指定目标模块。'
- en: '`init_weights` (`bool`) — Whether to perform initialization of adapter weights.
    This defaults to `True`, passing `False` is discouraged.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_weights` (`bool`) — 是否对适配器权重进行初始化。默认为 `True`，不建议传递 `False`。'
- en: '`layers_to_transform` (`Union[List[int], int]`) — The layer indices to transform.
    If a list of ints is passed, it will apply the adapter to the layer indices that
    are specified in this list. If a single integer is passed, it will apply the transformations
    on the layer at this index.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_to_transform` (`Union[List[int], int]`) — 要转换的层索引。如果传递一个整数列表，它将适用于在此列表中指定的层索引的适配器。如果传递一个整数，它将在该索引处的层上应用转换。'
- en: '`layers_pattern` (`str`) — The layer pattern name, used only if `layers_to_transform`
    is different from `None`.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_pattern` (`str`) — 层模式名称，仅在 `layers_to_transform` 不同于 `None` 时使用。'
- en: '`rank_pattern` (`dict`) — The mapping from layer names or regexp expression
    to ranks which are different from the default rank specified by `r`.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank_pattern` (`dict`) — 从层名称或正则表达式到与 `r` 指定的默认秩不同的秩的映射。'
- en: '`alpha_pattern` (`dict`) — The mapping from layer names or regexp expression
    to alphas which are different from the default alpha specified by `alpha`.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha_pattern` (`dict`) — 从层名称或正则表达式到与 `alpha` 指定的默认 alpha 不同的 alphas 的映射。'
- en: '`modules_to_save` (`Optional[List[str]]`) — List of modules apart from adapter
    layers to be set as trainable and saved in the final checkpoint.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules_to_save` (`Optional[List[str]]`) — 除了适配器层之外要设置为可训练并保存在最终检查点中的模块列表。'
- en: This is the configuration class to store the configuration of a [LoHaModel](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaModel).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '这是用于存储 [LoHaModel](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaModel)
    配置的类。 '
- en: LoHaModel
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoHaModel
- en: '### `class peft.LoHaModel`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.LoHaModel`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/model.py#L27)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/model.py#L27)'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — The model to which the adapter tuner layers will
    be attached.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.nn.Module`）— 适配器调谐层将附加到的模型。'
- en: '`config` ([LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig))
    — The configuration of the LoHa model.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig)）—
    LoHa模型的配置。'
- en: '`adapter_name` (`str`) — The name of the adapter, defaults to `"default"`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name`（`str`）— 适配器的名称，默认为`"default"`。'
- en: Returns
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.nn.Module`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The LoHa model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LoHa模型。
- en: Creates Low-Rank Hadamard Product model from a pretrained model. The method
    is partially described in [https://arxiv.org/abs/2108.06098](https://arxiv.org/abs/2108.06098)
    Current implementation heavily borrows from [https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py](https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型创建低秩哈达玛积模型。该方法部分描述在[https://arxiv.org/abs/2108.06098](https://arxiv.org/abs/2108.06098)。当前实现大量借鉴自[https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py](https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py)
- en: 'Example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Attributes**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性**：'
- en: '`model` (`~torch.nn.Module`) — The model to be adapted.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`~torch.nn.Module`）— 要适应的模型。'
- en: '`peft_config` ([LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig)):
    The configuration of the LoHa model.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config`（[LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig)）：LoHa模型的配置。'
