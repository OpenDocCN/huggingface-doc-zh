- en: Jukebox
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jukebox
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf)
    by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
    Ilya Sutskever. It introduces a generative music model which can produce minute
    long samples that can be conditioned on an artist, genres and lyrics.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jukebox 模型在 [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf)
    中由 Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
    Ilya Sutskever 提出。它引入了一个生成音乐模型，可以生成可以根据艺术家、流派和歌词进行条件化的一分钟长样本。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We introduce Jukebox, a model that generates music with singing in the raw
    audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE
    to compress it to discrete codes, and modeling those using autoregressive Transformers.
    We show that the combined model at scale can generate high-fidelity and diverse
    songs with coherence up to multiple minutes. We can condition on artist and genre
    to steer the musical and vocal style, and on unaligned lyrics to make the singing
    more controllable. We are releasing thousands of non cherry-picked samples, along
    with model weights and code.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们推出了 Jukebox，这是一个在原始音频领域生成带有歌唱的音乐的模型。我们使用多尺度 VQ-VAE 来压缩原始音频的长上下文为离散代码，并使用自回归
    Transformers 对其进行建模。我们展示了规模上的组合模型可以生成高保真度和多样化的歌曲，连贯性可达多分钟。我们可以根据艺术家和流派来引导音乐和声音风格，并根据不对齐的歌词来使歌唱更可控。我们发布了数千个非精选样本，以及模型权重和代码。*'
- en: As shown on the following figure, Jukebox is made of 3 `priors` which are decoder
    only models. They follow the architecture described in [Generating Long Sequences
    with Sparse Transformers](https://arxiv.org/abs/1904.10509), modified to support
    longer context length. First, a autoencoder is used to encode the text lyrics.
    Next, the first (also called `top_prior`) prior attends to the last hidden states
    extracted from the lyrics encoder. The priors are linked to the previous priors
    respectively via an `AudioConditionner` module. The`AudioConditioner` upsamples
    the outputs of the previous prior to raw tokens at a certain audio frame per second
    resolution. The metadata such as *artist, genre and timing* are passed to each
    prior, in the form of a start token and positional embedding for the timing data.
    The hidden states are mapped to the closest codebook vector from the VQVAE in
    order to convert them to raw audio.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，Jukebox 由 3 个仅解码器模型的 `priors` 组成。它们遵循 [使用稀疏 Transformers 生成长序列](https://arxiv.org/abs/1904.10509)
    中描述的架构，经过修改以支持更长的上下文长度。首先，使用自编码器对文本歌词进行编码。接下来，第一个（也称为 `top_prior`）prior 关注从歌词编码器提取的最后隐藏状态。先前的
    priors 通过 `AudioConditionner` 模块分别连接到前一个 priors。`AudioConditioner` 将先前 prior 的输出上采样到特定音频帧每秒的原始标记。元数据，如
    *艺术家、流派和时间*，以起始标记和时间数据的位置嵌入的形式传递给每个 prior。隐藏状态被映射到 VQVAE 中最接近的码书向量，以将它们转换为原始音频。
- en: '![JukeboxModel](../Images/5772cc27b249201f12c4d49bc48dfd5e.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![JukeboxModel](../Images/5772cc27b249201f12c4d49bc48dfd5e.png)'
- en: This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).
    The original code can be found [here](https://github.com/openai/jukebox).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由 [Arthur Zucker](https://huggingface.co/ArthurZ) 贡献。原始代码可以在 [这里](https://github.com/openai/jukebox)
    找到。
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: This model only supports inference. This is for a few reasons, mostly because
    it requires a crazy amount of memory to train. Feel free to open a PR and add
    what’s missing to have a full integration with the hugging face traineer!
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型仅支持推理。这主要是因为训练需要大量内存。欢迎提交 PR 并添加缺失的内容，以实现与 Hugging Face Trainer 的完全集成！
- en: This model is very slow, and takes 8h to generate a minute long audio using
    the 5b top prior on a V100 GPU. In order automaticallay handle the device on which
    the model should execute, use `accelerate`.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型非常慢，使用 V100 GPU 上的 5b 顶部 prior 生成一分钟长的音频需要 8 小时。为了自动处理模型应在其上执行的设备，请使用 `accelerate`。
- en: 'Contrary to the paper, the order of the priors goes from `0` to `1` as it felt
    more intuitive : we sample starting from `0`.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与论文相反，prior 的顺序从 `0` 到 `1`，因为这样更直观：我们从 `0` 开始采样。
- en: Primed sampling (conditioning the sampling on raw audio) requires more memory
    than ancestral sampling and should be used with `fp16` set to `True`.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于预先采样（在原始音频上进行采样）需要比祖先采样更多的内存，并且应该将 `fp16` 设置为 `True`。
- en: This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).
    The original code can be found [here](https://github.com/openai/jukebox).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由 [Arthur Zucker](https://huggingface.co/ArthurZ) 贡献。原始代码可以在 [这里](https://github.com/openai/jukebox)
    找到。
- en: JukeboxConfig
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JukeboxConfig
- en: '### `class transformers.JukeboxConfig`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.JukeboxConfig` 类'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L495)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L495)'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vqvae_config` (`JukeboxVQVAEConfig`, *optional*) — Configuration for the `JukeboxVQVAE`
    model.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqvae_config` (`JukeboxVQVAEConfig`, *可选*) — `JukeboxVQVAE` 模型的配置。'
- en: '`prior_config_list` (`List[JukeboxPriorConfig]`, *optional*) — List of the
    configs for each of the `JukeboxPrior` of the model. The original architecture
    uses 3 priors.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_config_list` (`List[JukeboxPriorConfig]`, *可选*) — 模型中每个 `JukeboxPrior`
    的配置列表。原始架构使用了 3 个 priors。'
- en: '`nb_priors` (`int`, *optional*, defaults to 3) — Number of prior models that
    will sequentially sample tokens. Each prior is conditional auto regressive (decoder)
    model, apart from the top prior, which can include a lyric encoder. The available
    models were trained using a top prior and 2 upsampler priors.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nb_priors` (`int`, *可选*, 默认为 3) — 将依次采样标记的先前模型数量。每个 prior 都是条件自回归（解码器）模型，除了顶部
    prior 可以包括歌词编码器。可用的模型是使用顶部 prior 和 2 个上采样 prior 进行训练的。'
- en: '`sampling_rate` (`int`, *optional*, defaults to 44100) — Sampling rate of the
    raw audio.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *optional*, defaults to 44100) — 原始音频的采样率。'
- en: '`timing_dims` (`int`, *optional*, defaults to 64) — Dimensions of the JukeboxRangeEmbedding
    layer which is equivalent to traditional positional embedding layer. The timing
    embedding layer converts the absolute and relative position in the currently sampled
    audio to a tensor of length `timing_dims` that will be added to the music tokens.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timing_dims` (`int`, *optional*, defaults to 64) — JukeboxRangeEmbedding层的维度，相当于传统的位置嵌入层。定时嵌入层将当前采样音频中的绝对位置和相对位置转换为长度为`timing_dims`的张量，该张量将添加到音乐标记中。'
- en: '`min_duration` (`int`, *optional*, defaults to 0) — Minimum duration of the
    audios to generate'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_duration` (`int`, *optional*, defaults to 0) — 生成音频的最小持续时间'
- en: '`max_duration` (`float`, *optional*, defaults to 600.0) — Maximum duration
    of the audios to generate'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_duration` (`float`, *optional*, defaults to 600.0) — 生成音频的最大持续时间'
- en: '`max_nb_genres` (`int`, *optional*, defaults to 5) — Maximum number of genres
    that can be used to condition a single sample.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_nb_genres` (`int`, *optional*, defaults to 5) — 可用于调节单个样本的最大流派数量。'
- en: '`metadata_conditioning` (`bool`, *optional*, defaults to `True`) — Whether
    or not to use metadata conditioning, corresponding to the artist, the genre and
    the min/maximum duration.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata_conditioning` (`bool`, *optional*, defaults to `True`) — 是否使用元数据调节，对应于艺术家、流派和最小/最大持续时间。'
- en: This is the configuration class to store the configuration of a [JukeboxModel](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxModel).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[JukeboxModel](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxModel)的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information. Instantiating a configuration with the defaults will yield
    a similar configuration to that of [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)
    architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。使用默认值实例化配置将产生类似于[openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)架构的配置。
- en: The downsampling and stride are used to determine downsampling of the input
    sequence. For example, downsampling = (5,3), and strides = (2, 2) will downsample
    the audio by 2^5 = 32 to get the first level of codes, and 2**8 = 256 to get the
    second level codes. This is mostly true for training the top level prior and the
    upsamplers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样和步幅用于确定输入序列的下采样。例如，下采样 = (5,3)，步幅 = (2, 2) 将使音频下采样为2^5 = 32，以获得第一级代码，以及2**8
    = 256，以获得第二级代码。这在训练顶层先验和上采样器时通常是正确的。
- en: 'Example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `from_configs`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L598)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L598)'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Returns
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[JukeboxConfig](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxConfig)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[JukeboxConfig](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxConfig)'
- en: An instance of a configuration object
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的一个实例
- en: Instantiate a [JukeboxConfig](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxConfig)
    (or a derived class) from clip text model configuration and clip vision model
    configuration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从剪辑文本模型配置和剪辑视觉模型配置实例化一个[JukeboxConfig](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxConfig)（或派生类）的实例。
- en: JukeboxPriorConfig
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JukeboxPriorConfig
- en: '### `class transformers.JukeboxPriorConfig`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.JukeboxPriorConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L143)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L143)'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`act_fn` (`str`, *optional*, defaults to `"quick_gelu"`) — Activation function.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn` (`str`, *optional*, defaults to `"quick_gelu"`) — 激活函数。'
- en: '`alignment_head` (`int`, *optional*, defaults to 2) — Head that is responsible
    of the alignment between lyrics and music. Only used to compute the lyric to audio
    alignment'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alignment_head` (`int`, *optional*, defaults to 2) — 负责歌词和音乐之间对齐的头部。仅用于计算歌词到音频的对齐'
- en: '`alignment_layer` (`int`, *optional*, defaults to 68) — Index of the layer
    that is responsible of the alignment between lyrics and music. Only used to compute
    the lyric to audio alignment'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alignment_layer` (`int`, *optional*, defaults to 68) — 负责歌词和音乐之间对齐的层的索引。仅用于计算歌词到音频的对齐'
- en: '`attention_multiplier` (`float`, *optional*, defaults to 0.25) — Multiplier
    coefficient used to define the hidden dimension of the attention layers. 0.25
    means that 0.25*width of the model will be used.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_multiplier` (`float`, *optional*, defaults to 0.25) — 用于定义注意力层隐藏维度的乘数系数。0.25表示将使用模型宽度的0.25。'
- en: '`attention_pattern` (`str`, *optional*, defaults to `"enc_dec_with_lyrics"`)
    — Which attention pattern to use for the decoder/'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_pattern` (`str`, *optional*, defaults to `"enc_dec_with_lyrics"`)
    — 解码器使用的注意力模式'
- en: '`attn_dropout` (`int`, *optional*, defaults to 0) — Dropout probability for
    the post-attention layer dropout in the decoder.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_dropout` (`int`, *optional*, defaults to 0) — 解码器中注意力层后的丢弃概率。'
- en: '`attn_res_scale` (`bool`, *optional*, defaults to `False`) — Whether or not
    to scale the residuals in the attention conditioner block.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_res_scale` (`bool`, *optional*, defaults to `False`) — 是否在注意力调节器块中缩放残差。'
- en: '`blocks` (`int`, *optional*, defaults to 64) — Number of blocks used in the
    `block_attn`. A sequence of length seq_len is factored as `[blocks, seq_len //
    blocks]` in the `JukeboxAttention` layer.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blocks` (`int`, *optional*, defaults to 64) — `block_attn` 中使用的块数。长度为seq_len的序列在`JukeboxAttention`层中被分解为`[blocks,
    seq_len // blocks]`。'
- en: '`conv_res_scale` (`int`, *optional*) — Whether or not to scale the residuals
    in the conditioner block. Since the top level prior does not have a conditioner,
    the default value is to None and should not be modified.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_res_scale` (`int`, *optional*) — 是否要在条件块中缩放残差。由于顶层先验没有条件块，因此默认值为`None`，不应修改。'
- en: '`num_layers` (`int`, *optional*, defaults to 72) — Number of layers of the
    transformer architecture.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, 默认为72) — 变压器架构的层数。'
- en: '`emb_dropout` (`int`, *optional*, defaults to 0) — Embedding dropout used in
    the lyric decoder.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emb_dropout` (`int`, *optional*, 默认为0) — 歌词解码器中使用的嵌入丢失。'
- en: '`encoder_config` (`JukeboxPriorConfig`, *optional*) — Configuration of the
    encoder which models the prior on the lyrics.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_config` (`JukeboxPriorConfig`, *optional*) — 对歌词先验进行建模的编码器配置。'
- en: '`encoder_loss_fraction` (`float`, *optional*, defaults to 0.4) — Multiplication
    factor used in front of the lyric encoder loss.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_loss_fraction` (`float`, *optional*, 默认为0.4) — 用于歌词编码器损失前面的乘法因子。'
- en: '`hidden_size` (`int`, *optional*, defaults to 2048) — Hidden dimension of the
    attention layers.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为2048) — 注意力层的隐藏维度。'
- en: '`init_scale` (`float`, *optional*, defaults to 0.2) — Initialization scales
    for the prior modules.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_scale` (`float`, *optional*, 默认为0.2) — 先验模块的初始化比例。'
- en: '`is_encoder_decoder` (`bool`, *optional*, defaults to `True`) — Whether or
    not the prior is an encoder-decoder model. In case it is not, and `nb_relevant_lyric_tokens`
    is greater than 0, the `encoder` args should be specified for the lyric encoding.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_encoder_decoder` (`bool`, *optional*, 默认为`True`) — 先验是否为编码器-解码器模型。如果不是，并且`nb_relevant_lyric_tokens`大于0，则应为歌词编码指定`encoder`参数。'
- en: '`mask` (`bool`, *optional*, defaults to `False`) — Whether or not to mask the
    previous positions in the attention.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask` (`bool`, *optional*, 默认为`False`) — 是否要屏蔽注意力中的先前位置。'
- en: '`max_duration` (`int`, *optional*, defaults to 600) — Maximum supported duration
    of the generated song in seconds.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_duration` (`int`, *optional*, 默认为600) — 生成歌曲的最大支持持续时间（以秒为单位）。'
- en: '`max_nb_genres` (`int`, *optional*, defaults to 1) — Maximum number of genres
    that can be used to condition the model.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_nb_genres` (`int`, *optional*, 默认为1) — 可用于条件模型的最大流派数量。'
- en: '`merged_decoder` (`bool`, *optional*, defaults to `True`) — Whether or not
    the decoder and the encoder inputs are merged. This is used for the separated
    encoder-decoder architecture'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merged_decoder` (`bool`, *optional*, 默认为`True`) — 解码器和编码器输入是否合并。这用于分离的编码器-解码器架构'
- en: '`metadata_conditioning` (`bool`, *optional*, defaults to `True)` — Whether
    or not to condition on the artist and genre metadata.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata_conditioning` (`bool`, *optional*, 默认为`True`) — 是否要在艺术家和流派元数据上进行条件。'
- en: '`metadata_dims` (`List[int]`, *optional*, defaults to `[604, 7898]`) — Number
    of genres and the number of artists that were used to train the embedding layers
    of the prior models.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata_dims` (`List[int]`, *optional*, 默认为`[604, 7898]`) — 用于训练先验模型的嵌入层的流派数量和艺术家数量。'
- en: '`min_duration` (`int`, *optional*, defaults to 0) — Minimum duration of the
    generated audio on which the model was trained.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_duration` (`int`, *optional*, 默认为0) — 模型训练的生成音频的最小持续时间。'
- en: '`mlp_multiplier` (`float`, *optional*, defaults to 1.0) — Multiplier coefficient
    used to define the hidden dimension of the MLP layers. 0.25 means that 0.25*width
    of the model will be used.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_multiplier` (`float`, *optional*, 默认为1.0) — 用于定义MLP层隐藏维度的乘数系数。0.25表示将使用模型宽度的0.25。'
- en: '`music_vocab_size` (`int`, *optional*, defaults to 2048) — Number of different
    music tokens. Should be similar to the `JukeboxVQVAEConfig.nb_discrete_codes`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`music_vocab_size` (`int`, *optional*, 默认为2048) — 不同音乐标记的数量。应与`JukeboxVQVAEConfig.nb_discrete_codes`类似。'
- en: '`n_ctx` (`int`, *optional*, defaults to 6144) — Number of context tokens for
    each prior. The context tokens are the music tokens that are attended to when
    generating music tokens.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_ctx` (`int`, *optional*, 默认为6144) — 每个先验的上下文标记数量。上下文标记是在生成音乐标记时所关注的音乐标记。'
- en: '`n_heads` (`int`, *optional*, defaults to 2) — Number of attention heads.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_heads` (`int`, *optional*, 默认为2) — 注意力头的数量。'
- en: '`nb_relevant_lyric_tokens` (`int`, *optional*, defaults to 384) — Number of
    lyric tokens that are used when sampling a single window of length `n_ctx`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nb_relevant_lyric_tokens` (`int`, *optional*, 默认为384) — 在采样长度为`n_ctx`的单个窗口时使用的歌词标记数量'
- en: '`res_conv_depth` (`int`, *optional*, defaults to 3) — Depth of the `JukeboxDecoderConvBock`
    used to upsample the previously sampled audio in the `JukeboxMusicTokenConditioner`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_conv_depth` (`int`, *optional*, 默认为3) — 用于在`JukeboxMusicTokenConditioner`中上采样先前采样音频的`JukeboxDecoderConvBock`的深度。'
- en: '`res_conv_width` (`int`, *optional*, defaults to 128) — Width of the `JukeboxDecoderConvBock`
    used to upsample the previously sampled audio in the `JukeboxMusicTokenConditioner`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_conv_width` (`int`, *optional*, 默认为128) — 用于在`JukeboxMusicTokenConditioner`中上采样先前采样音频的`JukeboxDecoderConvBock`的宽度。'
- en: '`res_convolution_multiplier` (`int`, *optional*, defaults to 1) — Multiplier
    used to scale the `hidden_dim` of the `JukeboxResConv1DBlock`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_convolution_multiplier` (`int`, *optional*, 默认为1) — 用于缩放`JukeboxResConv1DBlock`的`hidden_dim`的乘数。'
- en: '`res_dilation_cycle` (`int`, *optional*) — Dilation cycle used to define the
    `JukeboxMusicTokenConditioner`. Usually similar to the ones used in the corresponding
    level of the VQVAE. The first prior does not use it as it is not conditioned on
    upper level tokens.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_dilation_cycle` (`int`, *optional*) — 用于定义`JukeboxMusicTokenConditioner`的扩张周期。通常类似于VQVAE相应级别中使用的周期。第一个先验不使用它，因为它不是基于上一级标记的。'
- en: '`res_dilation_growth_rate` (`int`, *optional*, defaults to 1) — Dilation grow
    rate used between each convolutionnal block of the `JukeboxMusicTokenConditioner`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_dilation_growth_rate` (`int`, *optional*, 默认为1) — 用于`JukeboxMusicTokenConditioner`的每个卷积块之间的扩张增长率'
- en: '`res_downs_t` (`List[int]`, *optional*, defaults to `[3, 2, 2]`) — Downsampling
    rates used in the audio conditioning network'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_downs_t` (`List[int]`, *optional*, 默认为`[3, 2, 2]`) — 音频调节网络中使用的下采样率'
- en: '`res_strides_t` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — Striding
    used in the audio conditioning network'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_strides_t` (`List[int]`, *optional*, 默认为`[2, 2, 2]`) — 音频调节网络中使用的步幅'
- en: '`resid_dropout` (`int`, *optional*, defaults to 0) — Residual dropout used
    in the attention pattern.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resid_dropout` (`int`, *optional*, defaults to 0) — 注意力模式中使用的残差丢失。'
- en: '`sampling_rate` (`int`, *optional*, defaults to 44100) — Sampling rate used
    for training.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *optional*, defaults to 44100) — 用于训练的采样率。'
- en: '`spread` (`int`, *optional*) — Spread used in the `summary_spread_attention`
    pattern'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spread` (`int`, *optional*) — `summary_spread_attention`模式中使用的扩展'
- en: '`timing_dims` (`int`, *optional*, defaults to 64) — Dimension of the timing
    embedding.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timing_dims` (`int`, *optional*, defaults to 64) — 时间嵌入的维度。'
- en: '`zero_out` (`bool`, *optional*, defaults to `False`) — Whether or not to zero
    out convolution weights when initializing.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zero_out` (`bool`, *optional*, defaults to `False`) — 初始化时是否将卷积权重归零。'
- en: This is the configuration class to store the configuration of a [JukeboxPrior](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxPrior).
    It is used to instantiate a `JukeboxPrior` according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the top level prior from the [openai/jukebox-1b-lyrics]([https://huggingface.co/openai/jukebox](https://huggingface.co/openai/jukebox)
    -1b-lyrics) architecture.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置类，用于存储[JukeboxPrior](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxPrior)的配置。根据指定的参数实例化`JukeboxPrior`，定义模型架构。使用默认值实例化配置将产生类似于[openai/jukebox-1b-lyrics]([https://huggingface.co/openai/jukebox](https://huggingface.co/openai/jukebox)
    -1b-lyrics)架构顶级先验的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: JukeboxVQVAEConfig
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JukeboxVQVAEConfig
- en: '### `class transformers.JukeboxVQVAEConfig`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.JukeboxVQVAEConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L372)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L372)'
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`act_fn` (`str`, *optional*, defaults to `"relu"`) — Activation function of
    the model.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn` (`str`, *optional*, defaults to `"relu"`) — 模型的激活函数。'
- en: '`nb_discrete_codes` (`int`, *optional*, defaults to 2048) — Number of codes
    of the VQVAE.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nb_discrete_codes` (`int`, *optional*, defaults to 2048) — VQVAE的代码数量。'
- en: '`commit` (`float`, *optional*, defaults to 0.02) — Commit loss multiplier.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit` (`float`, *optional*, defaults to 0.02) — 提交损失乘数。'
- en: '`conv_input_shape` (`int`, *optional*, defaults to 1) — Number of audio channels.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_input_shape` (`int`, *optional*, defaults to 1) — 音频通道数。'
- en: '`conv_res_scale` (`bool`, *optional*, defaults to `False`) — Whether or not
    to scale the residuals of the `JukeboxResConv1DBlock`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_res_scale` (`bool`, *optional*, defaults to `False`) — 是否对`JukeboxResConv1DBlock`的残差进行缩放。'
- en: '`embed_dim` (`int`, *optional*, defaults to 64) — Embedding dimension of the
    codebook vectors.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_dim` (`int`, *optional*, defaults to 64) — 码书向量的嵌入维度。'
- en: '`hop_fraction` (`List[int]`, *optional*, defaults to `[0.125, 0.5, 0.5]`) —
    Fraction of non-intersecting window used when continuing the sampling process.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hop_fraction` (`List[int]`, *optional*, defaults to `[0.125, 0.5, 0.5]`) —
    在继续采样过程时使用的非交叉窗口的比例。'
- en: '`levels` (`int`, *optional*, defaults to 3) — Number of hierarchical levels
    that used in the VQVAE.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`levels` (`int`, *optional*, defaults to 3) — VQVAE中使用的分层级别数。'
- en: '`lmu` (`float`, *optional*, defaults to 0.99) — Used in the codebook update,
    exponential moving average coefficient. For more detail refer to Appendix A.1
    of the original [VQVAE paper](https://arxiv.org/pdf/1711.00937v2.pdf)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lmu` (`float`, *optional*, defaults to 0.99) — 用于码书更新的指数移动平均系数。有关详细信息，请参阅原始[VQVAE论文](https://arxiv.org/pdf/1711.00937v2.pdf)的附录A.1'
- en: '`multipliers` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — Depth and
    width multipliers used for each level. Used on the `res_conv_width` and `res_conv_depth`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multipliers` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — 每个级别使用的深度和宽度乘数。用于`res_conv_width`和`res_conv_depth`'
- en: '`res_conv_depth` (`int`, *optional*, defaults to 4) — Depth of the encoder
    and decoder block. If no `multipliers` are used, this is the same for each level.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_conv_depth` (`int`, *optional*, defaults to 4) — 编码器和解码器块的深度。如果没有使用`multipliers`，则每个级别的深度相同。'
- en: '`res_conv_width` (`int`, *optional*, defaults to 32) — Width of the encoder
    and decoder block. If no `multipliers` are used, this is the same for each level.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_conv_width` (`int`, *optional*, defaults to 32) — 编码器和解码器块的宽度。如果没有使用`multipliers`，则每个级别的宽度相同。'
- en: '`res_convolution_multiplier` (`int`, *optional*, defaults to 1) — Scaling factor
    of the hidden dimension used in the `JukeboxResConv1DBlock`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_convolution_multiplier` (`int`, *optional*, defaults to 1) — `JukeboxResConv1DBlock`中使用的隐藏维度的缩放因子。'
- en: '`res_dilation_cycle` (`int`, *optional*) — Dilation cycle value used in the
    `JukeboxResnet`. If an int is used, each new Conv1 block will have a depth reduced
    by a power of `res_dilation_cycle`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_dilation_cycle` (`int`, *optional*) — `JukeboxResnet`中使用的膨胀周期值。如果使用int，每个新的Conv1块的深度将减少`res_dilation_cycle`的幂。'
- en: '`res_dilation_growth_rate` (`int`, *optional*, defaults to 3) — Resnet dilation
    growth rate used in the VQVAE (dilation_growth_rate ** depth)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_dilation_growth_rate` (`int`, *optional*, defaults to 3) — VQVAE中使用的Resnet膨胀增长率（膨胀增长率
    ** 深度）'
- en: '`res_downs_t` (`List[int]`, *optional*, defaults to `[3, 2, 2]`) — Downsampling
    rate for each level of the hierarchical VQ-VAE.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_downs_t` (`List[int]`, *optional*, defaults to `[3, 2, 2]`) — 分层VQ-VAE每个级别的下采样率。'
- en: '`res_strides_t` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — Stride
    used for each level of the hierarchical VQ-VAE.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`res_strides_t` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — 分层VQ-VAE每个级别使用的步幅。'
- en: '`sample_length` (`int`, *optional*, defaults to 1058304) — Provides the max
    input shape of the VQVAE. Is used to compute the input shape of each level.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_length` (`int`, *optional*, defaults to 1058304) — 提供VQVAE的最大输入形状。用于计算每个级别的输入形状。'
- en: '`init_scale` (`float`, *optional*, defaults to 0.2) — Initialization scale.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_scale` (`float`, *optional*, defaults to 0.2) — 初始化比例。'
- en: '`zero_out` (`bool`, *optional*, defaults to `False`) — Whether or not to zero
    out convolution weights when initializing.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zero_out` (`bool`, *optional*, defaults to `False`) — 是否在初始化时将卷积权重归零。'
- en: This is the configuration class to store the configuration of a [JukeboxVQVAE](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxVQVAE).
    It is used to instantiate a `JukeboxVQVAE` according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the VQVAE from [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)
    architecture.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[JukeboxVQVAE](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxVQVAE)配置的配置类。它用于根据指定的参数实例化`JukeboxVQVAE`，定义模型架构。使用默认值实例化配置将产生与[openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)架构的VQVAE类似的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: JukeboxTokenizer
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JukeboxTokenizer
- en: '### `class transformers.JukeboxTokenizer`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.JukeboxTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L59)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L59)'
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`artists_file` (`str`) — Path to the vocabulary file which contains a mapping
    between artists and ids. The default file supports both “v2” and “v3”'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`artists_file` (`str`) — 包含艺术家和id之间映射的词汇文件的路径。默认文件支持“v2”和“v3”。'
- en: '`genres_file` (`str`) — Path to the vocabulary file which contain a mapping
    between genres and ids.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`genres_file` (`str`) — 包含流派和id之间映射的词汇文件的路径。'
- en: '`lyrics_file` (`str`) — Path to the vocabulary file which contains the accepted
    characters for the lyrics tokenization.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lyrics_file` (`str`) — 包含歌词分词所需字符的词汇文件的路径。'
- en: '`version` (`List[str]`, `optional`, default to `["v3", "v2", "v2"]`) — List
    of the tokenizer versions. The `5b-lyrics`’s top level prior model was trained
    using `v3` instead of `v2`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`version` (`List[str]`, `optional`, default to `["v3", "v2", "v2"]`) — 分词器版本的列表。`5b-lyrics`的顶级先验模型是使用`v3`而不是`v2`进行训练的。'
- en: '`n_genres` (`int`, `optional`, defaults to 1) — Maximum number of genres to
    use for composition.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_genres` (`int`, `optional`, defaults to 1) — 用于作曲的最大流派数量。'
- en: '`max_n_lyric_tokens` (`int`, `optional`, defaults to 512) — Maximum number
    of lyric tokens to keep.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_n_lyric_tokens` (`int`, `optional`, defaults to 512) — 保留的最大歌词标记数量。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。'
- en: 'Constructs a Jukebox tokenizer. Jukebox can be conditioned on 3 different inputs
    :'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Jukebox分词器。Jukebox可以根据3种不同的输入进行条件化：
- en: Artists, unique ids are associated to each artist from the provided dictionary.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艺术家，唯一的id与提供的字典中的每位艺术家相关联。
- en: Genres, unique ids are associated to each genre from the provided dictionary.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流派，唯一的id与提供的字典中的每个流派相关联。
- en: Lyrics, character based tokenization. Must be initialized with the list of characters
    that are inside the vocabulary.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 歌词，基于字符的分词。必须使用词汇表中包含的字符列表进行初始化。
- en: 'This tokenizer does not require training. It should be able to process a different
    number of inputs: as the conditioning of the model can be done on the three different
    queries. If None is provided, defaults values will be used.:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器不需要训练。它应该能够处理不同数量的输入：因为模型的条件化可以在三个不同的查询上进行。如果未提供None，则将使用默认值。
- en: Depending on the number of genres on which the model should be conditioned (`n_genres`).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 取决于模型应该被条件化的流派数量（`n_genres`）。
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在实例化此分词器时或在对某些文本调用它时传递`add_prefix_space=True`来避免这种行为，但由于模型不是以这种方式进行预训练的，因此可能会导致性能下降。
- en: If nothing is provided, the genres and the artist will either be selected randomly
    or set to None
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未提供任何内容，则流派和艺术家将随机选择或设置为None
- en: 'This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to: this superclass
    for more information regarding those methods.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应该参考：这个超类以获取有关这些方法的更多信息。
- en: However the code does not allow that and only supports composing from various
    genres.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但是代码不允许这样做，只支持从各种流派进行作曲。
- en: '#### `save_vocabulary`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L372)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L372)'
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str`) — A path to the directory where to saved. It will
    be created if it doesn’t exist.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory` (`str`) — 保存的目录路径。如果不存在，将创建该目录。'
- en: '`filename_prefix` (`Optional[str]`, *optional*) — A prefix to add to the names
    of the files saved by the tokenizer.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filename_prefix` (`Optional[str]`, *optional*) — 添加到分词器保存的文件名称前缀。'
- en: Saves the tokenizer’s vocabulary dictionary to the provided save_directory.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 将分词器的词汇字典保存到提供的保存目录中。
- en: JukeboxModel
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JukeboxModel
- en: '### `class transformers.JukeboxModel`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.JukeboxModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2291)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2291)'
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`JukeboxConfig`) — Model configuration class with all the parameters
    of the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`JukeboxConfig`）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: 'The bare JUKEBOX Model used for music generation. 4 sampling techniques are
    supported : `primed_sample`, `upsample`, `continue_sample` and `ancestral_sample`.
    It does not have a `forward` method as the training is not end to end. If you
    want to fine-tune the model, it is recommended to use the `JukeboxPrior` class
    and train each prior individually.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 用于音乐生成的基本JUKEBOX模型。支持4种采样技术：`primed_sample`、`upsample`、`continue_sample`和`ancestral_sample`。它没有`forward`方法，因为训练不是端到端的。如果要微调模型，建议使用`JukeboxPrior`类并分别训练每个先验。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `ancestral_sample`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `ancestral_sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2573)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2573)'
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`labels` (`List[torch.LongTensor]`) — List of length `n_sample`, and shape
    `(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)` metadata
    such as `artist_id`, `genre_id` and the full list of lyric tokens which are used
    to condition the generation.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（`List[torch.LongTensor]`）- 长度为`n_sample`的列表，形状为`(self.levels, 4 +
    self.config.max_nb_genre + lyric_sequence_length)`，元数据，例如`artist_id`、`genre_id`和用于条件生成的完整歌词标记列表。'
- en: '`n_samples` (`int`, *optional*, default to 1) — Number of samples to be generated
    in parallel.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_samples`（`int`，*可选*，默认为1）- 要并行生成的样本数量。'
- en: Generates music tokens based on the provided `labels. Will start at the desired
    prior level and automatically upsample the sequence. If you want to create the
    audio, you should call` model.decode(tokens)`, which will use the VQ-VAE decoder
    to convert the music tokens to raw audio.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供的`labels`生成音乐标记。将从所需的先验级别开始，并自动上采样序列。如果要创建音频，应调用`model.decode(tokens)`，这将使用VQ-VAE解码器将音乐标记转换为原始音频。
- en: 'Example:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `primed_sample`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `primed_sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2649)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2649)'
- en: '[PRE11]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`raw_audio` (`List[torch.Tensor]` of length `n_samples` ) — A list of raw audio
    that will be used as conditioning information for each samples that will be generated.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_audio`（长度为`n_samples`的`List[torch.Tensor]`）- 用作每个将生成的样本的条件信息的原始音频列表。'
- en: '`labels` (`List[torch.LongTensor]` of length `n_sample`, and shape `(self.levels,
    self.config.max_nb_genre + lyric_sequence_length)` — List of metadata such as
    `artist_id`, `genre_id` and the full list of lyric tokens which are used to condition
    the generation.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（长度为`n_sample`的`List[torch.LongTensor]`，形状为`(self.levels, self.config.max_nb_genre
    + lyric_sequence_length)`）- 元数据列表，例如`artist_id`、`genre_id`和用于条件生成的完整歌词标记列表。'
- en: '`sampling_kwargs` (`Dict[Any]`) — Various additional sampling arguments that
    are used by the `_sample` function. A detail list of the arguments can bee seen
    in the `_sample` function documentation.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_kwargs`（`Dict[Any]`）- `_sample`函数使用的各种额外采样参数。可以在`_sample`函数文档中看到参数的详细列表。'
- en: 'Generate a raw audio conditioned on the provided `raw_audio` which is used
    as conditioning at each of the generation levels. The audio is encoded to music
    tokens using the 3 levels of the VQ-VAE. These tokens are used: as conditioning
    for each level, which means that no ancestral sampling is required.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供的`raw_audio`生成原始音频，该音频将用作每个生成级别的条件。音频被编码为音乐标记，使用VQ-VAE的3个级别。这些标记被用作每个级别的条件，这意味着不需要祖先抽样。
- en: '#### `continue_sample`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `continue_sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2619)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2619)'
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`music_tokens` (`List[torch.LongTensor]` of length `self.levels` ) — A sequence
    of music tokens which will be used as context to continue the sampling process.
    Should have `self.levels` tensors, each corresponding to the generation at a certain
    level.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`music_tokens`（长度为`self.levels`的`List[torch.LongTensor]`）- 一系列音乐标记，将用作继续采样过程的上下文。应该有`self.levels`个张量，每个对应于特定级别的生成。'
- en: '`labels` (`List[torch.LongTensor]` of length `n_sample`, and shape `(self.levels,
    self.config.max_nb_genre + lyric_sequence_length)` — List of metadata such as
    `artist_id`, `genre_id` and the full list of lyric tokens which are used to condition
    the generation.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（长度为`n_sample`的`List[torch.LongTensor]`，形状为`(self.levels, self.config.max_nb_genre
    + lyric_sequence_length)`）- 元数据列表，例如`artist_id`、`genre_id`和用于条件生成的完整歌词标记列表。'
- en: '`sampling_kwargs` (`Dict[Any]`) — Various additional sampling arguments that
    are used by the `_sample` function. A detail list of the arguments can bee seen
    in the `_sample` function documentation.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_kwargs` (`Dict[Any]`) — 由`_sample`函数使用的各种额外采样参数。参数的详细列表可以在`_sample`函数文档中看到。'
- en: Generates a continuation of the previously generated tokens.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 生成先前生成标记的延续。
- en: '#### `upsample`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `upsample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2634)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2634)'
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`music_tokens` (`List[torch.LongTensor]` of length `self.levels` ) — A sequence
    of music tokens which will be used as context to continue the sampling process.
    Should have `self.levels` tensors, each corresponding to the generation at a certain
    level.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`music_tokens` (`List[torch.LongTensor]`，长度为`self.levels`) — 一系列音乐标记，将用作继续采样过程的上下文。应该有`self.levels`个张量，每个对应于某个级别的生成。'
- en: '`labels` (`List[torch.LongTensor]` of length `n_sample`, and shape `(self.levels,
    self.config.max_nb_genre + lyric_sequence_length)` — List of metadata such as
    `artist_id`, `genre_id` and the full list of lyric tokens which are used to condition
    the generation.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`List[torch.LongTensor]`，长度为`n_sample`，形状为`(self.levels, self.config.max_nb_genre
    + lyric_sequence_length)` — 包括元数据，如`artist_id`、`genre_id`以及用于条件生成的完整歌词标记列表。'
- en: '`sampling_kwargs` (`Dict[Any]`) — Various additional sampling arguments that
    are used by the `_sample` function. A detail list of the arguments can bee seen
    in the `_sample` function documentation.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_kwargs` (`Dict[Any]`) — 由`_sample`函数使用的各种额外采样参数。参数的详细列表可以在`_sample`函数文档中看到。'
- en: Upsamples a sequence of music tokens using the prior at level `level`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用级别`level`的先验对音乐标记序列进行上采样。
- en: '#### `_sample`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `_sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2434)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2434)'
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`music_tokens` (`List[torch.LongTensor]`) — A sequence of music tokens of length
    `self.levels` which will be used as context to continue the sampling process.
    Should have `self.levels` tensors, each corresponding to the generation at a certain
    level.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`music_tokens` (`List[torch.LongTensor]`) — 音乐标记序列，长度为`self.levels`，将用作继续采样过程的上下文。应该有`self.levels`个张量，每个对应于某个级别的生成。'
- en: '`labels` (`List[torch.LongTensor]`) — List of length `n_sample`, and shape
    `(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)` metadata
    such as `artist_id`, `genre_id` and the full list of lyric tokens which are used
    to condition the generation.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`List[torch.LongTensor]`) — 长度为`n_sample`的列表，形状为`(self.levels, 4
    + self.config.max_nb_genre + lyric_sequence_length)`，包括`artist_id`、`genre_id`等元数据以及用于条件生成的完整歌词标记列表。'
- en: '`sample_levels` (`List[int]`) — List of the desired levels at which the sampling
    will be done. A level is equivalent to the index of the prior in the list of priors'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_levels` (`List[int]`) — 要进行采样的期望级别列表。级别等同于先验列表中的先验索引'
- en: '`metas` (`List[Any]`, *optional*) — Metadatas used to generate the `labels`'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metas` (`List[Any]`, *可选*) — 用于生成`labels`的元数据'
- en: '`chunk_size` (`int`, *optional*, defaults to 32) — Size of a chunk of audio,
    used to fill up the memory in chuncks to prevent OOM erros. Bigger chunks means
    faster memory filling but more consumption.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_size` (`int`, *可选*, 默认为32) — 音频块的大小，用于以块的形式填充内存，以防止OOM错误。更大的块意味着更快的内存填充但更多的消耗。'
- en: '`sampling_temperature` (`float`, *optional*, defaults to 0.98) — Temperature
    used to ajust the randomness of the sampling.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_temperature` (`float`, *可选*, 默认为0.98) — 用于调整采样随机性的温度。'
- en: '`lower_batch_size` (`int`, *optional*, defaults to 16) — Maximum batch size
    for the lower level priors'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lower_batch_size` (`int`, *可选*, 默认为16) — 低级别先验的最大批处理大小'
- en: '`max_batch_size` (`int`, *optional*, defaults to 16) — Maximum batch size for
    the top level priors'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_batch_size` (`int`, *可选*, 默认为16) — 顶层先验的最大批处理大小'
- en: '`sample_length_in_seconds` (`int`, *optional*, defaults to 24) — Desired length
    of the generation in seconds'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_length_in_seconds` (`int`, *可选*, 默认为24) — 生成的长度（秒数）'
- en: '`compute_alignments` (`bool`, *optional*, defaults to `False`) — Whether or
    not to compute the alignment between the lyrics and the audio using the top_prior'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_alignments` (`bool`, *可选*, 默认为`False`) — 是否计算歌词和音频之间的对齐，使用顶级先验'
- en: '`sample_tokens` (`int`, *optional*) — Precise number of tokens that should
    be sampled at each level. This is mostly useful for running dummy experiments'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_tokens` (`int`, *可选*) — 每个级别应采样的精确标记数。这对于运行虚拟实验非常有用'
- en: '`offset` (`int`, *optional*, defaults to 0) — Audio offset used as conditioning,
    corresponds to the starting sample in the music. If the offset is greater than
    0, the lyrics will be shifted take that intoaccount'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offset` (`int`, *可选*, 默认为0) — 用作条件的音频偏移，对应于音乐中的起始样本。如果偏移大于0，则歌词将被移位以考虑这一点'
- en: '`save_results` (`bool`, *optional*, defaults to `True`) — Whether or not to
    save the intermediate results. If `True`, will generate a folder named with the
    start time.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_results` (`bool`, *可选*, 默认为`True`) — 是否保存中间结果。如果为`True`，将生成一个以开始时间命名的文件夹。'
- en: '`sample_length` (`int`, *optional*) — Desired length of the generation in samples.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_length` (`int`, *可选*) — 生成的长度（样本数）。'
- en: Core sampling function used to generate music tokens. Iterates over the provided
    list of levels, while saving the generated raw audio at each step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成音乐标记的核心采样函数。在每一步保存生成的原始音频，遍历提供的级别列表。
- en: 'Returns: torch.Tensor'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：torch.Tensor
- en: 'Example:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE15]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: JukeboxPrior
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JukeboxPrior
- en: '### `class transformers.JukeboxPrior`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.JukeboxPrior`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L1769)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L1769)'
- en: '[PRE16]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`JukeboxPriorConfig`) — Model configuration class with all the parameters
    of the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`JukeboxPriorConfig`）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`level` (`int`, *optional*) — Current level of the Prior. Should be in range
    `[0,nb_priors]`.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level`（`int`，*可选*）— 先验的当前级别。应在范围`[0,nb_priors]`内。'
- en: '`nb_priors` (`int`, *optional*, defaults to 3) — Total number of priors.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nb_priors`（`int`，*可选*，默认为3）— 先验总数。'
- en: '`vqvae_encoder` (`Callable`, *optional*) — Encoding method of the VQVAE encoder
    used in the forward pass of the model. Passing functions instead of the vqvae
    module to avoid getting the parameters.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqvae_encoder`（`Callable`，*可选*）— VQVAE编码器在模型前向传递中使用的编码方法。传递函数而不是vqvae模块以避免获取参数。'
- en: '`vqvae_decoder` (`Callable`, *optional*) — Decoding method of the VQVAE decoder
    used in the forward pass of the model. Passing functions instead of the vqvae
    module to avoid getting the parameters.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqvae_decoder`（`Callable`，*可选*）— 在模型的前向传递中使用的VQVAE解码器的解码方法。传递函数而不是vqvae模块以避免获取参数。'
- en: The JukeboxPrior class, which is a wrapper around the various conditioning and
    the transformer. JukeboxPrior can be seen as language models trained on music.
    They model the next `music token` prediction task. If a (lyric) `encoderù is defined,
    it also models the` next character` prediction on the lyrics. Can be conditionned
    on timing, artist, genre, lyrics and codes from lower-levels Priors.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: JukeboxPrior类是各种条件和变压器的包装器。JukeboxPrior可以被视为在音乐上训练的语言模型。它们对下一个`音乐标记`预测任务进行建模。如果定义了（歌词）`编码器`，它还对歌词上的`下一个字符`预测进行建模。可以基于定时、艺术家、流派、歌词和来自较低级先验的代码进行条件化。
- en: '#### `sample`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2059)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2059)'
- en: '[PRE17]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`n_samples` (`int`) — Number of samples to generate.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_samples`（`int`）— 要生成的样本数。'
- en: '`music_tokens` (`List[torch.LongTensor]`, *optional*) — Previously gemerated
    tokens at the current level. Used as context for the generation.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`music_tokens`（`List[torch.LongTensor]`，*可选*）— 当前级别上先前生成的标记。用作生成的上下文。'
- en: '`music_tokens_conds` (`List[torch.FloatTensor]`, *optional*) — Upper-level
    music tokens generated by the previous prior model. Is `None` if the generation
    is not conditionned on the upper-level tokens.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`music_tokens_conds`（`List[torch.FloatTensor]`，*可选*）— 由先前先验模型生成的上层音乐标记。如果生成不是基于上层标记的，则为`None`。'
- en: '`metadata` (`List[torch.LongTensor]`, *optional*) — List containing the metatdata
    tensor with the artist, genre and the lyric tokens.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`（`List[torch.LongTensor]`，*可选*）— 包含包含艺术家、流派和歌词标记的元数据张量的列表。'
- en: '`temp` (`float`, *optional*, defaults to 1.0) — Sampling temperature.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temp`（`float`，*可选*，默认为1.0）— 采样温度。'
- en: '`top_k` (`int`, *optional*, defaults to 0) — Top k probabilities used for filtering.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k`（`int`，*可选*，默认为0）— 用于过滤的前k个概率。'
- en: '`top_p` (`float`, *optional*, defaults to 0.0) — Top p probabilities used for
    filtering.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p`（`float`，*可选*，默认为0.0）— 用于过滤的前p个概率。'
- en: '`chunk_size` (`int`, *optional*) — Size of the chunks used to prepare the cache
    of the transformer.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_size`（`int`，*可选*）— 用于准备变压器缓存的块的大小。'
- en: '`sample_tokens` (`int`, *optional*) — Number of tokens to sample.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_tokens`（`int`，*可选*）— 要采样的标记数。'
- en: Ancestral/Prime sampling a window of tokens using the provided conditioning
    and metadatas.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提供的条件和元数据对标记窗口进行祖先/主要采样。
- en: '#### `forward`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2227)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2227)'
- en: '[PRE18]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.Tensor`) — Hidden states which should be raw audio'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`torch.Tensor`）— 应为原始音频的隐藏状态'
- en: '`metadata` (`List[torch.LongTensor]`, *optional*) — List containing the metadata
    conditioning tensorwith the lyric and the metadata tokens.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`（`List[torch.LongTensor]`，*可选*）— 包含歌词和元数据标记的元数据条件张量的列表。'
- en: '`decode` (`bool`, *optional*, defaults to `False`) — Whether or not to decode
    the encoded to tokens.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode`（`bool`，*可选*，默认为`False`）— 是否解码编码为标记。'
- en: '`get_preds` (`bool`, *optional*, defaults to `False`) — Whether or not to return
    the actual predicitons of the model.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_preds`（`bool`，*可选*，默认为`False`）— 是否返回模型的实际预测。'
- en: Encode the hidden states using the `vqvae` encoder, and then predicts the next
    token in the `forward_tokens` function. The loss is the sum of the `encoder` loss
    and the `decoder` loss.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`vqvae`编码器对隐藏状态进行编码，然后在`forward_tokens`函数中预测下一个标记。损失是`encoder`损失和`decoder`损失的总和。
- en: JukeboxVQVAE
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JukeboxVQVAE
- en: '### `class transformers.JukeboxVQVAE`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.JukeboxVQVAE`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L595)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L595)'
- en: '[PRE19]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`JukeboxConfig`) — Model configuration class with all the parameters
    of the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`JukeboxConfig`）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Hierarchical VQ-VAE model used in Jukebox. This model follows the Hierarchical
    VQVAE paper from [Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod,
    Jamie Dougherty](https://arxiv.org/abs/2002.08111).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Jukebox中使用的分层VQ-VAE模型。该模型遵循[Will Williams, Sam Ringer, Tom Ash, John Hughes,
    David MacLeod, Jamie Dougherty](https://arxiv.org/abs/2002.08111)的Hierarchical
    VQVAE论文。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L739)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L739)'
- en: '[PRE20]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`raw_audio` (`torch.FloatTensor`) — Audio input which will be encoded and decoded.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_audio` (`torch.FloatTensor`) — 音频输入，将被编码和解码。'
- en: Forward pass of the VQ-VAE, encodes the `raw_audio` to latent states, which
    are then decoded for each level. The commit loss, which ensure that the encoder’s
    computed embeddings are close to the codebook vectors, is computed.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-VAE的前向传递，将`raw_audio`编码为潜在状态，然后为每个级别解码。计算提交损失，确保编码器计算的嵌入接近码书向量。
- en: 'Example:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#### `encode`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L709)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L709)'
- en: '[PRE22]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_audio` (`torch.Tensor`) — Raw audio which will be encoded to its discrete
    representation using the codebook. The closest `code` form the codebook will be
    computed for each sequence of samples.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_audio`（`torch.Tensor`）— 将被编码为其离散表示的原始音频，使用码书。将为每个样本序列计算最接近码书的`code`。'
- en: '`start_level` (`int`, *optional*, defaults to 0) — Level at which the encoding
    process will start. Default to 0.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_level`（`int`，*可选*，默认为0）— 编码过程将开始的级别。默认为0。'
- en: '`end_level` (`int`, *optional*) — Level at which the encoding process will
    start. Default to None.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_level`（`int`，*可选*）— 编码过程将开始的级别。默认为None。'
- en: '`bs_chunks` (int, *optional*, defaults to 1) — Number of chunks of raw audio
    to process at the same time.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bs_chunks`（int，*可选*，默认为1）— 要同时处理的原始音频块数。'
- en: Transforms the `input_audio` to a discrete representation made out of `music_tokens`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 将`input_audio`转换为由`music_tokens`组成的离散表示。
- en: '#### `decode`'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L673)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L673)'
- en: '[PRE23]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`music_tokens` (`torch.LongTensor`) — Tensor of music tokens which will be
    decoded to raw audio by using the codebook. Each music token should be an index
    to a corresponding `code` vector in the codebook.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`music_tokens`（`torch.LongTensor`）— 将通过使用码书解码为原始音频的音乐标记张量。每个音乐标记应该是码书中对应`code`向量的索引。'
- en: '`start_level` (`int`, *optional*) — Level at which the decoding process will
    start. Default to 0.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_level`（`int`，*可选*）— 解码过程将开始的级别。默认为0。'
- en: '`end_level` (`int`, *optional*) — Level at which the decoding process will
    start. Default to None.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_level`（`int`，*可选*）— 解码过程将开始的级别。默认为None。'
- en: '`bs_chunks` (int, *optional*) — Number of chunks to process at the same time.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bs_chunks`（int，*可选*）— 要同时处理的块数。'
- en: Transforms the input `music_tokens` to their `raw_audio` representation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入的`music_tokens`转换为它们的`raw_audio`表示。
