- en: The advantages and disadvantages of policy-gradient methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ä¼˜ç¼ºç‚¹
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might ask, â€œbut Deep Q-Learning is excellent! Why use policy-gradient
    methods?â€œ. To answer this question, letâ€™s study the **advantages and disadvantages
    of policy-gradient methods**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œä½ å¯èƒ½ä¼šé—®ï¼Œâ€œä½†æ·±åº¦Qå­¦ä¹ å¾ˆæ£’ï¼ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Ÿâ€ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè®©æˆ‘ä»¬æ¥ç ”ç©¶**ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ä¼˜ç¼ºç‚¹**ã€‚
- en: Advantages
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿
- en: 'There are multiple advantages over value-based methods. Letâ€™s see some of them:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”å€¼åŸºæ–¹æ³•ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•æœ‰å¤šä¸ªä¼˜åŠ¿ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å…¶ä¸­ä¸€äº›ï¼š
- en: The simplicity of integration
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é›†æˆçš„ç®€å•æ€§
- en: We can estimate the policy directly without storing additional data (action
    values).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç›´æ¥ä¼°è®¡ç­–ç•¥ï¼Œè€Œæ— éœ€å­˜å‚¨é¢å¤–æ•°æ®ï¼ˆåŠ¨ä½œå€¼ï¼‰ã€‚
- en: Policy-gradient methods can learn a stochastic policy
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯ä»¥å­¦ä¹ éšæœºç­–ç•¥
- en: Policy-gradient methods canÂ **learn a stochastic policy while value functions
    canâ€™t**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯ä»¥**å­¦ä¹ éšæœºç­–ç•¥ï¼Œè€Œå€¼å‡½æ•°åˆ™ä¸èƒ½**ã€‚
- en: 'This has two consequences:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰ä¸¤ä¸ªåæœï¼š
- en: We **donâ€™t need to implement an exploration/exploitation trade-off by hand**.
    Since we output a probability distribution over actions, the agent exploresÂ **the
    state space without always taking the same trajectory.**
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬**ä¸éœ€è¦æ‰‹åŠ¨å®ç°æ¢ç´¢/åˆ©ç”¨æƒè¡¡**ã€‚ç”±äºæˆ‘ä»¬è¾“å‡ºäº†åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»£ç†ä¼šæ¢ç´¢**çŠ¶æ€ç©ºé—´ï¼Œè€Œä¸æ€»æ˜¯é‡‡å–ç›¸åŒçš„è½¨è¿¹ã€‚**
- en: We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing
    is when two states seem (or are) the same but need different actions.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æ‘†è„±äº†**æ„ŸçŸ¥åˆ«å**çš„é—®é¢˜ã€‚æ„ŸçŸ¥åˆ«åæ˜¯æŒ‡ä¸¤ä¸ªçŠ¶æ€çœ‹èµ·æ¥ï¼ˆæˆ–è€…ç¡®å®ï¼‰ç›¸åŒï¼Œä½†éœ€è¦ä¸åŒçš„åŠ¨ä½œã€‚
- en: 'Letâ€™s take an example: we have an intelligent vacuum cleaner whose goal is
    to suck the dust and avoid killing the hamsters.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªæ™ºèƒ½å¸å°˜å™¨ï¼Œå…¶ç›®æ ‡æ˜¯å¸å°˜å¹¶é¿å…æ€æ­»ä»“é¼ ã€‚
- en: '![Hamster 1](../Images/1b61fde218600e239ec27f3af716584c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![ä»“é¼ 1](../Images/1b61fde218600e239ec27f3af716584c.png)'
- en: Our vacuum cleaner can only perceive where the walls are.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¸å°˜å™¨åªèƒ½æ„ŸçŸ¥å¢™å£çš„ä½ç½®ã€‚
- en: The problem is that the **two red (colored) states are aliased states because
    the agent perceives an upper and lower wall for each**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜åœ¨äº**è¿™ä¸¤ä¸ªçº¢è‰²çŠ¶æ€æ˜¯åˆ«åçŠ¶æ€ï¼Œå› ä¸ºä»£ç†å¯¹æ¯ä¸ªçŠ¶æ€æ„ŸçŸ¥åˆ°ä¸Šä¸‹å¢™å£**ã€‚
- en: '![Hamster 1](../Images/63123f815fb96086071da70edf73b3fd.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![ä»“é¼ 1](../Images/63123f815fb96086071da70edf73b3fd.png)'
- en: Under a deterministic policy, the policy will either always move right when
    in a red state or always move left. **Either case will cause our agent to get
    stuck and never suck the dust**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¡®å®šæ€§ç­–ç•¥ä¸‹ï¼Œç­–ç•¥åœ¨çº¢è‰²çŠ¶æ€ä¸‹è¦ä¹ˆæ€»æ˜¯å‘å³ç§»åŠ¨ï¼Œè¦ä¹ˆæ€»æ˜¯å‘å·¦ç§»åŠ¨ã€‚**ä»»ä½•ä¸€ç§æƒ…å†µéƒ½ä¼šå¯¼è‡´æˆ‘ä»¬çš„ä»£ç†è¢«å¡ä½ï¼Œæ°¸è¿œæ— æ³•å¸å°˜**ã€‚
- en: Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic
    policy** (â€œgreedy epsilon strategyâ€). Consequently, our agent can **spend a lot
    of time before finding the dust**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºå€¼çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸‹ï¼Œæˆ‘ä»¬å­¦ä¹ ä¸€ä¸ª**å‡†ç¡®å®šç­–ç•¥**ï¼ˆâ€œè´ªå©ªÎµç­–ç•¥â€ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ä»£ç†å¯èƒ½**èŠ±è´¹å¾ˆé•¿æ—¶é—´æ‰èƒ½æ‰¾åˆ°ç°å°˜**ã€‚
- en: On the other hand, an optimal stochastic policy **will randomly move left or
    right in red (colored) states**. Consequently, **it will not be stuck and will
    reach the goal state with a high probability**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œä¸€ä¸ªæœ€ä½³çš„éšæœºç­–ç•¥**å°†åœ¨çº¢è‰²çŠ¶æ€ä¸­éšæœºå‘å·¦æˆ–å‘å³ç§»åŠ¨**ã€‚å› æ­¤ï¼Œ**å®ƒä¸ä¼šè¢«å¡ä½ï¼Œå¹¶ä¸”æœ‰å¾ˆé«˜çš„æ¦‚ç‡åˆ°è¾¾ç›®æ ‡çŠ¶æ€**ã€‚
- en: '![Hamster 1](../Images/947f544dc21feed19c3c29ad4cc261f3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![ä»“é¼ 1](../Images/947f544dc21feed19c3c29ad4cc261f3.png)'
- en: Policy-gradient methods are more effective in high-dimensional action spaces
    and continuous actions spaces
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨é«˜ç»´åŠ¨ä½œç©ºé—´å’Œè¿ç»­åŠ¨ä½œç©ºé—´ä¸­æ›´æœ‰æ•ˆã€‚
- en: The problem with Deep Q-learning is that their **predictions assign a score
    (maximum expected future reward) for each possible action**, at each time step,
    given the current state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦Qå­¦ä¹ çš„é—®é¢˜åœ¨äºï¼Œå®ƒä»¬çš„**é¢„æµ‹ä¸ºæ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œåˆ†é…ä¸€ä¸ªåˆ†æ•°ï¼ˆæœ€å¤§é¢„æœŸæœªæ¥å¥–åŠ±ï¼‰**ï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ä¸­ï¼Œç»™å®šå½“å‰çŠ¶æ€ã€‚
- en: But what if we have an infinite possibility of actions?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœæˆ‘ä»¬æœ‰æ— é™çš„åŠ¨ä½œå¯èƒ½æ€§å‘¢ï¼Ÿ
- en: For instance, with a self-driving car, at each state, you can have a (near)
    infinite choice of actions (turning the wheel at 15Â°, 17.2Â°, 19,4Â°, honking, etc.).
    **Weâ€™ll need to output a Q-value for each possible action**! And **taking the
    max action of a continuous output is an optimization problem itself**!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œæ¯ä¸ªçŠ¶æ€ä¸‹éƒ½å¯ä»¥æœ‰ï¼ˆè¿‘ä¹ï¼‰æ— é™çš„åŠ¨ä½œé€‰æ‹©ï¼ˆæ–¹å‘ç›˜è½¬åŠ¨15Â°ã€17.2Â°ã€19.4Â°ã€æŒ‰å–‡å­ç­‰ï¼‰ã€‚**æˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œè¾“å‡ºä¸€ä¸ªQå€¼**ï¼è€Œ**å¯¹è¿ç»­è¾“å‡ºå–æœ€å¤§åŠ¨ä½œæœ¬èº«å°±æ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜**ï¼
- en: Instead, with policy-gradient methods, we output aÂ **probability distribution
    over actions.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œæˆ‘ä»¬è¾“å‡ºä¸€ä¸ª**åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ**ã€‚
- en: Policy-gradient methods have better convergence properties
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ”¶æ•›æ€§è´¨
- en: 'In value-based methods, we use an aggressive operator to **change the value
    function: we take the maximum over Q-estimates**. Consequently, the action probabilities
    may change dramatically for an arbitrarily small change in the estimated action
    values if that change results in a different action having the maximal value.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºå€¼çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ¿€è¿›çš„è¿ç®—ç¬¦**æ”¹å˜å€¼å‡½æ•°ï¼šæˆ‘ä»¬å–Qä¼°è®¡çš„æœ€å¤§å€¼**ã€‚å› æ­¤ï¼Œå¦‚æœä¼°è®¡çš„åŠ¨ä½œå€¼å‘ç”Ÿå¾®å°å˜åŒ–å¯¼è‡´ä¸åŒçš„åŠ¨ä½œå…·æœ‰æœ€å¤§å€¼ï¼Œé‚£ä¹ˆåŠ¨ä½œæ¦‚ç‡å¯èƒ½ä¼šå› ä¸ºä¼°è®¡çš„åŠ¨ä½œå€¼çš„å¾®å°å˜åŒ–è€Œå‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚
- en: For instance, if during the training, the best action was left (with a Q-value
    of 0.22) and the training step after itâ€™s right (since the right Q-value becomes
    0.23), we dramatically changed the policy since now the policy will take most
    of the time right instead of left.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒæœŸé—´ï¼Œæœ€ä½³åŠ¨ä½œæ˜¯å‘å·¦ï¼ˆQå€¼ä¸º0.22ï¼‰ï¼Œç„¶ååœ¨è®­ç»ƒæ­¥éª¤ä¹‹åæ˜¯å‘å³ï¼ˆå› ä¸ºå³ä¾§Qå€¼å˜ä¸º0.23ï¼‰ï¼Œæˆ‘ä»¬å¤§å¹…æ”¹å˜äº†ç­–ç•¥ï¼Œå› ä¸ºç°åœ¨ç­–ç•¥å°†å¤§éƒ¨åˆ†æ—¶é—´å‘å³è€Œä¸æ˜¯å‘å·¦ã€‚
- en: On the other hand, in policy-gradient methods, stochastic policy action preferences
    (probability of taking action) **change smoothly over time**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œåœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼Œéšæœºç­–ç•¥åŠ¨ä½œåå¥½ï¼ˆé‡‡å–åŠ¨ä½œçš„æ¦‚ç‡ï¼‰**éšæ—¶é—´å¹³æ»‘å˜åŒ–**ã€‚
- en: Disadvantages
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼ºç‚¹
- en: 'Naturally, policy-gradient methods also have some disadvantages:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶åœ°ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¹Ÿæœ‰ä¸€äº›ç¼ºç‚¹ï¼š
- en: '**Frequently, policy-gradient methods converges to a local maximum instead
    of a global optimum.**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç»å¸¸æ”¶æ•›åˆ°å±€éƒ¨æœ€å¤§å€¼ï¼Œè€Œä¸æ˜¯å…¨å±€æœ€ä¼˜å€¼ã€‚**'
- en: 'Policy-gradient goes slower,Â **step by step: it can take longer to train (inefficient).**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦èµ°å¾—æ›´æ…¢ï¼Œ**ä¸€æ­¥ä¸€æ­¥ï¼šè®­ç»ƒå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼ˆä½æ•ˆï¼‰ã€‚**
- en: Policy-gradient can have high variance. Weâ€™ll see in the actor-critic unit why,
    and how we can solve this problem.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦å¯èƒ½å…·æœ‰å¾ˆé«˜çš„æ–¹å·®ã€‚æˆ‘ä»¬å°†åœ¨æ¼”å‘˜-è¯„è®ºå•å…ƒä¸­çœ‹åˆ°ä¸ºä»€ä¹ˆï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: ğŸ‘‰ If you want to go deeper into the advantages and disadvantages of policy-gradient
    methods, [you can check this video](https://youtu.be/y3oqOjHilio).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œ[æ‚¨å¯ä»¥æŸ¥çœ‹è¿™ä¸ªè§†é¢‘](https://youtu.be/y3oqOjHilio)ã€‚
