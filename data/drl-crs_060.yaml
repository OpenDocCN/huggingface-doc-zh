- en: The advantages and disadvantages of policy-gradient methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度方法的优缺点
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might ask, “but Deep Q-Learning is excellent! Why use policy-gradient
    methods?“. To answer this question, let’s study the **advantages and disadvantages
    of policy-gradient methods**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会问，“但深度Q学习很棒！为什么要使用策略梯度方法？”。为了回答这个问题，让我们来研究**策略梯度方法的优缺点**。
- en: Advantages
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势
- en: 'There are multiple advantages over value-based methods. Let’s see some of them:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 相比值基方法，策略梯度方法有多个优势。让我们看看其中一些：
- en: The simplicity of integration
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成的简单性
- en: We can estimate the policy directly without storing additional data (action
    values).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接估计策略，而无需存储额外数据（动作值）。
- en: Policy-gradient methods can learn a stochastic policy
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略梯度方法可以学习随机策略
- en: Policy-gradient methods can **learn a stochastic policy while value functions
    can’t**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法可以**学习随机策略，而值函数则不能**。
- en: 'This has two consequences:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这有两个后果：
- en: We **don’t need to implement an exploration/exploitation trade-off by hand**.
    Since we output a probability distribution over actions, the agent explores **the
    state space without always taking the same trajectory.**
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们**不需要手动实现探索/利用权衡**。由于我们输出了动作的概率分布，代理会探索**状态空间，而不总是采取相同的轨迹。**
- en: We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing
    is when two states seem (or are) the same but need different actions.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还摆脱了**感知别名**的问题。感知别名是指两个状态看起来（或者确实）相同，但需要不同的动作。
- en: 'Let’s take an example: we have an intelligent vacuum cleaner whose goal is
    to suck the dust and avoid killing the hamsters.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子：我们有一个智能吸尘器，其目标是吸尘并避免杀死仓鼠。
- en: '![Hamster 1](../Images/1b61fde218600e239ec27f3af716584c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![仓鼠1](../Images/1b61fde218600e239ec27f3af716584c.png)'
- en: Our vacuum cleaner can only perceive where the walls are.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的吸尘器只能感知墙壁的位置。
- en: The problem is that the **two red (colored) states are aliased states because
    the agent perceives an upper and lower wall for each**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于**这两个红色状态是别名状态，因为代理对每个状态感知到上下墙壁**。
- en: '![Hamster 1](../Images/63123f815fb96086071da70edf73b3fd.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![仓鼠1](../Images/63123f815fb96086071da70edf73b3fd.png)'
- en: Under a deterministic policy, the policy will either always move right when
    in a red state or always move left. **Either case will cause our agent to get
    stuck and never suck the dust**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定性策略下，策略在红色状态下要么总是向右移动，要么总是向左移动。**任何一种情况都会导致我们的代理被卡住，永远无法吸尘**。
- en: Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic
    policy** (“greedy epsilon strategy”). Consequently, our agent can **spend a lot
    of time before finding the dust**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于值的强化学习算法下，我们学习一个**准确定策略**（“贪婪ε策略”）。因此，我们的代理可能**花费很长时间才能找到灰尘**。
- en: On the other hand, an optimal stochastic policy **will randomly move left or
    right in red (colored) states**. Consequently, **it will not be stuck and will
    reach the goal state with a high probability**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一个最佳的随机策略**将在红色状态中随机向左或向右移动**。因此，**它不会被卡住，并且有很高的概率到达目标状态**。
- en: '![Hamster 1](../Images/947f544dc21feed19c3c29ad4cc261f3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![仓鼠1](../Images/947f544dc21feed19c3c29ad4cc261f3.png)'
- en: Policy-gradient methods are more effective in high-dimensional action spaces
    and continuous actions spaces
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略梯度方法在高维动作空间和连续动作空间中更有效。
- en: The problem with Deep Q-learning is that their **predictions assign a score
    (maximum expected future reward) for each possible action**, at each time step,
    given the current state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习的问题在于，它们的**预测为每个可能的动作分配一个分数（最大预期未来奖励）**，在每个时间步骤中，给定当前状态。
- en: But what if we have an infinite possibility of actions?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们有无限的动作可能性呢？
- en: For instance, with a self-driving car, at each state, you can have a (near)
    infinite choice of actions (turning the wheel at 15°, 17.2°, 19,4°, honking, etc.).
    **We’ll need to output a Q-value for each possible action**! And **taking the
    max action of a continuous output is an optimization problem itself**!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于自动驾驶汽车，每个状态下都可以有（近乎）无限的动作选择（方向盘转动15°、17.2°、19.4°、按喇叭等）。**我们需要为每个可能的动作输出一个Q值**！而**对连续输出取最大动作本身就是一个优化问题**！
- en: Instead, with policy-gradient methods, we output a **probability distribution
    over actions.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，使用策略梯度方法，我们输出一个**动作的概率分布**。
- en: Policy-gradient methods have better convergence properties
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略梯度方法具有更好的收敛性质
- en: 'In value-based methods, we use an aggressive operator to **change the value
    function: we take the maximum over Q-estimates**. Consequently, the action probabilities
    may change dramatically for an arbitrarily small change in the estimated action
    values if that change results in a different action having the maximal value.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于值的方法中，我们使用一个激进的运算符**改变值函数：我们取Q估计的最大值**。因此，如果估计的动作值发生微小变化导致不同的动作具有最大值，那么动作概率可能会因为估计的动作值的微小变化而发生剧烈变化。
- en: For instance, if during the training, the best action was left (with a Q-value
    of 0.22) and the training step after it’s right (since the right Q-value becomes
    0.23), we dramatically changed the policy since now the policy will take most
    of the time right instead of left.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在训练期间，最佳动作是向左（Q值为0.22），然后在训练步骤之后是向右（因为右侧Q值变为0.23），我们大幅改变了策略，因为现在策略将大部分时间向右而不是向左。
- en: On the other hand, in policy-gradient methods, stochastic policy action preferences
    (probability of taking action) **change smoothly over time**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在策略梯度方法中，随机策略动作偏好（采取动作的概率）**随时间平滑变化**。
- en: Disadvantages
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: 'Naturally, policy-gradient methods also have some disadvantages:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，策略梯度方法也有一些缺点：
- en: '**Frequently, policy-gradient methods converges to a local maximum instead
    of a global optimum.**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略梯度方法经常收敛到局部最大值，而不是全局最优值。**'
- en: 'Policy-gradient goes slower, **step by step: it can take longer to train (inefficient).**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度走得更慢，**一步一步：训练可能需要更长时间（低效）。**
- en: Policy-gradient can have high variance. We’ll see in the actor-critic unit why,
    and how we can solve this problem.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度可能具有很高的方差。我们将在演员-评论单元中看到为什么，以及我们如何解决这个问题。
- en: 👉 If you want to go deeper into the advantages and disadvantages of policy-gradient
    methods, [you can check this video](https://youtu.be/y3oqOjHilio).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 如果您想深入了解策略梯度方法的优缺点，[您可以查看这个视频](https://youtu.be/y3oqOjHilio)。
