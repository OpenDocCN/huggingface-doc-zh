- en: How do Unity ML-Agents work?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unity ML-Agents如何工作？
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works](https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works](https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Before training our agent, we need to understand **what ML-Agents is and how
    it works**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的代理之前，我们需要了解**ML-Agents是什么以及它是如何工作的**。
- en: What is Unity ML-Agents?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Unity ML-Agents？
- en: '[Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents) is a toolkit
    for the game engine Unity that **allows us to create environments using Unity
    or use pre-made environments to train our agents**.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents)是Unity游戏引擎的工具包，**允许我们使用Unity创建环境或使用预先制作的环境来训练我们的代理**。'
- en: 'It’s developed by [Unity Technologies](https://unity.com/), the developers
    of Unity, one of the most famous Game Engines used by the creators of Firewatch,
    Cuphead, and Cities: Skylines.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '它由[Unity Technologies](https://unity.com/)开发，是Unity的开发者之一，Unity是最著名的游戏引擎之一，被Firewatch、Cuphead和Cities:
    Skylines的创作者使用。'
- en: '![Firewatch](../Images/56b87aa0405b4b90defdc70caa6f9571.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Firewatch](../Images/56b87aa0405b4b90defdc70caa6f9571.png)'
- en: Firewatch was made with Unity
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Firewatch是用Unity制作的
- en: The six components
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 六个组件
- en: 'With Unity ML-Agents, you have six essential components:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Unity ML-Agents，您有六个基本组件：
- en: '![MLAgents](../Images/f1083a6f889f83b0d7ba8f82fe1c00da.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![MLAgents](../Images/f1083a6f889f83b0d7ba8f82fe1c00da.png)'
- en: 'Source: [Unity ML-Agents Documentation](https://unity-technologies.github.io/ml-agents/)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Unity ML-Agents文档](https://unity-technologies.github.io/ml-agents/)
- en: The first is the *Learning Environment*, which contains **the Unity scene (the
    environment) and the environment elements** (game characters).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个是*学习环境*，其中包含**Unity场景（环境）和环境元素**（游戏角色）。
- en: The second is the *Python Low-level API*, which contains **the low-level Python
    interface for interacting and manipulating the environment**. It’s the API we
    use to launch the training.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个是*Python低级API*，其中包含**用于与环境交互和操作的低级Python接口**。这是我们用来启动训练的API。
- en: Then, we have the *External Communicator* that **connects the Learning Environment
    (made with C#) with the low level Python API (Python)**.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们有*外部通信器*，它**连接学习环境（用C#制作）与低级Python API（Python）**。
- en: 'The *Python trainers*: the **Reinforcement algorithms made with PyTorch (PPO,
    SAC…)**.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python训练器*：使用PyTorch制作的**强化学习算法（PPO、SAC等）**。'
- en: 'The *Gym wrapper*: to encapsulate the RL environment in a gym wrapper.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gym包装器*：将RL环境封装在gym包装器中。'
- en: 'The *PettingZoo wrapper*: PettingZoo is the multi-agents version of the gym
    wrapper.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PettingZoo包装器*：PettingZoo是gym包装器的多代理版本。'
- en: Inside the Learning Component
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在学习组件内部
- en: 'Inside the Learning Component, we have **two important elements**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习组件内部，我们有**两个重要元素**：
- en: The first is the *agent component*, the actor of the scene. We’ll **train the
    agent by optimizing its policy** (which will tell us what action to take in each
    state). The policy is called the *Brain*.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个是*代理组件*，场景的执行者。我们将通过优化其策略来**训练代理**（这将告诉我们在每个状态下应该采取什么行动）。策略被称为*Brain*。
- en: Finally, there is the *Academy*. This component **orchestrates agents and their
    decision-making processes**. Think of this Academy as a teacher who handles Python
    API requests.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，有*学院*。这个组件**协调代理和它们的决策过程**。将这个学院想象成一个处理Python API请求的老师。
- en: 'To better understand its role, let’s remember the RL process. This can be modeled
    as a loop that works like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解其作用，让我们回顾一下RL过程。这可以被建模为一个循环，工作方式如下：
- en: '![The RL process](../Images/018079078cf4ad9c782cc74fc0ce7a20.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![RL过程](../Images/018079078cf4ad9c782cc74fc0ce7a20.png)'
- en: 'The RL Process: a loop of state, action, reward and next state'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RL过程：一个状态、动作、奖励和下一个状态的循环
- en: 'Source: [Reinforcement Learning: An Introduction, Richard Sutton and Andrew
    G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[强化学习：一种介绍，Richard Sutton和Andrew G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)
- en: 'Now, let’s imagine an agent learning to play a platform game. The RL process
    looks like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们想象一个代理学习玩平台游戏。RL过程看起来像这样：
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![RL过程](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
- en: Our Agent receives **state <math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​** from the **Environment** —
    we receive the first frame of our game (Environment).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的代理从**环境**接收**状态<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0** — 我们接收游戏的第一帧（环境）。
- en: Based on that **state<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​,** the Agent
    takes **action<math><semantics><mrow><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">A_0</annotation></semantics></math>A0​** — our Agent
    will move to the right.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于那个**状态<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0**，代理会采取**动作<math><semantics><mrow><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">A_0</annotation></semantics></math>A0** — 我们的代理会向右移动。
- en: The environment goes to a **new** **state<math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">S_1</annotation></semantics></math>S1​** — new frame.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境进入一个**新的**状态<math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">S_1</annotation></semantics></math>S1** — 新的帧。
- en: The environment gives some **reward<math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">R_1</annotation></semantics></math>R1​** to the Agent
    — we’re not dead *(Positive Reward +1)*.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境给代理一些**奖励<math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">R_1</annotation></semantics></math>R1** — 我们还没有死亡（正奖励+1）。
- en: This RL loop outputs a sequence of **state, action, reward and next state.**
    The goal of the agent is to **maximize the expected cumulative reward**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个RL循环输出一个序列**状态、动作、奖励和下一个状态**。代理的目标是**最大化预期的累积奖励**。
- en: 'The Academy will be the one that will **send the order to our Agents and ensure
    that agents are in sync**:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 学院将会**向我们的智能体发送指令，并确保智能体同步**：
- en: Collect Observations
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集观察数据
- en: Select your action using your policy
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你的策略选择动作
- en: Take the Action
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行动作
- en: Reset if you reached the max step or if you’re done.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你达到了最大步数或者已经完成了，就重置。
- en: '![The MLAgents Academy](../Images/ce5e350e3e33e7b48aee5a99e9c2666d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![MLAgents学院](../Images/ce5e350e3e33e7b48aee5a99e9c2666d.png)'
- en: Now that we understand how ML-Agents works, **we’re ready to train our agents.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了ML-Agents的工作原理，**我们准备好训练我们的智能体了。**
