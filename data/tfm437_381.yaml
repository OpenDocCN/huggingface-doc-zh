- en: Trajectory Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轨迹Transformer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This model is in maintenance mode only, so we won’t accept any new PRs changing
    its code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型仅处于维护模式，因此我们不会接受任何更改其代码的新PR。
- en: 'If you run into any issues running this model, please reinstall the last version
    that supported this model: v4.30.0. You can do so by running the following command:
    `pip install -U transformers==4.30.0`.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.30.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.30.0`。
- en: Overview
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The Trajectory Transformer model was proposed in [Offline Reinforcement Learning
    as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael
    Janner, Qiyang Li, Sergey Levine.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹Transformer模型是由Michael Janner、Qiyang Li、Sergey Levine在[离线强化学习作为一个大的序列建模问题](https://arxiv.org/abs/2106.02039)中提出的。
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Reinforcement learning (RL) is typically concerned with estimating stationary
    policies or single-step models, leveraging the Markov property to factorize problems
    in time. However, we can also view RL as a generic sequence modeling problem,
    with the goal being to produce a sequence of actions that leads to a sequence
    of high rewards. Viewed in this way, it is tempting to consider whether high-capacity
    sequence prediction models that work well in other domains, such as natural-language
    processing, can also provide effective solutions to the RL problem. To this end,
    we explore how RL can be tackled with the tools of sequence modeling, using a
    Transformer architecture to model distributions over trajectories and repurposing
    beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies
    a range of design decisions, allowing us to dispense with many of the components
    common in offline RL algorithms. We demonstrate the flexibility of this approach
    across long-horizon dynamics prediction, imitation learning, goal-conditioned
    RL, and offline RL. Further, we show that this approach can be combined with existing
    model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon
    tasks.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习（RL）通常涉及估计稳态策略或单步模型，利用马尔可夫性质在时间上分解问题。然而，我们也可以将RL视为一个通用的序列建模问题，目标是产生一系列导致高奖励序列的动作。从这个角度看，我们很容易考虑高容量序列预测模型在其他领域（如自然语言处理）中的良好工作是否也可以为RL问题提供有效的解决方案。为此，我们探讨了如何使用序列建模工具来解决RL问题，使用Transformer架构来建模轨迹分布，并将波束搜索重新用作规划算法。将RL作为序列建模问题来框定简化了一系列设计决策，使我们能够摒弃许多离线RL算法中常见的组件。我们展示了这种方法在长期动态预测、模仿学习、目标条件RL和离线RL等方面的灵活性。此外，我们展示了这种方法可以与现有的无模型算法结合，产生稀疏奖励、长期视野任务中的最先进规划器。*'
- en: This model was contributed by [CarlCochet](https://huggingface.co/CarlCochet).
    The original code can be found [here](https://github.com/jannerm/trajectory-transformer).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[CarlCochet](https://huggingface.co/CarlCochet)贡献的。原始代码可以在[这里](https://github.com/jannerm/trajectory-transformer)找到。
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: This Transformer is used for deep reinforcement learning. To use it, you need
    to create sequences from actions, states and rewards from all previous timesteps.
    This model will treat all these elements together as one big sequence (a trajectory).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Transformer用于深度强化学习。要使用它，您需要从所有先前时间步的动作、状态和奖励创建序列。这个模型将把所有这些元素一起视为一个大序列（一个轨迹）。
- en: TrajectoryTransformerConfig
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrajectoryTransformerConfig
- en: '### `class transformers.TrajectoryTransformerConfig`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrajectoryTransformerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/configuration_trajectory_transformer.py#L31)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/configuration_trajectory_transformer.py#L31)'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 100) — Vocabulary size of the
    TrajectoryTransformer model. Defines the number of different tokens that can be
    represented by the `trajectories` passed when calling [TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为100）—TrajectoryTransformer模型的词汇量。定义了在调用[TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel)时可以表示的`trajectories`的不同标记数量。'
- en: '`action_weight` (`int`, *optional*, defaults to 5) — Weight of the action in
    the loss function'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action_weight`（`int`，*可选*，默认为5）—损失函数中动作的权重'
- en: '`reward_weight` (`int`, *optional*, defaults to 1) — Weight of the reward in
    the loss function'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward_weight`（`int`，*可选*，默认为1）—损失函数中奖励的权重'
- en: '`value_weight` (`int`, *optional*, defaults to 1) — Weight of the value in
    the loss function'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value_weight`（`int`，*可选*，默认为1）—损失函数中值的权重'
- en: '`block_size` (`int`, *optional*, defaults to 249) — Size of the blocks in the
    trajectory transformer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_size`（`int`，*可选*，默认为249）—轨迹Transformer中块的大小。'
- en: '`action_dim` (`int`, *optional*, defaults to 6) — Dimension of the action space.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action_dim`（`int`，*可选*，默认为6）—动作空间的维度。'
- en: '`observation_dim` (`int`, *optional*, defaults to 17) — Dimension of the observation
    space.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation_dim`（`int`，*可选*，默认为17）—观察空间的维度。'
- en: '`transition_dim` (`int`, *optional*, defaults to 25) — Dimension of the transition
    space.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transition_dim`（`int`，*可选*，默认为25）—转换空间的维度。'
- en: '`n_layer` (`int`, *optional*, defaults to 4) — Number of hidden layers in the
    Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer`（`int`，*可选*，默认为4）—Transformer编码器中的隐藏层数。'
- en: '`n_head` (`int`, *optional*, defaults to 4) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head`（`int`，*可选*，默认为4）—Transformer编码器中每个注意力层的注意力头数。'
- en: '`n_embd` (`int`, *optional*, defaults to 128) — Dimensionality of the embeddings
    and hidden states.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_embd` (`int`, *optional*, defaults to 128) — 嵌入和隐藏状态的维度。'
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) — The dropout ratio for the
    embeddings.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) — 嵌入的dropout比率。'
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout ratio for
    the attention.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) — 注意力的dropout比率。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512或1024或2048）。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。'
- en: '`kaiming_initializer_range` (`float, *optional*, defaults to 1) — A coefficient
    scaling the negative slope of the kaiming initializer rectifier for EinLinear
    layers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kaiming_initializer_range` (`float, *optional*, defaults to 1) — 缩放kaiming初始化器整流器的负斜率的系数，用于EinLinear层。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`. Example —'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（不是所有模型都使用）。仅在`config.is_decoder=True`时相关。示例
    —'
- en: This is the configuration class to store the configuration of a [TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel).
    It is used to instantiate an TrajectoryTransformer model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the TrajectoryTransformer
    [CarlCochet/trajectory-transformer-halfcheetah-medium-v2](https://huggingface.co/CarlCochet/trajectory-transformer-halfcheetah-medium-v2)
    architecture.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel)配置的配置类。根据指定的参数实例化TrajectoryTransformer模型，定义模型架构。使用默认值实例化配置将产生类似于TrajectoryTransformer
    [CarlCochet/trajectory-transformer-halfcheetah-medium-v2](https://huggingface.co/CarlCochet/trajectory-transformer-halfcheetah-medium-v2)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TrajectoryTransformerModel
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrajectoryTransformerModel
- en: '### `class transformers.TrajectoryTransformerModel`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrajectoryTransformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py#L399)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py#L399)'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TrajectoryTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TrajectoryTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare TrajectoryTransformer Model transformer outputting raw hidden-states
    without any specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的TrajectoryTransformer模型，输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: the full GPT language model, with a context size of block_size
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的GPT语言模型，上下文大小为block_size
- en: '#### `forward`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py#L464)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py#L464)'
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`trajectories` (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Batch of trajectories, where a trajectory is a sequence of states, actions and
    rewards.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trajectories` (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — 轨迹的批次，其中轨迹是状态、动作和奖励的序列。'
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`,
    *optional*) — Contains precomputed hidden-states (key and values in the attention
    blocks) as computed by the model (see `past_key_values` output below). Can be
    used to speed up sequential decoding. The `input_ids` which have their past given
    to this model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` 长度为`config.n_layers`, *optional*)
    — 包含由模型计算的预先计算的隐藏状态（注意力块中的键和值）（参见下面的`past_key_values`输出）。可用于加速顺序解码。将过去给定给该模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`targets` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Desired targets used to compute the loss.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targets` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算损失的目标值。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1。
- en: 0 for tokens that are `masked`.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.deprecated.trajectory_transformer.modeling_trajectory_transformer.TrajectoryTransformerOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.deprecated.trajectory_transformer.modeling_trajectory_transformer.TrajectoryTransformerOutput`
    或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.deprecated.trajectory_transformer.modeling_trajectory_transformer.TrajectoryTransformerOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TrajectoryTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig))
    and inputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.deprecated.trajectory_transformer.modeling_trajectory_transformer.TrajectoryTransformerOutput`或一个`torch.FloatTensor`的元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[TrajectoryTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]`, *optional*, returned when
    `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of length
    `config.n_layers`, containing tuples of tensors of shape `(batch_size, num_heads,
    sequence_length, embed_size_per_head)`). Contains pre-computed hidden-states (key
    and values in the attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的元组，包含形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量元组。包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 长度为2的元组，包含形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`（一个用于嵌入的输出
    + 一个用于每个层的输出）。模型在每个层的输出隐藏状态加上初始嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    GPT2Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 长度为每层的元组，形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`。在注意力softmax之后的GPT2Attentions权重，用于计算自注意力头中的加权平均值。'
- en: The [TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Examples:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
