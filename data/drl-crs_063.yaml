- en: Glossary
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语表
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/glossary](https://huggingface.co/learn/deep-rl-course/unit4/glossary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit4/glossary](https://huggingface.co/learn/deep-rl-course/unit4/glossary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This is a community-created glossary. Contributions are welcome!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由社区创建的术语表。欢迎贡献！
- en: '**Deep Q-Learning:** A value-based deep reinforcement learning algorithm that
    uses a deep neural network to approximate Q-values for actions in a given state.
    The goal of Deep Q-learning is to find the optimal policy that maximizes the expected
    cumulative reward by learning the action-values.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度 Q 学习：** 一种基于值的深度强化学习算法，使用深度神经网络来逼近给定状态下的动作的 Q 值。深度 Q 学习的目标是通过学习动作值来找到最优策略，从而最大化期望的累积奖励。'
- en: '**Value-based methods:** Reinforcement Learning methods that estimate a value
    function as an intermediate step towards finding an optimal policy.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于价值的方法：估计值函数作为找到最优策略的中间步骤的强化学习方法。
- en: '**Policy-based methods:** Reinforcement Learning methods that directly learn
    to approximate the optimal policy without learning a value function. In practice
    they output a probability distribution over actions.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于策略的方法：直接学习逼近最优策略而不学习值函数的强化学习方法。在实践中，它们输出动作的概率分布。
- en: 'The benefits of using policy-gradient methods over value-based methods include:'
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用策略梯度方法而不是基于价值的方法的好处包括：
- en: 'simplicity of integration: no need to store action values;'
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成的简单性：无需存储动作值；
- en: 'ability to learn a stochastic policy: the agent explores the state space without
    always taking the same trajectory, and avoids the problem of perceptual aliasing;'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习随机策略的能力：代理探索状态空间而不总是采取相同的轨迹，并避免感知混淆的问题；
- en: effectiveness in high-dimensional and continuous action spaces; and
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维和连续动作空间中的有效性；以及
- en: improved convergence properties.
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的收敛性质。
- en: '**Policy Gradient:** A subset of policy-based methods where the objective is
    to maximize the performance of a parameterized policy using gradient ascent. The
    goal of a policy-gradient is to control the probability distribution of actions
    by tuning the policy such that good actions (that maximize the return) are sampled
    more frequently in the future.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略梯度：** 策略梯度方法的一个子集，其目标是通过梯度上升最大化参数化策略的性能。策略梯度的目标是通过调整策略来控制动作的概率分布，使得未来更频繁地采样好的动作（最大化回报）。'
- en: '**Monte Carlo Reinforce:** A policy-gradient algorithm that uses an estimated
    return from an entire episode to update the policy parameter.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒙特卡洛强化学习：** 一种策略梯度算法，使用整个回合的估计回报来更新策略参数。'
- en: If you want to improve the course, you can [open a Pull Request.](https://github.com/huggingface/deep-rl-class/pulls)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想改进课程，可以[提交拉取请求。](https://github.com/huggingface/deep-rl-class/pulls)
- en: 'This glossary was made possible thanks to:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语表得以实现，感谢：
- en: '[Diego Carpintero](https://github.com/dcarpintero)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Diego Carpintero](https://github.com/dcarpintero)'
