- en: Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit8/introduction](https://huggingface.co/learn/deep-rl-course/unit8/introduction)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit8/introduction](https://huggingface.co/learn/deep-rl-course/unit8/introduction)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
- en: 'In Unit 6, we learned about Advantage Actor Critic (A2C), a hybrid architecture
    combining value-based and policy-based methods that helps to stabilize the training
    by reducing the variance with:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6单元，我们学习了优势演员评论家（A2C），这是一种混合架构，结合了基于价值和基于策略的方法，有助于通过减少方差来稳定训练。
- en: '*An Actor* that controls **how our agent behaves** (policy-based method).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个控制**代理行为方式**的演员（基于策略的方法）。
- en: '*A Critic* that measures **how good the action taken is** (value-based method).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个度量**采取的行动有多好**的评论家（基于价值的方法）。
- en: Today we’ll learn about Proximal Policy Optimization (PPO), an architecture
    that **improves our agent’s training stability by avoiding policy updates that
    are too large**. To do that, we use a ratio that indicates the difference between
    our current and old policy and clip this ratio to a specific range<math><semantics><mrow><mo
    stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo separator="true">,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 - \epsilon,
    1 + \epsilon]</annotation></semantics></math> [1−ϵ,1+ϵ] .
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们将学习Proximal Policy Optimization（PPO），这是一种架构，通过避免过大的策略更新来提高我们代理的训练稳定性。为此，我们使用一个比率来指示当前策略和旧策略之间的差异，并将此比率剪切到特定范围[1−ϵ,1+ϵ]。
- en: Doing this will ensure **that our policy update will not be too large and that
    the training is more stable.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做将确保我们的策略更新不会太大，并且训练更加稳定。
- en: 'This Unit is in two parts:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本单元分为两部分：
- en: In this first part, you’ll learn the theory behind PPO and code your PPO agent
    from scratch using the [CleanRL](https://github.com/vwxyzjn/cleanrl) implementation.
    To test its robustness you’ll use LunarLander-v2\. LunarLander-v2 **is the first
    environment you used when you started this course**. At that time, you didn’t
    know how PPO worked, and now, **you can code it from scratch and train it. How
    incredible is that 🤩**.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这第一部分中，您将学习PPO背后的理论，并使用[CleanRL](https://github.com/vwxyzjn/cleanrl)实现从头开始编写您的PPO代理。为了测试其稳健性，您将使用LunarLander-v2。LunarLander-v2是您开始本课程时使用的第一个环境。那时，您不知道PPO是如何工作的，现在您可以从头开始编码并训练它。这是多么令人难以置信的事情🤩。
- en: In the second part, we’ll get deeper into PPO optimization by using [Sample-Factory](https://samplefactory.dev/)
    and train an agent playing vizdoom (an open source version of Doom).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二部分中，我们将通过使用[Sample-Factory](https://samplefactory.dev/)深入了解PPO优化，并训练一个玩vizdoom（Doom的开源版本）的代理。
- en: '![Environment](../Images/3244d92e568ba653445e92e451579990.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![环境](../Images/3244d92e568ba653445e92e451579990.png)'
- en: 'These are the environments you''re going to use to train your agents: VizDoom
    environments'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是您将用来训练代理的环境：VizDoom环境
- en: Sound exciting? Let’s get started! 🚀
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很激动人心吗？让我们开始吧！🚀
