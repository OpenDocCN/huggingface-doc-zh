- en: Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit8/introduction](https://huggingface.co/learn/deep-rl-course/unit8/introduction)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/learn/deep-rl-course/unit8/introduction](https://huggingface.co/learn/deep-rl-course/unit8/introduction)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
- en: 'In Unit 6, we learned about Advantage Actor Critic (A2C), a hybrid architecture
    combining value-based and policy-based methods that helps to stabilize the training
    by reducing the variance with:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬6å•å…ƒï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå®¶ï¼ˆA2Cï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œç»“åˆäº†åŸºäºä»·å€¼å’ŒåŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œæœ‰åŠ©äºé€šè¿‡å‡å°‘æ–¹å·®æ¥ç¨³å®šè®­ç»ƒã€‚
- en: '*An Actor* that controls **how our agent behaves** (policy-based method).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ§åˆ¶**ä»£ç†è¡Œä¸ºæ–¹å¼**çš„æ¼”å‘˜ï¼ˆåŸºäºç­–ç•¥çš„æ–¹æ³•ï¼‰ã€‚
- en: '*A Critic* that measures **how good the action taken is** (value-based method).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåº¦é‡**é‡‡å–çš„è¡ŒåŠ¨æœ‰å¤šå¥½**çš„è¯„è®ºå®¶ï¼ˆåŸºäºä»·å€¼çš„æ–¹æ³•ï¼‰ã€‚
- en: Today weâ€™ll learn about Proximal Policy Optimization (PPO), an architecture
    that **improves our agentâ€™s training stability by avoiding policy updates that
    are too large**. To do that, we use a ratio that indicates the difference between
    our current and old policy and clip this ratio to a specific range<math><semantics><mrow><mo
    stretchy="false">[</mo><mn>1</mn><mo>âˆ’</mo><mi>Ïµ</mi><mo separator="true">,</mo><mn>1</mn><mo>+</mo><mi>Ïµ</mi><mo
    stretchy="false">]</mo></mrow> <annotation encoding="application/x-tex">[1 - \epsilon,
    1 + \epsilon]</annotation></semantics></math> [1âˆ’Ïµ,1+Ïµ] .
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘ä»¬å°†å­¦ä¹ Proximal Policy Optimizationï¼ˆPPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¶æ„ï¼Œé€šè¿‡é¿å…è¿‡å¤§çš„ç­–ç•¥æ›´æ–°æ¥æé«˜æˆ‘ä»¬ä»£ç†çš„è®­ç»ƒç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ¯”ç‡æ¥æŒ‡ç¤ºå½“å‰ç­–ç•¥å’Œæ—§ç­–ç•¥ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å°†æ­¤æ¯”ç‡å‰ªåˆ‡åˆ°ç‰¹å®šèŒƒå›´[1âˆ’Ïµ,1+Ïµ]ã€‚
- en: Doing this will ensure **that our policy update will not be too large and that
    the training is more stable.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšå°†ç¡®ä¿æˆ‘ä»¬çš„ç­–ç•¥æ›´æ–°ä¸ä¼šå¤ªå¤§ï¼Œå¹¶ä¸”è®­ç»ƒæ›´åŠ ç¨³å®šã€‚
- en: 'This Unit is in two parts:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å•å…ƒåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š
- en: In this first part, youâ€™ll learn the theory behind PPO and code your PPO agent
    from scratch using the [CleanRL](https://github.com/vwxyzjn/cleanrl) implementation.
    To test its robustness youâ€™ll use LunarLander-v2\. LunarLander-v2 **is the first
    environment you used when you started this course**. At that time, you didnâ€™t
    know how PPO worked, and now, **you can code it from scratch and train it. How
    incredible is that ğŸ¤©**.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¬¬ä¸€éƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†å­¦ä¹ PPOèƒŒåçš„ç†è®ºï¼Œå¹¶ä½¿ç”¨[CleanRL](https://github.com/vwxyzjn/cleanrl)å®ç°ä»å¤´å¼€å§‹ç¼–å†™æ‚¨çš„PPOä»£ç†ã€‚ä¸ºäº†æµ‹è¯•å…¶ç¨³å¥æ€§ï¼Œæ‚¨å°†ä½¿ç”¨LunarLander-v2ã€‚LunarLander-v2æ˜¯æ‚¨å¼€å§‹æœ¬è¯¾ç¨‹æ—¶ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªç¯å¢ƒã€‚é‚£æ—¶ï¼Œæ‚¨ä¸çŸ¥é“PPOæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œç°åœ¨æ‚¨å¯ä»¥ä»å¤´å¼€å§‹ç¼–ç å¹¶è®­ç»ƒå®ƒã€‚è¿™æ˜¯å¤šä¹ˆä»¤äººéš¾ä»¥ç½®ä¿¡çš„äº‹æƒ…ğŸ¤©ã€‚
- en: In the second part, weâ€™ll get deeper into PPO optimization by using [Sample-Factory](https://samplefactory.dev/)
    and train an agent playing vizdoom (an open source version of Doom).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨[Sample-Factory](https://samplefactory.dev/)æ·±å…¥äº†è§£PPOä¼˜åŒ–ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªç©vizdoomï¼ˆDoomçš„å¼€æºç‰ˆæœ¬ï¼‰çš„ä»£ç†ã€‚
- en: '![Environment](../Images/3244d92e568ba653445e92e451579990.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![ç¯å¢ƒ](../Images/3244d92e568ba653445e92e451579990.png)'
- en: 'These are the environments you''re going to use to train your agents: VizDoom
    environments'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æ‚¨å°†ç”¨æ¥è®­ç»ƒä»£ç†çš„ç¯å¢ƒï¼šVizDoomç¯å¢ƒ
- en: Sound exciting? Letâ€™s get started! ğŸš€
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥å¾ˆæ¿€åŠ¨äººå¿ƒå—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å§ï¼ğŸš€
