- en: Hybrid Vision Transformer (ViT Hybrid)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合视觉Transformer（ViT Hybrid）
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_hybrid](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_hybrid)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_hybrid](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_hybrid)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth
    16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
    by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
    Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. It’s the first paper that successfully
    trains a Transformer encoder on ImageNet, attaining very good results compared
    to familiar convolutional architectures. ViT hybrid is a slight variant of the
    [plain Vision Transformer](vit), by leveraging a convolutional backbone (specifically,
    [BiT](bit)) whose features are used as initial “tokens” for the Transformer.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '混合视觉Transformer（ViT）模型是由Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby在[An Image
    is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)中提出的。这是第一篇成功在ImageNet上训练Transformer编码器的论文，与熟悉的卷积架构相比取得了非常好的结果。ViT混合是[plain
    Vision Transformer](vit)的一个轻微变体，通过利用卷积骨干（具体来说是[BiT](bit)）的特征作为Transformer的初始“标记”。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*While the Transformer architecture has become the de-facto standard for natural
    language processing tasks, its applications to computer vision remain limited.
    In vision, attention is either applied in conjunction with convolutional networks,
    or used to replace certain components of convolutional networks while keeping
    their overall structure in place. We show that this reliance on CNNs is not necessary
    and a pure transformer applied directly to sequences of image patches can perform
    very well on image classification tasks. When pre-trained on large amounts of
    data and transferred to multiple mid-sized or small image recognition benchmarks
    (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent
    results compared to state-of-the-art convolutional networks while requiring substantially
    fewer computational resources to train.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉中，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其整体结构不变。我们表明，在图像分类任务中，这种对CNN的依赖是不必要的，直接应用于图像块序列的纯Transformer可以在大量数据上进行预训练，并转移到多个中等或小型图像识别基准（ImageNet、CIFAR-100、VTAB等）时，Vision
    Transformer（ViT）可以取得与最先进的卷积网络相比非常好的结果，同时需要较少的计算资源来训练。*'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code (written in JAX) can be found [here](https://github.com/google-research/vision_transformer).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。原始代码（使用JAX编写）可以在[这里](https://github.com/google-research/vision_transformer)找到。
- en: Resources
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with ViT Hybrid.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用ViT Hybrid。
- en: Image Classification
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[ViTHybridForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ViTHybridForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification)
    是由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    和 [笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)
    支持的。'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[图像分类任务指南](../tasks/image_classification)
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将对其进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。
- en: ViTHybridConfig
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViTHybridConfig
- en: '### `class transformers.ViTHybridConfig`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViTHybridConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/configuration_vit_hybrid.py#L32)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/configuration_vit_hybrid.py#L32)'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`backbone_config` (`Union[Dict[str, Any], PretrainedConfig]`, *optional*) —
    The configuration of the backbone in a dictionary or the config object of the
    backbone.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` (`Union[Dict[str, Any], PretrainedConfig]`, *可选*) — 骨干的配置，可以是字典或骨干的配置对象。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *可选*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *可选*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — 嵌入、编码器和池化器中所有全连接层的丢失概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢失比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。'
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。'
- en: '`patch_size` (`int`, *optional*, defaults to 1) — The size (resolution) of
    each patch.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 1) — 每个补丁的大小（分辨率）。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) — 输入通道的数量。'
- en: '`backbone_featmap_shape` (`List[int]`, *optional*, defaults to `[1, 1024, 24,
    24]`) — Used only for the `hybrid` embedding type. The shape of the feature maps
    of the backbone.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_featmap_shape` (`List[int]`, *optional*, defaults to `[1, 1024, 24,
    24]`) — 仅用于`hybrid`嵌入类型。主干特征图的形状。'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。'
- en: This is the configuration class to store the configuration of a [ViTHybridModel](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel).
    It is used to instantiate a ViT Hybrid model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ViT Hybrid [google/vit-hybrid-base-bit-384](https://huggingface.co/google/vit-hybrid-base-bit-384)
    architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[ViTHybridModel](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel)的配置。根据指定的参数实例化一个ViT
    Hybrid模型，定义模型架构。使用默认值实例化配置将产生类似于ViT Hybrid [google/vit-hybrid-base-bit-384](https://huggingface.co/google/vit-hybrid-base-bit-384)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ViTHybridImageProcessor
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViTHybridImageProcessor
- en: '### `class transformers.ViTHybridImageProcessor`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViTHybridImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/image_processing_vit_hybrid.py#L50)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/image_processing_vit_hybrid.py#L50)'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by `do_resize` in the `preprocess` method.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, defaults to `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 224}`):
    Size of the image after resizing. The shortest edge of the image is resized to
    size[“shortest_edge”], with the longest edge resized to keep the input aspect
    ratio. Can be overridden by `size` in the `preprocess` method.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 224}`):
    调整大小后的图像尺寸。图像的最短边被调整为size[“shortest_edge”]，最长边被调整以保持输入的长宽比。可以被`preprocess`方法中的`size`覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`)
    — Resampling filter to use if resizing the image. Can be overridden by `resample`
    in the `preprocess` method.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`)
    — 如果调整图像大小，要使用的重采样滤波器。可以被`preprocess`方法中的`resample`覆盖。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image to the specified `crop_size`. Can be overridden by `do_center_crop`
    in the `preprocess` method.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — 是否将图像居中裁剪到指定的`crop_size`。可以被`preprocess`方法中的`do_center_crop`覆盖。'
- en: '`crop_size` (`Dict[str, int]` *optional*, defaults to 224) — Size of the output
    image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`
    method.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]` *optional*, defaults to 224) — 应用`center_crop`后的输出图像大小。可以被`preprocess`方法中的`crop_size`覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale`
    in the `preprocess` method.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, defaults to `True`) — 是否按指定比例`rescale_factor`重新缩放图像。可以被`preprocess`方法中的`do_rescale`覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by `rescale_factor` in
    the `preprocess` method. do_normalize — Whether to normalize the image. Can be
    overridden by `do_normalize` in the `preprocess` method.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — 如果重新调整图像，则使用的比例因子。可以被`preprocess`方法中的`rescale_factor`覆盖。do_normalize
    — 是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — 如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_mean`参数覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method. Can be overridden by
    the `image_std` parameter in the `preprocess` method.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_STD`) —
    如果对图像进行归一化，则使用的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过 `preprocess` 方法中的 `image_std`
    参数覆盖。可以通过 `preprocess` 方法中的 `image_std` 参数覆盖。'
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `True`) — Whether to convert
    the image to RGB.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_convert_rgb` (`bool`, *可选*, 默认为 `True`) — 是否将图像转换为 RGB。'
- en: Constructs a ViT Hybrid image processor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 ViT Hybrid 图像处理器。
- en: '#### `preprocess`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/image_processing_vit_hybrid.py#L174)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/image_processing_vit_hybrid.py#L174)'
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个图像或图像批处理，像素值范围为 0 到 255。如果传入像素值在 0 到
    1 之间的图像，请设置 `do_rescale=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after resizing. Shortest edge of the image is resized to size[“shortest_edge”],
    with the longest edge resized to keep the input aspect ratio.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 调整大小后的图像尺寸。图像的最短边被调整为 size[“shortest_edge”]，最长边被调整以保持输入的长宽比。'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) — Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`.
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *可选*, 默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。这可以是枚举
    `PILImageResampling` 之一。仅在 `do_resize` 设置为 `True` 时生效。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *可选*, 默认为 `self.do_center_crop`) — 是否对图像进行中心裁剪。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`, *可选*, 默认为 `self.crop_size`) — 中心裁剪的尺寸。仅在 `do_center_crop`
    设置为 `True` 时生效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否重新缩放图像。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 如果 `do_rescale`
    设置为 `True`，则对图像进行重新缩放的缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to use for normalization. Only has an effect if `do_normalize` is
    set to `True`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_mean`) — 用于归一化的图像均值。仅在
    `do_normalize` 设置为 `True` 时生效。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to use for normalization. Only has an effect if `do_normalize`
    is set to `True`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_std`) — 用于归一化的图像标准差。仅在
    `do_normalize` 设置为 `True` 时生效。'
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `self.do_convert_rgb`) —
    Whether to convert the image to RGB.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_convert_rgb` (`bool`, *可选*, 默认为 `self.do_convert_rgb`) — 是否将图像转换为 RGB。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：返回一个 `np.ndarray` 列表。
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`：返回一个 `tf.Tensor` 类型的批处理。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`：返回一个 `torch.Tensor` 类型的批处理。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`：返回一个 `np.ndarray` 类型的批处理。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`：返回一个 `jax.numpy.ndarray` 类型的批处理。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。'
- en: 'Unset: defaults to the channel dimension format of the input image.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：默认为输入图像的通道维度格式。
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`: 图像以 (高度, 宽度) 格式。'
- en: Preprocess an image or batch of images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或图像批处理。
- en: ViTHybridModel
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViTHybridModel
- en: '### `class transformers.ViTHybridModel`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViTHybridModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L518)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L518)'
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig)）-
    模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare ViT Hybrid Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的ViT混合模型变压器输出原始隐藏状态，没有特定的顶部头。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L548)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L548)'
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTHybridImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）-
    像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[ViTHybridImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部无效的掩码。在`[0, 1]`中选择的掩码值：'
- en: 1 indicates the head is `not masked`,
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被掩盖。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`,
    *optional*) — Boolean masked positions. Indicates which patches are masked (1)
    and which aren’t (0).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bool_masked_pos`（形状为`(batch_size, num_patches)`的`torch.BoolTensor`，*可选*）-
    布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。'
- en: Returns
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig))
    and inputs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包括根据配置([ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig))和输入的各种元素。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的输出的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 序列第一个标记（分类标记）的最后一层隐藏状态（经过用于辅助预训练任务的层进一步处理后）。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层权重是从预训练期间的下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出一个，+
    每一层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [ViTHybridModel](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel)
    forward method, overrides the `__call__` special method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViTHybridModel](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ViTHybridForImageClassification
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViTHybridForImageClassification
- en: '### `class transformers.ViTHybridForImageClassification`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViTHybridForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L634)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L634)'
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: ViT Hybrid Model transformer with an image classification head on top (a linear
    layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ViT混合模型变压器，顶部带有图像分类头（在[CLS]标记的最终隐藏状态之上的线性层），例如用于ImageNet。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L655)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L655)'
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTHybridImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ViTHybridImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块中选定头部失效的掩码。掩码值选定在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig))
    and inputs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`, *optional*, 当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+
    一个用于每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中使用的注意力softmax之后的注意力权重，用于计算加权平均值。
- en: The [ViTHybridForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViTHybridForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
