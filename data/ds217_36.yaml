- en: Create an image dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºå›¾åƒæ•°æ®é›†
- en: 'Original text: [https://huggingface.co/docs/datasets/image_dataset](https://huggingface.co/docs/datasets/image_dataset)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/image_dataset](https://huggingface.co/docs/datasets/image_dataset)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two methods for creating and sharing an image dataset. This guide
    will show you how to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åˆ›å»ºå’Œå…±äº«å›¾åƒæ•°æ®é›†ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Create an image dataset with `ImageFolder` and some metadata. This is a no-code
    solution for quickly creating an image dataset with several thousand images.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`ImageFolder`å’Œä¸€äº›å…ƒæ•°æ®åˆ›å»ºå›¾åƒæ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ä¸ªæ— ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¯å¿«é€Ÿåˆ›å»ºåŒ…å«æ•°åƒå¼ å›¾åƒçš„å›¾åƒæ•°æ®é›†ã€‚
- en: Create an image dataset by writing a loading script. This method is a bit more
    involved, but you have greater flexibility over how a dataset is defined, downloaded,
    and generated which can be useful for more complex or large scale image datasets.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ç¼–å†™åŠ è½½è„šæœ¬åˆ›å»ºå›¾åƒæ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•ç¨å¾®å¤æ‚ä¸€äº›ï¼Œä½†æ‚¨å¯ä»¥æ›´çµæ´»åœ°å®šä¹‰ã€ä¸‹è½½å’Œç”Ÿæˆæ•°æ®é›†ï¼Œè¿™å¯¹äºæ›´å¤æ‚æˆ–å¤§è§„æ¨¡çš„å›¾åƒæ•°æ®é›†å¯èƒ½å¾ˆæœ‰ç”¨ã€‚
- en: You can control access to your dataset by requiring users to share their contact
    information first. Check out the [Gated datasets](https://huggingface.co/docs/hub/datasets-gated)
    guide for more information about how to enable this feature on the Hub.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡è¦æ±‚ç”¨æˆ·é¦–å…ˆå…±äº«å…¶è”ç³»ä¿¡æ¯æ¥æ§åˆ¶å¯¹æ•°æ®é›†çš„è®¿é—®ã€‚æŸ¥çœ‹æœ‰å…³å¦‚ä½•åœ¨Hubä¸Šå¯ç”¨æ­¤åŠŸèƒ½çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[Gated datasets](https://huggingface.co/docs/hub/datasets-gated)æŒ‡å—ã€‚
- en: ImageFolder
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageFolder
- en: The `ImageFolder` is a dataset builder designed to quickly load an image dataset
    with several thousand images without requiring you to write any code.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageFolder`æ˜¯ä¸€ä¸ªæ•°æ®é›†æ„å»ºå™¨ï¼Œæ—¨åœ¨å¿«é€ŸåŠ è½½åŒ…å«æ•°åƒå¼ å›¾åƒçš„å›¾åƒæ•°æ®é›†ï¼Œè€Œæ— éœ€ç¼–å†™ä»»ä½•ä»£ç ã€‚'
- en: ğŸ’¡ Take a look at the [Split pattern hierarchy](repository_structure#split-pattern-hierarchy)
    to learn more about how `ImageFolder` creates dataset splits based on your dataset
    repository structure.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡æŸ¥çœ‹[Split pattern hierarchy](repository_structure#split-pattern-hierarchy)ä»¥äº†è§£æœ‰å…³`ImageFolder`å¦‚ä½•åŸºäºæ•°æ®é›†å­˜å‚¨åº“ç»“æ„åˆ›å»ºæ•°æ®é›†æ‹†åˆ†çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '`ImageFolder` automatically infers the class labels of your dataset based on
    the directory name. Store your dataset in a directory structure like:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageFolder`ä¼šæ ¹æ®ç›®å½•åç§°è‡ªåŠ¨æ¨æ–­æ•°æ®é›†çš„ç±»æ ‡ç­¾ã€‚å°†æ•°æ®é›†å­˜å‚¨åœ¨ä»¥ä¸‹ç»“æ„çš„ç›®å½•ä¸­ï¼š'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then users can load your dataset by specifying `imagefolder` in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    and the directory in `data_dir`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åç”¨æˆ·å¯ä»¥é€šè¿‡åœ¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)ä¸­æŒ‡å®š`imagefolder`å’Œ`data_dir`ä¸­çš„ç›®å½•æ¥åŠ è½½æ•°æ®é›†ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can also use `imagefolder` to load datasets involving multiple splits.
    To do so, your dataset directory should have the following structure:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`imagefolder`åŠ è½½æ¶‰åŠå¤šä¸ªæ‹†åˆ†çš„æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæ‚¨çš„æ•°æ®é›†ç›®å½•åº”å…·æœ‰ä»¥ä¸‹ç»“æ„ï¼š
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If all image files are contained in a single directory or if they are not on
    the same level of directory structure, `label` column wonâ€™t be added automatically.
    If you need it, set `drop_labels=False` explicitly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰€æœ‰å›¾åƒæ–‡ä»¶éƒ½åŒ…å«åœ¨å•ä¸ªç›®å½•ä¸­ï¼Œæˆ–è€…å®ƒä»¬ä¸åœ¨ç›¸åŒçº§åˆ«çš„ç›®å½•ç»“æ„ä¸­ï¼Œ`label`åˆ—å°†ä¸ä¼šè‡ªåŠ¨æ·»åŠ ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æ˜¾å¼è®¾ç½®`drop_labels=False`ã€‚
- en: If there is additional information youâ€™d like to include about your dataset,
    like text captions or bounding boxes, add it as a `metadata.csv` file in your
    folder. This lets you quickly create datasets for different computer vision tasks
    like text captioning or object detection. You can also use a JSONL file `metadata.jsonl`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦åŒ…å«æœ‰å…³æ•°æ®é›†çš„å…¶ä»–ä¿¡æ¯ï¼Œå¦‚æ–‡æœ¬æ ‡é¢˜æˆ–è¾¹ç•Œæ¡†ï¼Œè¯·å°†å…¶æ·»åŠ ä¸º`metadata.csv`æ–‡ä»¶æ”¾åœ¨æ‚¨çš„æ–‡ä»¶å¤¹ä¸­ã€‚è¿™æ ·å¯ä»¥å¿«é€Ÿä¸ºä¸åŒçš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡åˆ›å»ºæ•°æ®é›†ï¼Œå¦‚æ–‡æœ¬æ ‡é¢˜æˆ–ç›®æ ‡æ£€æµ‹ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨JSONLæ–‡ä»¶`metadata.jsonl`ã€‚
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also zip your images:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å‹ç¼©æ‚¨çš„å›¾åƒï¼š
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Your `metadata.csv` file must have a `file_name` column which links image files
    with their metadata:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„`metadata.csv`æ–‡ä»¶å¿…é¡»å…·æœ‰ä¸€ä¸ª`file_name`åˆ—ï¼Œå°†å›¾åƒæ–‡ä»¶ä¸å…¶å…ƒæ•°æ®é“¾æ¥èµ·æ¥ï¼š
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'or using `metadata.jsonl`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä½¿ç”¨`metadata.jsonl`ï¼š
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If metadata files are present, the inferred labels based on the directory name
    are dropped by default. To include those labels, set `drop_labels=False` in `load_dataset`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå­˜åœ¨å…ƒæ•°æ®æ–‡ä»¶ï¼Œåˆ™é»˜è®¤æƒ…å†µä¸‹åŸºäºç›®å½•åç§°æ¨æ–­çš„æ ‡ç­¾å°†è¢«åˆ é™¤ã€‚è¦åŒ…å«è¿™äº›æ ‡ç­¾ï¼Œè¯·åœ¨`load_dataset`ä¸­è®¾ç½®`drop_labels=False`ã€‚
- en: Image captioning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å›¾åƒæ ‡é¢˜
- en: 'Image captioning datasets have text describing an image. An example `metadata.csv`
    may look like:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒæ ‡é¢˜æ•°æ®é›†åŒ…å«æè¿°å›¾åƒçš„æ–‡æœ¬ã€‚ä¸€ä¸ªç¤ºä¾‹`metadata.csv`å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load the dataset with `ImageFolder`, and it will create a `text` column for
    the image captions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`ImageFolder`åŠ è½½æ•°æ®é›†ï¼Œå®ƒå°†ä¸ºå›¾åƒæ ‡é¢˜åˆ›å»ºä¸€ä¸ª`text`åˆ—ï¼š
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Object detection
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹
- en: 'Object detection datasets have bounding boxes and categories identifying objects
    in an image. An example `metadata.jsonl` may look like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹æ•°æ®é›†å…·æœ‰è¾¹ç•Œæ¡†å’Œç±»åˆ«ï¼Œç”¨äºè¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ã€‚ä¸€ä¸ªç¤ºä¾‹`metadata.jsonl`å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Load the dataset with `ImageFolder`, and it will create a `objects` column
    with the bounding boxes and the categories:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`ImageFolder`åŠ è½½æ•°æ®é›†ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªå¸¦æœ‰è¾¹ç•Œæ¡†å’Œç±»åˆ«çš„`objects`åˆ—ï¼š
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Upload dataset to the Hub
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hub
- en: Once youâ€™ve created a dataset, you can share it to the Hub with the [push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub)
    method. Make sure you have the [huggingface_hub](https://huggingface.co/docs/huggingface_hub/index)
    library installed and youâ€™re logged in to your Hugging Face account (see the [Upload
    with Python tutorial](upload_dataset#upload-with-python) for more details).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ•°æ®é›†åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub)æ–¹æ³•å°†å…¶å…±äº«åˆ°Hubã€‚ç¡®ä¿æ‚¨å·²å®‰è£…[huggingface_hub](https://huggingface.co/docs/huggingface_hub/index)åº“ï¼Œå¹¶ä¸”å·²ç™»å½•åˆ°æ‚¨çš„Hugging
    Faceå¸æˆ·ï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ä½¿ç”¨Pythonä¸Šä¼ æ•™ç¨‹](upload_dataset#upload-with-python)ï¼‰ã€‚
- en: 'Upload your dataset with [push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub)ä¸Šä¼ æ‚¨çš„æ•°æ®é›†ï¼š
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: WebDataset
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WebDataset
- en: 'The [WebDataset](https://github.com/webdataset/webdataset) format is based
    on TAR archives and is suitable for big image datasets. Indeed you can group your
    images in TAR archives (e.g. 1GB of images per TAR archive) and have thousands
    of TAR archives:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[WebDataset](https://github.com/webdataset/webdataset)æ ¼å¼åŸºäºTARå­˜æ¡£ï¼Œé€‚ç”¨äºå¤§å‹å›¾åƒæ•°æ®é›†ã€‚å®é™…ä¸Šï¼Œæ‚¨å¯ä»¥å°†å›¾åƒåˆ†ç»„åœ¨TARå­˜æ¡£ä¸­ï¼ˆä¾‹å¦‚æ¯ä¸ªTARå­˜æ¡£1GBçš„å›¾åƒï¼‰å¹¶æ‹¥æœ‰æ•°åƒä¸ªTARå­˜æ¡£ï¼š'
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the archives, each example is made of files sharing the same prefix:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å­˜æ¡£ä¸­ï¼Œæ¯ä¸ªç¤ºä¾‹ç”±å…±äº«ç›¸åŒå‰ç¼€çš„æ–‡ä»¶ç»„æˆï¼š
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can put your images labels/captions/bounding boxes using JSON or text files
    for example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨JSONæˆ–æ–‡æœ¬æ–‡ä»¶æ”¾ç½®å›¾åƒæ ‡ç­¾/æ ‡é¢˜/è¾¹ç•Œæ¡†ã€‚
- en: For more details on the WebDataset format and the python library, please check
    the [WebDataset documentation](https://webdataset.github.io/webdataset).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³WebDatasetæ ¼å¼å’ŒPythonåº“çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[WebDatasetæ–‡æ¡£](https://webdataset.github.io/webdataset)ã€‚
- en: 'Load your WebDataset and it will create on column per file suffix (here â€œjpgâ€
    and â€œjsonâ€):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ‚¨çš„WebDatasetï¼Œå®ƒå°†ä¸ºæ¯ä¸ªæ–‡ä»¶åç¼€åˆ›å»ºä¸€åˆ—ï¼ˆè¿™é‡Œæ˜¯â€œjpgâ€å’Œâ€œjsonâ€ï¼‰ï¼š
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Loading script
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½è„šæœ¬
- en: Write a dataset loading script to share a dataset. It defines a datasetâ€™s splits
    and configurations, and handles downloading and generating a dataset. The script
    is located in the same folder or repository as the dataset and should have the
    same name.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–å†™æ•°æ®é›†åŠ è½½è„šæœ¬ä»¥å…±äº«æ•°æ®é›†ã€‚å®ƒå®šä¹‰äº†æ•°æ®é›†çš„æ‹†åˆ†å’Œé…ç½®ï¼Œå¹¶å¤„ç†æ•°æ®é›†çš„ä¸‹è½½å’Œç”Ÿæˆã€‚è„šæœ¬ä½äºä¸æ•°æ®é›†ç›¸åŒçš„æ–‡ä»¶å¤¹æˆ–å­˜å‚¨åº“ä¸­ï¼Œå¹¶ä¸”åº”å…·æœ‰ç›¸åŒçš„åç§°ã€‚
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This structure allows your dataset to be loaded in one line:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç»“æ„å…è®¸æ‚¨çš„æ•°æ®é›†åœ¨ä¸€è¡Œä¸­åŠ è½½ï¼š
- en: '[PRE16]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This guide will show you how to create a dataset loading script for image datasets,
    which is a bit different from [creating a loading script for text datasets](./dataset_script).
    Youâ€™ll learn how to:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸ºå›¾åƒæ•°æ®é›†åˆ›å»ºæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œè¿™ä¸[ä¸ºæ–‡æœ¬æ•°æ®é›†åˆ›å»ºåŠ è½½è„šæœ¬](./dataset_script)æœ‰äº›ä¸åŒã€‚æ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š
- en: Create a dataset builder class.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ•°æ®é›†æ„å»ºå™¨ç±»ã€‚
- en: Create dataset configurations.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ•°æ®é›†é…ç½®ã€‚
- en: Add dataset metadata.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®ã€‚
- en: Download and define the dataset splits.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†æ‹†åˆ†ã€‚
- en: Generate the dataset.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ•°æ®é›†ã€‚
- en: Generate the dataset metadata (optional).
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰ã€‚
- en: Upload the dataset to the Hub.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hubã€‚
- en: The best way to learn is to open up an existing image dataset loading script,
    like [Food-101](https://huggingface.co/datasets/food101/blob/main/food101.py),
    and follow along!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰“å¼€ç°æœ‰çš„å›¾åƒæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œä¾‹å¦‚[Food-101](https://huggingface.co/datasets/food101/blob/main/food101.py)ï¼Œå¹¶è·Ÿéšæ“ä½œï¼
- en: To help you get started, we created a loading script [template](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)
    you can copy and use as a starting point!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŠ è½½è„šæœ¬[æ¨¡æ¿](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)ï¼Œæ‚¨å¯ä»¥å¤åˆ¶å¹¶ç”¨ä½œèµ·ç‚¹ï¼
- en: Create a dataset builder class
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ•°æ®é›†æ„å»ºå™¨ç±»
- en: '[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)
    is the base class for datasets generated from a dictionary generator. Within this
    class, there are three methods to help create your dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)
    æ˜¯ä»å­—å…¸ç”Ÿæˆçš„æ•°æ®é›†çš„åŸºç±»ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å¸®åŠ©åˆ›å»ºæ‚¨çš„æ•°æ®é›†ï¼š'
- en: '`info` stores information about your dataset like its description, license,
    and features.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info` å­˜å‚¨æœ‰å…³æ•°æ®é›†çš„ä¿¡æ¯ï¼Œå¦‚æè¿°ã€è®¸å¯è¯å’Œç‰¹å¾ã€‚'
- en: '`split_generators` downloads the dataset and defines its splits.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_generators` ä¸‹è½½æ•°æ®é›†å¹¶å®šä¹‰å…¶æ‹†åˆ†ã€‚'
- en: '`generate_examples` generates the images and labels for each split.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_examples` ä¸ºæ¯ä¸ªæ‹†åˆ†ç”Ÿæˆå›¾åƒå’Œæ ‡ç­¾ã€‚'
- en: 'Start by creating your dataset class as a subclass of [GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)
    and add the three methods. Donâ€™t worry about filling in each of these methods
    yet, youâ€™ll develop those over the next few sections:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆå°†æ‚¨çš„æ•°æ®é›†ç±»åˆ›å»ºä¸º[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)çš„å­ç±»ï¼Œå¹¶æ·»åŠ è¿™ä¸‰ç§æ–¹æ³•ã€‚æš‚æ—¶ä¸ç”¨æ‹…å¿ƒå¡«å†™è¿™äº›æ–¹æ³•ä¸­çš„æ¯ä¸€ä¸ªï¼Œæ‚¨å°†åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªéƒ¨åˆ†ä¸­å¼€å‘è¿™äº›æ–¹æ³•ï¼š
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Multiple configurations
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¤šä¸ªé…ç½®
- en: In some cases, a dataset may have more than one configuration. For example,
    if you check out the [Imagenette dataset](https://huggingface.co/datasets/frgfm/imagenette),
    youâ€™ll notice there are three subsets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ•°æ®é›†å¯èƒ½æœ‰å¤šä¸ªé…ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æŸ¥çœ‹[Imagenetteæ•°æ®é›†](https://huggingface.co/datasets/frgfm/imagenette)ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°æœ‰ä¸‰ä¸ªå­é›†ã€‚
- en: 'To create different configurations, use the [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    class to create a subclass for your dataset. Provide the links to download the
    images and labels in `data_url` and `metadata_urls`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åˆ›å»ºä¸åŒçš„é…ç½®ï¼Œè¯·ä½¿ç”¨[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)ç±»ä¸ºæ‚¨çš„æ•°æ®é›†åˆ›å»ºä¸€ä¸ªå­ç±»ã€‚åœ¨`data_url`å’Œ`metadata_urls`ä¸­æä¾›ä¸‹è½½å›¾åƒå’Œæ ‡ç­¾çš„é“¾æ¥ï¼š
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now you can define your subsets at the top of [GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder).
    Imagine you want to create two subsets in the Food-101 dataset based on whether
    it is a breakfast or dinner food.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥åœ¨[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)çš„é¡¶éƒ¨å®šä¹‰æ‚¨çš„å­é›†ã€‚å‡è®¾æ‚¨æƒ³åœ¨Food-101æ•°æ®é›†ä¸­åŸºäºæ—©é¤æˆ–æ™šé¤é£Ÿç‰©åˆ›å»ºä¸¤ä¸ªå­é›†ã€‚
- en: Define your subsets with `Food101Config` in a list in `BUILDER_CONFIGS`.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`Food101Config`åœ¨`BUILDER_CONFIGS`åˆ—è¡¨ä¸­å®šä¹‰æ‚¨çš„å­é›†ã€‚
- en: For each configuration, provide a name, description, and where to download the
    images and labels from.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªé…ç½®ï¼Œæä¾›åç§°ã€æè¿°ä»¥åŠä»å“ªé‡Œä¸‹è½½å›¾åƒå’Œæ ‡ç­¾ã€‚
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now if users want to load the `breakfast` configuration, they can use the configuration
    name:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœç”¨æˆ·æƒ³è¦åŠ è½½`breakfast`é…ç½®ï¼Œä»–ä»¬å¯ä»¥ä½¿ç”¨é…ç½®åç§°ï¼š
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Add dataset metadata
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®
- en: 'Adding information about your dataset is useful for users to learn more about
    it. This information is stored in the [DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    class which is returned by the `info` method. Users can access this information
    by:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ æœ‰å…³æ•°æ®é›†çš„ä¿¡æ¯å¯¹äºç”¨æˆ·äº†è§£æ›´å¤šä¿¡æ¯å¾ˆæœ‰ç”¨ã€‚è¿™äº›ä¿¡æ¯å­˜å‚¨åœ¨[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)ç±»ä¸­ï¼Œè¯¥ç±»ç”±`info`æ–¹æ³•è¿”å›ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®è¿™äº›ä¿¡æ¯ï¼š
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'There is a lot of information you can specify about your dataset, but some
    important ones to include are:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æŒ‡å®šæœ‰å…³æ•°æ®é›†çš„å¤§é‡ä¿¡æ¯ï¼Œä½†ä¸€äº›é‡è¦çš„ä¿¡æ¯åŒ…æ‹¬ï¼š
- en: '`description` provides a concise description of the dataset.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`description` æä¾›äº†æ•°æ®é›†çš„ç®€æ´æè¿°ã€‚'
- en: '`features` specify the dataset column types. Since youâ€™re creating an image
    loading script, youâ€™ll need to include the [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    feature.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`features` æŒ‡å®šæ•°æ®é›†åˆ—ç±»å‹ã€‚ç”±äºæ‚¨æ­£åœ¨åˆ›å»ºä¸€ä¸ªå›¾åƒåŠ è½½è„šæœ¬ï¼Œæ‚¨éœ€è¦åŒ…å«[Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)ç‰¹å¾ã€‚'
- en: '`supervised_keys` specify the input feature and label.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`supervised_keys` æŒ‡å®šè¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾ã€‚'
- en: '`homepage` provides a link to the dataset homepage.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`homepage` æä¾›äº†æŒ‡å‘æ•°æ®é›†ä¸»é¡µçš„é“¾æ¥ã€‚'
- en: '`citation` is a BibTeX citation of the dataset.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`citation` æ˜¯æ•°æ®é›†çš„ BibTeX å¼•ç”¨ã€‚'
- en: '`license` states the datasetâ€™s license.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`license` è¡¨æ˜æ•°æ®é›†çš„è®¸å¯è¯ã€‚'
- en: Youâ€™ll notice a lot of the dataset information is defined earlier in the loading
    script which makes it easier to read. There are also other `~Datasets.Features`
    you can input, so be sure to check out the full list for more details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¼šæ³¨æ„åˆ°å¾ˆå¤šæ•°æ®é›†ä¿¡æ¯åœ¨åŠ è½½è„šæœ¬ä¸­è¾ƒæ—©åœ°å®šä¹‰ï¼Œè¿™ä½¿å¾—é˜…è¯»æ›´åŠ å®¹æ˜“ã€‚è¿˜æœ‰å…¶ä»– `~Datasets.Features` æ‚¨å¯ä»¥è¾“å…¥ï¼Œå› æ­¤è¯·åŠ¡å¿…æŸ¥çœ‹å®Œæ•´åˆ—è¡¨ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Download and define the dataset splits
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†æ‹†åˆ†
- en: Now that youâ€™ve added some information about your dataset, the next step is
    to download the dataset and generate the splits.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»æ·»åŠ äº†ä¸€äº›å…³äºæ•°æ®é›†çš„ä¿¡æ¯ï¼Œä¸‹ä¸€æ­¥æ˜¯ä¸‹è½½æ•°æ®é›†å¹¶ç”Ÿæˆæ‹†åˆ†ã€‚
- en: 'Use the [DownloadManager.download()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download)
    method to download the dataset and any other metadata youâ€™d like to associate
    with it. This method accepts:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[DownloadManager.download()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download)æ–¹æ³•ä¸‹è½½æ•°æ®é›†å’Œä»»ä½•å…¶ä»–æ‚¨æƒ³è¦å…³è”çš„å…ƒæ•°æ®ã€‚æ­¤æ–¹æ³•æ¥å—ï¼š
- en: a name to a file inside a Hub dataset repository (in other words, the `data/`
    folder)
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†åç§°æ˜ å°„åˆ° Hub æ•°æ®é›†å­˜å‚¨åº“ä¸­çš„æ–‡ä»¶ï¼ˆæ¢å¥è¯è¯´ï¼Œ`data/` æ–‡ä»¶å¤¹ï¼‰
- en: a URL to a file hosted somewhere else
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡å‘å…¶ä»–åœ°æ–¹æ‰˜ç®¡çš„æ–‡ä»¶çš„URL
- en: a list or dictionary of file names or URLs
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡ä»¶åæˆ–URLçš„åˆ—è¡¨æˆ–å­—å…¸
- en: In the Food-101 loading script, youâ€™ll notice again the URLs are defined earlier
    in the script.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨ Food-101 åŠ è½½è„šæœ¬ä¸­ï¼Œæ‚¨ä¼šå†æ¬¡æ³¨æ„åˆ° URL åœ¨è„šæœ¬ä¸­è¾ƒæ—©åœ°å®šä¹‰äº†ã€‚
- en: 'After youâ€™ve downloaded the dataset, use the [SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)
    to organize the images and labels in each split. Name each split with a standard
    name like: `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ•°æ®é›†åï¼Œä½¿ç”¨[SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)æ¥ç»„ç»‡æ¯ä¸ªæ‹†åˆ†ä¸­çš„å›¾åƒå’Œæ ‡ç­¾ã€‚ä¸ºæ¯ä¸ªæ‹†åˆ†å‘½åä¸€ä¸ªæ ‡å‡†åç§°ï¼Œå¦‚ï¼š`Split.TRAIN`ã€`Split.TEST`
    å’Œ `SPLIT.Validation`ã€‚
- en: In the `gen_kwargs` parameter, specify the file paths to the `images` to iterate
    over and load. If necessary, you can use [DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)
    to iterate over images in TAR archives. You can also specify the associated labels
    in the `metadata_path`. The `images` and `metadata_path` are actually passed onto
    the next step where youâ€™ll actually generate the dataset.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨ `gen_kwargs` å‚æ•°ä¸­ï¼ŒæŒ‡å®šè¦è¿­ä»£å’ŒåŠ è½½çš„ `images` æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)æ¥è¿­ä»£
    TAR å­˜æ¡£ä¸­çš„å›¾åƒã€‚æ‚¨è¿˜å¯ä»¥åœ¨ `metadata_path` ä¸­æŒ‡å®šç›¸å…³æ ‡ç­¾ã€‚`images` å’Œ `metadata_path` å®é™…ä¸Šæ˜¯ä¼ é€’åˆ°ä¸‹ä¸€æ­¥çš„ï¼Œæ‚¨å°†åœ¨é‚£é‡Œå®é™…ç”Ÿæˆæ•°æ®é›†ã€‚
- en: To stream a TAR archive file, you need to use [DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)!
    The [DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)
    function does not support TAR archives in streaming mode.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æµå¼ä¼ è¾“ä¸€ä¸ª TAR å­˜æ¡£æ–‡ä»¶ï¼Œæ‚¨éœ€è¦ä½¿ç”¨[DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)ï¼[DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)å‡½æ•°ä¸æ”¯æŒæµå¼ä¼ è¾“æ¨¡å¼ä¸‹çš„
    TAR å­˜æ¡£ã€‚
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Generate the dataset
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ•°æ®é›†
- en: The last method in the [GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)
    class actually generates the images and labels in the dataset. It yields a dataset
    according to the stucture specified in `features` from the `info` method. As you
    can see, `generate_examples` accepts the `images` and `metadata_path` from the
    previous method as arguments.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)
    ç±»ä¸­çš„æœ€åä¸€ä¸ªæ–¹æ³•å®é™…ä¸Šç”Ÿæˆäº†æ•°æ®é›†ä¸­çš„å›¾åƒå’Œæ ‡ç­¾ã€‚å®ƒæ ¹æ® `info` æ–¹æ³•ä¸­æŒ‡å®šçš„ `features` ç»“æ„ç”Ÿæˆæ•°æ®é›†ã€‚æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œ`generate_examples`
    æ¥å—æ¥è‡ªå‰ä¸€ä¸ªæ–¹æ³•çš„ `images` å’Œ `metadata_path` ä½œä¸ºå‚æ•°ã€‚'
- en: To stream a TAR archive file, the `metadata_path` needs to be opened and read
    first. TAR files are accessed and yielded sequentially. This means you need to
    have the metadata information in hand first so you can yield it with its corresponding
    image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æµå¼ä¼ è¾“ä¸€ä¸ª TAR å­˜æ¡£æ–‡ä»¶ï¼Œéœ€è¦å…ˆæ‰“å¼€å¹¶è¯»å– `metadata_path`ã€‚TAR æ–‡ä»¶æ˜¯æŒ‰é¡ºåºè®¿é—®å’Œç”Ÿæˆçš„ã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦å…ˆæŒæ¡å…ƒæ•°æ®ä¿¡æ¯ï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†å…¶ä¸ç›¸åº”çš„å›¾åƒä¸€èµ·ç”Ÿæˆã€‚
- en: 'Now you can write a function for opening and loading examples from the dataset:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥æ‰“å¼€å’ŒåŠ è½½æ•°æ®é›†ä¸­çš„ç¤ºä¾‹ï¼š
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Generate the dataset metadata (optional)
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
- en: The dataset metadata can be generated and stored in the dataset card (`README.md`
    file).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å…ƒæ•°æ®å¯ä»¥ç”Ÿæˆå¹¶å­˜å‚¨åœ¨æ•°æ®é›†å¡ç‰‡ï¼ˆ`README.md`æ–‡ä»¶ï¼‰ä¸­ã€‚
- en: 'Run the following command to generate your dataset metadata in `README.md`
    and make sure your new loading script works correctly:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œä»¥ä¸‹å‘½ä»¤åœ¨ `README.md` ä¸­ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ï¼Œå¹¶ç¡®ä¿æ‚¨çš„æ–°åŠ è½½è„šæœ¬èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼š
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If your loading script passed the test, you should now have the `dataset_info`
    YAML fields in the header of the `README.md` file in your dataset folder.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„åŠ è½½è„šæœ¬é€šè¿‡äº†æµ‹è¯•ï¼Œç°åœ¨æ‚¨åº”è¯¥åœ¨æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­çš„ `README.md` æ–‡ä»¶çš„å¤´éƒ¨æœ‰ `dataset_info` YAML å­—æ®µã€‚
- en: Upload the dataset to the Hub
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ•°æ®é›†ä¸Šä¼ åˆ° Hub
- en: Once your script is ready, [create a dataset card](./dataset_card) and [upload
    it to the Hub](./share).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨çš„è„šæœ¬å‡†å¤‡å¥½äº†ï¼Œ[åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å¡ç‰‡](./dataset_card)å¹¶[å°†å…¶ä¸Šä¼ åˆ° Hub](./share)ã€‚
- en: Congratulations, you can now load your dataset from the Hub! ğŸ¥³
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼Œæ‚¨ç°åœ¨å¯ä»¥ä» Hub åŠ è½½æ‚¨çš„æ•°æ®é›†äº†ï¼ğŸ¥³
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
