- en: SeamlessM4T
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SeamlessM4T
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The SeamlessM4T model was proposed in [SeamlessM4T — Massively Multilingual
    & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)
    by the Seamless Communication team from Meta AI.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: SeamlessM4T模型是由Meta AI的Seamless Communication团队在[SeamlessM4T — 大规模多语言和多模态机器翻译](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)中提出的。
- en: This is the **version 1** release of the model. For the updated **version 2**
    release, refer to the [Seamless M4T v2 docs](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t_v2).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型的**版本1**发布。有关更新的**版本2**发布，请参阅[Seamless M4T v2文档](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t_v2)。
- en: SeamlessM4T is a collection of models designed to provide high quality translation,
    allowing people from different linguistic communities to communicate effortlessly
    through speech and text.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: SeamlessM4T是一系列旨在提供高质量翻译的模型，使来自不同语言社区的人们能够通过语音和文本轻松交流。
- en: 'SeamlessM4T enables multiple tasks without relying on separate models:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: SeamlessM4T可以在不依赖于单独模型的情况下执行多个任务：
- en: Speech-to-speech translation (S2ST)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音到语音翻译（S2ST）
- en: Speech-to-text translation (S2TT)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音到文本翻译（S2TT）
- en: Text-to-speech translation (T2ST)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到语音翻译（T2ST）
- en: Text-to-text translation (T2TT)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到文本翻译（T2TT）
- en: Automatic speech recognition (ASR)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动语音识别（ASR）
- en: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    can perform all the above tasks, but each task also has its own dedicated sub-model.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)可以执行所有上述任务，但每个任务也有自己的专用子模型。'
- en: 'The abstract from the paper is the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*What does it take to create the Babel Fish, a tool that can help individuals
    translate speech between any two languages? While recent breakthroughs in text-based
    models have pushed machine translation coverage beyond 200 languages, unified
    speech-to-speech translation models have yet to achieve similar strides. More
    specifically, conventional speech-to-speech translation systems rely on cascaded
    systems that perform translation progressively, putting high-performing unified
    systems out of reach. To address these gaps, we introduce SeamlessM4T, a single
    model that supports speech-to-speech translation, speech-to-text translation,
    text-to-speech translation, text-to-text translation, and automatic speech recognition
    for up to 100 languages. To build this, we used 1 million hours of open speech
    audio data to learn self-supervised speech representations with w2v-BERT 2.0\.
    Subsequently, we created a multimodal corpus of automatically aligned speech translations.
    Filtered and combined with human-labeled and pseudo-labeled data, we developed
    the first multilingual system capable of translating from and into English for
    both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations
    into multiple target languages, achieving an improvement of 20% BLEU over the
    previous SOTA in direct speech-to-text translation. Compared to strong cascaded
    models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU
    points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested
    for robustness, our system performs better against background noises and speaker
    variations in speech-to-text tasks compared to the current SOTA model. Critically,
    we evaluated SeamlessM4T on gender bias and added toxicity to assess translation
    safety. Finally, all contributions in this work are open-sourced and accessible
    at [https://github.com/facebookresearch/seamless_communication](https://github.com/facebookresearch/seamless_communication)*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 创建巴别鱼需要什么条件？这是一个可以帮助个人在任意两种语言之间翻译语音的工具。尽管最近基于文本的模型取得了突破性进展，将机器翻译覆盖范围推广到200多种语言，但统一的语音到语音翻译模型尚未取得类似的进展。更具体地说，传统的语音到语音翻译系统依赖于级联系统逐步执行翻译，使高性能的统一系统难以实现。为了解决这些差距，我们引入了SeamlessM4T，这是一个支持语音到语音翻译、语音到文本翻译、文本到语音翻译、文本到文本翻译以及最多100种语言的自动语音识别的单一模型。为了构建这个模型，我们使用了100万小时的开放语音音频数据，学习了自监督语音表示与w2v-BERT
    2.0。随后，我们创建了一个自动对齐的语音翻译的多模态语料库。经过筛选并与人工标记和伪标记数据结合，我们开发了第一个能够在语音和文本之间进行英语翻译的多语言系统。在FLEURS上，SeamlessM4T为多种目标语言的翻译设定了一个新标准，直接语音到文本翻译的BLEU值比之前的SOTA提高了20%。与强级联模型相比，SeamlessM4T在语音到文本翻译中将英语翻译质量提高了1.3个BLEU点，在语音到语音翻译中将ASR-BLEU值提高了2.6个点。在鲁棒性测试中，我们的系统在语音到文本任务中对抗背景噪音和说话者变化的表现优于当前的SOTA模型。关键是，我们评估了SeamlessM4T的性别偏见和添加了毒性以评估翻译安全性。最后，本工作中的所有贡献都是开源的，可在[https://github.com/facebookresearch/seamless_communication](https://github.com/facebookresearch/seamless_communication)上获取。
- en: Usage
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法
- en: 'First, load the processor and a checkpoint of the model:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载处理器和模型的检查点：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can seamlessly use this model on text or on audio, to generated either translated
    text or translated audio.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以无缝地在文本或音频上使用此模型，生成翻译后的文本或音频。
- en: 'Here is how to use the processor to process text and audio:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用处理器处理文本和音频：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Speech
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语音
- en: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    can *seamlessly* generate text or speech with few or no changes. Let’s target
    Russian voice translation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)可以*无缝地*生成文本或语音，几乎不需要任何更改。让我们以俄语语音翻译为目标：'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With basically the same code, I’ve translated English text and Arabic speech
    to Russian speech samples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上使用相同的代码，我已经将英文文本和阿拉伯语语音翻译成俄语语音样本。
- en: Text
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本
- en: Similarly, you can generate translated text from audio files or from text with
    the same model. You only have to pass `generate_speech=False` to [SeamlessM4TModel.generate()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel.generate).
    This time, let’s translate to French.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以使用相同模型从音频文件或文本生成翻译文本。您只需将 `generate_speech=False` 传递给 [SeamlessM4TModel.generate()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel.generate)。这次，让我们翻译成法语。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tips
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 1\. Use dedicated models
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1\. 使用专用模型
- en: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    is transformers top level model to generate speech and text, but you can also
    use dedicated models that perform the task without additional components, thus
    reducing the memory footprint. For example, you can replace the audio-to-audio
    generation snippet with the model dedicated to the S2ST task, the rest is exactly
    the same code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    是 transformers 的顶级模型，用于生成语音和文本，但您也可以使用专用模型执行任务而无需额外组件，从而减少内存占用。例如，您可以使用专用于 S2ST
    任务的模型替换音频到音频生成片段，其余代码完全相同：'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Or you can replace the text-to-text generation snippet with the model dedicated
    to the T2TT task, you only have to remove `generate_speech=False`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用专用于 T2TT 任务的模型替换文本到文本生成片段，只需删除 `generate_speech=False`。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Feel free to try out [SeamlessM4TForSpeechToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText)
    and [SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)
    as well.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随时尝试 [SeamlessM4TForSpeechToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText)
    和 [SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)。
- en: 2\. Change the speaker identity
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2\. 更改说话者身份
- en: You have the possibility to change the speaker used for speech synthesis with
    the `spkr_id` argument. Some `spkr_id` works better than other for some languages!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `spkr_id` 参数更改用于语音合成的说话者。对于某些语言，某些 `spkr_id` 的效果比其他的好！
- en: 3\. Change the generation strategy
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3\. 更改生成策略
- en: You can use different [generation strategies](./generation_strategies) for speech
    and text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, speech_do_sample=True)`
    which will successively perform beam-search decoding on the text model, and multinomial
    sampling on the speech model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为语音和文本生成使用不同的 [生成策略](./generation_strategies)，例如 `.generate(input_ids=input_ids,
    text_num_beams=4, speech_do_sample=True)`，这将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式采样。
- en: 4\. Generate speech and text at the same time
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4\. 同时生成语音和文本
- en: Use `return_intermediate_token_ids=True` with [SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    to return both speech and text !
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    的 `return_intermediate_token_ids=True` 来返回语音和文本！
- en: Model architecture
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: SeamlessM4T features a versatile architecture that smoothly handles the sequential
    generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq)
    models. The first model translates the input modality into translated text, while
    the second model generates speech tokens, known as “unit tokens,” from the translated
    text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: SeamlessM4T 具有一个多功能架构，可以平滑处理文本和语音的顺序生成。此设置包括两个序列到序列 (seq2seq) 模型。第一个模型将输入模态转换为翻译文本，而第二个模型从翻译文本生成称为“单元标记”的语音标记。
- en: Each modality has its own dedicated encoder with a unique architecture. Additionally,
    for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646)
    architecture is placed on top of the second seq2seq model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每种模态都有自己专用的编码器，具有独特的架构。此外，对于语音输出，一个受 [HiFi-GAN](https://arxiv.org/abs/2010.05646)
    架构启发的声码器被放置在第二个 seq2seq 模型的顶部。
- en: 'Here’s how the generation process works:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成过程的工作原理：
- en: Input text or speech is processed through its specific encoder.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入文本或语音通过其特定编码器进行处理。
- en: A decoder creates text tokens in the desired language.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器在所需语言中创建文本标记。
- en: If speech generation is required, the second seq2seq model, following a standard
    encoder-decoder structure, generates unit tokens.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要语音生成，第二个遵循标准编码器-解码器结构的 seq2seq 模型会生成单元标记。
- en: These unit tokens are then passed through the final vocoder to produce the actual
    speech.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，这些单元标记通过最终的声码器传递，产生实际的语音。
- en: This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The
    original code can be found [here](https://github.com/facebookresearch/seamless_communication).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由 [ylacombe](https://huggingface.co/ylacombe) 贡献。原始代码可以在 [这里](https://github.com/facebookresearch/seamless_communication)
    找到。
- en: SeamlessM4TModel
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TModel
- en: '### `class transformers.SeamlessM4TModel`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.SeamlessM4TModel` 类'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3927)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3927)'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: '`current_modality` (`str`, *optional*, defaults to `"text"`) — Default modality.
    Used to initialize the model.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`current_modality` (`str`, *可选*，默认为 `"text"`) — 默认模态。用于初始化模型。'
- en: The original SeamlessM4T Model transformer which can be used for every tasks
    available (S2ST, S2TT, T2TT, T2ST). This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的SeamlessM4T模型变压器，可用于所有可用任务（S2ST、S2TT、T2TT、T2ST）。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `generate`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L4141)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L4141)'
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`, *optional*) — Input audio features. This should be returnes by the
    [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`, *optional*) — 输入音频特征。这应该由[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)类或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)类返回。有关详细信息，请参阅[SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)。'
- en: '`return_intermediate_token_ids` (`bool`, *optional*) — If `True`, also returns
    the intermediate generated text and unit tokens. Set to `True` if you also want
    to get translated text alongside the audio. Note that if `generate_speech=True`,
    this parameter will be ignored.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_intermediate_token_ids` (`bool`, *optional*) — 如果为`True`，还会返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，则设置为`True`。请注意，如果`generate_speech=True`，则此参数将被忽略。'
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *optional*) — 用作翻译目标语言的语言。'
- en: '`spkr_id` (`int`, *optional*, defaults to 0) — The id of the speaker used for
    speech synthesis. Must be lower than `config.vocoder_num_spkrs`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spkr_id` (`int`, *optional*, defaults to 0) — 用于语音合成的说话者id。必须小于`config.vocoder_num_spkrs`。'
- en: '`generate_speech` (`bool`, *optional*, defaults to `True`) — If `False`, will
    only returns the text tokens and won’t generate speech.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_speech` (`bool`, *optional*, defaults to `True`) — 如果为`False`，将仅返回文本标记，不会生成语音。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Keyword arguments are of two types:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 将传递给[GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)的剩余关键字参数字典。关键字参数有两种类型：'
- en: Without a prefix, they will be entered as `**kwargs` for the `generate` method
    of each sub-model, except for `decoder_input_ids` which will only be passed through
    the text components.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有前缀，它们将作为`generate`方法的`**kwargs`输入到每个子模型中，除了`decoder_input_ids`只会通过文本组件传递。
- en: With a *text_* or *speech_* prefix, they will be input for the `generate` method
    of the text model and speech model respectively. It has the priority over the
    keywords without a prefix.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有*text_*或*speech_*前缀，它们将分别作为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。
- en: This means you can, for example, specify a generation strategy for one generation
    but not for the other.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这意味着您可以例如为一个生成指定一种生成策略，但对另一个不指定。
- en: Returns
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`'
- en: If `generate_speech` and `return_intermediate_token_ids`, returns `SeamlessM4TGenerationOutput`.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`generate_speech`且`return_intermediate_token_ids`，则返回`SeamlessM4TGenerationOutput`。
- en: If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple
    composed of waveforms of shape `(batch_size, sequence_length)`and and `waveform_lengths`
    which gives the length of each sample.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`generate_speech`且不是`return_intermediate_token_ids`，则返回形状为`(batch_size, sequence_length)`的波形和`waveform_lengths`的元组，其中给出每个样本的长度。
- en: If `generate_speech=False`, it will returns `ModelOutput`.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`generate_speech=False`，它将返回`ModelOutput`。
- en: Generates translated token ids and/or translated audio waveforms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 生成翻译的标记id和/或翻译的音频波形。
- en: 'This method successively calls the `.generate` function of two different sub-models.
    You can specify keyword arguments at two different levels: general arguments that
    will be passed to both models, or prefixed arguments that will be passed to one
    of them.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的通用参数，或将传递给其中一个模型的前缀参数。
- en: For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)`
    will successively perform beam-search decoding on the text model, and multinomial
    beam-search sampling on the speech model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，调用`.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成策略和代码示例的概述，请查看[以下指南](./generation_strategies)。
- en: SeamlessM4TForTextToSpeech
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TForTextToSpeech
- en: '### `class transformers.SeamlessM4TForTextToSpeech`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TForTextToSpeech`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3214)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3214)'
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法加载模型权重。'
- en: The text-to-speech SeamlessM4T Model transformer which can be used for T2ST.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 用于T2ST的文本到语音SeamlessM4T模型变换器。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `generate`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3366)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3366)'
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`return_intermediate_token_ids` (`bool`, *optional*) — If `True`, also returns
    the intermediate generated text and unit tokens. Set to `True` if you also want
    to get translated text alongside the audio.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_intermediate_token_ids` (`bool`，*可选*) — 如果为`True`，还会返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，请设置为`True`。'
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`，*可选*) — 用作翻译目标语言的语言。'
- en: '`spkr_id` (`int`, *optional*, defaults to 0) — The id of the speaker used for
    speech synthesis. Must be lower than `config.vocoder_num_spkrs`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spkr_id` (`int`，*可选*，默认为0) — 用于语音合成的说话者的id。必须小于`config.vocoder_num_spkrs`。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Keyword arguments are of two types:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*可选*) — 将传递给[GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)的剩余关键字参数字典。关键字参数有两种类型：'
- en: Without a prefix, they will be entered as `**kwargs` for the `generate` method
    of each sub-model, except for `decoder_input_ids` which will only be passed through
    the text components.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有前缀，它们将作为`**kwargs`输入到每个子模型的`generate`方法中，除了`decoder_input_ids`，它只会通过文本组件传递。
- en: With a *text_* or *speech_* prefix, they will be input for the `generate` method
    of the text model and speech model respectively. It has the priority over the
    keywords without a prefix.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有*text_*或*speech_*前缀，它们将成为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。
- en: This means you can, for example, specify a generation strategy for one generation
    but not for the other.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这意味着您可以为一个生成策略指定一个生成策略，但不能为另一个生成策略指定生成策略。
- en: Returns
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`'
- en: If `return_intermediate_token_ids`, returns `SeamlessM4TGenerationOutput`.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`return_intermediate_token_ids`，返回`SeamlessM4TGenerationOutput`。
- en: If not `return_intermediate_token_ids`, returns a tuple composed of waveforms
    of shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives
    the length of each sample.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不是`return_intermediate_token_ids`，返回一个由形状为`(batch_size, sequence_length)`的波形和`waveform_lengths`组成的元组，其中给出每个样本的长度。
- en: Generates translated audio waveforms.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 生成翻译后的音频波形。
- en: 'This method successively calls the `.generate` function of two different sub-models.
    You can specify keyword arguments at two different levels: general arguments that
    will be passed to both models, or prefixed arguments that will be passed to one
    of them.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的一般参数，或将传递给其中一个模型的带前缀的参数。
- en: For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)`
    will successively perform beam-search decoding on the text model, and multinomial
    beam-search sampling on the speech model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，调用`.generate(input_ids, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成策略和代码示例的概述，请查看[以下指南](./generation_strategies)。
- en: SeamlessM4TForSpeechToSpeech
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TForSpeechToSpeech
- en: '### `class transformers.SeamlessM4TForSpeechToSpeech`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TForSpeechToSpeech`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3566)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3566)'
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The speech-to-speech SeamlessM4T Model transformer which can be used for S2ST.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于S2ST的语音到语音SeamlessM4T模型变换器。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `generate`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3721)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3721)'
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`) — Input audio features. This should be returnes by the [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_banks)`)
    — 输入音频特征。这应该由[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)类或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)类返回。有关详细信息，请参阅[SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)。'
- en: '`return_intermediate_token_ids` (`bool`, *optional*) — If `True`, also returns
    the intermediate generated text and unit tokens. Set to `True` if you also want
    to get translated text alongside the audio.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_intermediate_token_ids` (`bool`, *可选*) — 如果为`True`，还会返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，请设置为`True`。'
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言。'
- en: '`spkr_id` (`int`, *optional*, defaults to 0) — The id of the speaker used for
    speech synthesis. Must be lower than `config.vocoder_num_spkrs`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spkr_id` (`int`, *可选*，默认为0) — 用于语音合成的说话者的id。必须小于`config.vocoder_num_spkrs`。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Keyword arguments are of two types:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*可选*) — 将传递给[GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)的剩余关键字参数字典。关键字参数有两种类型：'
- en: Without a prefix, they will be entered as `**kwargs` for the `generate` method
    of each sub-model, except for `decoder_input_ids` which will only be passed through
    the text components.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有前缀，它们将作为`**kwargs`输入到每个子模型的`generate`方法中，除了`decoder_input_ids`只会通过文本组件传递。
- en: With a *text_* or *speech_* prefix, they will be input for the `generate` method
    of the text model and speech model respectively. It has the priority over the
    keywords without a prefix.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*text_*或*speech_*前缀，它们将分别作为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。
- en: This means you can, for example, specify a generation strategy for one generation
    but not for the other.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '这意味着您可以为一个生成指定生成策略，但对另一个生成不指定。 '
- en: Returns
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`'
- en: If `return_intermediate_token_ids`, returns `SeamlessM4TGenerationOutput`.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`return_intermediate_token_ids`，则返回`SeamlessM4TGenerationOutput`。
- en: If not `return_intermediate_token_ids`, returns a tuple composed of waveforms
    of shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives
    the length of each sample.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不是`return_intermediate_token_ids`，则返回一个由形状为`(batch_size, sequence_length)`的波形和给出每个样本长度的`waveform_lengths`组成的元组。
- en: Generates translated audio waveforms.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 生成翻译音频波形。
- en: 'This method successively calls the `.generate` function of two different sub-models.
    You can specify keyword arguments at two different levels: general arguments that
    will be passed to both models, or prefixed arguments that will be passed to one
    of them.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的一般参数，或将传递给其中一个模型的前缀参数。
- en: For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)`
    will successively perform beam-search decoding on the text model, and multinomial
    beam-search sampling on the speech model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，调用`.generate(input_features, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成策略和代码示例的概述，请查看[以下指南](./generation_strategies)。
- en: SeamlessM4TForTextToText
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TForTextToText
- en: '### `class transformers.SeamlessM4TForTextToText`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TForTextToText`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2637)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2637)'
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The text-to-text SeamlessM4T Model transformer which can be used for T2TT. This
    model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到文本SeamlessM4T模型变压器可用于T2TT。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2689)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2689)'
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，为1。
- en: 0 for tokens that are `masked`.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）-
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Bart使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则可选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于翻译和总结训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将根据论文将`input_ids`向右移动以进行去噪预训练来创建此张量。
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）-
    默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: If you want to change padding behavior, you should read `modeling_bart._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改填充行为，您应该阅读`modeling_bart._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包含(`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*optional*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的元组，每个元组包含2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的输入），而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更好地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，可以选择仅输入最后的`decoder_inputs_embeds`（请参见`past_key_values`）。如果您希望更好地控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`将取`inputs_embeds`的值。
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（请参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`范围内的标记。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通的元组。'
- en: The [SeamlessM4TForTextToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText)
    forward method, overrides the `__call__` special method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[SeamlessM4TForTextToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: '#### `generate`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2781)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2781)'
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of varying shape depending on the modality, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（根据模态性质的不同形状的`torch.Tensor`，*可选*） — 词汇表中输入序列令牌的索引。'
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言。'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`~generation.GenerationConfig`, *可选*) — 用作生成调用的基本参数化的生成配置。与`generation_config`属性匹配的`**kwargs`传递给generate将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中获取，如果存在；2）从模型配置中获取。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor` (`LogitsProcessorList`, *可选*) — 自定义logits处理器，用于补充从参数和生成配置构建的默认logits处理器。如果传递了已经使用参数或生成配置创建的logit处理器，则会抛出错误。此功能适用于高级用户。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. This feature is
    intended for advanced users.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`StoppingCriteriaList`, *可选*) — 自定义停止标准，用于补充从参数和生成配置构建的默认停止标准。如果传递了已经使用参数或生成配置创建的停止标准，则会抛出错误。此功能适用于高级用户。'
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *可选*)
    — 如果提供，此函数将在每个步骤中将beam搜索限制为仅允许的令牌。如果未提供，则不应用约束。此函数接受2个参数：批次ID`batch_id`和`input_ids`。它必须返回一个列表，其中包含下一代步骤的允许令牌，条件是批次ID`batch_id`和先前生成的令牌`inputs_ids`。此参数对于基于前缀的受约束生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus` (`bool`, *可选*，默认为`False`) — 是否继续运行while循环直到max_length（需要ZeRO阶段3）'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。'
- en: Returns
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)或`torch.LongTensor`'
- en: 'A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`. The possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个`torch.FloatTensor`。可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型为：
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
- en: Generates sequences of token ids.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 生成标记ID序列。
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数生成控制参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过传递相应的参数给generate()来覆盖任何`generation_config`，例如`.generate(inputs,
    num_beams=4, do_sample=True)`。
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成策略和代码示例的概述，请查看[以下指南](./generation_strategies)。
- en: SeamlessM4TForSpeechToText
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TForSpeechToText
- en: '### `class transformers.SeamlessM4TForSpeechToText`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TForSpeechToText`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2924)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2924)'
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The speech-to-text SeamlessM4T Model transformer which can be used for S2TT.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 语音到文本SeamlessM4T模型变压器，可用于S2TT。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `前进`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2971)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2971)'
- en: '[PRE16]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`) — Input audio features. This should be returnes by the [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`（形状为`(batch_size, sequence_length, num_banks)`的`torch.FloatTensor`）—
    输入音频特征。这应该由[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)类或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)类返回。有关详细信息，请参阅[SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中。'
- en: 1 for tokens that are `not masked`,
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表未被“掩盖”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表被“掩盖”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[解码器输入ID是什么？](../glossary#decoder-input-ids)'
- en: Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Bart使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则可以选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于翻译和摘要训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将通过将`input_ids`向右移动来创建此张量，以进行去噪预训练，遵循论文。
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: If you want to change padding behavior, you should read `modeling_bart._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您想要更改填充行为，您应该阅读`modeling_bart._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括(`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*optional*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参阅`past_key_values`输入）。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后`decoder_input_ids`（这些没有将其过去键值状态提供给此模型的）而不是所有形状为`(batch_size,
    sequence_length)`的`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor`的形状为`(batch_size, target_sequence_length,
    hidden_size)`，*optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则只需输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`的形状为`(batch_size, sequence_length)`，*optional*)
    — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`内（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`内的标记。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: The [SeamlessM4TForSpeechToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText)
    forward method, overrides the `__call__` special method.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[SeamlessM4TForSpeechToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例而不是此函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: '#### `generate`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3070)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3070)'
- en: '[PRE17]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`) — Input audio features. This should be returnes by the [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`（形状为`(batch_size, sequence_length, num_banks)`的`torch.FloatTensor`）—
    输入音频特征。这应该由[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)类或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)类返回。有关详细信息，请参阅[SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)。'
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang`（`str`，*可选*）— 用作翻译目标语言的语言。'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config`（`~generation.GenerationConfig`，*可选*）— 用作生成调用的基本参数化的生成配置。传递给生成匹配`generation_config`属性的`**kwargs`将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）来自`generation_config.json`模型文件，如果存在；2）来自模型配置。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`（`LogitsProcessorList`，*可选*）— 自定义logits处理器，补充从参数和生成配置构建的默认logits处理器。如果传递的logit处理器已经使用参数或生成配置创建，则会引发错误。此功能适用于高级用户。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. This feature is
    intended for advanced users.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria`（`StoppingCriteriaList`，*可选*）— 自定义停止标准，补充从参数和生成配置构建的默认停止标准。如果传递的停止标准已经使用参数或生成配置创建，则会引发错误。此功能适用于高级用户。'
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_allowed_tokens_fn`（`Callable[[int, torch.Tensor], List[int]]`，*可选*）—
    如果提供，此函数将限制每一步的波束搜索仅允许特定的标记。如果未提供，则不应用任何约束。此函数接受2个参数：批次ID `batch_id` 和 `input_ids`。它必须返回一个列表，其中包含下一代步的允许标记，条件是批次ID
    `batch_id` 和先前生成的标记 `inputs_ids`。此参数对于基于前缀的受限生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus`（`bool`，*可选*，默认为`False`）— 是否继续运行while循环直到max_length（对于ZeRO阶段3是必需的）'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）— `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。'
- en: Returns
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)或`torch.LongTensor`'
- en: 'A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`. The possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个`torch.FloatTensor`。可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型是：
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
- en: Generates sequences of token ids.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 生成标记id序列。
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数控制生成的参数设置在`generation_config`中，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应参数传递给generate()来覆盖任何`generation_config`，例如`.generate(inputs,
    num_beams=4, do_sample=True)`。
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成策略和代码示例的概述，请查看[以下指南](./generation_strategies)。
- en: SeamlessM4TConfig
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TConfig
- en: '### `class transformers.SeamlessM4TConfig`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/configuration_seamless_m4t.py#L29)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/configuration_seamless_m4t.py#L29)'
- en: '[PRE18]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 256102) — Vocabulary size of the
    SeamlessM4T model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel),
    [~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)
    or [~SeamlessM4TForTextToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, 默认为 256102) — SeamlessM4T模型的词汇表大小。定义了在调用[~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)、[~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)或[~SeamlessM4TForTextToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText)
    时可以表示的不同标记数量。'
- en: '`t2u_vocab_size` (`int`, *optional*, defaults to 10082) — Unit vocabulary size
    of the SeamlessM4T model. Defines the number of different unit tokens that can
    be represented by the `inputs_ids` passed when calling the Text-To-Units sub-model
    of [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel),
    [~SeamlessM4TForSpeechToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToSpeech)
    or [~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_vocab_size` (`int`, *optional*, 默认为 10082) — SeamlessM4T模型的单元词汇表大小。定义了在调用[~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)、[~SeamlessM4TForSpeechToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToSpeech)或[~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)
    时可以表示的不同单元标记数量。'
- en: Parameters shared across sub-models
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在子模型之间共享的参数
- en: '`hidden_size` (`int`, *optional*, defaults to 1024) — Dimensionality of the
    “intermediate” layers in the architecture.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为 1024) — 架构中“中间”层的维度。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为 1e-05) — 层归一化层使用的epsilon。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, 默认为 `True`) — 模型是否应返回最后一个键/值注意力（并非所有模型都使用）。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 1024) — The maximum
    sequence length that this model text encoder and decoder might ever be used with.
    Typically set this to something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, 默认为 1024) — 此模型文本编码器和解码器可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512或1024或2048）。'
- en: '`is_encoder_decoder` (`bool`, *optional*, defaults to `True`) — Whether the
    model is used as an encoder/decoder or not.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_encoder_decoder` (`bool`, *optional*, 默认为 `True`) — 模型是否用作编码器/解码器。'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.05) — The LayerDrop
    probability for the encoders. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, 默认为 0.05) — 编码器的LayerDrop概率。更多细节请参阅[LayerDrop论文](https://arxiv.org/abs/1909.11556)。'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.05) — The LayerDrop
    probability for the decoders. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, 默认为 0.05) — 解码器的LayerDrop概率。更多细节请参阅[LayerDrop论文](https://arxiv.org/abs/1909.11556)。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"relu"`)
    — The non-linear activation function (function or string) in the decoder and feed-forward
    layers. If string, `"gelu"`, `"relu"`, `"selu"`, `"swish"` and `"gelu_new"` are
    supported.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` 或 `function`, *optional*, 默认为 `"relu"`) — 解码器和前馈层中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`, `"relu"`, `"selu"`, `"swish"` 和 `"gelu_new"`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, decoder, and pooler.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器、解码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all attention layers.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, 默认为 0.1) — 所有注意力层的dropout概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all activation layers in the model.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, 默认为 0.0) — 模型中所有激活层的dropout概率。'
- en: '`scale_embedding` (`bool`, *optional*, defaults to `True`) — Scale embeddings
    by diving by sqrt(d_model).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_embedding` (`bool`, *optional*, 默认为 `True`) — 通过将其除以sqrt(d_model)来缩放嵌入。'
- en: Text encoder and text decoder specific parameters
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器和文本解码器特定参数
- en: '`encoder_layers` (`int`, *optional*, defaults to 24) — Number of hidden layers
    in the Transformer text encoder.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers` (`int`, *optional*, defaults to 24) — Transformer文本编码器中的隐藏层数量。'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of the
    “intermediate” (i.e., feed-forward) layer in the Transformer text encoder.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer文本编码器中“中间”（即前馈）层的维度。'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer text encoder.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer文本编码器中每个注意力层的注意力头数。'
- en: '`decoder_layers` (`int`, *optional*, defaults to 24) — Number of hidden layers
    in the Transformer text decoder.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers` (`int`, *optional*, defaults to 24) — Transformer文本解码器中的隐藏层数量。'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of the
    “intermediate” (i.e., feed-forward) layer in the Transformer text decoder.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer文本解码器中“中间”（即前馈）层的维度。'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer text decoder.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer文本解码器中每个注意力层的注意力头数。'
- en: '`decoder_start_token_id` (`int`, *optional*, defaults to 3) — If an encoder-decoder
    model starts decoding with a different token than *bos*, the id of that token.
    Only applied in the text decoder.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_start_token_id` (`int`, *optional*, defaults to 3) — 如果编码器-解码器模型以与*bos*不同的标记开始解码，则该标记的id。仅在文本解码器中应用。'
- en: '`max_new_tokens` (`int`, *optional*, defaults to 256) — The maximum numbers
    of text tokens to generate, ignoring the number of tokens in the prompt.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens` (`int`, *optional*, defaults to 256) — 生成的文本标记的最大数量，忽略提示中的标记数量。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The id of the *padding*
    text token. Only applied to the text-decoder model.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, defaults to 0) — *填充*文本标记的id。仅适用于文本解码器模型。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 2) — The id of the *beginning-of-stream*
    text token. Only applied to the text-decoder model.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*, defaults to 2) — *流的开头*文本标记的id。仅适用于文本解码器模型。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 3) — The id of the *end-of-stream*
    text token. Only applied to the text-decoder model.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, defaults to 3) — *流的结尾*文本标记的id。仅适用于文本解码器模型。'
- en: Speech encoder specific parameters
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 语音编码器特定参数
- en: '`speech_encoder_layers` (`int`, *optional*, defaults to 24) — Number of hidden
    layers in the Transformer speech encoder.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_encoder_layers` (`int`, *optional*, defaults to 24) — Transformer语音编码器中的隐藏层数量。'
- en: '`speech_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer speech encoder.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer语音编码器中每个注意力层的注意力头数。'
- en: '`speech_encoder_intermediate_size` (`int`, *optional*, defaults to 4096) —
    Dimension of the “intermediate” (i.e., feed-forward) layer in the Transformer
    speech encoder.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_encoder_intermediate_size` (`int`, *optional*, defaults to 4096) —
    Transformer语音编码器中“中间”（即前馈）层的维度。'
- en: '`speech_encoder_hidden_act` (`str` or `function`, *optional*, defaults to `"swish"`)
    — The non-linear activation function (function or string) in the speech encoder.
    If string, `"gelu"`, `"relu"`, `"selu"`, `"swish"` and `"gelu_new"` are supported.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_encoder_hidden_act` (`str` or `function`, *optional*, defaults to `"swish"`)
    — 语音编码器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`、`"swish"`和`"gelu_new"`。'
- en: '`speech_encoder_dropout` (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all layers in the speech encoder.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_encoder_dropout` (`float`, *optional*, defaults to 0.0) — 语音编码器中所有层的dropout概率。'
- en: '`add_adapter` (`bool`, *optional*, defaults to `True`) — Add an adapter layer
    on top of the speech encoder.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_adapter` (`bool`, *optional*, defaults to `True`) — 在语音编码器顶部添加一个适配器层。'
- en: '`speech_encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop
    probability for the speech encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 语音编码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](https://arxiv.org/abs/1909.11556)。'
- en: '`feature_projection_input_dim` (`int`, *optional*, defaults to 160) — Input
    dimension of the input feature projection of the speech encoder, i.e the dimension
    after processing input audios with [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_projection_input_dim` (`int`, *optional*, defaults to 160) — 语音编码器输入特征投影的输入维度，即在使用[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)处理输入音频后的维度。'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer of the speech encoder.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义语音编码器的1D卷积位置嵌入层的内核大小。'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer of the speech encoder.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 语音编码器的1D卷积位置嵌入层的组数。'
- en: '`adaptor_kernel_size` (`int`, *optional*, defaults to 8) — Kernel size of the
    convolutional layers in the adapter network. Only relevant if `add_adapter is
    True`.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adaptor_kernel_size` (`int`, *optional*, defaults to 8) — 适配器网络中卷积层的内核大小。仅在`add_adapter`为True时相关。'
- en: '`adaptor_stride` (`int`, *optional*, defaults to 8) — Stride of the convolutional
    layers in the adapter network. Only relevant if `add_adapter is True`.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adaptor_stride` (`int`, *optional*, defaults to 8) — 适配器网络中卷积层的步幅。仅在`add_adapter`为True时相关。'
- en: '`adaptor_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all layers in the speech adapter.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adaptor_dropout` (`float`, *optional*, defaults to 0.1) — 语音适配器中所有层的dropout概率。'
- en: '`num_adapter_layers` (`int`, *optional*, defaults to 1) — Number of convolutional
    layers that should be used in the adapter network. Only relevant if `add_adapter
    is True`.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_adapter_layers` (`int`, *optional*, defaults to 1) — 适配器网络中应使用的卷积层数。仅在`add_adapter`为True时相关。'
- en: '`position_embeddings_type` (`str`, *optional*, defaults to `"relative"`) —
    Can be specified to `relative` or `rotary` for relative or rotary position embeddings
    respectively. If left `None` no relative position embedding is applied. Only applied
    to the speech encoder.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embeddings_type` (`str`, *optional*, defaults to `"relative"`) —
    可以指定为`relative`或`rotary`，分别用于相对或旋转位置嵌入。如果保持为`None`，则不应用相对位置嵌入。仅应用于语音编码器。'
- en: '`rotary_embedding_base` (`int`, *optional*, defaults to 10000) — If `"rotary"`
    position embeddings are used, defines the size of the embedding base. Only applied
    to the speech encoder.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rotary_embedding_base` (`int`, *optional*, defaults to 10000) — 如果使用“rotary”位置嵌入，则定义嵌入基数的大小。仅应用于语音编码器。'
- en: '`max_source_positions` (`int`, *optional*, defaults to 4096) — if `"relative"`
    position embeddings are used, defines the maximum source input positions. Only
    applied to the speech encoder.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_source_positions` (`int`, *optional*, defaults to 4096) — 如果使用“relative”位置嵌入，则定义最大源输入位置。仅应用于语音编码器。'
- en: '`conv_depthwise_kernel_size` (`int`, *optional*, defaults to 31) — Kernel size
    of convolutional depthwise 1D layer in Conformer blocks. Only applied to the speech
    encoder.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_depthwise_kernel_size` (`int`, *optional*, defaults to 31) — Conformer块中卷积深度1D层的内核大小。仅应用于语音编码器。'
- en: Text-To-Unit (t2u) model specific parameters
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到单元组件（t2u）模型特定参数
- en: '`t2u_bos_token_id` (`int`, *optional*, defaults to 0) — The id of the *beginning-of-stream*
    unit token. Only applied to the text-to-unit seq2seq model.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_bos_token_id` (`int`, *optional*, defaults to 0) — *beginning-of-stream*单元标记的id。仅应用于文本到单元seq2seq模型。'
- en: '`t2u_pad_token_id` (`int`, *optional*, defaults to 1) — The id of the *padding*
    unit token. Only applied to the text-to-unit seq2seq model.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_pad_token_id` (`int`, *optional*, defaults to 1) — *padding*单元标记的id。仅应用于文本到单元seq2seq模型。'
- en: '`t2u_eos_token_id` (`int`, *optional*, defaults to 2) — The id of the *end-of-stream*
    unit token. Only applied to the text-to-unit seq2seq model.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_eos_token_id` (`int`, *optional*, defaults to 2) — *end-of-stream*单元标记的id。仅应用于文本到单元seq2seq模型。'
- en: '`t2u_decoder_start_token_id` (`int`, *optional*, defaults to 2) — If an encoder-decoder
    model starts decoding with a different token than *bos*, the id of that token.
    Only applied to the text-to-unit seq2seq model.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_decoder_start_token_id` (`int`, *optional*, defaults to 2) — 如果编码器-解码器模型以与*bos*不同的标记开始解码，则为该标记的id。仅应用于文本到单元seq2seq模型。'
- en: '`t2u_max_new_tokens` (`int`, *optional*, defaults to 1024) — The maximum numbers
    of unit tokens to generate, ignoring the number of tokens in the prompt. Only
    applied to the text-to-unit seq2seq model.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_max_new_tokens` (`int`, *optional*, defaults to 1024) — 生成的单元标记的最大数量，忽略提示中的标记数量。仅应用于文本到单元seq2seq模型。'
- en: '`t2u_encoder_layers` (`int`, *optional*, defaults to 6) — Number of hidden
    layers in the Transformer text-to-unit encoder.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_encoder_layers` (`int`, *optional*, defaults to 6) — Transformer文本到单元编码器中的隐藏层数量。'
- en: '`t2u_encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of
    the “intermediate” (i.e., feed-forward) layer in the Transformer text-to-unit
    encoder.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer文本到单元编码器中“中间”（即前馈）层的维度。'
- en: '`t2u_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer text-to-unit encoder.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer文本到单元编码器中每个注意力层的注意力头数。'
- en: '`t2u_decoder_layers` (`int`, *optional*, defaults to 6) — Number of hidden
    layers in the Transformer text-to-unit decoder.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_decoder_layers` (`int`, *optional*, defaults to 6) — Transformer文本到单元解码器中的隐藏层数量。'
- en: '`t2u_decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of
    the “intermediate” (i.e., feed-forward) layer in the Transformer text-to-unit
    decoder.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer文本到单元解码器中“中间”（即前馈）层的维度。'
- en: '`t2u_decoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer text-to-unit decoder.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer文本到单元解码器中每个注意力层的注意力头数。'
- en: '`t2u_max_position_embeddings` (`int`, *optional*, defaults to 2048) — The maximum
    sequence length that this model text-to-unit component might ever be used with.
    Typically set this to something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t2u_max_position_embeddings` (`int`, *optional*, defaults to 2048) — 此模型文本到单元组件可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: Hifi-Gan Vocoder specific parameters
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Hifi-Gan声码器特定参数
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the output audio will be generated, expressed in hertz (Hz).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *optional*, defaults to 16000) — 生成输出音频的采样率，以赫兹（Hz）表示。'
- en: '`upsample_initial_channel` (`int`, *optional*, defaults to 512) — The number
    of input channels into the hifi-gan upsampling network. Applies to the vocoder
    only.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upsample_initial_channel` (`int`, *optional*, defaults to 512) — 输入通道数到hifi-gan上采样网络的数量。仅适用于声码器。'
- en: '`upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[5,
    4, 4, 2, 2]`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the vocoder upsampling network. The length of *upsample_rates* defines
    the number of convolutional layers and has to match the length of *upsample_kernel_sizes*.
    Applies to the vocoder only.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[5,
    4, 4, 2, 2]`) — 一个整数元组，定义声码器上采样网络中每个1D卷积层的步幅。*upsample_rates*的长度定义了卷积层的数量，并且必须与*upsample_kernel_sizes*的长度匹配。仅适用于声码器。'
- en: '`upsample_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[11, 8, 8, 4, 4]`) — A tuple of integers defining the kernel size of each
    1D convolutional layer in the vocoder upsampling network. The length of *upsample_kernel_sizes*
    defines the number of convolutional layers and has to match the length of *upsample_rates*.
    Applies to the vocoder only.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upsample_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[11, 8,
    8, 4, 4]`) — 一个整数元组，定义声码器上采样网络中每个1D卷积层的内核大小。*upsample_kernel_sizes*的长度定义了卷积层的数量，并且必须与*upsample_rates*的长度匹配。仅适用于声码器。'
- en: '`resblock_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[3, 7, 11]`) — A tuple of integers defining the kernel sizes of the vocoder
    1D convolutional layers in the multi-receptive field fusion (MRF) module. Applies
    to the vocoder only.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resblock_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[3, 7,
    11]`) — 一个整数元组，定义多接受域融合（MRF）模块中声码器1D卷积层的内核大小。仅适用于声码器。'
- en: '`resblock_dilation_sizes` (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*,
    defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — A nested tuple of integers
    defining the dilation rates of the vocoder dilated 1D convolutional layers in
    the multi-receptive field fusion (MRF) module. Applies to the vocoder only.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resblock_dilation_sizes` (`Tuple[Tuple[int]]` 或 `List[List[int]]`, *optional*,
    默认为`[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — 一个嵌套的整数元组，定义多接受域融合（MRF）模块中声码器扩张的1D卷积层的扩张率。仅适用于声码器。'
- en: '`leaky_relu_slope` (`float`, *optional*, defaults to 0.1) — The angle of the
    negative slope used by the leaky ReLU activation in the vocoder. Applies to the
    vocoder only.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leaky_relu_slope` (`float`, *optional*, 默认为0.1) — 在声码器中使用的leaky ReLU激活的负斜率角度。仅适用于声码器。'
- en: '`unit_hifi_gan_vocab_size` (`int`, *optional*, defaults to 10000) — Vocabulary
    size of the SeamlessM4T vocoder. Defines the number of different unit tokens that
    can be represented by the `inputs_ids` passed when calling the vocoder of [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel),
    [~SeamlessM4TForSpeechToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToSpeech)
    or [~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unit_hifi_gan_vocab_size` (`int`, *optional*, 默认为10000) — SeamlessM4T声码器的词汇大小。定义了在调用[~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)、[~SeamlessM4TForSpeechToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToSpeech)或[~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)时可以表示的不同单元标记数量。'
- en: '`unit_embed_dim` (`int`, *optional*, defaults to 1280) — The projection dimension
    of the input ids given to the hifi-gan vocoder. Applies to the vocoder only.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unit_embed_dim` (`int`, *optional*, 默认为1280) — 提供给hifi-gan声码器的输入id的投影维度。仅适用于声码器。'
- en: '`lang_embed_dim` (`int`, *optional*, defaults to 256) — The projection dimension
    of the target language given to the hifi-gan vocoder. Applies to the vocoder only.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lang_embed_dim` (`int`, *optional*, 默认为256) — 提供给hifi-gan声码器的目标语言的投影维度。仅适用于声码器。'
- en: '`spkr_embed_dim` (`int`, *optional*, defaults to 256) — The projection dimension
    of the speaker id given to the hifi-gan vocoder. Applies to the vocoder only.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spkr_embed_dim` (`int`, *optional*, 默认为256) — 提供给hifi-gan声码器的说话人id的投影维度。仅适用于声码器。'
- en: '`vocoder_num_langs` (`int`, *optional*, defaults to 36) — Number of langs supported
    by the vocoder. Might be different from `t2u_num_langs`.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocoder_num_langs` (`int`, *optional*, 默认为36) — 声码器支持的语言数量。可能与`t2u_num_langs`不同。'
- en: '`vocoder_num_spkrs` (`int`, *optional*, defaults to 200) — Number of speakers
    supported by the vocoder.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocoder_num_spkrs` (`int`, *optional*, 默认为200) — 声码器支持的说话人数量。'
- en: '`variance_predictor_kernel_size` (`int`, *optional*, defaults to 3) — Kernel
    size of the duration predictor. Applies to the vocoder only.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`variance_predictor_kernel_size` (`int`, *optional*, 默认为3) — 持续预测器的内核大小。仅适用于声码器。'
- en: '`var_pred_dropout` (`float`, *optional*, defaults to 0.5) — The dropout probabilitiy
    of the duration predictor. Applies to the vocoder only.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`var_pred_dropout` (`float`, *optional*, 默认为0.5) — 持续预测器的dropout概率。仅适用于声码器。'
- en: '`vocoder_offset` (`int`, *optional*, defaults to 4) — Offset the unit token
    ids by this number to account for symbol tokens. Applies to the vocoder only.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocoder_offset` (`int`, *optional*, 默认为4) — 将单元标记id偏移此数字以考虑符号标记。仅适用于声码器。'
- en: This is the configuration class to store the configuration of a [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel).
    It is used to instantiate an SeamlessM4T model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the SeamlessM4T [“facebook/hf-seamless-m4t-medium”](https://huggingface.co/%22facebook/hf-seamless-m4t-medium%22)
    architecture.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)配置的配置类。根据指定的参数实例化一个SeamlessM4T模型，定义模型架构。使用默认值实例化配置将产生类似于SeamlessM4T[“facebook/hf-seamless-m4t-medium”](https://huggingface.co/%22facebook/hf-seamless-m4t-medium%22)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: '[PRE19]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: SeamlessM4TTokenizer
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TTokenizer
- en: '### `class transformers.SeamlessM4TTokenizer`'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L54)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L54)'
- en: '[PRE20]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开头标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用特殊标记构建序列时，这不是用于序列开头的标记。所使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用特殊标记构建序列时，这不是用于序列结尾的标记。所使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如，用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于序列分类时使用的分类器标记（对整个序列进行分类，而不是对每个标记进行分类）。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时。'
- en: '`tokenizer_file` (`str`, *optional*) — The path to a tokenizer file to use
    instead of the vocab file.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file` (`str`, *optional*) — 要使用的分词器文件的路径，而不是词汇文件。'
- en: '`src_lang` (`str`, *optional*, defaults to `"eng"`) — The language to use as
    source language for translation.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang` (`str`, *optional*, defaults to `"eng"`) — 用作翻译源语言的语言。'
- en: '`tgt_lang` (`str`, *optional*, defaults to `"fra"`) — The language to use as
    target language for translation.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *optional*, defaults to `"fra"`) — 用作翻译目标语言的语言。'
- en: '`sp_model_kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments
    to pass to the model initialization.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs` (`Dict[str, Any]`, *optional*) — 传递给模型初始化的额外关键字参数。'
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) — A tuple or a list of additional special tokens. Can be used to specify
    the list of languages that will be supported by the tokenizer.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens`（元组或`str`或`tokenizers.AddedToken`的列表，*optional*）
    — 附加特殊标记的元组或列表。可用于指定将由分词器支持的语言列表。'
- en: Construct a SeamlessM4T tokenizer.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个SeamlessM4T分词器。
- en: Adapted from [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer).
    Based on [SentencePiece](https://github.com/google/sentencepiece).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 改编自[RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)和[XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)。基于[SentencePiece](https://github.com/google/sentencepiece)。
- en: The tokenization method is `<language code> <tokens> <eos>` for source language
    documents, and `<eos> <language code> <tokens> <eos>` for target language documents.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 源语言文档的标记方法为`<language code> <tokens> <eos>`，目标语言文档的标记方法为`<eos> <language code>
    <tokens> <eos>`。
- en: 'Examples:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE21]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#### `__call__`'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L217)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L217)'
- en: '[PRE22]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次。每个序列可以是一个字符串或一个字符串列表（预先标记化的字符串）。如果序列以字符串列表（预先标记化）的形式提供，则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`True`）
    — 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引）：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度（使用参数`max_length`）或填充到模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：无填充（即可以输出长度不同的序列批次）。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*） — 如果设置，将填充序列到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta).
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力为`>= 7.5`（Volta）。
- en: '`src_lang` (`str`, *optional*) — A string representing the source language.
    If not specified, the last `src_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang`（`str`，*可选*） — 表示源语言的字符串。如果未指定，则将使用上次指定的`src_lang`（在初始化时或调用此分词器时）。'
- en: '`tgt_lang` (`str`, *optional*) — A string representing the target language.
    If not specified, the last `tgt_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang`（`str`，*可选*） — 表示目标语言的字符串。如果未指定，则将使用上次指定的`tgt_lang`（在初始化时或调用此分词器时）。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（*可选*） — 传递给[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)的剩余关键字参数字典。'
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L347)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L347)'
- en: '[PRE23]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An NLLB sequence has the following
    format, where `X` represents the sequence:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。NLLB 序列具有以下格式，其中`X`表示序列：
- en: '`input_ids` (for encoder) `X [eos, src_lang_code]`'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（用于编码器）`X [eos, src_lang_code]`'
- en: '`decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`:（用于解码器）`X [eos, tgt_lang_code]`'
- en: BOS is never used. Pairs of sequences are not the expected use case, but they
    will be handled without a separator.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: BOS 从不使用。序列对不是预期的用例，但它们将在没有分隔符的情况下处理。
- en: '#### `get_special_tokens_mask`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L316)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L316)'
- en: '[PRE24]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`（`bool`，*可选*，默认为`False`） — 标记列表是否已经为模型格式化了特殊标记。'
- en: Returns
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L375)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L375)'
- en: '[PRE25]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 零的列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. nllb does not make use of token type ids, therefore a list of zeros is returned.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。nllb 不使用标记类型ID，因此返回一个零列表。
- en: '#### `save_vocabulary`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L498)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L498)'
- en: '[PRE26]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: SeamlessM4TTokenizerFast
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TTokenizerFast
- en: '### `class transformers.SeamlessM4TTokenizerFast`'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L54)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L54)'
- en: '[PRE27]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`, *optional*) — Path to the vocabulary file.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`, *可选*) — 词汇表文件的路径。'
- en: '`tokenizer_file` (`str`, *optional*) — The path to a tokenizer file to use
    instead of the vocab file.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file` (`str`, *可选*) — 要使用的分词器文件的路径，而不是词汇表文件。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开头标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是 `cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是 `sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *可选*, 默认为 `"<s>"`) — 在进行序列分类（整个序列而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`src_lang` (`str`, *optional*, defaults to `"eng"`) — The language to use as
    source language for translation.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang` (`str`, *可选*, 默认为 `"eng"`) — 用作翻译源语言的语言。'
- en: '`tgt_lang` (`str`, *optional*, defaults to `"fra"`) — The language to use as
    target language for translation.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *可选*, 默认为 `"fra"`) — 用作翻译目标语言的语言。'
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) — A tuple or a list of additional special tokens.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens`（`str` 或 `tokenizers.AddedToken` 的元组或列表，*可选*） —
    附加特殊标记的元组或列表。'
- en: Construct a “fast” SeamlessM4T tokenizer (backed by HuggingFace’s *tokenizers*
    library). Based on [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速” SeamlessM4T 分词器（由HuggingFace的 *tokenizers* 库支持）。基于[BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models)。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: The tokenization method is `<language code> <tokens> <eos>` for source language
    documents, and `<eos> <language code> <tokens> <eos>` for target language documents.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 对于源语言文档，标记方法是 `<语言代码> <标记> <eos>`，对于目标语言文档，标记方法是 `<eos> <语言代码> <标记> <eos>`。
- en: 'Examples:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE28]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#### `__call__`'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L390)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L390)'
- en: '[PRE29]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置
    `is_split_into_words=True`（以消除与序列批次的歧义）。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`True`）—
    选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest''`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则填充到模型可接受的最大输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_pad''`（默认）：不填充（即，可以输出具有不同长度序列的批次）。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta).
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于启用NVIDIA硬件上的Tensor Cores特别有用，其计算能力为`>= 7.5`（Volta）。
- en: '`src_lang` (`str`, *optional*) — A string representing the source language.
    If not specified, the last `src_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang`（`str`，*可选*）— 表示源语言的字符串。如果未指定，则将使用上次指定的`src_lang`（在初始化期间或在调用此分词器时）。'
- en: '`tgt_lang` (`str`, *optional*) — A string representing the target language.
    If not specified, the last `tgt_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang`（`str`，*可选*）— 表示目标语言的字符串。如果未指定，则将使用上次指定的`tgt_lang`（在初始化期间或在调用此分词器时）。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [PreTrainedTokenizerFast.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（*可选*）— 将传递给[PreTrainedTokenizerFast.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)的剩余关键字参数字典。'
- en: SeamlessM4TFeatureExtractor
  id: totrans-461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TFeatureExtractor
- en: '### `class transformers.SeamlessM4TFeatureExtractor`'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L38)'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L38)'
- en: '[PRE30]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_size` (`int`, *optional*, defaults to 80) — The feature dimension
    of the extracted features.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size`（`int`，*可选*，默认为80）— 提取特征的特征维度。'
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`（`int`，*可选*，默认为16000）— 应数字化音频文件的采样率，以赫兹（Hz）表示。'
- en: '`num_mel_bins` (`int`, *optional*, defaults to 80) — Number of Mel-frequency
    bins.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_mel_bins`（`int`，*可选*，默认为80）— Mel频率箱数。'
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — The value that is
    used to fill the padding vectors.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value`（`float`，*可选*，默认为0.0）— 用于填充向量的值。'
- en: '`stride` (`int`, *optional*, defaults to 2) — Stride used to reshape audios
    from shape (batch_size,num_frames,num_mel_bins) to (batch_size,num_frames//stride,num_mel_bins*stride).'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`（`int`，*可选*，默认为2）— 用于将音频从形状（batch_size，num_frames，num_mel_bins）重塑为（batch_size，num_frames//stride，num_mel_bins*stride）的步幅。'
- en: Constructs a SeamlessM4T feature extractor.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个SeamlessM4T特征提取器。
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 此特征提取器继承自[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: This class extracts mel-filter bank features from raw speech.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 该类从原始语音中提取mel滤波器组特征。
- en: '#### `__call__`'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L144)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L144)'
- en: '[PRE31]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`raw_speech` (`np.ndarray`, `torch.Tensor`, `List[float]`, `List[np.ndarray]`,
    `List[torch.Tensor]`, —'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_speech` (`np.ndarray`, `torch.Tensor`, `List[float]`, `List[np.ndarray]`,
    `List[torch.Tensor]`, —'
- en: '`List[List[float]],` `List[List[List[float]]]`) — The sequence or batch of
    sequences to be padded. Each sequence can be a numpy array, a torch tensor, a
    list of float values, a list of numpy arrays, a list of torch tensors, a list
    of list of float values or a list of a list of list of float values. If `raw_speech`
    is a one-dimensional `np.ndarray`, `torch.Tensor` or a `List[float]`, `raw_speech`
    is considered a single-channel, single-sample sound. In all other cases, the first
    dimension of `raw_speech`, whether from an `np.ndarray`, a `torch.Tensor` or a
    `List[...]`, corresponds to the number of samples in the batch, and the number
    of channels (i.e. mono or stereo character) is derived from the other dimensions
    (1D -> single-channel waveform batches; 2D-> stereo-channel waveform batches).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`List[List[float]],` `List[List[List[float]]]`) — 要填充的序列或序列批次。每个序列可以是一个 numpy
    数组、一个 torch 张量、一个浮点值列表、一个 numpy 数组列表、一个 torch 张量列表、一个浮点值列表的列表或一个浮点值列表的列表的列表。如果
    `raw_speech` 是一维的 `np.ndarray`、`torch.Tensor` 或 `List[float]`，则将 `raw_speech`
    视为单声道、单样本声音。在所有其他情况下，无论是来自 `np.ndarray`、`torch.Tensor` 还是 `List[...]` 的第一个维度，都对应于批次中的样本数，通道数（即单声道或立体声特征）从其他维度中派生（1D
    -> 单声道波形批次；2D -> 立体声波形批次）。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, 默认为 `True`) — 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引）：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 使用参数 `max_length` 指定的最大长度进行填充，或者如果未提供该参数，则填充到模型可接受的最大输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：无填充（即，可以输出长度不同的序列批次）。'
- en: '`pad_to_multiple_of` (`int`, *optional*, defaults to 2) — If set will pad the
    sequence to a multiple of the provided value.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*, 默认为 2) — 如果设置，将序列填充到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力 `>= 7.5`（Volta），或者对于受益于序列长度为 128 的倍数的
    TPU。
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) — 返回列表的最大长度和可选填充长度（见上文）。'
- en: '`truncation` (`bool`) — Activates truncation to cut input sequences longer
    than *max_length* to *max_length*.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`) — 激活截断，将输入序列截断为长于 *max_length* 的部分至 *max_length*。'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractor’s default.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定 feature_extractor
    的默认设置返回注意力掩码。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: For SeamlessM4T models, `attention_mask` should always be passed for batched
    inference, to avoid subtle bugs.
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 SeamlessM4T 模型，在批量推理时应始终传递 `attention_mask`，以避免细微错误。
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *optional*) — `raw_speech` 输入的采样率。强烈建议在前向调用时传递 `sampling_rate`，以防止潜在错误。'
- en: '`do_normalize_per_mel_bins` (`bool`, *optional*, defaults to `True`) — Whether
    or not to zero-mean unit-variance normalize the input per mel-channel.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize_per_mel_bins` (`bool`, *optional*, 默认为 `True`) — 是否对每个 mel 通道的输入进行零均值单位方差归一化。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to the tokenizer or the feature extractor.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 将传递给分词器或特征提取器的剩余关键字参数字典。'
- en: Main method to featurize and prepare for the model one or several sequence(s).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 对一个或多个序列进行特征化和准备模型的主要方法。
- en: SeamlessM4TProcessor
  id: totrans-499
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TProcessor
- en: '### `class transformers.SeamlessM4TProcessor`'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L22)'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L22)'
- en: '[PRE32]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_extractor` ([SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor))
    — The audio processor is a required input.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` ([SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor))
    — 音频处理器是必需的输入。'
- en: '`tokenizer` ([SeamlessM4TTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast))
    — The tokenizer is a required input.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([SeamlessM4TTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast))
    — 必需的输入是tokenizer。'
- en: Constructs a SeamlessM4T processor which wraps a SeamlessM4T feature extractor
    and a SeamlessM4T tokenizer into a single processor.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个SeamlessM4T处理器，将SeamlessM4T特征提取器和SeamlessM4T tokenizer封装成一个单一处理器。
- en: '[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    offers all the functionalities of [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    and [SeamlessM4TTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor.__call__)
    and `decode()` for more information.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)提供了[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)和[SeamlessM4TTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast)的所有功能。查看[**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor.__call__)和`decode()`获取更多信息。'
- en: '#### `__call__`'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L44)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L44)'
- en: '[PRE33]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence can be a string or a list of strings (pretokenized
    string). If the sequences are provided as list of strings (pretokenized), you
    must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。'
- en: '`audios` (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`)
    — The audio or batch of audios to be prepared. Each audio can be NumPy array or
    PyTorch tensor. In case of a NumPy array/PyTorch tensor, each audio should be
    of shape (C, T), where C is a number of channels, and T the sample length of the
    audio.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audios` (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`)
    — 要准备的音频或音频批次。每个音频可以是NumPy数组或PyTorch张量。对于NumPy数组/PyTorch张量，每个音频的形状应为(C, T)，其中C是通道数，T是音频的采样长度。'
- en: '`src_lang` (`str`, *optional*) — The language code of the input texts/audios.
    If not specified, the last `src_lang` specified will be used.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang` (`str`, *可选*) — 输入文本/音频的语言代码。如果未指定，将使用最后指定的`src_lang`。'
- en: '`tgt_lang` (`str`, *optional*) — The code of the target language. If not specified,
    the last `tgt_lang` specified will be used.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *可选*) — 目标语言的代码。如果未指定，将使用最后指定的`tgt_lang`。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to the feature extractor and/or the tokenizer.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*可选*) — 将传递给特征提取器和/或tokenizer的剩余关键字参数字典。'
- en: Returns
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '一个带有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding):'
- en: '`input_ids` — List of token ids to be fed to a model. Returned when `text`
    is not `None`.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要输入模型的标记id列表。当`text`不是`None`时返回。'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names` and if `text` is not `None`).'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些标记应该被模型关注的索引列表（当`return_attention_mask=True`或*“attention_mask”*在`self.model_input_names`中，且`text`不是`None`时）。'
- en: '`input_features` — Audio input features to be fed to a model. Returned when
    `audios` is not `None`.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` — 要输入模型的音频输入特征。当`audios`不是`None`时返回。'
- en: Main method to prepare for the model one or several sequences(s) and audio(s).
    This method forwards the `text` and `kwargs` arguments to SeamlessM4TTokenizerFast’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast.__call__)
    if `text` is not `None` to encode the text. To prepare the audio(s), this method
    forwards the `audios` and `kwrags` arguments to SeamlessM4TFeatureExtractor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    if `audios` is not `None`. Please refer to the doctsring of the above two methods
    for more information.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 准备模型的主要方法是准备一个或多个序列和音频。如果`text`不是`None`，则将`text`和`kwargs`参数转发给SeamlessM4TTokenizerFast的[**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast.__call__)来对文本进行编码。要准备音频，如果`audios`不是`None`，则将`audios`和`kwrags`参数转发给SeamlessM4TFeatureExtractor的[**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)。更多信息请参考上述两种方法的文档。
- en: SeamlessM4TCodeHifiGan
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TCodeHifiGan
- en: '### `class transformers.SeamlessM4TCodeHifiGan`'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TCodeHifiGan`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2477)'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2477)'
- en: '[PRE34]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Code HiFi-GAN vocoder as described in this [repository](https://github.com/facebookresearch/speech-resynthesis).
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 根据此[存储库](https://github.com/facebookresearch/speech-resynthesis)中描述的HiFi-GAN声码器代码。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2557)'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2557)'
- en: '[PRE35]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [SeamlessM4TTextToUnitForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTextToUnitForConditionalGeneration).
    [What are input IDs?](../glossary#input-ids)
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[SeamlessM4TTextToUnitForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTextToUnitForConditionalGeneration)获取索引。[什么是输入ID？](../glossary#input-ids)
- en: '`spkr_id` (`int`, *optional*) — The id of the speaker used for speech synthesis.
    Must be lower than `config.vocoder_num_spkrs`.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spkr_id` (`int`, *可选*) — 用于语音合成的说话者ID。必须小于`config.vocoder_num_spkrs`。'
- en: '`tgt_lang` (`str`, *optional*) — The language id to use as target language
    for translation.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言ID。'
- en: SeamlessM4THifiGan
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4THifiGan
- en: '### `class transformers.SeamlessM4THifiGan`'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4THifiGan`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2405)'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2405)'
- en: '[PRE36]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '#### `forward`'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2440)'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2440)'
- en: '[PRE37]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`spectrogram` (`torch.FloatTensor`) — Tensor containing the log-mel spectrograms.
    Can be batched and of shape `(batch_size, sequence_length, model_in_dim)`, or
    un-batched and of shape `(sequence_length, model_in_dim)`. Note that `model_in_dim`
    is the sum of `config.unit_embed_dim`, `config.lang_embed_dim` and `config.spkr_embed_dim`.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spectrogram` (`torch.FloatTensor`) — 包含对数梅尔频谱图的张量。可以是批处理的，形状为`(batch_size,
    sequence_length, model_in_dim)`，也可以是未经批处理的，形状为`(sequence_length, model_in_dim)`。请注意，`model_in_dim`是`config.unit_embed_dim`、`config.lang_embed_dim`和`config.spkr_embed_dim`的总和。'
- en: Returns
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor`'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`'
- en: Tensor containing the speech waveform. If the input spectrogram is batched,
    will be of shape `(batch_size, num_frames,)`. If un-batched, will be of shape
    `(num_frames,)`.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 包含语音波形的张量。如果输入的频谱图是批处理的，则形状为`(batch_size, num_frames,)`。如果未经批处理，则形状为`(num_frames,)`。
- en: Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel
    spectrograms returns a batch of speech waveforms. Passing a single, un-batched
    log-mel spectrogram returns a single, un-batched speech waveform.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 将对数梅尔频谱图转换为语音波形。传递一批对数梅尔频谱图将返回一批语音波形。传递单个、未经批处理的对数梅尔频谱图将返回单个、未经批处理的语音波形。
- en: SeamlessM4TTextToUnitModel
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TTextToUnitModel
- en: '### `class transformers.SeamlessM4TTextToUnitModel`'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TTextToUnitModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2039)'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2039)'
- en: '[PRE38]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`embed_tokens_decoder` (`nn.Embedding`, *optional*) — input embedding of the
    decoder.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_tokens_decoder` (`nn.Embedding`, *可选*) — 解码器的输入嵌入。'
- en: Transformer bare text-to-unit encoder-decoder. The encoder is a `SeamlessM4TEncoder`
    without embeddings and the decoder is a `SeamlessM4TDecoder`. This model is a
    PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer裸文本到单元编码器-解码器。编码器是一个没有嵌入的`SeamlessM4TEncoder`，解码器是一个`SeamlessM4TDecoder`。这个模型是PyTorch
    [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: SeamlessM4TTextToUnitForConditionalGeneration
  id: totrans-561
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SeamlessM4TTextToUnitForConditionalGeneration
- en: '### `class transformers.SeamlessM4TTextToUnitForConditionalGeneration`'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SeamlessM4TTextToUnitForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2128)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2128)'
- en: '[PRE39]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig)）-
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`embed_tokens_decoder` (`nn.Embedding`, *optional*) — input embedding of the
    decoder.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_tokens_decoder`（`nn.Embedding`，*可选*）- 解码器的输入嵌入。'
- en: Transformer text-to-unit encoder-decoder with a language model head. The base
    encoder-decoder model is a `SeamlessM4TTextToUnit`. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 带有语言模型头的文本到单元编码器-解码器。基本编码器-解码器模型是`SeamlessM4TTextToUnit`。这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2181)'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2181)'
- en: '[PRE40]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 输入序列标记的索引。'
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-574
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)或[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    避免对填充标记索引执行注意力的掩码。掩码值选在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-577
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-578
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）-
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Bart使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则可以选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  id: totrans-584
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于翻译和摘要训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将通过将`input_ids`向右移动来创建此张量，以进行去噪预训练，遵循论文。
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: If you want to change padding behavior, you should read `modeling_bart._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改填充行为，您应该阅读`modeling_bart._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包含（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）—
    元组由长度为`config.n_layers`的`tuple(torch.FloatTensor)`组成，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量，以及2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-590
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size,
    1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，可以选择仅输入最后的`decoder_inputs_embeds`（请参阅`past_key_values`）。如果您想要更多控制权来将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应在`[-100,
    0, ..., config.vocab_size]`范围内（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`范围内的标记。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: The [SeamlessM4TTextToUnitForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTextToUnitForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '[SeamlessM4TTextToUnitForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTextToUnitForConditionalGeneration)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
