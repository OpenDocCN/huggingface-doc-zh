- en: Token merging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记合并
- en: 'Original text: [https://huggingface.co/docs/diffusers/optimization/tome](https://huggingface.co/docs/diffusers/optimization/tome)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/optimization/tome](https://huggingface.co/docs/diffusers/optimization/tome)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Token merging](https://huggingface.co/papers/2303.17604) (ToMe) merges redundant
    tokens/patches progressively in the forward pass of a Transformer-based network
    which can speed-up the inference latency of [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[标记合并](https://huggingface.co/papers/2303.17604)（ToMe）在基于Transformer的网络的前向传递中逐渐合并冗余标记/补丁，可以加速[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)的推理延迟。'
- en: 'Install ToMe from `pip`:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从`pip`安装ToMe：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can use ToMe from the [`tomesd`](https://github.com/dbolya/tomesd) library
    with the [`apply_patch`](https://github.com/dbolya/tomesd?tab=readme-ov-file#usage)
    function:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用[`tomesd`](https://github.com/dbolya/tomesd)库中的ToMe，使用[`apply_patch`](https://github.com/dbolya/tomesd?tab=readme-ov-file#usage)函数：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `apply_patch` function exposes a number of [arguments](https://github.com/dbolya/tomesd#usage)
    to help strike a balance between pipeline inference speed and the quality of the
    generated tokens. The most important argument is `ratio` which controls the number
    of tokens that are merged during the forward pass.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_patch`函数公开了一些[参数](https://github.com/dbolya/tomesd#usage)，以帮助在管道推理速度和生成的标记质量之间取得平衡。最重要的参数是`ratio`，它控制在前向传递期间合并的标记数量。'
- en: As reported in the [paper](https://huggingface.co/papers/2303.17604), ToMe can
    greatly preserve the quality of the generated images while boosting inference
    speed. By increasing the `ratio`, you can speed-up inference even further, but
    at the cost of some degraded image quality.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[论文](https://huggingface.co/papers/2303.17604)中所报道的，ToMe可以在提升推理速度的同时极大地保留生成图像的质量。通过增加`ratio`，您可以进一步加快推理速度，但会以一定的图像质量降低为代价。
- en: 'To test the quality of the generated images, we sampled a few prompts from
    [Parti Prompts](https://parti.research.google/) and performed inference with the
    [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    with the following settings:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试生成图像的质量，我们从[Parti Prompts](https://parti.research.google/)中抽取了一些提示，并使用以下设置对[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)进行推理：
- en: '![](../Images/5751d285c18e53824ccfcaac8aef2dec.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5751d285c18e53824ccfcaac8aef2dec.png)'
- en: We didn’t notice any significant decrease in the quality of the generated samples,
    and you can check out the generated samples in this [WandB report](https://wandb.ai/sayakpaul/tomesd-results/runs/23j4bj3i?workspace=).
    If you’re interested in reproducing this experiment, use this [script](https://gist.github.com/sayakpaul/8cac98d7f22399085a060992f411ecbd).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有注意到生成样本质量有明显下降，您可以在这个[WandB报告](https://wandb.ai/sayakpaul/tomesd-results/runs/23j4bj3i?workspace=)中查看生成的样本。如果您有兴趣重现这个实验，请使用这个[脚本](https://gist.github.com/sayakpaul/8cac98d7f22399085a060992f411ecbd)。
- en: Benchmarks
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: 'We also benchmarked the impact of `tomesd` on the [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    with [xFormers](https://huggingface.co/docs/diffusers/optimization/xformers) enabled
    across several image resolutions. The results are obtained from A100 and V100
    GPUs in the following development environment:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对在几种图像分辨率下启用[xFormers](https://huggingface.co/docs/diffusers/optimization/xformers)的[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)上的`tomesd`的影响进行了基准测试。结果是从A100和V100
    GPU在以下开发环境中获得的：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To reproduce this benchmark, feel free to use this [script](https://gist.github.com/sayakpaul/27aec6bca7eb7b0e0aa4112205850335).
    The results are reported in seconds, and where applicable we report the speed-up
    percentage over the vanilla pipeline when using ToMe and ToMe + xFormers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要重现这个基准测试，请随时使用这个[脚本](https://gist.github.com/sayakpaul/27aec6bca7eb7b0e0aa4112205850335)。结果以秒为单位报告，必要时我们会报告使用ToMe和ToMe
    + xFormers时相对于普通管道的加速百分比。
- en: '| **GPU** | **Resolution** | **Batch size** | **Vanilla** | **ToMe** | **ToMe
    + xFormers** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **GPU** | **Resolution** | **Batch size** | **Vanilla** | **ToMe** | **ToMe
    + xFormers** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **A100** | 512 | 10 | 6.88 | 5.26 (+23.55%) | 4.69 (+31.83%) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **A100** | 512 | 10 | 6.88 | 5.26 (+23.55%) | 4.69 (+31.83%) |'
- en: '|  | 768 | 10 | OOM | 14.71 | 11 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | 768 | 10 | OOM | 14.71 | 11 |'
- en: '|  |  | 8 | OOM | 11.56 | 8.84 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 8 | OOM | 11.56 | 8.84 |'
- en: '|  |  | 4 | OOM | 5.98 | 4.66 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 4 | OOM | 5.98 | 4.66 |'
- en: '|  |  | 2 | 4.99 | 3.24 (+35.07%) | 2.1 (+37.88%) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2 | 4.99 | 3.24 (+35.07%) | 2.1 (+37.88%) |'
- en: '|  |  | 1 | 3.29 | 2.24 (+31.91%) | 2.03 (+38.3%) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1 | 3.29 | 2.24 (+31.91%) | 2.03 (+38.3%) |'
- en: '|  | 1024 | 10 | OOM | OOM | OOM |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | 1024 | 10 | OOM | OOM | OOM |'
- en: '|  |  | 8 | OOM | OOM | OOM |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 8 | OOM | OOM | OOM |'
- en: '|  |  | 4 | OOM | 12.51 | 9.09 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 4 | OOM | 12.51 | 9.09 |'
- en: '|  |  | 2 | OOM | 6.52 | 4.96 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2 | OOM | 6.52 | 4.96 |'
- en: '|  |  | 1 | 6.4 | 3.61 (+43.59%) | 2.81 (+56.09%) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1 | 6.4 | 3.61 (+43.59%) | 2.81 (+56.09%) |'
- en: '| **V100** | 512 | 10 | OOM | 10.03 | 9.29 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **V100** | 512 | 10 | OOM | 10.03 | 9.29 |'
- en: '|  |  | 8 | OOM | 8.05 | 7.47 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 8 | OOM | 8.05 | 7.47 |'
- en: '|  |  | 4 | 5.7 | 4.3 (+24.56%) | 3.98 (+30.18%) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 4 | 5.7 | 4.3 (+24.56%) | 3.98 (+30.18%) |'
- en: '|  |  | 2 | 3.14 | 2.43 (+22.61%) | 2.27 (+27.71%) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2 | 3.14 | 2.43 (+22.61%) | 2.27 (+27.71%) |'
- en: '|  |  | 1 | 1.88 | 1.57 (+16.49%) | 1.57 (+16.49%) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1 | 1.88 | 1.57 (+16.49%) | 1.57 (+16.49%) |'
- en: '|  | 768 | 10 | OOM | OOM | 23.67 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | 768 | 10 | OOM | OOM | 23.67 |'
- en: '|  |  | 8 | OOM | OOM | 18.81 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 8 | OOM | OOM | 18.81 |'
- en: '|  |  | 4 | OOM | 11.81 | 9.7 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 4 | OOM | 11.81 | 9.7 |'
- en: '|  |  | 2 | OOM | 6.27 | 5.2 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2 | OOM | 6.27 | 5.2 |'
- en: '|  |  | 1 | 5.43 | 3.38 (+37.75%) | 2.82 (+48.07%) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1 | 5.43 | 3.38 (+37.75%) | 2.82 (+48.07%) |'
- en: '|  | 1024 | 10 | OOM | OOM | OOM |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | 1024 | 10 | OOM | OOM | OOM |'
- en: '|  |  | 8 | OOM | OOM | OOM |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 8 | OOM | OOM | OOM |'
- en: '|  |  | 4 | OOM | OOM | 19.35 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 4 | OOM | OOM | 19.35 |'
- en: '|  |  | 2 | OOM | 13 | 10.78 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2 | OOM | 13 | 10.78 |'
- en: '|  |  | 1 | OOM | 6.66 | 5.54 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1 | OOM | 6.66 | 5.54 |'
- en: As seen in the tables above, the speed-up from `tomesd` becomes more pronounced
    for larger image resolutions. It is also interesting to note that with `tomesd`,
    it is possible to run the pipeline on a higher resolution like 1024x1024\. You
    may be able to speed-up inference even more with [`torch.compile`](torch2.0).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如上表所示，`tomesd` 的加速效果在更大的图像分辨率下变得更加明显。有趣的是，使用 `tomesd`，可以在更高分辨率（如1024x1024）上运行流水线。您可以通过
    [`torch.compile`](torch2.0) 进一步加快推理速度。
