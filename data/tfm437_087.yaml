- en: How to add a model to ğŸ¤— Transformers?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°†æ¨¡å‹æ·»åŠ åˆ°ğŸ¤— Transformersï¼Ÿ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/add_new_model](https://huggingface.co/docs/transformers/v4.37.2/en/add_new_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/add_new_model](https://huggingface.co/docs/transformers/v4.37.2/en/add_new_model)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The ğŸ¤— Transformers library is often able to offer new models thanks to community
    contributors. But this can be a challenging project and requires an in-depth knowledge
    of the ğŸ¤— Transformers library and the model to implement. At Hugging Face, weâ€™re
    trying to empower more of the community to actively add models and weâ€™ve put together
    this guide to walk you through the process of adding a PyTorch model (make sure
    you have [PyTorch installed](https://pytorch.org/get-started/locally/)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersåº“é€šå¸¸èƒ½å¤Ÿé€šè¿‡ç¤¾åŒºè´¡çŒ®è€…æä¾›æ–°æ¨¡å‹ã€‚ä½†è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¡¹ç›®ï¼Œéœ€è¦æ·±å…¥äº†è§£ğŸ¤— Transformersåº“å’Œè¦å®ç°çš„æ¨¡å‹ã€‚åœ¨Hugging
    Faceï¼Œæˆ‘ä»¬æ­£åœ¨åŠªåŠ›èµ‹äºˆæ›´å¤šç¤¾åŒºæˆå‘˜ç§¯ææ·»åŠ æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶ä¸ºæ‚¨æä¾›è¿™ä¸ªæŒ‡å—ï¼Œä»¥æŒ‡å¯¼æ‚¨æ·»åŠ ä¸€ä¸ªPyTorchæ¨¡å‹ï¼ˆç¡®ä¿æ‚¨å·²ç»[å®‰è£…äº†PyTorch](https://pytorch.org/get-started/locally/)ï¼‰ã€‚
- en: If youâ€™re interested in implementing a TensorFlow model, take a look at the
    [How to convert a ğŸ¤— Transformers model to TensorFlow](add_tensorflow_model) guide!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£å®ç°ä¸€ä¸ªTensorFlowæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[å¦‚ä½•å°†ğŸ¤— Transformersæ¨¡å‹è½¬æ¢ä¸ºTensorFlow](add_tensorflow_model)æŒ‡å—ï¼
- en: 'Along the way, youâ€™ll:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¿é€”ï¼Œæ‚¨å°†ï¼š
- en: get insights into open-source best practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº†è§£å¼€æºæœ€ä½³å®è·µ
- en: understand the design principles behind one of the most popular deep learning
    libraries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº†è§£æœ€å—æ¬¢è¿çš„æ·±åº¦å­¦ä¹ åº“èƒŒåçš„è®¾è®¡åŸåˆ™
- en: learn how to efficiently test large models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•æœ‰æ•ˆåœ°æµ‹è¯•å¤§å‹æ¨¡å‹
- en: learn how to integrate Python utilities like `black`, `ruff`, and `make fix-copies`
    to ensure clean and readable code
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•é›†æˆPythonå®ç”¨ç¨‹åºï¼Œå¦‚`black`ã€`ruff`å’Œ`make fix-copies`ï¼Œä»¥ç¡®ä¿ä»£ç æ•´æ´å¯è¯»
- en: A Hugging Face team member will be available to help you along the way so youâ€™ll
    never be alone. ğŸ¤— â¤ï¸
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Faceå›¢é˜Ÿæˆå‘˜å°†éšæ—¶ä¸ºæ‚¨æä¾›å¸®åŠ©ï¼Œå› æ­¤æ‚¨æ°¸è¿œä¸ä¼šå­¤å•ã€‚ğŸ¤— â¤ï¸
- en: To get started, open a [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml)
    issue for the model you want to see in ğŸ¤— Transformers. If youâ€™re not especially
    picky about contributing a specific model, you can filter by the [New model label](https://github.com/huggingface/transformers/labels/New%20model)
    to see if there are any unclaimed model requests and work on it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ï¼Œè¯·ä¸ºæ‚¨æƒ³åœ¨ğŸ¤— Transformersä¸­çœ‹åˆ°çš„æ¨¡å‹æ‰“å¼€ä¸€ä¸ª[æ–°æ¨¡å‹æ·»åŠ ](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml)é—®é¢˜ã€‚å¦‚æœæ‚¨å¯¹è´¡çŒ®ç‰¹å®šæ¨¡å‹ä¸æ˜¯ç‰¹åˆ«æŒ‘å‰”ï¼Œæ‚¨å¯ä»¥æŒ‰[New
    model label](https://github.com/huggingface/transformers/labels/New%20model)è¿›è¡Œç­›é€‰ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•æœªè®¤é¢†çš„æ¨¡å‹è¯·æ±‚å¹¶å¼€å§‹å¤„ç†ã€‚
- en: Once youâ€™ve opened a new model request, the first step is to get familiar with
    ğŸ¤— Transformers if you arenâ€™t already!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨æ‰“å¼€äº†ä¸€ä¸ªæ–°æ¨¡å‹è¯·æ±‚ï¼Œå¦‚æœæ‚¨è¿˜ä¸ç†Ÿæ‚‰ğŸ¤— Transformersï¼Œç¬¬ä¸€æ­¥æ˜¯ç†Ÿæ‚‰å®ƒï¼
- en: General overview of ğŸ¤— Transformers
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersçš„æ¦‚è¿°
- en: First, you should get a general overview of ğŸ¤— Transformers. ğŸ¤— Transformers is
    a very opinionated library, so there is a chance that you donâ€™t agree with some
    of the libraryâ€™s philosophies or design choices. From our experience, however,
    we found that the fundamental design choices and philosophies of the library are
    crucial to efficiently scale ğŸ¤— Transformers while keeping maintenance costs at
    a reasonable level.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ‚¨åº”è¯¥å¯¹ğŸ¤— Transformersæœ‰ä¸€ä¸ªæ€»ä½“äº†è§£ã€‚ğŸ¤— Transformersæ˜¯ä¸€ä¸ªéå¸¸ä¸»è§‚çš„åº“ï¼Œå› æ­¤æ‚¨å¯èƒ½ä¸åŒæ„ä¸€äº›åº“çš„ç†å¿µæˆ–è®¾è®¡é€‰æ‹©ã€‚ç„¶è€Œï¼Œæ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œæˆ‘ä»¬å‘ç°åº“çš„åŸºæœ¬è®¾è®¡é€‰æ‹©å’Œç†å¿µå¯¹äºæœ‰æ•ˆæ‰©å±•ğŸ¤—
    Transformerså¹¶ä¿æŒç»´æŠ¤æˆæœ¬åœ¨åˆç†æ°´å¹³ä¸Šè‡³å…³é‡è¦ã€‚
- en: 'A good first starting point to better understand the library is to read the
    [documentation of our philosophy](philosophy). As a result of our way of working,
    there are some choices that we try to apply to all models:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¥½åœ°äº†è§£åº“çš„ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹æ˜¯é˜…è¯»[æˆ‘ä»¬å“²å­¦çš„æ–‡æ¡£](philosophy)ã€‚ç”±äºæˆ‘ä»¬çš„å·¥ä½œæ–¹å¼ï¼Œæœ‰ä¸€äº›é€‰æ‹©æˆ‘ä»¬è¯•å›¾åº”ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼š
- en: Composition is generally favored over-abstraction
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šå¸¸æ›´å–œæ¬¢ç»„åˆè€Œä¸æ˜¯æŠ½è±¡
- en: Duplicating code is not always bad if it strongly improves the readability or
    accessibility of a model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤åˆ¶ä»£ç å¹¶ä¸æ€»æ˜¯åäº‹ï¼Œå¦‚æœå®ƒæå¤§åœ°æé«˜äº†æ¨¡å‹çš„å¯è¯»æ€§æˆ–å¯è®¿é—®æ€§
- en: Model files are as self-contained as possible so that when you read the code
    of a specific model, you ideally only have to look into the respective `modeling_....py`
    file.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ–‡ä»¶å°½å¯èƒ½è‡ªåŒ…å«ï¼Œè¿™æ ·å½“æ‚¨é˜…è¯»ç‰¹å®šæ¨¡å‹çš„ä»£ç æ—¶ï¼Œç†æƒ³æƒ…å†µä¸‹åªéœ€æŸ¥çœ‹ç›¸åº”çš„ `modeling_....py` æ–‡ä»¶ã€‚
- en: In our opinion, the libraryâ€™s code is not just a means to provide a product,
    *e.g.* the ability to use BERT for inference, but also as the very product that
    we want to improve. Hence, when adding a model, the user is not only the person
    who will use your model, but also everybody who will read, try to understand,
    and possibly tweak your code.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œåº“çš„ä»£ç ä¸ä»…ä»…æ˜¯æä¾›äº§å“çš„æ‰‹æ®µï¼Œ*ä¾‹å¦‚*ä½¿ç”¨BERTè¿›è¡Œæ¨æ–­çš„èƒ½åŠ›ï¼Œè€Œä¸”ä¹Ÿæ˜¯æˆ‘ä»¬æƒ³è¦æ”¹è¿›çš„äº§å“æœ¬èº«ã€‚å› æ­¤ï¼Œå½“æ·»åŠ ä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œç”¨æˆ·ä¸ä»…æ˜¯å°†ä½¿ç”¨æ‚¨çš„æ¨¡å‹çš„äººï¼Œè¿˜æœ‰æ‰€æœ‰å°†é˜…è¯»ã€å°è¯•ç†è§£å’Œå¯èƒ½è°ƒæ•´æ‚¨çš„ä»£ç çš„äººã€‚
- en: With this in mind, letâ€™s go a bit deeper into the general library design.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™ä¸ªæƒ³æ³•ï¼Œè®©æˆ‘ä»¬æ›´æ·±å…¥åœ°äº†è§£ä¸€ä¸‹ä¸€èˆ¬çš„åº“è®¾è®¡ã€‚
- en: Overview of models
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¦‚è¿°
- en: To successfully add a model, it is important to understand the interaction between
    your model and its config, [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    and [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).
    For exemplary purposes, we will call the model to be added to ğŸ¤— Transformers `BrandNewBert`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æˆåŠŸæ·»åŠ ä¸€ä¸ªæ¨¡å‹ï¼Œé‡è¦çš„æ˜¯è¦ç†è§£æ‚¨çš„æ¨¡å‹ä¸å…¶é…ç½®ã€[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)å’Œ[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ä¹‹é—´çš„äº¤äº’ã€‚ä¸ºäº†ä¸¾ä¾‹è¯´æ˜ï¼Œæˆ‘ä»¬å°†è¦æ·»åŠ åˆ°ğŸ¤—
    Transformersçš„æ¨¡å‹ç§°ä¸º`BrandNewBert`ã€‚
- en: 'Letâ€™s take a look:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€çœ‹ï¼š
- en: '![](../Images/ea4f05530b7bf13a323b2cf6fd4f78ca.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea4f05530b7bf13a323b2cf6fd4f78ca.png)'
- en: 'As you can see, we do make use of inheritance in ğŸ¤— Transformers, but we keep
    the level of abstraction to an absolute minimum. There are never more than two
    levels of abstraction for any model in the library. `BrandNewBertModel` inherits
    from `BrandNewBertPreTrainedModel` which in turn inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    and thatâ€™s it. As a general rule, we want to make sure that a new model only depends
    on [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    The important functionalities that are automatically provided to every new model
    are [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    which are used for serialization and deserialization. All of the other important
    functionalities, such as `BrandNewBertModel.forward` should be completely defined
    in the new `modeling_brand_new_bert.py` script. Next, we want to make sure that
    a model with a specific head layer, such as `BrandNewBertForMaskedLM` does not
    inherit from `BrandNewBertModel`, but rather uses `BrandNewBertModel` as a component
    that can be called in its forward pass to keep the level of abstraction low. Every
    new model requires a configuration class, called `BrandNewBertConfig`. This configuration
    is always stored as an attribute in [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    and thus can be accessed via the `config` attribute for all classes inheriting
    from `BrandNewBertPreTrainedModel`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬åœ¨ğŸ¤— Transformersä¸­ç¡®å®ä½¿ç”¨äº†ç»§æ‰¿ï¼Œä½†æˆ‘ä»¬å°†æŠ½è±¡çº§åˆ«ä¿æŒåˆ°ç»å¯¹æœ€ä½é™åº¦ã€‚åº“ä¸­ä»»ä½•æ¨¡å‹çš„æŠ½è±¡çº§åˆ«æ°¸è¿œä¸ä¼šè¶…è¿‡ä¸¤ä¸ªã€‚`BrandNewBertModel`ç»§æ‰¿è‡ª`BrandNewBertPreTrainedModel`ï¼Œåè€…åˆç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ï¼Œå°±æ˜¯è¿™æ ·ã€‚ä¸€èˆ¬è§„åˆ™æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›ç¡®ä¿æ–°æ¨¡å‹ä»…ä¾èµ–äº[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚è‡ªåŠ¨æä¾›ç»™æ¯ä¸ªæ–°æ¨¡å‹çš„é‡è¦åŠŸèƒ½æ˜¯[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)å’Œ[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)ï¼Œç”¨äºåºåˆ—åŒ–å’Œååºåˆ—åŒ–ã€‚æ‰€æœ‰å…¶ä»–é‡è¦åŠŸèƒ½ï¼Œå¦‚`BrandNewBertModel.forward`ï¼Œåº”å®Œå…¨åœ¨æ–°çš„`modeling_brand_new_bert.py`è„šæœ¬ä¸­å®šä¹‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦ç¡®ä¿å…·æœ‰ç‰¹å®šå¤´å±‚çš„æ¨¡å‹ï¼Œå¦‚`BrandNewBertForMaskedLM`ï¼Œä¸ç»§æ‰¿è‡ª`BrandNewBertModel`ï¼Œè€Œæ˜¯ä½¿ç”¨`BrandNewBertModel`ä½œä¸ºå¯ä»¥åœ¨å…¶å‰å‘ä¼ é€’ä¸­è°ƒç”¨çš„ç»„ä»¶ï¼Œä»¥ä¿æŒæŠ½è±¡çº§åˆ«ä½ã€‚æ¯ä¸ªæ–°æ¨¡å‹éƒ½éœ€è¦ä¸€ä¸ªé…ç½®ç±»ï¼Œç§°ä¸º`BrandNewBertConfig`ã€‚æ­¤é…ç½®å§‹ç»ˆå­˜å‚¨ä¸º[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ä¸­çš„å±æ€§ï¼Œå› æ­¤å¯ä»¥é€šè¿‡`config`å±æ€§è®¿é—®æ‰€æœ‰ç»§æ‰¿è‡ª`BrandNewBertPreTrainedModel`çš„ç±»ï¼š
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Similar to the model, the configuration inherits basic serialization and deserialization
    functionalities from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).
    Note that the configuration and the model are always serialized into two different
    formats - the model to a *pytorch_model.bin* file and the configuration to a *config.json*
    file. Calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    will automatically call [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained),
    so that both model and configuration are saved.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ¨¡å‹ç±»ä¼¼ï¼Œé…ç½®ä»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ç»§æ‰¿åŸºæœ¬çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–åŠŸèƒ½ã€‚è¯·æ³¨æ„ï¼Œé…ç½®å’Œæ¨¡å‹å§‹ç»ˆä»¥ä¸¤ç§ä¸åŒçš„æ ¼å¼è¿›è¡Œåºåˆ—åŒ–
    - æ¨¡å‹ä¿å­˜ä¸º*pytorch_model.bin*æ–‡ä»¶ï¼Œé…ç½®ä¿å­˜ä¸º*config.json*æ–‡ä»¶ã€‚è°ƒç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)å°†è‡ªåŠ¨è°ƒç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained)ï¼Œä»¥ä¾¿åŒæ—¶ä¿å­˜æ¨¡å‹å’Œé…ç½®ã€‚
- en: Code style
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»£ç é£æ ¼
- en: When coding your new model, keep in mind that Transformers is an opinionated
    library and we have a few quirks of our own regarding how code should be written
    :-)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¼–å†™æ–°æ¨¡å‹æ—¶ï¼Œè¯·è®°ä½Transformersæ˜¯ä¸€ä¸ªæŒæœ‰æ„è§çš„åº“ï¼Œå…³äºä»£ç åº”è¯¥å¦‚ä½•ç¼–å†™ï¼Œæˆ‘ä»¬æœ‰è‡ªå·±çš„ä¸€äº›æ€ªç™– :-)
- en: The forward pass of your model should be fully written in the modeling file
    while being fully independent of other models in the library. If you want to reuse
    a block from another model, copy the code and paste it with a `# Copied from`
    comment on top (see [here](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)
    for a good example and [there](pr_checks#check-copies) for more documentation
    on Copied from).
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‚¨çš„æ¨¡å‹çš„å‰å‘ä¼ é€’åº”å®Œå…¨åœ¨å»ºæ¨¡æ–‡ä»¶ä¸­ç¼–å†™ï¼ŒåŒæ—¶å®Œå…¨ç‹¬ç«‹äºåº“ä¸­çš„å…¶ä»–æ¨¡å‹ã€‚å¦‚æœè¦é‡ç”¨å¦ä¸€ä¸ªæ¨¡å‹ä¸­çš„å—ï¼Œè¯·å¤åˆ¶ä»£ç å¹¶åœ¨é¡¶éƒ¨æ·»åŠ `# Copied from`æ³¨é‡Šï¼ˆè¯·å‚è§[æ­¤å¤„](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)ä»¥è·å–ä¸€ä¸ªå¾ˆå¥½çš„ç¤ºä¾‹ï¼Œä»¥åŠ[æ­¤å¤„](pr_checks#check-copies)ä»¥è·å–æœ‰å…³å¤åˆ¶çš„æ›´å¤šæ–‡æ¡£ï¼‰ã€‚
- en: The code should be fully understandable, even by a non-native English speaker.
    This means you should pick descriptive variable names and avoid abbreviations.
    As an example, `activation` is preferred to `act`. One-letter variable names are
    strongly discouraged unless itâ€™s an index in a for loop.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç åº”è¯¥æ˜¯å®Œå…¨å¯ç†è§£çš„ï¼Œå³ä½¿å¯¹äºéæ¯è¯­è‹±è¯­çš„äººä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™æ„å‘³ç€æ‚¨åº”è¯¥é€‰æ‹©æè¿°æ€§çš„å˜é‡åç§°å¹¶é¿å…ç¼©å†™ã€‚ä¾‹å¦‚ï¼Œ`activation`æ¯”`act`æ›´å—æ¬¢è¿ã€‚é™¤éæ˜¯å¾ªç¯ä¸­çš„ç´¢å¼•ï¼Œå¦åˆ™å¼ºçƒˆä¸å»ºè®®ä½¿ç”¨ä¸€ä¸ªå­—æ¯çš„å˜é‡åã€‚
- en: More generally we prefer longer explicit code to short magical one.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬æ›´å–œæ¬¢é•¿è€Œæ˜ç¡®çš„ä»£ç ï¼Œè€Œä¸æ˜¯çŸ­è€Œç¥å¥‡çš„ä»£ç ã€‚
- en: Avoid subclassing `nn.Sequential` in PyTorch but subclass `nn.Module` and write
    the forward pass, so that anyone using your code can quickly debug it by adding
    print statements or breaking points.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨PyTorchä¸­é¿å…å¯¹`nn.Sequential`è¿›è¡Œå­ç±»åŒ–ï¼Œè€Œæ˜¯å¯¹`nn.Module`è¿›è¡Œå­ç±»åŒ–å¹¶ç¼–å†™å‰å‘ä¼ é€’ï¼Œä»¥ä¾¿ä½¿ç”¨æ‚¨çš„ä»£ç çš„ä»»ä½•äººéƒ½å¯ä»¥é€šè¿‡æ·»åŠ æ‰“å°è¯­å¥æˆ–æ–­ç‚¹æ¥å¿«é€Ÿè°ƒè¯•å®ƒã€‚
- en: Your function signature should be type-annotated. For the rest, good variable
    names are way more readable and understandable than type annotations.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‚¨çš„å‡½æ•°ç­¾ååº”è¯¥æœ‰ç±»å‹æ³¨é‡Šã€‚å¯¹äºå…¶ä½™éƒ¨åˆ†ï¼Œè‰¯å¥½çš„å˜é‡åç§°æ¯”ç±»å‹æ³¨é‡Šæ›´å¯è¯»å’Œå¯ç†è§£ã€‚
- en: Overview of tokenizers
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨æ¦‚è¿°
- en: Not quite ready yet :-( This section will be added soon!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜ä¸å¤ªå‡†å¤‡å¥½ :-( æ­¤éƒ¨åˆ†å°†å¾ˆå¿«æ·»åŠ ï¼
- en: Step-by-step recipe to add a model to ğŸ¤— Transformers
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹æ·»åŠ åˆ°ğŸ¤— Transformersçš„é€æ­¥é…æ–¹
- en: 'Everyone has different preferences of how to port a model so it can be very
    helpful for you to take a look at summaries of how other contributors ported models
    to Hugging Face. Here is a list of community blog posts on how to port a model:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªäººå¯¹å¦‚ä½•ç§»æ¤æ¨¡å‹éƒ½æœ‰ä¸åŒçš„åå¥½ï¼Œå› æ­¤æŸ¥çœ‹å…¶ä»–è´¡çŒ®è€…å¦‚ä½•å°†æ¨¡å‹ç§»æ¤åˆ°Hugging Faceå¯èƒ½ä¼šå¯¹æ‚¨éå¸¸æœ‰å¸®åŠ©ã€‚ä»¥ä¸‹æ˜¯å…³äºå¦‚ä½•ç§»æ¤æ¨¡å‹çš„ç¤¾åŒºåšå®¢æ–‡ç« åˆ—è¡¨ï¼š
- en: '[Porting GPT2 Model](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28)
    by [Thomas](https://huggingface.co/thomwolf)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç§»æ¤GPT2æ¨¡å‹](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28)
    by [Thomas](https://huggingface.co/thomwolf)'
- en: '[Porting WMT19 MT Model](https://huggingface.co/blog/porting-fsmt) by [Stas](https://huggingface.co/stas)'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç§»æ¤WMT19 MTæ¨¡å‹](https://huggingface.co/blog/porting-fsmt) by [Stas](https://huggingface.co/stas)'
- en: 'From experience, we can tell you that the most important things to keep in
    mind when adding a model are:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬å¯ä»¥å‘Šè¯‰æ‚¨åœ¨æ·»åŠ æ¨¡å‹æ—¶è¦ç‰¢è®°çš„æœ€é‡è¦çš„äº‹æƒ…æ˜¯ï¼š
- en: Donâ€™t reinvent the wheel! Most parts of the code you will add for the new ğŸ¤—
    Transformers model already exist somewhere in ğŸ¤— Transformers. Take some time to
    find similar, already existing models and tokenizers you can copy from. [grep](https://www.gnu.org/software/grep/)
    and [rg](https://github.com/BurntSushi/ripgrep) are your friends. Note that it
    might very well happen that your modelâ€™s tokenizer is based on one model implementation,
    and your modelâ€™s modeling code on another one. *E.g.* FSMTâ€™s modeling code is
    based on BART, while FSMTâ€™s tokenizer code is based on XLM.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸è¦é‡å¤é€ è½®å­ï¼æ‚¨å°†ä¸ºæ–°çš„ğŸ¤— Transformersæ¨¡å‹æ·»åŠ çš„å¤§éƒ¨åˆ†ä»£ç å·²ç»å­˜åœ¨äºğŸ¤— Transformersçš„æŸä¸ªåœ°æ–¹ã€‚èŠ±äº›æ—¶é—´æ‰¾åˆ°ç±»ä¼¼çš„ã€å·²ç»å­˜åœ¨çš„æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæ‚¨å¯ä»¥ä»ä¸­å¤åˆ¶ã€‚[grep](https://www.gnu.org/software/grep/)å’Œ[rg](https://github.com/BurntSushi/ripgrep)æ˜¯æ‚¨çš„æœ‹å‹ã€‚è¯·æ³¨æ„ï¼Œæ‚¨çš„æ¨¡å‹çš„åˆ†è¯å™¨å¯èƒ½åŸºäºä¸€ä¸ªæ¨¡å‹å®ç°ï¼Œè€Œæ‚¨çš„æ¨¡å‹çš„å»ºæ¨¡ä»£ç å¯èƒ½åŸºäºå¦ä¸€ä¸ªæ¨¡å‹å®ç°ã€‚*ä¾‹å¦‚*ï¼ŒFSMTçš„å»ºæ¨¡ä»£ç åŸºäºBARTï¼Œè€ŒFSMTçš„åˆ†è¯å™¨ä»£ç åŸºäºXLMã€‚
- en: Itâ€™s more of an engineering challenge than a scientific challenge. You should
    spend more time creating an efficient debugging environment rather than trying
    to understand all theoretical aspects of the model in the paper.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ›´å¤šæ˜¯ä¸€ä¸ªå·¥ç¨‹æŒ‘æˆ˜è€Œä¸æ˜¯ä¸€ä¸ªç§‘å­¦æŒ‘æˆ˜ã€‚æ‚¨åº”è¯¥èŠ±æ›´å¤šæ—¶é—´åˆ›å»ºä¸€ä¸ªé«˜æ•ˆçš„è°ƒè¯•ç¯å¢ƒï¼Œè€Œä¸æ˜¯è¯•å›¾ç†è§£è®ºæ–‡ä¸­æ¨¡å‹çš„æ‰€æœ‰ç†è®ºæ–¹é¢ã€‚
- en: Ask for help, when youâ€™re stuck! Models are the core component of ğŸ¤— Transformers
    so we at Hugging Face are more than happy to help you at every step to add your
    model. Donâ€™t hesitate to ask if you notice you are not making progress.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æ‚¨é‡åˆ°å›°éš¾æ—¶ï¼Œè¯·å¯»æ±‚å¸®åŠ©ï¼æ¨¡å‹æ˜¯ğŸ¤— Transformersçš„æ ¸å¿ƒç»„ä»¶ï¼Œå› æ­¤æˆ‘ä»¬åœ¨Hugging Faceéå¸¸ä¹æ„åœ¨æ¯ä¸ªæ­¥éª¤å¸®åŠ©æ‚¨æ·»åŠ æ‚¨çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨å‘ç°è‡ªå·±æ²¡æœ‰å–å¾—è¿›å±•ï¼Œè¯·ä¸è¦çŠ¹è±«è¯¢é—®ã€‚
- en: In the following, we try to give you a general recipe that we found most useful
    when porting a model to ğŸ¤— Transformers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°è¯•ä¸ºæ‚¨æä¾›ä¸€ä¸ªæˆ‘ä»¬åœ¨å°†æ¨¡å‹ç§»æ¤åˆ°ğŸ¤— Transformersæ—¶å‘ç°æœ€æœ‰ç”¨çš„ä¸€èˆ¬æ­¥éª¤ã€‚
- en: 'The following list is a summary of everything that has to be done to add a
    model and can be used by you as a To-Do List:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹åˆ—è¡¨æ€»ç»“äº†æ·»åŠ æ¨¡å‹æ—¶å¿…é¡»å®Œæˆçš„æ‰€æœ‰å·¥ä½œï¼Œå¹¶å¯ä»¥ä½œä¸ºå¾…åŠäº‹é¡¹æ¸…å•ä½¿ç”¨ï¼š
- en: â˜ (Optional) Understood the modelâ€™s theoretical aspects
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ ï¼ˆå¯é€‰ï¼‰ç†è§£æ¨¡å‹çš„ç†è®ºæ–¹é¢
- en: â˜ Prepared ğŸ¤— Transformers dev environment
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ å‡†å¤‡ğŸ¤— Transformerså¼€å‘ç¯å¢ƒ
- en: â˜ Set up debugging environment of the original repository
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ è®¾ç½®åŸå§‹å­˜å‚¨åº“çš„è°ƒè¯•ç¯å¢ƒ
- en: â˜ Created script that successfully runs the `forward()` pass using the original
    repository and checkpoint
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ åˆ›å»ºè„šæœ¬ï¼ŒæˆåŠŸä½¿ç”¨åŸå§‹å­˜å‚¨åº“å’Œæ£€æŸ¥ç‚¹è¿è¡Œ`forward()`ä¼ é€’
- en: â˜ Successfully added the model skeleton to ğŸ¤— Transformers
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ æˆåŠŸå°†æ¨¡å‹éª¨æ¶æ·»åŠ åˆ°ğŸ¤— Transformers
- en: â˜ Successfully converted original checkpoint to ğŸ¤— Transformers checkpoint
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ æˆåŠŸå°†åŸå§‹æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºğŸ¤— Transformersæ£€æŸ¥ç‚¹
- en: â˜ Successfully ran `forward()` pass in ğŸ¤— Transformers that gives identical output
    to original checkpoint
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ åœ¨ğŸ¤— Transformersä¸­æˆåŠŸè¿è¡Œ`forward()`ä¼ é€’ï¼Œè¾“å‡ºä¸åŸå§‹æ£€æŸ¥ç‚¹ç›¸åŒ
- en: â˜ Finished model tests in ğŸ¤— Transformers
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ åœ¨ğŸ¤— Transformersä¸­å®Œæˆæ¨¡å‹æµ‹è¯•
- en: â˜ Successfully added tokenizer in ğŸ¤— Transformers
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ åœ¨ğŸ¤— Transformersä¸­æˆåŠŸæ·»åŠ äº†åˆ†è¯å™¨
- en: â˜ Run end-to-end integration tests
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ è¿è¡Œç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
- en: â˜ Finished docs
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ å®Œæˆæ–‡æ¡£
- en: â˜ Uploaded model weights to the Hub
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ å°†æ¨¡å‹æƒé‡ä¸Šä¼ åˆ°Hub
- en: â˜ Submitted the pull request
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ æäº¤æ‹‰å–è¯·æ±‚
- en: â˜ (Optional) Added a demo notebook
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: â–¡ ï¼ˆå¯é€‰ï¼‰æ·»åŠ æ¼”ç¤ºç¬”è®°æœ¬
- en: To begin with, we usually recommend starting by getting a good theoretical understanding
    of `BrandNewBert`. However, if you prefer to understand the theoretical aspects
    of the model *on-the-job*, then it is totally fine to directly dive into the `BrandNewBert`â€™s
    code-base. This option might suit you better if your engineering skills are better
    than your theoretical skill, if you have trouble understanding `BrandNewBert`â€™s
    paper, or if you just enjoy programming much more than reading scientific papers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸å»ºè®®é¦–å…ˆå¯¹`BrandNewBert`æœ‰ä¸€ä¸ªè‰¯å¥½çš„ç†è®ºç†è§£ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æ›´å–œæ¬¢åœ¨å·¥ä½œä¸­ç†è§£æ¨¡å‹çš„ç†è®ºæ–¹é¢ï¼Œé‚£ä¹ˆç›´æ¥æ·±å…¥`BrandNewBert`çš„ä»£ç åº“ä¹Ÿæ˜¯å®Œå…¨å¯ä»¥çš„ã€‚å¦‚æœæ‚¨çš„å·¥ç¨‹æŠ€èƒ½æ¯”ç†è®ºæŠ€èƒ½æ›´å¼ºï¼Œå¦‚æœæ‚¨éš¾ä»¥ç†è§£`BrandNewBert`çš„è®ºæ–‡ï¼Œæˆ–è€…å¦‚æœæ‚¨æ›´å–œæ¬¢ç¼–ç¨‹è€Œä¸æ˜¯é˜…è¯»ç§‘å­¦è®ºæ–‡ï¼Œé‚£ä¹ˆè¿™ä¸ªé€‰é¡¹å¯èƒ½æ›´é€‚åˆæ‚¨ã€‚
- en: 1\. (Optional) Theoretical aspects of BrandNewBert
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. ï¼ˆå¯é€‰ï¼‰BrandNewBertçš„ç†è®ºæ–¹é¢
- en: 'You should take some time to read *BrandNewBertâ€™s* paper, if such descriptive
    work exists. There might be large sections of the paper that are difficult to
    understand. If this is the case, this is fine - donâ€™t worry! The goal is not to
    get a deep theoretical understanding of the paper, but to extract the necessary
    information required to effectively re-implement the model in ğŸ¤— Transformers.
    That being said, you donâ€™t have to spend too much time on the theoretical aspects,
    but rather focus on the practical ones, namely:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥èŠ±äº›æ—¶é—´é˜…è¯»*BrandNewBert*çš„è®ºæ–‡ï¼Œå¦‚æœå­˜åœ¨è¿™æ ·çš„æè¿°æ€§å·¥ä½œã€‚è®ºæ–‡ä¸­å¯èƒ½æœ‰ä¸€äº›éš¾ä»¥ç†è§£çš„å¤§æ®µå†…å®¹ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œæ²¡å…³ç³» - ä¸è¦æ‹…å¿ƒï¼ç›®æ ‡ä¸æ˜¯æ·±å…¥ç†è§£è®ºæ–‡ï¼Œè€Œæ˜¯æå–åœ¨ğŸ¤—
    Transformersä¸­æœ‰æ•ˆé‡æ–°å®ç°æ¨¡å‹æ‰€éœ€çš„å¿…è¦ä¿¡æ¯ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨ä¸å¿…èŠ±å¤ªå¤šæ—¶é—´åœ¨ç†è®ºæ–¹é¢ï¼Œè€Œæ˜¯è¦ä¸“æ³¨äºå®è·µæ–¹é¢ï¼Œå³ï¼š
- en: What type of model is *brand_new_bert*? BERT-like encoder-only model? GPT2-like
    decoder-only model? BART-like encoder-decoder model? Look at the [model_summary](model_summary)
    if youâ€™re not familiar with the differences between those.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*brand_new_bert*æ˜¯ä»€ä¹ˆç±»å‹çš„æ¨¡å‹ï¼Ÿç±»ä¼¼BERTçš„ä»…ç¼–ç å™¨æ¨¡å‹ï¼Ÿç±»ä¼¼GPT2çš„ä»…è§£ç å™¨æ¨¡å‹ï¼Ÿç±»ä¼¼BARTçš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Ÿå¦‚æœæ‚¨å¯¹è¿™äº›ä¹‹é—´çš„åŒºåˆ«ä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹[model_summary]ã€‚'
- en: What are the applications of *brand_new_bert*? Text classification? Text generation?
    Seq2Seq tasks, *e.g.,* summarization?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*brand_new_bert*çš„åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿæ–‡æœ¬åˆ†ç±»ï¼Ÿæ–‡æœ¬ç”Ÿæˆï¼ŸSeq2Seqä»»åŠ¡ï¼Œä¾‹å¦‚ï¼Œæ‘˜è¦ï¼Ÿ'
- en: What is the novel feature of the model that makes it different from BERT/GPT-2/BART?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹çš„æ–°ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Œä½¿å…¶ä¸BERT/GPT-2/BARTä¸åŒï¼Ÿ
- en: Which of the already existing [ğŸ¤— Transformers models](https://huggingface.co/transformers/#contents)
    is most similar to *brand_new_bert*?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·²ç»å­˜åœ¨çš„[ğŸ¤— Transformersæ¨¡å‹](https://huggingface.co/transformers/#contents)ä¸­å“ªä¸€ä¸ªä¸*brand_new_bert*æœ€ç›¸ä¼¼ï¼Ÿ
- en: What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer?
    Is it the same tokenizer as used for BERT or BART?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äº†ä»€ä¹ˆç±»å‹çš„åˆ†è¯å™¨ï¼Ÿæ˜¯å¥å­ç‰‡æ®µåˆ†è¯å™¨ï¼Ÿè¯ç‰‡æ®µåˆ†è¯å™¨ï¼Ÿå®ƒæ˜¯å¦ä¸BERTæˆ–BARTä½¿ç”¨çš„ç›¸åŒçš„åˆ†è¯å™¨ï¼Ÿ
- en: After you feel like you have gotten a good overview of the architecture of the
    model, you might want to write to the Hugging Face team with any questions you
    might have. This might include questions regarding the modelâ€™s architecture, its
    attention layer, etc. We will be more than happy to help you.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨æ„Ÿè§‰å¯¹æ¨¡å‹çš„æ¶æ„æœ‰äº†å¾ˆå¥½çš„æ¦‚è¿°åï¼Œæ‚¨å¯èƒ½å¸Œæœ›å‘Hugging Faceå›¢é˜Ÿå‘é€ä»»ä½•å¯èƒ½æœ‰çš„é—®é¢˜ã€‚è¿™å¯èƒ½åŒ…æ‹¬æœ‰å…³æ¨¡å‹æ¶æ„ã€æ³¨æ„åŠ›å±‚ç­‰çš„é—®é¢˜ã€‚æˆ‘ä»¬å°†éå¸¸ä¹æ„å¸®åŠ©æ‚¨ã€‚
- en: 2\. Next prepare your environment
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2. æ¥ä¸‹æ¥å‡†å¤‡æ‚¨çš„ç¯å¢ƒ
- en: Fork the [repository](https://github.com/huggingface/transformers) by clicking
    on the â€˜Forkâ€™ button on the repositoryâ€™s page. This creates a copy of the code
    under your GitHub user account.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‚¹å‡»å­˜å‚¨åº“é¡µé¢ä¸Šçš„â€œForkâ€æŒ‰é’®æ¥forkè¿™ä¸ª[å­˜å‚¨åº“](https://github.com/huggingface/transformers)ã€‚è¿™å°†åœ¨æ‚¨çš„GitHubç”¨æˆ·è´¦æˆ·ä¸‹åˆ›å»ºä»£ç çš„å‰¯æœ¬ã€‚
- en: 'Clone your `transformers` fork to your local disk, and add the base repository
    as a remote:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„`transformers` forkå…‹éš†åˆ°æœ¬åœ°ç£ç›˜ï¼Œå¹¶å°†åŸºæœ¬å­˜å‚¨åº“æ·»åŠ ä¸ºè¿œç¨‹ï¼š
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set up a development environment, for instance by running the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸€ä¸ªå¼€å‘ç¯å¢ƒï¼Œä¾‹å¦‚é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Depending on your OS, and since the number of optional dependencies of Transformers
    is growing, you might get a failure with this command. If thatâ€™s the case make
    sure to install the Deep Learning framework you are working with (PyTorch, TensorFlow
    and/or Flax) then do:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ‚¨çš„æ“ä½œç³»ç»Ÿï¼Œç”±äºTransformersçš„å¯é€‰ä¾èµ–é¡¹æ•°é‡æ­£åœ¨å¢åŠ ï¼Œæ‚¨å¯èƒ½ä¼šåœ¨æ­¤å‘½ä»¤ä¸­é‡åˆ°å¤±è´¥ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œè¯·ç¡®ä¿å®‰è£…æ‚¨æ­£åœ¨ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆPyTorchã€TensorFlowå’Œ/æˆ–Flaxï¼‰ï¼Œç„¶åæ‰§è¡Œï¼š
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which should be enough for most use cases. You can then return to the parent
    directory
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºå¤§å¤šæ•°ç”¨ä¾‹åº”è¯¥è¶³å¤Ÿäº†ã€‚ç„¶åæ‚¨å¯ä»¥è¿”å›åˆ°çˆ¶ç›®å½•
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We recommend adding the PyTorch version of *brand_new_bert* to Transformers.
    To install PyTorch, please follow the instructions on [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®å°†PyTorchç‰ˆæœ¬çš„*brand_new_bert*æ·»åŠ åˆ°Transformersä¸­ã€‚è¦å®‰è£…PyTorchï¼Œè¯·æŒ‰ç…§[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)ä¸Šçš„è¯´æ˜æ“ä½œã€‚
- en: '**Note:** You donâ€™t need to have CUDA installed. Making the new model work
    on CPU is sufficient.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** æ‚¨ä¸éœ€è¦å®‰è£…CUDAã€‚ä½¿æ–°æ¨¡å‹åœ¨CPUä¸Šè¿è¡Œå°±è¶³å¤Ÿäº†ã€‚'
- en: 'To port *brand_new_bert*, you will also need access to its original repository:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¦ç§»æ¤*brand_new_bert*ï¼Œæ‚¨è¿˜éœ€è¦è®¿é—®å…¶åŸå§‹å­˜å‚¨åº“ï¼š
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now you have set up a development environment to port *brand_new_bert* to ğŸ¤—
    Transformers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»è®¾ç½®å¥½äº†ä¸€ä¸ªå¼€å‘ç¯å¢ƒï¼Œå¯ä»¥å°†*brand_new_bert*ç§»æ¤åˆ°ğŸ¤— Transformersã€‚
- en: 3.-4\. Run a pretrained checkpoint using the original repository
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.-4. åœ¨åŸå§‹å­˜å‚¨åº“ä¸­è¿è¡Œé¢„è®­ç»ƒæ£€æŸ¥ç‚¹
- en: At first, you will work on the original *brand_new_bert* repository. Often,
    the original implementation is very â€œresearchyâ€. Meaning that documentation might
    be lacking and the code can be difficult to understand. But this should be exactly
    your motivation to reimplement *brand_new_bert*. At Hugging Face, one of our main
    goals is to *make people stand on the shoulders of giants* which translates here
    very well into taking a working model and rewriting it to make it as **accessible,
    user-friendly, and beautiful** as possible. This is the number-one motivation
    to re-implement models into ğŸ¤— Transformers - trying to make complex new NLP technology
    accessible to **everybody**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ‚¨å°†åœ¨åŸå§‹*brand_new_bert*å­˜å‚¨åº“ä¸Šå·¥ä½œã€‚é€šå¸¸ï¼ŒåŸå§‹å®ç°éå¸¸â€œç ”ç©¶æ€§â€ã€‚è¿™æ„å‘³ç€æ–‡æ¡£å¯èƒ½ç¼ºå¤±ï¼Œä»£ç å¯èƒ½éš¾ä»¥ç†è§£ã€‚ä½†è¿™åº”è¯¥æ­£æ˜¯æ‚¨é‡æ–°å®ç°*brand_new_bert*çš„åŠ¨åŠ›æ‰€åœ¨ã€‚åœ¨Hugging
    Faceï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡ä¹‹ä¸€æ˜¯è®©äººä»¬â€œç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šâ€ï¼Œè¿™åœ¨è¿™é‡Œéå¸¸å¥½åœ°ä½“ç°ä¸ºæ‹¿ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹å¹¶é‡å†™å®ƒï¼Œä½¿å…¶å°½å¯èƒ½**æ˜“äºè®¿é—®ã€ç”¨æˆ·å‹å¥½å’Œç¾è§‚**ã€‚è¿™æ˜¯é‡æ–°å®ç°æ¨¡å‹åˆ°ğŸ¤—
    Transformersçš„é¦–è¦åŠ¨æœº - å°è¯•ä½¿å¤æ‚çš„æ–°NLPæŠ€æœ¯å¯¹**æ¯ä¸ªäºº**éƒ½å¯è®¿é—®ã€‚
- en: You should start thereby by diving into the original repository.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ‚¨åº”è¯¥é¦–å…ˆæ·±å…¥ç ”ç©¶åŸå§‹å­˜å‚¨åº“ã€‚
- en: 'Successfully running the official pretrained model in the original repository
    is often **the most difficult** step. From our experience, it is very important
    to spend some time getting familiar with the original code-base. You need to figure
    out the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹å­˜å‚¨åº“ä¸­æˆåŠŸè¿è¡Œå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸æ˜¯**æœ€å›°éš¾**çš„ä¸€æ­¥ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼ŒèŠ±ä¸€äº›æ—¶é—´ç†Ÿæ‚‰åŸå§‹ä»£ç åº“éå¸¸é‡è¦ã€‚æ‚¨éœ€è¦å¼„æ¸…æ¥šä»¥ä¸‹å†…å®¹ï¼š
- en: Where to find the pretrained weights?
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å“ªé‡Œæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Ÿ
- en: How to load the pretrained weights into the corresponding model?
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°†é¢„è®­ç»ƒæƒé‡åŠ è½½åˆ°ç›¸åº”çš„æ¨¡å‹ä¸­ï¼Ÿ
- en: How to run the tokenizer independently from the model?
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•ç‹¬ç«‹äºæ¨¡å‹è¿è¡Œåˆ†è¯å™¨ï¼Ÿ
- en: Trace one forward pass so that you know which classes and functions are required
    for a simple forward pass. Usually, you only have to reimplement those functions.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªä¸€æ¬¡å‰å‘ä¼ é€’ï¼Œä»¥ä¾¿äº†è§£å“ªäº›ç±»å’Œå‡½æ•°éœ€è¦è¿›è¡Œç®€å•çš„å‰å‘ä¼ é€’ã€‚é€šå¸¸ï¼Œæ‚¨åªéœ€è¦é‡æ–°å®ç°è¿™äº›å‡½æ•°ã€‚
- en: 'Be able to locate the important components of the model: Where is the modelâ€™s
    class? Are there model sub-classes, *e.g.* EncoderModel, DecoderModel? Where is
    the self-attention layer? Are there multiple different attention layers, *e.g.*
    *self-attention*, *cross-attention*â€¦?'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿæ‰¾åˆ°æ¨¡å‹çš„é‡è¦ç»„ä»¶ï¼šæ¨¡å‹çš„ç±»åœ¨å“ªé‡Œï¼Ÿæ˜¯å¦æœ‰æ¨¡å‹å­ç±»ï¼Œä¾‹å¦‚EncoderModelï¼ŒDecoderModelï¼Ÿè‡ªæ³¨æ„åŠ›å±‚åœ¨å“ªé‡Œï¼Ÿæ˜¯å¦æœ‰å¤šä¸ªä¸åŒçš„æ³¨æ„åŠ›å±‚ï¼Œä¾‹å¦‚self-attentionï¼Œcross-attention...ï¼Ÿ
- en: How can you debug the model in the original environment of the repo? Do you
    have to add *print* statements, can you work with an interactive debugger like
    *ipdb*, or should you use an efficient IDE to debug the model, like PyCharm?
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨å­˜å‚¨åº“çš„åŸå§‹ç¯å¢ƒä¸­è°ƒè¯•æ¨¡å‹ï¼Ÿæ‚¨æ˜¯å¦éœ€è¦æ·»åŠ *print*è¯­å¥ï¼Œæ˜¯å¦å¯ä»¥ä½¿ç”¨äº¤äº’å¼è°ƒè¯•å™¨å¦‚*ipdb*ï¼Œæˆ–è€…æ˜¯å¦åº”è¯¥ä½¿ç”¨é«˜æ•ˆçš„IDEæ¥è°ƒè¯•æ¨¡å‹ï¼Œå¦‚PyCharmï¼Ÿ
- en: It is very important that before you start the porting process, you can **efficiently**
    debug code in the original repository! Also, remember that you are working with
    an open-source library, so do not hesitate to open an issue, or even a pull request
    in the original repository. The maintainers of this repository are most likely
    very happy about someone looking into their code!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ç§»æ¤è¿‡ç¨‹ä¹‹å‰ï¼Œéå¸¸é‡è¦çš„æ˜¯æ‚¨å¯ä»¥**æœ‰æ•ˆåœ°**è°ƒè¯•åŸå§‹å­˜å‚¨åº“ä¸­çš„ä»£ç ï¼è¿˜è¦è®°ä½ï¼Œæ‚¨æ­£åœ¨ä½¿ç”¨ä¸€ä¸ªå¼€æºåº“ï¼Œå› æ­¤ä¸è¦çŠ¹è±«åœ¨åŸå§‹å­˜å‚¨åº“ä¸­æ‰“å¼€é—®é¢˜ï¼Œç”šè‡³æäº¤æ‹‰å–è¯·æ±‚ã€‚è¿™ä¸ªå­˜å‚¨åº“çš„ç»´æŠ¤è€…å¾ˆå¯èƒ½ä¼šå¯¹æœ‰äººæŸ¥çœ‹ä»–ä»¬çš„ä»£ç æ„Ÿåˆ°éå¸¸é«˜å…´ï¼
- en: At this point, it is really up to you which debugging environment and strategy
    you prefer to use to debug the original model. We strongly advise against setting
    up a costly GPU environment, but simply work on a CPU both when starting to dive
    into the original repository and also when starting to write the ğŸ¤— Transformers
    implementation of the model. Only at the very end, when the model has already
    been successfully ported to ğŸ¤— Transformers, one should verify that the model also
    works as expected on GPU.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼ŒçœŸçš„å–å†³äºæ‚¨æ›´å–œæ¬¢ä½¿ç”¨å“ªç§è°ƒè¯•ç¯å¢ƒå’Œç­–ç•¥æ¥è°ƒè¯•åŸå§‹æ¨¡å‹ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä¸è¦è®¾ç½®æ˜‚è´µçš„GPUç¯å¢ƒï¼Œè€Œæ˜¯åœ¨å¼€å§‹æ·±å…¥ç ”ç©¶åŸå§‹å­˜å‚¨åº“å’Œå¼€å§‹ç¼–å†™ğŸ¤— Transformersæ¨¡å‹å®ç°æ—¶éƒ½ä½¿ç”¨CPUã€‚åªæœ‰åœ¨æ¨¡å‹å·²ç»æˆåŠŸç§»æ¤åˆ°ğŸ¤—
    Transformersåï¼Œæ‰åº”éªŒè¯æ¨¡å‹åœ¨GPUä¸Šæ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œã€‚
- en: In general, there are two possible debugging environments for running the original
    model
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œæœ‰ä¸¤ç§å¯èƒ½çš„è°ƒè¯•ç¯å¢ƒå¯ç”¨äºè¿è¡ŒåŸå§‹æ¨¡å‹
- en: '[Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jupyterç¬”è®°æœ¬](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)'
- en: Local python scripts.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ¬åœ°pythonè„šæœ¬ã€‚
- en: Jupyter notebooks have the advantage that they allow for cell-by-cell execution
    which can be helpful to better split logical components from one another and to
    have faster debugging cycles as intermediate results can be stored. Also, notebooks
    are often easier to share with other contributors, which might be very helpful
    if you want to ask the Hugging Face team for help. If you are familiar with Jupyter
    notebooks, we strongly recommend you work with them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyterç¬”è®°æœ¬çš„ä¼˜åŠ¿åœ¨äºå®ƒä»¬å…è®¸é€ä¸ªå•å…ƒæ ¼æ‰§è¡Œï¼Œè¿™æœ‰åŠ©äºæ›´å¥½åœ°å°†é€»è¾‘ç»„ä»¶å½¼æ­¤åˆ†ç¦»ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¿«çš„è°ƒè¯•å‘¨æœŸï¼Œå› ä¸ºä¸­é—´ç»“æœå¯ä»¥è¢«å­˜å‚¨ã€‚æ­¤å¤–ï¼Œç¬”è®°æœ¬é€šå¸¸æ›´å®¹æ˜“ä¸å…¶ä»–è´¡çŒ®è€…å…±äº«ï¼Œå¦‚æœæ‚¨æƒ³è¦å‘Hugging
    Faceå›¢é˜Ÿå¯»æ±‚å¸®åŠ©ï¼Œè¿™å¯èƒ½éå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæ‚¨ç†Ÿæ‚‰Jupyterç¬”è®°æœ¬ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨ä½¿ç”¨å®ƒä»¬ã€‚
- en: The obvious disadvantage of Jupyter notebooks is that if you are not used to
    working with them you will have to spend some time adjusting to the new programming
    environment and you might not be able to use your known debugging tools anymore,
    like `ipdb`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyterç¬”è®°æœ¬çš„æ˜æ˜¾ç¼ºç‚¹æ˜¯ï¼Œå¦‚æœæ‚¨ä¸ä¹ æƒ¯ä½¿ç”¨å®ƒä»¬ï¼Œæ‚¨å°†ä¸å¾—ä¸èŠ±è´¹ä¸€äº›æ—¶é—´é€‚åº”æ–°çš„ç¼–ç¨‹ç¯å¢ƒï¼Œå¯èƒ½æ— æ³•å†ä½¿ç”¨æ‚¨å·²çŸ¥çš„è°ƒè¯•å·¥å…·ï¼Œå¦‚`ipdb`ã€‚
- en: 'For each code-base, a good first step is always to load a **small** pretrained
    checkpoint and to be able to reproduce a single forward pass using a dummy integer
    vector of input IDs as an input. Such a script could look like this (in pseudocode):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªä»£ç åº“ï¼Œä¸€ä¸ªå¾ˆå¥½çš„ç¬¬ä¸€æ­¥æ€»æ˜¯åŠ è½½ä¸€ä¸ª**å°**çš„é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œå¹¶èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿæ•´æ•°å‘é‡çš„è¾“å…¥IDè¿›è¡Œå•ä¸ªå‰å‘ä¼ é€’ã€‚è¿™æ ·çš„è„šæœ¬å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼ˆä¼ªä»£ç ï¼‰ï¼š
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, regarding the debugging strategy, there are generally a few from which
    to choose from:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå…³äºè°ƒè¯•ç­–ç•¥ï¼Œé€šå¸¸æœ‰å‡ ç§é€‰æ‹©ï¼š
- en: Decompose the original model into many small testable components and run a forward
    pass on each of those for verification
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†åŸå§‹æ¨¡å‹åˆ†è§£ä¸ºè®¸å¤šå°çš„å¯æµ‹è¯•ç»„ä»¶ï¼Œå¹¶åœ¨æ¯ä¸ªç»„ä»¶ä¸Šè¿è¡Œå‰å‘ä¼ é€’ä»¥è¿›è¡ŒéªŒè¯
- en: Decompose the original model only into the original *tokenizer* and the original
    *model*, run a forward pass on those, and use intermediate print statements or
    breakpoints for verification
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†åŸå§‹æ¨¡å‹ä»…åˆ†è§£ä¸ºåŸå§‹*tokenizer*å’ŒåŸå§‹*model*ï¼Œåœ¨è¿™äº›ä¸Šè¿è¡Œå‰å‘ä¼ é€’ï¼Œå¹¶ä½¿ç”¨ä¸­é—´æ‰“å°è¯­å¥æˆ–æ–­ç‚¹è¿›è¡ŒéªŒè¯
- en: Again, it is up to you which strategy to choose. Often, one or the other is
    advantageous depending on the original code base.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œé€‰æ‹©å“ªç§ç­–ç•¥å–å†³äºä½ ã€‚é€šå¸¸ï¼Œæ ¹æ®åŸå§‹ä»£ç åº“çš„æƒ…å†µï¼Œä¸€ç§æˆ–å¦ä¸€ç§ç­–ç•¥éƒ½æœ‰ä¼˜åŠ¿ã€‚
- en: 'If the original code-base allows you to decompose the model into smaller sub-components,
    *e.g.* if the original code-base can easily be run in eager mode, it is usually
    worth the effort to do so. There are some important advantages to taking the more
    difficult road in the beginning:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåŸå§‹ä»£ç åº“å…è®¸æ‚¨å°†æ¨¡å‹åˆ†è§£ä¸ºè¾ƒå°çš„å­ç»„ä»¶ï¼Œ*ä¾‹å¦‚*ï¼Œå¦‚æœåŸå§‹ä»£ç åº“å¯ä»¥è½»æ¾åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹è¿è¡Œï¼Œé‚£ä¹ˆé€šå¸¸å€¼å¾—è¿™æ ·åšã€‚åœ¨ä¸€å¼€å§‹é‡‡å–æ›´å›°éš¾çš„é“è·¯æœ‰ä¸€äº›é‡è¦çš„ä¼˜åŠ¿ï¼š
- en: at a later stage when comparing the original model to the Hugging Face implementation,
    you can verify automatically for each component individually that the corresponding
    component of the ğŸ¤— Transformers implementation matches instead of relying on visual
    comparison via print statements
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¨åé˜¶æ®µï¼Œå½“å°†åŸå§‹æ¨¡å‹ä¸Hugging Faceå®ç°è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæ‚¨å¯ä»¥è‡ªåŠ¨éªŒè¯æ¯ä¸ªç»„ä»¶æ˜¯å¦ä¸ğŸ¤— Transformerså®ç°çš„ç›¸åº”ç»„ä»¶åŒ¹é…ï¼Œè€Œä¸æ˜¯ä¾èµ–é€šè¿‡æ‰“å°è¯­å¥è¿›è¡Œè§†è§‰æ¯”è¾ƒ
- en: it can give you some rope to decompose the big problem of porting a model into
    smaller problems of just porting individual components and thus structure your
    work better
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥è®©æ‚¨å°†å°†æ¨¡å‹ç§»æ¤ä¸ºè¾ƒå°é—®é¢˜çš„å¤§é—®é¢˜åˆ†è§£ä¸ºä»…å°†å•ä¸ªç»„ä»¶ç§»æ¤ä¸ºç»“æ„åŒ–å·¥ä½œçš„æ›´å¥½çš„æ–¹æ³•ã€‚
- en: separating the model into logical meaningful components will help you to get
    a better overview of the modelâ€™s design and thus to better understand the model
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åˆ†è§£ä¸ºé€»è¾‘æœ‰æ„ä¹‰çš„ç»„ä»¶å°†æœ‰åŠ©äºæ›´å¥½åœ°äº†è§£æ¨¡å‹çš„è®¾è®¡ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£æ¨¡å‹
- en: at a later stage those component-by-component tests help you to ensure that
    no regression occurs as you continue changing your code
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¨åé˜¶æ®µï¼Œè¿™äº›é€ä¸ªç»„ä»¶çš„æµ‹è¯•æœ‰åŠ©äºç¡®ä¿åœ¨ç»§ç»­æ›´æ”¹ä»£ç æ—¶ä¸ä¼šå‘ç”Ÿé€€åŒ–
- en: '[Lysandreâ€™s](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed)
    integration checks for ELECTRA gives a nice example of how this can be done.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lysandreçš„](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed)
    ELECTRAé›†æˆæ£€æŸ¥ä¸ºå¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œæä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„ç¤ºä¾‹ã€‚'
- en: However, if the original code-base is very complex or only allows intermediate
    components to be run in a compiled mode, it might be too time-consuming or even
    impossible to separate the model into smaller testable sub-components. A good
    example is [T5â€™s MeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow)
    library which is very complex and does not offer a simple way to decompose the
    model into its sub-components. For such libraries, one often relies on verifying
    print statements.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœåŸå§‹ä»£ç åº“éå¸¸å¤æ‚ï¼Œæˆ–è€…åªå…è®¸ä»¥ç¼–è¯‘æ¨¡å¼è¿è¡Œä¸­é—´ç»„ä»¶ï¼Œé‚£ä¹ˆå°†æ¨¡å‹åˆ†è§£ä¸ºå¯æµ‹è¯•çš„è¾ƒå°å­ç»„ä»¶å¯èƒ½ä¼šè€—è´¹å¤ªå¤šæ—¶é—´ï¼Œç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯[T5çš„MeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow)åº“ï¼Œå®ƒéå¸¸å¤æ‚ï¼Œæ²¡æœ‰æä¾›å°†æ¨¡å‹åˆ†è§£ä¸ºå­ç»„ä»¶çš„ç®€å•æ–¹æ³•ã€‚å¯¹äºè¿™ç§åº“ï¼Œäººä»¬é€šå¸¸ä¾èµ–äºéªŒè¯æ‰“å°è¯­å¥ã€‚
- en: No matter which strategy you choose, the recommended procedure is often the
    same that you should start to debug the starting layers first and the ending layers
    last.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ‚¨é€‰æ‹©å“ªç§ç­–ç•¥ï¼Œæ¨èçš„ç¨‹åºé€šå¸¸æ˜¯ç›¸åŒçš„ï¼Œå³åº”è¯¥ä»è°ƒè¯•èµ·å§‹å›¾å±‚å¼€å§‹ï¼Œæœ€åè°ƒè¯•ç»“æŸå›¾å±‚ã€‚
- en: 'It is recommended that you retrieve the output, either by print statements
    or sub-component functions, of the following layers in the following order:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºè®®æŒ‰ç…§ä»¥ä¸‹é¡ºåºæ£€ç´¢ä»¥ä¸‹å›¾å±‚çš„è¾“å‡ºï¼Œå¯ä»¥é€šè¿‡æ‰“å°è¯­å¥æˆ–å­ç»„ä»¶å‡½æ•°æ¥å®ç°ï¼š
- en: Retrieve the input IDs passed to the model
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€ç´¢ä¼ é€’ç»™æ¨¡å‹çš„è¾“å…¥ID
- en: Retrieve the word embeddings
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€ç´¢å•è¯åµŒå…¥
- en: Retrieve the input of the first Transformer layer
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€ç´¢ç¬¬ä¸€ä¸ªTransformerå±‚çš„è¾“å…¥
- en: Retrieve the output of the first Transformer layer
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€ç´¢ç¬¬ä¸€ä¸ªTransformerå±‚çš„è¾“å‡º
- en: Retrieve the output of the following n - 1 Transformer layers
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€ç´¢ä»¥ä¸‹n-1ä¸ªTransformerå±‚çš„è¾“å‡º
- en: Retrieve the output of the whole BrandNewBert Model
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€ç´¢æ•´ä¸ªBrandNewBertæ¨¡å‹çš„è¾“å‡º
- en: Input IDs should thereby consists of an array of integers, *e.g.* `input_ids
    = [0, 4, 4, 3, 2, 4, 1, 7, 19]`
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥IDåº”è¯¥ç”±æ•´æ•°æ•°ç»„ç»„æˆï¼Œä¾‹å¦‚ `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`
- en: 'The outputs of the following layers often consist of multi-dimensional float
    arrays and can look like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾å±‚çš„è¾“å‡ºé€šå¸¸ç”±å¤šç»´æµ®ç‚¹æ•°ç»„ç»„æˆï¼Œå¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We expect that every model added to ğŸ¤— Transformers passes a couple of integration
    tests, meaning that the original model and the reimplemented version in ğŸ¤— Transformers
    have to give the exact same output up to a precision of 0.001! Since it is normal
    that the exact same model written in different libraries can give a slightly different
    output depending on the library framework, we accept an error tolerance of 1e-3
    (0.001). It is not enough if the model gives nearly the same output, they have
    to be almost identical. Therefore, you will certainly compare the intermediate
    outputs of the ğŸ¤— Transformers version multiple times against the intermediate
    outputs of the original implementation of *brand_new_bert* in which case an **efficient**
    debugging environment of the original repository is absolutely important. Here
    is some advice to make your debugging environment as efficient as possible.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœŸæœ›æ¯ä¸ªæ·»åŠ åˆ°ğŸ¤— Transformersçš„æ¨¡å‹éƒ½ç»è¿‡å‡ ä¸ªé›†æˆæµ‹è¯•ï¼Œè¿™æ„å‘³ç€åŸå§‹æ¨¡å‹å’ŒğŸ¤— Transformersä¸­é‡æ–°å®ç°çš„ç‰ˆæœ¬å¿…é¡»åœ¨ç²¾åº¦ä¸º0.001çš„æƒ…å†µä¸‹ç»™å‡ºå®Œå…¨ç›¸åŒçš„è¾“å‡ºï¼ç”±äºç›¸åŒæ¨¡å‹åœ¨ä¸åŒåº“ä¸­ç¼–å†™å¯èƒ½ä¼šæ ¹æ®åº“æ¡†æ¶ç»™å‡ºç•¥æœ‰ä¸åŒçš„è¾“å‡ºï¼Œæˆ‘ä»¬æ¥å—1e-3ï¼ˆ0.001ï¼‰çš„è¯¯å·®å®¹é™ã€‚å¦‚æœæ¨¡å‹ç»™å‡ºçš„è¾“å‡ºå‡ ä¹ç›¸åŒæ˜¯ä¸å¤Ÿçš„ï¼Œå®ƒä»¬å¿…é¡»å‡ ä¹å®Œå…¨ç›¸åŒã€‚å› æ­¤ï¼Œæ‚¨è‚¯å®šä¼šå¤šæ¬¡å°†ğŸ¤—
    Transformersç‰ˆæœ¬çš„ä¸­é—´è¾“å‡ºä¸*brand_new_bert*çš„åŸå§‹å®ç°çš„ä¸­é—´è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŸå§‹å­˜å‚¨åº“çš„**é«˜æ•ˆ**è°ƒè¯•ç¯å¢ƒç»å¯¹é‡è¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼Œä»¥ä½¿æ‚¨çš„è°ƒè¯•ç¯å¢ƒå°½å¯èƒ½é«˜æ•ˆã€‚
- en: Find the best way of debugging intermediate results. Is the original repository
    written in PyTorch? Then you should probably take the time to write a longer script
    that decomposes the original model into smaller sub-components to retrieve intermediate
    values. Is the original repository written in Tensorflow 1? Then you might have
    to rely on TensorFlow print operations like [tf.print](https://www.tensorflow.org/api_docs/python/tf/print)
    to output intermediate values. Is the original repository written in Jax? Then
    make sure that the model is **not jitted** when running the forward pass, *e.g.*
    check-out [this link](https://github.com/google/jax/issues/196).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°è°ƒè¯•ä¸­é—´ç»“æœçš„æœ€ä½³æ–¹æ³•ã€‚åŸå§‹å­˜å‚¨åº“æ˜¯ç”¨PyTorchç¼–å†™çš„å—ï¼Ÿé‚£ä¹ˆæ‚¨å¯èƒ½éœ€è¦èŠ±æ—¶é—´ç¼–å†™ä¸€ä¸ªæ›´é•¿çš„è„šæœ¬ï¼Œå°†åŸå§‹æ¨¡å‹åˆ†è§£ä¸ºè¾ƒå°çš„å­ç»„ä»¶ä»¥æ£€ç´¢ä¸­é—´å€¼ã€‚åŸå§‹å­˜å‚¨åº“æ˜¯ç”¨Tensorflow
    1ç¼–å†™çš„å—ï¼Ÿé‚£ä¹ˆæ‚¨å¯èƒ½éœ€è¦ä¾èµ–TensorFlowçš„æ‰“å°æ“ä½œï¼Œå¦‚ [tf.print](https://www.tensorflow.org/api_docs/python/tf/print)
    æ¥è¾“å‡ºä¸­é—´å€¼ã€‚åŸå§‹å­˜å‚¨åº“æ˜¯ç”¨Jaxç¼–å†™çš„å—ï¼Ÿé‚£ä¹ˆè¯·ç¡®ä¿åœ¨è¿è¡Œå‰å‘ä¼ é€’æ—¶æ¨¡å‹**æœªè¢«jitç¼–è¯‘**ï¼Œä¾‹å¦‚æŸ¥çœ‹ [æ­¤é“¾æ¥](https://github.com/google/jax/issues/196)ã€‚
- en: Use the smallest pretrained checkpoint you can find. The smaller the checkpoint,
    the faster your debug cycle becomes. It is not efficient if your pretrained model
    is so big that your forward pass takes more than 10 seconds. In case only very
    large checkpoints are available, it might make more sense to create a dummy model
    in the new environment with randomly initialized weights and save those weights
    for comparison with the ğŸ¤— Transformers version of your model
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨èƒ½æ‰¾åˆ°çš„æœ€å°çš„é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚æ£€æŸ¥ç‚¹è¶Šå°ï¼Œæ‚¨çš„è°ƒè¯•å‘¨æœŸå°±è¶Šå¿«ã€‚å¦‚æœæ‚¨çš„é¢„è®­ç»ƒæ¨¡å‹å¤ªå¤§ï¼Œå¯¼è‡´å‰å‘ä¼ é€’è¶…è¿‡10ç§’ï¼Œé‚£å°±ä¸é«˜æ•ˆäº†ã€‚å¦‚æœåªæœ‰éå¸¸å¤§çš„æ£€æŸ¥ç‚¹å¯ç”¨ï¼Œå¯èƒ½æ›´æœ‰æ„ä¹‰çš„æ˜¯åœ¨æ–°ç¯å¢ƒä¸­åˆ›å»ºä¸€ä¸ªå¸¦æœ‰éšæœºåˆå§‹åŒ–æƒé‡çš„è™šæ‹Ÿæ¨¡å‹ï¼Œå¹¶ä¿å­˜è¿™äº›æƒé‡ä»¥ä¾¿ä¸æ‚¨æ¨¡å‹çš„ğŸ¤—
    Transformersç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒã€‚
- en: Make sure you are using the easiest way of calling a forward pass in the original
    repository. Ideally, you want to find the function in the original repository
    that **only** calls a single forward pass, *i.e.* that is often called `predict`,
    `evaluate`, `forward` or `__call__`. You donâ€™t want to debug a function that calls
    `forward` multiple times, *e.g.* to generate text, like `autoregressive_sample`,
    `generate`.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡®ä¿æ‚¨æ­£åœ¨ä½¿ç”¨åŸå§‹å­˜å‚¨åº“ä¸­è°ƒç”¨å‰å‘ä¼ é€’çš„æœ€ç®€å•æ–¹æ³•ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨å¸Œæœ›æ‰¾åˆ°åŸå§‹å­˜å‚¨åº“ä¸­**ä»…**è°ƒç”¨å•ä¸ªå‰å‘ä¼ é€’çš„å‡½æ•°ï¼Œå³é€šå¸¸ç§°ä¸º `predict`ã€`evaluate`ã€`forward`
    æˆ– `__call__`ã€‚æ‚¨ä¸å¸Œæœ›è°ƒè¯•å¤šæ¬¡è°ƒç”¨ `forward` çš„å‡½æ•°ï¼Œä¾‹å¦‚ç”Ÿæˆæ–‡æœ¬çš„ `autoregressive_sample`ã€`generate`ã€‚
- en: Try to separate the tokenization from the modelâ€™s *forward* pass. If the original
    repository shows examples where you have to input a string, then try to find out
    where in the forward call the string input is changed to input ids and start from
    this point. This might mean that you have to possibly write a small script yourself
    or change the original code so that you can directly input the ids instead of
    an input string.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•å°†æ ‡è®°åŒ–ä¸æ¨¡å‹çš„*forward*ä¼ é€’åˆ†å¼€ã€‚å¦‚æœåŸå§‹å­˜å‚¨åº“æ˜¾ç¤ºç¤ºä¾‹ï¼Œæ‚¨å¿…é¡»è¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œåˆ™å°è¯•æ‰¾å‡ºåœ¨å‰å‘è°ƒç”¨ä¸­å­—ç¬¦ä¸²è¾“å…¥ä½•æ—¶æ›´æ”¹ä¸ºè¾“å…¥idï¼Œå¹¶ä»æ­¤ç‚¹å¼€å§‹ã€‚è¿™å¯èƒ½æ„å‘³ç€æ‚¨å¯èƒ½éœ€è¦è‡ªå·±ç¼–å†™ä¸€ä¸ªå°è„šæœ¬æˆ–æ›´æ”¹åŸå§‹ä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ç›´æ¥è¾“å…¥idè€Œä¸æ˜¯è¾“å…¥å­—ç¬¦ä¸²ã€‚
- en: Make sure that the model in your debugging setup is **not** in training mode,
    which often causes the model to yield random outputs due to multiple dropout layers
    in the model. Make sure that the forward pass in your debugging environment is
    **deterministic** so that the dropout layers are not used. Or use *transformers.utils.set_seed*
    if the old and new implementations are in the same framework.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡®ä¿æ‚¨è°ƒè¯•è®¾ç½®ä¸­çš„æ¨¡å‹**ä¸**å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹ç”±äºæ¨¡å‹ä¸­çš„å¤šä¸ªdropoutå±‚è€Œäº§ç”Ÿéšæœºè¾“å‡ºã€‚ç¡®ä¿æ‚¨è°ƒè¯•ç¯å¢ƒä¸­çš„å‰å‘ä¼ é€’æ˜¯**ç¡®å®šæ€§**çš„ï¼Œä»¥ä¾¿ä¸ä½¿ç”¨dropoutå±‚ã€‚æˆ–è€…å¦‚æœæ—§å®ç°å’Œæ–°å®ç°åœ¨åŒä¸€æ¡†æ¶ä¸­ï¼Œåˆ™ä½¿ç”¨*transformers.utils.set_seed*ã€‚
- en: The following section gives you more specific details/tips on how you can do
    this for *brand_new_bert*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€èŠ‚å°†ä¸ºæ‚¨æä¾›æœ‰å…³å¦‚ä½•ä¸º*brand_new_bert*æ‰§è¡Œæ­¤æ“ä½œçš„æ›´å…·ä½“è¯¦ç»†ä¿¡æ¯/æç¤ºã€‚
- en: 5.-14\. Port BrandNewBert to ğŸ¤— Transformers
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.-14\. å°†BrandNewBertç§»æ¤åˆ°ğŸ¤— Transformers
- en: 'Next, you can finally start adding new code to ğŸ¤— Transformers. Go into the
    clone of your ğŸ¤— Transformersâ€™ fork:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥å¼€å§‹å‘ğŸ¤— Transformersæ·»åŠ æ–°ä»£ç ã€‚è¿›å…¥æ‚¨ğŸ¤— Transformersåˆ†æ”¯çš„å…‹éš†ï¼š
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the special case that you are adding a model whose architecture exactly matches
    the model architecture of an existing model you only have to add a conversion
    script as described in [this section](#write-a-conversion-script). In this case,
    you can just re-use the whole model architecture of the already existing model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œå¦‚æœæ‚¨è¦æ·»åŠ çš„æ¨¡å‹çš„æ¶æ„ä¸ç°æœ‰æ¨¡å‹çš„æ¨¡å‹æ¶æ„å®Œå…¨åŒ¹é…ï¼Œåˆ™åªéœ€æ·»åŠ ä¸€ä¸ªè½¬æ¢è„šæœ¬ï¼Œå¦‚[æ­¤éƒ¨åˆ†](#write-a-conversion-script)æ‰€è¿°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥ç›´æ¥é‡ç”¨å·²å­˜åœ¨æ¨¡å‹çš„æ•´ä¸ªæ¨¡å‹æ¶æ„ã€‚
- en: 'Otherwise, letâ€™s start generating a new model. You have two choices here:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç”Ÿæˆä¸€ä¸ªæ–°æ¨¡å‹ã€‚æ‚¨åœ¨è¿™é‡Œæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š
- en: '`transformers-cli add-new-model-like` to add a new model like an existing one'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-cli add-new-model-like`ä»¥æ·»åŠ ä¸€ä¸ªç±»ä¼¼äºç°æœ‰æ¨¡å‹çš„æ–°æ¨¡å‹'
- en: '`transformers-cli add-new-model` to add a new model from our template (will
    look like BERT or Bart depending on the type of model you select)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-cli add-new-model`ä»¥ä»æˆ‘ä»¬çš„æ¨¡æ¿ä¸­æ·»åŠ ä¸€ä¸ªæ–°æ¨¡å‹ï¼ˆå°†çœ‹èµ·æ¥åƒBERTæˆ–Bartï¼Œå…·ä½“å–å†³äºæ‚¨é€‰æ‹©çš„æ¨¡å‹ç±»å‹ï¼‰'
- en: In both cases, you will be prompted with a questionnaire to fill in the basic
    information of your model. The second command requires to install `cookiecutter`,
    you can find more information on it [here](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæ‚¨å°†è¢«æç¤ºå¡«å†™æœ‰å…³æ‚¨çš„æ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯çš„é—®å·ã€‚ç¬¬äºŒä¸ªå‘½ä»¤éœ€è¦å®‰è£…`cookiecutter`ï¼Œæ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model)æ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚
- en: '**Open a Pull Request on the main huggingface/transformers repo**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨ä¸»huggingface/transformersä»“åº“ä¸Šæ‰“å¼€ä¸€ä¸ªæ‹‰å–è¯·æ±‚**'
- en: Before starting to adapt the automatically generated code, now is the time to
    open a â€œWork in progress (WIP)â€ pull request, *e.g.* â€œ[WIP] Add *brand_new_bert*â€,
    in ğŸ¤— Transformers so that you and the Hugging Face team can work side-by-side
    on integrating the model into ğŸ¤— Transformers.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è°ƒæ•´è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç ä¹‹å‰ï¼Œç°åœ¨æ˜¯æ—¶å€™åœ¨ğŸ¤— Transformersä¸­æ‰“å¼€ä¸€ä¸ªâ€œè¿›è¡Œä¸­çš„å·¥ä½œï¼ˆWIPï¼‰â€æ‹‰å–è¯·æ±‚ï¼Œä¾‹å¦‚â€œ[WIP]æ·»åŠ *brand_new_bert*â€ï¼Œä»¥ä¾¿æ‚¨å’ŒHugging
    Faceå›¢é˜Ÿå¯ä»¥å¹¶è‚©åˆä½œå°†æ¨¡å‹é›†æˆåˆ°ğŸ¤— Transformersä¸­ã€‚
- en: 'You should do the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Create a branch with a descriptive name from your main branch
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ä¸»åˆ†æ”¯åˆ›å»ºä¸€ä¸ªå…·æœ‰æè¿°æ€§åç§°çš„åˆ†æ”¯
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Commit the automatically generated code:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æäº¤è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç ï¼š
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Fetch and rebase to current main
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è·å–å¹¶rebaseåˆ°å½“å‰ä¸»åˆ†æ”¯
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Push the changes to your account using:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ›´æ”¹æ¨é€åˆ°æ‚¨çš„å¸æˆ·ï¼š
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once you are satisfied, go to the webpage of your fork on GitHub. Click on â€œPull
    requestâ€. Make sure to add the GitHub handle of some members of the Hugging Face
    team as reviewers, so that the Hugging Face team gets notified for future changes.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨æ»¡æ„ï¼Œè½¬åˆ°GitHubä¸Šæ‚¨çš„åˆ†æ”¯çš„ç½‘é¡µã€‚ç‚¹å‡»â€œæ‹‰å–è¯·æ±‚â€ã€‚ç¡®ä¿å°†Hugging Faceå›¢é˜Ÿçš„ä¸€äº›æˆå‘˜çš„GitHubå¥æŸ„æ·»åŠ ä¸ºå®¡é˜…è€…ï¼Œä»¥ä¾¿Hugging
    Faceå›¢é˜Ÿåœ¨å°†æ¥çš„æ›´æ”¹ä¸­æ”¶åˆ°é€šçŸ¥ã€‚
- en: Change the PR into a draft by clicking on â€œConvert to draftâ€ on the right of
    the GitHub pull request web page.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‚¹å‡»GitHubæ‹‰å–è¯·æ±‚ç½‘é¡µå³ä¾§çš„â€œè½¬æ¢ä¸ºè‰ç¨¿â€å°†PRè½¬æ¢ä¸ºè‰ç¨¿ã€‚
- en: 'In the following, whenever you have made some progress, donâ€™t forget to commit
    your work and push it to your account so that it shows in the pull request. Additionally,
    you should make sure to update your work with the current main from time to time
    by doing:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„è¿‡ç¨‹ä¸­ï¼Œæ¯å½“æ‚¨å–å¾—ä¸€äº›è¿›å±•æ—¶ï¼Œä¸è¦å¿˜è®°æäº¤æ‚¨çš„å·¥ä½œå¹¶å°†å…¶æ¨é€åˆ°æ‚¨çš„å¸æˆ·ï¼Œä»¥ä¾¿åœ¨æ‹‰å–è¯·æ±‚ä¸­æ˜¾ç¤ºã€‚æ­¤å¤–ï¼Œæ‚¨åº”è¯¥ç¡®ä¿ä¸æ—¶ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•æ›´æ–°æ‚¨çš„å·¥ä½œä¸å½“å‰ä¸»åˆ†æ”¯ï¼š
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In general, all questions you might have regarding the model or your implementation
    should be asked in your PR and discussed/solved in the PR. This way, the Hugging
    Face team will always be notified when you are committing new code or if you have
    a question. It is often very helpful to point the Hugging Face team to your added
    code so that the Hugging Face team can efficiently understand your problem or
    question.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œæ‚¨å¯èƒ½å¯¹æ¨¡å‹æˆ–æ‚¨çš„å®ç°æœ‰ä»»ä½•é—®é¢˜éƒ½åº”è¯¥åœ¨æ‚¨çš„PRä¸­æå‡ºï¼Œå¹¶åœ¨PRä¸­è®¨è®º/è§£å†³ã€‚è¿™æ ·ï¼Œå½“æ‚¨æäº¤æ–°ä»£ç æˆ–æœ‰é—®é¢˜æ—¶ï¼ŒHugging Faceå›¢é˜Ÿå°†å§‹ç»ˆæ”¶åˆ°é€šçŸ¥ã€‚å°†Hugging
    Faceå›¢é˜ŸæŒ‡å‘æ‚¨æ·»åŠ çš„ä»£ç é€šå¸¸éå¸¸æœ‰å¸®åŠ©ï¼Œä»¥ä¾¿Hugging Faceå›¢é˜Ÿå¯ä»¥é«˜æ•ˆåœ°ç†è§£æ‚¨çš„é—®é¢˜æˆ–ç–‘é—®ã€‚
- en: To do so, you can go to the â€œFiles changedâ€ tab where you see all of your changes,
    go to a line regarding which you want to ask a question, and click on the â€œ+â€
    symbol to add a comment. Whenever a question or problem has been solved, you can
    click on the â€œResolveâ€ button of the created comment.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥è½¬åˆ°â€œæ›´æ”¹çš„æ–‡ä»¶â€é€‰é¡¹å¡ï¼Œåœ¨é‚£é‡Œæ‚¨å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ›´æ”¹ï¼Œè½¬åˆ°æ‚¨æƒ³è¦æé—®çš„è¡Œï¼Œå¹¶å•å‡»â€œ+â€ç¬¦å·æ·»åŠ è¯„è®ºã€‚æ¯å½“é—®é¢˜æˆ–é—®é¢˜å¾—åˆ°è§£å†³æ—¶ï¼Œæ‚¨å¯ä»¥å•å‡»å·²åˆ›å»ºè¯„è®ºçš„â€œè§£å†³â€æŒ‰é’®ã€‚
- en: In the same way, the Hugging Face team will open comments when reviewing your
    code. We recommend asking most questions on GitHub on your PR. For some very general
    questions that are not very useful for the public, feel free to ping the Hugging
    Face team by Slack or email.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼ŒHugging Faceå›¢é˜Ÿåœ¨å®¡æŸ¥æ‚¨çš„ä»£ç æ—¶ä¼šå¼€æ”¾è¯„è®ºã€‚æˆ‘ä»¬å»ºè®®åœ¨GitHubä¸Šçš„PRä¸Šæå‡ºå¤§å¤šæ•°é—®é¢˜ã€‚å¯¹äºä¸€äº›å¯¹å…¬ä¼—ä¸å¤ªæœ‰ç”¨çš„éå¸¸ä¸€èˆ¬æ€§çš„é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡Slackæˆ–ç”µå­é‚®ä»¶è”ç³»Hugging
    Faceå›¢é˜Ÿã€‚
- en: '**5\. Adapt the generated models code for brand_new_bert**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. ä¸ºbrand_new_bertè°ƒæ•´ç”Ÿæˆçš„æ¨¡å‹ä»£ç **'
- en: At first, we will focus only on the model itself and not care about the tokenizer.
    All the relevant code should be found in the generated files `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    and `src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†åªå…³æ³¨æ¨¡å‹æœ¬èº«ï¼Œä¸å…³å¿ƒåˆ†è¯å™¨ã€‚æ‰€æœ‰ç›¸å…³ä»£ç åº”è¯¥åœ¨ç”Ÿæˆçš„æ–‡ä»¶`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`å’Œ`src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`ä¸­æ‰¾åˆ°ã€‚
- en: 'Now you can finally start coding :). The generated code in `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    will either have the same architecture as BERT if itâ€™s an encoder-only model or
    BART if itâ€™s an encoder-decoder model. At this point, you should remind yourself
    what youâ€™ve learned in the beginning about the theoretical aspects of the model:
    *How is the model different from BERT or BART?*â€. Implement those changes which
    often means changing the *self-attention* layer, the order of the normalization
    layer, etcâ€¦ Again, it is often useful to look at the similar architecture of already
    existing models in Transformers to get a better feeling of how your model should
    be implemented.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹ç¼–ç äº† :). ç”Ÿæˆçš„ä»£ç åœ¨`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`ä¸­å°†å…·æœ‰ä¸BERTç›¸åŒçš„æ¶æ„ï¼ˆå¦‚æœæ˜¯ä»…ç¼–ç å™¨æ¨¡å‹ï¼‰æˆ–ä¸BARTç›¸åŒçš„æ¶æ„ï¼ˆå¦‚æœæ˜¯ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼‰ã€‚æ­¤æ—¶ï¼Œæ‚¨åº”è¯¥å›æƒ³èµ·æ‚¨åœ¨å¼€å§‹æ—¶å­¦åˆ°çš„å…³äºæ¨¡å‹ç†è®ºæ–¹é¢çš„çŸ¥è¯†ï¼šâ€œè¯¥æ¨¡å‹ä¸BERTæˆ–BARTæœ‰ä½•ä¸åŒï¼Ÿâ€å®ç°è¿™äº›å˜åŒ–é€šå¸¸æ„å‘³ç€æ›´æ”¹*self-attention*å±‚ï¼Œè§„èŒƒåŒ–å±‚çš„é¡ºåºç­‰ç­‰...å†æ¬¡å¼ºè°ƒï¼ŒæŸ¥çœ‹Transformersä¸­å·²ç»å­˜åœ¨çš„ç±»ä¼¼æ¨¡å‹çš„æ¶æ„é€šå¸¸æ˜¯æœ‰ç”¨çš„ï¼Œä»¥æ›´å¥½åœ°äº†è§£å¦‚ä½•å®ç°æ‚¨çš„æ¨¡å‹ã€‚
- en: '**Note** that at this point, you donâ€™t have to be very sure that your code
    is fully correct or clean. Rather, it is advised to add a first *unclean*, copy-pasted
    version of the original code to `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    until you feel like all the necessary code is added. From our experience, it is
    much more efficient to quickly add a first version of the required code and improve/correct
    the code iteratively with the conversion script as described in the next section.
    The only thing that has to work at this point is that you can instantiate the
    ğŸ¤— Transformers implementation of *brand_new_bert*, *i.e.* the following command
    should work:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼Œåœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæ‚¨ä¸å¿…éå¸¸ç¡®å®šæ‚¨çš„ä»£ç æ˜¯å¦å®Œå…¨æ­£ç¡®æˆ–å¹²å‡€ã€‚ç›¸åï¼Œå»ºè®®å°†åŸå§‹ä»£ç çš„ç¬¬ä¸€ä¸ª*ä¸å¹²å‡€*ã€å¤åˆ¶ç²˜è´´ç‰ˆæœ¬æ·»åŠ åˆ°`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`ï¼Œç›´åˆ°æ‚¨è§‰å¾—æ‰€æœ‰å¿…è¦çš„ä»£ç éƒ½å·²æ·»åŠ ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œå¿«é€Ÿæ·»åŠ æ‰€éœ€ä»£ç çš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼Œå¹¶ä½¿ç”¨ä¸‹ä¸€èŠ‚ä¸­æè¿°çš„è½¬æ¢è„šæœ¬è¿­ä»£åœ°æ”¹è¿›/çº æ­£ä»£ç æ•ˆç‡æ›´é«˜ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œå”¯ä¸€éœ€è¦å·¥ä½œçš„æ˜¯æ‚¨å¯ä»¥å®ä¾‹åŒ–ğŸ¤—
    Transformerså®ç°çš„*brand_new_bert*ï¼Œå³ä»¥ä¸‹å‘½ä»¤åº”è¯¥å¯ä»¥å·¥ä½œï¼š'
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The above command will create a model according to the default parameters as
    defined in `BrandNewBertConfig()` with random weights, thus making sure that the
    `init()` methods of all components works.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å‘½ä»¤å°†æ ¹æ®`BrandNewBertConfig()`ä¸­å®šä¹‰çš„é»˜è®¤å‚æ•°åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œå…·æœ‰éšæœºæƒé‡ï¼Œä»è€Œç¡®ä¿æ‰€æœ‰ç»„ä»¶çš„`init()`æ–¹æ³•æ­£å¸¸å·¥ä½œã€‚
- en: 'Note that all random initialization should happen in the `_init_weights` method
    of your `BrandnewBertPreTrainedModel` class. It should initialize all leaf modules
    depending on the variables of the config. Here is an example with the BERT `_init_weights`
    method:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ‰€æœ‰éšæœºåˆå§‹åŒ–åº”è¯¥åœ¨æ‚¨çš„`BrandnewBertPreTrainedModel`ç±»çš„`_init_weights`æ–¹æ³•ä¸­è¿›è¡Œã€‚å®ƒåº”è¯¥æ ¹æ®é…ç½®çš„å˜é‡åˆå§‹åŒ–æ‰€æœ‰å¶å­æ¨¡å—ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªä½¿ç”¨BERT
    `_init_weights`æ–¹æ³•çš„ç¤ºä¾‹ï¼š
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can have some more custom schemes if you need a special initialization
    for some modules. For instance, in `Wav2Vec2ForPreTraining`, the last two linear
    layers need to have the initialization of the regular PyTorch `nn.Linear` but
    all the other ones should use an initialization as above. This is coded like this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦æŸäº›æ¨¡å—çš„ç‰¹æ®Šåˆå§‹åŒ–ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸€äº›è‡ªå®šä¹‰æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œåœ¨`Wav2Vec2ForPreTraining`ä¸­ï¼Œæœ€åä¸¤ä¸ªçº¿æ€§å±‚éœ€è¦ä½¿ç”¨å¸¸è§„PyTorch
    `nn.Linear`çš„åˆå§‹åŒ–ï¼Œä½†æ‰€æœ‰å…¶ä»–å±‚åº”è¯¥ä½¿ç”¨ä¸Šè¿°åˆå§‹åŒ–ã€‚è¿™æ ·ç¼–ç ï¼š
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `_is_hf_initialized` flag is internally used to make sure we only initialize
    a submodule once. By setting it to `True` for `module.project_q` and `module.project_hid`,
    we make sure the custom initialization we did is not overridden later on, the
    `_init_weights` function wonâ€™t be applied to them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`_is_hf_initialized`æ ‡å¿—åœ¨å†…éƒ¨ç”¨äºç¡®ä¿æˆ‘ä»¬åªåˆå§‹åŒ–ä¸€ä¸ªå­æ¨¡å—ä¸€æ¬¡ã€‚é€šè¿‡å°†å…¶è®¾ç½®ä¸º`True`ï¼Œæˆ‘ä»¬ç¡®ä¿è‡ªå®šä¹‰åˆå§‹åŒ–ä¸ä¼šè¢«åæ¥è¦†ç›–ï¼Œ`_init_weights`å‡½æ•°ä¸ä¼šåº”ç”¨äºå®ƒä»¬ã€‚'
- en: '**6\. Write a conversion script**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. ç¼–å†™è½¬æ¢è„šæœ¬**'
- en: Next, you should write a conversion script that lets you convert the checkpoint
    you used to debug *brand_new_bert* in the original repository to a checkpoint
    compatible with your just created ğŸ¤— Transformers implementation of *brand_new_bert*.
    It is not advised to write the conversion script from scratch, but rather to look
    through already existing conversion scripts in ğŸ¤— Transformers for one that has
    been used to convert a similar model that was written in the same framework as
    *brand_new_bert*. Usually, it is enough to copy an already existing conversion
    script and slightly adapt it for your use case. Donâ€™t hesitate to ask the Hugging
    Face team to point you to a similar already existing conversion script for your
    model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨åº”è¯¥ç¼–å†™ä¸€ä¸ªè½¬æ¢è„šæœ¬ï¼Œè®©æ‚¨å¯ä»¥å°†æ‚¨åœ¨åŸå§‹å­˜å‚¨åº“ä¸­ç”¨äºè°ƒè¯•*brand_new_bert*çš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºä¸æ‚¨åˆšåˆšåˆ›å»ºçš„ğŸ¤— Transformerså®ç°*brand_new_bert*å…¼å®¹çš„æ£€æŸ¥ç‚¹ã€‚ä¸å»ºè®®ä»å¤´å¼€å§‹ç¼–å†™è½¬æ¢è„šæœ¬ï¼Œè€Œæ˜¯æŸ¥çœ‹ğŸ¤—
    Transformersä¸­å·²ç»å­˜åœ¨çš„è½¬æ¢è„šæœ¬ï¼Œæ‰¾åˆ°ä¸€ä¸ªå·²ç»ç”¨äºè½¬æ¢ä¸*brand_new_bert*ç›¸åŒæ¡†æ¶ç¼–å†™çš„ç±»ä¼¼æ¨¡å‹çš„è„šæœ¬ã€‚é€šå¸¸ï¼Œå¤åˆ¶ä¸€ä¸ªå·²ç»å­˜åœ¨çš„è½¬æ¢è„šæœ¬å¹¶ç¨å¾®è°ƒæ•´ä»¥é€‚åº”æ‚¨çš„ç”¨ä¾‹å°±è¶³å¤Ÿäº†ã€‚ä¸è¦çŠ¹è±«å‘Hugging
    Faceå›¢é˜Ÿè¯¢é—®æ˜¯å¦æœ‰ç±»ä¼¼çš„å·²ç»å­˜åœ¨çš„è½¬æ¢è„šæœ¬é€‚ç”¨äºæ‚¨çš„æ¨¡å‹ã€‚
- en: If you are porting a model from TensorFlow to PyTorch, a good starting point
    might be BERTâ€™s conversion script [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨å°†æ¨¡å‹ä»TensorFlowè½¬æ¢åˆ°PyTorchï¼Œä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹å¯èƒ½æ˜¯BERTçš„è½¬æ¢è„šæœ¬[æ­¤å¤„](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)
- en: If you are porting a model from PyTorch to PyTorch, a good starting point might
    be BARTâ€™s conversion script [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨å°†æ¨¡å‹ä»PyTorchè½¬æ¢åˆ°PyTorchï¼Œä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹å¯èƒ½æ˜¯BARTçš„è½¬æ¢è„šæœ¬[æ­¤å¤„](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)
- en: 'In the following, weâ€™ll quickly explain how PyTorch models store layer weights
    and define layer names. In PyTorch, the name of a layer is defined by the name
    of the class attribute you give the layer. Letâ€™s define a dummy model in PyTorch,
    called `SimpleModel` as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¿«é€Ÿè§£é‡ŠPyTorchæ¨¡å‹å¦‚ä½•å­˜å‚¨å±‚æƒé‡å¹¶å®šä¹‰å±‚åç§°ã€‚åœ¨PyTorchä¸­ï¼Œå±‚çš„åç§°ç”±æ‚¨ç»™äºˆè¯¥å±‚çš„ç±»å±æ€§çš„åç§°å®šä¹‰ã€‚è®©æˆ‘ä»¬åœ¨PyTorchä¸­å®šä¹‰ä¸€ä¸ªåä¸º`SimpleModel`çš„è™šæ‹Ÿæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can create an instance of this model definition which will fill all
    weights: `dense`, `intermediate`, `layer_norm` with random weights. We can print
    the model to see its architecture'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ›å»ºæ­¤æ¨¡å‹å®šä¹‰çš„å®ä¾‹ï¼Œè¯¥å®ä¾‹å°†å¡«å……æ‰€æœ‰æƒé‡ï¼š`dense`ã€`intermediate`ã€`layer_norm`ï¼Œä½¿ç”¨éšæœºæƒé‡ã€‚æˆ‘ä»¬å¯ä»¥æ‰“å°æ¨¡å‹ä»¥æŸ¥çœ‹å…¶æ¶æ„
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will print out the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ‰“å°å¦‚ä¸‹å†…å®¹ï¼š
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can see that the layer names are defined by the name of the class attribute
    in PyTorch. You can print out the weight values of a specific layer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å±‚çš„åç§°ç”±PyTorchä¸­ç±»å±æ€§çš„åç§°å®šä¹‰ã€‚æ‚¨å¯ä»¥æ‰“å°ç‰¹å®šå±‚çš„æƒé‡å€¼ï¼š
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: to see that the weights were randomly initialized
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æƒé‡æ˜¯å¦å·²éšæœºåˆå§‹åŒ–
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the conversion script, you should fill those randomly initialized weights
    with the exact weights of the corresponding layer in the checkpoint. *E.g.*
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è½¬æ¢è„šæœ¬ä¸­ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ç›¸åº”å±‚ä¸­çš„ç¡®åˆ‡æƒé‡å¡«å……è¿™äº›éšæœºåˆå§‹åŒ–çš„æƒé‡ã€‚*ä¾‹å¦‚*
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'While doing so, you must verify that each randomly initialized weight of your
    PyTorch model and its corresponding pretrained checkpoint weight exactly match
    in both **shape and name**. To do so, it is **necessary** to add assert statements
    for the shape and print out the names of the checkpoints weights. E.g. you should
    add statements like:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡Œæ­¤æ“ä½œæ—¶ï¼Œæ‚¨å¿…é¡»éªŒè¯æ‚¨çš„PyTorchæ¨¡å‹çš„æ¯ä¸ªéšæœºåˆå§‹åŒ–æƒé‡åŠå…¶å¯¹åº”çš„é¢„è®­ç»ƒæ£€æŸ¥ç‚¹æƒé‡åœ¨**å½¢çŠ¶å’Œåç§°**ä¸Šå®Œå…¨åŒ¹é…ã€‚ä¸ºæ­¤ï¼Œ**å¿…é¡»**æ·»åŠ å½¢çŠ¶çš„assertè¯­å¥å¹¶æ‰“å°å‡ºæ£€æŸ¥ç‚¹æƒé‡çš„åç§°ã€‚ä¾‹å¦‚ï¼Œæ‚¨åº”è¯¥æ·»åŠ ç±»ä¼¼ä»¥ä¸‹è¯­å¥ï¼š
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Besides, you should also print out the names of both weights to make sure they
    match, *e.g.*
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ‚¨è¿˜åº”è¯¥æ‰“å°å‡ºä¸¤ä¸ªæƒé‡çš„åç§°ï¼Œä»¥ç¡®ä¿å®ƒä»¬åŒ¹é…ï¼Œ*ä¾‹å¦‚*
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If either the shape or the name doesnâ€™t match, you probably assigned the wrong
    checkpoint weight to a randomly initialized layer of the ğŸ¤— Transformers implementation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå½¢çŠ¶æˆ–åç§°ä¸åŒ¹é…ï¼Œåˆ™æ‚¨å¯èƒ½å°†é”™è¯¯çš„æ£€æŸ¥ç‚¹æƒé‡åˆ†é…ç»™äº†ğŸ¤— Transformerså®ç°çš„éšæœºåˆå§‹åŒ–å±‚ã€‚
- en: An incorrect shape is most likely due to an incorrect setting of the config
    parameters in `BrandNewBertConfig()` that do not exactly match those that were
    used for the checkpoint you want to convert. However, it could also be that PyTorchâ€™s
    implementation of a layer requires the weight to be transposed beforehand.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ­£ç¡®çš„å½¢çŠ¶å¾ˆå¯èƒ½æ˜¯ç”±äºåœ¨`BrandNewBertConfig()`ä¸­ä¸æ­£ç¡®è®¾ç½®é…ç½®å‚æ•°ï¼Œè¿™äº›å‚æ•°ä¸æ‚¨è¦è½¬æ¢çš„æ£€æŸ¥ç‚¹ä½¿ç”¨çš„å‚æ•°ä¸å®Œå…¨åŒ¹é…ã€‚ä½†æ˜¯ï¼Œä¹Ÿå¯èƒ½æ˜¯PyTorchçš„å±‚å®ç°è¦æ±‚åœ¨ä¹‹å‰å¯¹æƒé‡è¿›è¡Œè½¬ç½®ã€‚
- en: Finally, you should also check that **all** required weights are initialized
    and print out all checkpoint weights that were not used for initialization to
    make sure the model is correctly converted. It is completely normal, that the
    conversion trials fail with either a wrong shape statement or a wrong name assignment.
    This is most likely because either you used incorrect parameters in `BrandNewBertConfig()`,
    have a wrong architecture in the ğŸ¤— Transformers implementation, you have a bug
    in the `init()` functions of one of the components of the ğŸ¤— Transformers implementation
    or you need to transpose one of the checkpoint weights.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨è¿˜åº”è¯¥æ£€æŸ¥**æ‰€æœ‰**å¿…éœ€çš„æƒé‡æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œå¹¶æ‰“å°å‡ºæ‰€æœ‰æœªç”¨äºåˆå§‹åŒ–çš„æ£€æŸ¥ç‚¹æƒé‡ï¼Œä»¥ç¡®ä¿æ¨¡å‹å·²æ­£ç¡®è½¬æ¢ã€‚å®Œå…¨æ­£å¸¸çš„æ˜¯ï¼Œè½¬æ¢å°è¯•å¯èƒ½ä¼šå› ä¸ºé”™è¯¯çš„å½¢çŠ¶è¯­å¥æˆ–é”™è¯¯çš„åç§°åˆ†é…è€Œå¤±è´¥ã€‚è¿™å¾ˆå¯èƒ½æ˜¯å› ä¸ºæ‚¨åœ¨`BrandNewBertConfig()`ä¸­ä½¿ç”¨äº†ä¸æ­£ç¡®çš„å‚æ•°ï¼Œåœ¨ğŸ¤—
    Transformerså®ç°ä¸­æœ‰é”™è¯¯çš„æ¶æ„ï¼Œæ‚¨åœ¨ğŸ¤— Transformerså®ç°çš„ä¸€ä¸ªç»„ä»¶çš„`init()`å‡½æ•°ä¸­æœ‰é”™è¯¯ï¼Œæˆ–è€…æ‚¨éœ€è¦è½¬ç½®ä¸€ä¸ªæ£€æŸ¥ç‚¹æƒé‡ã€‚
- en: 'This step should be iterated with the previous step until all weights of the
    checkpoint are correctly loaded in the Transformers model. Having correctly loaded
    the checkpoint into the ğŸ¤— Transformers implementation, you can then save the model
    under a folder of your choice `/path/to/converted/checkpoint/folder` that should
    then contain both a `pytorch_model.bin` file and a `config.json` file:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥é€šè¿‡å‰é¢çš„æ­¥éª¤è¿­ä»£æ­¤æ­¥éª¤ï¼Œç›´åˆ°æ­£ç¡®åŠ è½½æ‰€æœ‰æ£€æŸ¥ç‚¹çš„æƒé‡åˆ°Transformersæ¨¡å‹ä¸­ã€‚æ­£ç¡®åŠ è½½æ£€æŸ¥ç‚¹åˆ°ğŸ¤— Transformerså®ç°åï¼Œæ‚¨å¯ä»¥å°†æ¨¡å‹ä¿å­˜åœ¨æ‚¨é€‰æ‹©çš„æ–‡ä»¶å¤¹ä¸­`/path/to/converted/checkpoint/folder`ï¼Œè¯¥æ–‡ä»¶å¤¹åº”åŒ…å«ä¸€ä¸ª`pytorch_model.bin`æ–‡ä»¶å’Œä¸€ä¸ª`config.json`æ–‡ä»¶ï¼š
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**7\. Implement the forward pass**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**7. å®ç°æ­£å‘ä¼ é€’**'
- en: 'Having managed to correctly load the pretrained weights into the ğŸ¤— Transformers
    implementation, you should now make sure that the forward pass is correctly implemented.
    In [Get familiar with the original repository](#34-run-a-pretrained-checkpoint-using-the-original-repository),
    you have already created a script that runs a forward pass of the model using
    the original repository. Now you should write an analogous script using the ğŸ¤—
    Transformers implementation instead of the original one. It should look as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: æˆåŠŸå°†é¢„è®­ç»ƒæƒé‡æ­£ç¡®åŠ è½½åˆ°ğŸ¤— Transformerså®ç°ä¸­åï¼Œç°åœ¨åº”ç¡®ä¿æ­£å‘ä¼ é€’å·²æ­£ç¡®å®ç°ã€‚åœ¨[ç†Ÿæ‚‰åŸå§‹å­˜å‚¨åº“](#34-run-a-pretrained-checkpoint-using-the-original-repository)ä¸­ï¼Œæ‚¨å·²ç»åˆ›å»ºäº†ä¸€ä¸ªè„šæœ¬ï¼Œè¯¥è„šæœ¬ä½¿ç”¨åŸå§‹å­˜å‚¨åº“è¿è¡Œæ¨¡å‹çš„æ­£å‘ä¼ é€’ã€‚ç°åœ¨ï¼Œæ‚¨åº”è¯¥ç¼–å†™ä¸€ä¸ªç±»ä¼¼çš„è„šæœ¬ï¼Œä½¿ç”¨ğŸ¤—
    Transformerså®ç°è€Œä¸æ˜¯åŸå§‹å®ç°ã€‚å®ƒåº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It is very likely that the ğŸ¤— Transformers implementation and the original model
    implementation donâ€™t give the exact same output the very first time or that the
    forward pass throws an error. Donâ€™t be disappointed - itâ€™s expected! First, you
    should make sure that the forward pass doesnâ€™t throw any errors. It often happens
    that the wrong dimensions are used leading to a *Dimensionality mismatch* error
    or that the wrong data type object is used, *e.g.* `torch.long` instead of `torch.float32`.
    Donâ€™t hesitate to ask the Hugging Face team for help, if you donâ€™t manage to solve
    certain errors.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformers å®ç°å’ŒåŸå§‹æ¨¡å‹å®ç°å¾ˆå¯èƒ½ä¸ä¼šåœ¨ç¬¬ä¸€æ¬¡ç»™å‡ºå®Œå…¨ç›¸åŒçš„è¾“å‡ºï¼Œæˆ–è€…å‰å‘ä¼ é€’ä¼šå‡ºé”™ã€‚ä¸è¦å¤±æœ› - è¿™æ˜¯é¢„æœŸçš„ï¼é¦–å…ˆï¼Œæ‚¨åº”è¯¥ç¡®ä¿å‰å‘ä¼ é€’ä¸ä¼šå‡ºé”™ã€‚ç»å¸¸å‘ç”Ÿä½¿ç”¨äº†é”™è¯¯çš„ç»´åº¦å¯¼è‡´
    *ç»´åº¦ä¸åŒ¹é…* é”™è¯¯ï¼Œæˆ–è€…ä½¿ç”¨äº†é”™è¯¯çš„æ•°æ®ç±»å‹å¯¹è±¡ï¼Œä¾‹å¦‚ `torch.long` è€Œä¸æ˜¯ `torch.float32`ã€‚å¦‚æœæ‚¨æ— æ³•è§£å†³æŸäº›é”™è¯¯ï¼Œè¯·æ¯«ä¸çŠ¹è±«åœ°å‘
    Hugging Face å›¢é˜Ÿå¯»æ±‚å¸®åŠ©ã€‚
- en: 'The final part to make sure the ğŸ¤— Transformers implementation works correctly
    is to ensure that the outputs are equivalent to a precision of `1e-3`. First,
    you should ensure that the output shapes are identical, *i.e.* `outputs.shape`
    should yield the same value for the script of the ğŸ¤— Transformers implementation
    and the original implementation. Next, you should make sure that the output values
    are identical as well. This one of the most difficult parts of adding a new model.
    Common mistakes why the outputs are not identical are:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ ğŸ¤— Transformers å®ç°æ­£ç¡®å·¥ä½œçš„æœ€åä¸€éƒ¨åˆ†æ˜¯ç¡®ä¿è¾“å‡ºç²¾åº¦è¾¾åˆ° `1e-3`ã€‚é¦–å…ˆï¼Œæ‚¨åº”è¯¥ç¡®ä¿è¾“å‡ºå½¢çŠ¶ç›¸åŒï¼Œå³è„šæœ¬çš„ `outputs.shape`
    åº”è¯¥å¯¹ ğŸ¤— Transformers å®ç°å’ŒåŸå§‹å®ç°äº§ç”Ÿç›¸åŒçš„å€¼ã€‚æ¥ä¸‹æ¥ï¼Œæ‚¨åº”è¯¥ç¡®ä¿è¾“å‡ºå€¼ä¹Ÿç›¸åŒã€‚è¿™æ˜¯æ·»åŠ æ–°æ¨¡å‹ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚è¾“å‡ºä¸ç›¸åŒçš„å¸¸è§é”™è¯¯åŒ…æ‹¬ï¼š
- en: Some layers were not added, *i.e.* an *activation* layer was not added, or the
    residual connection was forgotten
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸäº›å±‚æœªæ·»åŠ ï¼Œå³æœªæ·»åŠ  *æ¿€æ´»* å±‚ï¼Œæˆ–è€…é—å¿˜äº†æ®‹å·®è¿æ¥ã€‚
- en: The word embedding matrix was not tied
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•è¯åµŒå…¥çŸ©é˜µæœªç»‘å®š
- en: The wrong positional embeddings are used because the original implementation
    uses on offset
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äº†é”™è¯¯çš„ä½ç½®åµŒå…¥ï¼Œå› ä¸ºåŸå§‹å®ç°ä½¿ç”¨äº†åç§»
- en: Dropout is applied during the forward pass. To fix this make sure *model.training
    is False* and that no dropout layer is falsely activated during the forward pass,
    *i.e.* pass *self.training* to [PyTorchâ€™s functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å‰å‘ä¼ é€’æœŸé—´åº”ç”¨äº†è¾å­¦ã€‚è¦ä¿®å¤æ­¤é—®é¢˜ï¼Œè¯·ç¡®ä¿ *model.training ä¸º False*ï¼Œå¹¶ä¸”åœ¨å‰å‘ä¼ é€’æœŸé—´ä¸ä¼šè¯¯æ¿€æ´»ä»»ä½•è¾å­¦å±‚ï¼Œå³å°† *self.training*
    ä¼ é€’ç»™ [PyTorch çš„åŠŸèƒ½æ€§è¾å­¦](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)
- en: The best way to fix the problem is usually to look at the forward pass of the
    original implementation and the ğŸ¤— Transformers implementation side-by-side and
    check if there are any differences. Ideally, you should debug/print out intermediate
    outputs of both implementations of the forward pass to find the exact position
    in the network where the ğŸ¤— Transformers implementation shows a different output
    than the original implementation. First, make sure that the hard-coded `input_ids`
    in both scripts are identical. Next, verify that the outputs of the first transformation
    of the `input_ids` (usually the word embeddings) are identical. And then work
    your way up to the very last layer of the network. At some point, you will notice
    a difference between the two implementations, which should point you to the bug
    in the ğŸ¤— Transformers implementation. From our experience, a simple and efficient
    way is to add many print statements in both the original implementation and ğŸ¤—
    Transformers implementation, at the same positions in the network respectively,
    and to successively remove print statements showing the same values for intermediate
    presentations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ä¿®å¤é—®é¢˜çš„æœ€ä½³æ–¹æ³•æ˜¯åŒæ—¶æŸ¥çœ‹åŸå§‹å®ç°å’Œ ğŸ¤— Transformers å®ç°çš„å‰å‘ä¼ é€’ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å·®å¼‚ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥è°ƒè¯•/æ‰“å°å‡ºä¸¤ä¸ªå®ç°çš„å‰å‘ä¼ é€’çš„ä¸­é—´è¾“å‡ºï¼Œä»¥æ‰¾åˆ°
    ğŸ¤— Transformers å®ç°æ˜¾ç¤ºä¸åŸå§‹å®ç°ä¸åŒè¾“å‡ºçš„ç½‘ç»œä¸­çš„ç¡®åˆ‡ä½ç½®ã€‚é¦–å…ˆï¼Œç¡®ä¿ä¸¤ä¸ªè„šæœ¬ä¸­ç¡¬ç¼–ç çš„ `input_ids` æ˜¯ç›¸åŒçš„ã€‚æ¥ä¸‹æ¥ï¼ŒéªŒè¯ `input_ids`
    çš„ç¬¬ä¸€ä¸ªè½¬æ¢çš„è¾“å‡ºï¼ˆé€šå¸¸æ˜¯å•è¯åµŒå…¥ï¼‰æ˜¯å¦ç›¸åŒã€‚ç„¶åé€å±‚å‘ç½‘ç»œçš„æœ€åä¸€å±‚å·¥ä½œã€‚åœ¨æŸä¸ªæ—¶å€™ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°ä¸¤ä¸ªå®ç°ä¹‹é—´çš„å·®å¼‚ï¼Œè¿™åº”è¯¥æŒ‡å‘ ğŸ¤— Transformers
    å®ç°ä¸­çš„é”™è¯¯ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ˜¯åœ¨åŸå§‹å®ç°å’Œ ğŸ¤— Transformers å®ç°ä¸­çš„ç›¸åŒä½ç½®åˆ†åˆ«æ·»åŠ è®¸å¤šæ‰“å°è¯­å¥ï¼Œå¹¶é€æ­¥åˆ é™¤æ˜¾ç¤ºä¸­é—´è¡¨ç¤ºå€¼ç›¸åŒçš„æ‰“å°è¯­å¥ã€‚
- en: When youâ€™re confident that both implementations yield the same output, verify
    the outputs with `torch.allclose(original_output, output, atol=1e-3)`, youâ€™re
    done with the most difficult part! Congratulations - the work left to be done
    should be a cakewalk ğŸ˜Š.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨ç¡®ä¿¡ä¸¤ä¸ªå®ç°äº§ç”Ÿç›¸åŒè¾“å‡ºæ—¶ï¼Œä½¿ç”¨ `torch.allclose(original_output, output, atol=1e-3)` éªŒè¯è¾“å‡ºï¼Œæ‚¨å·²ç»å®Œæˆäº†æœ€å›°éš¾çš„éƒ¨åˆ†ï¼æ­å–œ
    - å‰©ä¸‹çš„å·¥ä½œåº”è¯¥å¾ˆè½»æ¾ ğŸ˜Šã€‚
- en: '**8\. Adding all necessary model tests**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**8\. æ·»åŠ æ‰€æœ‰å¿…è¦çš„æ¨¡å‹æµ‹è¯•**'
- en: 'At this point, you have successfully added a new model. However, it is very
    much possible that the model does not yet fully comply with the required design.
    To make sure, the implementation is fully compatible with ğŸ¤— Transformers, all
    common tests should pass. The Cookiecutter should have automatically added a test
    file for your model, probably under the same `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`.
    Run this test file to verify that all common tests pass:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæ‚¨å·²æˆåŠŸæ·»åŠ äº†ä¸€ä¸ªæ–°æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œå¾ˆå¯èƒ½è¯¥æ¨¡å‹å°šæœªå®Œå…¨ç¬¦åˆæ‰€éœ€çš„è®¾è®¡ã€‚ä¸ºç¡®ä¿å®ç°ä¸ ğŸ¤— Transformers å®Œå…¨å…¼å®¹ï¼Œæ‰€æœ‰å¸¸è§æµ‹è¯•éƒ½åº”è¯¥é€šè¿‡ã€‚Cookiecutter
    åº”è¯¥å·²è‡ªåŠ¨ä¸ºæ‚¨çš„æ¨¡å‹æ·»åŠ äº†ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œå¯èƒ½ä½äºç›¸åŒçš„ `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`
    ä¸‹ã€‚è¿è¡Œæ­¤æµ‹è¯•æ–‡ä»¶ä»¥éªŒè¯æ‰€æœ‰å¸¸è§æµ‹è¯•æ˜¯å¦é€šè¿‡ï¼š
- en: '[PRE27]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Having fixed all common tests, it is now crucial to ensure that all the nice
    work you have done is well tested, so that
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¿®å¤æ‰€æœ‰å¸¸è§æµ‹è¯•åï¼Œç°åœ¨è‡³å…³é‡è¦çš„æ˜¯ç¡®ä¿æ‚¨æ‰€åšçš„æ‰€æœ‰å·¥ä½œéƒ½ç»è¿‡äº†å……åˆ†æµ‹è¯•ï¼Œä»¥ä¾¿
- en: a) The community can easily understand your work by looking at specific tests
    of *brand_new_bert*
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a) ç¤¾åŒºå¯ä»¥é€šè¿‡æŸ¥çœ‹ *brand_new_bert* çš„ç‰¹å®šæµ‹è¯•è½»æ¾ç†è§£æ‚¨çš„å·¥ä½œ
- en: b) Future changes to your model will not break any important feature of the
    model.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b) å¯¹æ¨¡å‹çš„æœªæ¥æ›´æ”¹ä¸ä¼šç ´åæ¨¡å‹çš„ä»»ä½•é‡è¦åŠŸèƒ½ã€‚
- en: At first, integration tests should be added. Those integration tests essentially
    do the same as the debugging scripts you used earlier to implement the model to
    ğŸ¤— Transformers. A template of those model tests has already added by the Cookiecutter,
    called `BrandNewBertModelIntegrationTests` and only has to be filled out by you.
    To ensure that those tests are passing, run
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåº”æ·»åŠ é›†æˆæµ‹è¯•ã€‚è¿™äº›é›†æˆæµ‹è¯•åŸºæœ¬ä¸Šä¸æ‚¨æ—©æœŸç”¨äºå°†æ¨¡å‹å®ç°åˆ°ğŸ¤— Transformersçš„è°ƒè¯•è„šæœ¬ç›¸åŒã€‚Cookiecutterå·²ç»æ·»åŠ äº†è¿™äº›æ¨¡å‹æµ‹è¯•çš„æ¨¡æ¿ï¼Œç§°ä¸º`BrandNewBertModelIntegrationTests`ï¼Œæ‚¨åªéœ€å¡«å†™å³å¯ã€‚ä¸ºç¡®ä¿è¿™äº›æµ‹è¯•é€šè¿‡ï¼Œè¯·è¿è¡Œ
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨Windowsï¼Œåº”å°†`RUN_SLOW=1`æ›¿æ¢ä¸º`SET RUN_SLOW=1`ã€‚
- en: 'Second, all features that are special to *brand_new_bert* should be tested
    additionally in a separate test under `BrandNewBertModelTester`/``BrandNewBertModelTest`.
    This part is often forgotten but is extremely useful in two ways:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œ*brand_new_bert*ç‰¹æœ‰çš„æ‰€æœ‰åŠŸèƒ½è¿˜åº”åœ¨`BrandNewBertModelTester`/`BrandNewBertModelTest`ä¸‹çš„å•ç‹¬æµ‹è¯•ä¸­è¿›è¡Œé¢å¤–æµ‹è¯•ã€‚è¿™éƒ¨åˆ†ç»å¸¸è¢«é—å¿˜ï¼Œä½†åœ¨ä¸¤ä¸ªæ–¹é¢éå¸¸æœ‰ç”¨ï¼š
- en: It helps to transfer the knowledge you have acquired during the model addition
    to the community by showing how the special features of *brand_new_bert* should
    work.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡å±•ç¤º*brand_new_bert*çš„ç‰¹æ®ŠåŠŸèƒ½åº”è¯¥å¦‚ä½•å·¥ä½œï¼Œå°†æ‚¨åœ¨æ¨¡å‹æ·»åŠ è¿‡ç¨‹ä¸­è·å¾—çš„çŸ¥è¯†ä¼ é€’ç»™ç¤¾åŒºæ˜¯æœ‰å¸®åŠ©çš„ã€‚
- en: Future contributors can quickly test changes to the model by running those special
    tests.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªæ¥çš„è´¡çŒ®è€…å¯ä»¥é€šè¿‡è¿è¡Œè¿™äº›ç‰¹æ®Šæµ‹è¯•å¿«é€Ÿæµ‹è¯•æ¨¡å‹çš„æ›´æ”¹ã€‚
- en: '**9\. Implement the tokenizer**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**9. å®ç°åˆ†è¯å™¨**'
- en: Next, we should add the tokenizer of *brand_new_bert*. Usually, the tokenizer
    is equivalent to or very similar to an already existing tokenizer of ğŸ¤— Transformers.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åº”è¯¥æ·»åŠ *brand_new_bert*çš„åˆ†è¯å™¨ã€‚é€šå¸¸ï¼Œåˆ†è¯å™¨ç­‰åŒäºæˆ–éå¸¸ç±»ä¼¼äºğŸ¤— Transformersçš„å·²æœ‰åˆ†è¯å™¨ã€‚
- en: It is very important to find/extract the original tokenizer file and to manage
    to load this file into the ğŸ¤— Transformersâ€™ implementation of the tokenizer.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°/æå–åŸå§‹çš„åˆ†è¯å™¨æ–‡ä»¶å¹¶æˆåŠŸåŠ è½½åˆ°ğŸ¤— Transformersçš„åˆ†è¯å™¨å®ç°ä¸­éå¸¸é‡è¦ã€‚
- en: 'To ensure that the tokenizer works correctly, it is recommended to first create
    a script in the original repository that inputs a string and returns the `input_idsâ€œ.
    It could look similar to this (in pseudo-code):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®ä¿åˆ†è¯å™¨æ­£å¸¸å·¥ä½œï¼Œå»ºè®®é¦–å…ˆåœ¨åŸå§‹å­˜å‚¨åº“ä¸­åˆ›å»ºä¸€ä¸ªè„šæœ¬ï¼Œè¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²å¹¶è¿”å›`input_ids`ã€‚å®ƒå¯èƒ½çœ‹èµ·æ¥ç±»ä¼¼äºè¿™æ ·ï¼ˆä¼ªä»£ç ï¼‰ï¼š
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You might have to take a deeper look again into the original repository to
    find the correct tokenizer function or you might even have to do changes to your
    clone of the original repository to only output the `input_ids`. Having written
    a functional tokenization script that uses the original repository, an analogous
    script for ğŸ¤— Transformers should be created. It should look similar to this:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½éœ€è¦å†æ¬¡æ·±å…¥ç ”ç©¶åŸå§‹å­˜å‚¨åº“ï¼Œæ‰¾åˆ°æ­£ç¡®çš„åˆ†è¯å™¨å‡½æ•°ï¼Œæˆ–è€…ç”šè‡³å¯èƒ½éœ€è¦å¯¹åŸå§‹å­˜å‚¨åº“çš„å…‹éš†è¿›è¡Œæ›´æ”¹ï¼Œä»¥ä»…è¾“å‡º`input_ids`ã€‚ç¼–å†™äº†ä¸€ä¸ªä½¿ç”¨åŸå§‹å­˜å‚¨åº“çš„åŠŸèƒ½æ€§åˆ†è¯è„šæœ¬åï¼Œåº”åˆ›å»ºä¸€ä¸ªç±»ä¼¼äºğŸ¤—
    Transformersçš„è„šæœ¬ã€‚å®ƒåº”è¯¥çœ‹èµ·æ¥ç±»ä¼¼äºè¿™æ ·ï¼š
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When both `input_ids` yield the same values, as a final step a tokenizer test
    file should also be added.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å½“`input_ids`äº§ç”Ÿç›¸åŒçš„å€¼æ—¶ï¼Œæœ€åä¸€æ­¥åº”è¯¥æ·»åŠ ä¸€ä¸ªåˆ†è¯å™¨æµ‹è¯•æ–‡ä»¶ã€‚
- en: Analogous to the modeling test files of *brand_new_bert*, the tokenization test
    files of *brand_new_bert* should contain a couple of hard-coded integration tests.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº*brand_new_bert*çš„å»ºæ¨¡æµ‹è¯•æ–‡ä»¶ï¼Œ*brand_new_bert*çš„åˆ†è¯æµ‹è¯•æ–‡ä»¶åº”åŒ…å«ä¸€äº›ç¡¬ç¼–ç çš„é›†æˆæµ‹è¯•ã€‚
- en: '**10\. Run End-to-end integration tests**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**10. è¿è¡Œç«¯åˆ°ç«¯é›†æˆæµ‹è¯•**'
- en: Having added the tokenizer, you should also add a couple of end-to-end integration
    tests using both the model and the tokenizer to `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`
    in ğŸ¤— Transformers. Such a test should show on a meaningful text-to-text sample
    that the ğŸ¤— Transformers implementation works as expected. A meaningful text-to-text
    sample can include *e.g.* a source-to-target-translation pair, an article-to-summary
    pair, a question-to-answer pair, etcâ€¦ If none of the ported checkpoints has been
    fine-tuned on a downstream task it is enough to simply rely on the model tests.
    In a final step to ensure that the model is fully functional, it is advised that
    you also run all tests on GPU. It can happen that you forgot to add some `.to(self.device)`
    statements to internal tensors of the model, which in such a test would show in
    an error. In case you have no access to a GPU, the Hugging Face team can take
    care of running those tests for you.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ äº†åˆ†è¯å™¨åï¼Œè¿˜åº”åœ¨ğŸ¤— Transformersçš„`tests/models/brand_new_bert/test_modeling_brand_new_bert.py`ä¸­æ·»åŠ ä¸€äº›ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•ï¼Œä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™æ ·çš„æµ‹è¯•åº”è¯¥åœ¨ä¸€ä¸ªæœ‰æ„ä¹‰çš„æ–‡æœ¬å¯¹æ–‡æœ¬ç¤ºä¾‹ä¸Šå±•ç¤ºğŸ¤—
    Transformersçš„å®ç°æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚æœ‰æ„ä¹‰çš„æ–‡æœ¬å¯¹æ–‡æœ¬ç¤ºä¾‹å¯ä»¥åŒ…æ‹¬*ä¾‹å¦‚*æºåˆ°ç›®æ ‡ç¿»è¯‘å¯¹ã€æ–‡ç« åˆ°æ‘˜è¦å¯¹ã€é—®é¢˜åˆ°ç­”æ¡ˆå¯¹ç­‰ã€‚å¦‚æœæ²¡æœ‰ä»»ä½•è¿ç§»æ£€æŸ¥ç‚¹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œä»…ä¾èµ–äºæ¨¡å‹æµ‹è¯•å°±è¶³å¤Ÿäº†ã€‚ä¸ºäº†ç¡®ä¿æ¨¡å‹å®Œå…¨åŠŸèƒ½æ­£å¸¸ï¼Œå»ºè®®æ‚¨è¿˜åœ¨GPUä¸Šè¿è¡Œæ‰€æœ‰æµ‹è¯•ã€‚æœ‰æ—¶æ‚¨å¯èƒ½ä¼šå¿˜è®°å‘æ¨¡å‹çš„å†…éƒ¨å¼ é‡æ·»åŠ ä¸€äº›`.to(self.device)`è¯­å¥ï¼Œåœ¨è¿™æ ·çš„æµ‹è¯•ä¸­ä¼šæ˜¾ç¤ºé”™è¯¯ã€‚å¦‚æœæ‚¨æ— æ³•è®¿é—®GPUï¼ŒHugging
    Faceå›¢é˜Ÿå¯ä»¥è´Ÿè´£ä¸ºæ‚¨è¿è¡Œè¿™äº›æµ‹è¯•ã€‚
- en: '**11\. Add Docstring**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**11. æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²**'
- en: Now, all the necessary functionality for *brand_new_bert* is added - youâ€™re
    almost done! The only thing left to add is a nice docstring and a doc page. The
    Cookiecutter should have added a template file called `docs/source/model_doc/brand_new_bert.md`
    that you should fill out. Users of your model will usually first look at this
    page before using your model. Hence, the documentation must be understandable
    and concise. It is very useful for the community to add some *Tips* to show how
    the model should be used. Donâ€™t hesitate to ping the Hugging Face team regarding
    the docstrings.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œ*brand_new_bert*æ‰€éœ€çš„æ‰€æœ‰åŠŸèƒ½éƒ½å·²æ·»åŠ  - æ‚¨å‡ ä¹å®Œæˆäº†ï¼å”¯ä¸€å‰©ä¸‹çš„æ˜¯æ·»åŠ ä¸€ä¸ªè‰¯å¥½çš„æ–‡æ¡£å­—ç¬¦ä¸²å’Œæ–‡æ¡£é¡µé¢ã€‚Cookiecutteråº”è¯¥å·²ç»æ·»åŠ äº†ä¸€ä¸ªåä¸º`docs/source/model_doc/brand_new_bert.md`çš„æ¨¡æ¿æ–‡ä»¶ï¼Œæ‚¨åº”è¯¥å¡«å†™è¯¥æ–‡ä»¶ã€‚æ‚¨çš„æ¨¡å‹ç”¨æˆ·é€šå¸¸ä¼šåœ¨ä½¿ç”¨æ‚¨çš„æ¨¡å‹ä¹‹å‰é¦–å…ˆæŸ¥çœ‹æ­¤é¡µé¢ã€‚å› æ­¤ï¼Œæ–‡æ¡£å¿…é¡»æ˜“äºç†è§£å’Œç®€æ´ã€‚å‘ç¤¾åŒºæ·»åŠ ä¸€äº›*æç¤º*ä»¥æ˜¾ç¤ºæ¨¡å‹åº”å¦‚ä½•ä½¿ç”¨æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚ä¸è¦çŠ¹è±«ä¸Hugging
    Faceå›¢é˜Ÿè”ç³»æœ‰å…³æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: Next, make sure that the docstring added to `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`
    is correct and included all necessary inputs and outputs. We have a detailed guide
    about writing documentation and our docstring format [here](writing-documentation).
    It is always to good to remind oneself that documentation should be treated at
    least as carefully as the code in ğŸ¤— Transformers since the documentation is usually
    the first contact point of the community with the model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œç¡®ä¿æ·»åŠ åˆ°`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py`çš„æ–‡æ¡£å­—ç¬¦ä¸²æ˜¯æ­£ç¡®çš„ï¼Œå¹¶åŒ…å«æ‰€æœ‰å¿…è¦çš„è¾“å…¥å’Œè¾“å‡ºã€‚æˆ‘ä»¬æœ‰å…³äºç¼–å†™æ–‡æ¡£å’Œæˆ‘ä»¬çš„æ–‡æ¡£å­—ç¬¦ä¸²æ ¼å¼çš„è¯¦ç»†æŒ‡å—[åœ¨è¿™é‡Œ](writing-documentation)ã€‚å€¼å¾—æé†’è‡ªå·±çš„æ˜¯ï¼Œæ–‡æ¡£åº”è¯¥è‡³å°‘åƒğŸ¤—
    Transformersä¸­çš„ä»£ç ä¸€æ ·å°å¿ƒå¯¹å¾…ï¼Œå› ä¸ºæ–‡æ¡£é€šå¸¸æ˜¯ç¤¾åŒºä¸æ¨¡å‹çš„ç¬¬ä¸€ä¸ªæ¥è§¦ç‚¹ã€‚
- en: '**Code refactor**'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç é‡æ„**'
- en: 'Great, now you have added all the necessary code for *brand_new_bert*. At this
    point, you should correct some potential incorrect code style by running:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨ä½ å·²ç»ä¸º*brand_new_bert*æ·»åŠ äº†æ‰€æœ‰å¿…è¦çš„ä»£ç ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œä½ åº”è¯¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç æ¥çº æ­£ä¸€äº›æ½œåœ¨çš„ä¸æ­£ç¡®çš„ä»£ç é£æ ¼ï¼š
- en: '[PRE31]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'and verify that your coding style passes the quality check:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶éªŒè¯ä½ çš„ç¼–ç é£æ ¼æ˜¯å¦é€šè¿‡äº†è´¨é‡æ£€æŸ¥ï¼š
- en: '[PRE32]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: There are a couple of other very strict design tests in ğŸ¤— Transformers that
    might still be failing, which shows up in the tests of your pull request. This
    is often because of some missing information in the docstring or some incorrect
    naming. The Hugging Face team will surely help you if youâ€™re stuck here.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Transformersä¸­è¿˜æœ‰ä¸€äº›å…¶ä»–éå¸¸ä¸¥æ ¼çš„è®¾è®¡æµ‹è¯•å¯èƒ½ä»ç„¶å¤±è´¥ï¼Œè¿™ä¼šåœ¨ä½ çš„æ‹‰å–è¯·æ±‚çš„æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæ¥ã€‚è¿™å¾€å¾€æ˜¯å› ä¸ºæ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç¼ºå°‘ä¸€äº›ä¿¡æ¯æˆ–ä¸€äº›åç§°ä¸æ­£ç¡®ã€‚å¦‚æœä½ é‡åˆ°å›°éš¾ï¼ŒHugging
    Faceå›¢é˜Ÿè‚¯å®šä¼šå¸®åŠ©ä½ ã€‚
- en: Lastly, it is always a good idea to refactor oneâ€™s code after having ensured
    that the code works correctly. With all tests passing, now itâ€™s a good time to
    go over the added code again and do some refactoring.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç»è¿‡ç¡®ä¿ä»£ç æ­£ç¡®è¿è¡Œåï¼Œé‡æ–°è®¾è®¡ä»£ç æ€»æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ç°åœ¨æ‰€æœ‰çš„æµ‹è¯•éƒ½é€šè¿‡äº†ï¼Œç°åœ¨æ˜¯ä¸€ä¸ªå¥½æ—¶æœºå†æ¬¡æ£€æŸ¥æ·»åŠ çš„ä»£ç å¹¶è¿›è¡Œä¸€äº›é‡æ„ã€‚
- en: You have now finished the coding part, congratulation! ğŸ‰ You are Awesome! ğŸ˜
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼ä½ ç°åœ¨å·²ç»å®Œæˆäº†ç¼–ç éƒ¨åˆ†ï¼Œå¤ªæ£’äº†ï¼ğŸ‰ ä½ çœŸæ£’ï¼ğŸ˜
- en: '**12\. Upload the models to the model hub**'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**12\. å°†æ¨¡å‹ä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒ**'
- en: 'In this final part, you should convert and upload all checkpoints to the model
    hub and add a model card for each uploaded model checkpoint. You can get familiar
    with the hub functionalities by reading our [Model sharing and uploading Page](model_sharing).
    You should work alongside the Hugging Face team here to decide on a fitting name
    for each checkpoint and to get the required access rights to be able to upload
    the model under the authorâ€™s organization of *brand_new_bert*. The `push_to_hub`
    method, present in all models in `transformers`, is a quick and efficient way
    to push your checkpoint to the hub. A little snippet is pasted below:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æœ€åä¸€éƒ¨åˆ†ï¼Œä½ åº”è¯¥å°†æ‰€æœ‰æ£€æŸ¥ç‚¹è½¬æ¢å¹¶ä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒï¼Œå¹¶ä¸ºæ¯ä¸ªä¸Šä¼ çš„æ¨¡å‹æ£€æŸ¥ç‚¹æ·»åŠ ä¸€ä¸ªæ¨¡å‹å¡ç‰‡ã€‚ä½ å¯ä»¥é€šè¿‡é˜…è¯»æˆ‘ä»¬çš„[æ¨¡å‹åˆ†äº«å’Œä¸Šä¼ é¡µé¢](model_sharing)æ¥ç†Ÿæ‚‰ä¸­å¿ƒçš„åŠŸèƒ½ã€‚åœ¨è¿™é‡Œï¼Œä½ åº”è¯¥ä¸Hugging
    Faceå›¢é˜Ÿä¸€èµ·å·¥ä½œï¼Œå†³å®šä¸ºæ¯ä¸ªæ£€æŸ¥ç‚¹é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„åç§°ï¼Œå¹¶è·å¾—æ‰€éœ€çš„è®¿é—®æƒé™ï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†æ¨¡å‹ä¸Šä¼ åˆ°ä½œè€…ç»„ç»‡*brand_new_bert*ä¸‹ã€‚`transformers`ä¸­çš„æ‰€æœ‰æ¨¡å‹ä¸­éƒ½æœ‰`push_to_hub`æ–¹æ³•ï¼Œè¿™æ˜¯å°†ä½ çš„æ£€æŸ¥ç‚¹å¿«é€Ÿæœ‰æ•ˆåœ°æ¨é€åˆ°ä¸­å¿ƒçš„æ–¹æ³•ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå°ç‰‡æ®µï¼š
- en: '[PRE33]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It is worth spending some time to create fitting model cards for each checkpoint.
    The model cards should highlight the specific characteristics of this particular
    checkpoint, *e.g.* On which dataset was the checkpoint pretrained/fine-tuned on?
    On what down-stream task should the model be used? And also include some code
    on how to correctly use the model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—èŠ±ä¸€äº›æ—¶é—´ä¸ºæ¯ä¸ªæ£€æŸ¥ç‚¹åˆ›å»ºåˆé€‚çš„æ¨¡å‹å¡ç‰‡ã€‚æ¨¡å‹å¡ç‰‡åº”è¯¥çªå‡ºæ˜¾ç¤ºè¿™ä¸ªç‰¹å®šæ£€æŸ¥ç‚¹çš„ç‰¹å®šç‰¹å¾ï¼Œ*ä¾‹å¦‚*è¿™ä¸ªæ£€æŸ¥ç‚¹æ˜¯åœ¨å“ªä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒ/å¾®è°ƒçš„ï¼Ÿè¿™ä¸ªæ¨¡å‹åº”è¯¥ç”¨äºå“ªä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Ÿè¿˜åº”è¯¥åŒ…æ‹¬ä¸€äº›å…³äºå¦‚ä½•æ­£ç¡®ä½¿ç”¨æ¨¡å‹çš„ä»£ç ã€‚
- en: '**13\. (Optional) Add notebook**'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**13\. ï¼ˆå¯é€‰ï¼‰æ·»åŠ ç¬”è®°æœ¬**'
- en: It is very helpful to add a notebook that showcases in-detail how *brand_new_bert*
    can be used for inference and/or fine-tuned on a downstream task. This is not
    mandatory to merge your PR, but very useful for the community.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸€ä¸ªå±•ç¤º*brand_new_bert*å¦‚ä½•ç”¨äºæ¨ç†å’Œ/æˆ–åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒçš„è¯¦ç»†ç¬”è®°æœ¬éå¸¸æœ‰å¸®åŠ©ã€‚è¿™ä¸æ˜¯åˆå¹¶ä½ çš„PRæ‰€å¿…éœ€çš„ï¼Œä½†å¯¹ç¤¾åŒºéå¸¸æœ‰ç”¨ã€‚
- en: '**14\. Submit your finished PR**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**14\. æäº¤ä½ å®Œæˆçš„PR**'
- en: Youâ€™re done programming now and can move to the last step, which is getting
    your PR merged into main. Usually, the Hugging Face team should have helped you
    already at this point, but it is worth taking some time to give your finished
    PR a nice description and eventually add comments to your code, if you want to
    point out certain design choices to your reviewer.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ç°åœ¨å·²ç»å®Œæˆäº†ç¼–ç¨‹å·¥ä½œï¼Œå¯ä»¥è¿›å…¥æœ€åä¸€æ­¥ï¼Œå³å°†ä½ çš„PRåˆå¹¶åˆ°ä¸»åˆ†æ”¯ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼ŒHugging Faceå›¢é˜Ÿåœ¨è¿™ä¸€ç‚¹ä¸Šåº”è¯¥å·²ç»å¸®åŠ©è¿‡ä½ äº†ï¼Œä½†å€¼å¾—èŠ±ä¸€äº›æ—¶é—´ä¸ºä½ çš„å®Œæˆçš„PRæ·»åŠ ä¸€ä¸ªå¥½çš„æè¿°ï¼Œå¹¶æœ€ç»ˆä¸ºä½ çš„ä»£ç æ·»åŠ æ³¨é‡Šï¼Œå¦‚æœä½ æƒ³æŒ‡å‡ºæŸäº›è®¾è®¡é€‰æ‹©ç»™ä½ çš„å®¡é˜…è€…ã€‚
- en: Share your work!!
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†äº«ä½ çš„å·¥ä½œï¼
- en: Now, itâ€™s time to get some credit from the community for your work! Having completed
    a model addition is a major contribution to Transformers and the whole NLP community.
    Your code and the ported pre-trained models will certainly be used by hundreds
    and possibly even thousands of developers and researchers. You should be proud
    of your work and share your achievements with the community.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ˜¯æ—¶å€™ä»ç¤¾åŒºä¸­è·å¾—ä¸€äº›å¯¹ä½ å·¥ä½œçš„è®¤å¯äº†ï¼å®Œæˆæ¨¡å‹æ·»åŠ æ˜¯å¯¹Transformerså’Œæ•´ä¸ªNLPç¤¾åŒºçš„é‡å¤§è´¡çŒ®ã€‚ä½ çš„ä»£ç å’Œç§»æ¤çš„é¢„è®­ç»ƒæ¨¡å‹è‚¯å®šä¼šè¢«æ•°ç™¾ç”šè‡³æ•°åƒåå¼€å‘äººå‘˜å’Œç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚ä½ åº”è¯¥ä¸ºè‡ªå·±çš„å·¥ä½œæ„Ÿåˆ°è‡ªè±ªï¼Œå¹¶ä¸ç¤¾åŒºåˆ†äº«ä½ çš„æˆå°±ã€‚
- en: '**You have made another model that is super easy to access for everyone in
    the community! ğŸ¤¯**'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½ åˆåˆ¶ä½œäº†ä¸€ä¸ªå¯¹ç¤¾åŒºä¸­æ¯ä¸ªäººéƒ½å¾ˆå®¹æ˜“è®¿é—®çš„æ¨¡å‹ï¼ğŸ¤¯**'
