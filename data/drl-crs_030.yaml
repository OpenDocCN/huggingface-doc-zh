- en: Two types of value-based methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种基于价值的方法
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods](https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods](https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In value-based methods, **we learn a value function** that **maps a state to
    the expected value of being at that state.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，**我们学习一个值函数**，它**将一个状态映射到在该状态时的预期值**。
- en: '![Value Based Methods](../Images/b4cdd11013b3a06c9b36019b40861897.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![基于价值的方法](../Images/b4cdd11013b3a06c9b36019b40861897.png)'
- en: The value of a state is the **expected discounted return** the agent can get
    if it **starts at that state and then acts according to our policy.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个状态的值是代理在该状态开始并根据我们的策略行动时可以获得的**预期折扣回报**。
- en: But what does it mean to act according to our policy? After all, we don't have
    a policy in value-based methods since we train a value function and not a policy.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是根据我们的策略采取行动是什么意思呢？毕竟，在基于价值的方法中，我们没有策略，因为我们训练的是值函数而不是策略。
- en: Remember that the goal of an **RL agent is to have an optimal policy π*.**
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，**强化学习代理的目标是拥有一个最优策略π*。**
- en: 'To find the optimal policy, we learned about two different methods:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最优策略，我们学习了两种不同的方法：
- en: '*Policy-based methods:* **Directly train the policy** to select what action
    to take given a state (or a probability distribution over actions at that state).
    In this case, we **don’t have a value function.**'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于策略的方法：**直接训练策略**来选择在给定状态下采取什么行动（或在该状态下采取行动的概率分布）。在这种情况下，我们**没有值函数。**'
- en: '![Two RL approaches](../Images/b0dc2fec6663d304a00631452105e70b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![两种强化学习方法](../Images/b0dc2fec6663d304a00631452105e70b.png)'
- en: 'The policy takes a state as input and outputs what action to take at that state
    (deterministic policy: a policy that output one action given a state, contrary
    to stochastic policy that output a probability distribution over actions).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 策略以状态作为输入，并输出在该状态下应采取的动作（确定性策略：在给定状态下输出一个动作的策略，与输出动作概率分布的随机策略相反）。
- en: And consequently, **we don’t define by hand the behavior of our policy; it’s
    the training that will define it.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**我们不手动定义策略的行为；训练将定义它。**
- en: '*Value-based methods:* **Indirectly, by training a value function** that outputs
    the value of a state or a state-action pair. Given this value function, our policy **will
    take an action.**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于价值的方法：**通过训练一个值函数间接地**输出一个状态或状态-动作对的值。有了这个值函数，我们的策略**将采取一个动作。**'
- en: Since the policy is not trained/learned, **we need to specify its behavior.** For
    instance, if we want a policy that, given the value function, will take actions
    that always lead to the biggest reward, **we’ll create a Greedy Policy.**
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于策略没有经过训练/学习，**我们需要指定其行为。**例如，如果我们想要一个策略，根据值函数，将采取总是导致最大奖励的行动，**我们将创建一个贪婪策略。**
- en: '![Two RL approaches](../Images/8be62737a01a28f07a8cddd7fd292be0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![两种强化学习方法](../Images/8be62737a01a28f07a8cddd7fd292be0.png)'
- en: Given a state, our action-value function (that we train) outputs the value of
    each action at that state. Then, our pre-defined Greedy Policy selects the action
    that will yield the highest value given a state or a state action pair.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个状态，我们训练的动作值函数输出该状态下每个动作的值。然后，我们预定义的贪婪策略选择将在给定状态或状态动作对下产生最高值的动作。
- en: 'Consequently, whatever method you use to solve your problem, **you will have
    a policy**. In the case of value-based methods, you don’t train the policy: your
    policy **is just a simple pre-specified function** (for instance, the Greedy Policy)
    that uses the values given by the value-function to select its actions.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论您使用什么方法来解决问题，**您都将有一个策略**。在基于价值的方法中，您不训练策略：您的策略**只是一个简单的预先指定函数**（例如，贪婪策略），它使用值函数给出的值来选择其动作。
- en: 'So the difference is:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，区别在于：
- en: In policy-based training, **the optimal policy (denoted π*) is found by training
    the policy directly.**
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于策略的训练中，**通过直接训练策略来找到最优策略（表示为π*）。**
- en: In value-based training, **finding an optimal value function (denoted Q* or
    V*, we’ll study the difference below) leads to having an optimal policy.**
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于价值的训练中，**找到一个最优值函数（表示为Q*或V*，我们将在下面研究区别）会导致有一个最优策略。**
- en: '![Link between value and policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![价值和策略之间的联系](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
- en: In fact, most of the time, in value-based methods, you’ll use **an Epsilon-Greedy
    Policy** that handles the exploration/exploitation trade-off; we’ll talk about
    this when we talk about Q-Learning in the second part of this unit.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在基于价值的方法中，大多数时间，您将使用**一个ε-贪婪策略**来处理探索/利用的权衡；我们将在本单元的第二部分讨论Q学习时详细讨论这一点。
- en: 'As we mentioned above, we have two types of value-based functions:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们上面提到的，我们有两种基于价值的函数：
- en: The state-value function
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态值函数
- en: 'We write the state value function under a policy π like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在策略π下编写状态值函数如下：
- en: '![State value function](../Images/c64cf990ee9037cd2363ddd5d4ee5887.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![状态值函数](../Images/c64cf990ee9037cd2363ddd5d4ee5887.png)'
- en: For each state, the state-value function outputs the expected return if the
    agent **starts at that state** and then follows the policy forever afterward (for
    all future timesteps, if you prefer).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态，状态值函数输出代理**从该状态开始**并在之后永远遵循策略时的预期回报（如果您愿意，对于所有未来时间步）。
- en: '![State value function](../Images/5035b02fa7e331c3ef7ca3e014d3458c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![状态值函数](../Images/5035b02fa7e331c3ef7ca3e014d3458c.png)'
- en: 'If we take the state with value -7: it''s the expected return starting at that
    state and taking actions according to our policy (greedy policy), so right, right,
    right, down, down, right, right.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑值为-7的状态：这是从该状态开始并根据我们的策略（贪婪策略）采取行动的预期回报，所以向右、向右、向右、向下、向下、向右、向右。
- en: The action-value function
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作值函数
- en: In the action-value function, for each state and action pair, the action-value
    function **outputs the expected return** if the agent starts in that state, takes
    that action, and then follows the policy forever after.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在动作值函数中，对于每个状态和动作对，动作值函数**输出了期望回报**，如果代理在该状态开始，采取该动作，然后按照策略永远继续。
- en: 'The value of taking action<math><semantics><mrow><mi>a</mi></mrow><annotation
    encoding="application/x-tex">a</annotation></semantics></math>a in state<math><semantics><mrow><mi>s</mi></mrow><annotation
    encoding="application/x-tex">s</annotation></semantics></math>s under a policy<math><semantics><mrow><mi>π</mi></mrow><annotation
    encoding="application/x-tex">π</annotation></semantics></math>π is:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略<math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">π</annotation></semantics></math>π下，采取动作<math><semantics><mrow><mi>a</mi></mrow><annotation
    encoding="application/x-tex">a</annotation></semantics></math>a在状态<math><semantics><mrow><mi>s</mi></mrow><annotation
    encoding="application/x-tex">s</annotation></semantics></math>s的价值是：
- en: '![Action State value function](../Images/bbd5a8264c0e47c0570c548e02e0804b.png)
    ![Action State value function](../Images/4b94e59b0a0a042bce0bc5932b95b154.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![动作状态值函数](../Images/bbd5a8264c0e47c0570c548e02e0804b.png) ![动作状态值函数](../Images/4b94e59b0a0a042bce0bc5932b95b154.png)'
- en: 'We see that the difference is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到差异是：
- en: For the state-value function, we calculate **the value of a state<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">S_t</annotation></semantics></math>St​**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于状态值函数，我们计算**状态<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">S_t</annotation></semantics></math>的价值St**
- en: For the action-value function, we calculate **the value of the state-action
    pair (<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">S_t, A_t</annotation></semantics></math>St​,At​ )
    hence the value of taking that action at that state.**
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于动作值函数，我们计算**状态-动作对（<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">S_t,
    A_t</annotation></semantics></math>）的价值，因此是在该状态采取该动作的价值。**
- en: '![Two types of value function](../Images/6d23342c957a8b891dfe93782cee22a4.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![两种价值函数](../Images/6d23342c957a8b891dfe93782cee22a4.png)'
- en: 'Note: We didn''t fill all the state-action pairs for the example of Action-value
    function'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们没有填充动作值函数示例中的所有状态-动作对
- en: In either case, whichever value function we choose (state-value or action-value
    function), **the returned value is the expected return.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们选择哪种价值函数（状态值函数或动作值函数），**返回的值都是期望回报。**
- en: However, the problem is that **to calculate EACH value of a state or a state-action
    pair, we need to sum all the rewards an agent can get if it starts at that state.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，问题在于**计算每个状态或状态-动作对的价值，我们需要总结代理在该状态开始时可以获得的所有奖励。**
- en: This can be a computationally expensive process, and that’s **where the Bellman
    equation comes in to help us.**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个计算昂贵的过程，这就是**贝尔曼方程帮助我们的地方。**
