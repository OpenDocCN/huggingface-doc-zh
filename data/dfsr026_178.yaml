- en: Text2Video-Zero
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Text2Video-Zero
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://huggingface.co/papers/2303.13439)
    is by Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel,
    [Zhangyang Wang](https://www.ece.utexas.edu/people/faculty/atlas-wang), Shant
    Navasardyan, [Humphrey Shi](https://www.humphreyshi.com).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Text2Video-Zero：文本到图像扩散模型是零样本视频生成器](https://huggingface.co/papers/2303.13439)
    作者是Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel,
    [Zhangyang Wang](https://www.ece.utexas.edu/people/faculty/atlas-wang), Shant
    Navasardyan, [Humphrey Shi](https://www.humphreyshi.com)。'
- en: 'Text2Video-Zero enables zero-shot video generation using either:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Text2Video-Zero使零样本视频生成成为可能：
- en: A textual prompt
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个文本提示
- en: A prompt combined with guidance from poses or edges
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个结合了姿势或边缘指导的提示
- en: Video Instruct-Pix2Pix (instruction-guided video editing)
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视频指导-Pix2Pix（指导视频编辑）
- en: Results are temporally consistent and closely follow the guidance and textual
    prompts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在时间上保持一致，并且紧密遵循指导和文本提示。
- en: '![teaser-img](../Images/f8f3b9ddb58b874876214c2f2682292d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![teaser-img](../Images/f8f3b9ddb58b874876214c2f2682292d.png)'
- en: 'The abstract from the paper is:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Recent text-to-video generation approaches rely on computationally heavy training
    and require large-scale video datasets. In this paper, we introduce a new task
    of zero-shot text-to-video generation and propose a low-cost approach (without
    any training or optimization) by leveraging the power of existing text-to-image
    synthesis methods (e.g., Stable Diffusion), making them suitable for the video
    domain. Our key modifications include (i) enriching the latent codes of the generated
    frames with motion dynamics to keep the global scene and the background time consistent;
    and (ii) reprogramming frame-level self-attention using a new cross-frame attention
    of each frame on the first frame, to preserve the context, appearance, and identity
    of the foreground object. Experiments show that this leads to low overhead, yet
    high-quality and remarkably consistent video generation. Moreover, our approach
    is not limited to text-to-video synthesis but is also applicable to other tasks
    such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix,
    i.e., instruction-guided video editing. As experiments show, our method performs
    comparably or sometimes better than recent approaches, despite not being trained
    on additional video data.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近的文本到视频生成方法依赖于计算密集型的训练，并需要大规模的视频数据集。在本文中，我们介绍了一种新的零样本文本到视频生成任务，并提出了一种低成本方法（无需任何训练或优化），通过利用现有文本到图像合成方法的能力（例如，稳定扩散），使其适用于视频领域。我们的关键修改包括（i）丰富生成帧的潜在代码，以保持全局场景和背景时间一致；和（ii）重新编程帧级自注意力，使用每帧对第一帧的新交叉帧注意力，以保留前景对象的上下文、外观和身份。实验表明，这导致低开销，但高质量且非常一致的视频生成。此外，我们的方法不仅限于文本到视频合成，还适用于其他任务，如有条件的和内容专门化的视频生成，以及视频指导-Pix2Pix，即指导视频编辑。正如实验所示，我们的方法在不经过额外视频数据训练的情况下表现出与最近方法相当甚至有时更好的性能。*'
- en: You can find additional information about Text2Video-Zero on the [project page](https://text2video-zero.github.io/),
    [paper](https://arxiv.org/abs/2303.13439), and [original codebase](https://github.com/Picsart-AI-Research/Text2Video-Zero).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[项目页面](https://text2video-zero.github.io/)、[论文](https://arxiv.org/abs/2303.13439)和[原始代码库](https://github.com/Picsart-AI-Research/Text2Video-Zero)上找到有关Text2Video-Zero的其他信息。
- en: Usage example
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法示例
- en: Text-To-Video
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本到视频
- en: 'To generate a video from prompt, run the following Python code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要从提示生成视频，请运行以下Python代码：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can change these parameters in the pipeline call:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在管道调用中更改这些参数：
- en: 'Motion field strength (see the [paper](https://arxiv.org/abs/2303.13439), Sect.
    3.3.1):'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运动场强度（参见[论文](https://arxiv.org/abs/2303.13439)，第3.3.1节）：
- en: '`motion_field_strength_x` and `motion_field_strength_y`. Default: `motion_field_strength_x=12`,
    `motion_field_strength_y=12`'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_field_strength_x`和`motion_field_strength_y`。默认值：`motion_field_strength_x=12`，`motion_field_strength_y=12`'
- en: '`T` and `T''` (see the [paper](https://arxiv.org/abs/2303.13439), Sect. 3.3.1)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T`和`T''`（参见[论文](https://arxiv.org/abs/2303.13439)，第3.3.1节）'
- en: '`t0` and `t1` in the range `{0, ..., num_inference_steps}`. Default: `t0=45`,
    `t1=48`'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t0`和`t1`在范围`{0, ..., num_inference_steps}`内。默认值：`t0=45`，`t1=48`'
- en: 'Video length:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频长度：
- en: '`video_length`, the number of frames video_length to be generated. Default:
    `video_length=8`'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`video_length`，要生成的帧数video_length。默认值：`video_length=8`'
- en: 'We can also generate longer videos by doing the processing in a chunk-by-chunk
    manner:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过分块处理来生成更长的视频：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: SDXL Support
  id: totrans-26
  prefs:
  - PREF_UL
  - PREF_H4
  type: TYPE_NORMAL
  zh: SDXL支持
- en: 'In order to use the SDXL model when generating a video from prompt, use the
    `TextToVideoZeroSDXLPipeline` pipeline:'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了在从提示生成视频时使用SDXL模型，请使用`TextToVideoZeroSDXLPipeline`管道：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Text-To-Video with Pose Control
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带姿势控制的文本到视频
- en: To generate a video from prompt with additional pose control
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要从提示生成带有额外姿势控制的视频
- en: Download a demo video
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载演示视频
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Read video containing extracted pose images
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取包含提取的姿势图像的视频
- en: '[PRE4]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To extract pose from actual video, read [ControlNet documentation](controlnet).
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要从实际视频中提取姿势，请阅读[ControlNet文档](controlnet)。
- en: Run `StableDiffusionControlNetPipeline` with our custom attention processor
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`StableDiffusionControlNetPipeline`与我们的自定义注意力处理器
- en: '[PRE5]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: SDXL Support
  id: totrans-38
  prefs:
  - PREF_UL
  - PREF_H4
  type: TYPE_NORMAL
  zh: SDXL支持
- en: 'Since our attention processor also works with SDXL, it can be utilized to generate
    a video from prompt using ControlNet models powered by SDXL:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们的注意力处理器也适用于SDXL，它可以用于使用SDXL提供动力的ControlNet模型生成视频：
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Text-To-Video with Edge Control
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带边缘控制的文本到视频
- en: To generate a video from prompt with additional Canny edge control, follow the
    same steps described above for pose-guided generation using [Canny edge ControlNet
    model](https://huggingface.co/lllyasviel/sd-controlnet-canny).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要从提示生成带有额外Canny边缘控制的视频，请按照上述描述的相同步骤进行，使用[Canny边缘ControlNet模型](https://huggingface.co/lllyasviel/sd-controlnet-canny)进行姿势引导生成。
- en: Video Instruct-Pix2Pix
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视频指导-Pix2Pix
- en: 'To perform text-guided video editing (with [InstructPix2Pix](pix2pix)):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 执行文本引导的视频编辑（使用[InstructPix2Pix](pix2pix)）：
- en: Download a demo video
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载演示视频
- en: '[PRE7]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Read video from path
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从路径读取视频
- en: '[PRE8]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Run `StableDiffusionInstructPix2PixPipeline` with our custom attention processor
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的自定义注意力处理器运行`StableDiffusionInstructPix2PixPipeline`
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: DreamBooth specialization
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DreamBooth专业化
- en: 'Methods **Text-To-Video**, **Text-To-Video with Pose Control** and **Text-To-Video
    with Edge Control** can run with custom [DreamBooth](../../training/dreambooth)
    models, as shown below for [Canny edge ControlNet model](https://huggingface.co/lllyasviel/sd-controlnet-canny)
    and [Avatar style DreamBooth](https://huggingface.co/PAIR/text2video-zero-controlnet-canny-avatar)
    model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 方法**文本到视频**、**文本到视频带姿势控制**和**文本到视频带边缘控制**可以使用自定义[DreamBooth](../../training/dreambooth)模型运行，如下所示为[Canny边缘ControlNet模型](https://huggingface.co/lllyasviel/sd-controlnet-canny)和[Avatar风格DreamBooth](https://huggingface.co/PAIR/text2video-zero-controlnet-canny-avatar)模型：
- en: Download a demo video
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载演示视频
- en: '[PRE10]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Read video from path
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从路径读取视频
- en: '[PRE11]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Run `StableDiffusionControlNetPipeline` with custom trained DreamBooth model
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自定义训练的DreamBooth模型运行`StableDiffusionControlNetPipeline`
- en: '[PRE12]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can filter out some available DreamBooth-trained models with [this link](https://huggingface.co/models?search=dreambooth).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用[此链接](https://huggingface.co/models?search=dreambooth)筛选出一些可用的DreamBooth训练模型。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨流程重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个流程中。
- en: TextToVideoZeroPipeline
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TextToVideoZeroPipeline
- en: '### `class diffusers.TextToVideoZeroPipeline`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.TextToVideoZeroPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L284)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L284)'
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）-
    变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`CLIPTextModel`）- 冻结文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`CLIPTokenizer`）- 用于对文本进行标记化的[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A [UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)
    to denoise the encoded video latents.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）-
    用于去噪编码视频潜变量的[UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）-
    用于与`unet`结合使用以去噪编码图像潜变量的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: '`safety_checker` (`StableDiffusionSafetyChecker`) — Classification module that
    estimates whether generated images could be considered offensive or harmful. Please
    refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for more details about a model’s potential harms.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safety_checker`（`StableDiffusionSafetyChecker`）- 用于估计生成图像是否可能被视为具有冒犯性或有害的分类模块。请参考[model
    card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。'
- en: '`feature_extractor` (`CLIPImageProcessor`) — A `CLIPImageProcessor` to extract
    features from generated images; used as inputs to the `safety_checker`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`（`CLIPImageProcessor`）- 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。'
- en: Pipeline for zero-shot text-to-video generation using Stable Diffusion.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Stable Diffusion进行零样本文本到视频生成的流程。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解所有流程（下载、保存、在特定设备上运行等）实现的通用方法。
- en: '#### `__call__`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L521)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L521)'
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）- 用于引导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`video_length` (`int`, *optional*, defaults to 8) — The number of generated
    video frames.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`video_length`（`int`，*可选*，默认为8）- 生成视频帧的数量。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated image.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推断速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的指导比例值鼓励模型生成与文本
    `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in video generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 指导视频生成中不包含的提示。如果未定义，则需要传递
    `negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。'
- en: '`num_videos_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    videos to generate per prompt.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_videos_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的视频数量。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *optional*, defaults to 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502)
    论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for video generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜变量，用作视频生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`output_type` (`str`, *optional*, defaults to `"numpy"`) — The output format
    of the generated video. Choose between `"latent"` and `"numpy"`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, defaults to `"numpy"`) — 生成视频的输出格式。选择 `"latent"`
    和 `"numpy"` 之间。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [TextToVideoPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video_zero#diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput)
    instead of a plain tuple.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回 [TextToVideoPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video_zero#diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput)
    而不是普通元组。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) — 在推断过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, defaults to 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步调用回调。'
- en: '`motion_field_strength_x` (`float`, *optional*, defaults to 12) — Strength
    of motion in generated video along x-axis. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_field_strength_x` (`float`, *optional*, defaults to 12) — 生成视频沿 x 轴的运动强度。参见
    [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。'
- en: '`motion_field_strength_y` (`float`, *optional*, defaults to 12) — Strength
    of motion in generated video along y-axis. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_field_strength_y` (`float`, *optional*, defaults to 12) — 生成视频沿 y 轴的运动强度。参见
    [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。'
- en: '`t0` (`int`, *optional*, defaults to 44) — Timestep t0\. Should be in the range
    [0, num_inference_steps - 1]. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t0` (`int`, *optional*, defaults to 44) — 时间步 t0。应在范围 [0, num_inference_steps
    - 1] 内。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。'
- en: '`t1` (`int`, *optional*, defaults to 47) — Timestep t0\. Should be in the range
    [t0 + 1, num_inference_steps - 1]. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t1` (`int`, *optional*, defaults to 47) — 时间步 t0。应在范围 [t0 + 1, num_inference_steps
    - 1] 内。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。'
- en: '`frame_ids` (`List[int]`, *optional*) — Indexes of the frames that are being
    generated. This is used when generating longer videos chunk-by-chunk.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frame_ids` (`List[int]`, *optional*) — 正在生成的帧的索引。在逐块生成较长视频时使用。'
- en: Returns
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[TextToVideoPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video_zero#diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextToVideoPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video_zero#diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput)'
- en: The output contains a `ndarray` of the generated video, when `output_type` !=
    `"latent"`, otherwise a latent code of generated videos and a list of `bool`s
    indicating whether the corresponding generated video contains “not-safe-for-work”
    (nsfw) content..
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当`output_type` != `"latent"`时，输出包含生成视频的`ndarray`，否则包含生成视频的潜在代码和一个`bool`列表，指示相应生成的视频是否包含“不安全内容”（nsfw）。
- en: The call function to the pipeline for generation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: '#### `backward_loop`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `backward_loop`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L375)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L375)'
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`（`Callable`，*可选*）—在推断过程中每`callback_steps`步调用一次的函数。该函数使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step. extra_step_kwargs — Extra_step_kwargs. cross_attention_kwargs — A
    kwargs dictionary that if specified is passed along to the `AttentionProcessor`
    as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
    num_warmup_steps — number of warmup steps.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps`（`int`，*可选*，默认为1）—调用`callback`函数的频率。如果未指定，将在每一步调用回调。extra_step_kwargs
    — 额外的步骤参数。cross_attention_kwargs — 如果指定，将传递给[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`的kwargs字典。num_warmup_steps
    — 预热步数。'
- en: Returns
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: latents
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在代码
- en: Latents of backward process output at time timesteps[-1].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步长[-1]处的反向过程输出的潜在代码。
- en: Perform backward process given list of time steps.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 给定时间步长列表的反向过程。
- en: '#### `encode_prompt`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L782)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L782)'
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）—要编码的提示设备 —（`torch.device`）：torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`（`int`）—应该为每个提示生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance`（`bool`）—是否使用分类器自由指导。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`（`str`或`List[str]`，*可选*）—不用来指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale`（`float`，*可选*）—将应用于文本编码器的所有LoRA层的LoRA比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip`（`int`，*可选*）—在计算提示嵌入时要跳过的CLIP层数。值为1意味着将使用预最终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: '#### `forward_loop`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward_loop`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L351)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L351)'
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）—用于使生成具有确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: Returns
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: x_t1
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: x_t1
- en: Forward process applied to x_t0 from time t0 to t1.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间t0到t1应用于x_t0的前向过程。
- en: Perform DDPM forward process from time t0 to t1\. This is the same as adding
    noise with corresponding variance.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 执行从时间t0到t1的DDPM前向过程。这与添加具有相应方差的噪声相同。
- en: TextToVideoZeroSDXLPipeline
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到视频零SDXL管道
- en: '### `class diffusers.TextToVideoZeroSDXLPipeline`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.TextToVideoZeroSDXLPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L328)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L328)'
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）—变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder. Stable Diffusion XL
    uses the text portion of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel),
    specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
    variant.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`CLIPTextModel`）- 冻结的文本编码器。 Stable Diffusion XL 使用 [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)
    的文本部分，具体来说是 [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
    变体。'
- en: '`text_encoder_2` ( `CLIPTextModelWithProjection`) — Second frozen text-encoder.
    Stable Diffusion XL uses the text and pool portion of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
    specifically the [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
    variant.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_2`（`CLIPTextModelWithProjection`）- 第二个冻结的文本编码器。 Stable Diffusion
    XL 使用 [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection)
    的文本和池部分，具体来说是 [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
    变体。'
- en: '`tokenizer` (`CLIPTokenizer`) — Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`CLIPTokenizer`）- 类 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)
    的分词器。'
- en: '`tokenizer_2` (`CLIPTokenizer`) — Second Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_2`（`CLIPTokenizer`）- 第二个分词器，类为 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — Conditional U-Net architecture to denoise the encoded image latents.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）-
    有条件的 U-Net 架构，用于去噪编码图像潜变量。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）-
    用于与 `unet` 结合使用的调度器，以去噪编码图像潜变量。可以是 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)
    或 [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)
    中的一个。'
- en: Pipeline for zero-shot text-to-video generation using Stable Diffusion XL.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Stable Diffusion XL 进行零样本文本到视频生成的流程。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看通用方法的超类文档，这些方法适用于所有流程（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L938)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L938)'
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    the image generation. If not defined, one has to pass `prompt_embeds`. instead.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str` 或 `List[str]`，*可选*）- 用于指导图像生成的提示。如果未定义，则必须传递 `prompt_embeds`。'
- en: '`prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts to be
    sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is used
    in both text-encoders'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_2`（`str` 或 `List[str]`，*可选*）- 要发送到 `tokenizer_2` 和 `text_encoder_2`
    的提示。如果未定义，则在两个文本编码器中使用 `prompt`。'
- en: '`video_length` (`int`, *optional*, defaults to 8) — The number of generated
    video frames.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`video_length`（`int`，*可选*，默认为 8）- 生成的视频帧数。'
- en: '`height` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — The height in pixels of the generated image.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height`（`int`，*可选*，默认为 self.unet.config.sample_size * self.vae_scale_factor）-
    生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — The width in pixels of the generated image.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width`（`int`，*可选*，默认为 self.unet.config.sample_size * self.vae_scale_factor）-
    生成图像的像素宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`（`int`，*可选*，默认为 50）- 去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`denoising_end` (`float`, *optional*) — When specified, determines the fraction
    (between 0.0 and 1.0) of the total denoising process to be completed before it
    is intentionally prematurely terminated. As a result, the returned sample will
    still retain a substantial amount of noise as determined by the discrete timesteps
    selected by the scheduler. The denoising_end parameter should ideally be utilized
    when this pipeline forms a part of a “Mixture of Denoisers” multi-pipeline setup,
    as elaborated in [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`denoising_end`（`float`，*可选*）- 当指定时，确定在故意提前终止之前完成的总去噪过程的比例（介于 0.0 和 1.0 之间）。因此，返回的样本仍将保留由调度器选择的离散时间步确定的大量噪声。当此流程作为“去噪混合”多流程设置的一部分时，应理想地利用
    `denoising_end` 参数，如 [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)
    中所述。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, 默认为 7.5) — 在 [Classifier-Free Diffusion
    Guidance](https://arxiv.org/abs/2207.12598) 中定义的指导比例。`guidance_scale` 被定义为 [Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale
    > 1` 来启用指导比例。更高的指导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *optional*) — 不指导图像生成的提示或提示。如果未定义，则必须传递
    `negative_prompt_embeds`。如果不使用指导（即，如果 `guidance_scale` 小于 `1`，则将被忽略）。'
- en: '`negative_prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_2` (`str` 或 `List[str]`, *optional*) — 不指导图像生成的提示或提示，将发送到
    `tokenizer_2` 和 `text_encoder_2`。如果未定义，则在两个文本编码器中使用 `negative_prompt`。'
- en: '`num_videos_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    videos to generate per prompt.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_videos_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的视频数量。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) in the DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    will be ignored for others.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *optional*, 默认为 0.0) — 对应于 DDIM 论文中的参数 eta (η)：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于
    [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，对其他情况将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 一个或多个
    [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`frame_ids` (`List[int]`, *optional*) — Indexes of the frames that are being
    generated. This is used when generating longer videos chunk-by-chunk.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frame_ids` (`List[int]`, *optional*) — 正在生成的帧的索引。在逐块生成较长视频时使用。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如
    prompt 加权。如果未提供，将从 `prompt` 输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面文本嵌入。可用于轻松调整文本输入，例如
    prompt 加权。如果未提供，将从 `negative_prompt` 输入参数生成负面文本嵌入。'
- en: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated pooled
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, pooled text embeddings will be generated from `prompt` input
    argument.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如
    prompt 加权。如果未提供，将从 `prompt` 输入参数生成池化文本嵌入。'
- en: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面池化文本嵌入。可用于轻松调整文本输入，例如
    prompt 加权。如果未提供，将从 `negative_prompt` 输入参数生成负面池化文本嵌入。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 预生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`motion_field_strength_x` (`float`, *optional*, defaults to 12) — Strength
    of motion in generated video along x-axis. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_field_strength_x` (`float`, *optional*, 默认为 12) — 生成视频中沿 x 轴的运动强度。参见
    [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。'
- en: '`motion_field_strength_y` (`float`, *optional*, defaults to 12) — Strength
    of motion in generated video along y-axis. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_field_strength_y` (`float`, *optional*, 默认为 12) — 生成视频中沿 y 轴的运动强度。参见
    [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。选择在 [PIL](https://pillow.readthedocs.io/en/stable/)
    中的 `PIL.Image.Image` 或 `np.array` 之间。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` instead
    of a plain tuple.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 `~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`
    而不是普通元组。'
- en: '`callback` (`Callable`, *optional*) — A function that will be called every
    `callback_steps` steps during inference. The function will be called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) — 在推断过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function will be called. If not specified, the callback will be
    called at every step.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, 默认为 1) — `callback` 函数将被调用的频率。如果未指定，将在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined under `self.processor`
    in [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 `AttentionProcessor`
    的 kwargs 字典，如在 [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py)
    中定义。'
- en: '`guidance_rescale` (`float`, *optional*, defaults to 0.7) — Guidance rescale
    factor proposed by [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)
    `guidance_scale` is defined as `φ` in equation 16\. of [Common Diffusion Noise
    Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
    Guidance rescale factor should fix overexposure when using zero terminal SNR.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_rescale` (`float`, *optional*, 默认为 0.7) — [Common Diffusion Noise
    Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 提出的指导缩放因子
    `guidance_scale` 在方程式 16 中定义为 `φ`。当使用零终端 SNR 时，指导缩放因子应该修复过度曝光。'
- en: '`original_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — If `original_size`
    is not the same as `target_size` the image will appear to be down- or upsampled.
    `original_size` defaults to `(width, height)` if not specified. Part of SDXL’s
    micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 如果 `original_size`
    与 `target_size` 不同，图像将呈现为缩小或放大。如果未指定，`original_size` 默认为 `(width, height)`。如在
    [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    的第2.2节中所述，这是 SDXL 的微调节的一部分。'
- en: '`crops_coords_top_left` (`Tuple[int]`, *optional*, defaults to (0, 0)) — `crops_coords_top_left`
    can be used to generate an image that appears to be “cropped” from the position
    `crops_coords_top_left` downwards. Favorable, well-centered images are usually
    achieved by setting `crops_coords_top_left` to (0, 0). Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_coords_top_left` (`Tuple[int]`, *optional*, 默认为 (0, 0)) — `crops_coords_top_left`
    可用于生成一个看起来被从位置 `crops_coords_top_left` 向下“裁剪”的图像。通常通过将 `crops_coords_top_left`
    设置为 (0, 0) 来实现有利的、居中的图像。如在 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    的第2.2节中所述，这是 SDXL 的微调节的一部分。'
- en: '`target_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — For most
    cases, `target_size` should be set to the desired height and width of the generated
    image. If not specified it will default to `(width, height)`. Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 对于大多数情况，`target_size`
    应设置为生成图像的期望高度和宽度。如果未指定，将默认为 `(width, height)`。如在 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    的第2.2节中所述，这是 SDXL 的微调节的一部分。'
- en: '`t0` (`int`, *optional*, defaults to 44) — Timestep t0\. Should be in the range
    [0, num_inference_steps - 1]. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t0` (`int`, *optional*, 默认为 44) — 时间步 t0。应在范围 [0, num_inference_steps - 1]
    内。参见 [paper](https://arxiv.org/abs/2303.13439)，第3.3.1节。'
- en: '`t1` (`int`, *optional*, defaults to 47) — Timestep t0\. Should be in the range
    [t0 + 1, num_inference_steps - 1]. See the [paper](https://arxiv.org/abs/2303.13439),
    Sect. 3.3.1.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t1` (`int`, *optional*, 默认为 47) — 时间步 t0。应在范围 [t0 + 1, num_inference_steps
    - 1] 内。参见 [paper](https://arxiv.org/abs/2303.13439)，第3.3.1节。'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 调用管道生成时调用的函数。
- en: '#### `backward_loop`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `backward_loop`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L853)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L853)'
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) — 在推断过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step. extra_step_kwargs — Extra_step_kwargs. cross_attention_kwargs — A
    kwargs dictionary that if specified is passed along to the `AttentionProcessor`
    as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
    num_warmup_steps — number of warmup steps.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调。extra_step_kwargs
    — 额外的步骤参数。cross_attention_kwargs — 如果指定，将传递给 `AttentionProcessor` 的 kwargs 字典，如在
    [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中定义。num_warmup_steps — 预热步数。'
- en: Returns
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: latents
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 潜变量
- en: latents of backward process output at time timesteps[-1]
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 反向过程在时间步 timesteps[-1] 输出的潜变量
- en: Perform backward process given list of time steps
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 给定时间步列表执行反向过程
- en: '#### `disable_vae_slicing`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L448)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L448)'
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片VAE解码。如果先前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `enable_vae_slicing`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L440)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L440)'
- en: '[PRE22]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。启用此选项时，VAE将在几个步骤中将输入张量分割为片段以进行解码。这对节省一些内存并允许更大的批量大小很有用。
- en: '#### `encode_prompt`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L594)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L594)'
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示'
- en: '`prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts to be
    sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is used
    in both text-encoders device — (`torch.device`): torch device'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_2` (`str` 或 `List[str]`, *可选*) — 发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则在两个文本编码器设备中使用`prompt`
    — (`torch.device`): torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 每个提示应生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。'
- en: '`negative_prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_2` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则在两个文本编码器中使用`negative_prompt`'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负negative_prompt_embeds。'
- en: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated pooled
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, pooled text embeddings will be generated from `prompt` input
    argument.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化文本嵌入。'
- en: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负池化的negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A lora scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *可选*) — 如果加载了LoRA层，则将应用于文本编码器所有LoRA层的lora比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 从CLIP中跳过的层数，用于计算提示嵌入。值为1意味着将使用预最终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: '#### `forward_loop`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward_loop`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L829)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L829)'
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: Returns
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: x_t1
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: x_t1
- en: Forward process applied to x_t0 from time t0 to t1.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间t0到t1应用于x_t0的前向过程。
- en: Perform DDPM forward process from time t0 to t1\. This is the same as adding
    noise with corresponding variance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间t0到t1执行DDPM前向过程。这与添加相应方差的噪声相同。
- en: TextToVideoPipelineOutput
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到视频管道输出
- en: '### `class diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L182)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L182)'
- en: '[PRE25]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`[List[PIL.Image.Image]`, `np.ndarray`]) — List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`[List[PIL.Image.Image]`, `np.ndarray`]) — 长度为 `batch_size` 的去噪 PIL
    图像列表，或者形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。'
- en: '`nsfw_content_detected` (`[List[bool]]`) — List indicating whether the corresponding
    generated image contains “not-safe-for-work” (nsfw) content or `None` if safety
    checking could not be performed.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nsfw_content_detected` (`[List[bool]]`) — 列表，指示相应生成的图像是否包含“不适宜工作场所”（nsfw）内容，如果无法执行安全检查，则为
    `None`。'
- en: Output class for zero-shot text-to-video pipeline.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本文本到视频管道的输出类。
