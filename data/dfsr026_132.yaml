- en: AudioLDM 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AudioLDM 2
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/audioldm2](https://huggingface.co/docs/diffusers/api/pipelines/audioldm2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/audioldm2](https://huggingface.co/docs/diffusers/api/pipelines/audioldm2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'AudioLDM 2 was proposed in [AudioLDM 2: Learning Holistic Audio Generation
    with Self-supervised Pretraining](https://arxiv.org/abs/2308.05734) by Haohe Liu
    et al. AudioLDM 2 takes a text prompt as input and predicts the corresponding
    audio. It can generate text-conditional sound effects, human speech and music.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 'AudioLDM 2是由刘浩和其他人在[AudioLDM 2: Learning Holistic Audio Generation with Self-supervised
    Pretraining](https://arxiv.org/abs/2308.05734)中提出的。AudioLDM 2以文本提示作为输入，预测相应的音频。它可以生成文本条件的音效、人类语音和音乐。'
- en: 'Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview),
    AudioLDM 2 is a text-to-audio *latent diffusion model (LDM)* that learns continuous
    audio representations from text embeddings. Two text encoder models are used to
    compute the text embeddings from a prompt input: the text-branch of [CLAP](https://huggingface.co/docs/transformers/main/en/model_doc/clap)
    and the encoder of [Flan-T5](https://huggingface.co/docs/transformers/main/en/model_doc/flan-t5).
    These text embeddings are then projected to a shared embedding space by an [AudioLDM2ProjectionModel](https://huggingface.co/docs/diffusers/main/api/pipelines/audioldm2#diffusers.AudioLDM2ProjectionModel).
    A [GPT2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2) *language
    model (LM)* is used to auto-regressively predict eight new embedding vectors,
    conditional on the projected CLAP and Flan-T5 embeddings. The generated embedding
    vectors and Flan-T5 text embeddings are used as cross-attention conditioning in
    the LDM. The [UNet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2UNet2DConditionModel)
    of AudioLDM 2 is unique in the sense that it takes **two** cross-attention embeddings,
    as opposed to one cross-attention conditioning, as in most other LDMs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 受[Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)启发，AudioLDM
    2是一个文本到音频的*潜在扩散模型（LDM）*，从文本嵌入中学习连续的音频表示。使用两个文本编码器模型从提示输入计算文本嵌入：[CLAP](https://huggingface.co/docs/transformers/main/en/model_doc/clap)的文本分支和[Flan-T5](https://huggingface.co/docs/transformers/main/en/model_doc/flan-t5)的编码器。然后，这些文本嵌入通过[AudioLDM2ProjectionModel](https://huggingface.co/docs/diffusers/main/api/pipelines/audioldm2#diffusers.AudioLDM2ProjectionModel)投影到共享的嵌入空间。使用[GPT2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2)的*语言模型（LM）*自回归地预测八个新的嵌入向量，条件是基于投影的CLAP和Flan-T5嵌入。生成的嵌入向量和Flan-T5文本嵌入用作LDM中的交叉注意力调节。AudioLDM
    2的[UNet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2UNet2DConditionModel)在于它采用**两个**交叉注意力嵌入，而不是大多数其他LDM中的一个交叉注意力调节。
- en: 'The abstract of the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Although audio generation shares commonalities across different types of audio,
    such as speech, music, and sound effects, designing models for each type requires
    careful consideration of specific objectives and biases that can significantly
    differ from those of other types. To bring us closer to a unified perspective
    of audio generation, this paper proposes a framework that utilizes the same learning
    method for speech, music, and sound effect generation. Our framework introduces
    a general representation of audio, called “language of audio” (LOA). Any audio
    can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation
    learning model. In the generation process, we translate any modalities into LOA
    by using a GPT-2 model, and we perform self-supervised audio generation learning
    with a latent diffusion model conditioned on LOA. The proposed framework naturally
    brings advantages such as in-context learning abilities and reusable self-supervised
    pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks
    of text-to-audio, text-to-music, and text-to-speech demonstrate state-of-the-art
    or competitive performance against previous approaches. Our code, pretrained model,
    and demo are available at [this https URL](https://audioldm.github.io/audioldm2).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管音频生成在不同类型的音频中有共同之处，比如语音、音乐和音效，但为每种类型设计模型需要仔细考虑特定目标和偏见，这些偏见可能与其他类型的偏见有很大不同。为了让我们更接近音频生成的统一视角，本文提出了一个框架，利用相同的学习方法来生成语音、音乐和音效。我们的框架引入了一个称为“音频语言”（LOA）的音频的通用表示。任何音频都可以基于AudioMAE转换为LOA，这是一个自监督预训练表示学习模型。在生成过程中，我们使用GPT-2模型将任何形式转换为LOA，并使用条件于LOA的潜在扩散模型进行自监督音频生成学习。所提出的框架自然地带来了优势，如上下文学习能力和可重复使用的自监督预训练的AudioMAE和潜在扩散模型。对文本到音频、文本到音乐和文本到语音的主要基准进行的实验表明，与以前的方法相比，我们的性能达到了最先进或具有竞争力。我们的代码、预训练模型和演示可在[此https
    URL](https://audioldm.github.io/audioldm2)上找到。*'
- en: This pipeline was contributed by [sanchit-gandhi](https://huggingface.co/sanchit-gandhi).
    The original codebase can be found at [haoheliu/audioldm2](https://github.com/haoheliu/audioldm2).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道由[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)贡献。原始代码库可在[haoheliu/audioldm2](https://github.com/haoheliu/audioldm2)找到。
- en: Tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: Choosing a checkpoint
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择检查点
- en: AudioLDM2 comes in three variants. Two of these checkpoints are applicable to
    the general task of text-to-audio generation. The third checkpoint is trained
    exclusively on text-to-music generation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AudioLDM2有三个变体。其中两个检查点适用于文本到音频生成的一般任务。第三个检查点专门用于文本到音乐生成。
- en: 'All checkpoints share the same model size for the text encoders and VAE. They
    differ in the size and depth of the UNet. See table below for details on the three
    checkpoints:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有检查点在文本编码器和VAE的模型大小上是相同的。它们在UNet的大小和深度上有所不同。有关三个检查点的详细信息，请参见下表：
- en: '| Checkpoint | Task | UNet Model Size | Total Model Size | Training Data /
    h |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 检查点 | 任务 | UNet模型大小 | 总模型大小 | 训练数据/小时 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| [audioldm2](https://huggingface.co/cvssp/audioldm2) | Text-to-audio | 350M
    | 1.1B | 1150k |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [audioldm2](https://huggingface.co/cvssp/audioldm2) | 文本到音频 | 350M | 1.1B
    | 1150k |'
- en: '| [audioldm2-large](https://huggingface.co/cvssp/audioldm2-large) | Text-to-audio
    | 750M | 1.5B | 1150k |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| [audioldm2-large](https://huggingface.co/cvssp/audioldm2-large) | 文本到音频 |
    750M | 1.5B | 1150k |'
- en: '| [audioldm2-music](https://huggingface.co/cvssp/audioldm2-music) | Text-to-music
    | 350M | 1.1B | 665k |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| [audioldm2-music](https://huggingface.co/cvssp/audioldm2-music) | 文本到音乐 |
    350M | 1.1B | 665k |'
- en: Constructing a prompt
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建提示
- en: 'Descriptive prompt inputs work best: use adjectives to describe the sound (e.g.
    “high quality” or “clear”) and make the prompt context specific (e.g. “water stream
    in a forest” instead of “stream”).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述性提示输入效果最佳：使用形容词描述声音（例如“高质量”或“清晰”），并使提示具体化（例如“森林中的水流”而不是“流”）。
- en: It’s best to use general terms like “cat” or “dog” instead of specific names
    or abstract objects the model may not be familiar with.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好使用“猫”或“狗”等一般术语，而不是模型可能不熟悉的具体名称或抽象对象。
- en: Using a **negative prompt** can significantly improve the quality of the generated
    waveform, by guiding the generation away from terms that correspond to poor quality
    audio. Try using a negative prompt of “Low quality.”
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**负面提示**可以显著提高生成波形的质量，通过引导生成远离对应于低质量音频的术语。尝试使用“低质量”的负面提示。
- en: Controlling inference
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制推理
- en: The *quality* of the predicted audio sample can be controlled by the `num_inference_steps`
    argument; higher steps give higher quality audio at the expense of slower inference.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测音频样本的*质量*可以通过`num_inference_steps`参数进行控制；更多步骤会提供更高质量的音频，但会降低推理速度。
- en: The *length* of the predicted audio sample can be controlled by varying the
    `audio_length_in_s` argument.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测音频样本的*长度*可以通过变化`audio_length_in_s`参数进行控制。
- en: 'Evaluating generated waveforms:'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估生成的波形：
- en: The quality of the generated waveforms can vary significantly based on the seed.
    Try generating with different seeds until you find a satisfactory generation.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的波形的质量可以根据种子而有很大差异。尝试使用不同的种子生成，直到找到令人满意的生成。
- en: 'Multiple waveforms can be generated in one go: set `num_waveforms_per_prompt`
    to a value greater than 1\. Automatic scoring will be performed between the generated
    waveforms and prompt text, and the audios ranked from best to worst accordingly.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以一次生成多个波形：将`num_waveforms_per_prompt`设置为大于1的值。将在生成的波形和提示文本之间执行自动评分，并相应地将音频从最佳到最差进行排名。
- en: 'The following example demonstrates how to construct good music generation using
    the aforementioned tips: [example](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2Pipeline.__call__.example).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何使用上述提示构建良好的音乐生成：[示例](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2Pipeline.__call__.example)。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看调度器[指南](../../using-diffusers/schedulers)以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。
- en: AudioLDM2Pipeline
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AudioLDM2Pipeline
- en: '### `class diffusers.AudioLDM2Pipeline`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AudioLDM2Pipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L103)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L103)'
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）-
    变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。'
- en: '`text_encoder` ([ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel))
    — First frozen text-encoder. AudioLDM2 uses the joint audio-text embedding model
    [CLAP](https://huggingface.co/docs/transformers/model_doc/clap#transformers.CLAPTextModelWithProjection),
    specifically the [laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)
    variant. The text branch is used to encode the text prompt to a prompt embedding.
    The full audio-text model is used to rank generated waveforms against the text
    prompt by computing similarity scores.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（[ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)）-
    第一个冻结的文本编码器。AudioLDM2使用联合音频文本嵌入模型[CLAP](https://huggingface.co/docs/transformers/model_doc/clap#transformers.CLAPTextModelWithProjection)，具体来说是[laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)变体。文本分支用于将文本提示编码为提示嵌入。完整的音频文本模型用于通过计算相似性分数对生成的波形与文本提示进行排名。'
- en: '`text_encoder_2` ([T5EncoderModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5EncoderModel))
    — Second frozen text-encoder. AudioLDM2 uses the encoder of [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel),
    specifically the [google/flan-t5-large](https://huggingface.co/google/flan-t5-large)
    variant.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_2`（[T5EncoderModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5EncoderModel)）-
    第二个冻结的文本编码器。AudioLDM2使用[T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel)的编码器，具体来说是[google/flan-t5-large](https://huggingface.co/google/flan-t5-large)变体。'
- en: '`projection_model` ([AudioLDM2ProjectionModel](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm2#diffusers.AudioLDM2ProjectionModel))
    — A trained model used to linearly project the hidden-states from the first and
    second text encoder models and insert learned SOS and EOS token embeddings. The
    projected hidden-states from the two text encoders are concatenated to give the
    input to the language model.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_model`（[AudioLDM2ProjectionModel](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm2#diffusers.AudioLDM2ProjectionModel)）-
    用于线性投影第一个和第二个文本编码器模型的隐藏状态，并插入学习的SOS和EOS令牌嵌入的训练模型。来自两个文本编码器的投影隐藏状态被连接在一起，以提供输入给语言模型。'
- en: '`language_model` ([GPT2Model](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model))
    — An auto-regressive language model used to generate a sequence of hidden-states
    conditioned on the projected outputs from the two text encoders.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language_model`（[GPT2Model](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model)）-
    用于生成一系列隐藏状态的自回归语言模型，条件是从两个文本编码器的投影输出。'
- en: '`tokenizer` ([RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer))
    — Tokenizer to tokenize text for the first frozen text-encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)）-
    用于为第一个冻结文本编码器对文本进行标记的标记器。'
- en: '`tokenizer_2` ([T5Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer))
    — Tokenizer to tokenize text for the second frozen text-encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_2`（[T5Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer)）-
    用于为第二个冻结文本编码器对文本进行标记的标记器。'
- en: '`feature_extractor` ([ClapFeatureExtractor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor))
    — Feature extractor to pre-process generated audio waveforms to log-mel spectrograms
    for automatic scoring.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`（[ClapFeatureExtractor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)）-
    用于将生成的音频波形预处理为自动评分的log-mel频谱图的特征提取器。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A `UNet2DConditionModel` to denoise the encoded audio latents.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）-
    用于去噪编码音频潜在空间的`UNet2DConditionModel`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded audio
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）-
    用于与`unet`结合使用以去噪编码音频潜在空间的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: '`vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan))
    — Vocoder of class `SpeechT5HifiGan` to convert the mel-spectrogram latents to
    the final audio waveform.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocoder`（[SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan)）-
    类`SpeechT5HifiGan`的声码器，用于将mel频谱潜在空间转换为最终音频波形。'
- en: Pipeline for text-to-audio generation using AudioLDM2.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AudioLDM2生成文本到音频的流水线。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L732)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L732)'
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    audio generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）- 用于指导音频生成的提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`audio_length_in_s` (`int`, *optional*, defaults to 10.24) — The length of
    the generated audio sample in seconds.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio_length_in_s`（`int`，*可选*，默认为10.24）- 生成的音频样本的长度（秒）。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 200) — The number of
    denoising steps. More denoising steps usually lead to a higher quality audio at
    the expense of slower inference.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`（`int`，*可选*，默认为200）- 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的音频，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 3.5) — A higher guidance
    scale value encourages the model to generate audio that is closely linked to the
    text `prompt` at the expense of lower sound quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`（`float`，*可选*，默认为3.5）- 更高的指导比例值鼓励模型生成与文本`prompt`密切相关的音频，但会降低声音质量。当`guidance_scale
    > 1`时启用指导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in audio generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`（`str`或`List[str]`，*可选*）- 用于指导音频生成中不包含的提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时（`guidance_scale
    < 1`）将被忽略。'
- en: '`num_waveforms_per_prompt` (`int`, *optional*, defaults to 1) — The number
    of waveforms to generate per prompt. If `num_waveforms_per_prompt > 1`, then automatic
    scoring is performed between the generated outputs and the text prompt. This scoring
    ranks the generated waveforms based on their cosine similarity with the text input
    in the joint text-audio embedding space.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_waveforms_per_prompt`（`int`，*可选*，默认为1）- 每个提示生成的波形数量。如果`num_waveforms_per_prompt
    > 1`，则在生成的输出和文本提示之间执行自动评分。此评分根据生成的波形在联合文本-音频嵌入空间中与文本输入的余弦相似度对生成的波形进行排名。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`（`float`，*可选*，默认为0.0）- 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta（η）。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成具有确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for spectrogram generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中预生成的嘈杂潜在变量，用作频谱图生成的输入。可用于使用不同提示调整相同生成。如果未提供，将通过使用提供的随机
    `generator` 进行采样生成潜在变量张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`
    将从 `negative_prompt` 输入参数生成。'
- en: '`generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    text embeddings from the GPT2 langauge model. Can be used to easily tweak text
    inputs, *e.g.* prompt weighting. If not provided, text embeddings will be generated
    from `prompt` input argument.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — 来自GPT2语言模型的预生成文本嵌入。可用于轻松调整文本输入，*例如*
    提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。'
- en: '`negative_generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings from the GPT2 language model. Can be used to easily tweak
    text inputs, *e.g.* prompt weighting. If not provided, negative_prompt_embeds
    will be computed from `negative_prompt` input argument.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — 来自GPT2语言模型的预生成的负面文本嵌入。可用于轻松调整文本输入，*例如*
    提示加权。如果未提供，将从 `negative_prompt` 输入参数计算负面提示嵌入。'
- en: '`attention_mask` (`torch.LongTensor`, *optional*) — Pre-computed attention
    mask to be applied to the `prompt_embeds`. If not provided, attention mask will
    be computed from `prompt` input argument.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`, *optional*) — 预先计算的注意力掩码，应用于 `prompt_embeds`。如果未提供，注意力掩码将从
    `prompt` 输入参数计算。'
- en: '`negative_attention_mask` (`torch.LongTensor`, *optional*) — Pre-computed attention
    mask to be applied to the `negative_prompt_embeds`. If not provided, attention
    mask will be computed from `negative_prompt` input argument.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_attention_mask` (`torch.LongTensor`, *optional*) — 预先计算的注意力掩码，应用于
    `negative_prompt_embeds`。如果未提供，注意力掩码将从 `negative_prompt` 输入参数计算。'
- en: '`max_new_tokens` (`int`, *optional*, defaults to None) — Number of new tokens
    to generate with the GPT2 language model. If not provided, number of tokens will
    be taken from the config of the model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens` (`int`, *optional*, 默认为 None) — 使用GPT2语言模型生成的新标记数量。如果未提供，将从模型的配置中获取标记数量。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    而不是普通元组。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) — 在推断期间每 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中定义的 `AttentionProcessor` 的 kwargs 字典。'
- en: '`output_type` (`str`, *optional*, defaults to `"np"`) — The output format of
    the generated audio. Choose between `"np"` to return a NumPy `np.ndarray` or `"pt"`
    to return a PyTorch `torch.Tensor` object. Set to `"latent"` to return the latent
    diffusion model (LDM) output.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认为 `"np"`) — 生成音频的输出格式。选择 `"np"` 返回一个 NumPy
    `np.ndarray` 或 `"pt"` 返回一个 PyTorch `torch.Tensor` 对象。设置为 `"latent"` 以返回潜在扩散模型（LDM）输出。'
- en: Returns
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated audio.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，将返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则将返回一个
    `tuple`，其中第一个元素是包含生成音频的列表。
- en: The call function to the pipeline for generation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `disable_vae_slicing`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L185)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L185)'
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片VAE解码。如果先前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `enable_model_cpu_offload`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_model_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L192)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L192)'
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Offloads all models to CPU using accelerate, reducing memory usage with a low
    impact on performance. Compared to `enable_sequential_cpu_offload`, this method
    moves one whole model at a time to the GPU when its `forward` method is called,
    and the model remains in GPU until the next model runs. Memory savings are lower
    than with `enable_sequential_cpu_offload`, but performance is much better due
    to the iterative execution of the `unet`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加速将所有模型转移到CPU，减少内存使用量对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将一个完整模型移动到GPU，并且模型保持在GPU中，直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较低，但由于`unet`的迭代执行，性能要好得多。
- en: '#### `enable_vae_slicing`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L177)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L177)'
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。当启用此选项时，VAE将在几个步骤中将输入张量分割成片段以计算解码。这对节省一些内存并允许更大的批量大小很有用。
- en: '#### `encode_prompt`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L270)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L270)'
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示'
- en: '`device` (`torch.device`) — torch device'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`torch.device`) — torch设备'
- en: '`num_waveforms_per_prompt` (`int`) — number of waveforms that should be generated
    per prompt'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_waveforms_per_prompt` (`int`) — 每个提示应生成的波形数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the audio generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导音频生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-computed text embeddings
    from the Flan T5 model. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, text embeddings will be computed from `prompt` input
    argument.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 从Flan T5模型预计算的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数计算文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-computed negative
    text embeddings from the Flan T5 model. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be computed
    from `negative_prompt` input argument.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 从Flan T5模型预计算的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数计算负面提示嵌入。'
- en: '`generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    text embeddings from the GPT2 langauge model. Can be used to easily tweak text
    inputs, *e.g.* prompt weighting. If not provided, text embeddings will be generated
    from `prompt` input argument.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generated_prompt_embeds` (`torch.FloatTensor`, *可选*) — 从GPT2语言模型预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings from the GPT2 language model. Can be used to easily tweak
    text inputs, *e.g.* prompt weighting. If not provided, negative_prompt_embeds
    will be computed from `negative_prompt` input argument.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_generated_prompt_embeds` (`torch.FloatTensor`, *可选*) — 从GPT2语言模型预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数计算负面提示嵌入。'
- en: '`attention_mask` (`torch.LongTensor`, *optional*) — Pre-computed attention
    mask to be applied to the `prompt_embeds`. If not provided, attention mask will
    be computed from `prompt` input argument.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`, *可选*) — 覦计算的注意力掩码，应用于`prompt_embeds`。如果未提供，注意力掩码将从`prompt`输入参数计算。'
- en: '`negative_attention_mask` (`torch.LongTensor`, *optional*) — Pre-computed attention
    mask to be applied to the `negative_prompt_embeds`. If not provided, attention
    mask will be computed from `negative_prompt` input argument.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_attention_mask` (`torch.LongTensor`, *可选*) — 预计算的注意力掩码，应用于`negative_prompt_embeds`。如果未提供，注意力掩码将从`negative_prompt`输入参数计算。'
- en: '`max_new_tokens` (`int`, *optional*, defaults to None) — The number of new
    tokens to generate with the GPT2 language model.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens` (`int`, *可选*, 默认为None) — 与GPT2语言模型生成的新令牌数量。'
- en: Returns
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: prompt_embeds (`torch.FloatTensor`)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: prompt_embeds (`torch.FloatTensor`)
- en: 'Text embeddings from the Flan T5 model. attention_mask (`torch.LongTensor`):
    Attention mask to be applied to the `prompt_embeds`. generated_prompt_embeds (`torch.FloatTensor`):
    Text embeddings generated from the GPT2 langauge model.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '来自Flan T5模型的文本嵌入。attention_mask (`torch.LongTensor`): 应用于`prompt_embeds`的注意力掩码。generated_prompt_embeds
    (`torch.FloatTensor`): 从GPT2语言模型生成的文本嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: 'Example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `generate_language_model`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate_language_model`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L229)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L229)'
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — The sequence used as a prompt for the generation.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 用作生成提示的序列。'
- en: '`max_new_tokens` (`int`) — Number of new tokens to generate.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens` (`int`) — 要生成的新标记数。'
- en: '`model_kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of additional
    model-specific kwargs that will be forwarded to the `forward` function of the
    model.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_kwargs` (`Dict[str, Any]`, *optional*) — 附加模型特定kwargs的特定参数化，将被转发到模型的`forward`函数。'
- en: Returns
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`inputs_embeds (`torch.FloatTensor`of shape`(batch_size, sequence_length, hidden_size)`)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputs_embeds (`torch.FloatTensor`of shape`(batch_size, sequence_length, hidden_size)`)'
- en: The sequence of generated hidden-states.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的隐藏状态序列。
- en: Generates a sequence of hidden-states from the language model, conditioned on
    the embedding inputs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从语言模型生成一系列隐藏状态，条件是嵌入输入。
- en: AudioLDM2ProjectionModel
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AudioLDM2ProjectionModel
- en: '### `class diffusers.AudioLDM2ProjectionModel`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AudioLDM2ProjectionModel`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L82)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L82)'
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_encoder_dim` (`int`) — Dimensionality of the text embeddings from the
    first text encoder (CLAP).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_dim` (`int`) — 第一个文本编码器（CLAP）中文本嵌入的维度。'
- en: '`text_encoder_1_dim` (`int`) — Dimensionality of the text embeddings from the
    second text encoder (T5 or VITS).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_1_dim` (`int`) — 第二个文本编码器（T5或VITS）中文本嵌入的维度。'
- en: '`langauge_model_dim` (`int`) — Dimensionality of the text embeddings from the
    language model (GPT2).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langauge_model_dim` (`int`) — 语言模型（GPT2）中文本嵌入的维度。'
- en: A simple linear projection model to map two text embeddings to a shared latent
    space. It also inserts learned embedding vectors at the start and end of each
    text embedding sequence respectively. Each variable appended with `_1` refers
    to that corresponding to the second text encoder. Otherwise, it is from the first.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的线性投影模型，将两个文本嵌入映射到共享的潜在空间。它还分别在每个文本嵌入序列的开头和结尾插入了学习到的嵌入向量。每个附加`_1`的变量指的是第二个文本编码器对应的变量。否则，它来自第一个。
- en: '#### `forward`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L111)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L111)'
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: AudioLDM2UNet2DConditionModel
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AudioLDM2UNet2DConditionModel
- en: '### `class diffusers.AudioLDM2UNet2DConditionModel`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AudioLDM2UNet2DConditionModel`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L148)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L148)'
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sample_size` (`int` or `Tuple[int, int]`, *optional*, defaults to `None`)
    — Height and width of input/output sample.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size` (`int` or `Tuple[int, int]`, *optional*, defaults to `None`)
    — 输入/输出样本的高度和宽度。'
- en: '`in_channels` (`int`, *optional*, defaults to 4) — Number of channels in the
    input sample.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels` (`int`, *optional*, defaults to 4) — 输入样本中的通道数。'
- en: '`out_channels` (`int`, *optional*, defaults to 4) — Number of channels in the
    output.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels` (`int`, *optional*, defaults to 4) — 输出中的通道数。'
- en: '`flip_sin_to_cos` (`bool`, *optional*, defaults to `False`) — Whether to flip
    the sin to cos in the time embedding.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flip_sin_to_cos` (`bool`, *optional*, defaults to `False`) — 是否在时间嵌入中将sin翻转为cos。'
- en: '`freq_shift` (`int`, *optional*, defaults to 0) — The frequency shift to apply
    to the time embedding.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`freq_shift` (`int`, *optional*, defaults to 0) — 要应用于时间嵌入的频率偏移。'
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `("CrossAttnDownBlock2D",
    "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D")`) — The tuple of
    downsample blocks to use.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `("CrossAttnDownBlock2D",
    "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D")`) — 要使用的下采样块的元组。'
- en: '`mid_block_type` (`str`, *optional*, defaults to `"UNetMidBlock2DCrossAttn"`)
    — Block type for middle of UNet, it can only be `UNetMidBlock2DCrossAttn` for
    AudioLDM2.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mid_block_type` (`str`, *optional*, defaults to `"UNetMidBlock2DCrossAttn"`)
    — UNet中间部分的块类型，对于AudioLDM2，它只能是`UNetMidBlock2DCrossAttn`。'
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpBlock2D", "CrossAttnUpBlock2D",
    "CrossAttnUpBlock2D", "CrossAttnUpBlock2D")`) — The tuple of upsample blocks to
    use.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpBlock2D", "CrossAttnUpBlock2D",
    "CrossAttnUpBlock2D", "CrossAttnUpBlock2D")`) — 要使用的上采样块的元组。'
- en: '`only_cross_attention` (`bool` or `Tuple[bool]`, *optional*, default to `False`)
    — Whether to include self-attention in the basic transformer blocks, see `BasicTransformerBlock`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_cross_attention` (`bool` or `Tuple[bool]`, *optional*, default to `False`)
    — 是否在基本变换器块中包含自注意力，参见`BasicTransformerBlock`。'
- en: '`block_out_channels` (`Tuple[int]`, *optional*, defaults to `(320, 640, 1280,
    1280)`) — The tuple of output channels for each block.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_out_channels` (`Tuple[int]`, *optional*, defaults to `(320, 640, 1280,
    1280)`) — 每个块的输出通道的元组。'
- en: '`layers_per_block` (`int`, *optional*, defaults to 2) — The number of layers
    per block.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_per_block` (`int`, *optional*, defaults to 2) — 每个块的层数。'
- en: '`downsample_padding` (`int`, *optional*, defaults to 1) — The padding to use
    for the downsampling convolution.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`downsample_padding` (`int`, *optional*, defaults to 1) — 用于下采样卷积的填充。'
- en: '`mid_block_scale_factor` (`float`, *optional*, defaults to 1.0) — The scale
    factor to use for the mid block.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mid_block_scale_factor` (`float`, *optional*, defaults to 1.0) — 中间块使用的比例因子。'
- en: '`act_fn` (`str`, *optional*, defaults to `"silu"`) — The activation function
    to use.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn` (`str`, *optional*, defaults to `"silu"`) — 要使用的激活函数。'
- en: '`norm_num_groups` (`int`, *optional*, defaults to 32) — The number of groups
    to use for the normalization. If `None`, normalization and activation layers is
    skipped in post-processing.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_num_groups` (`int`, *optional*, defaults to 32) — 用于规范化的组数。如果为`None`，则跳过后处理中的规范化和激活层。'
- en: '`norm_eps` (`float`, *optional*, defaults to 1e-5) — The epsilon to use for
    the normalization.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_eps` (`float`, *optional*, defaults to 1e-5) — 用于规范化的epsilon。'
- en: '`cross_attention_dim` (`int` or `Tuple[int]`, *optional*, defaults to 1280)
    — The dimension of the cross attention features.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`（`int`或`Tuple[int]`，*可选*，默认为1280）— 交叉注意力特征的维度。'
- en: '`transformer_layers_per_block` (`int` or `Tuple[int]`, *optional*, defaults
    to 1) — The number of transformer blocks of type `BasicTransformerBlock`. Only
    relevant for `~models.unet_2d_blocks.CrossAttnDownBlock2D`, `~models.unet_2d_blocks.CrossAttnUpBlock2D`,
    `~models.unet_2d_blocks.UNetMidBlock2DCrossAttn`.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_layers_per_block`（`int`或`Tuple[int]`，*可选*，默认为1）— `BasicTransformerBlock`类型的transformer块的数量。仅适用于`~models.unet_2d_blocks.CrossAttnDownBlock2D`、`~models.unet_2d_blocks.CrossAttnUpBlock2D`、`~models.unet_2d_blocks.UNetMidBlock2DCrossAttn`。'
- en: '`attention_head_dim` (`int`, *optional*, defaults to 8) — The dimension of
    the attention heads.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_head_dim`（`int`，*可选*，默认为8）— 注意力头的维度。'
- en: '`num_attention_heads` (`int`, *optional*) — The number of attention heads.
    If not defined, defaults to `attention_head_dim`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`（`int`，*可选*）— 注意力头的数量。如果未定义，默认为`attention_head_dim`。'
- en: '`resnet_time_scale_shift` (`str`, *optional*, defaults to `"default"`) — Time
    scale shift config for ResNet blocks (see `ResnetBlock2D`). Choose from `default`
    or `scale_shift`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resnet_time_scale_shift`（`str`，*可选*，默认为`"default"`）— ResNet块的时间尺度偏移配置（参见`ResnetBlock2D`）。选择`default`或`scale_shift`。'
- en: '`class_embed_type` (`str`, *optional*, defaults to `None`) — The type of class
    embedding to use which is ultimately summed with the time embeddings. Choose from
    `None`, `"timestep"`, `"identity"`, `"projection"`, or `"simple_projection"`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_embed_type`（`str`，*可选*，默认为`None`）— 要使用的类嵌入类型，最终与时间嵌入相加。选择`None`、`"timestep"`、`"identity"`、`"projection"`或`"simple_projection"`。'
- en: '`num_class_embeds` (`int`, *optional*, defaults to `None`) — Input dimension
    of the learnable embedding matrix to be projected to `time_embed_dim`, when performing
    class conditioning with `class_embed_type` equal to `None`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_class_embeds`（`int`，*可选*，默认为`None`）— 要投影到`time_embed_dim`的可学习嵌入矩阵的输入维度，在执行`class_embed_type`等于`None`的类条件时。'
- en: '`time_embedding_type` (`str`, *optional*, defaults to `positional`) — The type
    of position embedding to use for timesteps. Choose from `positional` or `fourier`.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_embedding_type`（`str`，*可选*，默认为`positional`）— 用于时间步的位置嵌入类型。选择`positional`或`fourier`。'
- en: '`time_embedding_dim` (`int`, *optional*, defaults to `None`) — An optional
    override for the dimension of the projected time embedding.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_embedding_dim`（`int`，*可选*，默认为`None`）— 用于投影时间嵌入的维度的可选覆盖。'
- en: '`time_embedding_act_fn` (`str`, *optional*, defaults to `None`) — Optional
    activation function to use only once on the time embeddings before they are passed
    to the rest of the UNet. Choose from `silu`, `mish`, `gelu`, and `swish`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_embedding_act_fn`（`str`，*可选*，默认为`None`）— 仅在时间嵌入传递到UNet的其余部分之前使用的可选激活函数。选择`silu`、`mish`、`gelu`和`swish`。'
- en: '`timestep_post_act` (`str`, *optional*, defaults to `None`) — The second activation
    function to use in timestep embedding. Choose from `silu`, `mish` and `gelu`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestep_post_act`（`str`，*可选*，默认为`None`）— 用于时间步嵌入中使用的第二个激活函数。选择`silu`、`mish`和`gelu`。'
- en: '`time_cond_proj_dim` (`int`, *optional*, defaults to `None`) — The dimension
    of `cond_proj` layer in the timestep embedding.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_cond_proj_dim`（`int`，*可选*，默认为`None`）— 时间步嵌入中`cond_proj`层的维度。'
- en: '`conv_in_kernel` (`int`, *optional*, default to `3`) — The kernel size of `conv_in`
    layer.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_in_kernel`（`int`，*可选*，默认为`3`）— `conv_in`层的内核大小。'
- en: '`conv_out_kernel` (`int`, *optional*, default to `3`) — The kernel size of
    `conv_out` layer.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_out_kernel`（`int`，*可选*，默认为`3`）— `conv_out`层的内核大小。'
- en: '`projection_class_embeddings_input_dim` (`int`, *optional*) — The dimension
    of the `class_labels` input when `class_embed_type="projection"`. Required when
    `class_embed_type="projection"`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_class_embeddings_input_dim`（`int`，*可选*）— 当`class_embed_type="projection"`时，`class_labels`输入的维度。在`class_embed_type="projection"`时需要。'
- en: '`class_embeddings_concat` (`bool`, *optional*, defaults to `False`) — Whether
    to concatenate the time embeddings with the class embeddings.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_embeddings_concat`（`bool`，*可选*，默认为`False`）— 是否将时间嵌入与类嵌入连接起来。'
- en: A conditional 2D UNet model that takes a noisy sample, conditional state, and
    a timestep and returns a sample shaped output. Compared to the vanilla [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel),
    this variant optionally includes an additional self-attention layer in each Transformer
    block, as well as multiple cross-attention layers. It also allows for up to two
    cross-attention embeddings, `encoder_hidden_states` and `encoder_hidden_states_1`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一个条件2D UNet模型，接受一个带噪声的样本、条件状态和一个时间步，并返回一个形状为样本的输出。与普通的[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)相比，这个变体在每个Transformer块中可选地包括一个额外的自注意力层，以及多个交叉注意力层。它还允许最多两个交叉注意力嵌入，`encoder_hidden_states`和`encoder_hidden_states_1`。
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for it’s generic methods implemented for all
    models (such as downloading or saving).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)。查看超类文档以了解为所有模型实现的通用方法（如下载或保存）。
- en: '#### `forward`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L662)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L662)'
- en: '[PRE12]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sample` (`torch.FloatTensor`) — The noisy input tensor with the following
    shape `(batch, channel, height, width)`.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`（`torch.FloatTensor`）— 具有以下形状的带噪声输入张量`(batch, channel, height, width)`。'
- en: '`timestep` (`torch.FloatTensor` or `float` or `int`) — The number of timesteps
    to denoise an input.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestep`（`torch.FloatTensor`或`float`或`int`）— 对输入进行去噪的时间步数。'
- en: '`encoder_hidden_states` (`torch.FloatTensor`) — The encoder hidden states with
    shape `(batch, sequence_length, feature_dim)`.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`torch.FloatTensor`）— 具有形状`(batch, sequence_length,
    feature_dim)`的编码器隐藏状态。'
- en: '`encoder_attention_mask` (`torch.Tensor`) — A cross-attention mask of shape
    `(batch, sequence_length)` is applied to `encoder_hidden_states`. If `True` the
    mask is kept, otherwise if `False` it is discarded. Mask will be converted into
    a bias, which adds large negative values to the attention scores corresponding
    to “discard” tokens.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask` (`torch.Tensor`) — 形状为`(batch, sequence_length)`的交叉注意力掩码应用于`encoder_hidden_states`。如果为`True`，则保留掩码，否则为`False`则丢弃。掩码将被转换为偏置，将大的负值添加到对应于“丢弃”标记的注意力分数。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)
    instead of a plain tuple.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个[UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)而不是一个普通的元组。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttnProcessor`.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 一个kwargs字典，如果指定了，则传递给`AttnProcessor`。'
- en: '`encoder_hidden_states_1` (`torch.FloatTensor`, *optional*) — A second set
    of encoder hidden states with shape `(batch, sequence_length_2, feature_dim_2)`.
    Can be used to condition the model on a different set of embeddings to `encoder_hidden_states`.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states_1` (`torch.FloatTensor`, *可选*) — 第二组形状为`(batch, sequence_length_2,
    feature_dim_2)`的编码器隐藏状态。可用于将模型条件化为与`encoder_hidden_states`不同的嵌入集。'
- en: '`encoder_attention_mask_1` (`torch.Tensor`, *optional*) — A cross-attention
    mask of shape `(batch, sequence_length_2)` is applied to `encoder_hidden_states_1`.
    If `True` the mask is kept, otherwise if `False` it is discarded. Mask will be
    converted into a bias, which adds large negative values to the attention scores
    corresponding to “discard” tokens.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask_1` (`torch.Tensor`, *可选*) — 形状为`(batch, sequence_length_2)`的交叉注意力掩码应用于`encoder_hidden_states_1`。如果为`True`，则保留掩码，否则为`False`则丢弃。掩码将被转换为偏置，将大的负值添加到对应于“丢弃”标记的注意力分数。'
- en: Returns
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)
    or `tuple`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)或`tuple`'
- en: If `return_dict` is True, an [UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)
    is returned, otherwise a `tuple` is returned where the first element is the sample
    tensor.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为True，则返回一个[UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)，否则返回一个`tuple`，其中第一个元素是样本张量。
- en: The [AudioLDM2UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm2#diffusers.AudioLDM2UNet2DConditionModel)
    forward method.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[AudioLDM2UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm2#diffusers.AudioLDM2UNet2DConditionModel)的前向方法。'
- en: AudioPipelineOutput
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AudioPipelineOutput
- en: '### `class diffusers.AudioPipelineOutput`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AudioPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L130)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L130)'
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`audios` (`np.ndarray`) — List of denoised audio samples of a NumPy array of
    shape `(batch_size, num_channels, sample_rate)`.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audios` (`np.ndarray`) — 形状为`(batch_size, num_channels, sample_rate)`的NumPy数组的去噪音频样本列表。'
- en: Output class for audio pipelines.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 音频管道的输出类。
