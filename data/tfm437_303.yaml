- en: Audio Spectrogram Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音频频谱变换器
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram
    Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James
    Glass. The Audio Spectrogram Transformer applies a [Vision Transformer](vit) to
    audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art
    results for audio classification.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '音频频谱变换器模型是由 Yuan Gong、Yu-An Chung、James Glass 在 [AST: 音频频谱变换器](https://arxiv.org/abs/2104.01778)
    中提出的。音频频谱变换器将视觉变换器应用于音频，通过将音频转换为图像（频谱图）。该模型在音频分类方面取得了最先进的结果。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*In the past decade, convolutional neural networks (CNNs) have been widely
    adopted as the main building block for end-to-end audio classification models,
    which aim to learn a direct mapping from audio spectrograms to corresponding labels.
    To better capture long-range global context, a recent trend is to add a self-attention
    mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it
    is unclear whether the reliance on a CNN is necessary, and if neural networks
    purely based on attention are sufficient to obtain good performance in audio classification.
    In this paper, we answer the question by introducing the Audio Spectrogram Transformer
    (AST), the first convolution-free, purely attention-based model for audio classification.
    We evaluate AST on various audio classification benchmarks, where it achieves
    new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,
    and 98.1% accuracy on Speech Commands V2.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*在过去的十年中，卷积神经网络（CNN）已被广泛采用作为端到端音频分类模型的主要构建模块，旨在学习从音频频谱到相应标签的直接映射。为了更好地捕获长距离全局上下文，最近的趋势是在
    CNN 之上添加自注意机制，形成 CNN-注意混合模型。然而，目前尚不清楚是否依赖于 CNN 是必要的，以及基于注意力的神经网络是否足以在音频分类中获得良好的性能。在本文中，我们通过引入音频频谱变换器（AST）来回答这个问题，这是第一个无卷积、纯注意力的音频分类模型。我们在各种音频分类基准上评估了
    AST，在这些基准上取得了新的最先进结果：在 AudioSet 上的 0.485 mAP，在 ESC-50 上的 95.6% 准确率，以及在 Speech
    Commands V2 上的 98.1% 准确率。*'
- en: '![drawing](../Images/9126a01e9659aadbacaabc0894d706f6.png) Audio Spectrogram
    Transformer architecture. Taken from the [original paper](https://arxiv.org/abs/2104.01778).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![drawing](../Images/9126a01e9659aadbacaabc0894d706f6.png) 音频频谱变换器架构。摘自[原始论文](https://arxiv.org/abs/2104.01778)。'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/YuanGongND/ast).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由 [nielsr](https://huggingface.co/nielsr) 贡献。原始代码可以在[这里](https://github.com/YuanGongND/ast)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset,
    it’s recommended to take care of the input normalization (to make sure the input
    has mean of 0 and std of 0.5). [ASTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor)
    takes care of this. Note that it uses the AudioSet mean and std by default. You
    can check [`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py)
    to see how the authors compute the stats for a downstream dataset.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自己的数据集上微调音频频谱变换器（AST）时，建议进行输入归一化处理（确保输入的均值为 0，标准差为 0.5）。[ASTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor)
    负责此操作。请注意，默认情况下它使用 AudioSet 的均值和标准差。您可以查看[`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py)来查看作者如何计算下游数据集的统计信息。
- en: Note that the AST needs a low learning rate (the authors use a 10 times smaller
    learning rate compared to their CNN model proposed in the [PSLA paper](https://arxiv.org/abs/2102.01243))
    and converges quickly, so please search for a suitable learning rate and learning
    rate scheduler for your task.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，AST 需要一个较低的学习率（作者使用比他们在 [PSLA 论文](https://arxiv.org/abs/2102.01243) 中提出的
    CNN 模型小 10 倍的学习率），并且收敛速度很快，因此请为您的任务搜索一个合适的学习率和学习率调度器。
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with the Audio Spectrogram Transformer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一份官方 Hugging Face 和社区（由 🌎 表示）资源列表，可帮助您开始使用音频频谱变换器。
- en: Audio Classification
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 音频分类
- en: A notebook illustrating inference with AST for audio classification can be found
    [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在[此处](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST)找到用于音频分类的
    AST 推理的笔记本。
- en: '[ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    受到这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)的支持。'
- en: 'See also: [Audio classification](../tasks/audio_classification).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[音频分类](../tasks/audio_classification)。
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。
- en: ASTConfig
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTConfig
- en: '### `class transformers.ASTConfig`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py#L31)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py#L31)'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为12) — Transformer编码器中每个注意力层的注意力头数量。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`function`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, 默认为0.0) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, 默认为0.0) — 注意力概率的丢弃比例。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, 默认为16) — 每个块的大小（分辨率）。'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, 默认为`True`) — 是否为查询、键和值添加偏置。'
- en: '`frequency_stride` (`int`, *optional*, defaults to 10) — Frequency stride to
    use when patchifying the spectrograms.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frequency_stride` (`int`, *optional*, 默认为10) — 在制作频谱图块时使用的频率步幅。'
- en: '`time_stride` (`int`, *optional*, defaults to 10) — Temporal stride to use
    when patchifying the spectrograms.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_stride` (`int`, *optional*, 默认为10) — 在制作频谱图块时使用的时间步幅。'
- en: '`max_length` (`int`, *optional*, defaults to 1024) — Temporal dimension of
    the spectrograms.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*, 默认为1024) — 频谱图的时间维度。'
- en: '`num_mel_bins` (`int`, *optional*, defaults to 128) — Frequency dimension of
    the spectrograms (number of Mel-frequency bins).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_mel_bins` (`int`, *optional*, 默认为128) — 频谱图的频率维度（Mel频率箱的数量）。'
- en: This is the configuration class to store the configuration of a [ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel).
    It is used to instantiate an AST model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the AST [MIT/ast-finetuned-audioset-10-10-0.4593](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)
    architecture.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel)配置的配置类。它用于根据指定的参数实例化AST模型，定义模型架构。使用默认值实例化配置将产生类似于AST
    [MIT/ast-finetuned-audioset-10-10-0.4593](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ASTFeatureExtractor
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTFeatureExtractor
- en: '### `class transformers.ASTFeatureExtractor`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L39)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L39)'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_size` (`int`, *optional*, defaults to 1) — The feature dimension of
    the extracted features.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size` (`int`, *optional*, 默认为1) — 提取特征的特征维度。'
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *optional*, 默认为16000) — 音频文件应数字化的采样率，以赫兹（Hz）表示。'
- en: '`num_mel_bins` (`int`, *optional*, defaults to 128) — Number of Mel-frequency
    bins.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_mel_bins` (`int`, *optional*, 默认为128) — Mel频率箱的数量。'
- en: '`max_length` (`int`, *optional*, defaults to 1024) — Maximum length to which
    to pad/truncate the extracted features.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*, 默认为1024) — 用于填充/截断提取特征的最大长度。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether or not to
    normalize the log-Mel features using `mean` and `std`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, 默认为`True`) — 是否归一化对数Mel特征使用`mean`和`std`。'
- en: '`mean` (`float`, *optional*, defaults to -4.2677393) — The mean value used
    to normalize the log-Mel features. Uses the AudioSet mean by default.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean` (`float`, *optional*, 默认为-4.2677393) — 用于归一化对数Mel特征的均值。默认使用AudioSet的均值。'
- en: '`std` (`float`, *optional*, defaults to 4.5689974) — The standard deviation
    value used to normalize the log-Mel features. Uses the AudioSet standard deviation
    by default.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std`（`float`，*可选*，默认为4.5689974）— 用于归一化log-Mel特征的标准差值。默认使用AudioSet的标准差。'
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)
    should return `attention_mask`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`（`bool`，*可选*，默认为`False`）— 是否[`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)应返回`attention_mask`。'
- en: Constructs a Audio Spectrogram Transformer (AST) feature extractor.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个音频频谱变换器（AST）特征提取器。
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此特征提取器继承自[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: This class extracts mel-filter bank features from raw speech using TorchAudio
    if installed or using numpy otherwise, pads/truncates them to a fixed length and
    normalizes them using a mean and standard deviation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类从原始语音中提取mel-filter bank特征，如果安装了TorchAudio，则使用TorchAudio，否则使用numpy，然后对它们进行填充/截断到固定长度，并使用均值和标准差进行归一化。
- en: '#### `__call__`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L161)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L161)'
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    — The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_speech`（`np.ndarray`，`List[float]`，`List[np.ndarray]`，`List[List[float]]`）—
    要填充的序列或批处理序列。每个序列可以是一个numpy数组，一个浮点值列表，一个numpy数组列表或一个浮点值列表的列表。必须是单声道音频，不是立体声，即每个时间步长一个浮点数。'
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`（`int`，*可选*）— `raw_speech` 输入采样的采样率。强烈建议在前向调用时传递`sampling_rate`以防止静默错误。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）—
    如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回Numpy `np.ndarray`对象。'
- en: Main method to featurize and prepare for the model one or several sequence(s).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对一个或多个序列进行特征化和准备模型的主要方法。
- en: ASTModel
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTModel
- en: '### `class transformers.ASTModel`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L430)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L430)'
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare AST Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 裸AST模型转换器输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L458)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L458)'
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    — Float values mel features extracted from the raw audio waveform. Raw audio waveform
    can be obtained by loading a `.flac` or `.wav` audio file into an array of type
    `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
    soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    — 从原始音频波形中提取的浮点值 mel 特征。原始音频波形可以通过将 `.flac` 或 `.wav` 音频文件加载到 `List[float]` 类型的数组或
    `numpy.ndarray` 中获得，例如通过声音文件库 (`pip install soundfile`)。要准备好数组以获得 `input_features`，应使用
    [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    提取 mel 特征，填充并转换为 `torch.FloatTensor` 类型的张量。参见 [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选在 `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `掩码`。
- en: 0 indicates the head is `masked`.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `掩码`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Returns
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    and inputs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含根据配置（[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)）和输入的各种元素。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — 经过用于辅助预训练任务的层进一步处理后，序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这返回经过线性层和双曲正切激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力头部中的注意力 softmax 后的注意力权重，用于计算自注意力头部中的加权平均值。
- en: The [ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ASTForAudioClassification
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTForAudioClassification
- en: '### `class transformers.ASTForAudioClassification`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTForAudioClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L527)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L527)'
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)）—
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Audio Spectrogram Transformer model with an audio classification head on top
    (a linear layer on top of the pooled output) e.g. for datasets like AudioSet,
    Speech Commands v2.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有音频分类头部的音频频谱变换器模型（在池化输出的顶部有一个线性层），例如用于AudioSet、Speech Commands v2等数据集。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L547)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L547)'
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    — Float values mel features extracted from the raw audio waveform. Raw audio waveform
    can be obtained by loading a `.flac` or `.wav` audio file into an array of type
    `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
    soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, max_length, num_mel_bins)`的`torch.FloatTensor`）—
    从原始音频波形中提取的浮点值mel特征。原始音频波形可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库（`pip
    install soundfile`）。要准备数组为`input_features`，应使用[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)来提取mel特征，填充并转换为`torch.FloatTensor`类型的张量。查看[`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块中选择的头部失效的掩码。掩码值选在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`not masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the audio classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（`torch.LongTensor`，形状为`(batch_size,)`，*可选*）— 用于计算音频分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    and inputs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回） — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`） — 分类（如果`config.num_labels==1`则为回归）得分（在SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
