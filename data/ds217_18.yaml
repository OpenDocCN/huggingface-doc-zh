- en: Stream
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ
- en: 'Original text: [https://huggingface.co/docs/datasets/stream](https://huggingface.co/docs/datasets/stream)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/stream](https://huggingface.co/docs/datasets/stream)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset streaming lets you work with a dataset without downloading it. The
    data is streamed as you iterate over the dataset. This is especially helpful when:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æµå¼ä¼ è¾“ä½¿æ‚¨å¯ä»¥åœ¨ä¸ä¸‹è½½æ•°æ®é›†çš„æƒ…å†µä¸‹ä½¿ç”¨æ•°æ®é›†ã€‚å½“æ‚¨éå†æ•°æ®é›†æ—¶ï¼Œæ•°æ®ä¼šéšä¹‹æµåŠ¨ã€‚è¿™åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ç‰¹åˆ«æœ‰å¸®åŠ©ï¼š
- en: You donâ€™t want to wait for an extremely large dataset to download.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨ä¸æƒ³ç­‰å¾…æå¤§çš„æ•°æ®é›†ä¸‹è½½ã€‚
- en: The dataset size exceeds the amount of available disk space on your computer.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å¤§å°è¶…è¿‡è®¡ç®—æœºä¸Šå¯ç”¨ç£ç›˜ç©ºé—´çš„é‡ã€‚
- en: You want to quickly explore just a few samples of a dataset.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨æƒ³å¿«é€Ÿæ¢ç´¢æ•°æ®é›†çš„ä¸€äº›æ ·æœ¬ã€‚
- en: '![](../Images/3a852aa4a279bb2476adf697aa4b6865.png) ![](../Images/a71d3ad700730cf461b4f099cd4a8c4b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a852aa4a279bb2476adf697aa4b6865.png) ![](../Images/a71d3ad700730cf461b4f099cd4a8c4b.png)'
- en: 'For example, the English split of the [oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)
    dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream
    a dataset by setting `streaming=True` in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    as shown below:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ[oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)
    æ•°æ®é›†çš„è‹±æ–‡éƒ¨åˆ†ä¸º1.2TBï¼Œä½†æ‚¨å¯ä»¥ç«‹å³ä½¿ç”¨æµå¼ä¼ è¾“ã€‚é€šè¿‡åœ¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)ä¸­è®¾ç½®`streaming=True`æ¥æµå¼ä¼ è¾“æ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Dataset streaming also lets you work with a dataset made of local files without
    doing any conversion. In this case, the data is streamed from the local files
    as you iterate over the dataset. This is especially helpful when:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æµå¼ä¼ è¾“è¿˜ä½¿æ‚¨å¯ä»¥å¤„ç†ç”±æœ¬åœ°æ–‡ä»¶ç»„æˆçš„æ•°æ®é›†ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•è½¬æ¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“æ‚¨éå†æ•°æ®é›†æ—¶ï¼Œæ•°æ®å°†ä»æœ¬åœ°æ–‡ä»¶æµåŠ¨ã€‚è¿™åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ç‰¹åˆ«æœ‰å¸®åŠ©ï¼š
- en: You donâ€™t want to wait for an extremely large local dataset to be converted
    to Arrow.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨ä¸æƒ³ç­‰å¾…æå¤§çš„æœ¬åœ°æ•°æ®é›†è½¬æ¢ä¸ºArrowã€‚
- en: The converted files size would exceed the amount of available disk space on
    your computer.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è½¬æ¢åçš„æ–‡ä»¶å¤§å°å°†è¶…è¿‡è®¡ç®—æœºä¸Šå¯ç”¨ç£ç›˜ç©ºé—´çš„é‡ã€‚
- en: You want to quickly explore just a few samples of a dataset.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨æƒ³å¿«é€Ÿæ¢ç´¢æ•°æ®é›†çš„ä¸€äº›æ ·æœ¬ã€‚
- en: 'For example, you can stream a local dataset of hundreds of compressed JSONL
    files like [oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)
    to use it instantly:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥é€šè¿‡æµå¼ä¼ è¾“æœ¬åœ°åŒ…å«æ•°ç™¾ä¸ªå‹ç¼©çš„JSONLæ–‡ä»¶çš„æ•°æ®é›†ï¼Œå¦‚[oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)ï¼š
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading a dataset in streaming mode creates a new dataset type instance (instead
    of the classic [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object), known as an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset).
    This special type of dataset has its own set of processing methods shown below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥æµå¼ä¼ è¾“æ¨¡å¼åŠ è½½æ•°æ®é›†ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ç±»å‹å®ä¾‹ï¼ˆè€Œä¸æ˜¯ç»å…¸çš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ï¼‰ï¼Œç§°ä¸º[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)ã€‚è¿™ç§ç‰¹æ®Šç±»å‹çš„æ•°æ®é›†å…·æœ‰è‡ªå·±çš„ä¸€ç»„å¤„ç†æ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: An [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    is useful for iterative jobs like training a model. You shouldnâ€™t use a [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    for jobs that require random access to examples because you have to iterate all
    over it using a for loop. Getting the last example in an iterable dataset would
    require you to iterate over all the previous examples. You can find more details
    in the [Dataset vs. IterableDataset guide](./about_mapstyle_vs_iterable).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    å¯¹äºåƒè®­ç»ƒæ¨¡å‹è¿™æ ·çš„è¿­ä»£ä½œä¸šéå¸¸æœ‰ç”¨ã€‚æ‚¨ä¸åº”è¯¥å°†[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)ç”¨äºéœ€è¦éšæœºè®¿é—®ç¤ºä¾‹çš„ä½œä¸šï¼Œå› ä¸ºæ‚¨å¿…é¡»ä½¿ç”¨forå¾ªç¯éå†æ•´ä¸ªæ•°æ®é›†ã€‚è·å–å¯è¿­ä»£æ•°æ®é›†ä¸­çš„æœ€åä¸€ä¸ªç¤ºä¾‹éœ€è¦æ‚¨éå†æ‰€æœ‰å…ˆå‰çš„ç¤ºä¾‹ã€‚æ‚¨å¯ä»¥åœ¨[æ•°æ®é›†ä¸IterableDatasetæŒ‡å—](./about_mapstyle_vs_iterable)ä¸­æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚'
- en: Convert from a Dataset
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»æ•°æ®é›†è½¬æ¢
- en: If you have an existing [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object, you can convert it to an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    with the [to_iterable_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset)
    function. This is actually faster than setting the `streaming=True` argument in
    [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    because the data is streamed from local files.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰ç°æœ‰çš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ï¼Œå¯ä»¥ä½¿ç”¨[to_iterable_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset)å‡½æ•°å°†å…¶è½¬æ¢ä¸º[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)ã€‚è¿™å®é™…ä¸Šæ¯”åœ¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)ä¸­è®¾ç½®`streaming=True`å‚æ•°æ›´å¿«ï¼Œå› ä¸ºæ•°æ®æ˜¯ä»æœ¬åœ°æ–‡ä»¶æµå¼ä¼ è¾“çš„ã€‚
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The [to_iterable_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset)
    function supports sharding when the [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    is instantiated. This is useful when working with big datasets, and youâ€™d like
    to shuffle the dataset or to enable fast parallel loading with a PyTorch DataLoader.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[to_iterable_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset)å‡½æ•°åœ¨å®ä¾‹åŒ–[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)æ—¶æ”¯æŒåˆ†ç‰‡ã€‚å½“å¤„ç†å¤§å‹æ•°æ®é›†å¹¶ä¸”å¸Œæœ›å¯¹æ•°æ®é›†è¿›è¡Œæ´—ç‰Œæˆ–ä½¿ç”¨PyTorch
    DataLoaderè¿›è¡Œå¿«é€Ÿå¹¶è¡ŒåŠ è½½æ—¶ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚'
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Shuffle
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ´—ç‰Œ
- en: Like a regular [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object, you can also shuffle a [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    with [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¸¸è§„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ä¸€æ ·ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)å¯¹[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)è¿›è¡Œæ´—ç‰Œã€‚
- en: The `buffer_size` argument controls the size of the buffer to randomly sample
    examples from. Letâ€™s say your dataset has one million examples, and you set the
    `buffer_size` to ten thousand. [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)
    will randomly select examples from the first ten thousand examples in the buffer.
    Selected examples in the buffer are replaced with new examples. By default, the
    buffer size is 1,000.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`buffer_size`å‚æ•°æ§åˆ¶ä»ä¸­éšæœºæŠ½æ ·ç¤ºä¾‹çš„ç¼“å†²åŒºçš„å¤§å°ã€‚å‡è®¾æ‚¨çš„æ•°æ®é›†æœ‰ä¸€ç™¾ä¸‡ä¸ªç¤ºä¾‹ï¼Œå¹¶ä¸”æ‚¨å°†`buffer_size`è®¾ç½®ä¸ºä¸€ä¸‡ã€‚[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)å°†ä»ç¼“å†²åŒºä¸­çš„å‰ä¸€ä¸‡ä¸ªç¤ºä¾‹ä¸­éšæœºé€‰æ‹©ç¤ºä¾‹ã€‚ç¼“å†²åŒºä¸­çš„é€‰å®šç¤ºä¾‹å°†è¢«æ–°ç¤ºä¾‹æ›¿æ¢ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œç¼“å†²åŒºå¤§å°ä¸º1,000ã€‚'
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)
    will also shuffle the order of the shards if the dataset is sharded into multiple
    files.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)è¿˜å°†å¯¹åˆ†ç‰‡çš„é¡ºåºè¿›è¡Œæ´—ç‰Œï¼Œå¦‚æœæ•°æ®é›†è¢«åˆ†ç‰‡åˆ°å¤šä¸ªæ–‡ä»¶ä¸­ã€‚'
- en: Reshuffle
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡æ–°æ´—ç‰Œ
- en: Sometimes you may want to reshuffle the dataset after each epoch. This will
    require you to set a different seed for each epoch. Use `IterableDataset.set_epoch()`
    in between epochs to tell the dataset what epoch youâ€™re on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶æ‚¨å¯èƒ½å¸Œæœ›åœ¨æ¯ä¸ªæ—¶æœŸä¹‹åé‡æ–°æ´—ç‰Œæ•°æ®é›†ã€‚è¿™å°†éœ€è¦æ‚¨ä¸ºæ¯ä¸ªæ—¶æœŸè®¾ç½®ä¸åŒçš„ç§å­ã€‚åœ¨æ—¶æœŸä¹‹é—´ä½¿ç”¨`IterableDataset.set_epoch()`å‘Šè¯‰æ•°æ®é›†æ‚¨åœ¨å“ªä¸ªæ—¶æœŸã€‚
- en: 'Your seed effectively becomes: `initial seed + current epoch`.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„ç§å­å®é™…ä¸Šå˜ä¸ºï¼š`åˆå§‹ç§å­ + å½“å‰æ—¶æœŸ`ã€‚
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Split dataset
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‹†åˆ†æ•°æ®é›†
- en: 'You can split your dataset one of two ways:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼æ‹†åˆ†æ•°æ®é›†ï¼š
- en: '[IterableDataset.take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)
    returns the first `n` examples in a dataset:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IterableDataset.take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)è¿”å›æ•°æ®é›†ä¸­çš„å‰`n`ä¸ªç¤ºä¾‹ï¼š'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[IterableDataset.skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)
    omits the first `n` examples in a dataset and returns the remaining examples:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IterableDataset.skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)çœç•¥æ•°æ®é›†ä¸­çš„å‰`n`ä¸ªç¤ºä¾‹ï¼Œå¹¶è¿”å›å‰©ä½™çš„ç¤ºä¾‹ï¼š'
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`take` and `skip` prevent future calls to `shuffle` because they lock in the
    order of the shards. You should `shuffle` your dataset before splitting it.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`take`å’Œ`skip`é˜»æ­¢å¯¹`shuffle`çš„æœªæ¥è°ƒç”¨ï¼Œå› ä¸ºå®ƒä»¬é”å®šäº†åˆ†ç‰‡çš„é¡ºåºã€‚åœ¨æ‹†åˆ†æ•°æ®é›†ä¹‹å‰ï¼Œåº”è¯¥å¯¹æ•°æ®é›†è¿›è¡Œ`shuffle`ã€‚'
- en: Interleave
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº¤é”™
- en: '[interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)
    can combine an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    with other datasets. The combined dataset returns alternating examples from each
    of the original datasets.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)å¯ä»¥å°†[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)ä¸å…¶ä»–æ•°æ®é›†ç»„åˆã€‚ç»„åˆæ•°æ®é›†è¿”å›æ¯ä¸ªåŸå§‹æ•°æ®é›†çš„äº¤æ›¿ç¤ºä¾‹ã€‚'
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define sampling probabilities from each of the original datasets for more control
    over how each of them are sampled and combined. Set the `probabilities` argument
    with your desired sampling probabilities:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¯ä¸ªåŸå§‹æ•°æ®é›†å®šä¹‰é‡‡æ ·æ¦‚ç‡ï¼Œä»¥æ›´å¥½åœ°æ§åˆ¶å¦‚ä½•å¯¹å®ƒä»¬è¿›è¡Œé‡‡æ ·å’Œç»„åˆã€‚ä½¿ç”¨æ‚¨æœŸæœ›çš„é‡‡æ ·æ¦‚ç‡è®¾ç½®`probabilities`å‚æ•°ï¼š
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Around 80% of the final dataset is made of the `en_dataset`, and 20% of the
    `fr_dataset`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ•°æ®é›†çš„çº¦80%ç”±`en_dataset`ç»„æˆï¼Œ20%ç”±`fr_dataset`ç»„æˆã€‚
- en: You can also specify the `stopping_strategy`. The default strategy, `first_exhausted`,
    is a subsampling strategy, i.e the dataset construction is stopped as soon one
    of the dataset runs out of samples. You can specify `stopping_strategy=all_exhausted`
    to execute an oversampling strategy. In this case, the dataset construction is
    stopped as soon as every samples in every dataset has been added at least once.
    In practice, it means that if a dataset is exhausted, it will return to the beginning
    of this dataset until the stop criterion has been reached. Note that if no sampling
    probabilities are specified, the new dataset will have `max_length_datasets*nb_dataset
    samples`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥æŒ‡å®š`stopping_strategy`ã€‚é»˜è®¤ç­–ç•¥`first_exhausted`æ˜¯ä¸€ç§å­é‡‡æ ·ç­–ç•¥ï¼Œå³æ•°æ®é›†æ„å»ºåœ¨å…¶ä¸­ä¸€ä¸ªæ•°æ®é›†è€—å°½æ ·æœ¬æ—¶åœæ­¢ã€‚æ‚¨å¯ä»¥æŒ‡å®š`stopping_strategy=all_exhausted`æ¥æ‰§è¡Œä¸€ç§è¿‡é‡‡æ ·ç­–ç•¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°æ®é›†æ„å»ºå°†åœ¨æ¯ä¸ªæ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬è‡³å°‘æ·»åŠ ä¸€æ¬¡ååœæ­¢ã€‚å®é™…ä¸Šï¼Œè¿™æ„å‘³ç€å¦‚æœä¸€ä¸ªæ•°æ®é›†è€—å°½ï¼Œå®ƒå°†è¿”å›åˆ°è¯¥æ•°æ®é›†çš„å¼€å¤´ï¼Œç›´åˆ°è¾¾åˆ°åœæ­¢æ ‡å‡†ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæœªæŒ‡å®šé‡‡æ ·æ¦‚ç‡ï¼Œåˆ™æ–°æ•°æ®é›†å°†å…·æœ‰`max_length_datasets*nb_dataset`ä¸ªæ ·æœ¬ã€‚
- en: Rename, remove, and cast
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡å‘½åã€åˆ é™¤å’Œè½¬æ¢
- en: The following methods allow you to modify the columns of a dataset. These methods
    are useful for renaming or removing columns and changing columns to a new set
    of features.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ–¹æ³•å…è®¸æ‚¨ä¿®æ”¹æ•°æ®é›†çš„åˆ—ã€‚è¿™äº›æ–¹æ³•å¯¹äºé‡å‘½åæˆ–åˆ é™¤åˆ—ä»¥åŠå°†åˆ—æ›´æ”¹ä¸ºæ–°çš„ç‰¹å¾é›†éå¸¸æœ‰ç”¨ã€‚
- en: Rename
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é‡å‘½å
- en: Use [IterableDataset.rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.rename_column)
    when you need to rename a column in your dataset. Features associated with the
    original column are actually moved under the new column name, instead of just
    replacing the original column in-place.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨éœ€è¦åœ¨æ•°æ®é›†ä¸­é‡å‘½ååˆ—æ—¶ï¼Œè¯·ä½¿ç”¨[IterableDataset.rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.rename_column)ã€‚ä¸åŸå§‹åˆ—ç›¸å…³çš„ç‰¹å¾å®é™…ä¸Šæ˜¯ç§»åŠ¨åˆ°æ–°åˆ—åä¸‹ï¼Œè€Œä¸ä»…ä»…æ˜¯æ›¿æ¢åŸå§‹åˆ—ã€‚
- en: 'Provide [IterableDataset.rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.rename_column)
    with the name of the original column, and the new column name:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[IterableDataset.rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.rename_column)æä¾›åŸå§‹åˆ—çš„åç§°å’Œæ–°åˆ—åï¼š
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remove
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ é™¤
- en: 'When you need to remove one or more columns, give [IterableDataset.remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.remove_columns)
    the name of the column to remove. Remove more than one column by providing a list
    of column names:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨éœ€è¦åˆ é™¤ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—æ—¶ï¼Œè¯·ç»™[IterableDataset.remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.remove_columns)è¦åˆ é™¤çš„åˆ—çš„åç§°ã€‚é€šè¿‡æä¾›åˆ—åç§°çš„åˆ—è¡¨æ¥åˆ é™¤å¤šä¸ªåˆ—ï¼š
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Cast
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è½¬æ¢
- en: '[IterableDataset.cast()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.cast)
    changes the feature type of one or more columns. This method takes your new `Features`
    as its argument. The following sample code shows how to change the feature types
    of `ClassLabel` and `Value`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[IterableDataset.cast()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.cast)æ›´æ”¹ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—çš„ç‰¹å¾ç±»å‹ã€‚æ­¤æ–¹æ³•å°†æ‚¨çš„æ–°`Features`ä½œä¸ºå…¶å‚æ•°ã€‚ä»¥ä¸‹ç¤ºä¾‹ä»£ç æ˜¾ç¤ºäº†å¦‚ä½•æ›´æ”¹`ClassLabel`å’Œ`Value`çš„ç‰¹å¾ç±»å‹ï¼š'
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Casting only works if the original feature type and new feature type are compatible.
    For example, you can cast a column with the feature type `Value('int32')` to `Value('bool')`
    if the original column only contains ones and zeros.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…å½“åŸå§‹ç‰¹å¾ç±»å‹å’Œæ–°ç‰¹å¾ç±»å‹å…¼å®¹æ—¶ï¼Œè½¬æ¢æ‰æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœåŸå§‹åˆ—ä»…åŒ…å«1å’Œ0ï¼Œåˆ™å¯ä»¥å°†å…·æœ‰ç‰¹å¾ç±»å‹`Value('int32')`çš„åˆ—è½¬æ¢ä¸º`Value('bool')`ã€‚
- en: 'Use [IterableDataset.cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.cast_column)
    to change the feature type of just one column. Pass the column name and its new
    feature type as arguments:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[IterableDataset.cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.cast_column)æ¥æ›´æ”¹ä¸€ä¸ªåˆ—çš„ç‰¹å¾ç±»å‹ã€‚å°†åˆ—åå’Œå…¶æ–°ç‰¹å¾ç±»å‹ä½œä¸ºå‚æ•°ä¼ é€’ï¼š
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Map
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜ å°„
- en: Similar to the [Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    function for a regular [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset),
    ğŸ¤— Datasets features [IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)
    for processing an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset).
    [IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)
    applies processing on-the-fly when examples are streamed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºå¸¸è§„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)çš„[Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°ï¼ŒğŸ¤—
    DatasetsåŠŸèƒ½[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)ç”¨äºå¤„ç†[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)ã€‚å½“ç¤ºä¾‹è¢«æµå¼ä¼ è¾“æ—¶ï¼Œ[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)ä¼šå®æ—¶åº”ç”¨å¤„ç†ã€‚
- en: It allows you to apply a processing function to each example in a dataset, independently
    or in batches. This function can even create new rows and columns.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå…è®¸æ‚¨å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹åº”ç”¨å¤„ç†å‡½æ•°ï¼Œç‹¬ç«‹æˆ–æ‰¹å¤„ç†ã€‚è¯¥å‡½æ•°ç”šè‡³å¯ä»¥åˆ›å»ºæ–°çš„è¡Œå’Œåˆ—ã€‚
- en: 'The following example demonstrates how to tokenize a [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset).
    The function needs to accept and output a `dict`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•å¯¹[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)è¿›è¡Œæ ‡è®°åŒ–ã€‚è¯¥å‡½æ•°éœ€è¦æ¥å—å¹¶è¾“å‡ºä¸€ä¸ª`dict`ï¼š
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, apply this function to the dataset with [IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä½¿ç”¨[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)å°†æ­¤å‡½æ•°åº”ç”¨äºæ•°æ®é›†ï¼š
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Letâ€™s take a look at another example, except this time, you will remove a column
    with [IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map).
    When you remove a column, it is only removed after the example has been provided
    to the mapped function. This allows the mapped function to use the content of
    the columns before they are removed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹å¦ä¸€ä¸ªä¾‹å­ï¼Œé™¤äº†è¿™æ¬¡æ‚¨å°†ä½¿ç”¨[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)åˆ é™¤ä¸€åˆ—ã€‚å½“åˆ é™¤åˆ—æ—¶ï¼Œä»…åœ¨å°†ç¤ºä¾‹æä¾›ç»™æ˜ å°„å‡½æ•°åæ‰ä¼šåˆ é™¤ã€‚è¿™ä½¿å¾—æ˜ å°„å‡½æ•°å¯ä»¥åœ¨åˆ é™¤ä¹‹å‰ä½¿ç”¨åˆ—çš„å†…å®¹ã€‚
- en: 'Specify the column to remove with the `remove_columns` argument in [IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)ä¸­çš„`remove_columns`å‚æ•°æŒ‡å®šè¦åˆ é™¤çš„åˆ—ï¼š
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Batch processing
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†
- en: '[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)
    also supports working with batches of examples. Operate on batches by setting
    `batched=True`. The default batch size is 1000, but you can adjust it with the
    `batch_size` argument. This opens the door to many interesting applications such
    as tokenization, splitting long sentences into shorter chunks, and data augmentation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)è¿˜æ”¯æŒå¯¹ç¤ºä¾‹æ‰¹æ¬¡è¿›è¡Œæ“ä½œã€‚é€šè¿‡è®¾ç½®`batched=True`æ¥æ“ä½œæ‰¹æ¬¡ã€‚é»˜è®¤æ‰¹å¤„ç†å¤§å°ä¸º1000ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨`batch_size`å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚è¿™ä¸ºè®¸å¤šæœ‰è¶£çš„åº”ç”¨æ‰“å¼€äº†å¤§é—¨ï¼Œä¾‹å¦‚æ ‡è®°åŒ–ã€å°†é•¿å¥å­æ‹†åˆ†ä¸ºè¾ƒçŸ­çš„å—ä»¥åŠæ•°æ®å¢å¼ºã€‚'
- en: Tokenization
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: See other examples of batch processing in the [batched map processing](./process#batch-processing)
    documentation. They work the same for iterable datasets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[æ‰¹å¤„ç†æ˜ å°„å¤„ç†](./process#batch-processing)æ–‡æ¡£ä¸­çš„å…¶ä»–æ‰¹å¤„ç†ç¤ºä¾‹ã€‚å®ƒä»¬å¯¹å¯è¿­ä»£æ•°æ®é›†çš„å·¥ä½œæ–¹å¼ç›¸åŒã€‚
- en: Filter
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿‡æ»¤
- en: 'You can filter rows in the dataset based on a predicate function using [Dataset.filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter).
    It returns rows that match a specified condition:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨[Dataset.filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)åŸºäºè°“è¯å‡½æ•°è¿‡æ»¤æ•°æ®é›†ä¸­çš„è¡Œã€‚å®ƒè¿”å›ç¬¦åˆæŒ‡å®šæ¡ä»¶çš„è¡Œï¼š
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Dataset.filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)
    can also filter by indices if you set `with_indices=True`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset.filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)è¿˜å¯ä»¥é€šè¿‡è®¾ç½®`with_indices=True`æŒ‰ç´¢å¼•è¿›è¡Œè¿‡æ»¤ï¼š'
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Stream in a training loop
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯ä¸­æµå¼ä¼ è¾“
- en: '[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    can be integrated into a training loop. First, shuffle the dataset:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)å¯ä»¥é›†æˆåˆ°è®­ç»ƒå¾ªç¯ä¸­ã€‚é¦–å…ˆï¼Œå¯¹æ•°æ®é›†è¿›è¡Œæ´—ç‰Œï¼š'
- en: PytorchHide Pytorch content
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè— Pytorchå†…å®¹
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Lastly, create a simple training loop and start training:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„è®­ç»ƒå¾ªç¯å¹¶å¼€å§‹è®­ç»ƒï¼š
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
