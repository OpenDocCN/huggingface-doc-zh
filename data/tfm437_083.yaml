- en: XLA Integration for TensorFlow Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlowæ¨¡å‹çš„XLAé›†æˆ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla](https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla](https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Accelerated Linear Algebra, dubbed XLA, is a compiler for accelerating the
    runtime of TensorFlow Models. From the [official documentation](https://www.tensorflow.org/xla):'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ é€Ÿçº¿æ€§ä»£æ•°ï¼Œç®€ç§°XLAï¼Œæ˜¯ç”¨äºåŠ é€ŸTensorFlowæ¨¡å‹è¿è¡Œæ—¶çš„ç¼–è¯‘å™¨ã€‚æ¥è‡ª[å®˜æ–¹æ–‡æ¡£](https://www.tensorflow.org/xla)ï¼š
- en: XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra
    that can accelerate TensorFlow models with potentially no source code changes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: XLAï¼ˆåŠ é€Ÿçº¿æ€§ä»£æ•°ï¼‰æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºçº¿æ€§ä»£æ•°çš„ç¼–è¯‘å™¨ï¼Œå¯ä»¥åŠ é€ŸTensorFlowæ¨¡å‹ï¼Œå¯èƒ½ä¸éœ€è¦æºä»£ç æ›´æ”¹ã€‚
- en: Using XLA in TensorFlow is simple â€“ it comes packaged inside the `tensorflow`
    library, and it can be triggered with the `jit_compile` argument in any graph-creating
    function such as [`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs).
    When using Keras methods like `fit()` and `predict()`, you can enable XLA simply
    by passing the `jit_compile` argument to `model.compile()`. However, XLA is not
    limited to these methods - it can also be used to accelerate any arbitrary `tf.function`.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨TensorFlowä¸­ä½¿ç”¨XLAå¾ˆç®€å• - å®ƒå·²ç»æ‰“åŒ…åœ¨`tensorflow`åº“ä¸­ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä»»ä½•åˆ›å»ºå›¾å½¢å‡½æ•°ï¼ˆä¾‹å¦‚[`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)ï¼‰ä¸­çš„`jit_compile`å‚æ•°è§¦å‘ã€‚å½“ä½¿ç”¨Kerasæ–¹æ³•å¦‚`fit()`å’Œ`predict()`æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†`jit_compile`å‚æ•°ä¼ é€’ç»™`model.compile()`æ¥ç®€å•å¯ç”¨XLAã€‚ä½†æ˜¯ï¼ŒXLAä¸ä»…é™äºè¿™äº›æ–¹æ³•
    - å®ƒè¿˜å¯ä»¥ç”¨äºåŠ é€Ÿä»»ä½•ä»»æ„çš„`tf.function`ã€‚
- en: Several TensorFlow methods in ğŸ¤— Transformers have been rewritten to be XLA-compatible,
    including text generation for models such as [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2),
    [T5](https://huggingface.co/docs/transformers/model_doc/t5) and [OPT](https://huggingface.co/docs/transformers/model_doc/opt),
    as well as speech processing for models such as [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersä¸­çš„å‡ ä¸ªTensorFlowæ–¹æ³•å·²ç»é‡å†™ä¸ºä¸XLAå…¼å®¹ï¼ŒåŒ…æ‹¬ç”¨äºæ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆï¼Œå¦‚[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ã€[T5](https://huggingface.co/docs/transformers/model_doc/t5)å’Œ[OPT](https://huggingface.co/docs/transformers/model_doc/opt)ï¼Œä»¥åŠç”¨äºè¯­éŸ³å¤„ç†çš„æ¨¡å‹ï¼Œå¦‚[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)ã€‚
- en: While the exact amount of speed-up is very much model-dependent, for TensorFlow
    text generation models inside ğŸ¤— Transformers, we noticed a speed-up of ~100x.
    This document will explain how you can use XLA for these models to get the maximum
    amount of performance. Weâ€™ll also provide links to additional resources if youâ€™re
    interested to learn more about the benchmarks and our design philosophy behind
    the XLA integration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Transformerså†…éƒ¨çš„TensorFlowæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ï¼ŒåŠ é€Ÿçš„ç¡®åˆ‡æ•°é‡éå¸¸ä¾èµ–äºæ¨¡å‹ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°é€Ÿåº¦æå‡äº†çº¦100å€ã€‚æœ¬æ–‡å°†è§£é‡Šå¦‚ä½•åœ¨è¿™äº›æ¨¡å‹ä¸­ä½¿ç”¨XLAæ¥è·å¾—æœ€å¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å°†æä¾›é¢å¤–èµ„æºçš„é“¾æ¥ï¼Œå¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šå…³äºåŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬åœ¨XLAé›†æˆèƒŒåçš„è®¾è®¡ç†å¿µã€‚
- en: Running TF functions with XLA
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨XLAè¿è¡ŒTFå‡½æ•°
- en: 'Let us consider the following model in TensorFlow:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹TensorFlowæ¨¡å‹ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The above model accepts inputs having a dimension of `(10, )`. We can use the
    model for running a forward pass like so:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°æ¨¡å‹æ¥å—ç»´åº¦ä¸º`(10, )`çš„è¾“å…¥ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹æ¥è¿è¡Œå‰å‘ä¼ é€’ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In order to run the forward pass with an XLA-compiled function, weâ€™d need to
    do:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿ç”¨XLAç¼–è¯‘å‡½æ•°è¿è¡Œå‰å‘ä¼ é€’ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The default `call()` function of the `model` is used for compiling the XLA
    graph. But if thereâ€™s any other model function you want to compile into XLA thatâ€™s
    also possible with:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`çš„é»˜è®¤`call()`å‡½æ•°ç”¨äºç¼–è¯‘XLAå›¾ã€‚ä½†æ˜¯ï¼Œå¦‚æœæœ‰ä»»ä½•å…¶ä»–æ¨¡å‹å‡½æ•°æ‚¨æƒ³è¦ç¼–è¯‘æˆXLAï¼Œä¹Ÿæ˜¯å¯èƒ½çš„ï¼Œä¾‹å¦‚ï¼š'
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running a TF text generation model with XLA from ğŸ¤— Transformers
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Transformersä¸­çš„XLAè¿è¡ŒTFæ–‡æœ¬ç”Ÿæˆæ¨¡å‹
- en: 'To enable XLA-accelerated generation within ğŸ¤— Transformers, you need to have
    a recent version of `transformers` installed. You can install it by running:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨ğŸ¤— Transformerså†…å¯ç”¨XLAåŠ é€Ÿç”Ÿæˆï¼Œæ‚¨éœ€è¦å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`transformers`ã€‚æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…ï¼š
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And then you can run the following code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can notice, enabling XLA on `generate()` is just a single line of code.
    The rest of the code remains unchanged. However, there are a couple of gotchas
    in the above code snippet that are specific to XLA. You need to be aware of those
    to realize the speed-ups that XLA can bring in. We discuss these in the following
    section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨å¯ä»¥æ³¨æ„åˆ°çš„ï¼Œåœ¨`generate()`ä¸Šå¯ç”¨XLAåªæ˜¯ä¸€è¡Œä»£ç ã€‚å…¶ä½™ä»£ç ä¿æŒä¸å˜ã€‚ä½†æ˜¯ï¼Œä¸Šé¢ä»£ç ç‰‡æ®µä¸­æœ‰ä¸€äº›ç‰¹å®šäºXLAçš„æ³¨æ„äº‹é¡¹ã€‚æ‚¨éœ€è¦æ³¨æ„è¿™äº›æ‰èƒ½å®ç°XLAå¸¦æ¥çš„åŠ é€Ÿã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®ºè¿™äº›ã€‚
- en: Gotchas to be aware of
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„äº‹é¡¹
- en: When you are executing an XLA-enabled function (like `xla_generate()` above)
    for the first time, it will internally try to infer the computation graph, which
    is time-consuming. This process is known as [â€œtracingâ€](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨é¦–æ¬¡æ‰§è¡Œå¯ç”¨XLAçš„å‡½æ•°ï¼ˆå¦‚ä¸Šé¢çš„`xla_generate()`ï¼‰æ—¶ï¼Œå®ƒå°†å†…éƒ¨å°è¯•æ¨æ–­è®¡ç®—å›¾ï¼Œè¿™æ˜¯è€—æ—¶çš„ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º[â€œè·Ÿè¸ªâ€](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)ã€‚
- en: You might notice that the generation time is not fast. Successive calls of `xla_generate()`
    (or any other XLA-enabled function) wonâ€™t have to infer the computation graph,
    given the inputs to the function follow the same shape with which the computation
    graph was initially built. While this is not a problem for modalities with fixed
    input shapes (e.g., images), you must pay attention if you are working with variable
    input shape modalities (e.g., text).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ç”Ÿæˆæ—¶é—´ä¸å¤Ÿå¿«ã€‚è¿ç»­è°ƒç”¨`xla_generate()`ï¼ˆæˆ–ä»»ä½•å…¶ä»–å¯ç”¨XLAçš„å‡½æ•°ï¼‰ä¸éœ€è¦æ¨æ–­è®¡ç®—å›¾ï¼Œåªè¦å‡½æ•°çš„è¾“å…¥éµå¾ªæœ€åˆæ„å»ºè®¡ç®—å›¾æ—¶çš„ç›¸åŒå½¢çŠ¶ã€‚è™½ç„¶å¯¹äºå…·æœ‰å›ºå®šè¾“å…¥å½¢çŠ¶çš„æ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒï¼‰è¿™ä¸æ˜¯é—®é¢˜ï¼Œä½†å¦‚æœæ‚¨æ­£åœ¨å¤„ç†å…·æœ‰å¯å˜è¾“å…¥å½¢çŠ¶çš„æ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ï¼‰ï¼Œåˆ™å¿…é¡»æ³¨æ„ã€‚
- en: To ensure `xla_generate()` always operates with the same input shapes, you can
    specify the `padding` arguments when calling the tokenizer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®ä¿`xla_generate()`å§‹ç»ˆä½¿ç”¨ç›¸åŒçš„è¾“å…¥å½¢çŠ¶ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨åˆ†è¯å™¨æ—¶æŒ‡å®š`padding`å‚æ•°ã€‚
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This way, you can ensure that the inputs to `xla_generate()` will always receive
    inputs with the shape it was traced with and thus leading to speed-ups in the
    generation time. You can verify this with the code below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæ‚¨å¯ä»¥ç¡®ä¿`xla_generate()`çš„è¾“å…¥å§‹ç»ˆæ¥æ”¶ä¸å…¶è·Ÿè¸ªæ—¶ç›¸åŒå½¢çŠ¶çš„è¾“å…¥ï¼Œä»è€ŒåŠ å¿«ç”Ÿæˆæ—¶é—´ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç è¿›è¡ŒéªŒè¯ï¼š
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'On a Tesla T4 GPU, you can expect the outputs like so:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Tesla T4 GPUä¸Šï¼Œæ‚¨å¯ä»¥æœŸæœ›è¾“å‡ºå¦‚ä¸‹ï¼š
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first call to `xla_generate()` is time-consuming because of tracing, but
    the successive calls are orders of magnitude faster. Keep in mind that any change
    in the generation options at any point with trigger re-tracing and thus leading
    to slow-downs in the generation time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡è°ƒç”¨`xla_generate()`ç”±äºè·Ÿè¸ªè€Œè€—æ—¶ï¼Œä½†åç»­è°ƒç”¨é€Ÿåº¦å¿«å¾—å¤šã€‚è¯·è®°ä½ï¼Œä»»ä½•æ—¶å€™å¯¹ç”Ÿæˆé€‰é¡¹è¿›è¡Œæ›´æ”¹éƒ½ä¼šè§¦å‘é‡æ–°è·Ÿè¸ªï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆæ—¶é—´å˜æ…¢ã€‚
- en: We didnâ€™t cover all the text generation options ğŸ¤— Transformers provides in this
    document. We encourage you to read the documentation for advanced use cases.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ²¡æœ‰åœ¨æœ¬æ–‡æ¡£ä¸­æ¶µç›–ğŸ¤— Transformersæä¾›çš„æ‰€æœ‰æ–‡æœ¬ç”Ÿæˆé€‰é¡¹ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨é˜…è¯»é«˜çº§ç”¨ä¾‹çš„æ–‡æ¡£ã€‚
- en: Additional Resources
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢å¤–èµ„æº
- en: Here, we leave you with some additional resources if you want to delve deeper
    into XLA in ğŸ¤— Transformers and in general.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œå¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£ğŸ¤— Transformersä¸­çš„XLAå’Œä¸€èˆ¬æƒ…å†µä¸‹çš„XLAï¼Œæˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†ä¸€äº›é¢å¤–èµ„æºã€‚
- en: '[This Colab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb)
    provides an interactive demonstration if you want to fiddle with the XLA-compatible
    encoder-decoder (like [T5](https://huggingface.co/docs/transformers/model_doc/t5))
    and decoder-only (like [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2))
    text generation models.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¿™ä¸ªColabç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb)æä¾›äº†ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºï¼Œå¦‚æœæ‚¨æƒ³è¦å°è¯•XLAå…¼å®¹çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆå¦‚[T5](https://huggingface.co/docs/transformers/model_doc/t5)ï¼‰å’Œä»…è§£ç å™¨ï¼ˆå¦‚[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ï¼‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚'
- en: '[This blog post](https://huggingface.co/blog/tf-xla-generate) provides an overview
    of the comparison benchmarks for XLA-compatible models along with a friendly introduction
    to XLA in TensorFlow.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/tf-xla-generate)æä¾›äº†XLAå…¼å®¹æ¨¡å‹çš„æ¯”è¾ƒåŸºå‡†æ¦‚è¿°ï¼Œä»¥åŠå¯¹TensorFlowä¸­XLAçš„å‹å¥½ä»‹ç»ã€‚'
- en: '[This blog post](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)
    discusses our design philosophy behind adding XLA support to the TensorFlow models
    in ğŸ¤— Transformers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¿™ç¯‡åšå®¢æ–‡ç« ](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)è®¨è®ºäº†æˆ‘ä»¬åœ¨ğŸ¤—
    Transformersä¸­ä¸ºTensorFlowæ¨¡å‹æ·»åŠ XLAæ”¯æŒçš„è®¾è®¡ç†å¿µã€‚'
- en: 'Recommended posts for learning more about XLA and TensorFlow graphs in general:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ æ›´å¤šå…³äºXLAå’ŒTensorFlowå›¾çš„æ¨èå¸–å­ï¼š
- en: '[XLA: Optimizing Compiler for Machine Learning](https://www.tensorflow.org/xla)'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XLAï¼šç”¨äºæœºå™¨å­¦ä¹ çš„ä¼˜åŒ–ç¼–è¯‘å™¨](https://www.tensorflow.org/xla)'
- en: '[Introduction to graphs and tf.function](https://www.tensorflow.org/guide/intro_to_graphs)'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾å½¢å’Œtf.functionç®€ä»‹](https://www.tensorflow.org/guide/intro_to_graphs)'
- en: '[Better performance with tf.function](https://www.tensorflow.org/guide/function)'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨tf.functionè·å¾—æ›´å¥½çš„æ€§èƒ½](https://www.tensorflow.org/guide/function)'
