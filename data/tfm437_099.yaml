- en: Attention mechanisms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意机制
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/attention](https://huggingface.co/docs/transformers/v4.37.2/en/attention)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/attention](https://huggingface.co/docs/transformers/v4.37.2/en/attention)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Most transformer models use full attention in the sense that the attention matrix
    is square. It can be a big computational bottleneck when you have long texts.
    Longformer and reformer are models that try to be more efficient and use a sparse
    version of the attention matrix to speed up training.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 transformer 模型在注意力矩阵是方形的意义上使用全注意力。当处理长文本时，这可能是一个巨大的计算瓶颈。Longformer 和 reformer
    是试图更高效并使用注意力矩阵的稀疏版本来加速训练的模型。
- en: LSH attention
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSH 注意力
- en: '[Reformer](#reformer) uses LSH attention. In the softmax(QK^t), only the biggest
    elements (in the softmax dimension) of the matrix QK^t are going to give useful
    contributions. So for each query q in Q, we can consider only the keys k in K
    that are close to q. A hash function is used to determine if q and k are close.
    The attention mask is modified to mask the current token (except at the first
    position), because it will give a query and a key equal (so very similar to each
    other). Since the hash can be a bit random, several hash functions are used in
    practice (determined by a n_rounds parameter) and then are averaged together.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[Reformer](#reformer) 使用 LSH 注意力。在 softmax(QK^t) 中，只有矩阵 QK^t 中最大的元素（在 softmax
    维度上）才会提供有用的贡献。因此，对于 Q 中的每个查询 q，我们只考虑与 q 接近的 K 中的键 k。使用哈希函数来确定 q 和 k 是否接近。注意力掩码被修改为掩盖当前标记（除了第一个位置），因为它会给出一个相等的查询和键（因此非常相似）。由于哈希可能有点随机，实践中使用了几个哈希函数（由
    n_rounds 参数确定），然后对它们进行平均。'
- en: Local attention
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地注意力
- en: '[Longformer](#longformer) uses local attention: often, the local context (e.g.,
    what are the two tokens to the left and right?) is enough to take action for a
    given token. Also, by stacking attention layers that have a small window, the
    last layer will have a receptive field of more than just the tokens in the window,
    allowing them to build a representation of the whole sentence.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Longformer](#longformer) 使用本地注意力：通常，局部上下文（例如，左右两个标记是什么？）足以为给定标记采取行动。此外，通过堆叠具有小窗口的注意力层，最后一层将具有超出窗口中标记的感受野，使它们能够构建整个句子的表示。'
- en: 'Some preselected input tokens are also given global attention: for those few
    tokens, the attention matrix can access all tokens and this process is symmetric:
    all other tokens have access to those specific tokens (on top of the ones in their
    local window). This is shown in Figure 2d of the paper, see below for a sample
    attention mask:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些预选的输入标记也被给予全局注意力：对于这几个标记，注意力矩阵可以访问所有标记，这个过程是对称的：所有其他标记都可以访问这些特定标记（除了它们本地窗口中的标记）。这在论文的图2d中显示，下面是一个示例注意力掩码：
- en: '![](../Images/1e06365ec5e9b29f09eaa36653e7f0cc.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e06365ec5e9b29f09eaa36653e7f0cc.png)'
- en: Using those attention matrices with less parameters then allows the model to
    have inputs having a bigger sequence length.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些具有更少参数的注意力矩阵使模型能够具有更大的序列长度。
- en: Other tricks
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他技巧
- en: Axial positional encodings
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 轴向位置编码
- en: '[Reformer](#reformer) uses axial positional encodings: in traditional transformer
    models, the positional encoding E is a matrix of size<math><semantics><mrow><mi>l</mi></mrow><annotation
    encoding="application/x-tex">l</annotation></semantics></math>l by<math><semantics><mrow><mi>d</mi></mrow><annotation
    encoding="application/x-tex">d</annotation></semantics></math>d,<math><semantics><mrow><mi>l</mi></mrow><annotation
    encoding="application/x-tex">l</annotation></semantics></math>l being the sequence
    length and<math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math>d
    the dimension of the hidden state. If you have very long texts, this matrix can
    be huge and take way too much space on the GPU. To alleviate that, axial positional
    encodings consist of factorizing that big matrix E in two smaller matrices E1
    and E2, with dimensions<math><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub><mo>×</mo><msub><mi>d</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">l_{1} \times d_{1}</annotation></semantics></math>l1​×d1​
    and<math><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub><mo>×</mo><msub><mi>d</mi><mn>2</mn></msub></mrow><annotation
    encoding="application/x-tex">l_{2} \times d_{2}</annotation></semantics></math>l2​×d2​,
    such that<math><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub><mo>×</mo><msub><mi>l</mi><mn>2</mn></msub><mo>=</mo><mi>l</mi></mrow><annotation
    encoding="application/x-tex">l_{1} \times l_{2} = l</annotation></semantics></math>l1​×l2​=l
    and<math><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>+</mo><msub><mi>d</mi><mn>2</mn></msub><mo>=</mo><mi>d</mi></mrow><annotation
    encoding="application/x-tex">d_{1} + d_{2} = d</annotation></semantics></math>d1​+d2​=d
    (with the product for the lengths, this ends up being way smaller). The embedding
    for time step<math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math>j
    in E is obtained by concatenating the embeddings for timestep<math><semantics><mrow><mi>j</mi><mi
    mathvariant="normal">%</mi><mi>l</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">j
    \% l1</annotation></semantics></math>j%l1 in E1 and<math><semantics><mrow><mi>j</mi><mi
    mathvariant="normal">/</mi><mi mathvariant="normal">/</mi><mi>l</mi><mn>1</mn></mrow><annotation
    encoding="application/x-tex">j // l1</annotation></semantics></math>j//l1 in E2.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[Reformer](#reformer) 使用轴向位置编码：在传统的Transformer模型中，位置编码E是一个大小为<math><semantics><mrow><mi>l</mi></mrow><annotation
    encoding="application/x-tex">l</annotation></semantics></math>l乘以<math><semantics><mrow><mi>d</mi></mrow><annotation
    encoding="application/x-tex">d</annotation></semantics></math>d的矩阵，其中<math><semantics><mrow><mi>l</mi></mrow><annotation
    encoding="application/x-tex">l</annotation></semantics></math>l是序列长度，<math><semantics><mrow><mi>d</mi></mrow><annotation
    encoding="application/x-tex">d</annotation></semantics></math>d是隐藏状态的维度。如果您有非常长的文本，这个矩阵可能会非常庞大，在GPU上占用太多空间。为了缓解这个问题，轴向位置编码包括将这个大矩阵E分解为两个较小的矩阵E1和E2，其维度分别为<math><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub><mo>×</mo><msub><mi>d</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">l_{1} \times d_{1}</annotation></semantics></math>l1​×d1​和<math><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub><mo>×</mo><msub><mi>d</mi><mn>2</mn></mrow><annotation
    encoding="application/x-tex">l_{2} \times d_{2}</annotation></semantics></math>l2​×d2​，使得<math><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub><mo>×</mo><msub><mi>l</mi><mn>2</mn></msub><mo>=</mo><mi>l</mi></mrow><annotation
    encoding="application/x-tex">l_{1} \times l_{2} = l</annotation></semantics></math>l1​×l2​=l和<math><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>+</mo><msub><mi>d</mi><mn>2</mn></mrow><annotation
    encoding="application/x-tex">d_{1} + d_{2} = d</annotation></semantics></math>d1​+d2​=d（长度的乘积使得结果变得更小）。在矩阵E中，时间步<math><semantics><mrow><mi>j</mi></mrow><annotation
    encoding="application/x-tex">j</annotation></semantics></math>j的嵌入是通过将E1中时间步<math><semantics><mrow><mi>j</mi><mi
    mathvariant="normal">%</mi><mi>l</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">j
    \% l1</annotation></semantics></math>j%l1的嵌入和E2中时间步<math><semantics><mrow><mi>j</mi><mi
    mathvariant="normal">/</mi><mi mathvariant="normal">/</mi><mi>l</mi><mn>1</mn></mrow><annotation
    encoding="application/x-tex">j // l1</annotation></semantics></math>j//l1的嵌入进行连接获得的。'
