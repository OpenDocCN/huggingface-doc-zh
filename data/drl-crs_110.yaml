- en: Offline vs. Online Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离线与在线强化学习
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/offline-online](https://huggingface.co/learn/deep-rl-course/unitbonus3/offline-online)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unitbonus3/offline-online](https://huggingface.co/learn/deep-rl-course/unitbonus3/offline-online)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning (RL) is a framework **to build decision-making agents**.
    These agents aim to learn optimal behavior (policy) by interacting with the environment
    through **trial and error and receiving rewards as unique feedback**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（RL）是一个**构建决策代理的框架**。这些代理通过**试错交互并接收奖励作为唯一反馈**来学习最佳行为（策略）。
- en: 'The agent’s goal **is to maximize its cumulative reward**, called return. Because
    RL is based on the *reward hypothesis*: all goals can be described as the **maximization
    of the expected cumulative reward**.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的目标**是最大化其累积奖励**，称为回报。因为强化学习基于*奖励假设*：所有目标都可以描述为**最大化预期累积奖励**。
- en: 'Deep Reinforcement Learning agents **learn with batches of experience**. The
    question is, how do they collect it?:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习代理**通过经验批次学习**。问题是，它们如何收集经验？：
- en: '![Unit bonus 3 thumbnail](../Images/88ecc42f5b64e1f4b4d04c4ba4639ae7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![单元奖励3缩略图](../Images/88ecc42f5b64e1f4b4d04c4ba4639ae7.png)'
- en: A comparison between Reinforcement Learning in an Online and Offline setting,
    figure taken from [this post](https://offline-rl.github.io/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在在线和离线环境中的比较，图表取自[这篇文章](https://offline-rl.github.io/)
- en: 'In *online reinforcement learning*, which is what we’ve learned during this
    course, the agent **gathers data directly**: it collects a batch of experience
    by **interacting with the environment**. Then, it uses this experience immediately
    (or via some replay buffer) to learn from it (update its policy).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*在线强化学习*中，这是我们在本课程中学到的，代理**直接收集数据**：它通过**与环境交互**收集一批经验。然后，它立即使用这些经验（或通过一些重放缓冲区）来学习（更新其策略）。
- en: But this implies that either you **train your agent directly in the real world
    or have a simulator**. If you don’t have one, you need to build it, which can
    be very complex (how to reflect the complex reality of the real world in an environment?),
    expensive, and insecure (if the simulator has flaws that may provide a competitive
    advantage, the agent will exploit them).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这意味着你要么**直接在现实世界中训练你的代理，要么有一个模拟器**。如果你没有一个，你需要构建一个，这可能非常复杂（如何在环境中反映现实世界的复杂现实？），昂贵，并且不安全（如果模拟器存在缺陷可能提供竞争优势，代理将利用它们）。
- en: On the other hand, in *offline reinforcement learning*, the agent only **uses
    data collected from other agents or human demonstrations**. It does **not interact
    with the environment**.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，在*离线强化学习*中，代理只**使用从其他代理或人类演示收集的数据**。它**不与环境交互**。
- en: 'The process is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 流程如下：
- en: '**Create a dataset** using one or more policies and/or human interactions.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个或多个策略和/或人类互动**创建一个数据集**。
- en: Run **offline RL on this dataset** to learn a policy
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个数据集上运行**离线强化学习**以学习一个策略
- en: 'This method has one drawback: the *counterfactual queries problem*. What do
    we do if our agent **decides to do something for which we don’t have the data?**
    For instance, turning right on an intersection but we don’t have this trajectory.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一个缺点：*反事实查询问题*。如果我们的代理**决定做一些我们没有数据的事情**怎么办？例如，在十字路口右转，但我们没有这个轨迹。
- en: There exist some solutions on this topic, but if you want to know more about
    offline reinforcement learning, you can [watch this video](https://www.youtube.com/watch?v=k08N5a0gG0A)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题存在一些解决方案，但如果你想了解更多关于离线强化学习的内容，你可以[观看这个视频](https://www.youtube.com/watch?v=k08N5a0gG0A)
- en: Further reading
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, we recommend you check out the following resources:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，我们建议您查看以下资源：
- en: '[Offline Reinforcement Learning, Talk by Sergei Levine](https://www.youtube.com/watch?v=qgZPZREor5I)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[离线强化学习，Sergei Levine的演讲](https://www.youtube.com/watch?v=qgZPZREor5I)'
- en: '[Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open
    Problems](https://arxiv.org/abs/2005.01643)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[离线强化学习：教程，评论和对开放问题的展望](https://arxiv.org/abs/2005.01643)'
- en: Author
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者
- en: This section was written by [Thomas Simonini](https://twitter.com/ThomasSimonini)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节由[Thomas Simonini](https://twitter.com/ThomasSimonini)撰写
