- en: How to convert a ğŸ¤— Transformers model to TensorFlow?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°†ğŸ¤— Transformersæ¨¡å‹è½¬æ¢ä¸ºTensorFlowï¼Ÿ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/add_tensorflow_model](https://huggingface.co/docs/transformers/v4.37.2/en/add_tensorflow_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/add_tensorflow_model](https://huggingface.co/docs/transformers/v4.37.2/en/add_tensorflow_model)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Having multiple frameworks available to use with ğŸ¤— Transformers gives you flexibility
    to play their strengths when designing your application, but it implies that compatibility
    must be added on a per-model basis. The good news is that adding TensorFlow compatibility
    to an existing model is simpler than [adding a new model from scratch](add_new_model)!
    Whether you wish to have a deeper understanding of large TensorFlow models, make
    a major open-source contribution, or enable TensorFlow for your model of choice,
    this guide is for you.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Transformersä¸­æœ‰å¤šä¸ªå¯ç”¨çš„æ¡†æ¶å¯ä»¥ä½¿ç”¨ï¼Œè¿™ä½¿æ‚¨åœ¨è®¾è®¡åº”ç”¨ç¨‹åºæ—¶å¯ä»¥çµæ´»å‘æŒ¥å…¶ä¼˜åŠ¿ï¼Œä½†è¿™æ„å‘³ç€å¿…é¡»æ ¹æ®æ¯ä¸ªæ¨¡å‹æ·»åŠ å…¼å®¹æ€§ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œå°†TensorFlowå…¼å®¹æ€§æ·»åŠ åˆ°ç°æœ‰æ¨¡å‹æ¯”[ä»å¤´å¼€å§‹æ·»åŠ æ–°æ¨¡å‹](add_new_model)æ›´ç®€å•ï¼æ— è®ºæ‚¨å¸Œæœ›æ›´æ·±å…¥åœ°äº†è§£å¤§å‹TensorFlowæ¨¡å‹ï¼Œåšå‡ºé‡å¤§çš„å¼€æºè´¡çŒ®ï¼Œè¿˜æ˜¯ä¸ºæ‚¨é€‰æ‹©çš„æ¨¡å‹å¯ç”¨TensorFlowï¼Œæœ¬æŒ‡å—éƒ½é€‚åˆæ‚¨ã€‚
- en: This guide empowers you, a member of our community, to contribute TensorFlow
    model weights and/or architectures to be used in ğŸ¤— Transformers, with minimal
    supervision from the Hugging Face team. Writing a new model is no small feat,
    but hopefully this guide will make it less of a rollercoaster ğŸ¢ and more of a
    walk in the park ğŸš¶. Harnessing our collective experiences is absolutely critical
    to make this process increasingly easier, and thus we highly encourage that you
    suggest improvements to this guide!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—æˆæƒæ‚¨ï¼Œæˆ‘ä»¬ç¤¾åŒºçš„ä¸€å‘˜ï¼Œè´¡çŒ®TensorFlowæ¨¡å‹æƒé‡å’Œ/æˆ–æ¶æ„ï¼Œä»¥ä¾›åœ¨ğŸ¤— Transformersä¸­ä½¿ç”¨ï¼Œå‡ ä¹ä¸éœ€è¦Hugging Faceå›¢é˜Ÿçš„ç›‘ç£ã€‚ç¼–å†™ä¸€ä¸ªæ–°æ¨¡å‹å¹¶ä¸æ˜¯ä¸€ä»¶å°äº‹ï¼Œä½†å¸Œæœ›è¿™ä¸ªæŒ‡å—èƒ½è®©å®ƒä¸å†åƒåè¿‡å±±è½¦ğŸ¢é‚£æ ·ï¼Œè€Œæ›´åƒåœ¨å…¬å›­é‡Œæ•£æ­¥ğŸš¶ã€‚åˆ©ç”¨æˆ‘ä»¬çš„é›†ä½“ç»éªŒç»å¯¹æ˜¯ä½¿è¿™ä¸ªè¿‡ç¨‹å˜å¾—æ›´åŠ å®¹æ˜“çš„å…³é”®ï¼Œå› æ­¤æˆ‘ä»¬å¼ºçƒˆé¼“åŠ±æ‚¨å¯¹æœ¬æŒ‡å—æå‡ºæ”¹è¿›å»ºè®®ï¼
- en: 'Before you dive deeper, it is recommended that you check the following resources
    if youâ€™re new to ğŸ¤— Transformers:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥ç ”ç©¶ä¹‹å‰ï¼Œå»ºè®®æ‚¨æŸ¥çœ‹ä»¥ä¸‹èµ„æºï¼Œå¦‚æœæ‚¨å¯¹ğŸ¤— Transformersè¿˜ä¸ç†Ÿæ‚‰ï¼š
- en: '[General overview of ğŸ¤— Transformers](add_new_model#general-overview-of-transformers)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ğŸ¤— Transformersçš„æ¦‚è¿°](add_new_model#general-overview-of-transformers)'
- en: '[Hugging Faceâ€™s TensorFlow Philosophy](https://huggingface.co/blog/tensorflow-philosophy)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ‹¥æŠ±é¢çš„TensorFlowå“²å­¦](https://huggingface.co/blog/tensorflow-philosophy)'
- en: In the remainder of this guide, you will learn whatâ€™s needed to add a new TensorFlow
    model architecture, the procedure to convert PyTorch into TensorFlow model weights,
    and how to efficiently debug mismatches across ML frameworks. Letâ€™s get started!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæ‚¨å°†å­¦ä¹ æ·»åŠ æ–°çš„TensorFlowæ¨¡å‹æ¶æ„æ‰€éœ€çš„å†…å®¹ï¼Œå°†PyTorchè½¬æ¢ä¸ºTensorFlowæ¨¡å‹æƒé‡çš„è¿‡ç¨‹ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°è°ƒè¯•è·¨MLæ¡†æ¶çš„ä¸åŒ¹é…ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: Are you unsure whether the model you wish to use already has a corresponding
    TensorFlow architecture?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨æ˜¯å¦ä¸ç¡®å®šæ‚¨æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹æ˜¯å¦å·²ç»æœ‰ç›¸åº”çš„TensorFlowæ¶æ„ï¼Ÿ
- en: Check the `model_type` field of the `config.json` of your model of choice ([example](https://huggingface.co/bert-base-uncased/blob/main/config.json#L14)).
    If the corresponding model folder in ğŸ¤— Transformers has a file whose name starts
    with â€œmodeling_tfâ€, it means that it has a corresponding TensorFlow architecture
    ([example](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ‚¨é€‰æ‹©çš„æ¨¡å‹çš„`config.json`æ–‡ä»¶ä¸­çš„`model_type`å­—æ®µï¼ˆ[ç¤ºä¾‹](https://huggingface.co/bert-base-uncased/blob/main/config.json#L14)ï¼‰ã€‚å¦‚æœğŸ¤—
    Transformersä¸­ç›¸åº”çš„æ¨¡å‹æ–‡ä»¶å¤¹æœ‰ä¸€ä¸ªä»¥â€œmodeling_tfâ€å¼€å¤´çš„æ–‡ä»¶ï¼Œè¿™æ„å‘³ç€å®ƒæœ‰ä¸€ä¸ªç›¸åº”çš„TensorFlowæ¶æ„ï¼ˆ[ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert)ï¼‰ã€‚
- en: Step-by-step guide to add TensorFlow model architecture code
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€æ­¥æŒ‡å—æ·»åŠ TensorFlowæ¨¡å‹æ¶æ„ä»£ç 
- en: 'There are many ways to design a large model architecture, and multiple ways
    of implementing said design. However, you might recall from our [general overview
    of ğŸ¤— Transformers](add_new_model#general-overview-of-transformers) that we are
    an opinionated bunch - the ease of use of ğŸ¤— Transformers relies on consistent
    design choices. From experience, we can tell you a few important things about
    adding TensorFlow models:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡å¤§å‹æ¨¡å‹æ¶æ„çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œå®ç°è¯¥è®¾è®¡çš„æ–¹å¼ä¹Ÿæœ‰å¤šç§ã€‚ç„¶è€Œï¼Œæ‚¨å¯èƒ½è¿˜è®°å¾—æˆ‘ä»¬åœ¨[ğŸ¤— Transformersçš„æ¦‚è¿°](add_new_model#general-overview-of-transformers)ä¸­æåˆ°ï¼Œæˆ‘ä»¬æ˜¯ä¸€ä¸ªæœ‰ä¸»è§çš„å›¢é˜Ÿ
    - ğŸ¤— Transformersçš„æ˜“ç”¨æ€§ä¾èµ–äºä¸€è‡´çš„è®¾è®¡é€‰æ‹©ã€‚ä»ç»éªŒä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å‘Šè¯‰æ‚¨ä¸€äº›å…³äºæ·»åŠ TensorFlowæ¨¡å‹çš„é‡è¦äº‹é¡¹ï¼š
- en: 'Donâ€™t reinvent the wheel! More often than not, there are at least two reference
    implementations you should check: the PyTorch equivalent of the model you are
    implementing and other TensorFlow models for the same class of problems.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸è¦é‡å¤é€ è½®å­ï¼å¾€å¾€è‡³å°‘æœ‰ä¸¤ä¸ªå‚è€ƒå®ç°æ‚¨åº”è¯¥æ£€æŸ¥ï¼šæ‚¨æ­£åœ¨å®ç°çš„æ¨¡å‹çš„PyTorchç­‰æ•ˆç‰ˆæœ¬ä»¥åŠåŒä¸€ç±»é—®é¢˜çš„å…¶ä»–TensorFlowæ¨¡å‹ã€‚
- en: Great model implementations survive the test of time. This doesnâ€™t happen because
    the code is pretty, but rather because the code is clear, easy to debug and build
    upon. If you make the life of the maintainers easy with your TensorFlow implementation,
    by replicating the same patterns as in other TensorFlow models and minimizing
    the mismatch to the PyTorch implementation, you ensure your contribution will
    be long lived.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡ºè‰²çš„æ¨¡å‹å®ç°ç»å¾—èµ·æ—¶é—´çš„è€ƒéªŒã€‚è¿™ä¸æ˜¯å› ä¸ºä»£ç æ¼‚äº®ï¼Œè€Œæ˜¯å› ä¸ºä»£ç æ¸…æ™°ï¼Œæ˜“äºè°ƒè¯•å’Œæ„å»ºã€‚å¦‚æœæ‚¨é€šè¿‡åœ¨TensorFlowå®ç°ä¸­å¤åˆ¶å…¶ä»–TensorFlowæ¨¡å‹ä¸­çš„ç›¸åŒæ¨¡å¼å¹¶æœ€å°åŒ–ä¸PyTorchå®ç°çš„ä¸åŒ¹é…ï¼Œä½¿ç»´æŠ¤è€…çš„ç”Ÿæ´»å˜å¾—è½»æ¾ï¼Œæ‚¨å°±ç¡®ä¿æ‚¨çš„è´¡çŒ®å°†é•¿æœŸå­˜åœ¨ã€‚
- en: Ask for help when youâ€™re stuck! The ğŸ¤— Transformers team is here to help, and
    weâ€™ve probably found solutions to the same problems youâ€™re facing.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“é‡åˆ°å›°éš¾æ—¶å¯»æ±‚å¸®åŠ©ï¼ğŸ¤— Transformerså›¢é˜Ÿåœ¨è¿™é‡Œå¸®åŠ©æ‚¨ï¼Œæˆ‘ä»¬å¯èƒ½å·²ç»æ‰¾åˆ°äº†æ‚¨é¢ä¸´çš„ç›¸åŒé—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚
- en: 'Hereâ€™s an overview of the steps needed to add a TensorFlow model architecture:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ·»åŠ TensorFlowæ¨¡å‹æ¶æ„æ‰€éœ€æ­¥éª¤çš„æ¦‚è¿°ï¼š
- en: Select the model you wish to convert
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ‚¨å¸Œæœ›è½¬æ¢çš„æ¨¡å‹
- en: Prepare transformers dev environment
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡†å¤‡transformerså¼€å‘ç¯å¢ƒ
- en: (Optional) Understand theoretical aspects and the existing implementation
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ï¼ˆå¯é€‰ï¼‰ç†è§£ç†è®ºæ–¹é¢å’Œç°æœ‰å®ç°
- en: Implement the model architecture
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ç°æ¨¡å‹æ¶æ„
- en: Implement model tests
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ç°æ¨¡å‹æµ‹è¯•
- en: Submit the pull request
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æäº¤æ‹‰å–è¯·æ±‚
- en: (Optional) Build demos and share with the world
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ï¼ˆå¯é€‰ï¼‰æ„å»ºæ¼”ç¤ºå¹¶ä¸ä¸–ç•Œåˆ†äº«
- en: 1.-3\. Prepare your model contribution
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.-3\. å‡†å¤‡æ‚¨çš„æ¨¡å‹è´¡çŒ®
- en: '**1\. Select the model you wish to convert**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\.é€‰æ‹©è¦è½¬æ¢çš„æ¨¡å‹**'
- en: 'Letâ€™s start off with the basics: the first thing you need to know is the architecture
    you want to convert. If you donâ€™t have your eyes set on a specific architecture,
    asking the ğŸ¤— Transformers team for suggestions is a great way to maximize your
    impact - we will guide you towards the most prominent architectures that are missing
    on the TensorFlow side. If the specific model you want to use with TensorFlow
    already has a TensorFlow architecture implementation in ğŸ¤— Transformers but is
    lacking weights, feel free to jump straight into the [weight conversion section](#adding-tensorflow-weights-to-hub)
    of this page.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»åŸºç¡€çŸ¥è¯†å¼€å§‹ï¼šæ‚¨éœ€è¦äº†è§£è¦è½¬æ¢çš„æ¶æ„ã€‚å¦‚æœæ‚¨æ²¡æœ‰ç‰¹å®šçš„æ¶æ„ï¼Œå‘ğŸ¤— Transformerså›¢é˜Ÿå¯»æ±‚å»ºè®®æ˜¯æœ€å¤§åŒ–å½±å“çš„å¥½æ–¹æ³• - æˆ‘ä»¬å°†æŒ‡å¯¼æ‚¨é€‰æ‹©åœ¨TensorFlowæ–¹é¢ç¼ºå¤±çš„æœ€çªå‡ºçš„æ¶æ„ã€‚å¦‚æœæ‚¨æƒ³è¦åœ¨TensorFlowä¸­ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹å·²ç»åœ¨ğŸ¤—
    Transformersä¸­å…·æœ‰TensorFlowæ¶æ„å®ç°ï¼Œä½†ç¼ºå°‘æƒé‡ï¼Œè¯·éšæ—¶ç›´æ¥è½¬åˆ°æœ¬é¡µçš„[æ·»åŠ TensorFlowæƒé‡åˆ°hub](#adding-tensorflow-weights-to-hub)éƒ¨åˆ†ã€‚
- en: For simplicity, the remainder of this guide assumes youâ€™ve decided to contribute
    with the TensorFlow version of *BrandNewBert* (the same example as in the [guide](add_new_model)
    to add a new model from scratch).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç®€å•èµ·è§ï¼Œæœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†å‡å®šæ‚¨å·²å†³å®šä½¿ç”¨TensorFlowç‰ˆæœ¬çš„*BrandNewBert*ï¼ˆä¸[æŒ‡å—](add_new_model)ä¸­æ·»åŠ æ–°æ¨¡å‹çš„ç¤ºä¾‹ç›¸åŒï¼‰åšå‡ºè´¡çŒ®ã€‚
- en: Before starting the work on a TensorFlow model architecture, double-check that
    there is no ongoing effort to do so. You can search for `BrandNewBert` on the
    [pull request GitHub page](https://github.com/huggingface/transformers/pulls?q=is%3Apr)
    to confirm that there is no TensorFlow-related pull request.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹TensorFlowæ¨¡å‹æ¶æ„çš„å·¥ä½œä¹‹å‰ï¼Œè¯·ä»”ç»†æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„å·¥ä½œã€‚æ‚¨å¯ä»¥åœ¨[æ‹‰å–è¯·æ±‚GitHubé¡µé¢](https://github.com/huggingface/transformers/pulls?q=is%3Apr)ä¸Šæœç´¢`BrandNewBert`ä»¥ç¡®è®¤æ˜¯å¦æœ‰ä¸TensorFlowç›¸å…³çš„æ‹‰å–è¯·æ±‚ã€‚
- en: '**2\. Prepare transformers dev environment**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\.å‡†å¤‡transformerså¼€å‘ç¯å¢ƒ**'
- en: Having selected the model architecture, open a draft PR to signal your intention
    to work on it. Follow the instructions below to set up your environment and open
    a draft PR.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ¨¡å‹æ¶æ„åï¼Œæ‰“å¼€ä¸€ä¸ªè‰ç¨¿PRä»¥è¡¨ç¤ºæ‚¨æ‰“ç®—è¿›è¡Œå·¥ä½œã€‚æŒ‰ç…§ä»¥ä¸‹è¯´æ˜è®¾ç½®æ‚¨çš„ç¯å¢ƒå¹¶æ‰“å¼€è‰ç¨¿PRã€‚
- en: Fork the [repository](https://github.com/huggingface/transformers) by clicking
    on the â€˜Forkâ€™ button on the repositoryâ€™s page. This creates a copy of the code
    under your GitHub user account.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡å•å‡»å­˜å‚¨åº“é¡µé¢ä¸Šçš„â€œForkâ€æŒ‰é’®æ¥åˆ†å‰[å­˜å‚¨åº“](https://github.com/huggingface/transformers)ã€‚è¿™å°†åœ¨æ‚¨çš„GitHubç”¨æˆ·å¸æˆ·ä¸‹åˆ›å»ºä»£ç å‰¯æœ¬ã€‚
- en: 'Clone your `transformers` fork to your local disk, and add the base repository
    as a remote:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„`transformers`åˆ†æ”¯å…‹éš†åˆ°æœ¬åœ°ç£ç›˜ï¼Œå¹¶å°†åŸºç¡€å­˜å‚¨åº“æ·»åŠ ä¸ºè¿œç¨‹å­˜å‚¨åº“ï¼š
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Set up a development environment, for instance by running the following command:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å»ºç«‹ä¸€ä¸ªå¼€å‘ç¯å¢ƒï¼Œä¾‹å¦‚é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Depending on your OS, and since the number of optional dependencies of Transformers
    is growing, you might get a failure with this command. If thatâ€™s the case make
    sure to install TensorFlow then do:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ‚¨çš„æ“ä½œç³»ç»Ÿï¼Œç”±äºTransformersçš„å¯é€‰ä¾èµ–é¡¹æ•°é‡æ­£åœ¨å¢åŠ ï¼Œæ‚¨å¯èƒ½ä¼šåœ¨æ­¤å‘½ä»¤ä¸­å¤±è´¥ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œè¯·ç¡®ä¿å®‰è£…TensorFlowï¼Œç„¶åæ‰§è¡Œï¼š
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Note:** You donâ€™t need to have CUDA installed. Making the new model work
    on CPU is sufficient.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š**æ‚¨ä¸éœ€è¦å®‰è£…CUDAã€‚ä½¿æ–°æ¨¡å‹åœ¨CPUä¸Šè¿è¡Œå°±è¶³å¤Ÿäº†ã€‚'
- en: Create a branch with a descriptive name from your main branch
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ä¸»åˆ†æ”¯åˆ›å»ºä¸€ä¸ªå…·æœ‰æè¿°æ€§åç§°çš„åˆ†æ”¯
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Fetch and rebase to current main
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è·å–å¹¶å°†å½“å‰ä¸»åˆ†æ”¯é‡æ–°è®¾ç½®ä¸ºåŸºç¡€
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Add an empty `.py` file in `transformers/src/models/brandnewbert/` named `modeling_tf_brandnewbert.py`.
    This will be your TensorFlow model file.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨`transformers/src/models/brandnewbert/`ä¸­æ·»åŠ ä¸€ä¸ªåä¸º`modeling_tf_brandnewbert.py`çš„ç©º`.py`æ–‡ä»¶ã€‚è¿™å°†æ˜¯æ‚¨çš„TensorFlowæ¨¡å‹æ–‡ä»¶ã€‚
- en: 'Push the changes to your account using:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ›´æ”¹æ¨é€åˆ°æ‚¨çš„å¸æˆ·ï¼š
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once you are satisfied, go to the webpage of your fork on GitHub. Click on â€œPull
    requestâ€. Make sure to add the GitHub handle of some members of the Hugging Face
    team as reviewers, so that the Hugging Face team gets notified for future changes.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨æ»¡æ„äº†ï¼Œè½¬åˆ°GitHubä¸Šæ‚¨çš„åˆ†æ”¯çš„ç½‘é¡µã€‚ç‚¹å‡»â€œæ‹‰å–è¯·æ±‚â€ã€‚ç¡®ä¿å°†Hugging Faceå›¢é˜Ÿçš„ä¸€äº›æˆå‘˜çš„GitHubå¥æŸ„æ·»åŠ ä¸ºå®¡é˜…è€…ï¼Œä»¥ä¾¿Hugging
    Faceå›¢é˜Ÿåœ¨æœªæ¥çš„æ›´æ”¹ä¸­æ”¶åˆ°é€šçŸ¥ã€‚
- en: Change the PR into a draft by clicking on â€œConvert to draftâ€ on the right of
    the GitHub pull request web page.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡å•å‡»GitHubæ‹‰å–è¯·æ±‚ç½‘é¡µå³ä¾§çš„â€œè½¬æ¢ä¸ºè‰ç¨¿â€å°†PRæ›´æ”¹ä¸ºè‰ç¨¿ã€‚
- en: Now you have set up a development environment to port *BrandNewBert* to TensorFlow
    in ğŸ¤— Transformers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»è®¾ç½®äº†ä¸€ä¸ªå¼€å‘ç¯å¢ƒï¼Œå¯ä»¥å°†*BrandNewBert*ç§»æ¤åˆ°ğŸ¤— Transformersä¸­çš„TensorFlowä¸­ã€‚
- en: '**3\. (Optional) Understand theoretical aspects and the existing implementation**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\.ï¼ˆå¯é€‰ï¼‰äº†è§£ç†è®ºæ–¹é¢å’Œç°æœ‰å®ç°**'
- en: You should take some time to read *BrandNewBertâ€™s* paper, if such descriptive
    work exists. There might be large sections of the paper that are difficult to
    understand. If this is the case, this is fine - donâ€™t worry! The goal is not to
    get a deep theoretical understanding of the paper, but to extract the necessary
    information required to effectively re-implement the model in ğŸ¤— Transformers using
    TensorFlow. That being said, you donâ€™t have to spend too much time on the theoretical
    aspects, but rather focus on the practical ones, namely the existing model documentation
    page (e.g. [model docs for BERT](model_doc/bert)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥èŠ±ä¸€äº›æ—¶é—´é˜…è¯»*BrandNewBert*çš„è®ºæ–‡ï¼Œå¦‚æœå­˜åœ¨è¿™æ ·çš„æè¿°æ€§å·¥ä½œã€‚è®ºæ–‡ä¸­å¯èƒ½æœ‰ä¸€äº›éš¾ä»¥ç†è§£çš„å¤§æ®µå†…å®¹ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œæ²¡å…³ç³» - ä¸è¦æ‹…å¿ƒï¼ç›®æ ‡ä¸æ˜¯æ·±å…¥ç†è§£è®ºæ–‡çš„ç†è®ºï¼Œè€Œæ˜¯æå–åœ¨ğŸ¤—
    Transformersä¸­ä½¿ç”¨TensorFlowæœ‰æ•ˆé‡æ–°å®ç°æ¨¡å‹æ‰€éœ€çš„å¿…è¦ä¿¡æ¯ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨ä¸å¿…èŠ±å¤ªå¤šæ—¶é—´åœ¨ç†è®ºæ–¹é¢ï¼Œè€Œæ˜¯è¦ä¸“æ³¨äºå®è·µæ–¹é¢ï¼Œå³ç°æœ‰æ¨¡å‹æ–‡æ¡£é¡µé¢ï¼ˆä¾‹å¦‚[BERTçš„æ¨¡å‹æ–‡æ¡£](model_doc/bert)ï¼‰ã€‚
- en: After youâ€™ve grasped the basics of the models you are about to implement, itâ€™s
    important to understand the existing implementation. This is a great chance to
    confirm that a working implementation matches your expectations for the model,
    as well as to foresee technical challenges on the TensorFlow side.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŒæ¡äº†å³å°†å®ç°çš„æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ä¹‹åï¼Œäº†è§£ç°æœ‰çš„å®ç°æ˜¯å¾ˆé‡è¦çš„ã€‚è¿™æ˜¯ç¡®è®¤å·¥ä½œå®ç°æ˜¯å¦ç¬¦åˆæ‚¨å¯¹æ¨¡å‹çš„æœŸæœ›ï¼Œä»¥åŠé¢„è§TensorFlowæ–¹é¢çš„æŠ€æœ¯æŒ‘æˆ˜çš„ç»ä½³æœºä¼šã€‚
- en: Itâ€™s perfectly natural that you feel overwhelmed with the amount of information
    that youâ€™ve just absorbed. It is definitely not a requirement that you understand
    all facets of the model at this stage. Nevertheless, we highly encourage you to
    clear any pressing questions in our [forum](https://discuss.huggingface.co/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ„Ÿåˆ°ä¸çŸ¥æ‰€æªåˆšåˆšå¸æ”¶äº†å¤§é‡ä¿¡æ¯æ˜¯éå¸¸è‡ªç„¶çš„ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å¹¶ä¸éœ€è¦ç†è§£æ¨¡å‹çš„æ‰€æœ‰æ–¹é¢ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬å¼ºçƒˆé¼“åŠ±ä½ åœ¨æˆ‘ä»¬çš„[è®ºå›](https://discuss.huggingface.co/)ä¸­è§£å†³ä»»ä½•ç´§è¿«çš„é—®é¢˜ã€‚
- en: 4\. Model implementation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. æ¨¡å‹å®ç°
- en: 'Now itâ€™s time to finally start coding. Our suggested starting point is the
    PyTorch file itself: copy the contents of `modeling_brand_new_bert.py` inside
    `src/transformers/models/brand_new_bert/` into `modeling_tf_brand_new_bert.py`.
    The goal of this section is to modify the file and update the import structure
    of ğŸ¤— Transformers such that you can import `TFBrandNewBert` and `TFBrandNewBert.from_pretrained(model_repo,
    from_pt=True)` successfully loads a working TensorFlow *BrandNewBert* model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™å¼€å§‹ç¼–ç äº†ã€‚æˆ‘ä»¬å»ºè®®çš„èµ·ç‚¹æ˜¯PyTorchæ–‡ä»¶æœ¬èº«ï¼šå°†`modeling_brand_new_bert.py`çš„å†…å®¹å¤åˆ¶åˆ°`src/transformers/models/brand_new_bert/`ä¸­çš„`modeling_tf_brand_new_bert.py`ã€‚æœ¬èŠ‚çš„ç›®æ ‡æ˜¯ä¿®æ”¹æ–‡ä»¶å¹¶æ›´æ–°ğŸ¤—
    Transformersçš„å¯¼å…¥ç»“æ„ï¼Œä»¥ä¾¿ä½ å¯ä»¥æˆåŠŸå¯¼å…¥`TFBrandNewBert`å’Œ`TFBrandNewBert.from_pretrained(model_repo,
    from_pt=True)`ï¼Œä»è€ŒåŠ è½½ä¸€ä¸ªå¯å·¥ä½œçš„TensorFlow *BrandNewBert*æ¨¡å‹ã€‚
- en: 'Sadly, there is no prescription to convert a PyTorch model into TensorFlow.
    You can, however, follow our selection of tips to make the process as smooth as
    possible:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é—æ†¾çš„æ˜¯ï¼Œæ²¡æœ‰å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºTensorFlowçš„è§„å®šã€‚ä½†æ˜¯ï¼Œä½ å¯ä»¥éµå¾ªæˆ‘ä»¬çš„ä¸€äº›æç¤ºï¼Œä½¿è¿™ä¸ªè¿‡ç¨‹å°½å¯èƒ½é¡ºåˆ©ï¼š
- en: Prepend `TF` to the name of all classes (e.g. `BrandNewBert` becomes `TFBrandNewBert`).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ç±»çš„åç§°å‰åŠ ä¸Š`TF`ï¼ˆä¾‹å¦‚ï¼Œ`BrandNewBert`å˜ä¸º`TFBrandNewBert`ï¼‰ã€‚
- en: Most PyTorch operations have a direct TensorFlow replacement. For example, `torch.nn.Linear`
    corresponds to `tf.keras.layers.Dense`, `torch.nn.Dropout` corresponds to `tf.keras.layers.Dropout`,
    etc. If youâ€™re not sure about a specific operation, you can use the [TensorFlow
    documentation](https://www.tensorflow.org/api_docs/python/tf) or the [PyTorch
    documentation](https://pytorch.org/docs/stable/).
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°PyTorchæ“ä½œéƒ½æœ‰ç›´æ¥çš„TensorFlowæ›¿ä»£ã€‚ä¾‹å¦‚ï¼Œ`torch.nn.Linear`å¯¹åº”äº`tf.keras.layers.Dense`ï¼Œ`torch.nn.Dropout`å¯¹åº”äº`tf.keras.layers.Dropout`ç­‰ã€‚å¦‚æœå¯¹ç‰¹å®šæ“ä½œä¸ç¡®å®šï¼Œå¯ä»¥ä½¿ç”¨[TensorFlowæ–‡æ¡£](https://www.tensorflow.org/api_docs/python/tf)æˆ–[PyTorchæ–‡æ¡£](https://pytorch.org/docs/stable/)ã€‚
- en: Look for patterns in the ğŸ¤— Transformers codebase. If you come across a certain
    operation that doesnâ€™t have a direct replacement, the odds are that someone else
    already had the same problem.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Transformersä»£ç åº“ä¸­å¯»æ‰¾æ¨¡å¼ã€‚å¦‚æœä½ é‡åˆ°æŸä¸ªæ“ä½œæ²¡æœ‰ç›´æ¥æ›¿ä»£ï¼Œé‚£ä¹ˆå¾ˆå¯èƒ½æœ‰å…¶ä»–äººå·²ç»é‡åˆ°äº†åŒæ ·çš„é—®é¢˜ã€‚
- en: By default, keep the same variable names and structure as in PyTorch. This will
    make it easier to debug, track issues, and add fixes down the line.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œä¿æŒä¸PyTorchç›¸åŒçš„å˜é‡åç§°å’Œç»“æ„ã€‚è¿™å°†ä½¿è°ƒè¯•ã€è·Ÿè¸ªé—®é¢˜å’Œæ·»åŠ ä¿®å¤æ›´å®¹æ˜“ã€‚
- en: Some layers have different default values in each framework. A notable example
    is the batch normalization layerâ€™s epsilon (`1e-5` in [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)
    and `1e-3` in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)).
    Double-check the documentation!
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€äº›å±‚åœ¨æ¯ä¸ªæ¡†æ¶ä¸­å…·æœ‰ä¸åŒçš„é»˜è®¤å€¼ã€‚ä¸€ä¸ªæ˜¾è‘—çš„ä¾‹å­æ˜¯æ‰¹é‡å½’ä¸€åŒ–å±‚çš„epsilonï¼ˆåœ¨[PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)ä¸­ä¸º`1e-5`ï¼Œåœ¨[TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)ä¸­ä¸º`1e-3`ï¼‰ã€‚åŠ¡å¿…ä»”ç»†æ£€æŸ¥æ–‡æ¡£ï¼
- en: 'PyTorchâ€™s `nn.Parameter` variables typically need to be initialized within
    TF Layerâ€™s `build()`. See the following example: [PyTorch](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_vit_mae.py#L212)
    / [TensorFlow](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L220)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorchçš„`nn.Parameter`å˜é‡é€šå¸¸éœ€è¦åœ¨TF Layerçš„`build()`ä¸­åˆå§‹åŒ–ã€‚å‚è§ä»¥ä¸‹ç¤ºä¾‹ï¼š[PyTorch](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_vit_mae.py#L212)
    / [TensorFlow](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L220)
- en: If the PyTorch model has a `#copied from ...` on top of a function, the odds
    are that your TensorFlow model can also borrow that function from the architecture
    it was copied from, assuming it has a TensorFlow architecture.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœPyTorchæ¨¡å‹åœ¨å‡½æ•°é¡¶éƒ¨æœ‰`#copied from ...`ï¼Œé‚£ä¹ˆä½ çš„TensorFlowæ¨¡å‹å¾ˆå¯èƒ½ä¹Ÿå¯ä»¥ä»è¢«å¤åˆ¶çš„æ¶æ„ä¸­å€Ÿç”¨è¯¥å‡½æ•°ï¼Œå‡è®¾å®ƒæœ‰ä¸€ä¸ªTensorFlowæ¶æ„ã€‚
- en: Assigning the `name` attribute correctly in TensorFlow functions is critical
    to do the `from_pt=True` weight cross-loading. `name` is almost always the name
    of the corresponding variable in the PyTorch code. If `name` is not properly set,
    you will see it in the error message when loading the model weights.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨TensorFlowå‡½æ•°ä¸­æ­£ç¡®è®¾ç½®`name`å±æ€§å¯¹äºè¿›è¡Œ`from_pt=True`æƒé‡äº¤å‰åŠ è½½è‡³å…³é‡è¦ã€‚`name`å‡ ä¹æ€»æ˜¯PyTorchä»£ç ä¸­ç›¸åº”å˜é‡çš„åç§°ã€‚å¦‚æœ`name`æ²¡æœ‰æ­£ç¡®è®¾ç½®ï¼ŒåŠ è½½æ¨¡å‹æƒé‡æ—¶ä¼šåœ¨é”™è¯¯æ¶ˆæ¯ä¸­çœ‹åˆ°ã€‚
- en: The logic of the base model class, `BrandNewBertModel`, will actually reside
    in `TFBrandNewBertMainLayer`, a Keras layer subclass ([example](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L719)).
    `TFBrandNewBertModel` will simply be a wrapper around this layer.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡å‹ç±»`BrandNewBertModel`çš„é€»è¾‘å®é™…ä¸Šå°†é©»ç•™åœ¨`TFBrandNewBertMainLayer`ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªKeraså±‚å­ç±»ï¼ˆ[ç¤ºä¾‹](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L719)ï¼‰ã€‚`TFBrandNewBertModel`å°†ç®€å•åœ°æ˜¯è¿™ä¸ªå±‚çš„åŒ…è£…å™¨ã€‚
- en: Keras models need to be built in order to load pretrained weights. For that
    reason, `TFBrandNewBertPreTrainedModel` will need to hold an example of inputs
    to the model, the `dummy_inputs` ([example](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L916)).
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kerasæ¨¡å‹éœ€è¦è¢«æ„å»ºä»¥åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚å› æ­¤ï¼Œ`TFBrandNewBertPreTrainedModel`å°†éœ€è¦ä¿å­˜æ¨¡å‹çš„è¾“å…¥ç¤ºä¾‹ï¼Œå³`dummy_inputs`ï¼ˆ[ç¤ºä¾‹](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L916)ï¼‰ã€‚
- en: If you get stuck, ask for help - weâ€™re here to help you! ğŸ¤—
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœé‡åˆ°å›°éš¾ï¼Œè¯·å¯»æ±‚å¸®åŠ© - æˆ‘ä»¬åœ¨è¿™é‡Œå¸®åŠ©ä½ ï¼ğŸ¤—
- en: 'In addition to the model file itself, you will also need to add the pointers
    to the model classes and related documentation pages. You can complete this part
    entirely following the patterns in other PRs ([example](https://github.com/huggingface/transformers/pull/18020/files)).
    Hereâ€™s a list of the needed manual changes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ¨¡å‹æ–‡ä»¶æœ¬èº«ï¼Œæ‚¨è¿˜éœ€è¦æ·»åŠ æŒ‡å‘æ¨¡å‹ç±»å’Œç›¸å…³æ–‡æ¡£é¡µé¢çš„æŒ‡é’ˆã€‚æ‚¨å¯ä»¥å®Œå…¨æŒ‰ç…§å…¶ä»– PR ä¸­çš„æ¨¡å¼å®Œæˆæ­¤éƒ¨åˆ†ï¼ˆ[ç¤ºä¾‹](https://github.com/huggingface/transformers/pull/18020/files)ï¼‰ã€‚ä»¥ä¸‹æ˜¯æ‰€éœ€æ‰‹åŠ¨æ›´æ”¹çš„åˆ—è¡¨ï¼š
- en: Include all public classes of *BrandNewBert* in `src/transformers/__init__.py`
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ `src/transformers/__init__.py` ä¸­åŒ…å« *BrandNewBert* çš„æ‰€æœ‰å…¬å…±ç±»
- en: Add *BrandNewBert* classes to the corresponding Auto classes in `src/transformers/models/auto/modeling_tf_auto.py`
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ `src/transformers/models/auto/modeling_tf_auto.py` ä¸­å°† *BrandNewBert* ç±»æ·»åŠ åˆ°ç›¸åº”çš„
    Auto ç±»ä¸­
- en: Add the lazy loading classes related to *BrandNewBert* in `src/transformers/utils/dummy_tf_objects.py`
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ `src/transformers/utils/dummy_tf_objects.py` ä¸­æ·»åŠ ä¸ *BrandNewBert* ç›¸å…³çš„å»¶è¿ŸåŠ è½½ç±»
- en: Update the import structures for the public classes in `src/transformers/models/brand_new_bert/__init__.py`
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´æ–° `src/transformers/models/brand_new_bert/__init__.py` ä¸­å…¬å…±ç±»çš„å¯¼å…¥ç»“æ„
- en: Add the documentation pointers to the public methods of *BrandNewBert* in `docs/source/en/model_doc/brand_new_bert.md`
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ `docs/source/en/model_doc/brand_new_bert.md` ä¸­ä¸º *BrandNewBert* çš„å…¬å…±æ–¹æ³•æ·»åŠ æ–‡æ¡£æŒ‡é’ˆ
- en: Add yourself to the list of contributors to *BrandNewBert* in `docs/source/en/model_doc/brand_new_bert.md`
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ `docs/source/en/model_doc/brand_new_bert.md` ä¸­å°†è‡ªå·±æ·»åŠ åˆ° *BrandNewBert* çš„è´¡çŒ®è€…åˆ—è¡¨ä¸­
- en: Finally, add a green tick âœ… to the TensorFlow column of *BrandNewBert* in `docs/source/en/index.md`
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨ `docs/source/en/index.md` ä¸­ *BrandNewBert* çš„ TensorFlow åˆ—ä¸­æ·»åŠ ä¸€ä¸ªç»¿è‰²å‹¾ âœ…
- en: 'When youâ€™re happy with your implementation, run the following checklist to
    confirm that your model architecture is ready:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨å¯¹å®ç°æ„Ÿåˆ°æ»¡æ„æ—¶ï¼Œè¯·è¿è¡Œä»¥ä¸‹æ£€æŸ¥è¡¨ä»¥ç¡®è®¤æ‚¨çš„æ¨¡å‹æ¶æ„å·²å‡†å¤‡å°±ç»ªï¼š
- en: All layers that behave differently at train time (e.g. Dropout) are called with
    a `training` argument, which is propagated all the way from the top-level classes
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ—¶è¡Œä¸ºä¸åŒçš„æ‰€æœ‰å±‚ï¼ˆä¾‹å¦‚ Dropoutï¼‰éƒ½ä½¿ç”¨ `training` å‚æ•°è°ƒç”¨ï¼Œå¹¶ä¸”è¯¥å‚æ•°ä»é¡¶å±‚ç±»ä¸€ç›´ä¼ æ’­ä¸‹å»
- en: You have used `#copied from ...` whenever possible
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œæ‚¨å·²ç»ä½¿ç”¨äº† `#copied from ...`
- en: '`TFBrandNewBertMainLayer` and all classes that use it have their `call` function
    decorated with `@unpack_inputs`'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`TFBrandNewBertMainLayer` å’Œæ‰€æœ‰ä½¿ç”¨å®ƒçš„ç±»éƒ½å°†å…¶ `call` å‡½æ•°è£…é¥°ä¸º `@unpack_inputs`'
- en: '`TFBrandNewBertMainLayer` is decorated with `@keras_serializable`'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`TFBrandNewBertMainLayer` è¢«è£…é¥°ä¸º `@keras_serializable`'
- en: A TensorFlow model can be loaded from PyTorch weights using `TFBrandNewBert.from_pretrained(model_repo,
    from_pt=True)`
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ `TFBrandNewBert.from_pretrained(model_repo, from_pt=True)` ä» PyTorch æƒé‡åŠ è½½
    TensorFlow æ¨¡å‹
- en: You can call the TensorFlow model using the expected input format
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨é¢„æœŸçš„è¾“å…¥æ ¼å¼è°ƒç”¨ TensorFlow æ¨¡å‹
- en: 5\. Add model tests
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. æ·»åŠ æ¨¡å‹æµ‹è¯•
- en: Hurray, youâ€™ve implemented a TensorFlow model! Now itâ€™s time to add tests to
    make sure that your model behaves as expected. As in the previous section, we
    suggest you start by copying the `test_modeling_brand_new_bert.py` file in `tests/models/brand_new_bert/`
    into `test_modeling_tf_brand_new_bert.py`, and continue by making the necessary
    TensorFlow replacements. For now, in all `.from_pretrained()` calls, you should
    use the `from_pt=True` flag to load the existing PyTorch weights.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‡å²ï¼Œæ‚¨å·²ç»å®ç°äº†ä¸€ä¸ª TensorFlow æ¨¡å‹ï¼ç°åœ¨æ˜¯æ·»åŠ æµ‹è¯•ä»¥ç¡®ä¿æ‚¨çš„æ¨¡å‹è¡¨ç°å¦‚é¢„æœŸçš„æ—¶å€™äº†ã€‚ä¸å‰ä¸€èŠ‚ä¸€æ ·ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨é¦–å…ˆå°† `tests/models/brand_new_bert/`
    ä¸­çš„ `test_modeling_brand_new_bert.py` æ–‡ä»¶å¤åˆ¶åˆ° `test_modeling_tf_brand_new_bert.py`
    ä¸­ï¼Œç„¶åç»§ç»­è¿›è¡Œå¿…è¦çš„ TensorFlow æ›¿æ¢ã€‚ç›®å‰ï¼Œåœ¨æ‰€æœ‰ `.from_pretrained()` è°ƒç”¨ä¸­ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ `from_pt=True`
    æ ‡å¿—æ¥åŠ è½½ç°æœ‰çš„ PyTorch æƒé‡ã€‚
- en: 'After youâ€™re done, itâ€™s time for the moment of truth: run the tests! ğŸ˜¬'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæ˜¯çœŸç›¸æ—¶åˆ»ï¼šè¿è¡Œæµ‹è¯•ï¼ğŸ˜¬
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The most likely outcome is that youâ€™ll see a bunch of errors. Donâ€™t worry, this
    is expected! Debugging ML models is notoriously hard, and the key ingredient to
    success is patience (and `breakpoint()`). In our experience, the hardest problems
    arise from subtle mismatches between ML frameworks, for which we have a few pointers
    at the end of this guide. In other cases, a general test might not be directly
    applicable to your model, in which case we suggest an override at the model test
    class level. Regardless of the issue, donâ€™t hesitate to ask for help in your draft
    pull request if youâ€™re stuck.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€æœ‰å¯èƒ½çš„ç»“æœæ˜¯æ‚¨ä¼šçœ‹åˆ°ä¸€å †é”™è¯¯ã€‚ä¸è¦æ‹…å¿ƒï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼è°ƒè¯• ML æ¨¡å‹æ˜¯éå¸¸å›°éš¾çš„ï¼ŒæˆåŠŸçš„å…³é”®å› ç´ æ˜¯è€å¿ƒï¼ˆå’Œ `breakpoint()`ï¼‰ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œæœ€å›°éš¾çš„é—®é¢˜æ¥è‡ªäº
    ML æ¡†æ¶ä¹‹é—´çš„å¾®å¦™ä¸åŒ¹é…ï¼Œæˆ‘ä»¬åœ¨æœ¬æŒ‡å—æœ«å°¾æä¾›äº†ä¸€äº›æŒ‡é’ˆã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œä¸€èˆ¬æµ‹è¯•å¯èƒ½ä¸ç›´æ¥é€‚ç”¨äºæ‚¨çš„æ¨¡å‹ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å»ºè®®åœ¨æ¨¡å‹æµ‹è¯•ç±»çº§åˆ«è¿›è¡Œè¦†ç›–ã€‚æ— è®ºé—®é¢˜æ˜¯ä»€ä¹ˆï¼Œè¯·ä¸è¦çŠ¹è±«åœ¨æ‚¨çš„è‰ç¨¿æ‹‰å–è¯·æ±‚ä¸­å¯»æ±‚å¸®åŠ©ï¼Œå¦‚æœæ‚¨é‡åˆ°å›°éš¾ã€‚
- en: When all tests pass, congratulations, your model is nearly ready to be added
    to the ğŸ¤— Transformers library! ğŸ‰
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‰€æœ‰æµ‹è¯•é€šè¿‡æ—¶ï¼Œæ­å–œï¼Œæ‚¨çš„æ¨¡å‹å‡ ä¹å¯ä»¥æ·»åŠ åˆ° ğŸ¤— Transformers åº“ä¸­äº†ï¼ğŸ‰
- en: 6.-7\. Ensure everyone can use your model
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.-7\. ç¡®ä¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹
- en: '**6\. Submit the pull request**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. æäº¤æ‹‰å–è¯·æ±‚**'
- en: Once youâ€™re done with the implementation and the tests, itâ€™s time to submit
    a pull request. Before pushing your code, run our code formatting utility, `make
    fixup` ğŸª„. This will automatically fix any formatting issues, which would cause
    our automatic checks to fail.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆå®ç°å’Œæµ‹è¯•åï¼Œç°åœ¨æ˜¯æäº¤æ‹‰å–è¯·æ±‚çš„æ—¶å€™äº†ã€‚åœ¨æ¨é€ä»£ç ä¹‹å‰ï¼Œè¯·è¿è¡Œæˆ‘ä»¬çš„ä»£ç æ ¼å¼åŒ–å·¥å…· `make fixup` ğŸª„ã€‚è¿™å°†è‡ªåŠ¨ä¿®å¤ä»»ä½•æ ¼å¼é—®é¢˜ï¼Œå¦åˆ™ä¼šå¯¼è‡´æˆ‘ä»¬çš„è‡ªåŠ¨æ£€æŸ¥å¤±è´¥ã€‚
- en: Itâ€™s now time to convert your draft pull request into a real pull request. To
    do so, click on the â€œReady for reviewâ€ button and add Joao (`@gante`) and Matt
    (`@Rocketknight1`) as reviewers. A model pull request will need at least 3 reviewers,
    but they will take care of finding appropriate additional reviewers for your model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯å°†è‰ç¨¿æ‹‰å–è¯·æ±‚è½¬æ¢ä¸ºçœŸæ­£æ‹‰å–è¯·æ±‚çš„æ—¶å€™äº†ã€‚ä¸ºæ­¤ï¼Œè¯·ç‚¹å‡»â€œå‡†å¤‡å¥½å®¡æŸ¥â€æŒ‰é’®ï¼Œå¹¶å°† Joao (`@gante`) å’Œ Matt (`@Rocketknight1`)
    æ·»åŠ ä¸ºå®¡é˜…è€…ã€‚æ¨¡å‹æ‹‰å–è¯·æ±‚å°†éœ€è¦è‡³å°‘3åå®¡é˜…è€…ï¼Œä½†ä»–ä»¬ä¼šè´Ÿè´£ä¸ºæ‚¨çš„æ¨¡å‹æ‰¾åˆ°åˆé€‚çš„é¢å¤–å®¡é˜…è€…ã€‚
- en: After all reviewers are happy with the state of your PR, the final action point
    is to remove the `from_pt=True` flag in `.from_pretrained()` calls. Since there
    are no TensorFlow weights, you will have to add them! Check the section below
    for instructions on how to do it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰å®¡é˜…è€…å¯¹æ‚¨çš„ PR çš„çŠ¶æ€æ»¡æ„åï¼Œæœ€åä¸€ä¸ªè¡ŒåŠ¨ç‚¹æ˜¯åœ¨ `.from_pretrained()` è°ƒç”¨ä¸­ç§»é™¤ `from_pt=True` æ ‡å¿—ã€‚ç”±äºæ²¡æœ‰
    TensorFlow æƒé‡ï¼Œæ‚¨å°†éœ€è¦æ·»åŠ å®ƒä»¬ï¼æŸ¥çœ‹ä¸‹é¢çš„éƒ¨åˆ†ä»¥è·å–å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œçš„è¯´æ˜ã€‚
- en: Finally, when the TensorFlow weights get merged, you have at least 3 reviewer
    approvals, and all CI checks are green, double-check the tests locally one last
    time
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå½“TensorFlowæƒé‡åˆå¹¶æ—¶ï¼Œä½ è‡³å°‘æœ‰3ä¸ªå®¡é˜…è€…çš„æ‰¹å‡†ï¼Œå¹¶ä¸”æ‰€æœ‰CIæ£€æŸ¥éƒ½æ˜¯ç»¿è‰²çš„æ—¶å€™ï¼Œæœ€åå†æœ¬åœ°å†æ¬¡æ£€æŸ¥æµ‹è¯•
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: and we will merge your PR! Congratulations on the milestone ğŸ‰
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆå¹¶ä½ çš„PRï¼ç¥è´ºä½ è¾¾åˆ°çš„é‡Œç¨‹ç¢‘ğŸ‰
- en: '**7\. (Optional) Build demos and share with the world**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. ï¼ˆå¯é€‰ï¼‰æ„å»ºæ¼”ç¤ºå¹¶ä¸ä¸–ç•Œåˆ†äº«**'
- en: One of the hardest parts about open-source is discovery. How can the other users
    learn about the existence of your fabulous TensorFlow contribution? With proper
    communication, of course! ğŸ“£
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€æºé¡¹ç›®ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†ä¹‹ä¸€æ˜¯å‘ç°ã€‚å…¶ä»–ç”¨æˆ·å¦‚ä½•äº†è§£ä½ å‡ºè‰²çš„TensorFlowè´¡çŒ®çš„å­˜åœ¨ï¼Ÿå½“ç„¶æ˜¯é€šè¿‡é€‚å½“çš„æ²Ÿé€šï¼
- en: 'There are two main ways to share your model with the community:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§ä¸»è¦çš„æ–¹æ³•å¯ä»¥ä¸ç¤¾åŒºåˆ†äº«ä½ çš„æ¨¡å‹ï¼š
- en: Build demos. These include Gradio demos, notebooks, and other fun ways to show
    off your model. We highly encourage you to add a notebook to our [community-driven
    demos](https://huggingface.co/docs/transformers/community).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºæ¼”ç¤ºã€‚è¿™äº›åŒ…æ‹¬Gradioæ¼”ç¤ºã€ç¬”è®°æœ¬å’Œå…¶ä»–æœ‰è¶£çš„æ–¹å¼æ¥å±•ç¤ºä½ çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼ºçƒˆé¼“åŠ±ä½ å‘æˆ‘ä»¬çš„[ç¤¾åŒºé©±åŠ¨æ¼”ç¤º](https://huggingface.co/docs/transformers/community)æ·»åŠ ä¸€ä¸ªç¬”è®°æœ¬ã€‚
- en: Share stories on social media like Twitter and LinkedIn. You should be proud
    of your work and share your achievement with the community - your model can now
    be used by thousands of engineers and researchers around the world ğŸŒ! We will
    be happy to retweet your posts and help you share your work with the community.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¤¾äº¤åª’ä½“ä¸Šåˆ†äº«æ•…äº‹ï¼Œæ¯”å¦‚Twitterå’ŒLinkedInã€‚ä½ åº”è¯¥ä¸ºè‡ªå·±çš„å·¥ä½œæ„Ÿåˆ°è‡ªè±ªï¼Œå¹¶ä¸ç¤¾åŒºåˆ†äº«ä½ çš„æˆå°± - ä½ çš„æ¨¡å‹ç°åœ¨å¯ä»¥è¢«å…¨çƒæ•°åƒåå·¥ç¨‹å¸ˆå’Œç ”ç©¶äººå‘˜ä½¿ç”¨ï¼æˆ‘ä»¬å°†å¾ˆä¹æ„è½¬å‘ä½ çš„å¸–å­ï¼Œå¹¶å¸®åŠ©ä½ ä¸ç¤¾åŒºåˆ†äº«ä½ çš„å·¥ä½œã€‚
- en: Adding TensorFlow weights to ğŸ¤— Hub
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†TensorFlowæƒé‡æ·»åŠ åˆ°ğŸ¤— Hub
- en: Assuming that the TensorFlow model architecture is available in ğŸ¤— Transformers,
    converting PyTorch weights into TensorFlow weights is a breeze!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾TensorFlowæ¨¡å‹æ¶æ„åœ¨ğŸ¤— Transformersä¸­å¯ç”¨ï¼Œå°†PyTorchæƒé‡è½¬æ¢ä¸ºTensorFlowæƒé‡å°†å˜å¾—è½»è€Œæ˜“ä¸¾ï¼
- en: 'Hereâ€™s how to do it:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼š
- en: Make sure you are logged into your Hugging Face account in your terminal. You
    can log in using the command `huggingface-cli login` (you can find your access
    tokens [here](https://huggingface.co/settings/tokens))
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä½ å·²ç»åœ¨ç»ˆç«¯ä¸­ç™»å½•åˆ°ä½ çš„Hugging Faceè´¦æˆ·ã€‚ä½ å¯ä»¥ä½¿ç”¨å‘½ä»¤`huggingface-cli login`ç™»å½•ï¼ˆä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/settings/tokens)æ‰¾åˆ°ä½ çš„è®¿é—®ä»¤ç‰Œï¼‰
- en: Run `transformers-cli pt-to-tf --model-name foo/bar`, where `foo/bar` is the
    name of the model repository containing the PyTorch weights you want to convert
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿è¡Œ`transformers-cli pt-to-tf --model-name foo/bar`ï¼Œå…¶ä¸­`foo/bar`æ˜¯åŒ…å«ä½ æƒ³è¦è½¬æ¢çš„PyTorchæƒé‡çš„æ¨¡å‹å­˜å‚¨åº“çš„åç§°
- en: Tag `@joaogante` and `@Rocketknight1` in the ğŸ¤— Hub PR the command above has
    just created
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Hub PRä¸­æ ‡è®°`@joaogante`å’Œ`@Rocketknight1`ï¼Œè¿™æ˜¯ä¸Šé¢å‘½ä»¤åˆ›å»ºçš„
- en: Thatâ€™s it! ğŸ‰
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼
- en: Debugging mismatches across ML frameworks ğŸ›
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è·¨MLæ¡†æ¶è°ƒè¯•ä¸åŒ¹é…ğŸ›
- en: At some point, when adding a new architecture or when creating TensorFlow weights
    for an existing architecture, you might come across errors complaining about mismatches
    between PyTorch and TensorFlow. You might even decide to open the model architecture
    code for the two frameworks, and find that they look identical. Whatâ€™s going on?
    ğŸ¤”
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·»åŠ æ–°æ¶æ„æˆ–ä¸ºç°æœ‰æ¶æ„åˆ›å»ºTensorFlowæƒé‡æ—¶ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°å…³äºPyTorchå’ŒTensorFlowä¹‹é—´ä¸åŒ¹é…çš„é”™è¯¯ã€‚ä½ ç”šè‡³å¯èƒ½å†³å®šæ‰“å¼€ä¸¤ä¸ªæ¡†æ¶çš„æ¨¡å‹æ¶æ„ä»£ç ï¼Œå¹¶å‘ç°å®ƒä»¬çœ‹èµ·æ¥æ˜¯ç›¸åŒçš„ã€‚å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
- en: First of all, letâ€™s talk about why understanding these mismatches matters. Many
    community members will use ğŸ¤— Transformers models out of the box, and trust that
    our models behave as expected. When there is a large mismatch between the two
    frameworks, it implies that the model is not following the reference implementation
    for at least one of the frameworks. This might lead to silent failures, in which
    the model runs but has poor performance. This is arguably worse than a model that
    fails to run at all! To that end, we aim at having a framework mismatch smaller
    than `1e-5` at all stages of the model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è°ˆè°ˆä¸ºä»€ä¹ˆç†è§£è¿™äº›ä¸åŒ¹é…å¾ˆé‡è¦ã€‚è®¸å¤šç¤¾åŒºæˆå‘˜å°†ç›´æ¥ä½¿ç”¨ğŸ¤— Transformersæ¨¡å‹ï¼Œå¹¶ç›¸ä¿¡æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å¦‚é¢„æœŸã€‚å½“ä¸¤ä¸ªæ¡†æ¶ä¹‹é—´å­˜åœ¨è¾ƒå¤§çš„ä¸åŒ¹é…æ—¶ï¼Œè¿™æ„å‘³ç€æ¨¡å‹è‡³å°‘åœ¨ä¸€ä¸ªæ¡†æ¶ä¸­æ²¡æœ‰éµå¾ªå‚è€ƒå®ç°ã€‚è¿™å¯èƒ½å¯¼è‡´æ‚„æ— å£°æ¯çš„å¤±è´¥ï¼Œå³æ¨¡å‹è¿è¡Œä½†æ€§èƒ½ä¸ä½³ã€‚è¿™å¯èƒ½æ¯”æ ¹æœ¬æ— æ³•è¿è¡Œçš„æ¨¡å‹æ›´ç³Ÿï¼å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨æ¨¡å‹çš„æ‰€æœ‰é˜¶æ®µéƒ½æœ‰å°äº`1e-5`çš„æ¡†æ¶ä¸åŒ¹é…ã€‚
- en: 'As in other numerical problems, the devil is in the details. And as in any
    detail-oriented craft, the secret ingredient here is patience. Here is our suggested
    workflow for when you come across this type of issues:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå…¶ä»–æ•°å€¼é—®é¢˜ä¸€æ ·ï¼Œé­”é¬¼å°±åœ¨ç»†èŠ‚ä¸­ã€‚å°±åƒä»»ä½•æ³¨é‡ç»†èŠ‚çš„å·¥è‰ºä¸€æ ·ï¼Œè€å¿ƒæ˜¯ç§˜å¯†çš„å…³é”®ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å»ºè®®çš„å·¥ä½œæµç¨‹ï¼Œå½“ä½ é‡åˆ°è¿™ç§ç±»å‹çš„é—®é¢˜æ—¶ï¼š
- en: Locate the source of mismatches. The model youâ€™re converting probably has near
    identical inner variables up to a certain point. Place `breakpoint()` statements
    in the two frameworksâ€™ architectures, and compare the values of the numerical
    variables in a top-down fashion until you find the source of the problems.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°ä¸åŒ¹é…çš„æºå¤´ã€‚ä½ è¦è½¬æ¢çš„æ¨¡å‹å¯èƒ½åœ¨æŸä¸ªç‰¹å®šç‚¹ä¸Šæœ‰å‡ ä¹ç›¸åŒçš„å†…éƒ¨å˜é‡ã€‚åœ¨ä¸¤ä¸ªæ¡†æ¶çš„æ¶æ„ä¸­æ”¾ç½®`breakpoint()`è¯­å¥ï¼Œå¹¶ä»¥è‡ªä¸Šè€Œä¸‹çš„æ–¹å¼æ¯”è¾ƒæ•°å€¼å˜é‡çš„å€¼ï¼Œç›´åˆ°æ‰¾åˆ°é—®é¢˜çš„æºå¤´ã€‚
- en: Now that youâ€™ve pinpointed the source of the issue, get in touch with the ğŸ¤—
    Transformers team. It is possible that weâ€™ve seen a similar problem before and
    can promptly provide a solution. As a fallback, scan popular pages like StackOverflow
    and GitHub issues.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»æ‰¾åˆ°äº†é—®é¢˜çš„æºå¤´ï¼Œè¯·ä¸ğŸ¤— Transformerså›¢é˜Ÿè”ç³»ã€‚å¯èƒ½æˆ‘ä»¬ä¹‹å‰è§è¿‡ç±»ä¼¼çš„é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥è¿…é€Ÿæä¾›è§£å†³æ–¹æ¡ˆã€‚ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼Œæµè§ˆåƒStackOverflowå’ŒGitHubé—®é¢˜è¿™æ ·çš„çƒ­é—¨é¡µé¢ã€‚
- en: If there is no solution in sight, it means youâ€™ll have to go deeper. The good
    news is that youâ€™ve located the issue, so you can focus on the problematic instruction,
    abstracting away the rest of the model! The bad news is that youâ€™ll have to venture
    into the source implementation of said instruction. In some cases, you might find
    an issue with a reference implementation - donâ€™t abstain from opening an issue
    in the upstream repository.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœçœ‹ä¸åˆ°è§£å†³æ–¹æ¡ˆï¼Œè¿™æ„å‘³ç€ä½ å°†ä¸å¾—ä¸æ·±å…¥ç ”ç©¶ã€‚å¥½æ¶ˆæ¯æ˜¯ä½ å·²ç»æ‰¾åˆ°äº†é—®é¢˜æ‰€åœ¨ï¼Œæ‰€ä»¥ä½ å¯ä»¥ä¸“æ³¨äºæœ‰é—®é¢˜çš„æŒ‡ä»¤ï¼Œå°†æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†æŠ½è±¡å‡ºæ¥ï¼åæ¶ˆæ¯æ˜¯ä½ å°†ä¸å¾—ä¸æ·±å…¥ç ”ç©¶è¯¥æŒ‡ä»¤çš„æºå®ç°ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½ä¼šå‘ç°å‚è€ƒå®ç°å­˜åœ¨é—®é¢˜
    - ä¸è¦æ”¾å¼ƒåœ¨ä¸Šæ¸¸å­˜å‚¨åº“ä¸­å¼€å¯é—®é¢˜ã€‚
- en: In some cases, in discussion with the ğŸ¤— Transformers team, we might find that
    fixing the mismatch is infeasible. When the mismatch is very small in the output
    layers of the model (but potentially large in the hidden states), we might decide
    to ignore it in favor of distributing the model. The `pt-to-tf` CLI mentioned
    above has a `--max-error` flag to override the error message at weight conversion
    time.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œåœ¨ä¸ğŸ¤— Transformerså›¢é˜Ÿè®¨è®ºåï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå‘ç°ä¿®å¤ä¸åŒ¹é…æ˜¯ä¸å¯è¡Œçš„ã€‚å½“æ¨¡å‹çš„è¾“å‡ºå±‚ä¸­ä¸åŒ¹é…éå¸¸å°ï¼ˆä½†åœ¨éšè—çŠ¶æ€ä¸­å¯èƒ½å¾ˆå¤§ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå†³å®šå¿½ç•¥å®ƒï¼Œä»¥ä¾¿åˆ†å‘æ¨¡å‹ã€‚ä¸Šé¢æåˆ°çš„`pt-to-tf`
    CLIå…·æœ‰ä¸€ä¸ª`--max-error`æ ‡å¿—ï¼Œå¯ä»¥åœ¨æƒé‡è½¬æ¢æ—¶è¦†ç›–é”™è¯¯æ¶ˆæ¯ã€‚
