- en: Multitask prompt tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多任务提示调整
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning](https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning](https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Multitask prompt tuning](https://huggingface.co/papers/2303.02861) decomposes
    the soft prompts of each task into a single learned transferable prompt instead
    of a separate prompt for each task. The single learned prompt can be adapted for
    each task by multiplicative low rank updates.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[多任务提示调整](https://huggingface.co/papers/2303.02861) 将每个任务的软提示分解为单个可学习的可转移提示，而不是为每个任务单独设置提示。单个学习的提示可以通过乘法低秩更新适应每个任务。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要是：
- en: '*Prompt tuning, in which a base pretrained model is adapted to each task via
    conditioning on learned prompt vectors, has emerged as a promising approach for
    efficiently adapting large language models to multiple downstream tasks. However,
    existing methods typically learn soft prompt vectors from scratch, and it has
    not been clear how to exploit the rich cross-task knowledge with prompt vectors
    in a multitask learning setting. We propose multitask prompt tuning (MPT), which
    first learns a single transferable prompt by distilling knowledge from multiple
    task-specific source prompts. We then learn multiplicative low rank updates to
    this shared prompt to efficiently adapt it to each downstream target task. Extensive
    experiments on 23 NLP datasets demonstrate that our proposed approach outperforms
    the state-of-the-art methods, including the full finetuning baseline in some cases,
    despite only tuning 0.035% as many task-specific parameters*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示调整是一种基于学习提示向量对基础预训练模型进行调整的方法，已经成为一种有效地将大型语言模型有效地适应多个下游任务的有前途的方法。然而，现有方法通常是从头开始学习软提示向量，并且尚不清楚如何在多任务学习环境中利用提示向量中的丰富跨任务知识。我们提出了多任务提示调整（MPT），首先通过从多个任务特定源提示中提炼知识来学习单个可转移提示。然后，我们学习对这个共享提示进行乘法低秩更新，以有效地将其适应到每个下游目标任务。对23个NLP数据集的大量实验表明，我们提出的方法在某些情况下优于最先进的方法，包括在某些情况下优于完全微调基线，尽管只调整了0.035%的任务特定参数*。'
- en: MultitaskPromptTuningConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务提示调整配置
- en: '### `class peft.MultitaskPromptTuningConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.MultitaskPromptTuningConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/multitask_prompt_tuning/config.py#L36)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/multitask_prompt_tuning/config.py#L36)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: MultitaskPromptEmbedding
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务提示嵌入
- en: '### `class peft.tuners.MultitaskPromptEmbedding`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.tuners.MultitaskPromptEmbedding`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/multitask_prompt_tuning/model.py#L27)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/multitask_prompt_tuning/model.py#L27)'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
