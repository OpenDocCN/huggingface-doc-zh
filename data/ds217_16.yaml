- en: Load
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载
- en: 'Original text: [https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Your data can be stored in various places; they can be on your local machine’s
    disk, in a Github repository, and in in-memory data structures like Python dictionaries
    and Pandas DataFrames. Wherever a dataset is stored, 🤗 Datasets can help you load
    it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据可以存储在各种位置；它们可以在本地计算机的磁盘上、在Github存储库中以及在Python字典和Pandas DataFrames等内存数据结构中。无论数据集存储在哪里，🤗
    数据集都可以帮助您加载它。
- en: 'This guide will show you how to load a dataset from:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将向您展示如何从以下位置加载数据集：
- en: The Hub without a dataset loading script
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有数据集加载脚本的Hub
- en: Local loading script
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地加载脚本
- en: Local files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地文件
- en: In-memory data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存数据
- en: Offline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离线
- en: A specific slice of a split
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割的特定切片
- en: For more details specific to loading other dataset modalities, take a look at
    the [load audio dataset guide](./audio_load), the [load image dataset guide](./image_load),
    or the [load text dataset guide](./nlp_load).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有关加载其他数据集模态的更多详细信息，请查看[加载音频数据集指南](./audio_load)、[加载图像数据集指南](./image_load)或[加载文本数据集指南](./nlp_load)。
- en: Hugging Face Hub
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face Hub
- en: Datasets are loaded from a dataset loading script that downloads and generates
    the dataset. However, you can also load a dataset from any dataset repository
    on the Hub without a loading script! Begin by [creating a dataset repository](share#create-the-repository)
    and upload your data files. Now you can use the [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    function to load the dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是从下载和生成数据集的数据集加载脚本中加载的。但是，您也可以从Hub上的任何数据集存储库加载数据集而无需加载脚本！首先[创建一个数据集存储库](share#create-the-repository)，并上传您的数据文件。现在您可以使用[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)函数来加载数据集。
- en: 'For example, try loading the files from this [demo repository](https://huggingface.co/datasets/lhoestq/demo1)
    by providing the repository namespace and dataset name. This dataset repository
    contains CSV files, and the code below loads the dataset from the CSV files:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，尝试通过提供存储库命名空间和数据集名称从这个[demo存储库](https://huggingface.co/datasets/lhoestq/demo1)加载文件。这个数据集存储库包含CSV文件，下面的代码从CSV文件加载数据集：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Some datasets may have more than one version based on Git tags, branches, or
    commits. Use the `revision` parameter to specify the dataset version you want
    to load:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 某些数据集可能基于Git标签、分支或提交有多个版本。使用`revision`参数指定要加载的数据集版本：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Refer to the [Upload a dataset to the Hub](./upload_dataset) tutorial for more
    details on how to create a dataset repository on the Hub, and how to upload your
    data files.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何在Hub上创建数据集存储库以及如何上传数据文件的更多详细信息，请参考[上传数据集到Hub](./upload_dataset)教程。
- en: 'A dataset without a loading script by default loads all the data into the `train`
    split. Use the `data_files` parameter to map data files to splits like `train`,
    `validation` and `test`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，没有加载脚本的数据集将所有数据加载到`train`分割中。使用`data_files`参数将数据文件映射到`train`、`validation`和`test`等分割：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you don’t specify which data files to use, [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    will return all the data files. This can take a long time if you load a large
    dataset like C4, which is approximately 13TB of data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有指定要使用哪些数据文件，[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)将返回所有数据文件。如果加载像C4这样的大型数据集，这可能需要很长时间，C4大约有13TB的数据。
- en: You can also load a specific subset of the files with the `data_files` or `data_dir`
    parameter. These parameters can accept a relative path which resolves to the base
    path corresponding to where the dataset is loaded from.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`data_files`或`data_dir`参数加载特定的文件子集。这些参数可以接受一个相对路径，该路径解析为数据集加载的基本路径。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `split` parameter can also map a data file to a specific split:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`split`参数还可以将数据文件映射到特定的分割：'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Local loading script
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地加载脚本
- en: 'You may have a 🤗 Datasets loading script locally on your computer. In this
    case, load the dataset by passing one of the following paths to [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能在计算机上本地拥有一个🤗 数据集加载脚本。在这种情况下，通过将以下路径之一传递给[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)来加载数据集：
- en: The local path to the loading script file.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载脚本文件的本地路径。
- en: The local path to the directory containing the loading script file (only if
    the script file has the same name as the directory).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含加载脚本文件的目录的本地路径（仅当脚本文件与目录同名时）。
- en: 'Pass `trust_remote_code=True` to allow 🤗 Datasets to execute the loading script:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 传递`trust_remote_code=True`以允许🤗 数据集执行加载脚本：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Edit loading script
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑加载脚本
- en: 'You can also edit a loading script from the Hub to add your own modifications.
    Download the dataset repository locally so any data files referenced by a relative
    path in the loading script can be loaded:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以编辑Hub中的加载脚本以添加自己的修改。将数据集存储库下载到本地，以便加载脚本中相对路径引用的任何数据文件都可以被加载：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Make your edits to the loading script and then load it by passing its local
    path to [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑加载脚本，然后通过将其本地路径传递给[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)来加载它：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Local and remote files
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地和远程文件
- en: Datasets can be loaded from local files stored on your computer and from remote
    files. The datasets are most likely stored as a `csv`, `json`, `txt` or `parquet`
    file. The [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    function can load each of these file types.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以从存储在计算机上的本地文件和远程文件加载。这些数据集很可能存储为`csv`、`json`、`txt`或`parquet`文件。[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)函数可以加载每种文件类型。
- en: CSV
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CSV
- en: '🤗 Datasets can read a dataset made up of one or several CSV files (in this
    case, pass your CSV files as a list):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 数据集可以读取由一个或多个CSV文件组成的数据集（在这种情况下，将CSV文件作为列表传递）：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For more details, check out the [how to load tabular datasets from CSV files](tabular_load#csv-files)
    guide.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请查看[如何从CSV文件加载表格数据集](tabular_load#csv-files)指南。
- en: JSON
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JSON
- en: 'JSON files are loaded directly with [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    as shown below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文件直接使用[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)加载，如下所示：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'JSON files have diverse formats, but we think the most efficient format is
    to have multiple JSON objects; each line represents an individual row of data.
    For example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文件有各种格式，但我们认为最有效的格式是具有多个JSON对象；每行代表一个数据行。例如：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Another JSON format you may encounter is a nested field, in which case you’ll
    need to specify the `field` argument as shown in the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到的另一种JSON格式是嵌套字段，这种情况下您需要像下面这样指定`field`参数：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To load remote JSON files via HTTP, pass the URLs instead:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HTTP加载远程JSON文件，传递URLs：
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: While these are the most common JSON formats, you’ll see other datasets that
    are formatted differently. 🤗 Datasets recognizes these other formats and will
    fallback accordingly on the Python JSON loading methods to handle them.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些是最常见的JSON格式，但您会看到其他格式不同的数据集。🤗数据集识别这些其他格式，并将相应地回退到Python JSON加载方法来处理它们。
- en: Parquet
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parquet
- en: Parquet files are stored in a columnar format, unlike row-based files like a
    CSV. Large datasets may be stored in a Parquet file because it is more efficient
    and faster at returning your query.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet文件以列格式存储，不同于像CSV这样的基于行的文件。由于其更高效和更快速地返回查询结果，大型数据集可能存储在Parquet文件中。
- en: 'To load a Parquet file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 加载Parquet文件：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To load remote Parquet files via HTTP, pass the URLs instead:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HTTP加载远程Parquet文件，传递URLs：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Arrow
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Arrow
- en: Arrow files are stored in an in-memory columnar format, unlike row-based formats
    like CSV and uncompressed formats like Parquet.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow文件以内存中的列格式存储，不同于基于行的格式如CSV和未压缩格式如Parquet。
- en: 'To load an Arrow file:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 加载Arrow文件：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To load remote Arrow files via HTTP, pass the URLs instead:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HTTP加载远程Arrow文件，传递URLs：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Arrow is the file format used by 🤗 Datasets under the hood, therefore you can
    load a local Arrow file using [Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)
    directly:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow是🤗数据集底层使用的文件格式，因此您可以直接使用[Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)加载本地Arrow文件：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Unlike [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset),
    [Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)
    memory maps the Arrow file without preparing the dataset in the cache, saving
    you disk space. The cache directory to store intermediate processing results will
    be the Arrow file directory in that case.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)不同，[Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)会在不准备缓存中的数据集的情况下内存映射Arrow文件，节省磁盘空间。在这种情况下，用于存储中间处理结果的缓存目录将是Arrow文件目录。
- en: For now only the Arrow streaming format is supported. The Arrow IPC file format
    (also known as Feather V2) is not supported.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 目前仅支持Arrow流格式。不支持Arrow IPC文件格式（也称为Feather V2）。
- en: SQL
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQL
- en: 'Read database contents with [from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)
    by specifying the URI to connect to your database. You can read both table names
    and queries:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定连接到数据库的URI使用[from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)读取数据库内容。您可以读取表名和查询：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For more details, check out the [how to load tabular datasets from SQL databases](tabular_load#databases)
    guide.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请查看[如何从SQL数据库加载表格数据集](tabular_load#databases)指南。
- en: WebDataset
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WebDataset
- en: The [WebDataset](https://github.com/webdataset/webdataset) format is based on
    TAR archives and is suitable for big image datasets. Because of their size, WebDatasets
    are generally loaded in streaming mode (using `streaming=True`).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[WebDataset](https://github.com/webdataset/webdataset)格式基于TAR存档，适用于大型图像数据集。由于其大小，WebDatasets通常以流模式加载（使用`streaming=True`）。'
- en: 'You can load a WebDataset like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以这样加载WebDataset：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To load remote WebDatasets via HTTP, pass the URLs instead:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HTTP加载远程WebDatasets，传递URLs：
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Multiprocessing
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多进程
- en: When a dataset is made of several files (that we call “shards”), it is possible
    to significantly speed up the dataset downloading and preparation step.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集由多个文件（我们称之为“分片”）组成时，可以显著加快数据集下载和准备步骤。
- en: 'You can choose how many processes you’d like to use to prepare a dataset in
    parallel using `num_proc`. In this case, each process is given a subset of shards
    to prepare:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择使用`num_proc`并行准备数据集的进程数。在这种情况下，每个进程将获得一组分片来准备：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In-memory data
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存数据
- en: 🤗 Datasets will also allow you to create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    directly from in-memory data structures like Python dictionaries and Pandas DataFrames.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗数据集还允许您直接从内存数据结构（如Python字典和Pandas DataFrames）创建[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)。
- en: Python dictionary
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python字典
- en: 'Load Python dictionaries with [from_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_dict):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[from_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_dict)加载Python字典：
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Python list of dictionaries
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python字典列表
- en: 'Load a list of Python dictionaries with `from_list()`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`from_list()`加载Python字典列表：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Python generator
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python生成器
- en: 'Create a dataset from a Python generator with [from_generator()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_generator):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[from_generator()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_generator)从Python生成器创建数据集：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This approach supports loading data larger than available memory.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法支持加载大于可用内存的数据。
- en: 'You can also define a sharded dataset by passing lists to `gen_kwargs`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过将列表传递给`gen_kwargs`来定义分片数据集：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Pandas DataFrame
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas DataFrame
- en: 'Load Pandas DataFrames with [from_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_pandas):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[from_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_pandas)加载Pandas数据框：
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: For more details, check out the [how to load tabular datasets from Pandas DataFrames](tabular_load#pandas-dataframes)
    guide.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请查看[如何从Pandas数据框加载表格数据集](tabular_load#pandas-dataframes)指南。
- en: Offline
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线
- en: Even if you don’t have an internet connection, it is still possible to load
    a dataset. As long as you’ve downloaded a dataset from the Hub repository before,
    it should be cached. This means you can reload the dataset from the cache and
    use it offline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有互联网连接，仍然可以加载数据集。只要之前从Hub存储库下载过数据集，它应该已被缓存。这意味着您可以从缓存中重新加载数据集并离线使用。
- en: If you know you won’t have internet access, you can run 🤗 Datasets in full offline
    mode. This saves time because instead of waiting for the Dataset builder download
    to time out, 🤗 Datasets will look directly in the cache. Set the environment variable
    `HF_DATASETS_OFFLINE` to `1` to enable full offline mode.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您知道自己将无法访问互联网，可以在完全离线模式下运行🤗数据集。这样做可以节省时间，因为🤗数据集不会等待数据集构建器下载超时，而是直接查找缓存。将环境变量`HF_DATASETS_OFFLINE`设置为`1`以启用完全离线模式。
- en: Slice splits
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切片拆分
- en: 'You can also choose only to load specific slices of a split. There are two
    options for slicing a split: using strings or the [ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    API. Strings are more compact and readable for simple cases, while [ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    is easier to use with variable slicing parameters.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择仅加载拆分的特定切片。有两种切分拆分的选项：使用字符串或[ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    API。对于简单情况，字符串更紧凑且易读，而[ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)更容易使用具有可变切片参数。
- en: 'Concatenate a `train` and `test` split by:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接`train`和`test`拆分来连接：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")
    Or select a percentage of a split with:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")
    或选择拆分的百分比：'
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")
    Finally, you can even create cross-validated splits. The example below creates
    10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the
    training dataset makes up the remaining complementary 90% chunk:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")
    最后，您甚至可以创建交叉验证拆分。下面的示例创建了10折交叉验证拆分。每个验证数据集是10%的块，训练数据集占剩余的补充90%的块：'
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 19 records, from 500 (included) to 519 (excluded).
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19条记录，从500（包括）到519（不包括）。
- en: '>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")'
- en: 20 records, from 519 (included) to 539 (excluded).
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20条记录，从519（包括）到539（不包括）。
- en: '>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")'
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 18 records, from 450 (included) to 468 (excluded).
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18条记录，从450（包括）到468（不包括）。
- en: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",
    from_=50, to=52, unit="%", rounding="pct1_dropremainder"))'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",
    from_=50, to=52, unit="%", rounding="pct1_dropremainder"))'
- en: 18 records, from 468 (included) to 486 (excluded).
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18条记录，从468（包括）到486（不包括）。
- en: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52,
    to=54, unit="%", rounding="pct1_dropremainder"))'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52,
    to=54, unit="%", rounding="pct1_dropremainder"))'
- en: 'Or equivalently:'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 或者等效地：
- en: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")'
- en: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")'
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '>>> dataset = load_dataset("matinf", "summarization")'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> 数据集 = load_dataset("matinf", "summarization")'
- en: 'Downloading and preparing dataset matinf/summarization (download: Unknown size,
    generated: 246.89 MiB, post-processed: Unknown size, total: 246.89 MiB) to /root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下载和准备数据集matinf/summarization（下载：未知大小，生成：246.89 MiB，后处理：未知大小，总共：246.89 MiB）到/root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
- en: 'AssertionError: The dataset matinf with config summarization requires manual
    data.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 断言错误：配置summarization的数据集matinf需要手动数据。
- en: 'Please follow the manual download instructions: To use MATINF you have to download
    it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9).
    You will receive a download link and a password once you complete the form. Please
    extract all files in one folder and load the dataset with: *datasets.load_dataset(''matinf'',
    data_dir=''path/to/folder/folder_name'')*.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照手动下载说明操作：要使用MATINF，您必须手动下载。请填写此google表格（https://forms.gle/nkH4LVE4iNQeDzsc9）。完成表格后，您将收到下载链接和密码。请将所有文件提取到一个文件夹中，并使用以下方式加载数据集：*datasets.load_dataset('matinf',
    data_dir='path/to/folder/folder_name')*。
- en: Manual data can be loaded with `datasets.load_dataset(matinf, data_dir='<path/to/manual/data>')
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`datasets.load_dataset(matinf, data_dir='<path/to/manual/data>')`加载手动数据
- en: '[PRE32]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]'
- en: '>>> emotion_features = Features({''text'': Value(''string''), ''label'': ClassLabel(names=class_names)})'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> 情感特征 = Features({''text'': Value(''string''), ''label'': ClassLabel(names=class_names)})'
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '>>> dataset = load_dataset(''csv'', data_files=file_dict, delimiter='';'',
    column_names=[''text'', ''label''], features=emotion_features)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> dataset = load_dataset(''csv'', data_files=file_dict, delimiter='';'',
    column_names=[''text'', ''label''], features=emotion_features)'
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '>>> dataset[''train''].features'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> dataset[''train''].features'
- en: '{''text'': Value(dtype=''string'', id=None),'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '{''text'': Value(dtype=''string'', id=None),'
- en: '''label'': ClassLabel(num_classes=6, names=[''sadness'', ''joy'', ''love'',
    ''anger'', ''fear'', ''surprise''], names_file=None, id=None)}'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '''label'': ClassLabel(num_classes=6, names=[''sadness'', ''joy'', ''love'',
    ''anger'', ''fear'', ''surprise''], names_file=None, id=None)}'
- en: '[PRE35]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '>>> from datasets import load_metric'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集导入加载度量
- en: '>>> metric = load_metric(''PATH/TO/MY/METRIC/SCRIPT'')'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 度量=load_metric('PATH/TO/MY/METRIC/SCRIPT')
- en: '>>> # Example of typical usage'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '# 典型用法示例'
- en: '>>> for batch in dataset:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中的每个批次：
- en: '...     inputs, references = batch'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 输入、参考=批次
- en: '...     predictions = model(inputs)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 预测=model(inputs)
- en: '...     metric.add_batch(predictions=predictions, references=references)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 度量.add_batch(predictions=predictions, references=references)
- en: '>>> score = metric.compute()'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 得分=度量计算()
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '>>> from datasets import load_metric'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集导入加载度量
- en: '>>> metric = load_metric(''bleurt'', name=''bleurt-base-128'')'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 度量=load_metric('bleurt', name='bleurt-base-128')
- en: '>>> metric = load_metric(''bleurt'', name=''bleurt-base-512'')'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 度量=load_metric('bleurt', name='bleurt-base-512')
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '>>> from datasets import load_metric'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集导入加载度量
- en: '>>> metric = load_metric(''glue'', ''mrpc'', num_process=num_process, process_id=rank)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 度量=load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)
- en: '[PRE38]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '>>> from datasets import load_metric'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集导入加载度量
- en: '>>> metric = load_metric(''glue'', ''mrpc'', num_process=num_process, process_id=process_id,
    experiment_id="My_experiment_10")'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 度量=load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id,
    experiment_id="My_experiment_10")
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
