- en: Load
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ è½½
- en: 'Original text: [https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Your data can be stored in various places; they can be on your local machineâ€™s
    disk, in a Github repository, and in in-memory data structures like Python dictionaries
    and Pandas DataFrames. Wherever a dataset is stored, ğŸ¤— Datasets can help you load
    it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„æ•°æ®å¯ä»¥å­˜å‚¨åœ¨å„ç§ä½ç½®ï¼›å®ƒä»¬å¯ä»¥åœ¨æœ¬åœ°è®¡ç®—æœºçš„ç£ç›˜ä¸Šã€åœ¨Githubå­˜å‚¨åº“ä¸­ä»¥åŠåœ¨Pythonå­—å…¸å’ŒPandas DataFramesç­‰å†…å­˜æ•°æ®ç»“æ„ä¸­ã€‚æ— è®ºæ•°æ®é›†å­˜å‚¨åœ¨å“ªé‡Œï¼ŒğŸ¤—
    æ•°æ®é›†éƒ½å¯ä»¥å¸®åŠ©æ‚¨åŠ è½½å®ƒã€‚
- en: 'This guide will show you how to load a dataset from:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä»ä»¥ä¸‹ä½ç½®åŠ è½½æ•°æ®é›†ï¼š
- en: The Hub without a dataset loading script
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰æ•°æ®é›†åŠ è½½è„šæœ¬çš„Hub
- en: Local loading script
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ¬åœ°åŠ è½½è„šæœ¬
- en: Local files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ¬åœ°æ–‡ä»¶
- en: In-memory data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å­˜æ•°æ®
- en: Offline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¦»çº¿
- en: A specific slice of a split
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†å‰²çš„ç‰¹å®šåˆ‡ç‰‡
- en: For more details specific to loading other dataset modalities, take a look at
    the [load audio dataset guide](./audio_load), the [load image dataset guide](./image_load),
    or the [load text dataset guide](./nlp_load).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³åŠ è½½å…¶ä»–æ•°æ®é›†æ¨¡æ€çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[åŠ è½½éŸ³é¢‘æ•°æ®é›†æŒ‡å—](./audio_load)ã€[åŠ è½½å›¾åƒæ•°æ®é›†æŒ‡å—](./image_load)æˆ–[åŠ è½½æ–‡æœ¬æ•°æ®é›†æŒ‡å—](./nlp_load)ã€‚
- en: Hugging Face Hub
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face Hub
- en: Datasets are loaded from a dataset loading script that downloads and generates
    the dataset. However, you can also load a dataset from any dataset repository
    on the Hub without a loading script! Begin by [creating a dataset repository](share#create-the-repository)
    and upload your data files. Now you can use the [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    function to load the dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æ˜¯ä»ä¸‹è½½å’Œç”Ÿæˆæ•°æ®é›†çš„æ•°æ®é›†åŠ è½½è„šæœ¬ä¸­åŠ è½½çš„ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä»Hubä¸Šçš„ä»»ä½•æ•°æ®é›†å­˜å‚¨åº“åŠ è½½æ•°æ®é›†è€Œæ— éœ€åŠ è½½è„šæœ¬ï¼é¦–å…ˆ[åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å­˜å‚¨åº“](share#create-the-repository)ï¼Œå¹¶ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶ã€‚ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)å‡½æ•°æ¥åŠ è½½æ•°æ®é›†ã€‚
- en: 'For example, try loading the files from this [demo repository](https://huggingface.co/datasets/lhoestq/demo1)
    by providing the repository namespace and dataset name. This dataset repository
    contains CSV files, and the code below loads the dataset from the CSV files:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå°è¯•é€šè¿‡æä¾›å­˜å‚¨åº“å‘½åç©ºé—´å’Œæ•°æ®é›†åç§°ä»è¿™ä¸ª[demoå­˜å‚¨åº“](https://huggingface.co/datasets/lhoestq/demo1)åŠ è½½æ–‡ä»¶ã€‚è¿™ä¸ªæ•°æ®é›†å­˜å‚¨åº“åŒ…å«CSVæ–‡ä»¶ï¼Œä¸‹é¢çš„ä»£ç ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼š
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Some datasets may have more than one version based on Git tags, branches, or
    commits. Use the `revision` parameter to specify the dataset version you want
    to load:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æŸäº›æ•°æ®é›†å¯èƒ½åŸºäºGitæ ‡ç­¾ã€åˆ†æ”¯æˆ–æäº¤æœ‰å¤šä¸ªç‰ˆæœ¬ã€‚ä½¿ç”¨`revision`å‚æ•°æŒ‡å®šè¦åŠ è½½çš„æ•°æ®é›†ç‰ˆæœ¬ï¼š
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Refer to the [Upload a dataset to the Hub](./upload_dataset) tutorial for more
    details on how to create a dataset repository on the Hub, and how to upload your
    data files.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å¦‚ä½•åœ¨Hubä¸Šåˆ›å»ºæ•°æ®é›†å­˜å‚¨åº“ä»¥åŠå¦‚ä½•ä¸Šä¼ æ•°æ®æ–‡ä»¶çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ[ä¸Šä¼ æ•°æ®é›†åˆ°Hub](./upload_dataset)æ•™ç¨‹ã€‚
- en: 'A dataset without a loading script by default loads all the data into the `train`
    split. Use the `data_files` parameter to map data files to splits like `train`,
    `validation` and `test`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œæ²¡æœ‰åŠ è½½è„šæœ¬çš„æ•°æ®é›†å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°`train`åˆ†å‰²ä¸­ã€‚ä½¿ç”¨`data_files`å‚æ•°å°†æ•°æ®æ–‡ä»¶æ˜ å°„åˆ°`train`ã€`validation`å’Œ`test`ç­‰åˆ†å‰²ï¼š
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you donâ€™t specify which data files to use, [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    will return all the data files. This can take a long time if you load a large
    dataset like C4, which is approximately 13TB of data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ²¡æœ‰æŒ‡å®šè¦ä½¿ç”¨å“ªäº›æ•°æ®æ–‡ä»¶ï¼Œ[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)å°†è¿”å›æ‰€æœ‰æ•°æ®æ–‡ä»¶ã€‚å¦‚æœåŠ è½½åƒC4è¿™æ ·çš„å¤§å‹æ•°æ®é›†ï¼Œè¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼ŒC4å¤§çº¦æœ‰13TBçš„æ•°æ®ã€‚
- en: You can also load a specific subset of the files with the `data_files` or `data_dir`
    parameter. These parameters can accept a relative path which resolves to the base
    path corresponding to where the dataset is loaded from.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`data_files`æˆ–`data_dir`å‚æ•°åŠ è½½ç‰¹å®šçš„æ–‡ä»¶å­é›†ã€‚è¿™äº›å‚æ•°å¯ä»¥æ¥å—ä¸€ä¸ªç›¸å¯¹è·¯å¾„ï¼Œè¯¥è·¯å¾„è§£æä¸ºæ•°æ®é›†åŠ è½½çš„åŸºæœ¬è·¯å¾„ã€‚
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `split` parameter can also map a data file to a specific split:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`split`å‚æ•°è¿˜å¯ä»¥å°†æ•°æ®æ–‡ä»¶æ˜ å°„åˆ°ç‰¹å®šçš„åˆ†å‰²ï¼š'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Local loading script
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬åœ°åŠ è½½è„šæœ¬
- en: 'You may have a ğŸ¤— Datasets loading script locally on your computer. In this
    case, load the dataset by passing one of the following paths to [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½åœ¨è®¡ç®—æœºä¸Šæœ¬åœ°æ‹¥æœ‰ä¸€ä¸ªğŸ¤— æ•°æ®é›†åŠ è½½è„šæœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šè¿‡å°†ä»¥ä¸‹è·¯å¾„ä¹‹ä¸€ä¼ é€’ç»™[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)æ¥åŠ è½½æ•°æ®é›†ï¼š
- en: The local path to the loading script file.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ è½½è„šæœ¬æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚
- en: The local path to the directory containing the loading script file (only if
    the script file has the same name as the directory).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…å«åŠ è½½è„šæœ¬æ–‡ä»¶çš„ç›®å½•çš„æœ¬åœ°è·¯å¾„ï¼ˆä»…å½“è„šæœ¬æ–‡ä»¶ä¸ç›®å½•åŒåæ—¶ï¼‰ã€‚
- en: 'Pass `trust_remote_code=True` to allow ğŸ¤— Datasets to execute the loading script:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ é€’`trust_remote_code=True`ä»¥å…è®¸ğŸ¤— æ•°æ®é›†æ‰§è¡ŒåŠ è½½è„šæœ¬ï¼š
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Edit loading script
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–è¾‘åŠ è½½è„šæœ¬
- en: 'You can also edit a loading script from the Hub to add your own modifications.
    Download the dataset repository locally so any data files referenced by a relative
    path in the loading script can be loaded:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ç¼–è¾‘Hubä¸­çš„åŠ è½½è„šæœ¬ä»¥æ·»åŠ è‡ªå·±çš„ä¿®æ”¹ã€‚å°†æ•°æ®é›†å­˜å‚¨åº“ä¸‹è½½åˆ°æœ¬åœ°ï¼Œä»¥ä¾¿åŠ è½½è„šæœ¬ä¸­ç›¸å¯¹è·¯å¾„å¼•ç”¨çš„ä»»ä½•æ•°æ®æ–‡ä»¶éƒ½å¯ä»¥è¢«åŠ è½½ï¼š
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Make your edits to the loading script and then load it by passing its local
    path to [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–è¾‘åŠ è½½è„šæœ¬ï¼Œç„¶åé€šè¿‡å°†å…¶æœ¬åœ°è·¯å¾„ä¼ é€’ç»™[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)æ¥åŠ è½½å®ƒï¼š
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Local and remote files
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬åœ°å’Œè¿œç¨‹æ–‡ä»¶
- en: Datasets can be loaded from local files stored on your computer and from remote
    files. The datasets are most likely stored as a `csv`, `json`, `txt` or `parquet`
    file. The [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    function can load each of these file types.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å¯ä»¥ä»å­˜å‚¨åœ¨è®¡ç®—æœºä¸Šçš„æœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹æ–‡ä»¶åŠ è½½ã€‚è¿™äº›æ•°æ®é›†å¾ˆå¯èƒ½å­˜å‚¨ä¸º`csv`ã€`json`ã€`txt`æˆ–`parquet`æ–‡ä»¶ã€‚[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)å‡½æ•°å¯ä»¥åŠ è½½æ¯ç§æ–‡ä»¶ç±»å‹ã€‚
- en: CSV
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CSV
- en: 'ğŸ¤— Datasets can read a dataset made up of one or several CSV files (in this
    case, pass your CSV files as a list):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— æ•°æ®é›†å¯ä»¥è¯»å–ç”±ä¸€ä¸ªæˆ–å¤šä¸ªCSVæ–‡ä»¶ç»„æˆçš„æ•°æ®é›†ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†CSVæ–‡ä»¶ä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼‰ï¼š
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For more details, check out the [how to load tabular datasets from CSV files](tabular_load#csv-files)
    guide.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å¦‚ä½•ä»CSVæ–‡ä»¶åŠ è½½è¡¨æ ¼æ•°æ®é›†](tabular_load#csv-files)æŒ‡å—ã€‚
- en: JSON
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JSON
- en: 'JSON files are loaded directly with [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    as shown below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: JSONæ–‡ä»¶ç›´æ¥ä½¿ç”¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)åŠ è½½ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'JSON files have diverse formats, but we think the most efficient format is
    to have multiple JSON objects; each line represents an individual row of data.
    For example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: JSONæ–‡ä»¶æœ‰å„ç§æ ¼å¼ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºæœ€æœ‰æ•ˆçš„æ ¼å¼æ˜¯å…·æœ‰å¤šä¸ªJSONå¯¹è±¡ï¼›æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ•°æ®è¡Œã€‚ä¾‹å¦‚ï¼š
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Another JSON format you may encounter is a nested field, in which case youâ€™ll
    need to specify the `field` argument as shown in the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½ä¼šé‡åˆ°çš„å¦ä¸€ç§JSONæ ¼å¼æ˜¯åµŒå¥—å­—æ®µï¼Œè¿™ç§æƒ…å†µä¸‹æ‚¨éœ€è¦åƒä¸‹é¢è¿™æ ·æŒ‡å®š`field`å‚æ•°ï¼š
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To load remote JSON files via HTTP, pass the URLs instead:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡HTTPåŠ è½½è¿œç¨‹JSONæ–‡ä»¶ï¼Œä¼ é€’URLsï¼š
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: While these are the most common JSON formats, youâ€™ll see other datasets that
    are formatted differently. ğŸ¤— Datasets recognizes these other formats and will
    fallback accordingly on the Python JSON loading methods to handle them.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›æ˜¯æœ€å¸¸è§çš„JSONæ ¼å¼ï¼Œä½†æ‚¨ä¼šçœ‹åˆ°å…¶ä»–æ ¼å¼ä¸åŒçš„æ•°æ®é›†ã€‚ğŸ¤—æ•°æ®é›†è¯†åˆ«è¿™äº›å…¶ä»–æ ¼å¼ï¼Œå¹¶å°†ç›¸åº”åœ°å›é€€åˆ°Python JSONåŠ è½½æ–¹æ³•æ¥å¤„ç†å®ƒä»¬ã€‚
- en: Parquet
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parquet
- en: Parquet files are stored in a columnar format, unlike row-based files like a
    CSV. Large datasets may be stored in a Parquet file because it is more efficient
    and faster at returning your query.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Parquetæ–‡ä»¶ä»¥åˆ—æ ¼å¼å­˜å‚¨ï¼Œä¸åŒäºåƒCSVè¿™æ ·çš„åŸºäºè¡Œçš„æ–‡ä»¶ã€‚ç”±äºå…¶æ›´é«˜æ•ˆå’Œæ›´å¿«é€Ÿåœ°è¿”å›æŸ¥è¯¢ç»“æœï¼Œå¤§å‹æ•°æ®é›†å¯èƒ½å­˜å‚¨åœ¨Parquetæ–‡ä»¶ä¸­ã€‚
- en: 'To load a Parquet file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½Parquetæ–‡ä»¶ï¼š
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To load remote Parquet files via HTTP, pass the URLs instead:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡HTTPåŠ è½½è¿œç¨‹Parquetæ–‡ä»¶ï¼Œä¼ é€’URLsï¼š
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Arrow
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Arrow
- en: Arrow files are stored in an in-memory columnar format, unlike row-based formats
    like CSV and uncompressed formats like Parquet.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Arrowæ–‡ä»¶ä»¥å†…å­˜ä¸­çš„åˆ—æ ¼å¼å­˜å‚¨ï¼Œä¸åŒäºåŸºäºè¡Œçš„æ ¼å¼å¦‚CSVå’Œæœªå‹ç¼©æ ¼å¼å¦‚Parquetã€‚
- en: 'To load an Arrow file:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½Arrowæ–‡ä»¶ï¼š
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To load remote Arrow files via HTTP, pass the URLs instead:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡HTTPåŠ è½½è¿œç¨‹Arrowæ–‡ä»¶ï¼Œä¼ é€’URLsï¼š
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Arrow is the file format used by ğŸ¤— Datasets under the hood, therefore you can
    load a local Arrow file using [Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)
    directly:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Arrowæ˜¯ğŸ¤—æ•°æ®é›†åº•å±‚ä½¿ç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œå› æ­¤æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨[Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)åŠ è½½æœ¬åœ°Arrowæ–‡ä»¶ï¼š
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Unlike [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset),
    [Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)
    memory maps the Arrow file without preparing the dataset in the cache, saving
    you disk space. The cache directory to store intermediate processing results will
    be the Arrow file directory in that case.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)ä¸åŒï¼Œ[Dataset.from_file()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_file)ä¼šåœ¨ä¸å‡†å¤‡ç¼“å­˜ä¸­çš„æ•°æ®é›†çš„æƒ…å†µä¸‹å†…å­˜æ˜ å°„Arrowæ–‡ä»¶ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨äºå­˜å‚¨ä¸­é—´å¤„ç†ç»“æœçš„ç¼“å­˜ç›®å½•å°†æ˜¯Arrowæ–‡ä»¶ç›®å½•ã€‚
- en: For now only the Arrow streaming format is supported. The Arrow IPC file format
    (also known as Feather V2) is not supported.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ä»…æ”¯æŒArrowæµæ ¼å¼ã€‚ä¸æ”¯æŒArrow IPCæ–‡ä»¶æ ¼å¼ï¼ˆä¹Ÿç§°ä¸ºFeather V2ï¼‰ã€‚
- en: SQL
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQL
- en: 'Read database contents with [from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)
    by specifying the URI to connect to your database. You can read both table names
    and queries:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æŒ‡å®šè¿æ¥åˆ°æ•°æ®åº“çš„URIä½¿ç”¨[from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)è¯»å–æ•°æ®åº“å†…å®¹ã€‚æ‚¨å¯ä»¥è¯»å–è¡¨åå’ŒæŸ¥è¯¢ï¼š
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For more details, check out the [how to load tabular datasets from SQL databases](tabular_load#databases)
    guide.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å¦‚ä½•ä»SQLæ•°æ®åº“åŠ è½½è¡¨æ ¼æ•°æ®é›†](tabular_load#databases)æŒ‡å—ã€‚
- en: WebDataset
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WebDataset
- en: The [WebDataset](https://github.com/webdataset/webdataset) format is based on
    TAR archives and is suitable for big image datasets. Because of their size, WebDatasets
    are generally loaded in streaming mode (using `streaming=True`).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[WebDataset](https://github.com/webdataset/webdataset)æ ¼å¼åŸºäºTARå­˜æ¡£ï¼Œé€‚ç”¨äºå¤§å‹å›¾åƒæ•°æ®é›†ã€‚ç”±äºå…¶å¤§å°ï¼ŒWebDatasetsé€šå¸¸ä»¥æµæ¨¡å¼åŠ è½½ï¼ˆä½¿ç”¨`streaming=True`ï¼‰ã€‚'
- en: 'You can load a WebDataset like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥è¿™æ ·åŠ è½½WebDatasetï¼š
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To load remote WebDatasets via HTTP, pass the URLs instead:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡HTTPåŠ è½½è¿œç¨‹WebDatasetsï¼Œä¼ é€’URLsï¼š
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Multiprocessing
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šè¿›ç¨‹
- en: When a dataset is made of several files (that we call â€œshardsâ€), it is possible
    to significantly speed up the dataset downloading and preparation step.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ•°æ®é›†ç”±å¤šä¸ªæ–‡ä»¶ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåˆ†ç‰‡â€ï¼‰ç»„æˆæ—¶ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«æ•°æ®é›†ä¸‹è½½å’Œå‡†å¤‡æ­¥éª¤ã€‚
- en: 'You can choose how many processes youâ€™d like to use to prepare a dataset in
    parallel using `num_proc`. In this case, each process is given a subset of shards
    to prepare:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨`num_proc`å¹¶è¡Œå‡†å¤‡æ•°æ®é›†çš„è¿›ç¨‹æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè¿›ç¨‹å°†è·å¾—ä¸€ç»„åˆ†ç‰‡æ¥å‡†å¤‡ï¼š
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In-memory data
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜æ•°æ®
- en: ğŸ¤— Datasets will also allow you to create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    directly from in-memory data structures like Python dictionaries and Pandas DataFrames.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—æ•°æ®é›†è¿˜å…è®¸æ‚¨ç›´æ¥ä»å†…å­˜æ•°æ®ç»“æ„ï¼ˆå¦‚Pythonå­—å…¸å’ŒPandas DataFramesï¼‰åˆ›å»º[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)ã€‚
- en: Python dictionary
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pythonå­—å…¸
- en: 'Load Python dictionaries with [from_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_dict):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[from_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_dict)åŠ è½½Pythonå­—å…¸ï¼š
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Python list of dictionaries
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pythonå­—å…¸åˆ—è¡¨
- en: 'Load a list of Python dictionaries with `from_list()`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`from_list()`åŠ è½½Pythonå­—å…¸åˆ—è¡¨ï¼š
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Python generator
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pythonç”Ÿæˆå™¨
- en: 'Create a dataset from a Python generator with [from_generator()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_generator):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[from_generator()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_generator)ä»Pythonç”Ÿæˆå™¨åˆ›å»ºæ•°æ®é›†ï¼š
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This approach supports loading data larger than available memory.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æ”¯æŒåŠ è½½å¤§äºå¯ç”¨å†…å­˜çš„æ•°æ®ã€‚
- en: 'You can also define a sharded dataset by passing lists to `gen_kwargs`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€šè¿‡å°†åˆ—è¡¨ä¼ é€’ç»™`gen_kwargs`æ¥å®šä¹‰åˆ†ç‰‡æ•°æ®é›†ï¼š
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Pandas DataFrame
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas DataFrame
- en: 'Load Pandas DataFrames with [from_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_pandas):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[from_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_pandas)åŠ è½½Pandasæ•°æ®æ¡†ï¼š
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: For more details, check out the [how to load tabular datasets from Pandas DataFrames](tabular_load#pandas-dataframes)
    guide.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å¦‚ä½•ä»Pandasæ•°æ®æ¡†åŠ è½½è¡¨æ ¼æ•°æ®é›†](tabular_load#pandas-dataframes)æŒ‡å—ã€‚
- en: Offline
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¦»çº¿
- en: Even if you donâ€™t have an internet connection, it is still possible to load
    a dataset. As long as youâ€™ve downloaded a dataset from the Hub repository before,
    it should be cached. This means you can reload the dataset from the cache and
    use it offline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ²¡æœ‰äº’è”ç½‘è¿æ¥ï¼Œä»ç„¶å¯ä»¥åŠ è½½æ•°æ®é›†ã€‚åªè¦ä¹‹å‰ä»Hubå­˜å‚¨åº“ä¸‹è½½è¿‡æ•°æ®é›†ï¼Œå®ƒåº”è¯¥å·²è¢«ç¼“å­˜ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä»ç¼“å­˜ä¸­é‡æ–°åŠ è½½æ•°æ®é›†å¹¶ç¦»çº¿ä½¿ç”¨ã€‚
- en: If you know you wonâ€™t have internet access, you can run ğŸ¤— Datasets in full offline
    mode. This saves time because instead of waiting for the Dataset builder download
    to time out, ğŸ¤— Datasets will look directly in the cache. Set the environment variable
    `HF_DATASETS_OFFLINE` to `1` to enable full offline mode.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çŸ¥é“è‡ªå·±å°†æ— æ³•è®¿é—®äº’è”ç½‘ï¼Œå¯ä»¥åœ¨å®Œå…¨ç¦»çº¿æ¨¡å¼ä¸‹è¿è¡ŒğŸ¤—æ•°æ®é›†ã€‚è¿™æ ·åšå¯ä»¥èŠ‚çœæ—¶é—´ï¼Œå› ä¸ºğŸ¤—æ•°æ®é›†ä¸ä¼šç­‰å¾…æ•°æ®é›†æ„å»ºå™¨ä¸‹è½½è¶…æ—¶ï¼Œè€Œæ˜¯ç›´æ¥æŸ¥æ‰¾ç¼“å­˜ã€‚å°†ç¯å¢ƒå˜é‡`HF_DATASETS_OFFLINE`è®¾ç½®ä¸º`1`ä»¥å¯ç”¨å®Œå…¨ç¦»çº¿æ¨¡å¼ã€‚
- en: Slice splits
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ‡ç‰‡æ‹†åˆ†
- en: 'You can also choose only to load specific slices of a split. There are two
    options for slicing a split: using strings or the [ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    API. Strings are more compact and readable for simple cases, while [ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    is easier to use with variable slicing parameters.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€‰æ‹©ä»…åŠ è½½æ‹†åˆ†çš„ç‰¹å®šåˆ‡ç‰‡ã€‚æœ‰ä¸¤ç§åˆ‡åˆ†æ‹†åˆ†çš„é€‰é¡¹ï¼šä½¿ç”¨å­—ç¬¦ä¸²æˆ–[ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)
    APIã€‚å¯¹äºç®€å•æƒ…å†µï¼Œå­—ç¬¦ä¸²æ›´ç´§å‡‘ä¸”æ˜“è¯»ï¼Œè€Œ[ReadInstruction](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ReadInstruction)æ›´å®¹æ˜“ä½¿ç”¨å…·æœ‰å¯å˜åˆ‡ç‰‡å‚æ•°ã€‚
- en: 'Concatenate a `train` and `test` split by:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿æ¥`train`å’Œ`test`æ‹†åˆ†æ¥è¿æ¥ï¼š
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")
    Or select a percentage of a split with:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")
    æˆ–é€‰æ‹©æ‹†åˆ†çš„ç™¾åˆ†æ¯”ï¼š'
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")
    Finally, you can even create cross-validated splits. The example below creates
    10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the
    training dataset makes up the remaining complementary 90% chunk:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")
    æœ€åï¼Œæ‚¨ç”šè‡³å¯ä»¥åˆ›å»ºäº¤å‰éªŒè¯æ‹†åˆ†ã€‚ä¸‹é¢çš„ç¤ºä¾‹åˆ›å»ºäº†10æŠ˜äº¤å‰éªŒè¯æ‹†åˆ†ã€‚æ¯ä¸ªéªŒè¯æ•°æ®é›†æ˜¯10%çš„å—ï¼Œè®­ç»ƒæ•°æ®é›†å å‰©ä½™çš„è¡¥å……90%çš„å—ï¼š'
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 19 records, from 500 (included) to 519 (excluded).
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19æ¡è®°å½•ï¼Œä»500ï¼ˆåŒ…æ‹¬ï¼‰åˆ°519ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚
- en: '>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")'
- en: 20 records, from 519 (included) to 539 (excluded).
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20æ¡è®°å½•ï¼Œä»519ï¼ˆåŒ…æ‹¬ï¼‰åˆ°539ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚
- en: '>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")'
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 18 records, from 450 (included) to 468 (excluded).
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18æ¡è®°å½•ï¼Œä»450ï¼ˆåŒ…æ‹¬ï¼‰åˆ°468ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚
- en: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",
    from_=50, to=52, unit="%", rounding="pct1_dropremainder"))'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",
    from_=50, to=52, unit="%", rounding="pct1_dropremainder"))'
- en: 18 records, from 468 (included) to 486 (excluded).
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18æ¡è®°å½•ï¼Œä»468ï¼ˆåŒ…æ‹¬ï¼‰åˆ°486ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚
- en: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52,
    to=54, unit="%", rounding="pct1_dropremainder"))'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52,
    to=54, unit="%", rounding="pct1_dropremainder"))'
- en: 'Or equivalently:'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ–è€…ç­‰æ•ˆåœ°ï¼š
- en: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")'
- en: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")'
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '>>> dataset = load_dataset("matinf", "summarization")'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> æ•°æ®é›† = load_dataset("matinf", "summarization")'
- en: 'Downloading and preparing dataset matinf/summarization (download: Unknown size,
    generated: 246.89 MiB, post-processed: Unknown size, total: 246.89 MiB) to /root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è½½å’Œå‡†å¤‡æ•°æ®é›†matinf/summarizationï¼ˆä¸‹è½½ï¼šæœªçŸ¥å¤§å°ï¼Œç”Ÿæˆï¼š246.89 MiBï¼Œåå¤„ç†ï¼šæœªçŸ¥å¤§å°ï¼Œæ€»å…±ï¼š246.89 MiBï¼‰åˆ°/root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
- en: 'AssertionError: The dataset matinf with config summarization requires manual
    data.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ–­è¨€é”™è¯¯ï¼šé…ç½®summarizationçš„æ•°æ®é›†matinféœ€è¦æ‰‹åŠ¨æ•°æ®ã€‚
- en: 'Please follow the manual download instructions: To use MATINF you have to download
    it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9).
    You will receive a download link and a password once you complete the form. Please
    extract all files in one folder and load the dataset with: *datasets.load_dataset(''matinf'',
    data_dir=''path/to/folder/folder_name'')*.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŒ‰ç…§æ‰‹åŠ¨ä¸‹è½½è¯´æ˜æ“ä½œï¼šè¦ä½¿ç”¨MATINFï¼Œæ‚¨å¿…é¡»æ‰‹åŠ¨ä¸‹è½½ã€‚è¯·å¡«å†™æ­¤googleè¡¨æ ¼ï¼ˆhttps://forms.gle/nkH4LVE4iNQeDzsc9ï¼‰ã€‚å®Œæˆè¡¨æ ¼åï¼Œæ‚¨å°†æ”¶åˆ°ä¸‹è½½é“¾æ¥å’Œå¯†ç ã€‚è¯·å°†æ‰€æœ‰æ–‡ä»¶æå–åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹æ–¹å¼åŠ è½½æ•°æ®é›†ï¼š*datasets.load_dataset('matinf',
    data_dir='path/to/folder/folder_name')*ã€‚
- en: Manual data can be loaded with `datasets.load_dataset(matinf, data_dir='<path/to/manual/data>')
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨`datasets.load_dataset(matinf, data_dir='<path/to/manual/data>')`åŠ è½½æ‰‹åŠ¨æ•°æ®
- en: '[PRE32]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]'
- en: '>>> emotion_features = Features({''text'': Value(''string''), ''label'': ClassLabel(names=class_names)})'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> æƒ…æ„Ÿç‰¹å¾ = Features({''text'': Value(''string''), ''label'': ClassLabel(names=class_names)})'
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '>>> dataset = load_dataset(''csv'', data_files=file_dict, delimiter='';'',
    column_names=[''text'', ''label''], features=emotion_features)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> dataset = load_dataset(''csv'', data_files=file_dict, delimiter='';'',
    column_names=[''text'', ''label''], features=emotion_features)'
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '>>> dataset[''train''].features'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> dataset[''train''].features'
- en: '{''text'': Value(dtype=''string'', id=None),'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '{''text'': Value(dtype=''string'', id=None),'
- en: '''label'': ClassLabel(num_classes=6, names=[''sadness'', ''joy'', ''love'',
    ''anger'', ''fear'', ''surprise''], names_file=None, id=None)}'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '''label'': ClassLabel(num_classes=6, names=[''sadness'', ''joy'', ''love'',
    ''anger'', ''fear'', ''surprise''], names_file=None, id=None)}'
- en: '[PRE35]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '>>> from datasets import load_metric'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡
- en: '>>> metric = load_metric(''PATH/TO/MY/METRIC/SCRIPT'')'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡=load_metric('PATH/TO/MY/METRIC/SCRIPT')
- en: '>>> # Example of typical usage'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '# å…¸å‹ç”¨æ³•ç¤ºä¾‹'
- en: '>>> for batch in dataset:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡ï¼š
- en: '...     inputs, references = batch'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥ã€å‚è€ƒ=æ‰¹æ¬¡
- en: '...     predictions = model(inputs)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹=model(inputs)
- en: '...     metric.add_batch(predictions=predictions, references=references)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡.add_batch(predictions=predictions, references=references)
- en: '>>> score = metric.compute()'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—åˆ†=åº¦é‡è®¡ç®—()
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '>>> from datasets import load_metric'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡
- en: '>>> metric = load_metric(''bleurt'', name=''bleurt-base-128'')'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡=load_metric('bleurt', name='bleurt-base-128')
- en: '>>> metric = load_metric(''bleurt'', name=''bleurt-base-512'')'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡=load_metric('bleurt', name='bleurt-base-512')
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '>>> from datasets import load_metric'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡
- en: '>>> metric = load_metric(''glue'', ''mrpc'', num_process=num_process, process_id=rank)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡=load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)
- en: '[PRE38]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '>>> from datasets import load_metric'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡
- en: '>>> metric = load_metric(''glue'', ''mrpc'', num_process=num_process, process_id=process_id,
    experiment_id="My_experiment_10")'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡=load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id,
    experiment_id="My_experiment_10")
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
