- en: Overview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'Original text: [https://huggingface.co/docs/api-inference/quicktour](https://huggingface.co/docs/api-inference/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/api-inference/quicktour](https://huggingface.co/docs/api-inference/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a quick look at the 🤗 Hosted Inference API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下🤗托管推理API。
- en: 'Main features:'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要特点：
- en: Leverage **150,000+ Transformer, Diffusers, or Timm models** (T5, Blenderbot,
    Bart, GPT-2, Pegasus...)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用**150,000多个Transformer、Diffusers或Timm模型**（T5、Blenderbot、Bart、GPT-2、Pegasus...）
- en: Upload, manage and serve your **own models privately**
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传、管理和私密地提供您的**自有模型**
- en: Run Classification, NER, Conversational, Summarization, Translation, Question-Answering,
    Embeddings Extraction tasks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行分类、NER、对话、摘要、翻译、问答、嵌入提取任务
- en: Get up to **10x inference speedup** to reduce user latency
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得**高达10倍的推理加速**以减少用户延迟
- en: Accelerated inference for a number of supported models on CPU
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CPU上加速推理的多个支持模型
- en: Run **large models** that are challenging to deploy in production
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行**难以部署的大型模型**
- en: Scale up to 1,000 requests per second with **automatic scaling** built-in
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过内置的**自动扩展**扩展到每秒1,000个请求
- en: '**Ship new NLP, CV, Audio, or RL features faster** as new models become available'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快地推出新的NLP、CV、音频或RL功能**，因为新模型变得可用'
- en: Build your business on a platform powered by the reference open source project
    in ML
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在由ML中的参考开源项目驱动的平台上构建您的业务
- en: Get your API Token
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取您的API令牌
- en: 'To get started you need to:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，您需要：
- en: '[Register](https://huggingface.co/join) or [Login](https://huggingface.co/login).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注册](https://huggingface.co/join)或[登录](https://huggingface.co/login)。'
- en: Get a User Access or API token [in your Hugging Face profile settings](https://huggingface.co/settings/tokens).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取用户访问或API令牌[在您的Hugging Face个人资料设置中](https://huggingface.co/settings/tokens)。
- en: You should see a token `hf_xxxxx` (old tokens are `api_XXXXXXXX` or `api_org_XXXXXXX`).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个令牌`hf_xxxxx`（旧令牌为`api_XXXXXXXX`或`api_org_XXXXXXX`）。
- en: If you do not submit your API token when sending requests to the API, you will
    not be able to run inference on your private models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在向API发送请求时未提交API令牌，则将无法在您的私有模型上运行推理。
- en: Running Inference with API Requests
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用API请求运行推理
- en: The first step is to choose which model you are going to run. Go to the [Model
    Hub](https://huggingface.co/models) and select the model you want to use. If you
    are unsure where to start, make sure to check the [recommended models for each
    ML task](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html#detailed-parameters)
    available, or the [Tasks](https://huggingface.co/tasks) overview.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是选择要运行的模型。转到[模型中心](https://huggingface.co/models)并选择要使用的模型。如果您不确定从哪里开始，请确保检查每个ML任务的[推荐模型](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html#detailed-parameters)，或者[Tasks](https://huggingface.co/tasks)概述。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s use [gpt2](https://huggingface.co/gpt2) as an example. To run inference,
    simply use this code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以[gpt2](https://huggingface.co/gpt2)为例。要运行推理，只需使用此代码：
- en: PythonJavaScriptcURL
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: API Options and Parameters
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API选项和参数
- en: Depending on the task (aka pipeline) the model is configured for, the request
    will accept specific parameters. When sending requests to run any model, API options
    allow you to specify the caching and model loading behavior. All API options and
    parameters are detailed here [`detailed_parameters`](detailed_parameters).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型配置的任务（即管道），请求将接受特定参数。在发送运行任何模型的请求时，API选项允许您指定缓存和模型加载行为。所有API选项和参数在此处详细说明[`详细参数`](详细参数)。
- en: Using CPU-Accelerated Inference
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CPU加速推理
- en: As an API customer, your API token will automatically enable CPU-Accelerated
    inference on your requests if the model type is supported. For instance, if you
    compare gpt2 model inference through our API with CPU-Acceleration, compared to
    running inference on the model out of the box on a local setup, you should measure
    a **~10x speedup**. The specific performance boost depends on the model and input
    payload (and your local hardware).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为API客户，如果支持模型类型，您的API令牌将自动在请求中启用CPU加速推理。例如，如果通过我们的API比较gpt2模型推理与CPU加速，与在本地设置中直接运行模型推理相比，您应该测量到**~10倍的加速**。具体的性能提升取决于模型和输入负载（以及您的本地硬件）。
- en: To verify you are using the CPU-Accelerated version of a model you can check
    the x-compute-type header of your requests, which should be cpu+optimized. If
    you do not see it, it simply means not all optimizations are turned on. This can
    be for various factors; the model might have been added recently to transformers,
    or the model can be optimized in several different ways and the best one depends
    on your use case.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证您是否使用了模型的CPU加速版本，您可以检查请求的x-compute-type标头，该标头应为cpu+optimized。如果您没有看到它，这仅意味着并非所有优化都已打开。这可能是由于各种因素造成的；模型可能是最近添加到transformers中的，或者模型可以以多种不同的方式进行优化，最佳方式取决于您的用例。
- en: If you contact us at [api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co),
    we’ll be able to increase the inference speed for you, depending on your actual
    use case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您联系我们[api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co)，我们将能够根据您的实际用例提高推理速度。
- en: Model Loading and latency
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型加载和延迟
- en: The Hosted Inference API can serve predictions on-demand from over 100,000 models
    deployed on the Hugging Face Hub, dynamically loaded on shared infrastructure.
    If the requested model is not loaded in memory, the Hosted Inference API will
    start by loading the model into memory and returning a 503 response, before it
    can respond with the prediction.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 托管推理API可以从Hugging Face Hub上部署的超过100,000个模型中动态加载共享基础设施上的预测。如果请求的模型未加载到内存中，托管推理API将首先将模型加载到内存中并返回503响应，然后才能响应预测。
- en: If your use case requires large volume or predictable latencies, you can use
    our paid solution [Inference Endpoints](https://huggingface.co/inference-endpoints)
    to easily deploy your models on dedicated, fully-managed infrastructure. With
    Inference Endpoints you can quickly create endpoints on the cloud, region, CPU
    or GPU compute instance of your choice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的使用情况需要大量数据或可预测的延迟，您可以使用我们的付费解决方案[推理端点](https://huggingface.co/inference-endpoints)来轻松部署您的模型在专用、完全托管的基础设施上。使用推理端点，您可以快速在云端、区域、CPU或GPU计算实例上创建端点。
