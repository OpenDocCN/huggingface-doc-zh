- en: Overview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'Original text: [https://huggingface.co/docs/api-inference/quicktour](https://huggingface.co/docs/api-inference/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/api-inference/quicktour](https://huggingface.co/docs/api-inference/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s have a quick look at the ğŸ¤— Hosted Inference API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¿«é€Ÿçœ‹ä¸€ä¸‹ğŸ¤—æ‰˜ç®¡æ¨ç†APIã€‚
- en: 'Main features:'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»è¦ç‰¹ç‚¹ï¼š
- en: Leverage **150,000+ Transformer, Diffusers, or Timm models** (T5, Blenderbot,
    Bart, GPT-2, Pegasus...)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ©ç”¨**150,000å¤šä¸ªTransformerã€Diffusersæˆ–Timmæ¨¡å‹**ï¼ˆT5ã€Blenderbotã€Bartã€GPT-2ã€Pegasus...ï¼‰
- en: Upload, manage and serve your **own models privately**
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šä¼ ã€ç®¡ç†å’Œç§å¯†åœ°æä¾›æ‚¨çš„**è‡ªæœ‰æ¨¡å‹**
- en: Run Classification, NER, Conversational, Summarization, Translation, Question-Answering,
    Embeddings Extraction tasks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œåˆ†ç±»ã€NERã€å¯¹è¯ã€æ‘˜è¦ã€ç¿»è¯‘ã€é—®ç­”ã€åµŒå…¥æå–ä»»åŠ¡
- en: Get up to **10x inference speedup** to reduce user latency
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è·å¾—**é«˜è¾¾10å€çš„æ¨ç†åŠ é€Ÿ**ä»¥å‡å°‘ç”¨æˆ·å»¶è¿Ÿ
- en: Accelerated inference for a number of supported models on CPU
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨CPUä¸ŠåŠ é€Ÿæ¨ç†çš„å¤šä¸ªæ”¯æŒæ¨¡å‹
- en: Run **large models** that are challenging to deploy in production
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œ**éš¾ä»¥éƒ¨ç½²çš„å¤§å‹æ¨¡å‹**
- en: Scale up to 1,000 requests per second with **automatic scaling** built-in
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡å†…ç½®çš„**è‡ªåŠ¨æ‰©å±•**æ‰©å±•åˆ°æ¯ç§’1,000ä¸ªè¯·æ±‚
- en: '**Ship new NLP, CV, Audio, or RL features faster** as new models become available'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ›´å¿«åœ°æ¨å‡ºæ–°çš„NLPã€CVã€éŸ³é¢‘æˆ–RLåŠŸèƒ½**ï¼Œå› ä¸ºæ–°æ¨¡å‹å˜å¾—å¯ç”¨'
- en: Build your business on a platform powered by the reference open source project
    in ML
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç”±MLä¸­çš„å‚è€ƒå¼€æºé¡¹ç›®é©±åŠ¨çš„å¹³å°ä¸Šæ„å»ºæ‚¨çš„ä¸šåŠ¡
- en: Get your API Token
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è·å–æ‚¨çš„APIä»¤ç‰Œ
- en: 'To get started you need to:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ä½¿ç”¨ï¼Œæ‚¨éœ€è¦ï¼š
- en: '[Register](https://huggingface.co/join) or [Login](https://huggingface.co/login).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ³¨å†Œ](https://huggingface.co/join)æˆ–[ç™»å½•](https://huggingface.co/login)ã€‚'
- en: Get a User Access or API token [in your Hugging Face profile settings](https://huggingface.co/settings/tokens).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è·å–ç”¨æˆ·è®¿é—®æˆ–APIä»¤ç‰Œ[åœ¨æ‚¨çš„Hugging Faceä¸ªäººèµ„æ–™è®¾ç½®ä¸­](https://huggingface.co/settings/tokens)ã€‚
- en: You should see a token `hf_xxxxx` (old tokens are `api_XXXXXXXX` or `api_org_XXXXXXX`).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªä»¤ç‰Œ`hf_xxxxx`ï¼ˆæ—§ä»¤ç‰Œä¸º`api_XXXXXXXX`æˆ–`api_org_XXXXXXX`ï¼‰ã€‚
- en: If you do not submit your API token when sending requests to the API, you will
    not be able to run inference on your private models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨å‘APIå‘é€è¯·æ±‚æ—¶æœªæäº¤APIä»¤ç‰Œï¼Œåˆ™å°†æ— æ³•åœ¨æ‚¨çš„ç§æœ‰æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ã€‚
- en: Running Inference with API Requests
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨APIè¯·æ±‚è¿è¡Œæ¨ç†
- en: The first step is to choose which model you are going to run. Go to the [Model
    Hub](https://huggingface.co/models) and select the model you want to use. If you
    are unsure where to start, make sure to check the [recommended models for each
    ML task](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html#detailed-parameters)
    available, or the [Tasks](https://huggingface.co/tasks) overview.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯é€‰æ‹©è¦è¿è¡Œçš„æ¨¡å‹ã€‚è½¬åˆ°[æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)å¹¶é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šä»å“ªé‡Œå¼€å§‹ï¼Œè¯·ç¡®ä¿æ£€æŸ¥æ¯ä¸ªMLä»»åŠ¡çš„[æ¨èæ¨¡å‹](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html#detailed-parameters)ï¼Œæˆ–è€…[Tasks](https://huggingface.co/tasks)æ¦‚è¿°ã€‚
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Letâ€™s use [gpt2](https://huggingface.co/gpt2) as an example. To run inference,
    simply use this code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥[gpt2](https://huggingface.co/gpt2)ä¸ºä¾‹ã€‚è¦è¿è¡Œæ¨ç†ï¼Œåªéœ€ä½¿ç”¨æ­¤ä»£ç ï¼š
- en: PythonJavaScriptcURL
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: API Options and Parameters
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: APIé€‰é¡¹å’Œå‚æ•°
- en: Depending on the task (aka pipeline) the model is configured for, the request
    will accept specific parameters. When sending requests to run any model, API options
    allow you to specify the caching and model loading behavior. All API options and
    parameters are detailed here [`detailed_parameters`](detailed_parameters).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ¨¡å‹é…ç½®çš„ä»»åŠ¡ï¼ˆå³ç®¡é“ï¼‰ï¼Œè¯·æ±‚å°†æ¥å—ç‰¹å®šå‚æ•°ã€‚åœ¨å‘é€è¿è¡Œä»»ä½•æ¨¡å‹çš„è¯·æ±‚æ—¶ï¼ŒAPIé€‰é¡¹å…è®¸æ‚¨æŒ‡å®šç¼“å­˜å’Œæ¨¡å‹åŠ è½½è¡Œä¸ºã€‚æ‰€æœ‰APIé€‰é¡¹å’Œå‚æ•°åœ¨æ­¤å¤„è¯¦ç»†è¯´æ˜[`è¯¦ç»†å‚æ•°`](è¯¦ç»†å‚æ•°)ã€‚
- en: Using CPU-Accelerated Inference
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨CPUåŠ é€Ÿæ¨ç†
- en: As an API customer, your API token will automatically enable CPU-Accelerated
    inference on your requests if the model type is supported. For instance, if you
    compare gpt2 model inference through our API with CPU-Acceleration, compared to
    running inference on the model out of the box on a local setup, you should measure
    a **~10x speedup**. The specific performance boost depends on the model and input
    payload (and your local hardware).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºAPIå®¢æˆ·ï¼Œå¦‚æœæ”¯æŒæ¨¡å‹ç±»å‹ï¼Œæ‚¨çš„APIä»¤ç‰Œå°†è‡ªåŠ¨åœ¨è¯·æ±‚ä¸­å¯ç”¨CPUåŠ é€Ÿæ¨ç†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœé€šè¿‡æˆ‘ä»¬çš„APIæ¯”è¾ƒgpt2æ¨¡å‹æ¨ç†ä¸CPUåŠ é€Ÿï¼Œä¸åœ¨æœ¬åœ°è®¾ç½®ä¸­ç›´æ¥è¿è¡Œæ¨¡å‹æ¨ç†ç›¸æ¯”ï¼Œæ‚¨åº”è¯¥æµ‹é‡åˆ°**~10å€çš„åŠ é€Ÿ**ã€‚å…·ä½“çš„æ€§èƒ½æå‡å–å†³äºæ¨¡å‹å’Œè¾“å…¥è´Ÿè½½ï¼ˆä»¥åŠæ‚¨çš„æœ¬åœ°ç¡¬ä»¶ï¼‰ã€‚
- en: To verify you are using the CPU-Accelerated version of a model you can check
    the x-compute-type header of your requests, which should be cpu+optimized. If
    you do not see it, it simply means not all optimizations are turned on. This can
    be for various factors; the model might have been added recently to transformers,
    or the model can be optimized in several different ways and the best one depends
    on your use case.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¦éªŒè¯æ‚¨æ˜¯å¦ä½¿ç”¨äº†æ¨¡å‹çš„CPUåŠ é€Ÿç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥æ£€æŸ¥è¯·æ±‚çš„x-compute-typeæ ‡å¤´ï¼Œè¯¥æ ‡å¤´åº”ä¸ºcpu+optimizedã€‚å¦‚æœæ‚¨æ²¡æœ‰çœ‹åˆ°å®ƒï¼Œè¿™ä»…æ„å‘³ç€å¹¶éæ‰€æœ‰ä¼˜åŒ–éƒ½å·²æ‰“å¼€ã€‚è¿™å¯èƒ½æ˜¯ç”±äºå„ç§å› ç´ é€ æˆçš„ï¼›æ¨¡å‹å¯èƒ½æ˜¯æœ€è¿‘æ·»åŠ åˆ°transformersä¸­çš„ï¼Œæˆ–è€…æ¨¡å‹å¯ä»¥ä»¥å¤šç§ä¸åŒçš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ä½³æ–¹å¼å–å†³äºæ‚¨çš„ç”¨ä¾‹ã€‚
- en: If you contact us at [api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co),
    weâ€™ll be able to increase the inference speed for you, depending on your actual
    use case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è”ç³»æˆ‘ä»¬[api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co)ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿæ ¹æ®æ‚¨çš„å®é™…ç”¨ä¾‹æé«˜æ¨ç†é€Ÿåº¦ã€‚
- en: Model Loading and latency
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹åŠ è½½å’Œå»¶è¿Ÿ
- en: The Hosted Inference API can serve predictions on-demand from over 100,000 models
    deployed on the Hugging Face Hub, dynamically loaded on shared infrastructure.
    If the requested model is not loaded in memory, the Hosted Inference API will
    start by loading the model into memory and returning a 503 response, before it
    can respond with the prediction.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰˜ç®¡æ¨ç†APIå¯ä»¥ä»Hugging Face Hubä¸Šéƒ¨ç½²çš„è¶…è¿‡100,000ä¸ªæ¨¡å‹ä¸­åŠ¨æ€åŠ è½½å…±äº«åŸºç¡€è®¾æ–½ä¸Šçš„é¢„æµ‹ã€‚å¦‚æœè¯·æ±‚çš„æ¨¡å‹æœªåŠ è½½åˆ°å†…å­˜ä¸­ï¼Œæ‰˜ç®¡æ¨ç†APIå°†é¦–å…ˆå°†æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­å¹¶è¿”å›503å“åº”ï¼Œç„¶åæ‰èƒ½å“åº”é¢„æµ‹ã€‚
- en: If your use case requires large volume or predictable latencies, you can use
    our paid solution [Inference Endpoints](https://huggingface.co/inference-endpoints)
    to easily deploy your models on dedicated, fully-managed infrastructure. With
    Inference Endpoints you can quickly create endpoints on the cloud, region, CPU
    or GPU compute instance of your choice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„ä½¿ç”¨æƒ…å†µéœ€è¦å¤§é‡æ•°æ®æˆ–å¯é¢„æµ‹çš„å»¶è¿Ÿï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ä»˜è´¹è§£å†³æ–¹æ¡ˆ[æ¨ç†ç«¯ç‚¹](https://huggingface.co/inference-endpoints)æ¥è½»æ¾éƒ¨ç½²æ‚¨çš„æ¨¡å‹åœ¨ä¸“ç”¨ã€å®Œå…¨æ‰˜ç®¡çš„åŸºç¡€è®¾æ–½ä¸Šã€‚ä½¿ç”¨æ¨ç†ç«¯ç‚¹ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿåœ¨äº‘ç«¯ã€åŒºåŸŸã€CPUæˆ–GPUè®¡ç®—å®ä¾‹ä¸Šåˆ›å»ºç«¯ç‚¹ã€‚
