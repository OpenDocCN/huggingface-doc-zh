- en: ConvNeXT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ConvNeXT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnext](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnext)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnext](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnext)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The ConvNeXT model was proposed in [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)
    by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
    Saining Xie. ConvNeXT is a pure convolutional model (ConvNet), inspired by the
    design of Vision Transformers, that claims to outperform them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNeXT模型是由Zhuang Liu、Hanzi Mao、Chao-Yuan Wu、Christoph Feichtenhofer、Trevor
    Darrell、Saining Xie在[2020年的ConvNet](https://arxiv.org/abs/2201.03545)中提出的。ConvNeXT是一种纯卷积模型（ConvNet），受到Vision
    Transformers设计的启发，声称胜过它们。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*The “Roaring 20s” of visual recognition began with the introduction of Vision
    Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art
    image classification model. A vanilla ViT, on the other hand, faces difficulties
    when applied to general computer vision tasks such as object detection and semantic
    segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that
    reintroduced several ConvNet priors, making Transformers practically viable as
    a generic vision backbone and demonstrating remarkable performance on a wide variety
    of vision tasks. However, the effectiveness of such hybrid approaches is still
    largely credited to the intrinsic superiority of Transformers, rather than the
    inherent inductive biases of convolutions. In this work, we reexamine the design
    spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize”
    a standard ResNet toward the design of a vision Transformer, and discover several
    key components that contribute to the performance difference along the way. The
    outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt.
    Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably
    with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet
    top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K
    segmentation, while maintaining the simplicity and efficiency of standard ConvNets.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*视觉识别的“繁荣20年”始于Vision Transformers（ViTs）的引入，它们迅速取代了ConvNets成为最先进的图像分类模型。另一方面，一个普通的ViT在应用于一般计算机视觉任务（如目标检测和语义分割）时会面临困难。正是分层Transformer（例如Swin
    Transformers）重新引入了几个ConvNet先验，使得Transformers在实践中成为通用视觉骨干，并在各种视觉任务上表现出色。然而，这种混合方法的有效性仍然主要归因于Transformers的固有优越性，而不是卷积的固有归纳偏差。在这项工作中，我们重新审视设计空间，并测试纯ConvNet可以实现的极限。我们逐渐将标准ResNet“现代化”到一个视觉Transformer的设计，并发现了沿途对性能差异的贡献的几个关键组件。这次探索的结果是一系列被称为ConvNeXt的纯ConvNet模型。ConvNeXt完全由标准ConvNet模块构建，以准确性和可扩展性方面与Transformers竞争，实现了87.8%的ImageNet
    top-1准确性，并在COCO检测和ADE20K分割上胜过Swin Transformers，同时保持了标准ConvNets的简单性和效率。*'
- en: '![drawing](../Images/c0224b7b7a3b550bc1f1bea36abd3d55.png) ConvNeXT architecture.
    Taken from the [original paper](https://arxiv.org/abs/2201.03545).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图示](../Images/c0224b7b7a3b550bc1f1bea36abd3d55.png) ConvNeXT架构。摘自[原始论文](https://arxiv.org/abs/2201.03545)。'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). TensorFlow
    version of the model was contributed by [ariG23498](https://github.com/ariG23498),
    [gante](https://github.com/gante), and [sayakpaul](https://github.com/sayakpaul)
    (equal contribution). The original code can be found [here](https://github.com/facebookresearch/ConvNeXt).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。模型的TensorFlow版本是由[ariG23498](https://github.com/ariG23498)、[gante](https://github.com/gante)和[sayakpaul](https://github.com/sayakpaul)（平等贡献）贡献的。原始代码可以在[这里](https://github.com/facebookresearch/ConvNeXt)找到。
- en: Resources
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with ConvNeXT.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个官方Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用ConvNeXT。
- en: Image Classification
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[ConvNextForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ConvNextForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[图像分类任务指南](../tasks/image_classification)
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将对其进行审查！资源应该理想地展示一些新东西，而不是重复现有资源。
- en: ConvNextConfig
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextConfig
- en: '### `class transformers.ConvNextConfig`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/configuration_convnext.py#L36)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/configuration_convnext.py#L36)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) — 输入通道数。'
- en: '`patch_size` (`int`, optional, defaults to 4) — Patch size to use in the patch
    embedding layer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, optional, defaults to 4) — 在补丁嵌入层中使用的补丁大小。'
- en: '`num_stages` (`int`, optional, defaults to 4) — The number of stages in the
    model.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_stages` (`int`, optional, defaults to 4) — 模型中的阶段数。'
- en: '`hidden_sizes` (`List[int]`, *optional*, defaults to [96, 192, 384, 768]) —
    Dimensionality (hidden size) at each stage.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_sizes` (`List[int]`, *optional*, defaults to [96, 192, 384, 768]) —
    每个阶段的维度（隐藏大小）。'
- en: '`depths` (`List[int]`, *optional*, defaults to [3, 3, 9, 3]) — Depth (number
    of blocks) for each stage.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depths` (`List[int]`, *可选*, 默认为[3, 3, 9, 3]) — 每个阶段的深度（块数）。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in each block. If string,
    `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`function`, *可选*, 默认为`"gelu"`) — 每个块中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *可选*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 1e-6) — The initial
    value for the layer scale.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_scale_init_value` (`float`, *可选*, 默认为1e-6) — 层比例的初始值。'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.0) — The drop rate for
    stochastic depth.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *可选*, 默认为0.0) — 随机深度的丢弃率。'
- en: '`out_features` (`List[str]`, *optional*) — If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_features` (`List[str]`, *可选*) — 如果用作骨干网络，要输出的特征列表。可以是`"stem"`、`"stage1"`、`"stage2"`等（取决于模型有多少阶段）。如果未设置且设置了`out_indices`，将默认为相应的阶段。如果未设置且`out_indices`也未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。'
- en: '`out_indices` (`List[int]`, *optional*) — If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *可选*) — 如果用作骨干网络，要输出的特征索引列表。可以是0、1、2等（取决于模型有多少阶段）。如果未设置且设置了`out_features`，将默认为相应的阶段。如果未设置且`out_features`也未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。'
- en: This is the configuration class to store the configuration of a [ConvNextModel](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextModel).
    It is used to instantiate an ConvNeXT model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ConvNeXT [facebook/convnext-tiny-224](https://huggingface.co/facebook/convnext-tiny-224)
    architecture.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[ConvNextModel](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextModel)配置的配置类。根据指定的参数实例化ConvNeXT模型，定义模型架构。使用默认值实例化配置将产生类似于ConvNeXT
    [facebook/convnext-tiny-224](https://huggingface.co/facebook/convnext-tiny-224)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ConvNextFeatureExtractor
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextFeatureExtractor
- en: '### `class transformers.ConvNextFeatureExtractor`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/feature_extraction_convnext.py#L26)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/feature_extraction_convnext.py#L26)'
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ConvNextImageProcessor
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextImageProcessor
- en: '### `class transformers.ConvNextImageProcessor`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/image_processing_convnext.py#L50)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/image_processing_convnext.py#L50)'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Controls whether to
    resize the image’s (height, width) dimensions to the specified `size`. Can be
    overriden by `do_resize` in the `preprocess` method.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为`True`) — 控制是否将图像的（高度、宽度）维度调整为指定的`size`。可以被`preprocess`方法中的`do_resize`覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 384}`):
    Resolution of the output image after `resize` is applied. If `size["shortest_edge"]`
    >= 384, the image is resized to `(size["shortest_edge"], size["shortest_edge"])`.
    Otherwise, the smaller edge of the image will be matched to `int(size["shortest_edge"]/crop_pct)`,
    after which the image is cropped to `(size["shortest_edge"], size["shortest_edge"])`.
    Only has an effect if `do_resize` is set to `True`. Can be overriden by `size`
    in the `preprocess` method.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *可选*, 默认为`{"shortest_edge" -- 384}`): 在应用`resize`后输出图像的分辨率。如果`size["shortest_edge"]`
    >= 384，则将图像调整为`(size["shortest_edge"], size["shortest_edge"])`。否则，图像的较小边将匹配到`int(size["shortest_edge"]/crop_pct)`，然后将图像裁剪为`(size["shortest_edge"],
    size["shortest_edge"])`。仅在`do_resize`设置为`True`时有效。可以被`preprocess`方法中的`size`覆盖。'
- en: '`crop_pct` (`float` *optional*, defaults to 224 / 256) — Percentage of the
    image to crop. Only has an effect if `do_resize` is `True` and size < 384\. Can
    be overriden by `crop_pct` in the `preprocess` method.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_pct` (`float` *可选*, 默认为224 / 256) — 要裁剪的图像百分比。仅在`do_resize`为`True`且大小<384时有效。可以被`preprocess`方法中的`crop_pct`覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overriden by `resample`
    in the `preprocess` method.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *可选*, 默认为`Resampling.BILINEAR`) — 如果调整图像大小，要使用的重采样滤波器。可以被`preprocess`方法中的`resample`覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale`
    in the `preprocess` method.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否按指定比例 `rescale_factor` 重新缩放图像。可以被
    `preprocess` 方法中的 `do_rescale` 覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overriden by `rescale_factor` in
    the `preprocess` method.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` 或 `float`, *optional*, 默认为 `1/255`) — 如果重新缩放图像，则使用的比例因子。可以被
    `preprocess` 方法中的 `rescale_factor` 覆盖。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, 默认为 `True`) — 是否对图像进行归一化。可以被 `preprocess`
    方法中的 `do_normalize` 参数覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_MEAN`)
    — 归一化图像时使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_STD`)
    — 用于归一化图像时使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。'
- en: Constructs a ConvNeXT image processor.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 构造一个 ConvNeXT 图像处理器。
- en: '#### `preprocess`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/image_processing_convnext.py#L185)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/image_processing_convnext.py#L185)'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个或批量像素值范围为 0 到 255 的图像。如果传入像素值在 0 到 1
    之间的图像，请设置 `do_rescale=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, 默认为 `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    output image after `resize` has been applied. If `size["shortest_edge"]` >= 384,
    the image is resized to `(size["shortest_edge"], size["shortest_edge"])`. Otherwise,
    the smaller edge of the image will be matched to `int(size["shortest_edge"]/ crop_pct)`,
    after which the image is cropped to `(size["shortest_edge"], size["shortest_edge"])`.
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, 默认为 `self.size`) — 在应用 `resize` 后输出图像的大小。如果
    `size["shortest_edge"]` >= 384，则将图像调整为 `(size["shortest_edge"], size["shortest_edge"])`。否则，图像的较小边将匹配到
    `int(size["shortest_edge"]/ crop_pct)`，然后将图像裁剪为 `(size["shortest_edge"], size["shortest_edge"])`。仅在
    `do_resize` 设置为 `True` 时有效。'
- en: '`crop_pct` (`float`, *optional*, defaults to `self.crop_pct`) — Percentage
    of the image to crop if size < 384.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_pct` (`float`, *optional*, 默认为 `self.crop_pct`) — 如果大小 < 384，则裁剪图像的百分比。'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) — Resampling filter
    to use if resizing the image. This can be one of `PILImageResampling`, filters.
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *optional*, 默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。这可以是
    `PILImageResampling` 中的一个滤波器。仅在 `do_resize` 设置为 `True` 时有效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放在 [0 -
    1] 之间。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, 默认为 `self.rescale_factor`) — 如果 `do_rescale`
    设置为 `True`，则用于重新缩放图像的重新缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `self.image_mean`) —
    图像均值。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *optional*, 默认为 `self.image_std`) — 图像标准差。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`, *optional*) — 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '未设置: 返回一个 `np.ndarray` 列表。'
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`: 返回类型为 `tf.Tensor` 的批处理。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`: 返回类型为 `torch.Tensor` 的批处理。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`: 返回类型为 `np.ndarray` 的批处理。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`: 返回类型为 `jax.numpy.ndarray` 的批处理。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` 或 `str`, *optional*, 默认为 `ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (num_channels, height, width)
    格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (height, width, num_channels)
    格式。'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '未设置: 使用输入图像的通道维度格式。'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension`或`str`, *optional*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`或`ChannelDimension.NONE`：图像以（高度，宽度）格式。'
- en: Preprocess an image or batch of images.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或图像批处理。
- en: PytorchHide Pytorch content
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: ConvNextModel
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextModel
- en: '### `class transformers.ConvNextModel`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L324)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L324)'
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare ConvNext model outputting raw features without any specific head on
    top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的ConvNext模型输出原始特征，没有特定的头部。这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L342)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L342)'
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    `tuple(torch.FloatTensor)`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    and inputs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 空间维度上池化操作后的最后一层隐藏状态。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个
    + 每个层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: The [ConvNextModel](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextModel)
    forward method, overrides the `__call__` special method.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[ConvNextModel](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ConvNextForImageClassification
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextForImageClassification
- en: '### `class transformers.ConvNextForImageClassification`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L387)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L387)'
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig)）—
    模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: ConvNext Model with an image classification head on top (a linear layer on top
    of the pooled features), e.g. for ImageNet.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNext模型，顶部带有图像分类头（池化特征之上的线性层），例如用于ImageNet。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L409)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_convnext.py#L409)'
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    and inputs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类（或如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）— 分类（或如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组。模型在每个阶段的输出的隐藏状态（也称为特征图）。'
- en: The [ConvNextForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[ConvNextForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextForImageClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数中定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TensorFlowHide TensorFlow content
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: TFConvNextModel
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFConvNextModel
- en: '### `class transformers.TFConvNextModel`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFConvNextModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L491)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L491)'
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare ConvNext model outputting raw features without any specific head on
    top. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的ConvNext模型输出原始特征，没有特定的头部。该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，您应该可以“轻松使用”
    - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量放在第一个位置参数中：
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个带有`pixel_values`的张量，没有其他内容：`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([pixel_values, attention_mask])`或`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些内容，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L500)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L500)'
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。这个参数只能在急切模式下使用，在图模式下，将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。这个参数可以在急切模式下使用，在图模式下，该值将始终设置为True。'
- en: Returns
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    或 `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    and inputs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）—
    模型最后一层的隐藏状态的序列。'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`tf.Tensor`）— 序列第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层和Tanh激活函数处理。线性层权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: This output is usually *not* a good summary of the semantic content of the input,
    you’re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个输出通常*不是*输入语义内容的良好摘要，您通常最好对整个输入序列的隐藏状态进行平均或池化。
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出+一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFConvNextModel](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.TFConvNextModel)
    forward method, overrides the `__call__` special method.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFConvNextModel](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.TFConvNextModel)前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: TFConvNextForImageClassification
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFConvNextForImageClassification
- en: '### `class transformers.TFConvNextForImageClassification`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFConvNextForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L563)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L563)'
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: ConvNext Model with an image classification head on top (a linear layer on top
    of the pooled features), e.g. for ImageNet.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNext模型，顶部带有图像分类头（在池化特征之上的线性层），例如用于ImageNet。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是，Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，在使用`model.fit()`等方法时，应该“只需工作”
    - 只需传递`model.fit()`支持的任何格式的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个包含`pixel_values`的张量，没有其他内容：`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个按照文档字符串中给定的顺序的输入张量：`model([pixel_values, attention_mask])`或`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些内容，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L586)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnext/modeling_tf_convnext.py#L586)'
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]` ``Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*)
    — Labels for computing the image classification/regression loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression
    loss is computed (Mean-Square loss), If `config.num_labels > 1` a classification
    loss is computed (Cross-Entropy).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`tf.Tensor`或`np.ndarray`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig))
    and inputs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    或一个 `tf.Tensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含各种元素，取决于配置（[ConvNextConfig](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextConfig)）和输入。
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor`，形状为 `(batch_size, )`，*可选*，当提供 `labels` 时返回) — 分类（如果 `config.num_labels==1`
    则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor`，形状为 `(batch_size, config.num_labels)`) — 分类（如果 `config.num_labels==1`
    则为回归）分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递 `output_hidden_states=True` 或
    `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)`
    的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递 `output_attentions=True` 或 `config.output_attentions=True`
    时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor`
    元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFConvNextForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.TFConvNextForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFConvNextForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.TFConvNextForImageClassification)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
