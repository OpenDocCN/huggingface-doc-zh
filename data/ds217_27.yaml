- en: Beam Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Beam数据集
- en: 'Original text: [https://huggingface.co/docs/datasets/beam](https://huggingface.co/docs/datasets/beam)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/datasets/beam](https://huggingface.co/docs/datasets/beam)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets are too large to be processed on a single machine. Instead, you
    can process them with [Apache Beam](https://beam.apache.org/), a library for parallel
    data processing. The processing pipeline is executed on a distributed processing
    backend such as [Apache Flink](https://flink.apache.org/), [Apache Spark](https://spark.apache.org/),
    or [Google Cloud Dataflow](https://cloud.google.com/dataflow).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集太大，无法在单台机器上处理。相反，您可以使用[Apache Beam](https://beam.apache.org/)进行处理，这是一个用于并行数据处理的库。处理管道在分布式处理后端上执行，如[Apache
    Flink](https://flink.apache.org/)、[Apache Spark](https://spark.apache.org/)或[Google
    Cloud Dataflow](https://cloud.google.com/dataflow)。
- en: 'We have already created Beam pipelines for some of the larger datasets like
    [wikipedia](https://huggingface.co/datasets/wikipedia), and [wiki40b](https://huggingface.co/datasets/wiki40b).
    You can load these normally with [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    But if you want to run your own Beam pipeline with Dataflow, here is how:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为一些较大的数据集（如[wikipedia](https://huggingface.co/datasets/wikipedia)和[wiki40b](https://huggingface.co/datasets/wiki40b)）创建了Beam管道。您可以使用[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)正常加载这些数据集。但是，如果您想要在Dataflow中运行自己的Beam管道，可以按照以下步骤操作：
- en: 'Specify the dataset and configuration you want to process:'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定您要处理的数据集和配置：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Input your Google Cloud Platform information:'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入您的Google Cloud Platform信息：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Specify your Python requirements:'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定您的Python要求：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run the pipeline:'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行管道：
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When you run your pipeline, you can adjust the parameters to change the runner
    (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 运行管道时，您可以调整参数以更改运行程序（Flink或Spark）、输出位置（S3存储桶或HDFS）和工作人员数量。
