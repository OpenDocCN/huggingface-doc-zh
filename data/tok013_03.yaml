- en: Quicktour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå…¥é—¨
- en: 'Original text: [https://huggingface.co/docs/tokenizers/quicktour](https://huggingface.co/docs/tokenizers/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/tokenizers/quicktour](https://huggingface.co/docs/tokenizers/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s have a quick look at the ğŸ¤— Tokenizers library features. The library provides
    an implementation of todayâ€™s most used tokenizers that is both easy to use and
    blazing fast.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¿«é€Ÿçœ‹ä¸€ä¸‹ğŸ¤— Tokenizersåº“çš„ç‰¹ç‚¹ã€‚è¯¥åº“æä¾›äº†ä»Šå¤©æœ€å¸¸ç”¨çš„æ ‡è®°å™¨çš„å®ç°ï¼Œæ—¢æ˜“äºä½¿ç”¨åˆé€Ÿåº¦å¿«ã€‚
- en: Build a tokenizer from scratch
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªæ ‡è®°å™¨
- en: 'To illustrate how fast the ğŸ¤— Tokenizers library is, letâ€™s train a new tokenizer
    on [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)
    (516M of text) in just a few seconds. First things first, you will need to download
    this dataset and unzip it with:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜ğŸ¤— Tokenizersåº“æœ‰å¤šå¿«ï¼Œè®©æˆ‘ä»¬åœ¨å‡ ç§’é’Ÿå†…åœ¨[wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)ï¼ˆ516Mæ–‡æœ¬ï¼‰ä¸Šè®­ç»ƒä¸€ä¸ªæ–°çš„æ ‡è®°å™¨ã€‚é¦–å…ˆï¼Œæ‚¨éœ€è¦ä¸‹è½½è¿™ä¸ªæ•°æ®é›†å¹¶è§£å‹ç¼©ï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Training the tokenizer
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ ‡è®°å™¨
- en: 'In this tour, we will build and train a Byte-Pair Encoding (BPE) tokenizer.
    For more information about the different type of tokenizers, check out this [guide](https://huggingface.co/transformers/tokenizer_summary.html)
    in the ğŸ¤— Transformers documentation. Here, training the tokenizer means it will
    learn merge rules by:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ¬¡å¿«é€Ÿå…¥é—¨ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºå’Œè®­ç»ƒä¸€ä¸ªå­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æ ‡è®°å™¨ã€‚æœ‰å…³ä¸åŒç±»å‹çš„æ ‡è®°å™¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ğŸ¤— Transformersæ–‡æ¡£ä¸­çš„[æŒ‡å—](https://huggingface.co/transformers/tokenizer_summary.html)ã€‚åœ¨è¿™é‡Œï¼Œè®­ç»ƒæ ‡è®°å™¨æ„å‘³ç€å®ƒå°†é€šè¿‡ä»¥ä¸‹æ–¹å¼å­¦ä¹ åˆå¹¶è§„åˆ™ï¼š
- en: Start with all the characters present in the training corpus as tokens.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒè¯­æ–™åº“ä¸­çš„æ‰€æœ‰å­—ç¬¦å¼€å§‹ä½œä¸ºæ ‡è®°ã€‚
- en: Identify the most common pair of tokens and merge it into one token.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯†åˆ«æœ€å¸¸è§çš„ä¸€å¯¹æ ‡è®°å¹¶å°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªæ ‡è®°ã€‚
- en: Repeat until the vocabulary (e.g., the number of tokens) has reached the size
    we want.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡å¤ç›´åˆ°è¯æ±‡è¡¨ï¼ˆä¾‹å¦‚ï¼Œæ ‡è®°æ•°ï¼‰è¾¾åˆ°æˆ‘ä»¬æƒ³è¦çš„å¤§å°ã€‚
- en: 'The main API of the library is the `class` `Tokenizer`, here is how we instantiate
    one with a BPE model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥åº“çš„ä¸»è¦APIæ˜¯`class` `Tokenizer`ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨BPEæ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªï¼š
- en: PythonRustNode
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To train our tokenizer on the wikitext files, we will need to instantiate a
    [trainer]{.title-ref}, in this case a `BpeTrainer`
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨wikitextæ–‡ä»¶ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ ‡è®°å™¨ï¼Œæˆ‘ä»¬å°†éœ€è¦å®ä¾‹åŒ–ä¸€ä¸ª[trainer]{.title-ref}ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ä¸€ä¸ª`BpeTrainer`
- en: PythonRustNode
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can set the training arguments like `vocab_size` or `min_frequency` (here
    left at their default values of 30,000 and 0) but the most important part is to
    give the `special_tokens` we plan to use later on (they are not used at all during
    training) so that they get inserted in the vocabulary.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è®¾ç½®è®­ç»ƒå‚æ•°ï¼Œå¦‚`vocab_size`æˆ–`min_frequency`ï¼ˆè¿™é‡Œä¿æŒé»˜è®¤å€¼ä¸º30,000å’Œ0ï¼‰ï¼Œä½†æœ€é‡è¦çš„éƒ¨åˆ†æ˜¯æä¾›æˆ‘ä»¬ä»¥åæ‰“ç®—ä½¿ç”¨çš„`special_tokens`ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æœ¬ä¸ä½¿ç”¨ï¼‰ï¼Œä»¥ä¾¿å®ƒä»¬è¢«æ’å…¥åˆ°è¯æ±‡è¡¨ä¸­ã€‚
- en: 'The order in which you write the special tokens list matters: here `"[UNK]"`
    will get the ID 0, `"[CLS]"` will get the ID 1 and so forth.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–å†™ç‰¹æ®Šæ ‡è®°åˆ—è¡¨çš„é¡ºåºå¾ˆé‡è¦ï¼šè¿™é‡Œ`"[UNK]"`å°†è·å¾—ID 0ï¼Œ`"[CLS]"`å°†è·å¾—ID 1ï¼Œä¾æ­¤ç±»æ¨ã€‚
- en: 'We could train our tokenizer right now, but it wouldnâ€™t be optimal. Without
    a pre-tokenizer that will split our inputs into words, we might get tokens that
    overlap several words: for instance we could get an `"it is"` token since those
    two words often appear next to each other. Using a pre-tokenizer will ensure no
    token is bigger than a word returned by the pre-tokenizer. Here we want to train
    a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by
    splitting on whitespace.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„æ ‡è®°å™¨ï¼Œä½†è¿™å¹¶ä¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚æ²¡æœ‰ä¸€ä¸ªé¢„æ ‡è®°å™¨å°†æˆ‘ä»¬çš„è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ°é‡å å‡ ä¸ªå•è¯çš„æ ‡è®°ï¼šä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ª`"it
    is"`æ ‡è®°ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªå•è¯ç»å¸¸ç›¸é‚»å‡ºç°ã€‚ä½¿ç”¨é¢„æ ‡è®°å™¨å°†ç¡®ä¿æ²¡æœ‰ä¸€ä¸ªæ ‡è®°æ¯”é¢„æ ‡è®°å™¨è¿”å›çš„å•è¯æ›´å¤§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³è®­ç»ƒä¸€ä¸ªå­è¯BPEæ ‡è®°å™¨ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†ä½¿ç”¨æœ€ç®€å•çš„é¢„æ ‡è®°å™¨ï¼Œå³æŒ‰ç©ºæ ¼æ‹†åˆ†ã€‚
- en: PythonRustNode
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can just call the `Tokenizer.train` method with any list of files we
    want to use:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬åªéœ€è°ƒç”¨`Tokenizer.train`æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ä»»ä½•æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ–‡ä»¶åˆ—è¡¨ï¼š
- en: PythonRustNode
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This should only take a few seconds to train our tokenizer on the full wikitext
    dataset! To save the tokenizer in one file that contains all its configuration
    and vocabulary, just use the `Tokenizer.save` method:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®Œæ•´çš„ç»´åŸºæ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ ‡è®°å™¨åº”è¯¥åªéœ€è¦å‡ ç§’é’Ÿï¼è¦å°†æ ‡è®°å™¨ä¿å­˜åœ¨åŒ…å«æ‰€æœ‰é…ç½®å’Œè¯æ±‡è¡¨çš„æ–‡ä»¶ä¸­ï¼Œåªéœ€ä½¿ç”¨`Tokenizer.save`æ–¹æ³•ï¼š
- en: PythonRustNode
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'and you can reload your tokenizer from that file with the `Tokenizer.from_file`
    `classmethod`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`Tokenizer.from_file`çš„`classmethod`ä»è¯¥æ–‡ä»¶é‡æ–°åŠ è½½æ‚¨çš„æ ‡è®°å™¨ï¼š
- en: PythonRustNode
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using the tokenizer
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ ‡è®°å™¨
- en: 'Now that we have trained a tokenizer, we can use it on any text we want with
    the `Tokenizer.encode` method:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è®­ç»ƒäº†ä¸€ä¸ªæ ‡è®°å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Tokenizer.encode`æ–¹æ³•å¯¹ä»»ä½•æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼š
- en: PythonRustNode
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This applied the full pipeline of the tokenizer on the text, returning an `Encoding`
    object. To learn more about this pipeline, and how to apply (or customize) parts
    of it, check out [this page](pipeline).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™å°†åœ¨æ–‡æœ¬ä¸Šåº”ç”¨æ ‡è®°å™¨çš„å®Œæ•´æµç¨‹ï¼Œè¿”å›ä¸€ä¸ª`Encoding`å¯¹è±¡ã€‚è¦äº†è§£æœ‰å…³æ­¤æµç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¦‚ä½•åº”ç”¨ï¼ˆæˆ–è‡ªå®šä¹‰ï¼‰å…¶ä¸­çš„éƒ¨åˆ†ï¼Œè¯·æŸ¥çœ‹[æ­¤é¡µé¢](pipeline)ã€‚ '
- en: 'This `Encoding` object then has all the attributes you need for your deep learning
    model (or other). The `tokens` attribute contains the segmentation of your text
    in tokens:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª`Encoding`å¯¹è±¡å…·æœ‰æ‚¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆæˆ–å…¶ä»–æ¨¡å‹ï¼‰æ‰€éœ€çš„æ‰€æœ‰å±æ€§ã€‚`tokens`å±æ€§åŒ…å«äº†æ–‡æœ¬åœ¨æ ‡è®°ä¸­çš„åˆ†å‰²ï¼š
- en: PythonRustNode
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similarly, the `ids` attribute will contain the index of each of those tokens
    in the tokenizerâ€™s vocabulary:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œ`ids`å±æ€§å°†åŒ…å«è¿™äº›æ ‡è®°åœ¨æ ‡è®°å™¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ï¼š
- en: PythonRustNode
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'An important feature of the ğŸ¤— Tokenizers library is that it comes with full
    alignment tracking, meaning you can always get the part of your original sentence
    that corresponds to a given token. Those are stored in the `offsets` attribute
    of our `Encoding` object. For instance, letâ€™s assume we would want to find back
    what caused the `"[UNK]"` token to appear, which is the token at index 9 in the
    list, we can just ask for the offset at the index:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Tokenizersåº“çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§æ˜¯å®ƒå…·æœ‰å®Œæ•´çš„å¯¹é½è·Ÿè¸ªï¼Œè¿™æ„å‘³ç€æ‚¨å§‹ç»ˆå¯ä»¥è·å¾—åŸå§‹å¥å­ä¸­ä¸ç»™å®šæ ‡è®°å¯¹åº”çš„éƒ¨åˆ†ã€‚è¿™äº›å­˜å‚¨åœ¨æˆ‘ä»¬çš„`Encoding`å¯¹è±¡çš„`offsets`å±æ€§ä¸­ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³æ‰¾å›å¯¼è‡´`"[UNK]"`æ ‡è®°å‡ºç°çš„åŸå› ï¼Œè¯¥æ ‡è®°åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ä¸º9ï¼Œæˆ‘ä»¬åªéœ€è¯·æ±‚è¯¥ç´¢å¼•å¤„çš„åç§»é‡ï¼š
- en: PythonRustNode
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'and those are the indices that correspond to the emoji in the original sentence:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯åŸå§‹å¥å­ä¸­å¯¹åº”è¡¨æƒ…ç¬¦å·çš„ç´¢å¼•ï¼š
- en: PythonRustNode
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Post-processing
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åå¤„ç†
- en: We might want our tokenizer to automatically add special tokens, like `"[CLS]"`
    or `"[SEP]"`. To do this, we use a post-processor. `TemplateProcessing` is the
    most commonly used, you just have to specify a template for the processing of
    single sentences and pairs of sentences, along with the special tokens and their
    IDs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯èƒ½å¸Œæœ›æˆ‘ä»¬çš„åˆ†è¯å™¨è‡ªåŠ¨æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œå¦‚`"[CLS]"`æˆ–`"[SEP]"`ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨åå¤„ç†å™¨ã€‚`TemplateProcessing`æ˜¯æœ€å¸¸ç”¨çš„ï¼Œæ‚¨åªéœ€ä¸ºå•ä¸ªå¥å­å’Œå¥å­å¯¹çš„å¤„ç†æŒ‡å®šä¸€ä¸ªæ¨¡æ¿ï¼Œä»¥åŠç‰¹æ®Šæ ‡è®°åŠå…¶IDã€‚
- en: 'When we built our tokenizer, we set `"[CLS]"` and `"[SEP]"` in positions 1
    and 2 of our list of special tokens, so this should be their IDs. To double-check,
    we can use the `Tokenizer.token_to_id` method:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ„å»ºåˆ†è¯å™¨æ—¶ï¼Œæˆ‘ä»¬å°†`"[CLS]"`å’Œ`"[SEP]"`è®¾ç½®ä¸ºç‰¹æ®Šæ ‡è®°åˆ—è¡¨ä¸­çš„ä½ç½®1å’Œ2ï¼Œå› æ­¤è¿™åº”è¯¥æ˜¯å®ƒä»¬çš„IDã€‚ä¸ºäº†åŒé‡æ£€æŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Tokenizer.token_to_id`æ–¹æ³•ï¼š
- en: PythonRustNode
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is how we can set the post-processing to give us the traditional BERT
    inputs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å¦‚ä½•è®¾ç½®åå¤„ç†ä»¥è·å¾—ä¼ ç»Ÿçš„BERTè¾“å…¥ï¼š
- en: PythonRustNode
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Letâ€™s go over this snippet of code in more details. First we specify the template
    for single sentences: those should have the form `"[CLS] $A [SEP]"` where `$A`
    represents our sentence.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æŸ¥çœ‹è¿™æ®µä»£ç ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æŒ‡å®šäº†å•ä¸ªå¥å­çš„æ¨¡æ¿ï¼šè¿™äº›åº”è¯¥æ˜¯`"[CLS] $A [SEP]"`çš„å½¢å¼ï¼Œå…¶ä¸­`$A`ä»£è¡¨æˆ‘ä»¬çš„å¥å­ã€‚
- en: 'Then, we specify the template for sentence pairs, which should have the form
    `"[CLS] $A [SEP] $B [SEP]"` where `$A` represents the first sentence and `$B`
    the second one. The `:1` added in the template represent the `type IDs` we want
    for each part of our input: it defaults to 0 for everything (which is why we donâ€™t
    have `$A:0`) and here we set it to 1 for the tokens of the second sentence and
    the last `"[SEP]"` token.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æŒ‡å®šäº†å¥å­å¯¹çš„æ¨¡æ¿ï¼Œåº”è¯¥æ˜¯`"[CLS] $A [SEP] $B [SEP]"`çš„å½¢å¼ï¼Œå…¶ä¸­`$A`ä»£è¡¨ç¬¬ä¸€ä¸ªå¥å­ï¼Œ`$B`ä»£è¡¨ç¬¬äºŒä¸ªå¥å­ã€‚æ¨¡æ¿ä¸­æ·»åŠ çš„`:1`ä»£è¡¨æˆ‘ä»¬å¸Œæœ›ä¸ºè¾“å…¥çš„æ¯ä¸ªéƒ¨åˆ†è®¾ç½®çš„`ç±»å‹ID`ï¼šé»˜è®¤ä¸º0ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æ²¡æœ‰`$A:0`ï¼‰ï¼Œè¿™é‡Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸º1ï¼Œç”¨äºç¬¬äºŒä¸ªå¥å­çš„æ ‡è®°å’Œæœ€åçš„`"[SEP]"`æ ‡è®°ã€‚
- en: Lastly, we specify the special tokens we used and their IDs in our tokenizerâ€™s
    vocabulary.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åœ¨åˆ†è¯å™¨çš„è¯æ±‡è¡¨ä¸­æŒ‡å®šäº†æˆ‘ä»¬ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°åŠå…¶IDã€‚
- en: 'To check out this worked properly, letâ€™s try to encode the same sentence as
    before:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ£€æŸ¥è¿™æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œè®©æˆ‘ä»¬å°è¯•å¯¹ä¹‹å‰çš„ç›¸åŒå¥å­è¿›è¡Œç¼–ç ï¼š
- en: PythonRustNode
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To check the results on a pair of sentences, we just pass the two sentences
    to `Tokenizer.encode`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ£€æŸ¥ä¸€å¯¹å¥å­çš„ç»“æœï¼Œæˆ‘ä»¬åªéœ€å°†è¿™ä¸¤ä¸ªå¥å­ä¼ é€’ç»™`Tokenizer.encode`ï¼š
- en: PythonRustNode
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can then check the type IDs attributed to each token is correct with
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•æ£€æŸ¥æ¯ä¸ªæ ‡è®°åˆ†é…çš„ç±»å‹IDæ˜¯å¦æ­£ç¡®
- en: PythonRustNode
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you save your tokenizer with `Tokenizer.save`, the post-processor will be
    saved along.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨`Tokenizer.save`ä¿å­˜äº†æ‚¨çš„åˆ†è¯å™¨ï¼Œåå¤„ç†å™¨ä¹Ÿä¼šè¢«ä¿å­˜ã€‚
- en: Encoding multiple sentences in a batch
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨æ‰¹é‡ä¸­ç¼–ç å¤šä¸ªå¥å­
- en: 'To get the full speed of the ğŸ¤— Tokenizers library, itâ€™s best to process your
    texts by batches by using the `Tokenizer.encode_batch` method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—ğŸ¤— Tokenizersåº“çš„å®Œæ•´é€Ÿåº¦ï¼Œæœ€å¥½é€šè¿‡ä½¿ç”¨`Tokenizer.encode_batch`æ–¹æ³•æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼š
- en: PythonRustNode
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output is then a list of `Encoding` objects like the ones we saw before.
    You can process together as many texts as you like, as long as it fits in memory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¾“å‡ºæ˜¯ä¸€ä¸ª`Encoding`å¯¹è±¡çš„åˆ—è¡¨ï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„é‚£æ ·ã€‚åªè¦å†…å­˜è¶³å¤Ÿï¼Œæ‚¨å¯ä»¥ä¸€èµ·å¤„ç†å°½å¯èƒ½å¤šçš„æ–‡æœ¬ã€‚
- en: 'To process a batch of sentences pairs, pass two lists to the `Tokenizer.encode_batch`
    method: the list of sentences A and the list of sentences B:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¤„ç†ä¸€æ‰¹å¥å­å¯¹ï¼Œå°†ä¸¤ä¸ªåˆ—è¡¨ä¼ é€’ç»™`Tokenizer.encode_batch`æ–¹æ³•ï¼šå¥å­Açš„åˆ—è¡¨å’Œå¥å­Bçš„åˆ—è¡¨ï¼š
- en: PythonRustNode
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When encoding multiple sentences, you can automatically pad the outputs to
    the longest sentence present by using `Tokenizer.enable_padding`, with the `pad_token`
    and its ID (which we can double-check the id for the padding token with `Tokenizer.token_to_id`
    like before):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¼–ç å¤šä¸ªå¥å­æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`Tokenizer.enable_padding`è‡ªåŠ¨å°†è¾“å‡ºå¡«å……åˆ°å­˜åœ¨çš„æœ€é•¿å¥å­ï¼Œä½¿ç”¨`pad_token`åŠå…¶IDï¼ˆæˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·ä½¿ç”¨`Tokenizer.token_to_id`æ¥åŒé‡æ£€æŸ¥å¡«å……æ ‡è®°çš„IDï¼‰ï¼š
- en: PythonRustNode
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can set the `direction` of the padding (defaults to the right) or a given
    `length` if we want to pad every sample to that specific number (here we leave
    it unset to pad to the size of the longest text).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è®¾ç½®å¡«å……çš„`æ–¹å‘`ï¼ˆé»˜è®¤ä¸ºå³ä¾§ï¼‰æˆ–ç»™å®šçš„`é•¿åº¦`ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦å°†æ¯ä¸ªæ ·æœ¬å¡«å……åˆ°ç‰¹å®šæ•°é‡ï¼ˆè¿™é‡Œæˆ‘ä»¬å°†å…¶ä¿æŒæœªè®¾ç½®ï¼Œä»¥ä¾¿å¡«å……åˆ°æœ€é•¿æ–‡æœ¬çš„å¤§å°ï¼‰ã€‚
- en: PythonRustNode
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this case, the `attention mask` generated by the tokenizer takes the padding
    into account:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†è¯å™¨ç”Ÿæˆçš„`æ³¨æ„åŠ›æ©ç `è€ƒè™‘äº†å¡«å……ï¼š
- en: PythonRustNode
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Pretrained
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒ
- en: PythonRustNode
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: Using a pretrained tokenizer
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†è¯å™¨
- en: You can load any tokenizer from the Hugging Face Hub as long as a `tokenizer.json`
    file is available in the repository.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦å­˜å‚¨åº“ä¸­æœ‰ä¸€ä¸ª`tokenizer.json`æ–‡ä»¶å¯ç”¨ï¼Œæ‚¨å°±å¯ä»¥ä»Hugging Face HubåŠ è½½ä»»ä½•åˆ†è¯å™¨ã€‚
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Importing a pretrained tokenizer from legacy vocabulary files
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»ä¼ ç»Ÿè¯æ±‡æ–‡ä»¶å¯¼å…¥é¢„è®­ç»ƒçš„åˆ†è¯å™¨
- en: 'You can also import a pretrained tokenizer directly in, as long as you have
    its vocabulary file. For instance, here is how to import the classic pretrained
    BERT tokenizer:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦æ‚¨æœ‰å…¶è¯æ±‡æ–‡ä»¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥å¯¼å…¥ä¸€ä¸ªé¢„è®­ç»ƒçš„åˆ†è¯å™¨ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•å¯¼å…¥ç»å…¸çš„é¢„è®­ç»ƒBERTåˆ†è¯å™¨çš„æ–¹æ³•ï¼š
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: as long as you have downloaded the file `bert-base-uncased-vocab.txt` with
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦æ‚¨å·²ç»ä¸‹è½½äº†æ–‡ä»¶`bert-base-uncased-vocab.txt`ï¼Œå°±å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†è¯å™¨
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
