- en: Quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速导览
- en: 'Original text: [https://huggingface.co/docs/accelerate/quicktour](https://huggingface.co/docs/accelerate/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/accelerate/quicktour](https://huggingface.co/docs/accelerate/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This guide aims to help you get started with 🤗 Accelerate quickly. It covers
    the essential steps you need to take to enable distributed training, as well as
    the adjustments that you need to make in some common scenarios.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南旨在帮助您快速入门🤗 Accelerate。它涵盖了启用分布式训练所需的基本步骤，以及在一些常见情况下需要进行的调整。
- en: 'To help you navigate, the guide is split into two sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您导航，指南分为两个部分：
- en: '[Getting Started with 🤗 Accelerate](#getting-started-with--accelerate): start
    here to learn how to modify your script to enable distributed training with 🤗
    Accelerate'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Getting Started with 🤗 Accelerate](#getting-started-with--accelerate)：从这里开始学习如何修改您的脚本以启用🤗
    Accelerate的分布式训练'
- en: '[Common adaptations to the base case](#common-adaptations-to-the-base-case):
    check out this section for common deviations from the baseline scenario and what
    adjustments may need to be made to support them.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基本情况下的常见适应](#common-adaptations-to-the-base-case)：查看此部分以了解与基线情况的常见偏差以及可能需要进行的调整。'
- en: Getting started with 🤗 Accelerate
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用🤗 Accelerate
- en: Enable distributed training in your script
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在您的脚本中启用分布式训练
- en: 'To use 🤗 Accelerate in your own training script, you have to modify four things:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要在自己的训练脚本中使用🤗 Accelerate，您需要修改四个内容：
- en: Import the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    main class and instantiate one in an `accelerator` object.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)主类并在`accelerator`对象中实例化一个。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Add this at the beginning of your training script as it will initialize everything
    necessary for distributed training. You don’t need to indicate the kind of environment
    you are in (a single machine with a GPU, a machine with several GPUs, or several
    machines with multiple GPUs or a TPU), the library will detect this automatically.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将此添加到您的训练脚本开头，因为它将初始化分布式训练所需的一切。您无需指示您所处的环境类型（具有GPU的单台机器，具有多个GPU的机器，或具有多个GPU或TPU的多台机器），库将自动检测到这一点。
- en: Remove the `.to(device)` or `.cuda()` calls for your model and input data.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除模型和输入数据的`.to(device)`或`.cuda()`调用。
- en: 'The `accelerator` object will handle placing these objects on the right device
    for you. If you choose to leave those `.to(device)` calls, make sure to use the
    device provided by the `accelerator` object: `accelerator.device`.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`accelerator`对象将为您处理将这些对象放置在正确设备上。如果选择保留那些`.to(device)`调用，请确保使用`accelerator`对象提供的设备：`accelerator.device`。'
- en: You can fully deactivate the automatic device placement by passing along `device_placement=False`
    when initializing the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator).
    However, if you place your objects manually on the proper device, be careful to
    create your optimizer after putting your model on `accelerator.device` or your
    training will fail on TPU.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在初始化[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)时传递`device_placement=False`来完全停用自动设备放置。但是，如果您手动将对象放置在正确的设备上，请注意在将模型放置在`accelerator.device`上后再创建优化器，否则您的TPU训练将失败。
- en: 'Pass all PyTorch objects relevant to training (optimizer, model, dataloader(s),
    learning rate scheduler) to the [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method as soon as these objects are created, before starting your actual training
    loop:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有与训练相关的PyTorch对象（优化器，模型，数据加载器，学习率调度器）在创建这些对象后立即传递给[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)方法，然后再开始实际的训练循环：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Important notes**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要提示**：'
- en: You should always pass the the learning rate scheduler to [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare),
    however if the scheduler should *not* be stepped at each optimization step, pass
    `step_with_optimizer=False` to the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    init.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该始终将学习率调度器传递给[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)，但是如果调度器在每次优化步骤时*不*应该被步进，请在[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)初始化时传递`step_with_optimizer=False`。
- en: While you can send your dataloader to [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    on its own (and there are cases for doing so, such as distributed inference),
    it’s best to send it to [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    together with the model and optimizer.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然您可以将您的数据加载器单独发送到[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)（并且有时需要这样做，比如分布式推理），但最好将其与模型和优化器一起发送到[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)。
- en: If you wish to run distributed evaluation, send your validation dataloader to
    [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    as well. There are some nuances to distributed validation, check the [Distributed
    evaluation](#add-distributed-evaluation) section of the guide.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望运行分布式评估，请将您的验证数据加载器也发送到[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)。分布式评估有一些细微差别，请查看指南中的[Distributed
    evaluation](#add-distributed-evaluation)部分。
- en: Any instruction using your training dataloader length (for instance if you want
    to log the number of total training steps) should go after the call to [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何使用您的训练数据加载器长度的指令（例如，如果您想记录总训练步数）应该在调用[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)之后进行。
- en: Passing `DataLoader` objects to the [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method ensures that your dataloader will be sharded across all GPUs/TPU cores
    available so that each one sees a different portion of the training dataset. In
    other words, if there are 8 processes and a dataset of 64 items, each process
    will see 8 of these items per iteration. Also, the random states of all processes
    will be synchronized at the beginning of each iteration through your dataloader,
    to make sure the data is shuffled the same way (if you decided to use `shuffle=True`
    or any kind of random sampler).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `DataLoader` 对象传递给 [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    方法可确保您的数据加载器将在所有可用的GPU/TPU核心上进行分片，以便每个核心看到训练数据集的不同部分。换句话说，如果有 8 个进程和一个包含 64 个项目的数据集，每个进程将在每次迭代中看到其中的
    8 个项目。此外，所有进程的随机状态将在每次迭代开始时通过您的数据加载器进行同步，以确保数据以相同的方式洗牌（如果您决定使用 `shuffle=True`
    或任何类型的随机采样器）。
- en: 'The actual batch size for your training will be the number of devices used
    multiplied by the batch size you set in your script. For instance, training on
    4 GPUs with a batch size of 16 set when creating the training dataloader will
    train at an actual batch size of 64 (4 * 16). If you want the batch size remain
    the same regardless of how many GPUs the script is run on, you can use the option
    `split_batches=True` when creating and initializing [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator).
    Your training dataloader may change length when going through this method: if
    you run on X GPUs, it will have its length divided by X (since your actual batch
    size will be multiplied by X), unless you set `split_batches=True`.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您的训练实际批量大小将是使用的设备数量乘以脚本中设置的批量大小。例如，使用批量大小为 16 创建训练数据加载器时在 4 个GPU上训练将以实际批量大小
    64（4 * 16）进行训练。如果希望无论脚本在多少个GPU上运行，批量大小保持不变，可以在创建和初始化 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    时使用选项 `split_batches=True`。通过此方法进行训练数据加载器可能会改变长度：如果在 X 个GPU上运行，它的长度将被 X 整除（因为实际批量大小将乘以
    X），除非设置 `split_batches=True`。
- en: Replace the `loss.backward()` line with `accelerator.backward(loss)`.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `loss.backward()` 行替换为 `accelerator.backward(loss)`。
- en: And you’re all set! With all these changes, your script will run on your local
    machine as well as on multiple GPUs or a TPU! You can either use your favorite
    tool to launch the distributed training, or you can use the 🤗 Accelerate launcher.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成！通过所有这些更改，您的脚本将在本地机器上以及多个GPU或TPU上运行！您可以使用您喜欢的工具启动分布式训练，或者您可以使用 🤗 Accelerate
    启动器。
- en: Add distributed evaluation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加分布式评估
- en: You can perform regular evaluation in your training script if you leave your
    validation dataloader out of the [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method. In this case, you will need to put the input data on the `accelerator.device`
    manually.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将验证数据加载器排除在 [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    方法之外，您可以在训练脚本中执行定期评估。在这种情况下，您需要手动将输入数据放在 `accelerator.device` 上。
- en: 'To perform distributed evaluation, send along your validation dataloader to
    the [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行分布式评估，请将验证数据加载器发送到 [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    方法中：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Same as with your training dataloader, each device will only see part of the
    evaluation data should you run your script on multiple devices. This means you
    will need to group your predictions together which you can do with the [gather_for_metrics()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather_for_metrics)
    method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练数据加载器一样，如果在多个设备上运行脚本，则每个设备只会看到评估数据的一部分。这意味着您需要将预测结果组合在一起，可以使用 [gather_for_metrics()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather_for_metrics)
    方法来实现。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similar to the training dataloader, passing your validation dataloader through
    [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    may change it: if you run on X GPUs, it will have its length divided by X (since
    your actual batch size will be multiplied by X), unless you set `split_batches=True`.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练数据加载器类似，通过 [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    传递验证数据加载器可能会改变它：如果在 X 个GPU上运行，它的长度将被 X 整除（因为实际批量大小将乘以 X），除非设置 `split_batches=True`。
- en: Some data at the end of the dataset may be duplicated so the batch can be divided
    equally among all workers. As a result, metrics should be calculated through the
    [gather_for_metrics()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather_for_metrics)
    method to automatically remove the duplicated data while gathering and provide
    a more accurate metric.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集末尾的一些数据可能会重复，以便批次可以在所有工作进程之间平均分配。因此，应通过 [gather_for_metrics()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather_for_metrics)
    方法计算指标，以在收集数据时自动删除重复数据，并提供更准确的指标。
- en: If for some reason you don’t wish to have this automatically done, [gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)
    can be used instead to gather the data across all processes and this can manually
    be done instead.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出于某种原因您不希望自动执行此操作，可以使用 [gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)
    来代替，以收集所有进程中的数据，这可以手动完成。
- en: The [gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)
    and [gather_for_metrics()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather_for_metrics)
    methods require the tensors to be all the same size on each process. If you have
    tensors of different sizes on each process (for instance when dynamically padding
    to the maximum length in a batch), you should use the [pad_across_processes()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.pad_across_processes)
    method to pad you tensor to the biggest size across processes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)和[gather_for_metrics()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather_for_metrics)方法要求每个进程上的张量大小相同。如果每个进程上的张量大小不同（例如在批处理中动态填充到最大长度时），您应该使用[pad_across_processes()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.pad_across_processes)方法将您的张量填充到跨进程的最大大小。'
- en: Launch your distributed script
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动您的分布式脚本
- en: You can use the regular commands to launch your distributed training (like `torch.distributed.run`
    for PyTorch) - they are fully compatible with 🤗 Accelerate.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用常规命令来启动您的分布式训练（例如PyTorch的`torch.distributed.run`）-它们与🤗 Accelerate完全兼容。
- en: 'Alternatively, 🤗 Accelerate provides a CLI tool that unifies all launchers,
    so you only have to remember one command. \ To use it, run a quick configuration
    setup first on your machine and answer the questions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，🤗 Accelerate提供了一个CLI工具，统一了所有启动器，因此您只需记住一个命令。要使用它，在您的机器上首先运行快速配置设置并回答问题：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At the end of the setup, a *default_config.yaml* file will be saved in your
    cache folder for 🤗 Accelerate. That cache folder is (with decreasing order of
    priority):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置结束时，将在您的缓存文件夹中保存一个*default_config.yaml*文件供🤗 Accelerate使用。该缓存文件夹为（按优先级递减的顺序）：
- en: The content of your environment variable `HF_HOME` suffixed with *accelerate*.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的环境变量`HF_HOME`的内容后缀为*accelerate*。
- en: If it does not exist, the content of your environment variable `XDG_CACHE_HOME`
    suffixed with *huggingface/accelerate*.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不存在，则将您的环境变量`XDG_CACHE_HOME`的内容后缀为*huggingface/accelerate*。
- en: If this does not exist either, the folder *~/.cache/huggingface/accelerate*.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这也不存在，则文件夹*~/.cache/huggingface/accelerate*。
- en: 'By specifying the `--config_file` flag you can specify an alternative location
    of the configuration file. Once the configuration setup is complete, you can test
    your setup by running:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定`--config_file`标志，您可以指定配置文件的替代位置。一旦配置设置完成，您可以通过运行以下命令来测试您的设置：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will launch a short script that will test the distributed environment.
    If it runs without issues, you are ready for the next step!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动一个简短的脚本，测试分布式环境。如果没有问题运行，您已经准备好进行下一步了！
- en: 'Note that if you specified a location for the config file in the previous step,
    you need to pass it here as well:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果在上一步中指定了配置文件的位置，则需要在此处传递它：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that this is done, you can run your script with the following command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这样做后，您可以使用以下命令运行您的脚本：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you stored the config file in a non-default location, you can indicate it
    to the launcher like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将配置文件存储在非默认位置，可以像这样向启动器指示它：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can override any of the arguments determined by your config file. To see
    the complete list of parameters that you can pass in, run `accelerate launch -h`.
    (And further niche argument help by passing in partial commands, such as `accelerate
    launch --multi_gpu -h` for all `multi_gpu` args)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以覆盖由配置文件确定的任何参数。要查看可以传递的完整参数列表，请运行`accelerate launch -h`。（通过传递部分命令，例如`accelerate
    launch --multi_gpu -h`，可以获取更多关于特定参数的帮助）
- en: Check out the [Launch tutorial](basic_tutorials/launch) for more information
    about launching your scripts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[启动教程](basic_tutorials/launch)以获取有关启动您的脚本的更多信息。
- en: Common modifications of the base case
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本情况的常见修改
- en: The previous section covers the minimal essential steps to move a training script
    into a distributed setup with 🤗 Accelerate. Here we describe common modifications/deviations
    from the base case scenario and the adjustments you need to make to accommodate
    for them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节涵盖了将训练脚本移至具有🤗 Accelerate的分布式设置的最小基本步骤。在这里，我们描述了与基本情况场景的常见修改/偏差以及您需要进行的调整。
- en: Launch distributed training from a notebook
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从笔记本启动分布式训练
- en: Accelerate has a [notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)
    to help you launch your training function from a notebook. This launcher supports
    launching a training with TPUs on Colab or Kaggle, as well as training on several
    GPUs and machines (if the machine on which you are running your notebook has them).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Accelerate有一个[notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)，可以帮助您从笔记本启动训练函数。该启动器支持在Colab或Kaggle上使用TPU进行训练，以及在多个GPU和机器上进行训练（如果运行笔记本的机器具有这些设备）。
- en: 'Define a function responsible for your whole training and/or evaluation in
    a cell of the notebook, then execute a cell with the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的一个单元格中定义一个负责整个训练和/或评估的函数，然后执行以下代码的单元格：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    object should only be defined inside the training function. This is because the
    initialization should be done inside the launcher only.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您的[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)对象应该只在训练函数内部定义。这是因为初始化应该只在启动器内部完成。
- en: Check out the [Notebook Launcher tutorial](basic_tutorials/notebook) for more
    information about training on TPUs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[笔记本启动器教程](basic_tutorials/notebook)以获取有关在TPU上进行训练的更多信息。
- en: Specifics of training on TPU
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在TPU上训练的具体情况
- en: If you want to launch your script on TPUs, there are a few caveats you should
    be aware of. Behind the scenes, the TPUs will create a graph of all the operations
    happening in your training step (forward pass, backward pass and optimizer step).
    This is why your first step of training will always be very long as building and
    compiling this graph for optimizations takes some time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在TPU上运行您的脚本，您应该注意一些注意事项。在幕后，TPU将创建一个图，显示训练步骤中发生的所有操作（前向传递、反向传递和优化器步骤）。这就是为什么您的训练的第一步总是非常长的原因，因为构建和编译此图以进行优化需要一些时间。
- en: 'The good news is that this compilation will be cached so the second step and
    all the following will be much faster. The bad news is that it only applies if
    all of your steps do exactly the same operations, which implies:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是这个编译将被缓存，因此第二步和所有后续步骤将会快得多。坏消息是它仅适用于所有步骤完全相同的操作，这意味着：
- en: having all tensors of the same length in all your batches
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有批次中具有相同长度的所有张量
- en: having static code (i.e., not a for loop of length that could change from step
    to step)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有静态代码（即，不是长度可能会在步骤之间改变的for循环）
- en: Having any of the things above change between two steps will trigger a new compilation
    which will, once again, take a lot of time. In practice, that means you must take
    special care to have all your tensors in your inputs of the same shape (so no
    dynamic padding for instance if you are in an NLP problem) and should not use
    layers with for loops that have different lengths depending on the inputs (such
    as an LSTM) or the training will be excruciatingly slow.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个步骤之间任何上述事物的改变都会触发新的编译，这将再次花费大量时间。实际上，这意味着您必须特别注意确保所有输入张量具有相同的形状（例如，如果您在处理自然语言处理问题，则不能使用动态填充），并且不应该使用具有不同长度的输入的for循环的层（例如LSTM），否则训练将变得非常缓慢。
- en: 'To introduce special behavior in your script for TPUs you can check the `distributed_type`
    of your `accelerator`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要在脚本中为TPU引入特殊行为，您可以检查`accelerator`的`distributed_type`：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The [NLP example](https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py)
    shows an example in a situation with dynamic padding.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[NLP示例](https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py)展示了一个具有动态填充情况的示例。'
- en: 'One last thing to pay close attention to: if your model has tied weights (such
    as language models which tie the weights of the embedding matrix with the weights
    of the decoder), moving this model to the TPU (either yourself or after you passed
    your model to [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare))
    will break the tying. You will need to retie the weights after. You can find an
    example of this in the [run_clm_no_trainer](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py)
    script in the Transformers repository.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要特别注意的一点是：如果您的模型具有绑定权重（例如将嵌入矩阵的权重与解码器的权重绑定的语言模型），将此模型移动到TPU（无论是您自己还是在将模型传递给[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)之后）将会破坏这种绑定。您需要在之后重新绑定权重。您可以在Transformers存储库中的[run_clm_no_trainer](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py)脚本中找到一个示例。
- en: Check out the [TPU tutorial](concept_guides/training_tpu) for more information
    about training on TPUs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[TPU教程](concept_guides/training_tpu)以获取有关在TPU上训练的更多信息。
- en: Execute a statement only on one processes
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仅在一个进程上执行语句
- en: 'Some of your instructions only need to run for one process on a given server:
    for instance a data download or a log statement. To do this, wrap the statement
    in a test like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您的一些指令只需要在给定服务器上的一个进程上运行：例如数据下载或日志语句。为此，请将语句包装在类似于以下的测试中：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Another example is progress bars: to avoid having multiple progress bars in
    your output, you should only display one on the local main process:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是进度条：为了避免在输出中有多个进度条，您应该仅在本地主进程上显示一个：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The *local* means per machine: if you are running your training on two servers
    with several GPUs, the instruction will be executed once on each of those servers.
    If you need to execute something only once for all processes (and not per machine)
    for instance, uploading the final model to the 🤗 model hub, wrap it in a test
    like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*本地*表示每台机器：如果您在两台具有多个GPU的服务器上运行训练，该指令将在每台服务器上执行一次。如果您需要为所有进程执行一次（而不是每台机器）的某些操作，例如将最终模型上传到🤗模型中心，请将其包装在类似于以下的测试中：'
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For printing statements you only want executed once per machine, you can just
    replace the `print` function by `accelerator.print`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于只想在每台机器上执行一次的打印语句，您可以将`print`函数替换为`accelerator.print`。
- en: Defer execution on multiple GPUs
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推迟在多个GPU上执行
- en: 'When you run your usual script, instructions are executed in order. Using 🤗
    Accelerate to deploy your script on several GPUs at the same time introduces a
    complication: while each process executes all instructions in order, some may
    be faster than others.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行您的常规脚本时，指令是按顺序执行的。使用🤗 Accelerate在多个GPU上同时部署您的脚本会引入一个复杂性：虽然每个进程按顺序执行所有指令，但有些可能比其他进程快。
- en: 'You might need to wait for all processes to have reached a certain point before
    executing a given instruction. For instance, you shouldn’t save a model before
    making sure every process is done with training. To do this, add the following
    line in your code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行特定指令之前，您可能需要等待所有进程都达到某个特定点。例如，在确保每个进程都完成训练之前，您不应该保存模型。为此，请在您的代码中添加以下行：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This instruction will block all the processes that arrive first until all the
    other processes have reached that point (if you run your script on just one GPU
    or CPU, this won’t do anything).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此指令将阻止所有最先到达的进程，直到所有其他进程都达到该点（如果您只在一个GPU或CPU上运行脚本，则不会执行任何操作）。
- en: Save/load a model in a distributed setup
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在分布式设置中保存/加载模型
- en: 'Saving the model you trained might need a bit of adjustment: first you should
    wait for all processes to reach that point in the script as shown above, and then,
    you should unwrap your model before saving it. This is because when going through
    the [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method, your model may have been placed inside a bigger model, which deals with
    the distributed training. This in turn means that saving your model state dictionary
    without taking any precaution will take that potential extra layer into account,
    and you will end up with weights you can’t load back in your base model. The [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    method will help you to achieve that. It will unwrap your model and save the model
    state dictionary.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 保存训练过的模型可能需要一些调整：首先，您应该等待所有进程达到脚本中显示的那一点，然后，您应该在保存之前取消包装模型。这是因为当通过[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)方法时，您的模型可能已放置在一个更大的模型中，该模型处理分布式训练。这反过来意味着，保存模型状态字典而不采取任何预防措施将考虑到该潜在的额外层，并且您最终将得到无法在基本模型中加载回的权重。[save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)方法将帮助您实现这一点。它将取消包装您的模型并保存模型状态字典。
- en: 'Here is an example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个例子：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    method can also save a model into sharded checkpoints or with safetensors format:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: save_model()方法还可以将模型保存为分片检查点或使用safetensors格式：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If your script contains logic to load a checkpoint, we also recommend you load
    your weights in the unwrapped model (this is only useful if you use the load function
    after making your model go through [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)).
    Here is an example:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的脚本包含加载检查点的逻辑，我们还建议您在未包装的模型中加载权重（仅在使模型通过[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)函数后使用加载函数时才有用）。这里是一个例子：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that since all the model parameters are references to tensors, this will
    load your weights inside `model`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于所有模型参数都是对张量的引用，这将在`model`内加载您的权重。
- en: 'If you want to load a sharded checkpoint or a checkpoint with safetensors format
    into the model with a specific `device`, we recommend you to load it with [load_checkpoint_in_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.load_checkpoint_in_model)
    function. Here’s an example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想将分片检查点或带有safetensors格式的检查点加载到具有特定`device`的模型中，我们建议您使用[load_checkpoint_in_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.load_checkpoint_in_model)函数加载。这里是一个例子：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Save/load entire states
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存/加载整个状态
- en: When training your model, you may want to save the current state of the model,
    optimizer, random generators, and potentially learning rate schedulers to be restored
    in the *same script*. You can use [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    and [load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)
    respectively to do so.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，您可能希望保存模型的当前状态、优化器、随机生成器以及可能的学习率调度器，以便在*同一脚本*中恢复。您可以分别使用[save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)和[load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)来实现。
- en: To further customize where and how states saved through [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    the [ProjectConfiguration](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.ProjectConfiguration)
    class can be used. For example if `automatic_checkpoint_naming` is enabled each
    saved checkpoint will be located then at `Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步自定义通过[save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)保存的状态的位置和方式，可以使用[ProjectConfiguration](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.ProjectConfiguration)类。例如，如果启用了`automatic_checkpoint_naming`，每个保存的检查点将位于`Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`。
- en: If you have registered any other stateful items to be stored through [register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing)
    they will also be saved and/or loaded.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经通过[register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing)注册了其他需要存储的有状态项目，它们也将被保存和/或加载。
- en: Every object passed to [register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing)
    must have a `load_state_dict` and `state_dict` function to be stored
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing)传递的每个对象必须具有`load_state_dict`和`state_dict`函数才能被存储
- en: Use gradient clipping
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用梯度裁剪
- en: If you are using gradient clipping in your script, you should replace the calls
    to `torch.nn.utils.clip_grad_norm_` or `torch.nn.utils.clip_grad_value_` with
    [clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)
    and [clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)
    respectively.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在脚本中使用梯度裁剪，应该将对`torch.nn.utils.clip_grad_norm_`或`torch.nn.utils.clip_grad_value_`的调用替换为[clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)和[clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)。
- en: Train with mixed precision
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用混合精度进行训练
- en: 'If you are running your training in Mixed Precision with 🤗 Accelerate, you
    will get the best result with your loss being computed inside your model (like
    in Transformer models for instance). Every computation outside of the model will
    be executed in full precision (which is generally what you want for loss computation,
    especially if it involves a softmax). However, you might want to put your loss
    computation inside the [autocast()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.autocast)
    context manager:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在🤗 Accelerate中以混合精度运行训练，则最好的结果是在模型内计算损失（例如在Transformer模型中）。模型外的每个计算将以完整精度执行（这通常是损失计算所需的，特别是如果涉及softmax）。但是，您可能希望将损失计算放在[autocast()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.autocast)上下文管理器中：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Another caveat with Mixed Precision training is that the gradient will skip
    a few updates at the beginning and sometimes during training: because of the dynamic
    loss scaling strategy, there are points during training where the gradients have
    overflown, and the loss scaling factor is reduced to avoid this happening again
    at the next step.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度训练的另一个注意事项是，梯度在开始时会跳过几次更新，有时在训练过程中也会跳过：由于动态损失缩放策略，训练过程中存在梯度溢出的点，损失缩放因子会减小，以避免在下一步再次发生这种情况。
- en: 'This means that you may update your learning rate scheduler when there was
    no update, which is fine in general, but may have an impact when you have very
    little training data, or if the first learning rate values of your scheduler are
    very important. In this case, you can skip the learning rate scheduler updates
    when the optimizer step was not done like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当没有更新时，您可以更新学习率调度器，这在一般情况下是可以的，但在训练数据很少或者调度器的第一个学习率值非常重要时可能会产生影响。在这种情况下，您可以跳过学习率调度器的更新，就像这样：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Use gradient accumulation
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用梯度累积
- en: 'To perform gradient accumulation use [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    and specify a `gradient_accumulation_steps`. This will also automatically ensure
    the gradients are synced or unsynced when on multi-device training, check if the
    step should actually be performed, and auto-scale the loss:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行梯度累积，请使用[accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)并指定`gradient_accumulation_steps`。这也会自动确保在多设备训练时梯度是同步的或异步的，检查步骤是否实际执行，并自动缩放损失：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
