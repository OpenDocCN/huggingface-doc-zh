- en: Installation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/installation](https://huggingface.co/docs/text-generation-inference/installation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/text-generation-inference/installation](https://huggingface.co/docs/text-generation-inference/installation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This section explains how to install the CLI tool as well as installing TGI
    from source. **The strongly recommended approach is to use Docker, as it does
    not require much setup. Check [the Quick Tour](./quicktour) to learn how to run
    TGI with Docker.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍如何安装CLI工具以及从源代码安装TGI。**强烈推荐的方法是使用Docker，因为它不需要太多设置。查看[快速入门](./quicktour)了解如何使用Docker运行TGI。**
- en: Install CLI
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装CLI
- en: You can use TGI command-line interface (CLI) to download weights, serve and
    quantize models, or get information on serving parameters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用TGI命令行界面（CLI）来下载权重，提供和量化模型，或获取有关提供参数的信息。
- en: To install the CLI, you need to first clone the TGI repository and then run
    `make`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装CLI，您需要首先克隆TGI存储库，然后运行`make`。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you would like to serve models with custom kernels, run
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要使用自定义内核提供模型，请运行
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Local Installation from Source
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从源代码进行本地安装
- en: Before you start, you will need to setup your environment, and install Text
    Generation Inference. Text Generation Inference is tested on **Python 3.9+**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，您需要设置您的环境，并安装文本生成推理。文本生成推理已在**Python 3.9+**上进行了测试。
- en: Text Generation Inference is available on pypi, conda and GitHub.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成推理可在pypi、conda和GitHub上获得。
- en: 'To install and launch locally, first [install Rust](https://rustup.rs/) and
    create a Python virtual environment with at least Python 3.9, e.g. using conda:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行安装和本地启动，首先[安装Rust](https://rustup.rs/)并创建一个至少包含Python 3.9的Python虚拟环境，例如使用conda：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You may also need to install Protoc.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还需要安装Protoc。
- en: 'On Linux:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'On MacOS, using Homebrew:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在MacOS上，使用Homebrew：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then run to install Text Generation Inference:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以安装文本生成推理：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On some machines, you may also need the OpenSSL libraries and gcc. On Linux
    machines, run:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些机器上，您可能还需要OpenSSL库和gcc。在Linux机器上运行：
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once installation is done, simply run:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，只需运行：
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will serve Falcon 7B Instruct model from the port 8080, which we can query.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从端口8080提供Falcon 7B Instruct模型，我们可以查询。
