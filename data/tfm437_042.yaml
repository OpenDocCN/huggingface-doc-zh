- en: Visual Question Answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§†è§‰é—®ç­”
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/visual_question_answering](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/visual_question_answering)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/visual_question_answering](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/visual_question_answering)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Visual Question Answering (VQA) is the task of answering open-ended questions
    based on an image. The input to models supporting this task is typically a combination
    of an image and a question, and the output is an answer expressed in natural language.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ˜¯æ ¹æ®å›¾åƒå›ç­”å¼€æ”¾å¼é—®é¢˜çš„ä»»åŠ¡ã€‚æ”¯æŒæ­¤ä»»åŠ¡çš„æ¨¡å‹çš„è¾“å…¥é€šå¸¸æ˜¯å›¾åƒå’Œé—®é¢˜çš„ç»„åˆï¼Œè¾“å‡ºæ˜¯ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„ç­”æ¡ˆã€‚
- en: 'Some noteworthy use case examples for VQA include:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: VQAçš„ä¸€äº›å€¼å¾—æ³¨æ„çš„ç”¨ä¾‹ç¤ºä¾‹åŒ…æ‹¬ï¼š
- en: Accessibility applications for visually impaired individuals.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§†éšœäººå£«çš„è¾…åŠ©åº”ç”¨ç¨‹åºã€‚
- en: 'Education: posing questions about visual materials presented in lectures or
    textbooks. VQA can also be utilized in interactive museum exhibits or historical
    sites.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•™è‚²ï¼šæå‡ºå…³äºè®²åº§æˆ–æ•™ç§‘ä¹¦ä¸­å‘ˆç°çš„è§†è§‰ææ–™çš„é—®é¢˜ã€‚VQAä¹Ÿå¯ä»¥ç”¨äºäº’åŠ¨åšç‰©é¦†å±•è§ˆæˆ–å†å²é—å€ã€‚
- en: 'Customer service and e-commerce: VQA can enhance user experience by letting
    users ask questions about products.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®¢æˆ·æœåŠ¡å’Œç”µå­å•†åŠ¡ï¼šVQAå¯ä»¥é€šè¿‡è®©ç”¨æˆ·è¯¢é—®æœ‰å…³äº§å“çš„é—®é¢˜æ¥å¢å¼ºç”¨æˆ·ä½“éªŒã€‚
- en: 'Image retrieval: VQA models can be used to retrieve images with specific characteristics.
    For example, the user can ask â€œIs there a dog?â€ to find all images with dogs from
    a set of images.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒæ£€ç´¢ï¼šVQAæ¨¡å‹å¯ç”¨äºæ£€ç´¢å…·æœ‰ç‰¹å®šç‰¹å¾çš„å›¾åƒã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥è¯¢é—®â€œæœ‰ç‹—å—ï¼Ÿâ€ä»¥æ‰¾åˆ°ä¸€ç»„å›¾åƒä¸­æ‰€æœ‰å¸¦æœ‰ç‹—çš„å›¾åƒã€‚
- en: 'In this guide youâ€™ll learn how to:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š
- en: Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt),
    on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨[`Graphcore/vqa`æ•°æ®é›†](https://huggingface.co/datasets/Graphcore/vqa)ä¸Šå¯¹åˆ†ç±»VQAæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯[ViLT](../model_doc/vilt)ï¼‰è¿›è¡Œå¾®è°ƒã€‚
- en: Use your fine-tuned ViLT for inference.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„ViLTè¿›è¡Œæ¨æ–­ã€‚
- en: Run zero-shot VQA inference with a generative model, like BLIP-2.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚BLIP-2ï¼‰è¿›è¡Œé›¶æ ·æœ¬VQAæ¨æ–­ã€‚
- en: Fine-tuning ViLT
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¾®è°ƒViLT
- en: ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing
    it to have a minimal design for Vision-and-Language Pre-training (VLP). This model
    can be used for several downstream tasks. For the VQA task, a classifier head
    is placed on top (a linear layer on top of the final hidden state of the `[CLS]`
    token) and randomly initialized. Visual Question Answering is thus treated as
    a **classification problem**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ViLTæ¨¡å‹å°†æ–‡æœ¬åµŒå…¥é›†æˆåˆ°Vision Transformerï¼ˆViTï¼‰ä¸­ï¼Œä½¿å…¶åœ¨è§†è§‰å’Œè¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ–¹é¢å…·æœ‰æœ€å°çš„è®¾è®¡ã€‚è¯¥æ¨¡å‹å¯ç”¨äºå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ã€‚å¯¹äºVQAä»»åŠ¡ï¼Œåˆ†ç±»å™¨å¤´éƒ¨æ”¾ç½®åœ¨é¡¶éƒ¨ï¼ˆçº¿æ€§å±‚æ”¾åœ¨`[CLS]`æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šï¼‰å¹¶éšæœºåˆå§‹åŒ–ã€‚å› æ­¤ï¼Œè§†è§‰é—®ç­”è¢«è§†ä¸º**åˆ†ç±»é—®é¢˜**ã€‚
- en: More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative
    task. Later in this guide we illustrate how to use them for zero-shot VQA inference.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„æ¨¡å‹ï¼Œå¦‚BLIPã€BLIP-2å’ŒInstructBLIPï¼Œå°†VQAè§†ä¸ºç”Ÿæˆä»»åŠ¡ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•å°†å®ƒä»¬ç”¨äºé›¶æ ·æœ¬VQAæ¨æ–­ã€‚
- en: Before you begin, make sure you have all the necessary libraries installed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to share your model with the community. Log in to your Hugging
    Face account to upload it to the ğŸ¤— Hub. When prompted, enter your token to log
    in:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚ç™»å½•åˆ°æ‚¨çš„Hugging Faceå¸æˆ·å°†å…¶ä¸Šä¼ åˆ°ğŸ¤— Hubã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Letâ€™s define the model checkpoint as a global variable.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†æ¨¡å‹æ£€æŸ¥ç‚¹å®šä¹‰ä¸ºå…¨å±€å˜é‡ã€‚
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Load the data
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®
- en: For illustration purposes, in this guide we use a very small sample of the annotated
    visual question answering `Graphcore/vqa` dataset. You can find the full dataset
    on [ğŸ¤— Hub](https://huggingface.co/datasets/Graphcore/vqa).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ºäºè¯´æ˜ç›®çš„ï¼Œåœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¸¦æ³¨é‡Šçš„è§†è§‰é—®ç­”`Graphcore/vqa`æ•°æ®é›†çš„ä¸€ä¸ªéå¸¸å°çš„æ ·æœ¬ã€‚æ‚¨å¯ä»¥åœ¨[ğŸ¤— Hub](https://huggingface.co/datasets/Graphcore/vqa)ä¸Šæ‰¾åˆ°å®Œæ•´çš„æ•°æ®é›†ã€‚
- en: As an alternative to the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa),
    you can download the same data manually from the official [VQA dataset page](https://visualqa.org/download.html).
    If you prefer to follow the tutorial with your custom data, check out how to [Create
    an image dataset](https://huggingface.co/docs/datasets/image_dataset#loading-script)
    guide in the ğŸ¤— Datasets documentation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºå¯¹[`Graphcore/vqa`æ•°æ®é›†](https://huggingface.co/datasets/Graphcore/vqa)çš„æ›¿ä»£ï¼Œæ‚¨å¯ä»¥ä»å®˜æ–¹çš„[VQAæ•°æ®é›†é¡µé¢](https://visualqa.org/download.html)æ‰‹åŠ¨ä¸‹è½½ç›¸åŒçš„æ•°æ®ã€‚å¦‚æœæ‚¨å¸Œæœ›ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è·Ÿéšæ•™ç¨‹ï¼Œè¯·æŸ¥çœ‹ğŸ¤—æ•°æ®é›†æ–‡æ¡£ä¸­çš„[åˆ›å»ºå›¾åƒæ•°æ®é›†](https://huggingface.co/docs/datasets/image_dataset#loading-script)æŒ‡å—ã€‚
- en: 'Letâ€™s load the first 200 examples from the validation split and explore the
    datasetâ€™s features:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åŠ è½½éªŒè¯é›†ä¸­çš„å‰200ä¸ªç¤ºä¾‹å¹¶æ¢ç´¢æ•°æ®é›†çš„ç‰¹ç‚¹ï¼š
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Letâ€™s take a look at an example to understand the datasetâ€™s features:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­æ¥äº†è§£æ•°æ®é›†çš„ç‰¹ç‚¹ï¼š
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The features relevant to the task include:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾åŒ…æ‹¬ï¼š
- en: '`question`: the question to be answered from the image'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question`ï¼šè¦ä»å›¾åƒå›ç­”çš„é—®é¢˜'
- en: '`image_id`: the path to the image the question refers to'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_id`ï¼šé—®é¢˜æ‰€æŒ‡å›¾åƒçš„è·¯å¾„'
- en: '`label`: the annotations'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`ï¼šæ³¨é‡Š'
- en: 'We can remove the rest of the features as they wonâ€™t be necessary:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åˆ é™¤å…¶ä½™çš„ç‰¹å¾ï¼Œå› ä¸ºå®ƒä»¬ä¸ä¼šæ˜¯å¿…è¦çš„ï¼š
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the `label` feature contains several answers to the same question
    (called `ids` here) collected by different human annotators. This is because the
    answer to a question can be subjective. In this case, the question is â€œwhere is
    he looking?â€œ. Some people annotated this with â€œdownâ€, others with â€œat tableâ€,
    another one with â€œskateboardâ€, etc.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œ`label`ç‰¹å¾åŒ…å«äº†åŒä¸€ä¸ªé—®é¢˜çš„å‡ ä¸ªç­”æ¡ˆï¼ˆè¿™é‡Œç§°ä¸º`ids`ï¼‰ï¼Œè¿™äº›ç­”æ¡ˆæ˜¯ç”±ä¸åŒçš„äººç±»æ³¨é‡Šè€…æ”¶é›†çš„ã€‚è¿™æ˜¯å› ä¸ºå¯¹é—®é¢˜çš„ç­”æ¡ˆå¯èƒ½æ˜¯ä¸»è§‚çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé—®é¢˜æ˜¯â€œä»–åœ¨çœ‹å“ªé‡Œï¼Ÿâ€ã€‚æœ‰äº›äººç”¨â€œå‘ä¸‹â€æ³¨é‡Šï¼Œå…¶ä»–äººç”¨â€œçœ‹ç€æ¡Œå­â€ï¼Œå¦ä¸€ä¸ªäººç”¨â€œæ»‘æ¿â€ç­‰ç­‰ã€‚
- en: 'Take a look at the image and consider which answer would you give:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸€çœ‹å›¾åƒï¼Œè€ƒè™‘ä½ ä¼šç»™å‡ºä»€ä¹ˆç­”æ¡ˆï¼š
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![VQA Image Example](../Images/236d4cd08347455e7e0a1b6648436b20.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![VQAå›¾åƒç¤ºä¾‹](../Images/236d4cd08347455e7e0a1b6648436b20.png)'
- en: Due to the questionsâ€™ and answersâ€™ ambiguity, datasets like this are treated
    as a multi-label classification problem (as multiple answers are possibly valid).
    Moreover, rather than just creating a one-hot encoded vector, one creates a soft
    encoding, based on the number of times a certain answer appeared in the annotations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºé—®é¢˜å’Œç­”æ¡ˆçš„æ¨¡ç³Šæ€§ï¼Œåƒè¿™æ ·çš„æ•°æ®é›†è¢«è§†ä¸ºå¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ï¼ˆå› ä¸ºå¯èƒ½æœ‰å¤šä¸ªç­”æ¡ˆæœ‰æ•ˆï¼‰ã€‚æ­¤å¤–ï¼Œä¸å…¶åªåˆ›å»ºä¸€ä¸ªç‹¬çƒ­ç¼–ç å‘é‡ï¼Œä¸å¦‚åˆ›å»ºä¸€ä¸ªè½¯ç¼–ç ï¼ŒåŸºäºæŸä¸ªç­”æ¡ˆåœ¨æ³¨é‡Šä¸­å‡ºç°çš„æ¬¡æ•°ã€‚
- en: For instance, in the example above, because the answer â€œdownâ€ is selected way
    more often than other answers, it has a score (called `weight` in the dataset)
    of 1.0, and the rest of the answers have scores < 1.0.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œå› ä¸ºç­”æ¡ˆâ€œdownâ€è¢«é€‰ä¸­çš„æ¬¡æ•°è¿œè¿œè¶…è¿‡å…¶ä»–ç­”æ¡ˆï¼Œå®ƒçš„å¾—åˆ†ï¼ˆæ•°æ®é›†ä¸­ç§°ä¸º`weight`ï¼‰ä¸º1.0ï¼Œè€Œå…¶ä½™ç­”æ¡ˆçš„å¾—åˆ†<1.0ã€‚
- en: 'To later instantiate the model with an appropriate classification head, letâ€™s
    create two dictionaries: one that maps the label name to an integer and vice versa:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä»¥åç”¨é€‚å½“çš„åˆ†ç±»å¤´å®ä¾‹åŒ–æ¨¡å‹ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸¤ä¸ªå­—å…¸ï¼šä¸€ä¸ªå°†æ ‡ç­¾åç§°æ˜ å°„åˆ°æ•´æ•°ï¼Œå¦ä¸€ä¸ªå°†æ•´æ•°æ˜ å°„å›æ ‡ç­¾åç§°ï¼š
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have the mappings, we can replace the string answers with their
    ids, and flatten the dataset for a more convenient further preprocessing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ˜ å°„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒä»¬çš„idæ›¿æ¢å­—ç¬¦ä¸²ç­”æ¡ˆï¼Œå¹¶å°†æ•°æ®é›†æ‰å¹³åŒ–ï¼Œä»¥ä¾¿è¿›è¡Œæ›´æ–¹ä¾¿çš„è¿›ä¸€æ­¥é¢„å¤„ç†ã€‚
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Preprocessing data
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®é¢„å¤„ç†
- en: 'The next step is to load a ViLT processor to prepare the image and text data
    for the model. [ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)
    wraps a BERT tokenizer and ViLT image processor into a convenient single processor:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ViLTå¤„ç†å™¨ï¼Œä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒå’Œæ–‡æœ¬æ•°æ®ã€‚[ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)å°†BERTæ ‡è®°å™¨å’ŒViLTå›¾åƒå¤„ç†å™¨å°è£…åˆ°ä¸€ä¸ªæ–¹ä¾¿çš„å•å¤„ç†å™¨ä¸­ï¼š
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To preprocess the data we need to encode the images and questions using the
    [ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor).
    The processor will use the [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids`
    for the text data. As for images, the processor will leverage [ViltImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltImageProcessor)
    to resize and normalize the image, and create `pixel_values` and `pixel_mask`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¢„å¤„ç†æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨[ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)å¯¹å›¾åƒå’Œé—®é¢˜è¿›è¡Œç¼–ç ã€‚å¤„ç†å™¨å°†ä½¿ç”¨[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶ä¸ºæ–‡æœ¬æ•°æ®åˆ›å»º`input_ids`ã€`attention_mask`å’Œ`token_type_ids`ã€‚è‡³äºå›¾åƒï¼Œå¤„ç†å™¨å°†åˆ©ç”¨[ViltImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltImageProcessor)æ¥è°ƒæ•´å¤§å°å’Œè§„èŒƒåŒ–å›¾åƒï¼Œå¹¶åˆ›å»º`pixel_values`å’Œ`pixel_mask`ã€‚
- en: All these preprocessing steps are done under the hood, we only need to call
    the `processor`. However, we still need to prepare the target labels. In this
    representation, each element corresponds to a possible answer (label). For correct
    answers, the element holds their respective score (weight), while the remaining
    elements are set to zero.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›é¢„å¤„ç†æ­¥éª¤éƒ½æ˜¯åœ¨å¹•åå®Œæˆçš„ï¼Œæˆ‘ä»¬åªéœ€è¦è°ƒç”¨`processor`ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦å‡†å¤‡ç›®æ ‡æ ‡ç­¾ã€‚åœ¨è¿™ç§è¡¨ç¤ºä¸­ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼ˆæ ‡ç­¾ï¼‰ã€‚å¯¹äºæ­£ç¡®ç­”æ¡ˆï¼Œå…ƒç´ ä¿å­˜å…¶ç›¸åº”çš„åˆ†æ•°ï¼ˆæƒé‡ï¼‰ï¼Œè€Œå…¶ä½™å…ƒç´ è®¾ç½®ä¸ºé›¶ã€‚
- en: 'The following function applies the `processor` to the images and questions
    and formats the labels as described above:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å‡½æ•°å°†`processor`åº”ç”¨äºå›¾åƒå’Œé—®é¢˜ï¼Œå¹¶æŒ‰ä¸Šè¿°æè¿°æ ¼å¼åŒ–æ ‡ç­¾ï¼š
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets
    `map` function. You can speed up `map` by setting `batched=True` to process multiple
    elements of the dataset at once. At this point, feel free to remove the columns
    you donâ€™t need.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨ğŸ¤— Datasetsçš„`map`å‡½æ•°ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®`batched=True`æ¥åŠ é€Ÿ`map`ï¼Œä»¥ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ã€‚æ­¤æ—¶ï¼Œå¯ä»¥éšæ„åˆ é™¤ä¸éœ€è¦çš„åˆ—ã€‚
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As a final step, create a batch of examples using [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæœ€åä¸€æ­¥ï¼Œä½¿ç”¨[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ï¼š
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Train the model
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹
- en: 'Youâ€™re ready to start training your model now! Load ViLT with [ViltForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForQuestionAnswering).
    Specify the number of labels along with the label mappings:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[ViltForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForQuestionAnswering)åŠ è½½ViLTã€‚æŒ‡å®šæ ‡ç­¾æ•°é‡ä»¥åŠæ ‡ç­¾æ˜ å°„ï¼š
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At this point, only three steps remain:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: 'Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments):'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ï¼š
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, processor, and data collator.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼ŒåŒæ—¶è¿˜éœ€è¦ä¼ é€’æ¨¡å‹ã€æ•°æ®é›†ã€å¤„ç†å™¨å’Œæ•°æ®æ”¶é›†å™¨ã€‚
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method to share your final model on the ğŸ¤— Hub:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ°ğŸ¤—
    Hubä¸Šï¼š
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Inference
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Now that you have fine-tuned a ViLT model, and uploaded it to the ğŸ¤— Hub, you
    can use it for inference. The simplest way to try out your fine-tuned model for
    inference is to use it in a [Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å¯¹ViLTæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ°ğŸ¤— Hubï¼Œæ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ã€‚å°è¯•ä½¿ç”¨[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)ä¸­çš„å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•ã€‚
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The model in this guide has only been trained on 200 examples, so donâ€™t expect
    a lot from it. Letâ€™s see if it at least learned something from the data and take
    the first example from the dataset to illustrate inference:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—ä¸­çš„æ¨¡å‹ä»…åœ¨200ä¸ªç¤ºä¾‹ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå› æ­¤ä¸è¦å¯¹å…¶æŠ±æœ‰å¾ˆå¤§æœŸæœ›ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦è‡³å°‘ä»æ•°æ®ä¸­å­¦åˆ°äº†ä¸€äº›ä¸œè¥¿ï¼Œå¹¶ä»æ•°æ®é›†ä¸­å–ç¬¬ä¸€ä¸ªç¤ºä¾‹æ¥è¯´æ˜æ¨ç†ï¼š
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Even though not very confident, the model indeed has learned something. With
    more examples and longer training, youâ€™ll get far better results!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä¸æ˜¯å¾ˆè‡ªä¿¡ï¼Œä½†æ¨¡å‹ç¡®å®å­¦åˆ°äº†ä¸€äº›ä¸œè¥¿ã€‚æœ‰äº†æ›´å¤šçš„ä¾‹å­å’Œæ›´é•¿çš„è®­ç»ƒï¼Œä½ ä¼šå¾—åˆ°æ›´å¥½çš„ç»“æœï¼
- en: 'You can also manually replicate the results of the pipeline if youâ€™d like:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ç®¡é“çš„ç»“æœï¼š
- en: Take an image and a question, prepare them for the model using the processor
    from your model.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‹¿ä¸€å¼ å›¾ç‰‡å’Œä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ä½ æ¨¡å‹çš„å¤„ç†å™¨ä¸ºæ¨¡å‹å‡†å¤‡å®ƒä»¬ã€‚
- en: Forward the result or preprocessing through the model.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç»“æœæˆ–é¢„å¤„ç†é€šè¿‡æ¨¡å‹ä¼ é€’ã€‚
- en: From the logits, get the most likely answerâ€™s id, and find the actual answer
    in the `id2label`.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»logitsä¸­è·å–æœ€å¯èƒ½ç­”æ¡ˆçš„idï¼Œå¹¶åœ¨`id2label`ä¸­æ‰¾åˆ°å®é™…ç­”æ¡ˆã€‚
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Zero-shot VQA
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›¶æ ·æœ¬VQA
- en: The previous model treated VQA as a classification task. Some recent models,
    such as BLIP, BLIP-2, and InstructBLIP approach VQA as a generative task. Letâ€™s
    take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language
    pre-training paradigm in which any combination of pre-trained vision encoder and
    LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)).
    This enables achieving state-of-the-art results on multiple visual-language tasks
    including visual question answering.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆå‰çš„æ¨¡å‹å°†VQAè§†ä¸ºåˆ†ç±»ä»»åŠ¡ã€‚ä¸€äº›æœ€è¿‘çš„æ¨¡å‹ï¼Œå¦‚BLIPã€BLIP-2å’ŒInstructBLIPï¼Œå°†VQAè§†ä¸ºç”Ÿæˆä»»åŠ¡ã€‚è®©æˆ‘ä»¬ä»¥[BLIP-2](../model_doc/blip-2)ä¸ºä¾‹ã€‚å®ƒå¼•å…¥äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒèŒƒå¼ï¼Œå…¶ä¸­å¯ä»¥ä½¿ç”¨ä»»ä½•ç»„åˆçš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’ŒLLMï¼ˆåœ¨[BLIP-2åšå®¢æ–‡ç« ](https://huggingface.co/blog/blip-2)ä¸­äº†è§£æ›´å¤šï¼‰ã€‚è¿™ä½¿å¾—åœ¨å¤šä¸ªè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­åŒ…æ‹¬è§†è§‰é—®ç­”ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
- en: 'Letâ€™s illustrate how you can use this model for VQA. First, letâ€™s load the
    model. Here weâ€™ll explicitly send the model to a GPU, if available, which we didnâ€™t
    need to do earlier when training, as [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    handles this automatically:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯´æ˜å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡ŒVQAã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬åŠ è½½æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œå¦‚æœå¯ç”¨ï¼Œæˆ‘ä»¬å°†æ˜ç¡®å°†æ¨¡å‹å‘é€åˆ°GPUï¼Œè¿™åœ¨è®­ç»ƒæ—¶ä¸éœ€è¦åšï¼Œå› ä¸º[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä¼šè‡ªåŠ¨å¤„ç†ï¼š
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The model takes image and text as input, so letâ€™s use the exact same image/question
    pair from the first example in the VQA dataset:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹å°†å›¾åƒå’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤è®©æˆ‘ä»¬ä½¿ç”¨VQAæ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸­å®Œå…¨ç›¸åŒçš„å›¾åƒ/é—®é¢˜å¯¹ï¼š
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To use BLIP-2 for visual question answering task, the textual prompt has to
    follow a specific format: `Question: {} Answer:`.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†BLIP-2ç”¨äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼Œæ–‡æœ¬æç¤ºå¿…é¡»éµå¾ªç‰¹å®šæ ¼å¼ï¼š`é—®é¢˜ï¼š{} ç­”æ¡ˆï¼š`ã€‚
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we need to preprocess the image/prompt with the modelâ€™s processor, pass
    the processed input through the model, and decode the output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦ä½¿ç”¨æ¨¡å‹çš„å¤„ç†å™¨å¯¹å›¾åƒ/æç¤ºè¿›è¡Œé¢„å¤„ç†ï¼Œé€šè¿‡æ¨¡å‹ä¼ é€’å¤„ç†åçš„è¾“å…¥ï¼Œå¹¶è§£ç è¾“å‡ºï¼š
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, the model recognized the crowd, and the direction of the face
    (looking down), however, it seems to miss the fact the crowd is behind the skater.
    Still, in cases where acquiring human-annotated datasets is not feasible, this
    approach can quickly produce useful results.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæ¨¡å‹è¯†åˆ«äº†äººç¾¤å’Œè„¸éƒ¨çš„æ–¹å‘ï¼ˆå‘ä¸‹çœ‹ï¼‰ï¼Œä½†ä¼¼ä¹å¿½ç•¥äº†äººç¾¤åœ¨æ»‘å†°è€…åé¢çš„äº‹å®ã€‚ç„¶è€Œï¼Œåœ¨æ— æ³•è·å–äººç±»æ³¨é‡Šæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¿«é€Ÿäº§ç”Ÿæœ‰ç”¨çš„ç»“æœã€‚
