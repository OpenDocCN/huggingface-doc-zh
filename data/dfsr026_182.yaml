- en: Würstchen
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 香肠
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen](https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen](https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e29b341d4d756c0a6cf4d1a787243bfc.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e29b341d4d756c0a6cf4d1a787243bfc.png)'
- en: '[Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion
    Models](https://huggingface.co/papers/2306.00637) is by Pablo Pernias, Dominic
    Rampas, Mats L. Richter and Christopher Pal and Marc Aubreville.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wuerstchen：大规模文本到图像扩散模型的高效架构](https://huggingface.co/papers/2306.00637) 由Pablo
    Pernias、Dominic Rampas、Mats L. Richter、Christopher Pal和Marc Aubreville撰写。'
- en: 'The abstract from the paper is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We introduce Würstchen, a novel architecture for text-to-image synthesis that
    combines competitive performance with unprecedented cost-effectiveness for large-scale
    text-to-image diffusion models. A key contribution of our work is to develop a
    latent diffusion technique in which we learn a detailed but extremely compact
    semantic image representation used to guide the diffusion process. This highly
    compressed representation of an image provides much more detailed guidance compared
    to latent representations of language and this significantly reduces the computational
    requirements to achieve state-of-the-art results. Our approach also improves the
    quality of text-conditioned image generation based on our user preference study.
    The training requirements of our approach consists of 24,602 A100-GPU hours -
    compared to Stable Diffusion 2.1’s 200,000 GPU hours. Our approach also requires
    less training data to achieve these results. Furthermore, our compact latent representations
    allows us to perform inference over twice as fast, slashing the usual costs and
    carbon footprint of a state-of-the-art (SOTA) diffusion model significantly, without
    compromising the end performance. In a broader comparison against SOTA models
    our approach is substantially more efficient and compares favorably in terms of
    image quality. We believe that this work motivates more emphasis on the prioritization
    of both performance and computational accessibility.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们介绍了香肠，这是一种新颖的文本到图像合成架构，结合了竞争性能和大规模文本到图像扩散模型的前所未有的成本效益。我们工作的一个关键贡献是开发一种潜在扩散技术，通过这种技术我们学习了一种详细但极其紧凑的语义图像表示，用于引导扩散过程。与语言的潜在表示相比，这种高度压缩的图像表示提供了更详细的引导，这显著降低了实现最先进结果所需的计算要求。我们的方法还根据用户偏好研究改进了基于文本条件的图像生成的质量。我们的方法的训练需求为24,602个A100-GPU小时
    - 相比于Stable Diffusion 2.1的200,000个GPU小时。我们的方法还需要更少的训练数据来实现这些结果。此外，我们紧凑的潜在表示使我们能够执行两倍速度的推断，大幅削减了一种最先进（SOTA）扩散模型的通常成本和碳足迹，而不会影响最终性能。在与SOTA模型的更广泛比较中，我们的方法在效率上更为显著，并在图像质量方面具有可比性。我们相信这项工作促使更多关注性能和计算可访问性的优先考虑。*'
- en: Würstchen Overview
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 香肠概述
- en: Würstchen is a diffusion model, whose text-conditional model works in a highly
    compressed latent space of images. Why is this important? Compressing data can
    reduce computational costs for both training and inference by magnitudes. Training
    on 1024x1024 images is way more expensive than training on 32x32\. Usually, other
    works make use of a relatively small compression, in the range of 4x - 8x spatial
    compression. Würstchen takes this to an extreme. Through its novel design, we
    achieve a 42x spatial compression. This was unseen before because common methods
    fail to faithfully reconstruct detailed images after 16x spatial compression.
    Würstchen employs a two-stage compression, what we call Stage A and Stage B. Stage
    A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found
    in the [paper](https://huggingface.co/papers/2306.00637)). A third model, Stage
    C, is learned in that highly compressed latent space. This training requires fractions
    of the compute used for current top-performing models, while also allowing cheaper
    and faster inference.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 香肠是一种扩散模型，其文本条件模型在图像的高度压缩潜在空间中运行。为什么这很重要？压缩数据可以将训练和推断的计算成本降低数倍。在1024x1024图像上训练比在32x32图像上训练要昂贵得多。通常，其他作品使用相对较小的压缩，范围在4倍至8倍的空间压缩。香肠将这一点推向了极端。通过其新颖的设计，我们实现了42倍的空间压缩。这之前是看不到的，因为常见方法在16倍空间压缩后无法忠实地重建详细图像。香肠采用了两阶段压缩，我们称之为A阶段和B阶段。A阶段是VQGAN，B阶段是扩散自动编码器（更多细节可以在[论文](https://huggingface.co/papers/2306.00637)中找到）。第三个模型，C阶段，在这个高度压缩的潜在空间中学习。这种训练只需要当前性能最佳模型使用的计算的一小部分，同时还可以实现更便宜和更快的推断。
- en: Würstchen v2 comes to Diffusers
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 香肠v2来到Diffusers
- en: After the initial paper release, we have improved numerous things in the architecture,
    training and sampling, making Würstchen competitive to current state-of-the-art
    models in many ways. We are excited to release this new version together with
    Diffusers. Here is a list of the improvements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的论文发布后，我们在架构、训练和采样方面改进了许多内容，使香肠在许多方面与当前最先进的模型竞争力十足。我们很高兴与Diffusers一起发布这个新版本。以下是改进列表。
- en: Higher resolution (1024x1024 up to 2048x2048)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高分辨率（从1024x1024到2048x2048）
- en: Faster inference
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快的推断
- en: Multi Aspect Resolution Sampling
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多方面分辨率采样
- en: Better quality
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的质量
- en: 'We are releasing 3 checkpoints for the text-conditional image generation model
    (Stage C). Those are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发布了文本条件图像生成模型（C阶段）的3个检查点。它们是：
- en: v2-base
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: v2-基础
- en: v2-aesthetic
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: v2-美学
- en: '**(default)** v2-interpolated (50% interpolation between v2-base and v2-aesthetic)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （默认）v2-插值（在v2-基础和v2-美学之间进行50%的插值）
- en: 'We recommend using v2-interpolated, as it has a nice touch of both photorealism
    and aesthetics. Use v2-base for finetunings as it does not have a style bias and
    use v2-aesthetic for very artistic generations. A comparison can be seen here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用v2-插值，因为它既具有照片逼真感又具有美学感。对于微调，请使用v2-基础，因为它没有风格偏见，对于非常艺术的生成，请使用v2-美学。可以在这里看到比较：
- en: '![](../Images/b06dcd2bc467532747508fc8c277921a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b06dcd2bc467532747508fc8c277921a.png)'
- en: Text-to-Image Generation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像生成
- en: 'For the sake of usability, Würstchen can be used with a single pipeline. This
    pipeline can be used as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可用性，Würstchen 可以与单个管道一起使用。可以按以下方式使用此管道：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For explanation purposes, we can also initialize the two main pipelines of
    Würstchen individually. Würstchen consists of 3 stages: Stage C, Stage B, Stage
    A. They all have different jobs and work only together. When generating text-conditional
    images, Stage C will first generate the latents in a very compressed latent space.
    This is what happens in the `prior_pipeline`. Afterwards, the generated latents
    will be passed to Stage B, which decompresses the latents into a bigger latent
    space of a VQGAN. These latents can then be decoded by Stage A, which is a VQGAN,
    into the pixel-space. Stage B & Stage A are both encapsulated in the `decoder_pipeline`.
    For more details, take a look at the [paper](https://huggingface.co/papers/2306.00637).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释目的，我们还可以单独初始化 Würstchen 的两个主要管道。Würstchen 由 3 个阶段组成：阶段 C、阶段 B、阶段 A。它们都有不同的工作，并且只能一起工作。在生成文本条件图像时，阶段
    C 将首先在非常压缩的潜在空间中生成潜在变量。这就是在 `prior_pipeline` 中发生的事情。然后，生成的潜在变量将传递给阶段 B，后者将潜在变量解压缩为
    VQGAN 的更大潜在空间。然后，这些潜在变量可以由阶段 A 解码为像素空间。阶段 B 和阶段 A 都封装在 `decoder_pipeline` 中。有关更多详细信息，请查看
    [paper](https://huggingface.co/papers/2306.00637)。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Speed-Up Inference
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速推理
- en: 'You can make use of `torch.compile` function and gain a speed-up of about 2-3x:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `torch.compile` 函数，获得大约 2-3 倍的加速：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Limitations
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Due to the high compression employed by Würstchen, generations can lack a good
    amount of detail. To our human eye, this is especially noticeable in faces, hands
    etc.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 Würstchen 使用了高度压缩，生成的图像可能缺乏大量细节。对于我们的肉眼来说，这在面部、手部等方面尤为明显。
- en: '**Images can only be generated in 128-pixel steps**, e.g. the next higher resolution
    after 1024x1024 is 1152x1152'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像只能以 128 像素的步长生成**，例如，1024x1024 之后的下一个更高分辨率是 1152x1152'
- en: The model lacks the ability to render correct text in images
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型缺乏在图像中正确呈现文本的能力
- en: The model often does not achieve photorealism
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型通常无法实现照片级逼真度
- en: Difficult compositional prompts are hard for the model
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的组合提示对模型来说很难
- en: The original codebase, as well as experimental ideas, can be found at [dome272/Wuerstchen](https://github.com/dome272/Wuerstchen).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码库以及实验性想法可以在 [dome272/Wuerstchen](https://github.com/dome272/Wuerstchen)
    找到。
- en: WuerstchenCombinedPipeline
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenCombinedPipeline
- en: '### `class diffusers.WuerstchenCombinedPipeline`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.WuerstchenCombinedPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L43)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L43)'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`CLIPTokenizer`) — The decoder tokenizer to be used for text inputs.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — 用于文本输入的解码器分词器。'
- en: '`text_encoder` (`CLIPTextModel`) — The decoder text encoder to be used for
    text inputs.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — 用于文本输入的解码器文本编码器。'
- en: '`decoder` (`WuerstchenDiffNeXt`) — The decoder model to be used for decoder
    image generation pipeline.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` (`WuerstchenDiffNeXt`) — 用于解码器图像生成管道的解码器模型。'
- en: '`scheduler` (`DDPMWuerstchenScheduler`) — The scheduler to be used for decoder
    image generation pipeline.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`DDPMWuerstchenScheduler`) — 用于解码器图像生成管道的调度器。'
- en: '`vqgan` (`PaellaVQModel`) — The VQGAN model to be used for decoder image generation
    pipeline.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqgan` (`PaellaVQModel`) — 用于解码器图像生成管道的 VQGAN 模型。'
- en: '`prior_tokenizer` (`CLIPTokenizer`) — The prior tokenizer to be used for text
    inputs.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_tokenizer` (`CLIPTokenizer`) — 用于文本输入的先前分词器。'
- en: '`prior_text_encoder` (`CLIPTextModel`) — The prior text encoder to be used
    for text inputs.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_text_encoder` (`CLIPTextModel`) — 用于文本输入的先前文本编码器。'
- en: '`prior_prior` (`WuerstchenPrior`) — The prior model to be used for prior pipeline.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_prior` (`WuerstchenPrior`) — 用于先前管道的先前模型。'
- en: '`prior_scheduler` (`DDPMWuerstchenScheduler`) — The scheduler to be used for
    prior pipeline.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler` (`DDPMWuerstchenScheduler`) — 用于先前管道的调度器。'
- en: Combined Pipeline for text-to-image generation using Wuerstchen
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Wuerstchen 进行文本到图像生成的组合管道
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。检查超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L143)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L143)'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`) — The prompt or prompts to guide the image
    generation for the prior and decoder.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`) — 用于指导先前和解码器图像生成的提示或提示。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果不使用指导（即如果
    `guidance_scale` 小于 `1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings
    for the prior. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, text embeddings will be generated from `prompt` input argument.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 用于先前的预生成文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings for the prior. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 用于先前的预生成负面文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从
    `negative_prompt` 输入参数生成负面提示嵌入。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, 默认为1) — 每个提示生成的图像数量。'
- en: '`height` (`int`, *optional*, defaults to 512) — The height in pixels of the
    generated image.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, 默认为512) — 生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to 512) — The width in pixels of the generated
    image.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, 默认为512) — 生成图像的像素宽度。'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) — Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `prior_guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `prior_guidance_scale > 1`. Higher guidance
    scale encourages to generate images that are closely linked to the text `prompt`,
    usually at the expense of lower image quality.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale` (`float`, *optional*, 默认为4.0) — 如[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`prior_guidance_scale`定义为[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`prior_guidance_scale
    > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。'
- en: '`prior_num_inference_steps` (`Union[int, Dict[float, int]]`, *optional*, defaults
    to 60) — The number of prior denoising steps. More denoising steps usually lead
    to a higher quality image at the expense of slower inference. For more specific
    timestep spacing, you can pass customized `prior_timesteps`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps` (`Union[int, Dict[float, int]]`, *optional*, 默认为60)
    — 先验去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。要获得更具体的时间步长间距，可以传递自定义的`prior_timesteps`。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 12) — The number of decoder
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference. For more specific timestep spacing, you can pass
    customized `timesteps`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, 默认为12) — 解码器去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。要获得更具体的时间步长间距，可以传递自定义的`timesteps`。'
- en: '`prior_timesteps` (`List[float]`, *optional*) — Custom timesteps to use for
    the denoising process for the prior. If not defined, equal spaced `prior_num_inference_steps`
    timesteps are used. Must be in descending order.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_timesteps` (`List[float]`, *optional*) — 用于先验去噪过程的自定义时间步长。如果未定义，则使用等间距的`prior_num_inference_steps`时间步长。必须按降序排列。'
- en: '`decoder_timesteps` (`List[float]`, *optional*) — Custom timesteps to use for
    the denoising process for the decoder. If not defined, equal spaced `num_inference_steps`
    timesteps are used. Must be in descending order.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_timesteps` (`List[float]`, *optional*) — 用于解码器去噪过程的自定义时间步长。如果未定义，则使用等间距的`num_inference_steps`时间步长。必须按降序排列。'
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 0.0) — Guidance
    scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_guidance_scale` (`float`, *optional*, 默认为0.0) — 如[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`定义为[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`或`List[torch.Generator]`, *optional*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 预先生成的嘈杂潜在向量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则将使用提供的随机`generator`进行采样生成潜在向量。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。可在`"pil"`（`PIL.Image.Image`）、`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）之间选择。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。'
- en: '`prior_callback_on_step_end` (`Callable`, *optional*) — A function that calls
    at the end of each denoising steps during the inference. The function is called
    with the following arguments: `prior_callback_on_step_end(self: DiffusionPipeline,
    step: int, timestep: int, callback_kwargs: Dict)`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_callback_on_step_end` (`Callable`, *optional*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`prior_callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。'
- en: '`prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list
    of tensor inputs for the `prior_callback_on_step_end` function. The tensors specified
    in the list will be passed as `callback_kwargs` argument. You will only be able
    to include variables listed in the `._callback_tensor_inputs` attribute of your
    pipeline class.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) — `prior_callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在您的管道类的`._callback_tensor_inputs`属性中列出的变量。'
- en: '`callback_on_step_end` (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Callable`, *可选*) — 在推断期间每个去噪步骤结束时调用的函数。该函数使用以下参数调用：`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 调用管道以进行生成时调用的函数。
- en: 'Examples:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `enable_model_cpu_offload`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_model_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L115)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L115)'
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Offloads all models to CPU using accelerate, reducing memory usage with a low
    impact on performance. Compared to `enable_sequential_cpu_offload`, this method
    moves one whole model at a time to the GPU when its `forward` method is called,
    and the model remains in GPU until the next model runs. Memory savings are lower
    than with `enable_sequential_cpu_offload`, but performance is much better due
    to the iterative execution of the `unet`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加速将所有模型转移到CPU，减少内存使用并对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将一个完整模型移至GPU，并且该模型保持在GPU中，直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较低，但由于`unet`的迭代执行，性能更好。
- en: '#### `enable_sequential_cpu_offload`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_sequential_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L125)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L125)'
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Offloads all models (`unet`, `text_encoder`, `vae`, and `safety checker` state
    dicts) to CPU using 🤗 Accelerate, significantly reducing memory usage. Models
    are moved to a `torch.device('meta')` and loaded on a GPU only when their specific
    submodule’s `forward` method is called. Offloading happens on a submodule basis.
    Memory savings are higher than using `enable_model_cpu_offload`, but performance
    is lower.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用🤗 Accelerate将所有模型（`unet`、`text_encoder`、`vae`和`safety checker`状态字典）全部转移到CPU，显著减少内存使用。模型被移动到`torch.device('meta')`，仅当调用其特定子模块的`forward`方法时才会在GPU上加载。转移是基于子模块的。内存节省高于使用`enable_model_cpu_offload`，但性能较低。
- en: WuerstchenPriorPipeline
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenPriorPipeline
- en: '### `class diffusers.WuerstchenPriorPipeline`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.WuerstchenPriorPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L65)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L65)'
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prior` (`Prior`) — The canonical unCLIP prior to approximate the image embedding
    from the text embedding.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior` (`Prior`) — 用于从文本嵌入逼近图像嵌入的经典unCLIP先验。'
- en: '`text_encoder` (`CLIPTextModelWithProjection`) — Frozen text-encoder.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。'
- en: '`tokenizer` (`CLIPTokenizer`) — Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。'
- en: '`scheduler` (`DDPMWuerstchenScheduler`) — A scheduler to be used in combination
    with `prior` to generate image embedding.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`DDPMWuerstchenScheduler`) — 与`prior`结合使用的调度器，用于生成图像嵌入。'
- en: '`latent_mean` (‘float’, *optional*, defaults to 42.0) — Mean value for latent
    diffusers.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_mean`（‘float’, *可选*, 默认为42.0）— 潜在扩散的均值。'
- en: '`latent_std` (‘float’, *optional*, defaults to 1.0) — Standard value for latent
    diffusers.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_std`（‘float’, *可选*, 默认为1.0）— 潜在扩散的标准值。'
- en: '`resolution_multiple` (‘float’, *optional*, defaults to 42.67) — Default resolution
    for multiple images generated.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resolution_multiple`（‘float’, *可选*, 默认为42.67）— 生成多个图像的默认分辨率。'
- en: Pipeline for generating image prior for Wuerstchen.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成Wuerstchen图像先验的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档，了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道还继承了以下加载方法：
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    用于加载LoRA权重'
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    用于保存LoRA权重'
- en: '#### `__call__`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L280)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L280)'
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`) — The prompt or prompts to guide the image
    generation.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示或提示。'
- en: '`height` (`int`, *optional*, defaults to 1024) — The height in pixels of the
    generated image.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *可选*, 默认为1024) — 生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to 1024) — The width in pixels of the
    generated image.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, defaults to 1024) — 生成图像的像素宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 60) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, defaults to 60) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但推理速度会变慢。'
- en: '`timesteps` (`List[int]`, *optional*) — Custom timesteps to use for the denoising
    process. If not defined, equal spaced `num_inference_steps` timesteps are used.
    Must be in descending order.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps` (`List[int]`, *optional*) — 用于去噪过程的自定义时间步。如果未定义，则使用等间距的 `num_inference_steps`
    个时间步。必须按降序排列。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 8.0) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `decoder_guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `decoder_guidance_scale > 1`. Higher guidance
    scale encourages to generate images that are closely linked to the text `prompt`,
    usually at the expense of lower image quality.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, defaults to 8.0) — 如 [Classifier-Free
    Diffusion Guidance](https://arxiv.org/abs/2207.12598) 中定义的引导比例。`decoder_guidance_scale`
    定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 方程式 2 的 `w`。通过设置 `decoder_guidance_scale
    > 1` 启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `decoder_guidance_scale` is less than `1`).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 不用来引导图像生成的提示或提示。当不使用引导时（即，如果
    `decoder_guidance_scale` 小于 `1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如调整提示权重。如果未提供，文本嵌入将从
    `prompt` 输入参数中生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如调整提示权重。如果未提供，将从
    `negative_prompt` 输入参数中生成负文本嵌入。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 一个或多个
    [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 预生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。可选择的格式包括：`"pil"`
    (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回一个 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    而不是一个普通的元组。'
- en: '`callback_on_step_end` (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Callable`, *optional*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`
    将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end`
    函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs`
    属性中列出的变量。'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 调用管道进行生成时调用的函数。
- en: 'Examples:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: WuerstchenPriorPipelineOutput
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenPriorPipelineOutput
- en: '### `class diffusers.pipelines.wuerstchen.pipeline_wuerstchen_prior.WuerstchenPriorPipelineOutput`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.pipelines.wuerstchen.pipeline_wuerstchen_prior.WuerstchenPriorPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L51)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L51)'
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_embeddings` (`torch.FloatTensor` or `np.ndarray`) — Prior image embeddings
    for text prompt'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeddings` (`torch.FloatTensor` or `np.ndarray`) — 用于文本提示的先验图像嵌入。'
- en: Output class for WuerstchenPriorPipeline.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: WuerstchenPriorPipeline 的输出类。
- en: WuerstchenDecoderPipeline
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WuerstchenDecoderPipeline
- en: '### `class diffusers.WuerstchenDecoderPipeline`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.WuerstchenDecoderPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L51)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L51)'
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`CLIPTokenizer`) — The CLIP tokenizer.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — CLIP 分词器。'
- en: '`text_encoder` (`CLIPTextModel`) — The CLIP text encoder.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — CLIP 文本编码器。'
- en: '`decoder` (`WuerstchenDiffNeXt`) — The WuerstchenDiffNeXt unet decoder.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` (`WuerstchenDiffNeXt`) — WuerstchenDiffNeXt unet 解码器。'
- en: '`vqgan` (`PaellaVQModel`) — The VQGAN model.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqgan` (`PaellaVQModel`) — VQGAN 模型。'
- en: '`scheduler` (`DDPMWuerstchenScheduler`) — A scheduler to be used in combination
    with `prior` to generate image embedding.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`DDPMWuerstchenScheduler`) — 用于与`prior`结合使用以生成图像嵌入的调度器。'
- en: '`latent_dim_scale` (float, `optional`, defaults to 10.67) — Multiplier to determine
    the VQ latent space size from the image embeddings. If the image embeddings are
    height=24 and width=24, the VQ latent shape needs to be height=int(24*10.67)=256
    and width=int(24*10.67)=256 in order to match the training conditions.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_dim_scale` (float, *可选*, 默认为10.67) — 从图像嵌入确定 VQ 潜变量空间大小的乘数。如果图像嵌入的高度=24，宽度=24，则
    VQ 潜变量形状需要为高度=int(24*10.67)=256，宽度=int(24*10.67)=256，以匹配训练条件。'
- en: Pipeline for generating images from the Wuerstchen model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 用于从 Wuerstchen 模型生成图像的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L208)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L208)'
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_embedding` (`torch.FloatTensor` or `List[torch.FloatTensor]`) — Image
    Embeddings either extracted from an image or generated by a Prior Model.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embedding` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 图像嵌入，可以是从图像中提取的，也可以是由先验模型生成的。'
- en: '`prompt` (`str` or `List[str]`) — The prompt or prompts to guide the image
    generation.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 12) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为12) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`timesteps` (`List[int]`, *optional*) — Custom timesteps to use for the denoising
    process. If not defined, equal spaced `num_inference_steps` timesteps are used.
    Must be in descending order.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps` (`List[int]`, *可选*) — 用于去噪过程的自定义时间步。如果未定义，则使用等间隔的`num_inference_steps`时间步。必须按降序排列。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 0.0) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `decoder_guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `decoder_guidance_scale > 1`. Higher guidance
    scale encourages to generate images that are closely linked to the text `prompt`,
    usually at the expense of lower image quality.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为0.0) — 如[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义的指导比例。`decoder_guidance_scale`定义为[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`decoder_guidance_scale
    > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `decoder_guidance_scale` is less than `1`).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于指导图像生成的提示。如果不使用指导（即如果`decoder_guidance_scale`小于`1`），则忽略。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *可选*, 默认为1) — 每个提示生成的图像数量。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch
    生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则将使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择 `"pil"` (`PIL.Image.Image`)、`"np"`
    (`np.array`) 或 `"pt"` (`torch.Tensor`)。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    而不是普通元组。'
- en: '`callback_on_step_end` (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end`（`Callable`，*可选*）—在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs`（`List`，*可选*）—`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在您的管道类的`._callback_tensor_inputs`属性中列出的变量。'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用管道生成时调用的函数。
- en: 'Examples:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Citation
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
