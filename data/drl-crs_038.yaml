- en: Glossary
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语表
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/glossary](https://huggingface.co/learn/deep-rl-course/unit2/glossary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/learn/deep-rl-course/unit2/glossary](https://huggingface.co/learn/deep-rl-course/unit2/glossary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This is a community-created glossary. Contributions are welcomed!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由社区创建的术语表。欢迎贡献！
- en: Strategies to find the optimal policy
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找最优策略的策略
- en: '**Policy-based methods.** The policy is usually trained with a neural network
    to select what action to take given a state. In this case it is the neural network
    which outputs the action that the agent should take instead of using a value function.
    Depending on the experience received by the environment, the neural network will
    be re-adjusted and will provide better actions.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的方法。**通常使用神经网络训练策略，以选择在给定状态下采取什么行动。在这种情况下，神经网络输出代理应该采取的行动，而不是使用值函数。根据环境接收到的经验，神经网络将被重新调整，并提供更好的行动。'
- en: '**Value-based methods.** In this case, a value function is trained to output
    the value of a state or a state-action pair that will represent our policy. However,
    this value doesn’t define what action the agent should take. In contrast, we need
    to specify the behavior of the agent given the output of the value function. For
    example, we could decide to adopt a policy to take the action that always leads
    to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy
    (or whatever decision the user takes) that uses the values of the value-function
    to decide the actions to take.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于价值的方法。**在这种情况下，训练值函数以输出将代表我们策略的状态或状态-动作对的值。然而，这个值并不定义代理应该采取什么行动。相反，我们需要指定值函数的输出给定代理的行为。例如，我们可以决定采用一个策略，始终采取导致最大奖励的行动（贪婪策略）。总之，策略是一个贪婪策略（或用户采取的任何决定），它使用值函数的值来决定采取的行动。'
- en: Among the value-based methods, we can find two main strategies
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，我们可以找到两种主要策略
- en: '**The state-value function.** For each state, the state-value function is the
    expected return if the agent starts in that state and follows the policy until
    the end.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态值函数。**对于每个状态，状态值函数是代理在该状态开始并遵循策略直到结束时的期望回报。'
- en: '**The action-value function.** In contrast to the state-value function, the
    action-value calculates for each state and action pair the expected return if
    the agent starts in that state, takes that action, and then follows the policy
    forever after.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作值函数。**与状态值函数相反，动作值函数计算了每个状态和动作对的期望回报，如果代理在该状态开始，采取该动作，然后按照策略永远执行。'
- en: 'Epsilon-greedy strategy:'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Epsilon-贪婪策略：
- en: Common strategy used in reinforcement learning that involves balancing exploration
    and exploitation.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在强化学习中常用的一种策略，涉及平衡探索和开发。
- en: Chooses the action with the highest expected reward with a probability of 1-epsilon.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择具有最高期望奖励的动作，概率为1-epsilon。
- en: Chooses a random action with a probability of epsilon.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以epsilon的概率选择随机动作。
- en: Epsilon is typically decreased over time to shift focus towards exploitation.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着时间的推移，epsilon通常会减少，以将焦点转向开发。
- en: 'Greedy strategy:'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贪婪策略：
- en: Involves always choosing the action that is expected to lead to the highest
    reward, based on the current knowledge of the environment. (Only exploitation)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 始终选择预期导致最高奖励的动作，基于对环境当前知识的了解。（仅开发）
- en: Always chooses the action with the highest expected reward.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 始终选择具有最高期望奖励的动作。
- en: Does not include any exploration.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不包括任何探索。
- en: Can be disadvantageous in environments with uncertainty or unknown optimal actions.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在存在不确定性或未知最优动作的环境中可能具有劣势。
- en: Off-policy vs on-policy algorithms
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离策略与在策略算法
- en: '**Off-policy algorithms:** A different policy is used at training time and
    inference time'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离策略算法：**在训练时间和推断时间使用不同的策略'
- en: '**On-policy algorithms:** The same policy is used during training and inference'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在策略算法：**训练和推断期间使用相同的策略'
- en: Monte Carlo and Temporal Difference learning strategies
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蒙特卡洛和时间差异学习策略
- en: '**Monte Carlo (MC):** Learning at the end of the episode. With Monte Carlo,
    we wait until the episode ends and then we update the value function (or policy
    function) from a complete episode.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒙特卡洛（MC）：**在情节结束时学习。通过蒙特卡洛，我们等到情节结束，然后从完整的情节更新值函数（或策略函数）。'
- en: '**Temporal Difference (TD):** Learning at each step. With Temporal Difference
    Learning, we update the value function (or policy function) at each step without
    requiring a complete episode.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间差异（TD）：**每一步学习。通过时间差异学习，我们在每一步更新值函数（或策略函数），而无需完整的情节。'
- en: If you want to improve the course, you can [open a Pull Request.](https://github.com/huggingface/deep-rl-class/pulls)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想改进课程，可以[发起拉取请求。](https://github.com/huggingface/deep-rl-class/pulls)
- en: 'This glossary was made possible thanks to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语表得以实现，感谢：
- en: '[Ramón Rueda](https://github.com/ramon-rd)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ramón Rueda](https://github.com/ramon-rd)'
- en: '[Hasarindu Perera](https://github.com/hasarinduperera/)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hasarindu Perera](https://github.com/hasarinduperera/)'
- en: '[Arkady Arkhangorodsky](https://github.com/arkadyark/)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Arkady Arkhangorodsky](https://github.com/arkadyark/)'
