- en: Text generation strategies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç”Ÿæˆç­–ç•¥
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/generation_strategies](https://huggingface.co/docs/transformers/v4.37.2/en/generation_strategies)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/generation_strategies](https://huggingface.co/docs/transformers/v4.37.2/en/generation_strategies)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Text generation is essential to many NLP tasks, such as open-ended text generation,
    summarization, translation, and more. It also plays a role in a variety of mixed-modality
    applications that have text as an output like speech-to-text and vision-to-text.
    Some of the models that can generate text include GPT2, XLNet, OpenAI GPT, CTRL,
    TransformerXL, XLM, Bart, T5, GIT, Whisper.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç”Ÿæˆå¯¹äºè®¸å¤šNLPä»»åŠ¡è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€ç¿»è¯‘ç­‰ã€‚å®ƒè¿˜åœ¨å„ç§æ··åˆæ¨¡æ€åº”ç”¨ä¸­å‘æŒ¥ä½œç”¨ï¼Œè¿™äº›åº”ç”¨çš„è¾“å‡ºæ˜¯æ–‡æœ¬ï¼Œå¦‚è¯­éŸ³è½¬æ–‡æœ¬å’Œè§†è§‰è½¬æ–‡æœ¬ã€‚ä¸€äº›å¯ä»¥ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹åŒ…æ‹¬GPT2ã€XLNetã€OpenAI
    GPTã€CTRLã€TransformerXLã€XLMã€Bartã€T5ã€GITã€Whisperã€‚
- en: 'Check out a few examples that use [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method to produce text outputs for different tasks:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ä¸€äº›ä½¿ç”¨[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•ä¸ºä¸åŒä»»åŠ¡ç”Ÿæˆæ–‡æœ¬è¾“å‡ºçš„ç¤ºä¾‹ï¼š
- en: '[Text summarization](./tasks/summarization#inference)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬æ‘˜è¦](./tasks/summarization#inference)'
- en: '[Image captioning](./model_doc/git#transformers.GitForCausalLM.forward.example)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åƒæ ‡é¢˜](./model_doc/git#transformers.GitForCausalLM.forward.example)'
- en: '[Audio transcription](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[éŸ³é¢‘è½¬å½•](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)'
- en: Note that the inputs to the generate method depend on the modelâ€™s modality.
    They are returned by the modelâ€™s preprocessor class, such as AutoTokenizer or
    AutoProcessor. If a modelâ€™s preprocessor creates more than one kind of input,
    pass all the inputs to generate(). You can learn more about the individual modelâ€™s
    preprocessor in the corresponding modelâ€™s documentation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œç”Ÿæˆæ–¹æ³•çš„è¾“å…¥å–å†³äºæ¨¡å‹çš„æ¨¡æ€ã€‚å®ƒä»¬ç”±æ¨¡å‹çš„é¢„å¤„ç†å™¨ç±»è¿”å›ï¼Œä¾‹å¦‚AutoTokenizeræˆ–AutoProcessorã€‚å¦‚æœæ¨¡å‹çš„é¢„å¤„ç†å™¨åˆ›å»ºå¤šç§ç±»å‹çš„è¾“å…¥ï¼Œè¯·å°†æ‰€æœ‰è¾“å…¥ä¼ é€’ç»™generate()ã€‚æ‚¨å¯ä»¥åœ¨ç›¸åº”æ¨¡å‹çš„æ–‡æ¡£ä¸­äº†è§£æ›´å¤šå…³äºå„ä¸ªæ¨¡å‹çš„é¢„å¤„ç†å™¨çš„ä¿¡æ¯ã€‚
- en: The process of selecting output tokens to generate text is known as decoding,
    and you can customize the decoding strategy that the `generate()` method will
    use. Modifying a decoding strategy does not change the values of any trainable
    parameters. However, it can have a noticeable impact on the quality of the generated
    output. It can help reduce repetition in the text and make it more coherent.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©ç”Ÿæˆæ–‡æœ¬çš„è¾“å‡ºæ ‡è®°çš„è¿‡ç¨‹ç§°ä¸ºè§£ç ï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰`generate()`æ–¹æ³•å°†ä½¿ç”¨çš„è§£ç ç­–ç•¥ã€‚ä¿®æ”¹è§£ç ç­–ç•¥ä¸ä¼šæ”¹å˜ä»»ä½•å¯è®­ç»ƒå‚æ•°çš„å€¼ã€‚ä½†æ˜¯ï¼Œå®ƒå¯èƒ½ä¼šæ˜¾è‘—å½±å“ç”Ÿæˆè¾“å‡ºçš„è´¨é‡ã€‚å®ƒå¯ä»¥å¸®åŠ©å‡å°‘æ–‡æœ¬ä¸­çš„é‡å¤ï¼Œå¹¶ä½¿å…¶æ›´è¿è´¯ã€‚
- en: 'This guide describes:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—æè¿°ï¼š
- en: default generation configuration
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é»˜è®¤ç”Ÿæˆé…ç½®
- en: common decoding strategies and their main parameters
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¸¸è§çš„è§£ç ç­–ç•¥åŠå…¶ä¸»è¦å‚æ•°
- en: saving and sharing custom generation configurations with your fine-tuned model
    on ğŸ¤— Hub
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Hubä¸Šä¿å­˜å’Œå…±äº«è‡ªå®šä¹‰ç”Ÿæˆé…ç½®ä¸æ‚¨çš„å¾®è°ƒæ¨¡å‹
- en: Default text generation configuration
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é»˜è®¤æ–‡æœ¬ç”Ÿæˆé…ç½®
- en: A decoding strategy for a model is defined in its generation configuration.
    When using pre-trained models for inference within a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline),
    the models call the `PreTrainedModel.generate()` method that applies a default
    generation configuration under the hood. The default configuration is also used
    when no custom configuration has been saved with the model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„è§£ç ç­–ç•¥åœ¨å…¶ç”Ÿæˆé…ç½®ä¸­å®šä¹‰ã€‚åœ¨ç®¡é“å†…ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨æ–­æ—¶ï¼Œæ¨¡å‹è°ƒç”¨`PreTrainedModel.generate()`æ–¹æ³•ï¼Œåœ¨å¹•ååº”ç”¨é»˜è®¤ç”Ÿæˆé…ç½®ã€‚å½“æ²¡æœ‰ä¿å­˜è‡ªå®šä¹‰é…ç½®ä¸æ¨¡å‹ä¸€èµ·æ—¶ï¼Œä¹Ÿä¼šä½¿ç”¨é»˜è®¤é…ç½®ã€‚
- en: 'When you load a model explicitly, you can inspect the generation configuration
    that comes with it through `model.generation_config`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨æ˜¾å¼åŠ è½½æ¨¡å‹æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡`model.generation_config`æ£€æŸ¥éšä¹‹æä¾›çš„ç”Ÿæˆé…ç½®ï¼š
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Printing out the `model.generation_config` reveals only the values that are
    different from the default generation configuration, and does not list any of
    the default values.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰“å°å‡º`model.generation_config`åªæ˜¾ç¤ºä¸é»˜è®¤ç”Ÿæˆé…ç½®ä¸åŒçš„å€¼ï¼Œå¹¶ä¸åˆ—å‡ºä»»ä½•é»˜è®¤å€¼ã€‚
- en: The default generation configuration limits the size of the output combined
    with the input prompt to a maximum of 20 tokens to avoid running into resource
    limitations. The default decoding strategy is greedy search, which is the simplest
    decoding strategy that picks a token with the highest probability as the next
    token. For many tasks and small output sizes this works well. However, when used
    to generate longer outputs, greedy search can start producing highly repetitive
    results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤ç”Ÿæˆé…ç½®é™åˆ¶è¾“å‡ºä¸è¾“å…¥æç¤ºçš„ç»„åˆå¤§å°æœ€å¤šä¸º20ä¸ªæ ‡è®°ï¼Œä»¥é¿å…é‡åˆ°èµ„æºé™åˆ¶ã€‚é»˜è®¤è§£ç ç­–ç•¥æ˜¯è´ªå©ªæœç´¢ï¼Œè¿™æ˜¯ä¸€ç§æœ€ç®€å•çš„è§£ç ç­–ç•¥ï¼Œå®ƒé€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°ä½œä¸ºä¸‹ä¸€ä¸ªæ ‡è®°ã€‚å¯¹äºè®¸å¤šä»»åŠ¡å’Œå°è¾“å‡ºå¤§å°ï¼Œè¿™ç§æ–¹æ³•æ•ˆæœå¾ˆå¥½ã€‚ç„¶è€Œï¼Œå½“ç”¨äºç”Ÿæˆè¾ƒé•¿çš„è¾“å‡ºæ—¶ï¼Œè´ªå©ªæœç´¢å¯èƒ½ä¼šå¼€å§‹äº§ç”Ÿé«˜åº¦é‡å¤çš„ç»“æœã€‚
- en: Customize text generation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰æ–‡æœ¬ç”Ÿæˆ
- en: 'You can override any `generation_config` by passing the parameters and their
    values directly to the `generate` method:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡ç›´æ¥å°†å‚æ•°åŠå…¶å€¼ä¼ é€’ç»™`generate`æ–¹æ³•æ¥è¦†ç›–ä»»ä½•`generation_config`ï¼š
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Even if the default decoding strategy mostly works for your task, you can still
    tweak a few things. Some of the commonly adjusted parameters include:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿é»˜è®¤è§£ç ç­–ç•¥å¯¹æ‚¨çš„ä»»åŠ¡å¤§éƒ¨åˆ†æœ‰æ•ˆï¼Œæ‚¨ä»ç„¶å¯ä»¥å¾®è°ƒä¸€äº›å†…å®¹ã€‚ä¸€äº›å¸¸è°ƒæ•´çš„å‚æ•°åŒ…æ‹¬ï¼š
- en: '`max_new_tokens`: the maximum number of tokens to generate. In other words,
    the size of the output sequence, not including the tokens in the prompt. As an
    alternative to using the outputâ€™s length as a stopping criteria, you can choose
    to stop generation whenever the full generation exceeds some amount of time. To
    learn more, check [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens`ï¼šè¦ç”Ÿæˆçš„æ ‡è®°çš„æœ€å¤§æ•°é‡ã€‚æ¢å¥è¯è¯´ï¼Œè¾“å‡ºåºåˆ—çš„å¤§å°ï¼Œä¸åŒ…æ‹¬æç¤ºä¸­çš„æ ‡è®°ã€‚ä½œä¸ºä½¿ç”¨è¾“å‡ºé•¿åº¦ä½œä¸ºåœæ­¢æ ‡å‡†çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ‚¨å¯ä»¥é€‰æ‹©åœ¨å®Œæ•´ç”Ÿæˆè¶…è¿‡æŸä¸ªæ—¶é—´é‡æ—¶åœæ­¢ç”Ÿæˆã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)ã€‚'
- en: '`num_beams`: by specifying a number of beams higher than 1, you are effectively
    switching from greedy search to beam search. This strategy evaluates several hypotheses
    at each time step and eventually chooses the hypothesis that has the overall highest
    probability for the entire sequence. This has the advantage of identifying high-probability
    sequences that start with a lower probability initial tokens and wouldâ€™ve been
    ignored by the greedy search.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams`ï¼šé€šè¿‡æŒ‡å®šé«˜äº1çš„æ³¢æŸæ•°é‡ï¼Œæ‚¨å®é™…ä¸Šæ˜¯ä»è´ªå©ªæœç´¢åˆ‡æ¢åˆ°æ³¢æŸæœç´¢ã€‚è¿™ç§ç­–ç•¥åœ¨æ¯ä¸ªæ—¶é—´æ­¥è¯„ä¼°å‡ ä¸ªå‡è®¾ï¼Œæœ€ç»ˆé€‰æ‹©å…·æœ‰æ•´ä¸ªåºåˆ—çš„æœ€é«˜æ¦‚ç‡çš„å‡è®¾ã€‚è¿™æœ‰ä¸€ä¸ªä¼˜ç‚¹ï¼Œå¯ä»¥è¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹æ ‡è®°å¼€å¤´çš„é«˜æ¦‚ç‡åºåˆ—ï¼Œå¹¶ä¸”ä¼šè¢«è´ªå©ªæœç´¢å¿½ç•¥ã€‚'
- en: '`do_sample`: if set to `True`, this parameter enables decoding strategies such
    as multinomial sampling, beam-search multinomial sampling, Top-K sampling and
    Top-p sampling. All these strategies select the next token from the probability
    distribution over the entire vocabulary with various strategy-specific adjustments.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_sample`ï¼šå¦‚æœè®¾ç½®ä¸º`True`ï¼Œæ­¤å‚æ•°å°†å¯ç”¨è§£ç ç­–ç•¥ï¼Œå¦‚å¤šé¡¹å¼é‡‡æ ·ã€æ³¢æŸæœç´¢å¤šé¡¹å¼é‡‡æ ·ã€Top-Ké‡‡æ ·å’ŒTop-pé‡‡æ ·ã€‚æ‰€æœ‰è¿™äº›ç­–ç•¥ä»æ•´ä¸ªè¯æ±‡è¡¨çš„æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå…·æœ‰å„ç§ç‰¹å®šç­–ç•¥çš„è°ƒæ•´ã€‚'
- en: '`num_return_sequences`: the number of sequence candidates to return for each
    input. This option is only available for the decoding strategies that support
    multiple sequence candidates, e.g. variations of beam search and sampling. Decoding
    strategies like greedy search and contrastive search return a single output sequence.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_return_sequences`ï¼šè¦ä¸ºæ¯ä¸ªè¾“å…¥è¿”å›çš„åºåˆ—å€™é€‰æ•°ã€‚æ­¤é€‰é¡¹ä»…é€‚ç”¨äºæ”¯æŒå¤šä¸ªåºåˆ—å€™é€‰çš„è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚æ³¢æŸæœç´¢å’Œé‡‡æ ·çš„å˜ä½“ã€‚è´ªå©ªæœç´¢å’Œå¯¹æ¯”æœç´¢ç­‰è§£ç ç­–ç•¥è¿”å›å•ä¸ªè¾“å‡ºåºåˆ—ã€‚'
- en: Save a custom decoding strategy with your model
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿å­˜å¸¦æœ‰æ‚¨çš„æ¨¡å‹çš„è‡ªå®šä¹‰è§£ç ç­–ç•¥
- en: 'If you would like to share your fine-tuned model with a specific generation
    configuration, you can:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦ä¸ç‰¹å®šç”Ÿæˆé…ç½®å…±äº«æ‚¨å¾®è°ƒçš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ï¼š
- en: Create a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    class instance
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ª[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)ç±»å®ä¾‹
- en: Specify the decoding strategy parameters
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡å®šè§£ç ç­–ç•¥å‚æ•°
- en: Save your generation configuration with [GenerationConfig.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained),
    making sure to leave its `config_file_name` argument empty
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[GenerationConfig.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)ä¿å­˜æ‚¨çš„ç”Ÿæˆé…ç½®ï¼Œç¡®ä¿å°†å…¶`config_file_name`å‚æ•°ç•™ç©º
- en: Set `push_to_hub` to `True` to upload your config to the modelâ€™s repo
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†`push_to_hub`è®¾ç½®ä¸º`True`ï¼Œå°†æ‚¨çš„é…ç½®ä¸Šä¼ åˆ°æ¨¡å‹çš„å­˜å‚¨åº“
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can also store several generation configurations in a single directory,
    making use of the `config_file_name` argument in [GenerationConfig.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained).
    You can later instantiate them with [GenerationConfig.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained).
    This is useful if you want to store several generation configurations for a single
    model (e.g. one for creative text generation with sampling, and one for summarization
    with beam search). You must have the right Hub permissions to add configuration
    files to a model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥åœ¨å•ä¸ªç›®å½•ä¸­å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®ï¼Œåˆ©ç”¨[GenerationConfig.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)ä¸­çš„`config_file_name`å‚æ•°ã€‚æ‚¨å¯ä»¥ç¨åä½¿ç”¨[GenerationConfig.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained)å®ä¾‹åŒ–å®ƒä»¬ã€‚å¦‚æœæ‚¨æƒ³ä¸ºå•ä¸ªæ¨¡å‹å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç”¨äºé‡‡æ ·çš„åˆ›æ„æ–‡æœ¬ç”Ÿæˆï¼Œä¸€ä¸ªç”¨äºæ³¢æŸæœç´¢çš„æ‘˜è¦ï¼‰ï¼Œåˆ™å¿…é¡»å…·æœ‰æ­£ç¡®çš„Hubæƒé™ä»¥å‘æ¨¡å‹æ·»åŠ é…ç½®æ–‡ä»¶ã€‚
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Streaming
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµå¼ä¼ è¾“
- en: 'The `generate()` supports streaming, through its `streamer` input. The `streamer`
    input is compatible with any instance from a class that has the following methods:
    `put()` and `end()`. Internally, `put()` is used to push new tokens and `end()`
    is used to flag the end of text generation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate()`æ”¯æŒæµå¼ä¼ è¾“ï¼Œé€šè¿‡å…¶`streamer`è¾“å…¥ã€‚`streamer`è¾“å…¥ä¸å…·æœ‰ä»¥ä¸‹æ–¹æ³•çš„ç±»çš„ä»»ä½•å®ä¾‹å…¼å®¹ï¼š`put()`å’Œ`end()`ã€‚åœ¨å†…éƒ¨ï¼Œ`put()`ç”¨äºæ¨é€æ–°æ ‡è®°ï¼Œ`end()`ç”¨äºæ ‡è®°æ–‡æœ¬ç”Ÿæˆçš„ç»“æŸã€‚'
- en: The API for the streamer classes is still under development and may change in
    the future.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æµåª’ä½“ç±»çš„APIä»åœ¨å¼€å‘ä¸­ï¼Œå¯èƒ½ä¼šåœ¨æœªæ¥å‘ç”Ÿå˜åŒ–ã€‚
- en: 'In practice, you can craft your own streaming class for all sorts of purposes!
    We also have basic streaming classes ready for you to use. For example, you can
    use the [TextStreamer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TextStreamer)
    class to stream the output of `generate()` into your screen, one word at a time:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæ‚¨å¯ä»¥ä¸ºå„ç§ç›®çš„åˆ¶ä½œè‡ªå·±çš„æµå¼ä¼ è¾“ç±»ï¼æˆ‘ä»¬è¿˜ä¸ºæ‚¨å‡†å¤‡äº†åŸºæœ¬çš„æµå¼ä¼ è¾“ç±»ä¾›æ‚¨ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[TextStreamer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TextStreamer)ç±»å°†`generate()`çš„è¾“å‡ºæµå¼ä¼ è¾“åˆ°å±å¹•ä¸Šï¼Œæ¯æ¬¡ä¸€ä¸ªè¯ï¼š
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Decoding strategies
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£ç ç­–ç•¥
- en: Certain combinations of the `generate()` parameters, and ultimately `generation_config`,
    can be used to enable specific decoding strategies. If you are new to this concept,
    we recommend reading [this blog post that illustrates how common decoding strategies
    work](https://huggingface.co/blog/how-to-generate).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æŸäº›`generate()`å‚æ•°çš„ç»„åˆï¼Œæœ€ç»ˆ`generation_config`å¯ä»¥ç”¨äºå¯ç”¨ç‰¹å®šçš„è§£ç ç­–ç•¥ã€‚å¦‚æœæ‚¨å¯¹è¿™ä¸ªæ¦‚å¿µè¿˜ä¸ç†Ÿæ‚‰ï¼Œæˆ‘ä»¬å»ºè®®é˜…è¯»[è¿™ç¯‡åšæ–‡ï¼Œå±•ç¤ºäº†å¸¸è§çš„è§£ç ç­–ç•¥å¦‚ä½•å·¥ä½œ](https://huggingface.co/blog/how-to-generate)ã€‚
- en: Here, weâ€™ll show some of the parameters that control the decoding strategies
    and illustrate how you can use them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å±•ç¤ºæ§åˆ¶è§£ç ç­–ç•¥çš„ä¸€äº›å‚æ•°ï¼Œå¹¶è¯´æ˜å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚
- en: Greedy Search
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è´ªå©ªæœç´¢
- en: '`generate` uses greedy search decoding by default so you donâ€™t have to pass
    any parameters to enable it. This means the parameters `num_beams` is set to 1
    and `do_sample=False`.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate`é»˜è®¤ä½¿ç”¨è´ªå©ªæœç´¢è§£ç ï¼Œå› æ­¤æ‚¨æ— éœ€ä¼ é€’ä»»ä½•å‚æ•°æ¥å¯ç”¨å®ƒã€‚è¿™æ„å‘³ç€å‚æ•°`num_beams`è®¾ç½®ä¸º1ï¼Œ`do_sample=False`ã€‚'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Contrastive search
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¯¹æ¯”æœç´¢
- en: 'The contrastive search decoding strategy was proposed in the 2022 paper [A
    Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417).
    It demonstrates superior results for generating non-repetitive yet coherent long
    outputs. To learn how contrastive search works, check out [this blog post](https://huggingface.co/blog/introducing-csearch).
    The two main parameters that enable and control the behavior of contrastive search
    are `penalty_alpha` and `top_k`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¯”æœç´¢è§£ç ç­–ç•¥æ˜¯åœ¨2022å¹´çš„è®ºæ–‡[A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)ä¸­æå‡ºçš„ã€‚å®ƒå±•ç¤ºäº†ç”Ÿæˆéé‡å¤ä½†è¿è´¯çš„é•¿è¾“å‡ºçš„ä¼˜è¶Šç»“æœã€‚è¦äº†è§£å¯¹æ¯”æœç´¢çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/introducing-csearch)ã€‚å¯ç”¨å’Œæ§åˆ¶å¯¹æ¯”æœç´¢è¡Œä¸ºçš„ä¸¤ä¸ªä¸»è¦å‚æ•°æ˜¯`penalty_alpha`å’Œ`top_k`ï¼š
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Multinomial sampling
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šé¡¹å¼æŠ½æ ·
- en: As opposed to greedy search that always chooses a token with the highest probability
    as the next token, multinomial sampling (also called ancestral sampling) randomly
    selects the next token based on the probability distribution over the entire vocabulary
    given by the model. Every token with a non-zero probability has a chance of being
    selected, thus reducing the risk of repetition.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ€»æ˜¯é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°ä½œä¸ºä¸‹ä¸€ä¸ªæ ‡è®°çš„è´ªå©ªæœç´¢ç›¸åï¼Œå¤šé¡¹å¼æŠ½æ ·ï¼ˆä¹Ÿç§°ä¸ºç¥–å…ˆæŠ½æ ·ï¼‰æ ¹æ®æ¨¡å‹ç»™å‡ºçš„æ•´ä¸ªè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚æ¯ä¸ªå…·æœ‰éé›¶æ¦‚ç‡çš„æ ‡è®°éƒ½æœ‰è¢«é€‰æ‹©çš„æœºä¼šï¼Œä»è€Œé™ä½é‡å¤çš„é£é™©ã€‚
- en: To enable multinomial sampling set `do_sample=True` and `num_beams=1`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯ç”¨å¤šé¡¹å¼æŠ½æ ·ï¼Œè¯·è®¾ç½®`do_sample=True`å’Œ`num_beams=1`ã€‚
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Beam-search decoding
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æŸæœç´¢è§£ç 
- en: Unlike greedy search, beam-search decoding keeps several hypotheses at each
    time step and eventually chooses the hypothesis that has the overall highest probability
    for the entire sequence. This has the advantage of identifying high-probability
    sequences that start with lower probability initial tokens and wouldâ€™ve been ignored
    by the greedy search.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è´ªå©ªæœç´¢ä¸åŒï¼ŒæŸæœç´¢è§£ç åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™å‡ ä¸ªå‡è®¾ï¼Œå¹¶æœ€ç»ˆé€‰æ‹©æ•´ä¸ªåºåˆ—çš„æ€»ä½“æ¦‚ç‡æœ€é«˜çš„å‡è®¾ã€‚è¿™æœ‰åŠ©äºè¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹æ ‡è®°å¼€å¤´çš„é«˜æ¦‚ç‡åºåˆ—ï¼Œè¿™äº›åºåˆ—åœ¨è´ªå©ªæœç´¢ä¸­ä¼šè¢«å¿½ç•¥ã€‚
- en: To enable this decoding strategy, specify the `num_beams` (aka number of hypotheses
    to keep track of) that is greater than 1.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯ç”¨è¿™ç§è§£ç ç­–ç•¥ï¼Œè¯·æŒ‡å®š`num_beams`ï¼ˆå³è¦è·Ÿè¸ªçš„å‡è®¾æ•°é‡ï¼‰å¤§äº1ã€‚
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Beam-search multinomial sampling
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æŸæœç´¢å¤šé¡¹å¼æŠ½æ ·
- en: As the name implies, this decoding strategy combines beam search with multinomial
    sampling. You need to specify the `num_beams` greater than 1, and set `do_sample=True`
    to use this decoding strategy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚å…¶åç§°æ‰€ç¤ºï¼Œè¿™ç§è§£ç ç­–ç•¥å°†æŸæœç´¢ä¸å¤šé¡¹å¼æŠ½æ ·ç»“åˆåœ¨ä¸€èµ·ã€‚æ‚¨éœ€è¦æŒ‡å®š`num_beams`å¤§äº1ï¼Œå¹¶è®¾ç½®`do_sample=True`ä»¥ä½¿ç”¨è¿™ç§è§£ç ç­–ç•¥ã€‚
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Diverse beam search decoding
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šæ ·æŸæœç´¢è§£ç 
- en: 'The diverse beam search decoding strategy is an extension of the beam search
    strategy that allows for generating a more diverse set of beam sequences to choose
    from. To learn how it works, refer to [Diverse Beam Search: Decoding Diverse Solutions
    from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf). This approach
    has three main parameters: `num_beams`, `num_beam_groups`, and `diversity_penalty`.
    The diversity penalty ensures the outputs are distinct across groups, and beam
    search is used within each group.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¤šæ ·æŸæœç´¢è§£ç ç­–ç•¥æ˜¯æŸæœç´¢ç­–ç•¥çš„æ‰©å±•ï¼Œå…è®¸ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æŸåºåˆ—ä¾›é€‰æ‹©ã€‚è¦äº†è§£å…¶å·¥ä½œåŸç†ï¼Œè¯·å‚è€ƒ[Diverse Beam Search: Decoding
    Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf)ã€‚è¿™ç§æ–¹æ³•æœ‰ä¸‰ä¸ªä¸»è¦å‚æ•°ï¼š`num_beams`ã€`num_beam_groups`å’Œ`diversity_penalty`ã€‚å¤šæ ·æ€§æƒ©ç½šç¡®ä¿è¾“å‡ºåœ¨ç»„é—´æ˜¯ä¸åŒçš„ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªç»„å†…ä½¿ç”¨æŸæœç´¢ã€‚'
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This guide illustrates the main parameters that enable various decoding strategies.
    More advanced parameters exist for the `generate` method, which gives you even
    further control over the `generate` methodâ€™s behavior. For the complete list of
    the available parameters, refer to the [API documentation](./main_classes/text_generation.md).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—è¯´æ˜äº†å¯ç”¨å„ç§è§£ç ç­–ç•¥çš„ä¸»è¦å‚æ•°ã€‚`generate`æ–¹æ³•è¿˜æœ‰æ›´é«˜çº§çš„å‚æ•°ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ§åˆ¶`generate`æ–¹æ³•çš„è¡Œä¸ºã€‚æœ‰å…³å¯ç”¨å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚è€ƒ[APIæ–‡æ¡£](./main_classes/text_generation.md)ã€‚
- en: Speculative Decoding
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨æµ‹è§£ç 
- en: Speculative decoding (also known as assisted decoding) is a modification of
    the decoding strategies above, that uses an assistant model (ideally a much smaller
    one) with the same tokenizer, to generate a few candidate tokens. The main model
    then validates the candidate tokens in a single forward pass, which speeds up
    the decoding process. If `do_sample=True`, then the token validation with resampling
    introduced in the [speculative decoding paper](https://arxiv.org/pdf/2211.17192.pdf)
    is used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨æµ‹è§£ç ï¼ˆä¹Ÿç§°ä¸ºè¾…åŠ©è§£ç ï¼‰æ˜¯ä¸Šè¿°è§£ç ç­–ç•¥çš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªåŠ©ç†æ¨¡å‹ï¼ˆç†æƒ³æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªæ›´å°çš„æ¨¡å‹ï¼‰ä¸ç›¸åŒçš„åˆ†è¯å™¨ï¼Œç”Ÿæˆä¸€äº›å€™é€‰æ ‡è®°ã€‚ç„¶åä¸»æ¨¡å‹åœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­éªŒè¯å€™é€‰æ ‡è®°ï¼Œä»è€ŒåŠ å¿«è§£ç è¿‡ç¨‹ã€‚å¦‚æœ`do_sample=True`ï¼Œåˆ™ä½¿ç”¨[æ¨æµ‹è§£ç è®ºæ–‡](https://arxiv.org/pdf/2211.17192.pdf)ä¸­å¼•å…¥çš„é‡æ–°æŠ½æ ·è¿›è¡Œæ ‡è®°éªŒè¯ã€‚
- en: Currently, only greedy search and sampling are supported with assisted decoding,
    and assisted decoding doesnâ€™t support batched inputs. To learn more about assisted
    decoding, check [this blog post](https://huggingface.co/blog/assisted-generation).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œåªæ”¯æŒè´ªå©ªæœç´¢å’ŒæŠ½æ ·ä¸è¾…åŠ©è§£ç ï¼Œå¹¶ä¸”è¾…åŠ©è§£ç ä¸æ”¯æŒæ‰¹é‡è¾“å…¥ã€‚è¦äº†è§£æ›´å¤šå…³äºè¾…åŠ©è§£ç çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/assisted-generation)ã€‚
- en: To enable assisted decoding, set the `assistant_model` argument with a model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯ç”¨è¾…åŠ©è§£ç ï¼Œè¯·ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹è®¾ç½®`assistant_model`å‚æ•°ã€‚
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When using assisted decoding with sampling methods, you can use the `temperature`
    argument to control the randomness, just like in multinomial sampling. However,
    in assisted decoding, reducing the temperature may help improve the latency.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨è¾…åŠ©è§£ç ä¸æŠ½æ ·æ–¹æ³•æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`temperature`å‚æ•°æ¥æ§åˆ¶éšæœºæ€§ï¼Œå°±åƒåœ¨å¤šé¡¹å¼æŠ½æ ·ä¸­ä¸€æ ·ã€‚ç„¶è€Œï¼Œåœ¨è¾…åŠ©è§£ç ä¸­ï¼Œé™ä½æ¸©åº¦å¯èƒ½æœ‰åŠ©äºæé«˜å»¶è¿Ÿã€‚
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
