- en: Convolutional Vision Transformer (CvT)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·ç§¯è§†è§‰Transformerï¼ˆCvTï¼‰
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808)
    by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei
    Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer
    (ViT)](vit) in performance and efficiency by introducing convolutions into ViT
    to yield the best of both designs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'CvTæ¨¡å‹ç”±å´æµ·å¹³ã€è‚–æ–Œã€è¯ºå°”Â·ç§‘ä»£æ‹‰ã€åˆ˜æ¢¦æ™¨ã€æˆ´å¸Œæ‰¬ã€è¢ç’å’Œå¼ ç£Šåœ¨[CvT: Introducing Convolutions to Vision
    Transformers](https://arxiv.org/abs/2103.15808)ä¸­æå‡ºã€‚å·ç§¯è§†è§‰Transformerï¼ˆCvTï¼‰é€šè¿‡å°†å·ç§¯å¼•å…¥ViTä¸­ï¼Œæé«˜äº†[è§†è§‰Transformerï¼ˆViTï¼‰](vit)çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä»¥è·å¾—è¿™ä¸¤ç§è®¾è®¡çš„æœ€ä½³æ•ˆæœã€‚'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*We present in this paper a new architecture, named Convolutional vision Transformer
    (CvT), that improves Vision Transformer (ViT) in performance and efficiency by
    introducing convolutions into ViT to yield the best of both designs. This is accomplished
    through two primary modifications: a hierarchy of Transformers containing a new
    convolutional token embedding, and a convolutional Transformer block leveraging
    a convolutional projection. These changes introduce desirable properties of convolutional
    neural networks (CNNs) to the ViT architecture (\ie shift, scale, and distortion
    invariance) while maintaining the merits of Transformers (\ie dynamic attention,
    global context, and better generalization). We validate CvT by conducting extensive
    experiments, showing that this approach achieves state-of-the-art performance
    over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters
    and lower FLOPs. In addition, performance gains are maintained when pretrained
    on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained
    on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k
    val set. Finally, our results show that the positional encoding, a crucial component
    in existing Vision Transformers, can be safely removed in our model, simplifying
    the design for higher resolution vision tasks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§åä¸ºå·ç§¯è§†è§‰Transformerï¼ˆCvTï¼‰çš„æ–°æ¶æ„ï¼Œé€šè¿‡å°†å·ç§¯å¼•å…¥ViTä¸­ï¼Œæé«˜äº†ViTçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä»¥è·å¾—è¿™ä¸¤ç§è®¾è®¡çš„æœ€ä½³æ•ˆæœã€‚è¿™é€šè¿‡ä¸¤ä¸ªä¸»è¦ä¿®æ”¹å®ç°ï¼šåŒ…å«æ–°çš„å·ç§¯æ ‡è®°åµŒå…¥çš„Transformerå±‚æ¬¡ç»“æ„ï¼Œä»¥åŠåˆ©ç”¨å·ç§¯æŠ•å½±çš„å·ç§¯Transformerå—ã€‚è¿™äº›æ”¹å˜å°†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ç†æƒ³ç‰¹æ€§å¼•å…¥ViTæ¶æ„ï¼ˆå³å¹³ç§»ã€ç¼©æ”¾å’Œå¤±çœŸä¸å˜æ€§ï¼‰ï¼ŒåŒæ—¶ä¿æŒTransformerçš„ä¼˜ç‚¹ï¼ˆå³åŠ¨æ€æ³¨æ„åŠ›ã€å…¨å±€ä¸Šä¸‹æ–‡å’Œæ›´å¥½çš„æ³›åŒ–ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡è¿›è¡Œå¹¿æ³›å®éªŒéªŒè¯äº†CvTï¼Œæ˜¾ç¤ºè¿™ç§æ–¹æ³•åœ¨ImageNet-1kä¸Šå®ç°äº†å…¶ä»–è§†è§‰Transformerå’ŒResNetçš„æœ€æ–°æ€§èƒ½ï¼Œå‚æ•°æ›´å°‘ï¼ŒFLOPsæ›´ä½ã€‚æ­¤å¤–ï¼Œå½“åœ¨æ›´å¤§çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ImageNet-22kï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¹¶å¾®è°ƒåˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½å¢ç›Šå¾—ä»¥ä¿æŒã€‚åœ¨ImageNet-22kä¸Šé¢„è®­ç»ƒï¼Œæˆ‘ä»¬çš„CvT-W24åœ¨ImageNet-1kéªŒè¯é›†ä¸Šè·å¾—äº†87.7\%çš„top-1å‡†ç¡®ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½ç½®ç¼–ç ï¼Œç°æœ‰è§†è§‰Transformerä¸­çš„å…³é”®ç»„ä»¶ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­å®‰å…¨åœ°ç§»é™¤ï¼Œç®€åŒ–äº†æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ä»»åŠ¡çš„è®¾è®¡ã€‚*'
- en: This model was contributed by [anugunj](https://huggingface.co/anugunj). The
    original code can be found [here](https://github.com/microsoft/CvT).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[anugunj](https://huggingface.co/anugunj)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/microsoft/CvT)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: CvT models are regular Vision Transformers, but trained with convolutions. They
    outperform the [original model (ViT)](vit) when fine-tuned on ImageNet-1K and
    CIFAR-100.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CvTæ¨¡å‹æ˜¯å¸¸è§„çš„è§†è§‰Transformerï¼Œä½†æ˜¯ç»è¿‡å·ç§¯è®­ç»ƒã€‚å½“åœ¨ImageNet-1Kå’ŒCIFAR-100ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼˜äº[åŸå§‹æ¨¡å‹ï¼ˆViTï¼‰](vit)ã€‚
- en: You can check out demo notebooks regarding inference as well as fine-tuning
    on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)
    (you can just replace [ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)
    by [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    and [ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    by [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æŸ¥çœ‹å…³äºæ¨ç†ä»¥åŠåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„æ¼”ç¤ºç¬”è®°æœ¬[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)ï¼ˆæ‚¨åªéœ€å°†[ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)æ›¿æ¢ä¸º[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)ï¼Œå°†[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)æ›¿æ¢ä¸º[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)ï¼‰ã€‚
- en: The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/)
    (a collection of 14 million images and 22k classes) only, (2) also fine-tuned
    on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ç”¨çš„æ£€æŸ¥ç‚¹è¦ä¹ˆï¼ˆ1ï¼‰ä»…åœ¨[ImageNet-22k](http://www.image-net.org/)ï¼ˆåŒ…å«1400ä¸‡å›¾åƒå’Œ22kç±»åˆ«ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¦ä¹ˆï¼ˆ2ï¼‰åœ¨ImageNet-22kä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¦ä¹ˆï¼ˆ3ï¼‰åœ¨[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)ï¼ˆä¹Ÿç§°ä¸ºILSVRC
    2012ï¼ŒåŒ…å«130ä¸‡å›¾åƒå’Œ1000ç±»åˆ«ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒã€‚
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with CvT.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨CvTã€‚
- en: Image Classification
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: '[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: CvtConfig
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CvtConfig
- en: '### `class transformers.CvtConfig`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CvtConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)'
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“æ•°'
- en: '`patch_sizes` (`List[int]`, *optional*, defaults to `[7, 3, 3]`) â€” The kernel
    size of each encoderâ€™s patch embedding.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_sizes` (`List[int]`, *optional*, defaults to `[7, 3, 3]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„è¡¥ä¸åµŒå…¥çš„å†…æ ¸å¤§å°'
- en: '`patch_stride` (`List[int]`, *optional*, defaults to `[4, 2, 2]`) â€” The stride
    size of each encoderâ€™s patch embedding.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_stride` (`List[int]`, *optional*, defaults to `[4, 2, 2]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„è¡¥ä¸åµŒå…¥çš„æ­¥å¹…å¤§å°'
- en: '`patch_padding` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) â€” The padding
    size of each encoderâ€™s patch embedding.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_padding` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„è¡¥ä¸åµŒå…¥çš„å¡«å……å¤§å°'
- en: '`embed_dim` (`List[int]`, *optional*, defaults to `[64, 192, 384]`) â€” Dimension
    of each of the encoder blocks.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_dim` (`List[int]`, *optional*, defaults to `[64, 192, 384]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„ç»´åº¦'
- en: '`num_heads` (`List[int]`, *optional*, defaults to `[1, 3, 6]`) â€” Number of
    attention heads for each attention layer in each block of the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`List[int]`, *optional*, defaults to `[1, 3, 6]`) â€” æ¯ä¸ªTransformerç¼–ç å™¨å—ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`depth` (`List[int]`, *optional*, defaults to `[1, 2, 10]`) â€” The number of
    layers in each encoder block.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth` (`List[int]`, *optional*, defaults to `[1, 2, 10]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¸­çš„å±‚æ•°'
- en: '`mlp_ratios` (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`)
    â€” Ratio of the size of the hidden layer compared to the size of the input layer
    of the Mix FFNs in the encoder blocks.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_ratios` (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`)
    â€” åœ¨ç¼–ç å™¨å—ä¸­Mix FFNçš„éšè—å±‚å¤§å°ä¸è¾“å…¥å±‚å¤§å°çš„æ¯”ç‡'
- en: '`attention_drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`)
    â€” The dropout ratio for the attention probabilities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`)
    â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡'
- en: '`drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) â€” The
    dropout ratio for the patch embeddings probabilities.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) â€” è¡¥ä¸åµŒå…¥æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡'
- en: '`drop_path_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`)
    â€” The dropout probability for stochastic depth, used in the blocks of the Transformer
    encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`)
    â€” éšæœºæ·±åº¦çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œç”¨äºTransformerç¼–ç å™¨å—ä¸­'
- en: '`qkv_bias` (`List[bool]`, *optional*, defaults to `[True, True, True]`) â€” The
    bias bool for query, key and value in attentions'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`List[bool]`, *optional*, defaults to `[True, True, True]`) â€” æŸ¥è¯¢ã€é”®å’Œå€¼çš„æ³¨æ„åŠ›ä¸­çš„åç½®å¸ƒå°”å€¼'
- en: '`cls_token` (`List[bool]`, *optional*, defaults to `[False, False, True]`)
    â€” Whether or not to add a classification token to the output of each of the last
    3 stages.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`List[bool]`, *optional*, defaults to `[False, False, True]`)
    â€” æ˜¯å¦å‘æ¯ä¸ªæœ€å3ä¸ªé˜¶æ®µçš„è¾“å‡ºæ·»åŠ åˆ†ç±»ä»¤ç‰Œ'
- en: '`qkv_projection_method` (`List[string]`, *optional*, defaults to [â€œdw_bnâ€,
    â€œdw_bnâ€, â€œdw_bnâ€]`) â€” The projection method for query, key and value Default is
    depth-wise convolutions with batch norm. For Linear projection use â€œavgâ€.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_projection_method` (`List[string]`, *optional*, defaults to [â€œdw_bnâ€,
    â€œdw_bnâ€, â€œdw_bnâ€]`) â€” æŸ¥è¯¢ã€é”®å’Œå€¼çš„æŠ•å½±æ–¹æ³•ï¼Œé»˜è®¤ä¸ºæ·±åº¦å·ç§¯å’Œæ‰¹é‡å½’ä¸€åŒ–ã€‚ä½¿ç”¨â€œavgâ€è¿›è¡Œçº¿æ€§æŠ•å½±ã€‚'
- en: '`kernel_qkv` (`List[int]`, *optional*, defaults to `[3, 3, 3]`) â€” The kernel
    size for query, key and value in attention layer'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_qkv` (`List[int]`, *optional*, defaults to `[3, 3, 3]`) â€” æ³¨æ„åŠ›å±‚ä¸­æŸ¥è¯¢ã€é”®å’Œå€¼çš„å†…æ ¸å¤§å°'
- en: '`padding_kv` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” The padding
    size for key and value in attention layer'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_kv` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” æ³¨æ„åŠ›å±‚ä¸­é”®å’Œå€¼çš„å¡«å……å¤§å°'
- en: '`stride_kv` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) â€” The stride
    size for key and value in attention layer'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride_kv` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) â€” æ³¨æ„åŠ›å±‚ä¸­é”®å’Œå€¼çš„æ­¥å¹…å¤§å°'
- en: '`padding_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” The padding
    size for query in attention layer'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” æ³¨æ„åŠ›å±‚ä¸­æŸ¥è¯¢çš„å¡«å……å¤§å°'
- en: '`stride_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” The stride
    size for query in attention layer'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” æŸ¥è¯¢åœ¨æ³¨æ„åŠ›å±‚ä¸­çš„æ­¥å¹…å¤§å°'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-6) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-6) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilon'
- en: This is the configuration class to store the configuration of a [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel).
    It is used to instantiate a CvT model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the CvT [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)
    architecture.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–CvTæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºCvT
    [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PytorchHide Pytorch content
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: CvtModel
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CvtModel
- en: '### `class transformers.CvtModel`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CvtModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)'
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰-
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸Cvtæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)'
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰-
    åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`CvtImageProcessor.__call__`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or `tuple(torch.FloatTensor)`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`æˆ–`tuple(torch.FloatTensor)`'
- en: A `transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰-
    æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`cls_token_value` (`torch.FloatTensor` of shape `(batch_size, 1, hidden_size)`)
    â€” Classification token at the output of the last layer of the model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_value`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„`torch.FloatTensor`ï¼‰- æ¨¡å‹æœ€åä¸€å±‚çš„åˆ†ç±»ä»¤ç‰Œã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€
    + åˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: The [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)
    forward method, overrides the `__call__` special method.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: CvtForImageClassification
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CvtForImageClassification
- en: '### `class transformers.CvtForImageClassification`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CvtForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)'
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´éƒ¨çš„ Cvt æ¨¡å‹å˜å‹å™¨ï¼ˆåœ¨ [CLS] æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº ImageNetã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `å‰è¿›`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[< æºä»£ç  >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)'
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height,
    width)`ï¼‰ â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/zh/model_doc/auto#transformers.AutoImageProcessor)
    è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… `CvtImageProcessor.__call__`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨
    `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ
    `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ç»“æœ
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ config.num_labels==1
    åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.num_labels)`ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ
    config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’äº† `output_hidden_states=True`
    æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_channels, height,
    width)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: The [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[CvtForImageClassification](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtForImageClassification)
    å‰è¿›æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow éšè— TensorFlow å†…å®¹
- en: TFCvtModel
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFCvtModel
- en: '### `class transformers.TFCvtModel`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '### `ç±» transformers.TFCvtModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)'
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸ Cvt æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: 'TF 2.0 models accepts two formats as inputs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: TF 2.0 æ¨¡å‹æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆå¦‚ PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨ `tf.keras.Model.fit` æ–¹æ³•æ—¶ï¼Œç¬¬äºŒä¸ªé€‰é¡¹å¾ˆæœ‰ç”¨ï¼Œè¯¥æ–¹æ³•å½“å‰è¦æ±‚åœ¨æ¨¡å‹è°ƒç”¨å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°ä¸­å…·æœ‰æ‰€æœ‰å¼ é‡ï¼š`model(inputs)`ã€‚
- en: '#### `call`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)'
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) â€” Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆ`np.ndarray`ã€`tf.Tensor`ã€`List[tf.Tensor]`ã€`Dict[str, tf.Tensor]`
    æˆ– `Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€”
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… `CvtImageProcessor.__call__`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯ä»¥åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `Falseâ€œ) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆæŸäº›æ¨¡å—å¦‚ä¸¢å¼ƒæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: Returns
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    `tuple(tf.Tensor)`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` æˆ– `tuple(tf.Tensor)`'
- en: A `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` æˆ–ä¸€ä¸ª
    `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    â€” Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼‰â€”
    æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`cls_token_value` (`tf.Tensor` of shape `(batch_size, 1, hidden_size)`) â€” Classification
    token at the output of the last layer of the model.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_value`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„`tf.Tensor`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„åˆ†ç±»æ ‡è®°ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“
    `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„
    `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: The [TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)
    forward method, overrides the `__call__` special method.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TFCvtForImageClassification
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFCvtForImageClassification
- en: '### `class transformers.TFCvtForImageClassification`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFCvtForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)'
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Cvtæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨[CLS]æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: 'TF 2.0 models accepts two formats as inputs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: TF 2.0æ¨¡å‹æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨`tf.keras.Model.fit`æ–¹æ³•æ—¶ï¼Œç¬¬äºŒä¸ªé€‰é¡¹å¾ˆæœ‰ç”¨ï¼Œè¯¥æ–¹æ³•å½“å‰è¦æ±‚åœ¨æ¨¡å‹è°ƒç”¨å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°ä¸­å…·æœ‰æ‰€æœ‰å¼ é‡ï¼š`model(inputs)`ã€‚
- en: '#### `call`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)'
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) â€” Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆ`np.ndarray`ã€`tf.Tensor`ã€`List[tf.Tensor]`ã€`Dict[str, tf.Tensor]`æˆ–`Dict[str,
    np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§`CvtImageProcessor.__call__`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `Falseâ€œ) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆæŸäº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*)
    â€” Labels for computing the image classification/regression loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression
    loss is computed (Mean-Square loss), If `config.num_labels > 1` a classification
    loss is computed (Cross-Entropy).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`æˆ–`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`æˆ–`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` æˆ–è€…ä¸€ä¸ª
    `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ å…¥äº† `return_dict=False` æˆ–è€… `config.return_dict=False`ï¼‰åŒ…å«ä¸åŒçš„å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥ã€‚'
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º `(1,)` çš„ `tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›äº† `labels` æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1`
    åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, config.num_labels)` çš„ `tf.Tensor`ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1`
    åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ å…¥ `output_hidden_states=True` æˆ–è€…
    `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, num_channels, height,
    width)` çš„ `tf.Tensor` å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: The [TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification)
    çš„å‰å‘æ–¹æ³•ï¼Œé‡å†™äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
