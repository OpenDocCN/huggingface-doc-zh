- en: Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/package_reference/modeling](https://huggingface.co/docs/optimum-neuron/package_reference/modeling)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/optimum-neuron/package_reference/modeling](https://huggingface.co/docs/optimum-neuron/package_reference/modeling)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Generic model classes
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用模型类
- en: NeuronBaseModel
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronBaseModel
- en: The `NeuronBaseModel` class is available for instantiating a base Neuron model
    without a specific head. It is used as the base class for all tasks but text generation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '`NeuronBaseModel` 类可用于实例化一个没有特定头部的基础神经元模型。它用作所有任务的基类，但不包括文本生成。'
- en: '### `class optimum.neuron.NeuronBaseModel`'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronBaseModel`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L55)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L55)'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Base class running compiled and optimized models on Neuron devices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经元设备上运行编译和优化模型的基类。
- en: It implements generic methods for interacting with the Hugging Face Hub as well
    as compiling vanilla transformers models to neuron-optimized TorchScript module
    and export it using `optimum.exporters.neuron` toolchain.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它实现了与 Hugging Face Hub 交互的通用方法，以及将原始变压器模型编译为经过神经元优化的 TorchScript 模块并使用 `optimum.exporters.neuron`
    工具链导出它。
- en: 'Class attributes:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 类属性：
- en: model_type (`str`, *optional*, defaults to `"neuron_model"`) — The name of the
    model type to use when registering the NeuronBaseModel classes.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model_type（`str`，*可选*，默认为`"neuron_model"`）- 在注册 `NeuronBaseModel` 类时要使用的模型类型的名称。
- en: auto_model_class (`Type`, *optional*, defaults to `AutoModel`) — The `AutoModel`
    class to be represented by the current NeuronBaseModel class.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: auto_model_class（`Type`，*可选*，默认为`AutoModel`）- 要由当前 `NeuronBaseModel` 类表示的 `AutoModel`
    类。
- en: 'Common attributes:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 常见属性：
- en: model (`torch.jit._script.ScriptModule`) — The loaded `ScriptModule` compiled
    for neuron devices.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model（`torch.jit._script.ScriptModule`）- 为神经元设备编译的加载的 `ScriptModule`。
- en: config ([PretrainedConfig](https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig))
    — The configuration of the model.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置（[PretrainedConfig](https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig)）-
    模型的配置。
- en: model_save_dir (`Path`) — The directory where a neuron compiled model is saved.
    By default, if the loaded model is local, the directory where the original model
    will be used. Otherwise, the cache directory will be used.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model_save_dir（`Path`）- 保存神经元编译模型的目录。默认情况下，如果加载的模型是本地的，则将使用原始模型的目录。否则，将使用缓存目录。
- en: '#### `get_input_static_shapes`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_input_static_shapes`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L451)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L451)'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Gets a dictionary of inputs with their valid static shapes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 获取具有其有效静态形状的输入字典。
- en: '#### `load_model`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_model`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L98)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L98)'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`path` (`Union[str, Path]`) — Path of the compiled model.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`（`Union[str, Path]`）- 编译模型的路径。'
- en: Loads a TorchScript module compiled by neuron(x)-cc compiler. It will be first
    loaded onto CPU and then moved to one or multiple [NeuronCore](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuroncores-arch.html).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 加载由神经元（x）-cc 编译器编译的 TorchScript 模块。它将首先加载到 CPU，然后移动到一个或多个 [NeuronCore](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuroncores-arch.html)。
- en: '#### `remove_padding`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `remove_padding`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L548)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_base.py#L548)'
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` (`List[torch.Tensor]`) — List of torch tensors which are inference
    output.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`（`List[torch.Tensor]`）- 推理输出的 torch 张量列表。'
- en: '`dims` (`List[int]`) — List of dimensions in which we slice a tensor.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dims`（`List[int]`）- 我们在其中切片张量的维度列表。'
- en: '`indices` (`List[int]`) — List of indices in which we slice a tensor along
    an axis.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indices`（`List[int]`）- 我们沿着一个轴切片张量的索引列表。'
- en: '`padding_side` (`Literal["right", "left"]`, defaults to “right”) — The side
    on which the padding has been applied.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side`（`Literal["right", "left"]`，默认为“right”）- 应用填充的一侧。'
- en: Removes padding from output tensors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出张量中删除填充。
- en: NeuronDecoderModel
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronDecoderModel
- en: The `NeuronDecoderModel` class is the base class for text generation models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`NeuronDecoderModel` 类是文本生成模型的基类。'
- en: '### `class optimum.neuron.NeuronDecoderModel`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronDecoderModel`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_decoder.py#L76)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_decoder.py#L76)'
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Base class to convert and run pre-trained transformers decoder models on Neuron
    devices.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将预训练的变压器解码器模型转换并在神经元设备上运行的基类。
- en: 'It implements the methods to convert a pre-trained transformers decoder model
    into a Neuron transformer model by:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它实现了将预训练的变压器解码器模型转换为神经元变压器模型的方法：
- en: transferring the checkpoint weights of the original into an optimized neuron
    graph,
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始模型的检查点权重转移到优化的神经元图中，
- en: compiling the resulting graph using the Neuron compiler.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经元编译器编译生成的图。
- en: 'Common attributes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 常见属性：
- en: model (`torch.nn.Module`) — The decoder model with a graph optimized for neuron
    devices.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model（`torch.nn.Module`）- 具有为神经元设备优化的图形的解码器模型。
- en: config ([PretrainedConfig](https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig))
    — The configuration of the original model.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: config（[PretrainedConfig](https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig)）-
    原始模型的配置。
- en: generation_config ([GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig))
    — The generation configuration used by default when calling `generate()`.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: generation_config（[GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig)）-
    调用 `generate()` 时默认使用的生成配置。
- en: Natural Language Processing
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: The following Neuron model classes are available for natural language processing
    tasks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Neuron模型类适用于自然语言处理任务。
- en: NeuronModelForFeatureExtraction
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于特征提取的NeuronModelForFeatureExtraction
- en: '### `class optimum.neuron.NeuronModelForFeatureExtraction`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForFeatureExtraction`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L122)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L122)'
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    method to load the model weights.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`transformers.PretrainedConfig`）— [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)是具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看`optimum.neuron.modeling.NeuronBaseModel.from_pretrained`方法以加载模型权重。'
- en: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    is the TorchScript module with embedded NEFF(Neuron Executable File Format) compiled
    by neuron(x) compiler.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.jit._script.ScriptModule`）— [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)是由neuron(x)编译器编译的嵌入NEFF（Neuron
    Executable File Format）的TorchScript模块。'
- en: Neuron Model with a BaseModelOutput for feature-extraction tasks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用于特征提取任务的带有BaseModelOutput的Neuron模型。
- en: This model inherits from `~neuron.modeling.NeuronBaseModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自`~neuron.modeling.NeuronBaseModel`。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存）
- en: Feature Extraction model on Neuron devices.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron设备上的特征提取模型。
- en: '#### `forward`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L135)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L135)'
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer).
    See [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    and [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](https://huggingface.co/docs/transformers/glossary#input-ids)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.Tensor`）— 词汇表中输入序列标记的索引。可以使用[`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer)获取索引。有关详细信息，请参阅[`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)和[`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](https://huggingface.co/docs/transformers/glossary#input-ids)'
- en: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Mask to avoid performing attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`Union[torch.Tensor, None]`，默认为`None`）—
    避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于“未被掩盖”的标记，
- en: 0 for tokens that are `masked`. [What are attention masks?](https://huggingface.co/docs/transformers/glossary#attention-mask)
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于“被掩盖”的标记。[什么是注意力掩码？](https://huggingface.co/docs/transformers/glossary#attention-mask)
- en: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Segment token indices to indicate first and second portions
    of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`Union[torch.Tensor, None]`，默认为`None`）—
    分段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 1 for tokens that are `sentence A`,
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于“句子A”的标记，
- en: 0 for tokens that are `sentence B`. [What are token type IDs?](https://huggingface.co/docs/transformers/glossary#token-type-ids)
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于“句子B”的标记。[什么是标记类型ID？](https://huggingface.co/docs/transformers/glossary#token-type-ids)
- en: The [NeuronModelForFeatureExtraction](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForFeatureExtraction)
    forward method, overrides the `__call__` special method.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForFeatureExtraction](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForFeatureExtraction)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of feature extraction: *(Following model is compiled with neuronx compiler
    and can only be run on INF2\. Replace “neuronx” with “neuron” if you are using
    INF1.)*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取示例：*(以下模型使用neuronx编译器编译，只能在INF2上运行。如果您使用INF1，请将“neuronx”替换为“neuron”)*
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: NeuronModelForSentenceTransformers
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronModelForSentenceTransformers
- en: '### `class optimum.neuron.NeuronModelForSentenceTransformers`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForSentenceTransformers`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L195)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L195)'
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    method to load the model weights.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    是具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    方法以加载模型权重。'
- en: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    is the TorchScript module with embedded NEFF(Neuron Executable File Format) compiled
    by neuron(x) compiler.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    是由 neuron(x) 编译器编译的嵌入 NEFF(Neuron Executable File Format) 的 TorchScript 模块。'
- en: Neuron Model for Sentence Transformers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 句子转换模型的神经元模型。
- en: This model inherits from `~neuron.modeling.NeuronBaseModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自`~neuron.modeling.NeuronBaseModel`。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存）
- en: Sentence Transformers model on Neuron devices.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron 设备上的句子转换模型。
- en: '#### `forward`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L208)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L208)'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer).
    See [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    and [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](https://huggingface.co/docs/transformers/glossary#input-ids)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。可以使用
    [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer)
    获取索引。有关详细信息，请参阅 [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    和 [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入
    ID？](https://huggingface.co/docs/transformers/glossary#input-ids)'
- en: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Mask to avoid performing attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在 `[0, 1]` 范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被遮罩的标记为 1，
- en: 0 for tokens that are `masked`. [What are attention masks?](https://huggingface.co/docs/transformers/glossary#attention-mask)
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被遮罩的标记为 0。[什么是注意力掩码？](https://huggingface.co/docs/transformers/glossary#attention-mask)
- en: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Segment token indices to indicate first and second portions
    of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — 段标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 范围内选择：'
- en: 1 for tokens that are `sentence A`,
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`句子 A`的标记为 1，
- en: 0 for tokens that are `sentence B`. [What are token type IDs?](https://huggingface.co/docs/transformers/glossary#token-type-ids)
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`句子 B`的标记为 0。[什么是标记类型 ID？](https://huggingface.co/docs/transformers/glossary#token-type-ids)
- en: The [NeuronModelForSentenceTransformers](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForSentenceTransformers)
    forward method, overrides the `__call__` special method.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForSentenceTransformers](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForSentenceTransformers)
    的前向方法覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example of TEXT Sentence Transformers:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 文本句子转换模型示例：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: NeuronModelForMaskedLM
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronModelForMaskedLM
- en: '### `class optimum.neuron.NeuronModelForMaskedLM`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForMaskedLM`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L266)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L266)'
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    method to load the model weights.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    是具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    方法以加载模型权重。'
- en: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    is the TorchScript module with embedded NEFF(Neuron Executable File Format) compiled
    by neuron(x) compiler.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    是由 neuron(x) 编译器编译的嵌入 NEFF(Neuron Executable File Format) 的 TorchScript 模块。'
- en: Neuron Model with a MaskedLMOutput for masked language modeling tasks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 MaskedLMOutput 用于遮罩语言建模任务的神经元模型。
- en: This model inherits from `~neuron.modeling.NeuronBaseModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自`~neuron.modeling.NeuronBaseModel`。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存）
- en: Masked language model for on Neuron devices.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元设备上的掩码语言模型。
- en: '#### `forward`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L279)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L279)'
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer).
    See [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    and [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](https://huggingface.co/docs/transformers/glossary#input-ids)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.Tensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。可以使用[`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer)获取索引。查看[`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)和[`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)以获取详细信息。[什么是输入ID？](https://huggingface.co/docs/transformers/glossary#input-ids)'
- en: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Mask to avoid performing attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`Union[torch.Tensor, None]`，形状为`(batch_size, sequence_length)`，默认为`None`)
    — 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“未掩码”的标记为1，
- en: 0 for tokens that are `masked`. [What are attention masks?](https://huggingface.co/docs/transformers/glossary#attention-mask)
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩码”的标记为0。[什么是注意力掩码？](https://huggingface.co/docs/transformers/glossary#attention-mask)
- en: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Segment token indices to indicate first and second portions
    of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`Union[torch.Tensor, None]`，形状为`(batch_size, sequence_length)`，默认为`None`)
    — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 1 for tokens that are `sentence A`,
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“句子A”的标记为1，
- en: 0 for tokens that are `sentence B`. [What are token type IDs?](https://huggingface.co/docs/transformers/glossary#token-type-ids)
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“句子B”的标记为0。[什么是标记类型ID？](https://huggingface.co/docs/transformers/glossary#token-type-ids)
- en: The [NeuronModelForMaskedLM](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForMaskedLM](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForMaskedLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example of fill mask: *(Following model is compiled with neuronx compiler and
    can only be run on INF2\. Replace “neuronx” with “neuron” if you are using INF1.)*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 填充掩码的示例：*(以下模型使用neuronx编译器编译，并且只能在INF2上运行。如果您使用INF1，请将“neuronx”替换为“neuron”)*
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: NeuronModelForSequenceClassification
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronModelForSequenceClassification
- en: '### `class optimum.neuron.NeuronModelForSequenceClassification`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L404)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L404)'
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    method to load the model weights.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    是具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看`optimum.neuron.modeling.NeuronBaseModel.from_pretrained`方法以加载模型权重。'
- en: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    is the TorchScript module with embedded NEFF(Neuron Executable File Format) compiled
    by neuron(x) compiler.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    是带有由neuron(x)编译器编译的嵌入式NEFF（Neuron Executable File Format）的TorchScript模块。'
- en: Neuron Model with a sequence classification/regression head on top (a linear
    layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有序列分类/回归头部的神经元模型（在汇总输出的顶部有一个线性层），例如用于GLUE任务。
- en: This model inherits from `~neuron.modeling.NeuronBaseModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自`~neuron.modeling.NeuronBaseModel`。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存）
- en: Sequence Classification model on Neuron devices.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元设备上的序列分类模型。
- en: '#### `forward`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L418)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L418)'
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer).
    See [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    and [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](https://huggingface.co/docs/transformers/glossary#input-ids)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.Tensor`）- 词汇表中输入序列标记的索引。可以使用[`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer)获取索引。有关详细信息，请参阅[`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)和[`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](https://huggingface.co/docs/transformers/glossary#input-ids)'
- en: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Mask to avoid performing attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`Union[torch.Tensor, None]`，默认为`None`）-
    避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`. [What are attention masks?](https://huggingface.co/docs/transformers/glossary#attention-mask)
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。[什么是注意力掩码？](https://huggingface.co/docs/transformers/glossary#attention-mask)
- en: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Segment token indices to indicate first and second portions
    of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`Union[torch.Tensor, None]`，默认为`None`）-
    段标记索引，指示输入的第一部分和第二部分。索引选在`[0, 1]`之间：'
- en: 1 for tokens that are `sentence A`,
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`sentence A`的标记为1。
- en: 0 for tokens that are `sentence B`. [What are token type IDs?](https://huggingface.co/docs/transformers/glossary#token-type-ids)
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`sentence B`的标记为0。[什么是标记类型ID？](https://huggingface.co/docs/transformers/glossary#token-type-ids)
- en: The [NeuronModelForSequenceClassification](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForSequenceClassification](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example of single-label classification: *(Following model is compiled with
    neuronx compiler and can only be run on INF2.)*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签分类示例：*(以下模型使用neuronx编译器编译，并且只能在INF2上运行。)*
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: NeuronModelForQuestionAnswering
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronModelForQuestionAnswering
- en: '### `class optimum.neuron.NeuronModelForQuestionAnswering`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L336)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L336)'
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    method to load the model weights.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`transformers.PretrainedConfig`）- [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)是具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看`optimum.neuron.modeling.NeuronBaseModel.from_pretrained`方法以加载模型权重。'
- en: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    is the TorchScript module with embedded NEFF(Neuron Executable File Format) compiled
    by neuron(x) compiler.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.jit._script.ScriptModule`）- [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)是由neuron(x)编译器编译的嵌入NEFF（Neuron
    Executable File Format）的TorchScript模块。'
- en: Neuron Model with a QuestionAnsweringModelOutput for extractive question-answering
    tasks like SQuAD.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 具有用于提取式问答任务（如SQuAD）的QuestionAnsweringModelOutput的Neuron模型。
- en: This model inherits from `~neuron.modeling.NeuronBaseModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自`~neuron.modeling.NeuronBaseModel`。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存）
- en: Question Answering model on Neuron devices.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron设备上的问答模型。
- en: '#### `forward`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L349)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L349)'
- en: '[PRE18]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer).
    See [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    and [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](https://huggingface.co/docs/transformers/glossary#input-ids)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.Tensor`）- 词汇表中输入序列标记的索引。可以使用[`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer)获取索引。有关详细信息，请参阅[`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)和[`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](https://huggingface.co/docs/transformers/glossary#input-ids)'
- en: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Mask to avoid performing attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`Union[torch.Tensor, None]`，形状为`(batch_size, sequence_length)`，默认为`None`）—
    用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`. [What are attention masks?](https://huggingface.co/docs/transformers/glossary#attention-mask)
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。[什么是注意力掩码？](https://huggingface.co/docs/transformers/glossary#attention-mask)
- en: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Segment token indices to indicate first and second portions
    of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（`Union[torch.Tensor, None]`，形状为`(batch_size, sequence_length)`，默认为`None`）—
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`范围内选择：'
- en: 1 for tokens that are `sentence A`,
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`sentence A`的标记为1，
- en: 0 for tokens that are `sentence B`. [What are token type IDs?](https://huggingface.co/docs/transformers/glossary#token-type-ids)
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`sentence B`的标记为0。[什么是标记类型ID？](https://huggingface.co/docs/transformers/glossary#token-type-ids)
- en: The [NeuronModelForQuestionAnswering](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForQuestionAnswering](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForQuestionAnswering)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of question answering: *(Following model is compiled with neuronx compiler
    and can only be run on INF2.)*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 问答示例：*(以下模型使用neuronx编译器编译，只能在INF2上运行。)*
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: NeuronModelForTokenClassification
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronModelForTokenClassification
- en: '### `class optimum.neuron.NeuronModelForTokenClassification`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L472)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L472)'
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    method to load the model weights.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`transformers.PretrainedConfig`）— [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)是具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看`optimum.neuron.modeling.NeuronBaseModel.from_pretrained`方法以加载模型权重。'
- en: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    is the TorchScript module with embedded NEFF(Neuron Executable File Format) compiled
    by neuron(x) compiler.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.jit._script.ScriptModule`）— [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)是由neuron(x)编译器编译的带有嵌入NEFF（Neuron
    Executable File Format）的TorchScript模块。'
- en: Neuron Model with a token classification head on top (a linear layer on top
    of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元模型，顶部带有一个标记分类头（隐藏状态输出顶部的线性层），例如用于命名实体识别（NER）任务。
- en: This model inherits from `~neuron.modeling.NeuronBaseModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自`~neuron.modeling.NeuronBaseModel`。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存）
- en: Token Classification model on Neuron devices.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元设备上的标记分类模型。
- en: '#### `forward`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L486)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L486)'
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer).
    See [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    and [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](https://huggingface.co/docs/transformers/glossary#input-ids)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.Tensor`）— 词汇表中输入序列标记的索引。可以使用[`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer)获取索引。有关详细信息，请参阅[`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)和[`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](https://huggingface.co/docs/transformers/glossary#input-ids)'
- en: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Mask to avoid performing attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`Union[torch.Tensor, None]`，形状为`(batch_size, sequence_length)`，默认为`None`）—
    用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`. [What are attention masks?](https://huggingface.co/docs/transformers/glossary#attention-mask)
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。[什么是注意力掩码？](https://huggingface.co/docs/transformers/glossary#attention-mask)
- en: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, sequence_length)`,
    defaults to `None`) — Segment token indices to indicate first and second portions
    of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（`Union[torch.Tensor, None]`，形状为`(batch_size, sequence_length)`，默认为`None`）—
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`范围内选择：'
- en: 1 for tokens that are `sentence A`,
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“句子A”,
- en: 0 for tokens that are `sentence B`. [What are token type IDs?](https://huggingface.co/docs/transformers/glossary#token-type-ids)
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“句子B”的标记。[什么是标记类型ID？](https://huggingface.co/docs/transformers/glossary#token-type-ids)
- en: The [NeuronModelForTokenClassification](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForTokenClassification](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of token classification: *(Following model is compiled with neuronx
    compiler and can only be run on INF2.)*'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 标记分类示例：*(以下模型使用neuronx编译器编译，只能在INF2上运行。)*
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: NeuronModelForMultipleChoice
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronModelForMultipleChoice
- en: '### `class optimum.neuron.NeuronModelForMultipleChoice`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForMultipleChoice`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L553)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L553)'
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the `optimum.neuron.modeling.NeuronBaseModel.from_pretrained`
    method to load the model weights.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`transformers.PretrainedConfig`）- [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)是具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看`optimum.neuron.modeling.NeuronBaseModel.from_pretrained`方法以加载模型权重。'
- en: '`model` (`torch.jit._script.ScriptModule`) — [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)
    is the TorchScript module with embedded NEFF(Neuron Executable File Format) compiled
    by neuron(x) compiler.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.jit._script.ScriptModule`）- [torch.jit._script.ScriptModule](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)是由neuron(x)编译器编译的嵌入NEFF（Neuron
    Executable File Format）的TorchScript模块。'
- en: Neuron Model with a multiple choice classification head on top (a linear layer
    on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 带有多选分类头部的神经元模型（池化输出顶部的线性层和softmax），例如用于RocStories/SWAG任务。
- en: This model inherits from `~neuron.modeling.NeuronBaseModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自`~neuron.modeling.NeuronBaseModel`。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存）
- en: Multiple choice model on Neuron devices.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经元设备上的多选模型。
- en: '#### `forward`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L567)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L567)'
- en: '[PRE24]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, num_choices, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. Indices can be obtained
    using [`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer).
    See [`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)
    and [`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](https://huggingface.co/docs/transformers/glossary#input-ids)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, num_choices, sequence_length)`的`torch.Tensor`）-
    词汇表中输入序列标记的索引。可以使用[`AutoTokenizer`](https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer)获取索引。有关详细信息，请参阅[`PreTrainedTokenizer.encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode)和[`PreTrainedTokenizer.__call__`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](https://huggingface.co/docs/transformers/glossary#input-ids)'
- en: '`attention_mask` (`Union[torch.Tensor, None]` of shape `(batch_size, num_choices,
    sequence_length)`, defaults to `None`) — Mask to avoid performing attention on
    padding token indices. Mask values selected in `[0, 1]`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, num_choices, sequence_length)`的`Union[torch.Tensor,
    None]`，默认为`None`）- 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“未被掩盖”的标记，
- en: 0 for tokens that are `masked`. [What are attention masks?](https://huggingface.co/docs/transformers/glossary#attention-mask)
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“被掩盖”的标记。[什么是注意力掩码？](https://huggingface.co/docs/transformers/glossary#attention-mask)
- en: '`token_type_ids` (`Union[torch.Tensor, None]` of shape `(batch_size, num_choices,
    sequence_length)`, defaults to `None`) — Segment token indices to indicate first
    and second portions of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, num_choices, sequence_length)`的`Union[torch.Tensor,
    None]`，默认为`None`）- 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 1 for tokens that are `sentence A`,
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“句子A”,
- en: 0 for tokens that are `sentence B`. [What are token type IDs?](https://huggingface.co/docs/transformers/glossary#token-type-ids)
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“句子B”的标记。[什么是标记类型ID？](https://huggingface.co/docs/transformers/glossary#token-type-ids)
- en: The [NeuronModelForMultipleChoice](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForMultipleChoice](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForMultipleChoice)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of mutliple choice: *(Following model is compiled with neuronx compiler
    and can only be run on INF2.)*'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 多项选择示例：*(以下模型使用神经元编译器编译，只能在INF2上运行。)*
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: NeuronModelForCausalLM
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronModelForCausalLM
- en: '### `class optimum.neuron.NeuronModelForCausalLM`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronModelForCausalLM`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L640)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L640)'
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)
    is the neuron decoder graph.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) — [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)
    是神经元解码器图。'
- en: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    is the Model configuration class with all the parameters of the model.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`transformers.PretrainedConfig`) — [PretrainedConfig](https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig)
    是包含模型所有参数的模型配置类。'
- en: '`model_path` (`Path`) — The directory where the compiled artifacts for the
    model are stored. It can be a temporary directory if the model has never been
    saved locally before.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_path` (`Path`) — 存储模型编译成果的目录。如果模型以前从未在本地保存过，则可以是一个临时目录。'
- en: '`generation_config` (`transformers.GenerationConfig`) — [GenerationConfig](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)
    holds the configuration for the model generation task.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`transformers.GenerationConfig`) — [GenerationConfig](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)
    包含模型生成任务的配置。'
- en: Neuron model with a causal language modeling head for inference on Neuron devices.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元模型，带有用于神经元设备推理的因果语言建模头。
- en: This model inherits from `~neuron.modeling.NeuronDecoderModel`. Check the superclass
    documentation for the generic methods the library implements for all its model
    (such as downloading or saving)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自`~neuron.modeling.NeuronDecoderModel`。请查看超类文档，了解库为所有模型实现的通用方法（如下载或保存）。
- en: '#### `can_generate`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `can_generate`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L717)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L717)'
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Returns True to validate the check made in `GenerationMixin.generate()`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 返回True以验证在`GenerationMixin.generate()`中进行的检查。
- en: '#### `forward`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L667)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L667)'
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor`) — Indices of decoder input sequence tokens
    in the vocabulary of shape `(batch_size, sequence_length)`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`) — 解码器输入序列标记的索引，形状为`(batch_size, sequence_length)`。'
- en: '`cache_ids` (`torch.LongTensor`) — The indices at which the cached key and
    value for the current inputs need to be stored.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_ids` (`torch.LongTensor`) — 需要存储当前输入的缓存键和值的索引。'
- en: '`start_ids` (`torch.LongTensor`) — The indices of the first tokens to be processed,
    deduced form the attention masks.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_ids` (`torch.LongTensor`) — 要处理的第一个标记的索引，从注意力掩码中推断出。'
- en: The [NeuronModelForCausalLM](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeuronModelForCausalLM](/docs/optimum.neuron/main/en/package_reference/modeling#optimum.neuron.NeuronModelForCausalLM)
    的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of text generation:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成示例：
- en: '[PRE29]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#### `generate`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L721)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L721)'
- en: '[PRE30]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, sequence_length)`) — The
    sequence used as a prompt for the generation.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.Tensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) —
    用于避免在填充标记索引上执行注意力的掩码。'
- en: '`generation_config` (`~transformers.generation.GenerationConfig`, *optional*)
    — The generation configuration to be used as base parametrization for the generation
    call. `**kwargs` passed to generate matching the attributes of `generation_config`
    will override them. If `generation_config` is not provided, default will be used,
    which had the following loading priority: 1) from the `generation_config.json`
    model file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit `GenerationConfig`’s default values, whose documentation
    should be checked to parameterize generation.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`~transformers.generation.GenerationConfig`，*可选*) — 用作生成调用的基本参数化的生成配置。传递给与`generation_config`属性匹配的`**kwargs`将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1)
    从`generation_config.json`模型文件中；2) 从模型配置中。请注意，未指定的参数将继承`GenerationConfig`的默认值，应检查其文档以参数化生成。'
- en: '`stopping_criteria` (`Optional[transformers.generation.StoppingCriteriaList],
    defaults to` None`) — Custom stopping criteria that complement the default stopping
    criteria built from arguments and a generation config.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`Optional[transformers.generation.StoppingCriteriaList]，默认为`None`)
    — 自定义停止标准，补充了从参数和生成配置构建的默认停止标准。'
- en: Returns
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: A `torch.FloatTensor`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`torch.FloatTensor`。
- en: A streamlined generate() method overriding the transformers.GenerationMixin.generate()
    method.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简化的generate()方法，覆盖了transformers.GenerationMixin.generate()方法。
- en: This method uses the same logits processors/warpers and stopping criterias as
    the transformers library `generate()` method but restricts the generation to greedy
    search and sampling.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法使用与transformers库`generate()`方法相同的logits处理器/变形器和停止标准，但将生成限制为贪婪搜索和抽样。
- en: It does not support transformers `generate()` advanced options.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 它不支持transformers `generate()` 的高级选项。
- en: Please refer to [https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    for details on generation configuration.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成配置的详细信息，请参阅[https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate)。
- en: '#### `generate_tokens`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate_tokens`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L802)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling.py#L802)'
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。'
- en: '`selector` (`TokenSelector`) — The object implementing the generation logic
    based on transformers processors and stopping criterias.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selector` (`TokenSelector`) — 基于transformers处理器和停止标准实现生成逻辑的对象。'
- en: '`batch_size` (`int`) — The actual input batch size. Used to avoid generating
    tokens for padded inputs.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`) — 实际输入批处理大小。用于避免为填充输入生成标记。'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. model_kwargs
    — Additional model specific kwargs will be forwarded to the `forward` function
    of the model.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) —
    用于避免在填充标记索引上执行注意力的掩码。model_kwargs — 额外的模型特定kwargs将被转发到模型的`forward`函数。'
- en: Returns
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.LongTensor`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.LongTensor`'
- en: A `torch.LongTensor` containing the generated tokens.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 包含生成标记的`torch.LongTensor`。
- en: Generate tokens using sampling or greedy search.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用抽样或贪婪搜索生成标记。
- en: Stable Diffusion
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定扩散
- en: NeuronStableDiffusionPipelineBase
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronStableDiffusionPipelineBase
- en: '### `class optimum.neuron.modeling_diffusion.NeuronStableDiffusionPipelineBase`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.modeling_diffusion.NeuronStableDiffusionPipelineBase`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L81)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L81)'
- en: '[PRE32]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#### `load_model`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_model`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L243)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L243)'
- en: '[PRE33]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`data_parallel_mode` (`Optional[str]`) — Mode to decide what components to
    load into both NeuronCores of a Neuron device. Can be “none”(no data parallel),
    “unet”(only load unet into both cores of each device), “all”(load the whole pipeline
    into both cores).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_parallel_mode` (`Optional[str]`) — 决定将哪些组件加载到Neuron设备的两个NeuronCores中的模式。可以是“none”（无数据并行），“unet”（仅将unet加载到每个设备的两个核心中），“all”（将整个管道加载到两个核心中）。'
- en: '`text_encoder_path` (`Union[str, Path]`) — Path of the compiled text encoder.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_path` (`Union[str, Path]`) — 编译后的文本编码器路径。'
- en: '`unet_path` (`Union[str, Path]`) — Path of the compiled U-NET.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet_path` (`Union[str, Path]`) — 编译后的U-NET路径。'
- en: '`vae_decoder_path` (`Optional[Union[str, Path]]`, defaults to `None`) — Path
    of the compiled VAE decoder.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae_decoder_path` (`Optional[Union[str, Path]]`，默认为`None`) — 编译后的VAE解码器路径。'
- en: '`vae_encoder_path` (`Optional[Union[str, Path]]`, defaults to `None`) — Path
    of the compiled VAE encoder. It is optional, only used for tasks taking images
    as input.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae_encoder_path` (`Optional[Union[str, Path]]`，默认为`None`) — 编译后的VAE编码器路径。这是可选的，仅用于接受图像作为输入的任务。'
- en: '`text_encoder_2_path` (`Optional[Union[str, Path]]`, defaults to `None`) —
    Path of the compiled second frozen text encoder. SDXL only.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_2_path` (`Optional[Union[str, Path]]`，默认为`None`) — 编译后的第二个冻结文本编码器的路径。仅适用于SDXL。'
- en: '`dynamic_batch_size` (`bool`, defaults to `False`) — Whether enable dynamic
    batch size for neuron compiled model. If `True`, the input batch size can be a
    multiple of the batch size during the compilation.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamic_batch_size` (`bool`，默认为`False`) — 是否为神经元编译模型启用动态批处理大小。如果为`True`，输入批处理大小可以是编译期间批处理大小的倍数。'
- en: Loads Stable Diffusion TorchScript modules compiled by neuron(x)-cc compiler.
    It will be first loaded onto CPU and then moved to one or multiple [NeuronCore](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuroncores-arch.html).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 加载由neuron(x)-cc编译器编译的稳定扩散TorchScript模块。首先加载到CPU，然后移动到一个或多个[NeuronCore](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuroncores-arch.html)。
- en: NeuronStableDiffusionPipeline
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronStableDiffusionPipeline
- en: '### `class optimum.neuron.NeuronStableDiffusionPipeline`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronStableDiffusionPipeline`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L798)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L798)'
- en: '[PRE34]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '#### `__call__`'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion.py#L33)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion.py#L33)'
- en: '[PRE35]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`Optional[Union[str, List[str]]]`，默认为`None`) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`num_inference_steps` (`int`, defaults to 50) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`guidance_scale` (`float`, defaults to 7.5) — A higher guidance scale value
    encourages the model to generate images closely linked to the text `prompt` at
    the expense of lower image quality. Guidance scale is enabled when `guidance_scale
    > 1`.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当
    `guidance_scale > 1` 时启用引导比例。'
- en: '`negative_prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) —
    The prompt or prompts to guide what to not include in image generation. If not
    defined, you need to pass `negative_prompt_embeds` instead. Ignored when not using
    guidance (`guidance_scale < 1`).'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 用于指导图像生成中不包含的提示或提示。如果未定义，则需要传递
    `negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）忽略。'
- en: '`num_images_per_prompt` (`int`, defaults to 1) — The number of images to generate
    per prompt. If it is different from the batch size used for the compiltaion, it
    will be overriden by the static batch size of neuron (except for dynamic batching).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, 默认为 1) — 每个提示生成的图像数量。如果与编译使用的批处理大小不同，则会被神经元的静态批处理大小覆盖（除了动态批处理）。'
- en: '`eta` (`float`, defaults to 0.0) — Corresponds to parameter eta (η) from the
    [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the `diffusers.schedulers.DDIMScheduler`,
    and is ignored in other schedulers.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数
    eta (η)。仅适用于 `diffusers.schedulers.DDIMScheduler`，在其他调度程序中被忽略。'
- en: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, 默认为
    `None`) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    noisy latents sampled from a Gaussian distribution, to be used as inputs for image
    generation. Can be used to tweak the same generation with different prompts. If
    not provided, a latents tensor is generated by sampling using the supplied random
    `generator`.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    text embeddings. Can be used to easily tweak text inputs (prompt weighting). If
    not provided, text embeddings are generated from the `prompt` input argument.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，则从
    `prompt` 输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated negative text embeddings. Can be used to easily tweak text inputs
    (prompt weighting). If not provided, `negative_prompt_embeds` are generated from
    the `negative_prompt` input argument.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的负面文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，`negative_prompt_embeds`
    将从 `negative_prompt` 输入参数生成。'
- en: '`output_type` (`Optional[str]`, defaults to `"pil"`) — The output format of
    the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`Optional[str]`, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array`
    之间的一个。'
- en: '`return_dict` (`bool`, defaults to `True`) — Whether or not to return a `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    instead of a plain tuple.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, 默认为 `True`) — 是否返回 `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    而不是普通的 `tuple`。'
- en: '`callback` (`Optional[Callable]`, defaults to `None`) — A function that calls
    every `callback_steps` steps during inference. The function is called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Optional[Callable]`, 默认为 `None`) — 在推理过程中每隔 `callback_steps` 步调用的函数。该函数使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, defaults to 1) — The frequency at which the `callback`
    function is called. If not specified, the callback is called at every step.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, defaults to `None`) — A kwargs dictionary
    that if specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, 默认为 `None`) — 如果指定了，则作为参数传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中定义的 `AttentionProcessor`。'
- en: '`guidance_rescale` (`float`, defaults to 0.0) — Guidance rescale factor from
    [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
    Guidance rescale factor should fix overexposure when using zero terminal SNR.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_rescale` (`float`, 默认为 0.0) — 来自 [Common Diffusion Noise Schedules
    and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 的引导重新缩放因子。引导重新缩放因子应在使用零终端
    SNR 时修复过曝光。'
- en: Returns
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput` or `tuple`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput` 或 `tuple`'
- en: If `return_dict` is `True`, `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains “not-safe-for-work” (nsfw)
    content.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，则返回 `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`，否则返回一个
    `tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含对应生成图像是否包含“不适宜工作场所”（nsfw）内容的 `bool` 列表。
- en: The call function to the pipeline for generation.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE36]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: NeuronStableDiffusionImg2ImgPipeline
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronStableDiffusionImg2ImgPipeline
- en: '### `class optimum.neuron.NeuronStableDiffusionImg2ImgPipeline`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronStableDiffusionImg2ImgPipeline`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L802)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L802)'
- en: '[PRE37]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#### `__call__`'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_img2img.py#L83)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_img2img.py#L83)'
- en: '[PRE38]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`image` (`Optional["PipelineImageInput"]`, defaults to `None`) — `Image`, numpy
    array or tensor representing an image batch to be used as the starting point.
    For both numpy array and pytorch tensor, the expected value range is between `[0,
    1]` If it’s a tensor or a list or tensors, the expected shape should be `(B, C,
    H, W)` or `(C, H, W)`. If it is a numpy array or a list of arrays, the expected
    shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image latents
    as `image`, but if passing latents directly it is not encoded again.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`Optional["PipelineImageInput"]`, 默认为 `None`) — 代表要用作起始点的图像批处理的`Image`、numpy数组或张量。对于numpy数组和pytorch张量，期望值范围在`[0,
    1]`之间。如果是张量或张量列表，期望形状应为`(B, C, H, W)`或`(C, H, W)`。如果是numpy数组或数组列表，期望形状应为`(B, H,
    W, C)`或`(H, W, C)`。它还可以接受图像潜变量作为`image`，但如果直接传递潜变量，则不会再次编码。'
- en: '`strength` (`float`, defaults to 0.8) — Indicates extent to transform the reference
    `image`. Must be between 0 and 1\. `image` is used as a starting point and more
    noise is added the higher the `strength`. The number of denoising steps depends
    on the amount of noise initially added. When `strength` is 1, added noise is maximum
    and the denoising process runs for the full number of iterations specified in
    `num_inference_steps`. A value of 1 essentially ignores `image`.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength` (`float`, 默认为 0.8) — 表示转换参考`image`的程度。必须在0和1之间。`image`用作起始点，随着`strength`的增加，添加的噪音越多。降噪步骤的数量取决于最初添加的噪音量。当`strength`为1时，添加的噪音最大，降噪过程将运行指定的`num_inference_steps`次迭代。值为1基本上忽略`image`。'
- en: '`num_inference_steps` (`int`, defaults to 50) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference. This parameter is modulated by `strength`.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。此参数由`strength`调节。'
- en: '`guidance_scale` (`float`, defaults to 7.5) — A higher guidance scale value
    encourages the model to generate images closely linked to the text `prompt` at
    the expense of lower image quality. Guidance scale is enabled when `guidance_scale
    > 1`.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`Optional[Union[str, List[str]`, defaults to `None`) — The
    prompt or prompts to guide what to not include in image generation. If not defined,
    you need to pass `negative_prompt_embeds` instead. Ignored when not using guidance
    (`guidance_scale < 1`).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`Optional[Union[str, List[str]`, 默认为 `None`) — 用于指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale
    < 1`）忽略。'
- en: '`num_images_per_prompt` (`int`, defaults to 1) — The number of images to generate
    per prompt. If it is different from the batch size used for the compiltaion, it
    will be overriden by the static batch size of neuron (except for dynamic batching).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, 默认为 1) — 每个提示生成的图像数量。如果与编译使用的批处理大小不同，则将被神经元的静态批处理大小覆盖（除了动态批处理）。'
- en: '`eta` (`float`, defaults to 0.0) — Corresponds to parameter eta (η) from the
    [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the `diffusers.schedulers.DDIMScheduler`,
    and is ignored in other schedulers.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, 默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数
    eta (η)。仅适用于`diffusers.schedulers.DDIMScheduler`，在其他调度程序中被忽略。'
- en: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, 默认为
    `None`) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    text embeddings. Can be used to easily tweak text inputs (prompt weighting). If
    not provided, text embeddings are generated from the `prompt` input argument.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated negative text embeddings. Can be used to easily tweak text inputs
    (prompt weighting). If not provided, `negative_prompt_embeds` are generated from
    the `negative_prompt` input argument.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。'
- en: '`output_type` (`Optional[str]`, defaults to `"pil"`) — The output format of
    the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`Optional[str]`, 默认为 `"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。'
- en: '`return_dict` (`bool`, defaults to `True`) — Whether or not to return a `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    instead of a plain tuple.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, 默认为 `True`) — 是否返回`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`而不是普通元组。'
- en: '`callback` (`Optional[Callable]`, defaults to `None`) — A function that calls
    every `callback_steps` steps during inference. The function is called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Optional[Callable]`, 默认为`None`) — 在推理过程中每隔`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, defaults to 1) — The frequency at which the `callback`
    function is called. If not specified, the callback is called at every step.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, 默认为1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, defaults to `None`) — A kwargs dictionary
    that if specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, 默认为`None`) — 如果指定，将传递给`AttentionProcessor`的kwargs字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的。'
- en: Returns
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput` or `tuple`'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`或`tuple`'
- en: If `return_dict` is `True`, `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains “not-safe-for-work” (nsfw)
    content.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`，否则返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”(nsfw)内容的`bool`列表。
- en: The call function to the pipeline for generation.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE39]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: NeuronStableDiffusionInpaintPipeline
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronStableDiffusionInpaintPipeline
- en: '### `class optimum.neuron.NeuronStableDiffusionInpaintPipeline`'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronStableDiffusionInpaintPipeline`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L808)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L808)'
- en: '[PRE40]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '#### `__call__`'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_inpaint.py#L48)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_inpaint.py#L48)'
- en: '[PRE41]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`Optional[Union[str, List[str]]]`, 默认为`None`) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`image` (`Optional["PipelineImageInput"]`, defaults to `None`) — `Image`, numpy
    array or tensor representing an image batch to be inpainted (which parts of the
    image to be masked out with `mask_image` and repainted according to `prompt`).
    For both numpy array and pytorch tensor, the expected value range is between `[0,
    1]` If it’s a tensor or a list or tensors, the expected shape should be `(B, C,
    H, W)` or `(C, H, W)`. If it is a numpy array or a list of arrays, the expected
    shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image latents
    as `image`, but if passing latents directly it is not encoded again.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`Optional["PipelineImageInput"]`, 默认为`None`) — 代表要修复的图像批次的图像、numpy数组或张量（使用`mask_image`遮罩和根据`prompt`重新绘制图像的部分）。对于numpy数组和pytorch张量，期望值范围在`[0,
    1]`之间。如果是张量或张量列表，期望形状应为`(B, C, H, W)`或`(C, H, W)`。如果是numpy数组或数组列表，期望形状应为`(B, H,
    W, C)`或`(H, W, C)`。它还可以接受图像潜变量作为`image`，但如果直接传递潜变量，则不会再次编码。'
- en: '`mask_image` (`Optional["PipelineImageInput"]`, defaults to `None`) — `Image`,
    numpy array or tensor representing an image batch to mask `image`. White pixels
    in the mask are repainted while black pixels are preserved. If `mask_image` is
    a PIL image, it is converted to a single channel (luminance) before use. If it’s
    a numpy array or pytorch tensor, it should contain one color channel (L) instead
    of 3, so the expected shape for pytorch tensor would be `(B, 1, H, W)`, `(B, H,
    W)`, `(1, H, W)`, `(H, W)`. And for numpy array would be for `(B, H, W, 1)`, `(B,
    H, W)`, `(H, W, 1)`, or `(H, W)`.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_image` (`Optional["PipelineImageInput"]`, 默认为`None`) — 代表要遮罩`image`的图像、numpy数组或张量。遮罩中的白色像素被重新绘制，而黑色像素被保留。如果`mask_image`是PIL图像，则在使用之前将其转换为单通道（亮度）。如果是numpy数组或pytorch张量，则应包含一个颜色通道（L）而不是3个，因此pytorch张量的预期形状为`(B,
    1, H, W)`、`(B, H, W)`、`(1, H, W)`、`(H, W)`。对于numpy数组，预期形状为`(B, H, W, 1)`、`(B,
    H, W)`、`(H, W, 1)`或`(H, W)`。'
- en: '`strength` (`float`, defaults to 1.0) — Indicates extent to transform the reference
    `image`. Must be between 0 and 1\. `image` is used as a starting point and more
    noise is added the higher the `strength`. The number of denoising steps depends
    on the amount of noise initially added. When `strength` is 1, added noise is maximum
    and the denoising process runs for the full number of iterations specified in
    `num_inference_steps`. A value of 1 essentially ignores `image`.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength` (`float`, 默认为1.0) — 表示转换参考`image`的程度。必须在0和1之间。`image`被用作起点，`strength`越高，添加的噪音就越多。降噪步骤的数量取决于最初添加的噪音量。当`strength`为1时，添加的噪音最大，降噪过程将运行指定的`num_inference_steps`的全部迭代次数。值为1实际上忽略了`image`。'
- en: '`num_inference_steps` (`int`, defaults to 50) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference. This parameter is modulated by `strength`.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。此参数由`strength`调节。'
- en: '`guidance_scale` (`float`, defaults to 7.5) — A higher guidance scale value
    encourages the model to generate images closely linked to the text `prompt` at
    the expense of lower image quality. Guidance scale is enabled when `guidance_scale
    > 1`.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, 默认为7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`Optional[Union[str, List[str]`, defaults to `None`) — The
    prompt or prompts to guide what to not include in image generation. If not defined,
    you need to pass `negative_prompt_embeds` instead. Ignored when not using guidance
    (`guidance_scale < 1`).'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`Optional[Union[str, List[str]`, 默认为 `None`) — 用于指导图像生成中不包含的提示或提示。如果未定义，则需要传递
    `negative_prompt_embeds`。在不使用指导时被忽略 (`guidance_scale < 1`)。'
- en: '`num_images_per_prompt` (`int`, defaults to 1) — The number of images to generate
    per prompt. If it is different from the batch size used for the compiltaion, it
    will be overriden by the static batch size of neuron (except for dynamic batching).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, 默认为 1) — 每个提示生成的图像数量。如果与用于编译的批处理大小不同，则会被 neuron
    的静态批处理大小覆盖（除了动态批处理）。'
- en: '`eta` (`float`, defaults to 0.0) — Corresponds to parameter eta (η) from the
    [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the `diffusers.schedulers.DDIMScheduler`,
    and is ignored in other schedulers.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数
    eta (η)。仅适用于 `diffusers.schedulers.DDIMScheduler`，在其他调度程序中被忽略。'
- en: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, 默认为
    `None`) — 用于使生成结果确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    noisy latents sampled from a Gaussian distribution, to be used as inputs for image
    generation. Can be used to tweak the same generation with different prompts. If
    not provided, a latents tensor is generated by sampling using the supplied random
    `generator`.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可以用来使用不同提示调整相同生成。如果未提供，则通过使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    text embeddings. Can be used to easily tweak text inputs (prompt weighting). If
    not provided, text embeddings are generated from the `prompt` input argument.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从
    `prompt` 输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated negative text embeddings. Can be used to easily tweak text inputs
    (prompt weighting). If not provided, `negative_prompt_embeds` are generated from
    the `negative_prompt` input argument.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的负面文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从
    `negative_prompt` 输入参数生成 `negative_prompt_embeds`。'
- en: '`output_type` (`Optional[str]`, defaults to `"pil"`) — The output format of
    the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`Optional[str]`, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array`
    之间。'
- en: '`return_dict` (`bool`, defaults to `True`) — Whether or not to return a `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    instead of a plain tuple.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, 默认为 `True`) — 是否返回 `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    而不是普通的 tuple。'
- en: '`callback` (`Optional[Callable]`, defaults to `None`) — A function that calls
    every `callback_steps` steps during inference. The function is called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Optional[Callable]`, 默认为 `None`) — 在推断过程中每 `callback_steps` 步调用的函数。该函数使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, defaults to 1) — The frequency at which the `callback`
    function is called. If not specified, the callback is called at every step.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, defaults to `None`) — A kwargs dictionary
    that if specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, 默认为 `None`) — 一个 kwargs 字典，如果指定了，则会传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中定义的 `AttentionProcessor`。'
- en: '`clip_skip` (`int`, defaults to `None`) — Number of layers to be skipped from
    CLIP while computing the prompt embeddings. A value of 1 means that the output
    of the pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, 默认为 `None`) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用前一个层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput` or `tuple`'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput` 或 `tuple`'
- en: If `return_dict` is `True`, `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains “not-safe-for-work” (nsfw)
    content.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，则返回 `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`，否则返回一个
    `tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含对应生成图像是否包含“不安全内容”（nsfw）的 `bool` 列表。
- en: The call function to the pipeline for generation.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE42]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: NeuronLatentConsistencyModelPipeline
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronLatentConsistencyModelPipeline
- en: '### `class optimum.neuron.NeuronLatentConsistencyModelPipeline`'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronLatentConsistencyModelPipeline`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L814)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L814)'
- en: '[PRE43]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#### `__call__`'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_latent_consistency_text2img.py#L67)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_latent_consistency_text2img.py#L67)'
- en: '[PRE44]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`Optional[Union[str, List[str]]]`，默认为`None`） - 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`num_inference_steps` (`int`, defaults to 50) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`（`int`，默认为50） - 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`original_inference_steps` (`Optional[int]`, defaults to `None`) — The original
    number of inference steps use to generate a linearly-spaced timestep schedule,
    from which we will draw `num_inference_steps` evenly spaced timesteps from as
    our final timestep schedule, following the Skipping-Step method in the paper (see
    Section 4.3). If not set this will default to the scheduler’s `original_inference_steps`
    attribute.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_inference_steps`（`Optional[int]`，默认为`None`） - 用于生成线性间隔时间步骤表的原始推理步骤数，从中我们将均匀间隔地从中抽取`num_inference_steps`个时间步骤作为我们的最终时间步骤表，遵循论文中的Skipping-Step方法（参见第4.3节）。如果未设置，这将默认为调度器的`original_inference_steps`属性。'
- en: '`guidance_scale` (`float`, defaults to 8.5) — A higher guidance scale value
    encourages the model to generate images closely linked to the text `prompt` at
    the expense of lower image quality. Guidance scale is enabled when `guidance_scale
    > 1`. Note that the original latent consistency models paper uses a different
    CFG formulation where the guidance scales are decreased by 1 (so in the paper
    formulation CFG is enabled when `guidance_scale > 0`).'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`（`float`，默认为8.5） - 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。请注意，原始的潜变量一致性模型论文使用了不同的CFG公式，其中引导比例减1（因此在论文公式中，当`guidance_scale >
    0`时启用CFG）。'
- en: '`num_images_per_prompt` (`int`, defaults to 1) — The number of images to generate
    per prompt.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`（`int`，默认为1） - 每个提示生成的图像数量。'
- en: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（`Optional[Union[torch.Generator, List[torch.Generator]]]`，默认为`None`）
    - 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    noisy latents sampled from a Gaussian distribution, to be used as inputs for image
    generation. Can be used to tweak the same generation with different prompts. If
    not provided, a latents tensor is generated by sampling using the supplied random
    `generator`.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`（`Optional[torch.FloatTensor]`，默认为`None`） - 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可以用来使用不同提示微调相同的生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    text embeddings. Can be used to easily tweak text inputs (prompt weighting). If
    not provided, text embeddings are generated from the `prompt` input argument.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds`（`Optional[torch.FloatTensor]`，默认为`None`） - 预生成的文本嵌入。可以用来轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`output_type` (`str`, defaults to `"pil"`) — The output format of the generated
    image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type`（`str`，默认为`"pil"`） - 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。'
- en: '`return_dict` (`bool`, defaults to `True`) — Whether or not to return a `~pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    instead of a plain tuple.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，默认为`True`） - 是否返回`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`而不是普通元组。'
- en: '`cross_attention_kwargs` (`Optional[Dict[str, Any]]`, defaults to `None`) —
    A kwargs dictionary that if specified is passed along to the `AttentionProcessor`
    as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs`（`Optional[Dict[str, Any]]`，默认为`None`） - 如果指定了kwargs字典，则将其传递给[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`。'
- en: '`clip_skip` (`Optional[int]`, defaults to `None`) — Number of layers to be
    skipped from CLIP while computing the prompt embeddings. A value of 1 means that
    the output of the pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip`（`Optional[int]`，默认为`None`） - 在计算提示嵌入时要跳过的CLIP层数。值为1意味着将使用预最终层的输出来计算提示嵌入。'
- en: '`callback_on_step_end` (`Optional[Callable]`, defaults to `None`) — A function
    that calls at the end of each denoising steps during the inference. The function
    is called with the following arguments: `callback_on_step_end(self: DiffusionPipeline,
    step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include
    a list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end`（`Optional[Callable]`，默认为`None`） - 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List[str]`, defaults to `["latents"]`)
    — The list of tensor inputs for the `callback_on_step_end` function. The tensors
    specified in the list will be passed as `callback_kwargs` argument. You will only
    be able to include variables listed in the `._callback_tensor_inputs` attribute
    of your pipeine class.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs`（`List[str]`，默认为`["latents"]`） - `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在您的管道类的`._callback_tensor_inputs`属性中列出的变量。'
- en: Returns
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput` or `tuple`'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`或`tuple`'
- en: If `return_dict` is `True`, `diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains “not-safe-for-work” (nsfw)
    content.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回`diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`，否则返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个`bool`列表，指示相应生成的图像是否包含“不适宜工作”（nsfw）内容。
- en: The call function to the pipeline for generation.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: NeuronStableDiffusionXLPipeline
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元稳定扩散XL管道
- en: '### `class optimum.neuron.NeuronStableDiffusionXLPipeline`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronStableDiffusionXLPipeline`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L873)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L873)'
- en: '[PRE45]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#### `__call__`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_xl.py#L38)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_xl.py#L38)'
- en: '[PRE46]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
    instead.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`Optional[Union[str, List[str]]]`, 默认为`None`) — 用于指导图像生成的提示或提示。如果未定义，则必须传递`prompt_embeds`。'
- en: '`prompt_2` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined,
    `prompt` is used in both text-encoders'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_2` (`Optional[Union[str, List[str]]]`, 默认为`None`) — 发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则在两个文本编码器中使用`prompt`。'
- en: '`num_inference_steps` (`int`, defaults to 50) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, 默认为50) — 去噪步数。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`denoising_end` (`Optional[float]`, defaults to `None`) — When specified, determines
    the fraction (between 0.0 and 1.0) of the total denoising process to be completed
    before it is intentionally prematurely terminated. As a result, the returned sample
    will still retain a substantial amount of noise as determined by the discrete
    timesteps selected by the scheduler. The denoising_end parameter should ideally
    be utilized when this pipeline forms a part of a “Mixture of Denoisers” multi-pipeline
    setup, as elaborated in [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`denoising_end` (`Optional[float]`, 默认为`None`) — 当指定时，确定在故意提前终止之前完成的去噪过程的比例（介于0.0和1.0之间）。因此，返回的样本仍将保留由调度程序选择的离散时间步确定的大量噪声。当此管道作为“去噪器混合”多管道设置的一部分时，应理想地利用`denoising_end`参数，如[`精细调整图像输出`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)中所详细阐述'
- en: '`guidance_scale` (`float`, defaults to 5.0) — Guidance scale as defined in
    [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598). `guidance_scale`
    is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, 默认为5.0`) — 在[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。'
- en: '`negative_prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) —
    The prompt or prompts not to guide the image generation. If not defined, one has
    to pass `negative_prompt_embeds` instead. Ignored when not using guidance (i.e.,
    ignored if `guidance_scale` is less than `1`).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`Optional[Union[str, List[str]]]`, 默认为`None`) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。当不使用指导时（即，如果`guidance_scale`小于`1`，则会被忽略）。'
- en: '`negative_prompt_2` (`Optional[Union[str, List[str]]]`, defaults to `None`)
    — The prompt or prompts not to guide the image generation to be sent to `tokenizer_2`
    and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_2` (`Optional[Union[str, List[str]]]`, 默认为`None`) — 不指导图像生成的提示或提示，将发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则在两个文本编码器中使用`negative_prompt`。'
- en: '`num_images_per_prompt` (`int`, defaults to 1) — The number of images to generate
    per prompt. If it is different from the batch size used for the compiltaion, it
    will be overriden by the static batch size of neuron (except for dynamic batching).'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, 默认为1`) — 每个提示生成的图像数量。如果与用于编译的批处理大小不同，则将被神经元的静态批处理大小覆盖（除了动态批处理）。'
- en: '`eta` (`float`, defaults to 0.0) — Corresponds to parameter eta (η) in the
    DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to `schedulers.DDIMScheduler`, will be ignored for others.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, 默认为0.0`) — 对应于DDIM论文中的参数eta（η）：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于`schedulers.DDIMScheduler`，对其他情况将被忽略。'
- en: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, 默认为`None`)
    — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)用于使生成过程确定性。'
- en: '`latents` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    noisy latents, sampled from a Gaussian distribution, to be used as inputs for
    image generation. Can be used to tweak the same generation with different prompts.
    If not provided, a latents tensor will ge generated by sampling using the supplied
    random `generator`.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, text embeddings will be generated from `prompt` input argument.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从
    `prompt` 输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated negative text embeddings. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从
    `negative_prompt` 输入参数生成负 prompt 嵌入。'
- en: '`pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated pooled text embeddings. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, pooled text embeddings will be generated
    from `prompt` input argument.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从
    `prompt` 输入参数生成池化的文本嵌入。'
- en: '`negative_pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to
    `None`) — Pre-generated negative pooled text embeddings. Can be used to easily
    tweak text inputs, *e.g.* prompt weighting. If not provided, pooled negative_prompt_embeds
    will be generated from `negative_prompt` input argument.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`)
    — 预生成的负池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成池化的负 prompt
    嵌入。'
- en: '`output_type` (`Optional[str]`, defaults to `"pil"`) — The output format of
    the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`Optional[str]`, 默认为 `"pil"`) — 生成图像的输出格式。选择在 [PIL](https://pillow.readthedocs.io/en/stable/)
    中的 `PIL.Image.Image` 或 `np.array` 之间。'
- en: '`return_dict` (`bool`, defaults to `True`) — Whether or not to return a `diffusers.pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`
    instead of a plain tuple.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, 默认为 `True`) — 是否返回一个 `diffusers.pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`
    而不是一个普通的元组。'
- en: '`callback` (`Optional[Callable]`, defaults to `None`) — A function that will
    be called every `callback_steps` steps during inference. The function will be
    called with the following arguments: `callback(step: int, timestep: int, latents:
    torch.FloatTensor)`.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Optional[Callable]`, 默认为 `None`) — 一个函数，在推理过程中每 `callback_steps`
    步会被调用一次。该函数将会以以下参数被调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, defaults to 1) — The frequency at which the `callback`
    function will be called. If not specified, the callback will be called at every
    step.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, 默认为 1) — `callback` 函数被调用的频率。如果未指定，将在每一步调用回调函数。'
- en: '`cross_attention_kwargs` (`dict`, defaults to `None`) — A kwargs dictionary
    that if specified is passed along to the `AttentionProcessor` as defined under
    `self.processor` in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, 默认为 `None`) — 一个 kwargs 字典，如果指定，将传递给 `AttentionProcessor`，在
    [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中由 `self.processor` 定义。'
- en: '`guidance_rescale` (`float`, *optional*, defaults to 0.0) — Guidance rescale
    factor proposed by [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)
    `guidance_scale` is defined as `φ` in equation 16\. of [Common Diffusion Noise
    Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
    Guidance rescale factor should fix overexposure when using zero terminal SNR.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_rescale` (`float`, *可选*, 默认为 0.0) — [Common Diffusion Noise Schedules
    and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 提出的指导重缩放因子。`guidance_scale`
    在 [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)
    的方程式 16 中定义。指导重缩放因子应该在使用零终端 SNR 时修复过曝光问题。'
- en: '`original_size` (`Optional[Tuple[int, int]]`, defaults to (1024, 1024)) — If
    `original_size` is not the same as `target_size` the image will appear to be down-
    or upsampled. `original_size` defaults to `(width, height)` if not specified.
    Part of SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_size` (`Optional[Tuple[int, int]]`, 默认为 (1024, 1024)) — 如果 `original_size`
    与 `target_size` 不同，图像将会被缩小或放大。如果未指定，`original_size` 默认为 `(width, height)`。这是 SDXL
    微调的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    第2.2节。'
- en: '`crops_coords_top_left` (`Tuple[int]`, defaults to (0, 0)) — `crops_coords_top_left`
    can be used to generate an image that appears to be “cropped” from the position
    `crops_coords_top_left` downwards. Favorable, well-centered images are usually
    achieved by setting `crops_coords_top_left` to (0, 0). Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_coords_top_left` (`Tuple[int]`, 默认为 (0, 0)) — `crops_coords_top_left`
    可用于生成一个看起来被从位置 `crops_coords_top_left` 向下“裁剪”的图像。通常通过将 `crops_coords_top_left`
    设置为 (0, 0) 来获得令人满意、居中的图像。这是 SDXL 微调的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    第2.2节。'
- en: '`target_size` (`Tuple[int]`,defaults to (1024, 1024)) — For most cases, `target_size`
    should be set to the desired height and width of the generated image. If not specified
    it will default to `(width, height)`. Part of SDXL’s micro-conditioning as explained
    in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_size` (`Tuple[int]`, 默认为 (1024, 1024)) — 对于大多数情况，`target_size` 应设置为生成图像的期望高度和宽度。如果未指定，将默认为
    `(width, height)`。作为 SDXL 的微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    的第 2.2 节。'
- en: '`negative_original_size` (`Tuple[int]`, defaults to (1024, 1024)) — To negatively
    condition the generation process based on a specific image resolution. Part of
    SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_original_size` (`Tuple[int]`, 默认为 (1024, 1024)) — 根据特定图像分辨率对生成过程进行负面条件。作为
    SDXL 的微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    的第 2.2 节。更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`negative_crops_coords_top_left` (`Tuple[int]`, defaults to (0, 0)) — To negatively
    condition the generation process based on a specific crop coordinates. Part of
    SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_crops_coords_top_left` (`Tuple[int]`, 默认为 (0, 0)) — 根据特定的裁剪坐标对生成过程进行负面条件。作为
    SDXL 的微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    的第 2.2 节。更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`negative_target_size` (`Tuple[int]`, defaults to (1024, 1024)) — To negatively
    condition the generation process based on a target image resolution. It should
    be as same as the `target_size` for most cases. Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_target_size` (`Tuple[int]`, 默认为 (1024, 1024)) — 根据目标图像分辨率对生成过程进行负面条件。对于大多数情况，它应与
    `target_size` 相同。作为 SDXL 的微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)
    的第 2.2 节。更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`clip_skip` (`Optional[int]`, defaults to `None`) — Number of layers to be
    skipped from CLIP while computing the prompt embeddings. A value of 1 means that
    the output of the pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`Optional[int]`, 默认为 `None`) — 在计算提示嵌入时，要从 CLIP 中跳过的层数。值为 1 表示将使用预终层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`diffusers.pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` or
    `tuple`'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` 或
    `tuple`'
- en: '`diffusers.pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` if
    `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first
    element is a list with the generated images.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 True，则返回 `diffusers.pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`，否则返回一个
    `tuple`。当返回一个 tuple 时，第一个元素是一个包含生成图像的列表。
- en: Function invoked when calling the pipeline for generation.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 调用管道进行生成时调用的函数。
- en: 'Examples:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE47]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: NeuronStableDiffusionXLImg2ImgPipeline
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronStableDiffusionXLImg2ImgPipeline
- en: '### `class optimum.neuron.NeuronStableDiffusionXLImg2ImgPipeline`'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronStableDiffusionXLImg2ImgPipeline`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L877)'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L877)'
- en: '[PRE48]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '#### `__call__`'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_xl_img2img.py#L109)'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_xl_img2img.py#L109)'
- en: '[PRE49]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
    instead.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 用于引导图像生成的提示或提示。如果未定义，则必须传递
    `prompt_embeds`。'
- en: '`prompt_2` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined,
    `prompt` is used in both text-encoders'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_2` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 要发送到 `tokenizer_2`
    和 `text_encoder_2` 的提示或提示。如果未定义，则在两个文本编码器中都使用 `prompt`。'
- en: '`image` (`Optional["PipelineImageInput"]`, defaults to `None`) — The image(s)
    to modify with the pipeline.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`Optional["PipelineImageInput"]`, 默认为 `None`) — 要使用管道修改的图像。'
- en: '`strength` (`float`, defaults to 0.3) — Conceptually, indicates how much to
    transform the reference `image`. Must be between 0 and 1\. `image` will be used
    as a starting point, adding more noise to it the larger the `strength`. The number
    of denoising steps depends on the amount of noise initially added. When `strength`
    is 1, added noise will be maximum and the denoising process will run for the full
    number of iterations specified in `num_inference_steps`. A value of 1, therefore,
    essentially ignores `image`. Note that in the case of `denoising_start` being
    declared as an integer, the value of `strength` will be ignored.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength` (`float`, 默认为 0.3) — 在概念上，指示要转换参考 `image` 的程度。必须在 0 和 1 之间。`image`
    将被用作起点，添加的噪音越大，`strength` 越大。去噪步骤的数量取决于最初添加的噪音量。当 `strength` 为 1 时，添加的噪音将达到最大，并且去噪过程将运行指定的
    `num_inference_steps` 的完整迭代次数。因此，值为 1 的情况下，实际上忽略了 `image`。请注意，在声明 `denoising_start`
    为整数的情况下，将忽略 `strength` 的值。'
- en: '`num_inference_steps` (`int`, defaults to 50) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, 默认为 50) — 去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`denoising_start` (`Optional[float]`, defaults to `None`) — When specified,
    indicates the fraction (between 0.0 and 1.0) of the total denoising process to
    be bypassed before it is initiated. Consequently, the initial part of the denoising
    process is skipped and it is assumed that the passed `image` is a partly denoised
    image. Note that when this is specified, strength will be ignored. The `denoising_start`
    parameter is particularly beneficial when this pipeline is integrated into a “Mixture
    of Denoisers” multi-pipeline setup, as detailed in [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output).'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`denoising_start` (`Optional[float]`, 默认为 `None`) — 当指定时，指示在启动之前要跳过的总去噪过程的比例（介于
    0.0 和 1.0 之间）。因此，将跳过去噪过程的初始部分，并假定传递的 `image` 是部分去噪的图像。请注意，当指定此参数时，将忽略强度。当此流水线被集成到“去噪器混合”多流水线设置中时，`denoising_start`
    参数特别有益，如 [`精炼图像输出`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)
    中详细说明。'
- en: '`denoising_end` (`Optional[float]`, defaults to `None`) — When specified, determines
    the fraction (between 0.0 and 1.0) of the total denoising process to be completed
    before it is intentionally prematurely terminated. As a result, the returned sample
    will still retain a substantial amount of noise (ca. final 20% of timesteps still
    needed) and should be denoised by a successor pipeline that has `denoising_start`
    set to 0.8 so that it only denoises the final 20% of the scheduler. The denoising_end
    parameter should ideally be utilized when this pipeline forms a part of a “Mixture
    of Denoisers” multi-pipeline setup, as elaborated in [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`denoising_end` (`Optional[float]`, 默认为 `None`) — 当指定时，确定在意图上提前终止之前完成的去噪过程的比例（介于
    0.0 和 1.0 之间）。因此，返回的样本仍将保留相当多的噪音（大约还需要最后 20% 的时间步），应由具有 `denoising_start` 设置为
    0.8 的后续流水线进行去噪，以便仅对调度器的最后 20% 进行去噪。当此流水线形成“去噪器混合”多流水线设置的一部分时，应理想地利用 `denoising_end`
    参数，如 [`精炼图像输出`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)
    中详细说明。'
- en: '`guidance_scale` (`float`, defaults to 7.5) — Guidance scale as defined in
    [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598). `guidance_scale`
    is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, 默认为 7.5) — 在 [无分类器扩散指导](https://arxiv.org/abs/2207.12598)
    中定义的指导比例。`guidance_scale` 定义为 [Imagen 论文](https://arxiv.org/pdf/2205.11487.pdf)
    中方程 2 的 `w`。通过设置 `guidance_scale > 1` 启用指导比例。更高的指导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。'
- en: '`negative_prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) —
    The prompt or prompts not to guide the image generation. If not defined, one has
    to pass `negative_prompt_embeds` instead. Ignored when not using guidance (i.e.,
    ignored if `guidance_scale` is less than `1`).'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 不用来指导图像生成的提示。如果未定义，则必须传递
    `negative_prompt_embeds`。当不使用指导时（即，如果 `guidance_scale` 小于 `1`，则忽略）。'
- en: '`negative_prompt_2` (`Optional[Union[str, List[str]]]`, defaults to `None`)
    — The prompt or prompts not to guide the image generation to be sent to `tokenizer_2`
    and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_2` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 不用来指导发送到
    `tokenizer_2` 和 `text_encoder_2` 的图像生成的提示。如果未定义，则在两个文本编码器中都使用 `negative_prompt`。'
- en: '`num_images_per_prompt` (`int`, defaults to 1) — The number of images to generate
    per prompt. If it is different from the batch size used for the compiltaion, it
    will be overriden by the static batch size of neuron (except for dynamic batching).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, 默认为 1) — 每个提示生成的图像数量。如果与用于编译的批处理大小不同，则将被神经元的静态批处理大小覆盖（除了动态批处理）。'
- en: '`eta` (`float`, defaults to 0.0) — Corresponds to parameter eta (η) in the
    DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to `schedulers.DDIMScheduler`, will be ignored for others.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, 默认为 0.0) — 对应于 DDIM 论文中的参数 eta (η)：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于
    `schedulers.DDIMScheduler`，对于其他情况将被忽略。'
- en: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, 默认为
    `None`) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    的列表，用于使生成过程确定性。'
- en: '`latents` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    noisy latents, sampled from a Gaussian distribution, to be used as inputs for
    image generation. Can be used to tweak the same generation with different prompts.
    If not provided, a latents tensor will ge generated by sampling using the supplied
    random `generator`.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，将使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, text embeddings will be generated from `prompt` input argument.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`Optional[torch.FloatTensor]`, 默认为 `None`) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated negative text embeddings. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`，默认为`None`) — 预生成的负面文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负面文本嵌入。'
- en: '`pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated pooled text embeddings. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, pooled text embeddings will be generated
    from `prompt` input argument.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`Optional[torch.FloatTensor]`，默认为`None`) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化文本嵌入。'
- en: '`negative_pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to
    `None`) — Pre-generated negative pooled text embeddings. Can be used to easily
    tweak text inputs, *e.g.* prompt weighting. If not provided, pooled negative_prompt_embeds
    will be generated from `negative_prompt` input argument.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`Optional[torch.FloatTensor]`，默认为`None`) —
    预生成的负面池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负面池化文本嵌入。'
- en: '`output_type` (`Optional[str]`, defaults to `"pil"`) — The output format of
    the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`Optional[str]`，默认为`"pil"`) — 生成图像的输出格式。选择[PIL](https://pillow.readthedocs.io/en/stable/)：`PIL.Image.Image`或`np.array`之间的选项。'
- en: '`return_dict` (`bool`, defaults to `True`) — Whether or not to return a `diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`
    instead of a plain tuple.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，默认为`True`) — 是否返回`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`而不是普通元组。'
- en: '`callback` (`Optional[Callable]`, defaults to `None`) — A function that will
    be called every `callback_steps` steps during inference. The function will be
    called with the following arguments: `callback(step: int, timestep: int, latents:
    torch.FloatTensor)`.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Optional[Callable]`，默认为`None`) — 在推断过程中，每`callback_steps`步调用一次的函数。该函数将使用以下参数进行调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_stcallback_steps` (`int`, defaults to 1) — The frequency at which
    the `callback` function will be called. If not specified, the callback will be
    called at every step.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_stcallback_steps` (`int`，默认为1) — `callback`函数将被调用的频率。如果未指定，将在每一步调用回调。'
- en: '`cross_attention_kwargs` (`Optional[Dict[str, Any]]`, defaults to `None`) —
    A kwargs dictionary that if specified is passed along to the `AttentionProcessor`
    as defined under `self.processor` in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`Optional[Dict[str, Any]]`，默认为`None`) — 如果指定，将传递给`AttentionProcessor`中的`self.processor`定义的kwargs字典，在[diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中有详细定义。'
- en: '`guidance_rescale` (`float`, defaults to 0.0) — Guidance rescale factor proposed
    by [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)
    `guidance_scale` is defined as `φ` in equation 16\. of [Common Diffusion Noise
    Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
    Guidance rescale factor should fix overexposure when using zero terminal SNR.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_rescale` (`float`，默认为0.0）— [Common Diffusion Noise Schedules and
    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 提出的指导重缩放因子，`guidance_scale`
    在方程式16中被定义为`φ`。当使用零终端信噪比时，指导重缩放因子应该修复过曝光问题。'
- en: '`original_size` (`Optional[Tuple[int, int]]`, defaults to (1024, 1024)) — If
    `original_size` is not the same as `target_size` the image will appear to be down-
    or upsampled. `original_size` defaults to `(width, height)` if not specified.
    Part of SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_size` (`Optional[Tuple[int, int]`，默认为(1024, 1024)) — 如果`original_size`与`target_size`不同，图像将呈现为缩小或放大。如果未指定，`original_size`默认为`(width,
    height)`。作为SDXL微调的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。'
- en: '`crops_coords_top_left` (`Tuple[int]`, defaults to (0, 0)) — `crops_coords_top_left`
    can be used to generate an image that appears to be “cropped” from the position
    `crops_coords_top_left` downwards. Favorable, well-centered images are usually
    achieved by setting `crops_coords_top_left` to (0, 0). Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_coords_top_left` (`Tuple[int]`，默认为(0, 0)) — `crops_coords_top_left`
    可用于生成一个看起来被“裁剪”到`crops_coords_top_left`位置以下的图像。通常通过将`crops_coords_top_left`设置为(0,
    0)来实现有利的、居中的图像。作为SDXL微调的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。'
- en: '`target_size` (`Tuple[int]`,defaults to (1024, 1024)) — For most cases, `target_size`
    should be set to the desired height and width of the generated image. If not specified
    it will default to `(width, height)`. Part of SDXL’s micro-conditioning as explained
    in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_size` (`Tuple[int]`，默认为(1024, 1024)) — 对于大多数情况，`target_size` 应设置为生成图像的期望高度和宽度。如果未指定，将默认为`(width,
    height)`。作为SDXL微调的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。'
- en: '`negative_original_size` (`Tuple[int]`, defaults to (1024, 1024)) — To negatively
    condition the generation process based on a specific image resolution. Part of
    SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_original_size` (`Tuple[int]`, 默认为(1024, 1024)) — 基于特定图像分辨率对生成过程进行负面调节。SDXL的微调条件的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节所述。有关更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`negative_crops_coords_top_left` (`Tuple[int]`, defaults to (0, 0)) — To negatively
    condition the generation process based on a specific crop coordinates. Part of
    SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_crops_coords_top_left` (`Tuple[int]`, 默认为(0, 0)) — 基于特定裁剪坐标对生成过程进行负面调节。SDXL的微调条件的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节所述。有关更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`negative_target_size` (`Tuple[int]`, defaults to (1024, 1024)) — To negatively
    condition the generation process based on a target image resolution. It should
    be as same as the `target_size` for most cases. Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_target_size` (`Tuple[int]`, 默认为(1024, 1024)) — 基于目标图像分辨率对生成过程进行负面调节。对于大多数情况，它应与`target_size`相同。SDXL的微调条件的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节所述。有关更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`aesthetic_score` (`float`, defaults to 6.0) — Used to simulate an aesthetic
    score of the generated image by influencing the positive text condition. Part
    of SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aesthetic_score` (`float`, 默认为6.0) — 用于通过影响正文条件来模拟生成图像的美学评分。SDXL的微调条件的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节所述。'
- en: '`negative_aesthetic_score` (`float`, defaults to 2.5) — Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    Can be used to simulate an aesthetic score of the generated image by influencing
    the negative text condition.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_aesthetic_score` (`float`, 默认为2.5) — SDXL的微调条件的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节所述。可用于通过影响负面文本条件来模拟生成图像的美学评分。'
- en: '`clip_skip` (`Optional[int]`, defaults to `None`) — Number of layers to be
    skipped from CLIP while computing the prompt embeddings. A value of 1 means that
    the output of the pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`Optional[int]`, 默认为`None`) — 在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用预最终层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` or `tuple`'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`或`tuple`'
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` if `return_dict`
    is True, otherwise a `tuple. When returning a tuple, the first element is a list
    with the generated images.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`如果`return_dict`为True，否则为一个元组。当返回一个元组时，第一个元素是包含生成图像的列表。'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 调用管道进行生成时调用的函数。
- en: 'Examples:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE50]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: NeuronStableDiffusionXLInpaintPipeline
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronStableDiffusionXLInpaintPipeline
- en: '### `class optimum.neuron.NeuronStableDiffusionXLInpaintPipeline`'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.NeuronStableDiffusionXLInpaintPipeline`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L883)'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/modeling_diffusion.py#L883)'
- en: '[PRE51]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `__call__`'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_xl_inpaint.py#L80)'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/pipelines/diffusers/pipeline_stable_diffusion_xl_inpaint.py#L80)'
- en: '[PRE52]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
    instead.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`Optional[Union[str, List[str]]]`, 默认为`None`) — 用于引导图像生成的提示或提示。如果未定义，则必须传递`prompt_embeds`。'
- en: '`prompt_2` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The prompt
    or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined,
    `prompt` is used in both text-encoders'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_2` (`Optional[Union[str, List[str]]]`, 默认为`None`) — 要发送给`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，`prompt`将在两个文本编码器中使用'
- en: '`image` (`Optional["PipelineImageInput"]`, defaults to `None`) — `Image`, or
    tensor representing an image batch which will be inpainted, *i.e.* parts of the
    image will be masked out with `mask_image` and repainted according to `prompt`.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`Optional["PipelineImageInput"]`, 默认为`None`) — `Image`，或表示将被修复的图像批次的张量，即图像的部分将被`mask_image`遮盖并根据`prompt`重新绘制。'
- en: '`mask_image` (`Optional["PipelineImageInput"]`, defaults to `None`) — `Image`,
    or tensor representing an image batch, to mask `image`. White pixels in the mask
    will be repainted, while black pixels will be preserved. If `mask_image` is a
    PIL image, it will be converted to a single channel (luminance) before use. If
    it’s a tensor, it should contain one color channel (L) instead of 3, so the expected
    shape would be `(B, H, W, 1)`.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_image` (`Optional["PipelineImageInput"]`, defaults to `None`) — 用于遮罩`image`的`Image`或表示图像批次的张量。遮罩中的白色像素将被重新绘制，而黑色像素将被保留。如果`mask_image`是PIL图像，它将在使用之前转换为单通道（亮度）。如果它是张量，则应包含一个颜色通道（L）而不是3，因此预期形状将是`(B,
    H, W, 1)`。'
- en: '`padding_mask_crop` (`Optional[int]`, defaults to `None`) — The size of margin
    in the crop to be applied to the image and masking. If `None`, no crop is applied
    to image and mask_image. If `padding_mask_crop` is not `None`, it will first find
    a rectangular region with the same aspect ration of the image and contains all
    masked area, and then expand that area based on `padding_mask_crop`. The image
    and mask_image will then be cropped based on the expanded area before resizing
    to the original image size for inpainting. This is useful when the masked area
    is small while the image is large and contain information inreleant for inpainging,
    such as background.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_mask_crop` (`Optional[int]`, defaults to `None`) — 要应用于图像和遮罩的裁剪边距大小。如果为`None`，则不对图像和mask_image进行裁剪。如果`padding_mask_crop`不是`None`，它将首先找到一个具有与图像相同纵横比且包含所有遮罩区域的矩形区域，然后根据`padding_mask_crop`扩展该区域。然后将根据扩展的区域对图像和mask_image进行裁剪，然后将其调整为原始图像大小以进行修复。当遮罩区域较小而图像较大且包含与修复无关的信息（例如背景）时，这是有用的。'
- en: '`strength` (`float`, defaults to 0.9999) — Conceptually, indicates how much
    to transform the masked portion of the reference `image`. Must be between 0 and
    1\. `image` will be used as a starting point, adding more noise to it the larger
    the `strength`. The number of denoising steps depends on the amount of noise initially
    added. When `strength` is 1, added noise will be maximum and the denoising process
    will run for the full number of iterations specified in `num_inference_steps`.
    A value of 1, therefore, essentially ignores the masked portion of the reference
    `image`. Note that in the case of `denoising_start` being declared as an integer,
    the value of `strength` will be ignored.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength` (`float`, defaults to 0.9999) — 在概念上，指示要如何转换参考`image`的遮罩部分的程度。必须介于0和1之间。`image`将被用作起点，添加的噪声越大，`strength`越大。去噪步骤的数量取决于最初添加的噪声量。当`strength`为1时，添加的噪声将达到最大，并且去噪过程将运行指定的`num_inference_steps`的完整迭代次数。因此，值为1基本上忽略了参考`image`的遮罩部分。请注意，在将`denoising_start`声明为整数的情况下，将忽略`strength`的值。'
- en: '`num_inference_steps` (`int`, defaults to 50) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, defaults to 50) — 去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`timesteps` (`Optional[List[int]]`, defaults to `None`) — Custom timesteps
    to use for the denoising process with schedulers which support a `timesteps` argument
    in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps`
    is passed will be used. Must be in descending order.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps` (`Optional[List[int]]`, defaults to `None`) — 用于具有支持`timesteps`参数的调度程序的去噪过程的自定义时间步。如果未定义，则将使用传递`num_inference_steps`时的默认行为。必须按降序排列。'
- en: '`denoising_start` (`Optional[float]`, defaults to `None`) — When specified,
    indicates the fraction (between 0.0 and 1.0) of the total denoising process to
    be bypassed before it is initiated. Consequently, the initial part of the denoising
    process is skipped and it is assumed that the passed `image` is a partly denoised
    image. Note that when this is specified, strength will be ignored. The `denoising_start`
    parameter is particularly beneficial when this pipeline is integrated into a “Mixture
    of Denoisers” multi-pipeline setup, as detailed in [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output).'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`denoising_start` (`Optional[float]`, defaults to `None`) — 当指定时，指示在启动之前要绕过的总去噪过程的比例（介于0.0和1.0之间）。因此，将跳过去噪过程的初始部分，并假定传递的`image`是部分去噪的图像。请注意，当指定此参数时，将忽略强度。当此流水线集成到“去噪器混合”多流水线设置中时，`denoising_start`参数特别有益，如[`精炼图像输出`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)中详细说明。'
- en: '`denoising_end` (`Optional[float]`, defaults to `None`) — When specified, determines
    the fraction (between 0.0 and 1.0) of the total denoising process to be completed
    before it is intentionally prematurely terminated. As a result, the returned sample
    will still retain a substantial amount of noise (ca. final 20% of timesteps still
    needed) and should be denoised by a successor pipeline that has `denoising_start`
    set to 0.8 so that it only denoises the final 20% of the scheduler. The denoising_end
    parameter should ideally be utilized when this pipeline forms a part of a “Mixture
    of Denoisers” multi-pipeline setup, as elaborated in [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output).'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`denoising_end` (`Optional[float]`, defaults to `None`) — 当指定时，确定在有意提前终止之前完成的总去噪过程的比例（介于0.0和1.0之间）。因此，返回的样本仍将保留相当多的噪声（大约还需要最后20%的时间步），应由具有`denoising_start`设置为0.8的后续流水线进行去噪，以便仅对调度程序的最后20%进行去噪。当此流水线形成“去噪器混合”多流水线设置的一部分时，应理想地利用`denoising_end`参数，如[`精炼图像输出`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)中所述。'
- en: '`guidance_scale` (`float`, defaults to 7.5) — Guidance scale as defined in
    [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598). `guidance_scale`
    is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, defaults to 7.5) — 在[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`定义为[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。'
- en: '`negative_prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) —
    The prompt or prompts not to guide the image generation. If not defined, one has
    to pass `negative_prompt_embeds` instead. Ignored when not using guidance (i.e.,
    ignored if `guidance_scale` is less than `1`).'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`Optional[Union[str, List[str]]]`, defaults to `None`) —
    不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时将被忽略（即，如果`guidance_scale`小于`1`，则将被忽略）。'
- en: '`negative_prompt_2` (`Optional[Union[str, List[str]]]`, defaults to `None`)
    — The prompt or prompts not to guide the image generation to be sent to `tokenizer_2`
    and `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_2` (`Optional[Union[str, List[str]]]`, defaults to `None`)
    — 不指导图像生成的提示或提示，将发送到`tokenizer_2`和`text_encoder_2`。如果未定义，`negative_prompt`将在两个文本编码器中使用。'
- en: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, text embeddings will be generated from `prompt` input argument.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated negative text embeddings. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。'
- en: '`pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — Pre-generated pooled text embeddings. Can be used to easily tweak text inputs,
    *e.g.* prompt weighting. If not provided, pooled text embeddings will be generated
    from `prompt` input argument.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to `None`)
    — 预生成的汇集文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成汇集文本嵌入。'
- en: '`negative_pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to
    `None`) — Pre-generated negative pooled text embeddings. Can be used to easily
    tweak text inputs, *e.g.* prompt weighting. If not provided, pooled negative_prompt_embeds
    will be generated from `negative_prompt` input argument. ip_adapter_image — (`Optional[PipelineImageInput]`,
    defaults to `None`): Optional image input to work with IP Adapters.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`Optional[torch.FloatTensor]`, defaults to
    `None`) — 预生成的负汇集文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负汇集文本嵌入。ip_adapter_image
    — (`Optional[PipelineImageInput]`, defaults to `None`): 可选的图像输入以与IP适配器一起使用。'
- en: '`num_images_per_prompt` (`int`, defaults to 1) — The number of images to generate
    per prompt.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, defaults to 1) — 每个提示生成的图像数量。'
- en: '`eta` (`float`, defaults to 0.0) — Corresponds to parameter eta (η) in the
    DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to `schedulers.DDIMScheduler`, will be ignored for others.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, defaults to 0.0) — 对应于DDIM论文中参数eta (η)：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于`schedulers.DDIMScheduler`，对其他情况将被忽略。'
- en: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`Optional[Union[torch.Generator, List[torch.Generator]]]`, defaults
    to `None`) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`latents` (`Optional[torch.FloatTensor]`, defaults to `None`) — Pre-generated
    noisy latents, sampled from a Gaussian distribution, to be used as inputs for
    image generation. Can be used to tweak the same generation with different prompts.
    If not provided, a latents tensor will ge generated by sampling using the supplied
    random `generator`.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`Optional[torch.FloatTensor]`, defaults to `None`) — 预生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成一个潜变量张量。'
- en: '`output_type` (`Optional[str]`, defaults to `"pil"`) — The output format of
    the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`Optional[str]`, defaults to `"pil"`) — 生成图像的输出格式。选择[PIL](https://pillow.readthedocs.io/en/stable/)之间的格式：`PIL.Image.Image`或`np.array`。'
- en: '`return_dict` (`bool`, defaults to `True`) — Whether or not to return a `~pipelines.stable_diffusion.StableDiffusionPipelineOutput`
    instead of a plain tuple.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, defaults to `True`) — 是否返回`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`而不是普通元组。'
- en: '`cross_attention_kwargs` (`Optional[Dict[str, Any]]`, defaults to `None`) —
    A kwargs dictionary that if specified is passed along to the `AttentionProcessor`
    as defined under `self.processor` in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`Optional[Dict[str, Any]]`, defaults to `None`) —
    如果指定，将传递给`AttentionProcessor`的kwargs字典，如在[diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中所定义。'
- en: '`original_size` (`Tuple[int]`, defaults to (1024, 1024)) — If `original_size`
    is not the same as `target_size` the image will appear to be down- or upsampled.
    `original_size` defaults to `(height, width)` if not specified. Part of SDXL’s
    micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_size` (`Tuple[int]`, 默认值为(1024, 1024)) — 如果`original_size`与`target_size`不同，图像将呈现为缩小或放大。如果未指定，`original_size`默认为`(height,
    width)`。作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。'
- en: '`crops_coords_top_left` (`Tuple[int]`, defaults to (0, 0)) — `crops_coords_top_left`
    can be used to generate an image that appears to be “cropped” from the position
    `crops_coords_top_left` downwards. Favorable, well-centered images are usually
    achieved by setting `crops_coords_top_left` to (0, 0). Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_coords_top_left` (`Tuple[int]`, 默认值为(0, 0)) — `crops_coords_top_left`
    可用于生成一个看起来被从位置`crops_coords_top_left`向下“裁剪”的图像。通常通过将`crops_coords_top_left`设置为(0,
    0)来实现令人满意、居中的图像。作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。'
- en: '`target_size` (`Tuple[int]`, defaults to (1024, 1024)) — For most cases, `target_size`
    should be set to the desired height and width of the generated image. If not specified
    it will default to `(height, width)`. Part of SDXL’s micro-conditioning as explained
    in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_size` (`Tuple[int]`, 默认值为(1024, 1024)) — 在大多数情况下，`target_size`应设置为生成图像的期望高度和宽度。如果未指定，将默认为`(height,
    width)`。作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。'
- en: '`negative_original_size` (`Tuple[int]`, defaults to (1024, 1024)) — To negatively
    condition the generation process based on a specific image resolution. Part of
    SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_original_size` (`Tuple[int]`, 默认值为(1024, 1024)) — 基于特定图像分辨率负向调节生成过程。作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`negative_crops_coords_top_left` (`Tuple[int]`, defaults to (0, 0)) — To negatively
    condition the generation process based on a specific crop coordinates. Part of
    SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_crops_coords_top_left` (`Tuple[int]`, 默认值为(0, 0)) — 基于特定裁剪坐标来负向调节生成过程。作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`negative_target_size` (`Tuple[int]`, defaults to (1024, 1024)) — To negatively
    condition the generation process based on a target image resolution. It should
    be as same as the `target_size` for most cases. Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_target_size` (`Tuple[int]`, 默认值为(1024, 1024)) — 基于目标图像分辨率负向调节生成过程。在大多数情况下，应与`target_size`相同。作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。更多信息，请参考此问题线程：[https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208)。'
- en: '`aesthetic_score` (`float`, defaults to 6.0) — Used to simulate an aesthetic
    score of the generated image by influencing the positive text condition. Part
    of SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aesthetic_score` (`float`, 默认值为6.0）— 用于通过影响正文条件来模拟生成图像的审美评分。作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。'
- en: '`negative_aesthetic_score` (`float`, defaults to 2.5) — Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    Can be used to simulate an aesthetic score of the generated image by influencing
    the negative text condition.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_aesthetic_score` (`float`, 默认值为2.5) — 作为SDXL微调节的一部分，详见[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节。可用于通过影响负文本条件来模拟生成图像的审美评分。'
- en: '`clip_skip` (`Optional[int]`, defaults to `None`) — Number of layers to be
    skipped from CLIP while computing the prompt embeddings. A value of 1 means that
    the output of the pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`Optional[int]`, 默认值为`None`) — 在计算提示嵌入时，要从CLIP中跳过的层数。值为1表示将使用前一层的输出来计算提示嵌入。'
- en: '`callback_on_step_end` (`Optional[Callable[[int, int, Dict], None]]`, defaults
    to `None`) — A function that calls at the end of each denoising steps during the
    inference. The function is called with the following arguments: `callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs`
    will include a list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Optional[Callable[[int, int, Dict], None]]`, 默认值为`None`)
    — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline,
    step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List[str]`, defaults to [“latents”])
    — The list of tensor inputs for the `callback_on_step_end` function. The tensors
    specified in the list will be passed as `callback_kwargs` argument. You will only
    be able to include variables listed in the `._callback_tensor_inputs` attribute
    of your pipeline class.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List[str]`，默认为[“latents”]) — `callback_on_step_end`
    函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在管道类的 `._callback_tensor_inputs`
    属性中列出的变量。'
- en: Returns
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` or `tuple`'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` 或 `tuple`'
- en: '`diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` if `return_dict`
    is True, otherwise a `tuple.` tuple. When returning a tuple, the first element
    is a list with the generated images.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 True，则返回 `diffusers.pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`，否则返回一个
    `tuple.` 元组。当返回一个元组时，第一个元素是包含生成图像的列表。
- en: Function invoked when calling the pipeline for generation.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用管道生成时调用的函数。
- en: 'Examples:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE53]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
