- en: 'The Bellman Equation: simplify our value estimation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝尔曼方程：简化我们的值估计
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation](https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation](https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation **simplifies our state value or state-action value calculation.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程**简化了我们的状态值或状态-动作值计算。**
- en: '![Bellman equation](../Images/2bf6b05b2e64c9c62e78603164e99c30.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/2bf6b05b2e64c9c62e78603164e99c30.png)'
- en: With what we have learned so far, we know that if we calculate<math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​) (the
    value of a state), we need to calculate the return starting at that state and
    then follow the policy forever after. **(The policy we defined in the following
    example is a Greedy Policy; for simplification, we don’t discount the reward).**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们目前学到的知识，我们知道如果计算<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​)（状态的值），我们需要计算从该状态开始的回报，然后永远遵循策略。（我们在下面的示例中定义的策略是贪婪策略；为简化起见，我们不打折奖励）。
- en: 'So to calculate<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​),
    we need to calculate the sum of the expected rewards. Hence:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要计算<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​)，我们需要计算预期奖励的总和。因此：
- en: '![Bellman equation](../Images/f3af14f7f97044c041b7bab60378775a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/f3af14f7f97044c041b7bab60378775a.png)'
- en: 'To calculate the value of State 1: the sum of rewards if the agent started
    in that state and then followed the greedy policy (taking actions that leads to
    the best states values) for all the time steps.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算状态1的值：如果代理从该状态开始，然后按照贪婪策略（采取导致最佳状态值的动作）进行所有时间步骤的奖励总和。
- en: Then, to calculate the<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>V(St+1​),
    we need to calculate the return starting at that state<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，要计算<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>V(St+1​)，我们需要计算从该状态<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​开始的回报。
- en: '![Bellman equation](../Images/168b16071cc51b3d9fd633df82adf056.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/168b16071cc51b3d9fd633df82adf056.png)'
- en: 'To calculate the value of State 2: the sum of rewards **if the agent started
    in that state**, and then followed the **policy for all the time steps.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算状态2的值：如果代理从该状态开始，然后按照**所有时间步骤的策略**进行奖励的总和。
- en: So you may have noticed, we’re repeating the computation of the value of different
    states, which can be tedious if you need to do it for each state value or state-action
    value.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能已经注意到，我们在计算不同状态的值时重复了计算，如果你需要为每个状态值或状态-动作值都这样做，这可能会很繁琐。
- en: 'Instead of calculating the expected return for each state or each state-action
    pair, **we can use the Bellman equation.** (hint: if you know what Dynamic Programming
    is, this is very similar! if you don’t know what it is, no worries!)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不是为每个状态或每个状态-动作对计算预期回报，**我们可以使用贝尔曼方程。**（提示：如果你知道什么是动态规划，这非常相似！如果你不知道是什么，不用担心！）
- en: 'The Bellman equation is a recursive equation that works like this: instead
    of starting for each state from the beginning and calculating the return, we can
    consider the value of any state as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程是一个递归方程，工作原理如下：不是从每个状态的开始计算回报，而是将任何状态的值视为：
- en: '**The immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + the
    discounted value of the state that follows (<math><semantics><mrow><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+1})</annotation></semantics></math> gamma∗V(St+1​) ) .**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**即时奖励<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + 后续状态的折现值（<math><semantics><mrow><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+1})</annotation></semantics></math> gamma∗V(St+1​) )。**'
- en: '![Bellman equation](../Images/a9dac28cddf8b65584ecce42d2ee7bb0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/a9dac28cddf8b65584ecce42d2ee7bb0.png)'
- en: If we go back to our example, we can say that the value of State 1 is equal
    to the expected cumulative return if we start at that state.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到我们的例子，我们可以说状态1的值等于如果我们从该状态开始的预期累积回报。
- en: '![Bellman equation](../Images/f3af14f7f97044c041b7bab60378775a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/f3af14f7f97044c041b7bab60378775a.png)'
- en: 'To calculate the value of State 1: the sum of rewards **if the agent started
    in that state 1** and then followed the **policy for all the time steps.**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算状态1的值：如果代理从该状态1开始，然后按照**所有时间步骤的策略**进行奖励的总和。
- en: This is equivalent to <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t})</annotation></semantics></math>V(St​)
    = Immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + Discounted
    value of the next state <math><semantics><mrow><mi>γ</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\gamma
    * V(S_{t+1})</annotation></semantics></math>γ∗V(St+1​)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于 <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t})</annotation></semantics></math>V(St​)
    = 立即奖励 <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + 下一个状态的折现价值
    <math><semantics><mrow><mi>γ</mi><mo>∗</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\gamma
    * V(S_{t+1})</annotation></semantics></math>γ∗V(St+1​)
- en: '![Bellman equation](../Images/41ddd09d9f3e231dd44290e310a72771.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/41ddd09d9f3e231dd44290e310a72771.png)'
- en: For simplification, here we don’t discount so gamma = 1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为简化起见，这里我们不打折，所以γ = 1。
- en: In the interest of simplicity, here we don’t discount, so gamma = 1. But you’ll
    study an example with gamma = 0.99 in the Q-Learning section of this unit.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，这里我们不打折，所以γ = 1。但在本单元的Q学习部分，您将学习一个γ = 0.99的示例。
- en: The value of <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>
    V(St+1​) = Immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+2}</annotation></semantics></math>Rt+2​ + Discounted
    value of the next state (<math><semantics><mrow><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+2})</annotation></semantics></math>gamma∗V(St+2​) ).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>
    V(St+1​) = 立即奖励 <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+2}</annotation></semantics></math>Rt+2​ + 下一个状态的折现价值
    (<math><semantics><mrow><mi>γ</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+2})</annotation></semantics></math>gamma∗V(St+2​) ).
- en: And so on.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: To recap, the idea of the Bellman equation is that instead of calculating each
    value as the sum of the expected return, **which is a long process**, we calculate
    the value as **the sum of immediate reward + the discounted value of the state
    that follows.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，贝尔曼方程的思想是，**不是将每个值计算为预期回报的总和，这是一个漫长的过程**，而是将值计算为**立即奖励的总和 + 随后状态的折现价值**。
- en: Before going to the next section, think about the role of gamma in the Bellman
    equation. What happens if the value of gamma is very low (e.g. 0.1 or even 0)?
    What happens if the value is 1? What happens if the value is very high, such as
    a million?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，请考虑贝尔曼方程中γ的作用。如果γ的值非常低（例如0.1甚至为0），会发生什么？如果值为1会发生什么？如果值非常高，比如一百万会发生什么？
