- en: Glossary
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语表
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/glossary](https://huggingface.co/docs/transformers/v4.37.2/en/glossary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/glossary](https://huggingface.co/docs/transformers/v4.37.2/en/glossary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This glossary defines general machine learning and 🤗 Transformers terms to help
    you better understand the documentation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语表定义了一般的机器学习和🤗 Transformers术语，以帮助您更好地理解文档。
- en: A
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A
- en: attention mask
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力掩码
- en: The attention mask is an optional argument used when batching sequences together.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力掩码是一个可选参数，用于将序列批处理在一起时使用。
- en: '[https://www.youtube-nocookie.com/embed/M6adb1j2jPI](https://www.youtube-nocookie.com/embed/M6adb1j2jPI)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/M6adb1j2jPI](https://www.youtube-nocookie.com/embed/M6adb1j2jPI)'
- en: This argument indicates to the model which tokens should be attended to, and
    which should not.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个参数指示模型应该关注哪些令牌，哪些不应该。
- en: 'For example, consider these two sequences:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这两个序列：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The encoded versions have different lengths:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 编码版本的长度不同：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Therefore, we can’t put them together in the same tensor as-is. The first sequence
    needs to be padded up to the length of the second one, or the second one needs
    to be truncated down to the length of the first one.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不能将它们直接放在同一个张量中。第一个序列需要填充到第二个序列的长度，或者第二个序列需要截断到第一个序列的长度。
- en: 'In the first case, the list of IDs will be extended by the padding indices.
    We can pass a list to the tokenizer and ask it to pad like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，ID列表将通过填充索引进行扩展。我们可以将列表传递给分词器，并要求它这样填充：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can see that 0s have been added on the right of the first sentence to make
    it the same length as the second one:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在第一句的右侧添加了0，使其与第二句长度相同：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This can then be converted into a tensor in PyTorch or TensorFlow. The attention
    mask is a binary tensor indicating the position of the padded indices so that
    the model does not attend to them. For the [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer),
    `1` indicates a value that should be attended to, while `0` indicates a padded
    value. This attention mask is in the dictionary returned by the tokenizer under
    the key “attention_mask”:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以在PyTorch或TensorFlow中将其转换为张量。注意力掩码是一个二进制张量，指示填充索引的位置，以便模型不会关注它们。对于[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)，`1`表示应该关注的值，而`0`表示填充值。这个注意力掩码在令牌化器返回的字典中，键为“attention_mask”：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: autoencoding models
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自编码模型
- en: See [encoder models](#encoder-models) and [masked language modeling](#masked-language-modeling-mlm)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[编码器模型](#encoder-models)和[掩码语言建模](#masked-language-modeling-mlm)
- en: autoregressive models
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自回归模型
- en: See [causal language modeling](#causal-language-modeling) and [decoder models](#decoder-models)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[因果语言建模](#causal-language-modeling)和[解码器模型](#decoder-models)
- en: B
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B
- en: backbone
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 骨干
- en: The backbone is the network (embeddings and layers) that outputs the raw hidden
    states or features. It is usually connected to a [head](#head) which accepts the
    features as its input to make a prediction. For example, [ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)
    is a backbone without a specific head on top. Other models can also use `VitModel`
    as a backbone such as [DPT](model_doc/dpt).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 骨干是输出原始隐藏状态或特征的网络（嵌入和层）。通常与一个[头](#head)连接，该头接受特征作为其输入以进行预测。例如，[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)是一个没有特定头部的骨干。其他模型也可以使用`VitModel`作为骨干，如[DPT](model_doc/dpt)。
- en: C
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C
- en: causal language modeling
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果语言建模
- en: A pretraining task where the model reads the texts in order and has to predict
    the next word. It’s usually done by reading the whole sentence but using a mask
    inside the model to hide the future tokens at a certain timestep.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个预训练任务，其中模型按顺序阅读文本，并预测下一个单词。通常通过阅读整个句子来完成，但在模型内部使用掩码来隐藏某个时间步长的未来令牌。
- en: channel
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通道
- en: 'Color images are made up of some combination of values in three channels: red,
    green, and blue (RGB) and grayscale images only have one channel. In 🤗 Transformers,
    the channel can be the first or last dimension of an image’s tensor: [`n_channels`,
    `height`, `width`] or [`height`, `width`, `n_channels`].'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像由三个通道中的值的某种组合组成：红色、绿色和蓝色（RGB），而灰度图像只有一个通道。在🤗 Transformers中，通道可以是图像张量的第一个或最后一个维度：[`n_channels`，`height`，`width`]或[`height`，`width`，`n_channels`]。
- en: connectionist temporal classification (CTC)
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接主义时间分类（CTC）
- en: An algorithm which allows a model to learn without knowing exactly how the input
    and output are aligned; CTC calculates the distribution of all possible outputs
    for a given input and chooses the most likely output from it. CTC is commonly
    used in speech recognition tasks because speech doesn’t always cleanly align with
    the transcript for a variety of reasons such as a speaker’s different speech rates.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一种允许模型学习而不知道输入和输出如何对齐的算法；CTC计算给定输入的所有可能输出的分布，并从中选择最可能的输出。CTC通常用于语音识别任务，因为语音不总是与文本干净地对齐，原因有很多，比如说话者的不同语速。
- en: convolution
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积
- en: A type of layer in a neural network where the input matrix is multiplied element-wise
    by a smaller matrix (kernel or filter) and the values are summed up in a new matrix.
    This is known as a convolutional operation which is repeated over the entire input
    matrix. Each operation is applied to a different segment of the input matrix.
    Convolutional neural networks (CNNs) are commonly used in computer vision.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的一种层类型，其中输入矩阵与较小的矩阵（卷积核或滤波器）逐元素相乘，然后在新矩阵中求和。这被称为卷积操作，它在整个输入矩阵上重复。每个操作应用于输入矩阵的不同部分。卷积神经网络（CNNs）通常用于计算机视觉。
- en: D
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D
- en: DataParallel (DP)
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DataParallel（DP）
- en: Parallelism technique for training on multiple GPUs where the same setup is
    replicated multiple times, with each instance receiving a distinct data slice.
    The processing is done in parallel and all setups are synchronized at the end
    of each training step.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个GPU上进行训练的并行技术，其中相同的设置被复制多次，每个实例接收一个不同的数据切片。处理是并行进行的，并且所有设置在每个训练步骤结束时都是同步的。
- en: Learn more about how DataParallel works [here](perf_train_gpu_many#dataparallel-vs-distributeddataparallel).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关DataParallel如何工作的更多信息[在这里](perf_train_gpu_many#dataparallel-vs-distributeddataparallel)。
- en: decoder input IDs
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器输入ID
- en: This input is specific to encoder-decoder models, and contains the input IDs
    that will be fed to the decoder. These inputs should be used for sequence to sequence
    tasks, such as translation or summarization, and are usually built in a way specific
    to each model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输入是特定于编码器-解码器模型的，包含将馈送给解码器的输入ID。这些输入应该用于序列到序列任务，例如翻译或摘要，并且通常以每个模型特定的方式构建。
- en: Most encoder-decoder models (BART, T5) create their `decoder_input_ids` on their
    own from the `labels`. In such models, passing the `labels` is the preferred way
    to handle training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数编码器-解码器模型（BART，T5）会自行从`labels`创建它们的`decoder_input_ids`。在这种模型中，传递`labels`是处理训练的首选方式。
- en: Please check each model’s docs to see how they handle these input IDs for sequence
    to sequence training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看每个模型的文档，了解它们如何处理这些输入ID以进行序列到序列训练。
- en: decoder models
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器模型
- en: Also referred to as autoregressive models, decoder models involve a pretraining
    task (called causal language modeling) where the model reads the texts in order
    and has to predict the next word. It’s usually done by reading the whole sentence
    with a mask to hide future tokens at a certain timestep.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器模型，也称为自回归模型，涉及一个预训练任务（称为因果语言建模），其中模型按顺序阅读文本，并预测下一个单词。通常通过在某个时间步骤隐藏未来标记的掩码来读取整个句子来完成。
- en: '[https://www.youtube-nocookie.com/embed/d_ixlCubqQw](https://www.youtube-nocookie.com/embed/d_ixlCubqQw)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/d_ixlCubqQw](https://www.youtube-nocookie.com/embed/d_ixlCubqQw)'
- en: deep learning (DL)
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习（DL）
- en: Machine learning algorithms which uses neural networks with several layers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多层神经网络的机器学习算法。
- en: E
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E
- en: encoder models
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器模型
- en: Also known as autoencoding models, encoder models take an input (such as text
    or images) and transform them into a condensed numerical representation called
    an embedding. Oftentimes, encoder models are pretrained using techniques like
    [masked language modeling](#masked-language-modeling-mlm), which masks parts of
    the input sequence and forces the model to create more meaningful representations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为自编码模型，编码器模型接受输入（如文本或图像）并将它们转换为称为嵌入的简化数值表示。通常，编码器模型使用诸如[masked language modeling](#masked-language-modeling-mlm)之类的技术进行预训练，该技术对输入序列的部分进行掩码，并迫使模型创建更有意义的表示。
- en: '[https://www.youtube-nocookie.com/embed/H39Z_720T5s](https://www.youtube-nocookie.com/embed/H39Z_720T5s)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/H39Z_720T5s](https://www.youtube-nocookie.com/embed/H39Z_720T5s)'
- en: F
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F
- en: feature extraction
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征提取
- en: The process of selecting and transforming raw data into a set of features that
    are more informative and useful for machine learning algorithms. Some examples
    of feature extraction include transforming raw text into word embeddings and extracting
    important features such as edges or shapes from image/video data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据选择和转换为一组更具信息性和有用的特征的过程，以供机器学习算法使用。特征提取的一些示例包括将原始文本转换为词嵌入，从图像/视频数据中提取重要特征，如边缘或形状。
- en: feed forward chunking
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前馈分块
- en: In each residual attention block in transformers the self-attention layer is
    usually followed by 2 feed forward layers. The intermediate embedding size of
    the feed forward layers is often bigger than the hidden size of the model (e.g.,
    for `bert-base-uncased`).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformers中的每个残差注意力块中，自注意力层通常后面跟着2个前馈层。前馈层的中间嵌入大小通常大于模型的隐藏大小（例如，对于`bert-base-uncased`）。
- en: 'For an input of size `[batch_size, sequence_length]`, the memory required to
    store the intermediate feed forward embeddings `[batch_size, sequence_length,
    config.intermediate_size]` can account for a large fraction of the memory use.
    The authors of [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    noticed that since the computation is independent of the `sequence_length` dimension,
    it is mathematically equivalent to compute the output embeddings of both feed
    forward layers `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`
    individually and concat them afterward to `[batch_size, sequence_length, config.hidden_size]`
    with `n = sequence_length`, which trades increased computation time against reduced
    memory use, but yields a mathematically **equivalent** result.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '对于大小为`[batch_size, sequence_length]`的输入，存储中间前馈嵌入`[batch_size, sequence_length,
    config.intermediate_size]`所需的内存可能占据了大部分内存使用量。[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)的作者注意到，由于计算与`sequence_length`维度无关，数学上等价于分别计算两个前馈层的输出嵌入`[batch_size,
    config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`，然后将它们连接到`[batch_size,
    sequence_length, config.hidden_size]`，其中`n = sequence_length`，这会增加计算时间，减少内存使用，但产生数学上**等价**的结果。'
- en: For models employing the function [apply_chunking_to_forward()](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.apply_chunking_to_forward),
    the `chunk_size` defines the number of output embeddings that are computed in
    parallel and thus defines the trade-off between memory and time complexity. If
    `chunk_size` is set to 0, no feed forward chunking is done.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用函数[apply_chunking_to_forward()](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.apply_chunking_to_forward)的模型，`chunk_size`定义了并行计算的输出嵌入数量，从而定义了内存和时间复杂度之间的权衡。如果将`chunk_size`设置为0，则不会进行前馈分块。
- en: finetuned models
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调模型
- en: Finetuning is a form of transfer learning which involves taking a pretrained
    model, freezing its weights, and replacing the output layer with a newly added
    [model head](#head). The model head is trained on your target dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一种迁移学习形式，涉及采用预训练模型，冻结其权重，并用新添加的[model head](#head)替换输出层。模型头在目标数据集上进行训练。
- en: See the [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)
    tutorial for more details, and learn how to fine-tune models with 🤗 Transformers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)教程以获取更多详细信息，并了解如何使用🤗
    Transformers微调模型。
- en: H
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: H
- en: head
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 头
- en: 'The model head refers to the last layer of a neural network that accepts the
    raw hidden states and projects them onto a different dimension. There is a different
    model head for each task. For example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型头指的是神经网络的最后一层，它接受原始隐藏状态并将其投影到不同的维度。每个任务都有一个不同的模型头。例如：
- en: '[GPT2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification)
    is a sequence classification head - a linear layer - on top of the base [GPT2Model](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification)是一个序列分类头
    - 一个线性层 - 位于基础[GPT2Model](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model)之上。'
- en: '[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    is an image classification head - a linear layer on top of the final hidden state
    of the `CLS` token - on top of the base [ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)是一个图像分类头
    - 位于基础[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)的`CLS`令牌的最终隐藏状态之上的线性层。'
- en: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    is a language modeling head with [CTC](#connectionist-temporal-classification-(CTC))
    on top of the base [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)是一个带有[CTC](#connectionist-temporal-classification-(CTC))的语言建模头，位于基础[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)之上。'
- en: I
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我
- en: image patch
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像补丁
- en: Vision-based Transformers models split an image into smaller patches which are
    linearly embedded, and then passed as a sequence to the model. You can find the
    `patch_size` - or resolution - of the model in its configuration.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉的Transformer模型将图像分成较小的补丁，这些补丁被线性嵌入，然后作为序列传递给模型。您可以在配置中找到模型的`patch_size`
    - 或分辨率。
- en: inference
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理
- en: Inference is the process of evaluating a model on new data after training is
    complete. See the [Pipeline for inference](https://huggingface.co/docs/transformers/pipeline_tutorial)
    tutorial to learn how to perform inference with 🤗 Transformers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是在训练完成后对新数据评估模型的过程。请参阅[用于推理的Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial)教程，了解如何使用🤗
    Transformers执行推理。
- en: input IDs
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入ID
- en: The input ids are often the only required parameters to be passed to the model
    as input. They are token indices, numerical representations of tokens building
    the sequences that will be used as input by the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输入ID通常是传递给模型的唯一必需参数。它们是标记索引，是构建序列的标记的数值表示，这些序列将作为模型的输入使用。
- en: '[https://www.youtube-nocookie.com/embed/VFp38yj8h3A](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/VFp38yj8h3A](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)'
- en: 'Each tokenizer works differently but the underlying mechanism remains the same.
    Here’s an example using the BERT tokenizer, which is a [WordPiece](https://arxiv.org/pdf/1609.08144.pdf)
    tokenizer:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分词器的工作方式不同，但基本机制保持不变。以下是使用BERT分词器的示例，它是一个[WordPiece](https://arxiv.org/pdf/1609.08144.pdf)分词器：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The tokenizer takes care of splitting the sequence into tokens available in
    the tokenizer vocabulary.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器负责将序列拆分为分词器词汇表中可用的标记。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The tokens are either words or subwords. Here for instance, “VRAM” wasn’t in
    the model vocabulary, so it’s been split in “V”, “RA” and “M”. To indicate those
    tokens are not separate words but parts of the same word, a double-hash prefix
    is added for “RA” and “M”:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标记可以是单词或子词。例如，在这里，“VRAM”不在模型词汇表中，因此它被分割为“V”、“RA”和“M”。为了指示这些标记不是单独的单词而是同一个单词的部分，为“RA”和“M”添加了双哈希前缀：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These tokens can then be converted into IDs which are understandable by the
    model. This can be done by directly feeding the sentence to the tokenizer, which
    leverages the Rust implementation of [🤗 Tokenizers](https://github.com/huggingface/tokenizers)
    for peak performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这些标记可以转换为模型可理解的ID。这可以通过直接将句子提供给分词器来完成，分词器利用[Rust实现的🤗 Tokenizers](https://github.com/huggingface/tokenizers)以获得最佳性能。
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The tokenizer returns a dictionary with all the arguments necessary for its
    corresponding model to work properly. The token indices are under the key `input_ids`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器返回一个包含其对应模型正常工作所需的所有参数的字典。标记索引位于键`input_ids`下：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the tokenizer automatically adds “special tokens” (if the associated
    model relies on them) which are special IDs the model sometimes uses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分词器会自动添加“特殊标记”（如果相关模型依赖于它们），这些特殊标记是模型有时使用的特殊ID。
- en: If we decode the previous sequence of ids,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们解码先前的ID序列，
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: we will see
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: because this is the way a [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)
    is going to expect its inputs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是[BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)预期其输入的方式。
- en: L
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L
- en: labels
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签
- en: 'The labels are an optional argument which can be passed in order for the model
    to compute the loss itself. These labels should be the expected prediction of
    the model: it will use the standard loss in order to compute the loss between
    its predictions and the expected value (the label).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是一个可选参数，可以传递给模型以便计算损失。这些标签应该是模型的预期预测：它将使用标准损失来计算其预测值和预期值（标签）之间的损失。
- en: 'These labels are different according to the model head, for example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签根据模型头不同而不同，例如：
- en: For sequence classification models, ([BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForSequenceClassification)),
    the model expects a tensor of dimension `(batch_size)` with each value of the
    batch corresponding to the expected label of the entire sequence.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于序列分类模型（[BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForSequenceClassification)），模型期望一个维度为`(batch_size)`的张量，其中批次的每个值对应于整个序列的预期标签。
- en: For token classification models, ([BertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForTokenClassification)),
    the model expects a tensor of dimension `(batch_size, seq_length)` with each value
    corresponding to the expected label of each individual token.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于标记分类模型，（[BertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForTokenClassification)），模型期望一个维度为`(batch_size,
    seq_length)`的张量，每个值对应于每个单独标记的预期标签。
- en: 'For masked language modeling, ([BertForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForMaskedLM)),
    the model expects a tensor of dimension `(batch_size, seq_length)` with each value
    corresponding to the expected label of each individual token: the labels being
    the token ID for the masked token, and values to be ignored for the rest (usually
    -100).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于掩码语言建模，（[BertForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForMaskedLM)），模型期望一个维度为`(batch_size,
    seq_length)`的张量，每个值对应于每个单独标记的预期标签：标签是被掩码标记的标记ID，其余标记的值将被忽略（通常为-100）。
- en: For sequence to sequence tasks, ([BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration),
    [MBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration)),
    the model expects a tensor of dimension `(batch_size, tgt_seq_length)` with each
    value corresponding to the target sequences associated with each input sequence.
    During training, both BART and T5 will make the appropriate `decoder_input_ids`
    and decoder attention masks internally. They usually do not need to be supplied.
    This does not apply to models leveraging the Encoder-Decoder framework.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于序列到序列的任务，（[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration),
    [MBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration)），模型期望一个维度为`(batch_size,
    tgt_seq_length)`的张量，每个值对应于与每个输入序列相关联的目标序列。在训练期间，BART和T5都会在内部生成适当的`decoder_input_ids`和解码器注意力掩码。通常不需要提供它们。这不适用于利用编码器-解码器框架的模型。
- en: For image classification models, ([ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)),
    the model expects a tensor of dimension `(batch_size)` with each value of the
    batch corresponding to the expected label of each individual image.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于图像分类模型，（[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)），模型期望一个维度为`(batch_size)`的张量，批次中的每个值对应于每个单独图像的预期标签。
- en: For semantic segmentation models, ([SegformerForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation)),
    the model expects a tensor of dimension `(batch_size, height, width)` with each
    value of the batch corresponding to the expected label of each individual pixel.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于语义分割模型，（[SegformerForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation)），模型期望一个维度为`(batch_size,
    height, width)`的张量，批次中的每个值对应于每个单独像素的预期标签。
- en: For object detection models, ([DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)),
    the model expects a list of dictionaries with a `class_labels` and `boxes` key
    where each value of the batch corresponds to the expected label and number of
    bounding boxes of each individual image.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于目标检测模型，（[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)），模型期望一个带有`class_labels`和`boxes`键的字典列表，批次中的每个值对应于每个单独图像的预期标签和边界框数量。
- en: For automatic speech recognition models, ([Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)),
    the model expects a tensor of dimension `(batch_size, target_length)` with each
    value corresponding to the expected label of each individual token.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于自动语音识别模型，（[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)），模型期望一个维度为`(batch_size,
    target_length)`的张量，每个值对应于每个单独标记的预期标签。
- en: Each model’s labels may be different, so be sure to always check the documentation
    of each model for more information about their specific labels!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的标签可能不同，因此请务必始终查看每个模型的文档以获取有关其特定标签的更多信息！
- en: The base models ([BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel))
    do not accept labels, as these are the base transformer models, simply outputting
    features.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型（[BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)）不接受标签，因为它们是基础变压器模型，只输出特征。
- en: large language models (LLM)
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）
- en: A generic term that refers to transformer language models (GPT-3, BLOOM, OPT)
    that were trained on a large quantity of data. These models also tend to have
    a large number of learnable parameters (e.g. 175 billion for GPT-3).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个泛指，指的是在大量数据上训练的变压器语言模型（GPT-3、BLOOM、OPT）。这些模型通常也具有大量可学习参数（例如GPT-3的1750亿）。
- en: M
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: M
- en: masked language modeling (MLM)
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掩码语言建模（MLM）
- en: A pretraining task where the model sees a corrupted version of the texts, usually
    done by masking some tokens randomly, and has to predict the original text.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型看到文本的损坏版本，通常是通过随机屏蔽一些标记来完成，并且必须预测原始文本的预训练任务。
- en: multimodal
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态
- en: A task that combines texts with another kind of inputs (for instance images).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本与其他类型的输入（例如图像）结合的任务。
- en: N
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N
- en: Natural language generation (NLG)
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）
- en: All tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/),
    translation).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与生成文本相关的任务（例如，[使用Transformer写作](https://transformer.huggingface.co/)，翻译）。
- en: Natural language processing (NLP)
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）
- en: A generic way to say “deal with texts”.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的说法是“处理文本”。
- en: Natural language understanding (NLU)
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言理解（NLU）
- en: All tasks related to understanding what is in a text (for instance classifying
    the whole text, individual words).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与理解文本内容相关的任务（例如对整个文本进行分类，对单词进行分类）。
- en: P
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: P
- en: pipeline
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道
- en: A pipeline in 🤗 Transformers is an abstraction referring to a series of steps
    that are executed in a specific order to preprocess and transform data and return
    a prediction from a model. Some example stages found in a pipeline might be data
    preprocessing, feature extraction, and normalization.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在🤗 Transformers中，管道是一个抽象，指的是按特定顺序执行的一系列步骤，用于预处理和转换数据，并从模型返回预测。管道中可能包含的一些示例阶段可能是数据预处理、特征提取和归一化。
- en: For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅[用于推断的管道](https://huggingface.co/docs/transformers/pipeline_tutorial)。
- en: PipelineParallel (PP)
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PipelineParallel（PP）
- en: Parallelism technique in which the model is split up vertically (layer-level)
    across multiple GPUs, so that only one or several layers of the model are placed
    on a single GPU. Each GPU processes in parallel different stages of the pipeline
    and working on a small chunk of the batch. Learn more about how PipelineParallel
    works [here](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一种并行技术，其中模型在多个GPU上垂直分割（层级），以便将模型的一个或多个层放置在单个GPU上。每个GPU并行处理管道的不同阶段，并处理一小批次的数据。了解有关PipelineParallel如何工作的更多信息，请查看[这里](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism)。
- en: pixel values
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 像素值
- en: A tensor of the numerical representations of an image that is passed to a model.
    The pixel values have a shape of [`batch_size`, `num_channels`, `height`, `width`],
    and are generated from an image processor.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给模型的图像的数值表示的张量。像素值的形状为[`batch_size`, `num_channels`, `height`, `width`]，并且是从图像处理器生成的。
- en: pooling
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化
- en: An operation that reduces a matrix into a smaller matrix, either by taking the
    maximum or average of the pooled dimension(s). Pooling layers are commonly found
    between convolutional layers to downsample the feature representation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵缩小为较小矩阵的操作，可以通过取池化维度的最大值或平均值来实现。池化层通常位于卷积层之间，用于降采样特征表示。
- en: position IDs
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置ID
- en: Contrary to RNNs that have the position of each token embedded within them,
    transformers are unaware of the position of each token. Therefore, the position
    IDs (`position_ids`) are used by the model to identify each token’s position in
    the list of tokens.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与RNN不同，RNN的每个标记的位置都嵌入在其中，transformers不知道每个标记的位置。因此，模型使用位置ID（`position_ids`）来识别列表中每个标记的位置。
- en: They are an optional parameter. If no `position_ids` are passed to the model,
    the IDs are automatically created as absolute positional embeddings.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是一个可选参数。如果没有将`position_ids`传递给模型，那么这些ID将自动创建为绝对位置嵌入。
- en: Absolute positional embeddings are selected in the range `[0, config.max_position_embeddings
    - 1]`. Some models use other types of positional embeddings, such as sinusoidal
    position embeddings or relative position embeddings.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对位置嵌入被选择在范围`[0, config.max_position_embeddings - 1]`内。一些模型使用其他类型的位置嵌入，如正弦位置嵌入或相对位置嵌入。
- en: preprocessing
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: The task of preparing raw data into a format that can be easily consumed by
    machine learning models. For example, text is typically preprocessed by tokenization.
    To gain a better idea of what preprocessing looks like for other input types,
    check out the [Preprocess](https://huggingface.co/docs/transformers/preprocessing)
    tutorial.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据准备成机器学习模型可以轻松消化的格式的任务。例如，文本通常通过标记化进行预处理。要了解其他输入类型的预处理是什么样子，可以查看[预处理](https://huggingface.co/docs/transformers/preprocessing)教程。
- en: pretrained model
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练模型
- en: A model that has been pretrained on some data (for instance all of Wikipedia).
    Pretraining methods involve a self-supervised objective, which can be reading
    the text and trying to predict the next word (see [causal language modeling](#causal-language-modeling))
    or masking some words and trying to predict them (see [masked language modeling](#masked-language-modeling-mlm)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在某些数据上进行了预训练的模型（例如整个维基百科）。预训练方法涉及自监督目标，可以是阅读文本并尝试预测下一个单词（参见[因果语言建模](#causal-language-modeling)），或者遮蔽一些单词并尝试预测它们（参见[遮蔽语言建模](#masked-language-modeling-mlm)）。
- en: Speech and vision models have their own pretraining objectives. For example,
    Wav2Vec2 is a speech model pretrained on a contrastive task which requires the
    model to identify the “true” speech representation from a set of “false” speech
    representations. On the other hand, BEiT is a vision model pretrained on a masked
    image modeling task which masks some of the image patches and requires the model
    to predict the masked patches (similar to the masked language modeling objective).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 语音和视觉模型有自己的预训练目标。例如，Wav2Vec2是一个语音模型，它在对比任务上进行了预训练，要求模型从一组“假”语音表示中识别“真实”语音表示。另一方面，BEiT是一个视觉模型，它在一个遮蔽图像建模任务上进行了预训练，遮蔽了一些图像块，并要求模型预测被遮蔽的块（类似于遮蔽语言建模目标）。
- en: R
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R
- en: recurrent neural network (RNN)
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）
- en: A type of model that uses a loop over a layer to process texts.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一种使用循环处理文本的模型类型。
- en: representation learning
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示学习
- en: A subfield of machine learning which focuses on learning meaningful representations
    of raw data. Some examples of representation learning techniques include word
    embeddings, autoencoders, and Generative Adversarial Networks (GANs).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一种机器学习的子领域，专注于学习原始数据的有意义表示。一些表示学习技术的例子包括词嵌入、自编码器和生成对抗网络（GANs）。
- en: S
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: S
- en: sampling rate
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 采样率
- en: A measurement in hertz of the number of samples (the audio signal) taken per
    second. The sampling rate is a result of discretizing a continuous signal such
    as speech.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒采样的样本数（音频信号）。采样率是将连续信号（如语音）离散化的结果。
- en: self-attention
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力
- en: Each element of the input finds out which other elements of the input they should
    attend to.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的每个元素找出它们应该关注的其他输入元素。
- en: self-supervised learning
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自监督学习
- en: A category of machine learning techniques in which a model creates its own learning
    objective from unlabeled data. It differs from [unsupervised learning](#unsupervised-learning)
    and [supervised learning](#supervised-learning) in that the learning process is
    supervised, but not explicitly from the user.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一类机器学习技术，其中模型从未标记数据中创建自己的学习目标。它与[无监督学习](#unsupervised-learning)和[监督学习](#supervised-learning)不同，学习过程是受监督的，但不是明确来自用户。
- en: One example of self-supervised learning is [masked language modeling](#masked-language-modeling-mlm),
    where a model is passed sentences with a proportion of its tokens removed and
    learns to predict the missing tokens.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习的一个例子是[掩码语言建模](#masked-language-modeling-mlm)，其中模型传递带有一定比例标记的句子，并学习预测缺失的标记。
- en: semi-supervised learning
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半监督学习
- en: A broad category of machine learning training techniques that leverages a small
    amount of labeled data with a larger quantity of unlabeled data to improve the
    accuracy of a model, unlike [supervised learning](#supervised-learning) and [unsupervised
    learning](#unsupervised-learning).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一种广泛的机器学习训练技术类别，利用少量标记数据和大量未标记数据来提高模型的准确性，与[监督学习](#supervised-learning)和[无监督学习](#unsupervised-learning)不同。
- en: An example of a semi-supervised learning approach is “self-training”, in which
    a model is trained on labeled data, and then used to make predictions on the unlabeled
    data. The portion of the unlabeled data that the model predicts with the most
    confidence gets added to the labeled dataset and used to retrain the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习方法的一个例子是“自训练”，其中模型在标记数据上进行训练，然后用于对未标记数据进行预测。模型以最大置信度预测的未标记数据部分被添加到标记数据集中，并用于重新训练模型。
- en: sequence-to-sequence (seq2seq)
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列到序列（seq2seq）
- en: Models that generate a new sequence from an input, like translation models,
    or summarization models (such as [Bart](model_doc/bart) or [T5](model_doc/t5)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入生成新序列的模型，比如翻译模型或总结模型（比如[Bart](model_doc/bart)或[T5](model_doc/t5)）。
- en: Sharded DDP
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分片DDP
- en: Another name for the foundational [ZeRO](#zero-redundancy-optimizer--zero-)
    concept as used by various other implementations of ZeRO.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 作为ZeRO概念的另一个名称，被各种其他ZeRO实现所使用。
- en: stride
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步幅
- en: In [convolution](#convolution) or [pooling](#pooling), the stride refers to
    the distance the kernel is moved over a matrix. A stride of 1 means the kernel
    is moved one pixel over at a time, and a stride of 2 means the kernel is moved
    two pixels over at a time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在[卷积](#convolution)或[池化](#pooling)中，步幅指的是核在矩阵上移动的距离。步幅为1表示核每次移动一个像素，步幅为2表示核每次移动两个像素。
- en: supervised learning
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: A form of model training that directly uses labeled data to correct and instruct
    model performance. Data is fed into the model being trained, and its predictions
    are compared to the known labels. The model updates its weights based on how incorrect
    its predictions were, and the process is repeated to optimize model performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接使用标记数据来纠正和指导模型性能的模型训练形式。数据被馈送到正在训练的模型中，其预测与已知标签进行比较。模型根据其预测的不正确程度更新其权重，并重复该过程以优化模型性能。
- en: T
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T
- en: Tensor Parallelism (TP)
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量并行性（TP）
- en: Parallelism technique for training on multiple GPUs in which each tensor is
    split up into multiple chunks, so instead of having the whole tensor reside on
    a single GPU, each shard of the tensor resides on its designated GPU. Shards gets
    processed separately and in parallel on different GPUs and the results are synced
    at the end of the processing step. This is what is sometimes called horizontal
    parallelism, as the splitting happens on horizontal level. Learn more about Tensor
    Parallelism [here](perf_train_gpu_many#tensor-parallelism).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个GPU上进行训练的并行技术，其中每个张量被分割成多个块，因此每个张量的碎片都驻留在其指定的GPU上，而不是整个张量驻留在单个GPU上。碎片在不同GPU上分别并行处理，并在处理步骤结束时进行同步。这有时被称为水平并行，因为分割发生在水平级别。在这里了解更多关于张量并行性的信息。
- en: token
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记
- en: A part of a sentence, usually a word, but can also be a subword (non-common
    words are often split in subwords) or a punctuation symbol.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 句子的一部分，通常是一个词，但也可以是一个子词（不常见的词通常被分割为子词）或标点符号。
- en: token Type IDs
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记类型ID
- en: Some models’ purpose is to do classification on pairs of sentences or question
    answering.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型的目的是对句子对或问题回答进行分类。
- en: '[https://www.youtube-nocookie.com/embed/0u3ioSwev3s](https://www.youtube-nocookie.com/embed/0u3ioSwev3s)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/0u3ioSwev3s](https://www.youtube-nocookie.com/embed/0u3ioSwev3s)'
- en: 'These require two different sequences to be joined in a single “input_ids”
    entry, which usually is performed with the help of special tokens, such as the
    classifier (`[CLS]`) and separator (`[SEP]`) tokens. For example, the BERT model
    builds its two sequence input as such:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些需要将两个不同序列连接在一个“input_ids”条目中，通常使用特殊标记的帮助来执行，例如分类器（`[CLS]`）和分隔符（`[SEP]`）标记。例如，BERT模型构建其两个序列输入如下：
- en: '[PRE12]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can use our tokenizer to automatically generate such a sentence by passing
    the two sequences to `tokenizer` as two arguments (and not a list, like before)
    like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们的分词器通过将两个序列作为两个参数（而不是像以前那样作为列表）传递给`tokenizer`来自动生成这样一个句子，就像这样：
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'which will return:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回：
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is enough for some models to understand where one sequence ends and where
    another begins. However, other models, such as BERT, also deploy token type IDs
    (also called segment IDs). They are represented as a binary mask identifying the
    two types of sequence in the model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于一些模型来说足够了解一个序列的结束和另一个序列的开始。然而，其他模型，如BERT，还部署了标记类型ID（也称为段ID）。它们表示模型中两种序列的二进制掩码。
- en: 'The tokenizer returns this mask as the “token_type_ids” entry:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器将此掩码返回为“token_type_ids”条目：
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first sequence, the “context” used for the question, has all its tokens
    represented by a `0`, whereas the second sequence, corresponding to the “question”,
    has all its tokens represented by a `1`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个序列，用于问题的“上下文”，其所有标记都表示为`0`，而第二个序列，对应于“问题”，其所有标记都表示为`1`。
- en: Some models, like [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    use an additional token represented by a `2`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型，比如[XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)使用一个额外的标记，表示为`2`。
- en: transfer learning
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习
- en: A technique that involves taking a pretrained model and adapting it to a dataset
    specific to your task. Instead of training a model from scratch, you can leverage
    knowledge obtained from an existing model as a starting point. This speeds up
    the learning process and reduces the amount of training data needed.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一种技术，涉及采用预训练模型并将其调整为特定于您任务的数据集。您可以利用从现有模型获得的知识作为起点，而不是从头开始训练模型。这加快了学习过程并减少了所需的训练数据量。
- en: transformer
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变压器
- en: Self-attention based deep learning model architecture.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 基于自注意力的深度学习模型架构。
- en: U
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U
- en: unsupervised learning
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: A form of model training in which data provided to the model is not labeled.
    Unsupervised learning techniques leverage statistical information of the data
    distribution to find patterns useful for the task at hand.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一种模型训练形式，其中提供给模型的数据没有标记。无监督学习技术利用数据分布的统计信息来找到对当前任务有用的模式。
- en: Z
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Z
- en: Zero Redundancy Optimizer (ZeRO)
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零冗余优化器（ZeRO）
- en: Parallelism technique which performs sharding of the tensors somewhat similar
    to [TensorParallel](#tensor-parallelism-tp), except the whole tensor gets reconstructed
    in time for a forward or backward computation, therefore the model doesn’t need
    to be modified. This method also supports various offloading techniques to compensate
    for limited GPU memory. Learn more about ZeRO [here](perf_train_gpu_many#zero-data-parallelism).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一种并行技术，类似于[TensorParallel](#tensor-parallelism-tp)对张量进行分片，除了整个张量在前向或后向计算时会重新构建，因此模型不需要被修改。这种方法还支持各种卸载技术，以弥补有限的GPU内存。在这里了解更多关于ZeRO的信息(perf_train_gpu_many#zero-data-parallelism)。
