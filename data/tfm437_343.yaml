- en: DePlot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DePlot
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deplot](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deplot)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deplot](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deplot)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'DePlot was proposed in the paper [DePlot: One-shot visual language reasoning
    by plot-to-table translation](https://arxiv.org/abs/2212.10505) from Fangyu Liu,
    Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton
    Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'DePlot是由Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene,
    Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun在论文[DePlot:
    One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505)中提出的。'
- en: 'The abstract of the paper states the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Visual language such as charts and plots is ubiquitous in the human world.
    Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art
    (SOTA) models require at least tens of thousands of training examples and their
    reasoning capabilities are still much limited, especially on complex human-written
    queries. This paper presents the first one-shot solution to visual language reasoning.
    We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text
    translation, and (2) reasoning over the translated text. The key in this method
    is a modality conversion module, named as DePlot, which translates the image of
    a plot or chart to a linearized table. The output of DePlot can then be directly
    used to prompt a pretrained large language model (LLM), exploiting the few-shot
    reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table
    task by establishing unified task formats and metrics, and train DePlot end-to-end
    on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
    fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM
    with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA
    on human-written queries from the task of chart QA.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*图表等视觉语言在人类世界中无处不在。理解图表和图表需要强大的推理能力。先前的最先进（SOTA）模型至少需要数万个训练示例，它们的推理能力仍然非常有限，特别是对于复杂的人类编写的查询。本文提出了视觉语言推理的第一个一次性解决方案。我们将视觉语言推理的挑战分解为两个步骤：（1）图表到文本的翻译，以及（2）对翻译文本进行推理。该方法的关键是一个名为DePlot的模态转换模块，它将图表或图表的图像转换为线性化表格。然后可以直接使用DePlot的输出来提示预训练的大型语言模型（LLM），利用LLM的少量推理能力。为了获得DePlot，我们通过建立统一的任务格式和度量标准，对绘图到表格任务进行了标准化，并在此任务上端到端地训练DePlot。然后可以将DePlot与LLM一起以即插即用的方式使用。与在超过28k数据点上微调的SOTA模型相比，DePlot+LLM仅通过一次提示就实现了对人类编写查询的图表QA任务的微调SOTA的24.0%的改进。*'
- en: DePlot is a model that is trained using `Pix2Struct` architecture. You can find
    more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).
    DePlot is a Visual Question Answering subset of `Pix2Struct` architecture. It
    renders the input question on the image and predicts the answer.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: DePlot是使用`Pix2Struct`架构训练的模型。您可以在[Pix2Struct文档](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct)中找到有关`Pix2Struct`的更多信息。DePlot是`Pix2Struct`架构的视觉问答子集。它在图像上呈现输入问题并预测答案。
- en: Usage example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示例
- en: 'Currently one checkpoint is available for DePlot:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前DePlot有一个检查点可用：
- en: '`google/deplot`: DePlot fine-tuned on ChartQA dataset'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`google/deplot`：在ChartQA数据集上微调的DePlot'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Fine-tuning
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: 'To fine-tune DePlot, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb).
    For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor
    and cosine learning rate scheduler leads to faster convergence:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要微调DePlot，请参考pix2struct的[fine-tuning笔记本](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)。对于`Pix2Struct`模型，我们发现使用Adafactor和余弦学习率调度程序对模型进行微调可以实现更快的收敛：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DePlot is a model trained using `Pix2Struct` architecture. For API reference,
    see [`Pix2Struct` documentation](pix2struct).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DePlot是使用`Pix2Struct`架构训练的模型。有关API参考，请参阅[`Pix2Struct`文档](pix2struct)。
