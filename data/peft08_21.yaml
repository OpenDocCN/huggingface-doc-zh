- en: DeepSpeed
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepSpeed
- en: 'Original text: [https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and
    scale for distributed training of large models with billions of parameters. At
    its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states
    (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes.
    This drastically reduces memory usage, allowing you to scale your training to
    billion parameter models. To unlock even more memory efficiency, ZeRO-Offload
    reduces GPU compute and memory by leveraging CPU resources during optimization.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[DeepSpeed](https://www.deepspeed.ai/) æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§å‹æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒçš„é€Ÿåº¦å’Œè§„æ¨¡è€Œè®¾è®¡çš„åº“ï¼Œå…·æœ‰æ•°åäº¿å‚æ•°ã€‚å…¶æ ¸å¿ƒæ˜¯
    Zero Redundancy Optimizer (ZeRO)ï¼Œå®ƒå°†ä¼˜åŒ–å™¨çŠ¶æ€ (ZeRO-1)ã€æ¢¯åº¦ (ZeRO-2) å’Œå‚æ•° (ZeRO-3) åˆ†ç‰‡åˆ°æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¸­ã€‚è¿™å¤§å¤§å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼Œä½¿æ‚¨å¯ä»¥å°†è®­ç»ƒæ‰©å±•åˆ°æ•°åäº¿å‚æ•°æ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å†…å­˜æ•ˆç‡ï¼ŒZeRO-Offload
    é€šè¿‡åˆ©ç”¨ CPU èµ„æºåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡å°‘ GPU è®¡ç®—å’Œå†…å­˜ã€‚'
- en: Both of these features are supported in ğŸ¤— Accelerate, and you can use them with
    ğŸ¤— PEFT. This guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py).
    Youâ€™ll configure the script to train a large model for conditional generation
    with ZeRO-3 and ZeRO-Offload.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªåŠŸèƒ½éƒ½å—åˆ° ğŸ¤— Accelerate çš„æ”¯æŒï¼Œæ‚¨å¯ä»¥åœ¨ ğŸ¤— PEFT ä¸­ä½¿ç”¨å®ƒä»¬ã€‚æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å­¦ä¹ å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ DeepSpeed [è®­ç»ƒè„šæœ¬](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)ã€‚æ‚¨å°†é…ç½®è„šæœ¬ä»¥ä½¿ç”¨
    ZeRO-3 å’Œ ZeRO-Offload è®­ç»ƒå¤§å‹æ¡ä»¶ç”Ÿæˆæ¨¡å‹ã€‚
- en: ğŸ’¡ To help you get started, check out our example training scripts for [causal
    language modeling](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py)
    and [conditional generation](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py).
    You can adapt these scripts for your own applications or even use them out of
    the box if your task is similar to the one in the scripts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ç¤ºä¾‹è®­ç»ƒè„šæœ¬ï¼Œç”¨äº[å› æœè¯­è¨€å»ºæ¨¡](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py)å’Œ[æ¡ä»¶ç”Ÿæˆ](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)ã€‚æ‚¨å¯ä»¥ä¸ºè‡ªå·±çš„åº”ç”¨ç¨‹åºè°ƒæ•´è¿™äº›è„šæœ¬ï¼Œç”šè‡³åœ¨æ‚¨çš„ä»»åŠ¡ç±»ä¼¼äºè„šæœ¬ä¸­çš„ä»»åŠ¡æ—¶ç›´æ¥ä½¿ç”¨å®ƒä»¬ã€‚
- en: Configuration
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®
- en: Start by running the following command to [create a DeepSpeed configuration
    file](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script)
    with ğŸ¤— Accelerate. The `--config_file` flag allows you to save the configuration
    file to a specific location, otherwise it is saved as a `default_config.yaml`
    file in the ğŸ¤— Accelerate cache.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥[åˆ›å»ºä¸€ä¸ª DeepSpeed é…ç½®æ–‡ä»¶](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script)ä¸
    ğŸ¤— Accelerateã€‚`--config_file` æ ‡å¿—å…è®¸æ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç‰¹å®šä½ç½®ï¼Œå¦åˆ™å®ƒå°†ä¿å­˜ä¸º `default_config.yaml` æ–‡ä»¶åœ¨
    ğŸ¤— Accelerate ç¼“å­˜ä¸­ã€‚
- en: The configuration file is used to set the default options when you launch the
    training script.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®æ–‡ä»¶ç”¨äºåœ¨å¯åŠ¨è®­ç»ƒè„šæœ¬æ—¶è®¾ç½®é»˜è®¤é€‰é¡¹ã€‚
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Youâ€™ll be asked a few questions about your setup, and configure the following
    arguments. In this example, youâ€™ll use ZeRO-3 and ZeRO-Offload so make sure you
    pick those options.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†è¢«é—®åŠæœ‰å…³æ‚¨çš„è®¾ç½®çš„å‡ ä¸ªé—®é¢˜ï¼Œå¹¶é…ç½®ä»¥ä¸‹å‚æ•°ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ ZeRO-3 å’Œ ZeRO-Offloadï¼Œè¯·ç¡®ä¿é€‰æ‹©è¿™äº›é€‰é¡¹ã€‚
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: An example [configuration file](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/accelerate_ds_zero3_cpu_offload_config.yaml)
    might look like the following. The most important thing to notice is that `zero_stage`
    is set to `3`, and `offload_optimizer_device` and `offload_param_device` are set
    to the `cpu`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[é…ç½®æ–‡ä»¶](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/accelerate_ds_zero3_cpu_offload_config.yaml)å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚æœ€é‡è¦çš„æ˜¯è¦æ³¨æ„
    `zero_stage` è®¾ç½®ä¸º `3`ï¼Œ`offload_optimizer_device` å’Œ `offload_param_device` è®¾ç½®ä¸º `cpu`ã€‚
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The important parts
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡è¦éƒ¨åˆ†
- en: Letâ€™s dive a little deeper into the script so you can see whatâ€™s going on, and
    understand how it works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥äº†è§£è„šæœ¬ï¼Œä»¥ä¾¿æ‚¨äº†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œå¹¶ç†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: Within the [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103)
    function, the script creates an [Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    class to initialize all the necessary requirements for distributed training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103)
    å‡½æ•°ä¸­ï¼Œè„šæœ¬åˆ›å»ºäº†ä¸€ä¸ª [Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    ç±»æ¥åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰å¿…è¦æ¡ä»¶ã€‚
- en: ğŸ’¡ Feel free to change the model and dataset inside the `main` function. If your
    dataset format is different from the one in the script, you may also need to write
    your own preprocessing function.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ éšæ„æ›´æ”¹ `main` å‡½æ•°ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†æ ¼å¼ä¸è„šæœ¬ä¸­çš„ä¸åŒï¼Œæ‚¨å¯èƒ½è¿˜éœ€è¦ç¼–å†™è‡ªå·±çš„é¢„å¤„ç†å‡½æ•°ã€‚
- en: The script also creates a configuration for the ğŸ¤— PEFT method youâ€™re using,
    which in this case, is LoRA. The [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    specifies the task type and important parameters such as the dimension of the
    low-rank matrices, the matrices scaling factor, and the dropout probability of
    the LoRA layers. If you want to use a different ğŸ¤— PEFT method, make sure you replace
    `LoraConfig` with the appropriate [class](../package_reference/tuners).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬è¿˜ä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„ ğŸ¤— PEFT æ–¹æ³•åˆ›å»ºé…ç½®ï¼Œæœ¬ä¾‹ä¸­ä¸º LoRAã€‚[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    æŒ‡å®šäº†ä»»åŠ¡ç±»å‹å’Œé‡è¦å‚æ•°ï¼Œå¦‚ä½ç§©çŸ©é˜µçš„ç»´åº¦ã€çŸ©é˜µç¼©æ”¾å› å­ä»¥åŠ LoRA å±‚çš„ dropout æ¦‚ç‡ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨å…¶ä»– ğŸ¤— PEFT æ–¹æ³•ï¼Œè¯·ç¡®ä¿å°† `LoraConfig`
    æ›¿æ¢ä¸ºé€‚å½“çš„ [class](../package_reference/tuners)ã€‚
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Throughout the script, youâ€™ll see the [main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)
    and [wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)
    functions which help control and synchronize when processes are executed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªè„šæœ¬ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°[main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)å’Œ[wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)å‡½æ•°ï¼Œå®ƒä»¬æœ‰åŠ©äºæ§åˆ¶å’ŒåŒæ­¥è¿›ç¨‹çš„æ‰§è¡Œæ—¶é—´ã€‚
- en: 'The [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function takes a base model and the `peft_config` you prepared earlier to create
    a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)å‡½æ•°æ¥å—ä¸€ä¸ªåŸºç¡€æ¨¡å‹å’Œæ‚¨ä¹‹å‰å‡†å¤‡çš„`peft_config`ï¼Œä»¥åˆ›å»ºä¸€ä¸ª[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ï¼š'
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Pass all the relevant training objects to ğŸ¤— Accelerateâ€™s [prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    which makes sure everything is ready for training:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒå¯¹è±¡ä¼ é€’ç»™ğŸ¤— Accelerateçš„[prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ï¼Œç¡®ä¿ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼š
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next bit of code checks whether the DeepSpeed plugin is used in the `Accelerator`,
    and if the plugin exists, then the `Accelerator` uses ZeRO-3 as specified in the
    configuration file:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„ä»£ç æ®µæ£€æŸ¥`Accelerator`ä¸­æ˜¯å¦ä½¿ç”¨äº†DeepSpeedæ’ä»¶ï¼Œå¦‚æœæ’ä»¶å­˜åœ¨ï¼Œåˆ™`Accelerator`å°†æ ¹æ®é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„ZeRO-3æ¥ä½¿ç”¨å®ƒï¼š
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Inside the training loop, the usual `loss.backward()` is replaced by ğŸ¤— Accelerateâ€™s
    [backward](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)
    which uses the correct `backward()` method based on your configuration:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œé€šå¸¸çš„`loss.backward()`è¢«ğŸ¤— Accelerateçš„[backward](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)æ›¿æ¢ï¼Œå®ƒæ ¹æ®æ‚¨çš„é…ç½®ä½¿ç”¨æ­£ç¡®çš„`backward()`æ–¹æ³•ï¼š
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: That is all! The rest of the script handles the training loop, evaluation, and
    even pushes it to the Hub for you.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›äº†ï¼è„šæœ¬çš„å…¶ä½™éƒ¨åˆ†å¤„ç†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ï¼Œç”šè‡³ä¸ºæ‚¨å°†å…¶æ¨é€åˆ°Hubã€‚
- en: Train
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'Run the following command to launch the training script. Earlier, you saved
    the configuration file to `ds_zero3_cpu.yaml`, so youâ€™ll need to pass the path
    to the launcher with the `--config_file` argument like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚ä¹‹å‰ï¼Œæ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸º`ds_zero3_cpu.yaml`ï¼Œå› æ­¤æ‚¨éœ€è¦é€šè¿‡`--config_file`å‚æ•°å°†è·¯å¾„ä¼ é€’ç»™å¯åŠ¨å™¨ï¼Œå°±åƒè¿™æ ·ï¼š
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Youâ€™ll see some output logs that track memory usage during training, and once
    itâ€™s completed, the script returns the accuracy and compares the predictions to
    the labels:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†çœ‹åˆ°ä¸€äº›è·Ÿè¸ªå†…å­˜ä½¿ç”¨æƒ…å†µçš„è¾“å‡ºæ—¥å¿—ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œè„šæœ¬å°†è¿”å›å‡†ç¡®æ€§å¹¶å°†é¢„æµ‹ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼š
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
