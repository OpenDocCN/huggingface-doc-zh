- en: unCLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: unCLIP
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/unclip](https://huggingface.co/docs/diffusers/api/pipelines/unclip)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/unclip](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125)
    is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The
    unCLIP model in 🤗 Diffusers comes from kakaobrain’s [karlo](https://github.com/kakaobrain/karlo).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[具有CLIP潜在特征的分层文本条件图像生成](https://huggingface.co/papers/2204.06125) 是由Aditya
    Ramesh，Prafulla Dhariwal，Alex Nichol，Casey Chu，Mark Chen撰写的。🤗 Diffusers中的unCLIP模型来自kakaobrain的[karlo](https://github.com/kakaobrain/karlo)。'
- en: 'The abstract from the paper is following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Contrastive models like CLIP have been shown to learn robust representations
    of images that capture both semantics and style. To leverage these representations
    for image generation, we propose a two-stage model: a prior that generates a CLIP
    image embedding given a text caption, and a decoder that generates an image conditioned
    on the image embedding. We show that explicitly generating image representations
    improves image diversity with minimal loss in photorealism and caption similarity.
    Our decoders conditioned on image representations can also produce variations
    of an image that preserve both its semantics and style, while varying the non-essential
    details absent from the image representation. Moreover, the joint embedding space
    of CLIP enables language-guided image manipulations in a zero-shot fashion. We
    use diffusion models for the decoder and experiment with both autoregressive and
    diffusion models for the prior, finding that the latter are computationally more
    efficient and produce higher-quality samples.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*像CLIP这样的对比模型已经被证明可以学习到捕捉语义和风格的图像的稳健表示。为了利用这些表示来进行图像生成，我们提出了一个两阶段模型：一个先验模型根据文本标题生成一个CLIP图像嵌入，一个解码器根据图像嵌入生成一个图像。我们展示了明确生成图像表示可以提高图像多样性，同时最小化逼真度和标题相似性的损失。我们的解码器根据图像表示可以产生保留其语义和风格的图像变体，同时变化非必要的细节，这些细节在图像表示中不存在。此外，CLIP的联合嵌入空间使得可以通过语言引导的图像操作以零样本的方式进行。我们使用扩散模型进行解码器，并尝试先验模型使用自回归模型和扩散模型，发现后者在计算上更有效，并产生更高质量的样本。*'
- en: You can find lucidrains’ DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)找到lucidrains的DALL-E
    2重建。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看调度程序指南以了解如何在调度程序速度和质量之间进行权衡，并查看重用组件跨管道部分以了解如何有效地将相同组件加载到多个管道中。
- en: UnCLIPPipeline
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UnCLIPPipeline
- en: '### `class diffusers.UnCLIPPipeline`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.UnCLIPPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L34)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L34)'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    — Frozen text-encoder.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    — 冻结的文本编码器。'
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    — A `CLIPTokenizer` to tokenize text.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    — 用于对文本进行标记化的 `CLIPTokenizer`。'
- en: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    — The canonical unCLIP prior to approximate the image embedding from the text
    embedding.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    — 用于从文本嵌入近似图像嵌入的规范unCLIP先验。'
- en: '`text_proj` (`UnCLIPTextProjModel`) — Utility class to prepare and combine
    the embeddings before they are passed to the decoder.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_proj` (`UnCLIPTextProjModel`) — 在传递给解码器之前准备和组合嵌入的实用类。'
- en: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — The decoder to invert the image embedding into an image.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 将图像嵌入反转为图像的解码器。'
- en: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — Super resolution UNet. Used in all but the last step of the super resolution
    diffusion process.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — 超分辨率UNet。用于超分辨率扩散过程的除最后一步之外的所有步骤。'
- en: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — Super resolution UNet. Used in the last step of the super resolution diffusion
    process.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — 超分辨率UNet。用于超分辨率扩散过程的最后一步。'
- en: '`prior_scheduler` (`UnCLIPScheduler`) — Scheduler used in the prior denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler` (`UnCLIPScheduler`) — 用于先验去噪过程的调度程序（修改的[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)）。'
- en: '`decoder_scheduler` (`UnCLIPScheduler`) — Scheduler used in the decoder denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_scheduler` (`UnCLIPScheduler`) — 用于解码器去噪过程的调度程序（修改的[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)）。'
- en: '`super_res_scheduler` (`UnCLIPScheduler`) — Scheduler used in the super resolution
    denoising process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_scheduler` (`UnCLIPScheduler`) — 用于超分辨率去噪过程的调度程序（修改的[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)）。'
- en: Pipeline for text-to-image generation using unCLIP.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 unCLIP 进行文本到图像生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L211)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L211)'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`) — The prompt or prompts to guide image generation.
    This can only be left undefined if `text_model_output` and `text_attention_mask`
    is passed.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示。只有在传递 `text_model_output` 和 `text_attention_mask`
    时才能将其留空。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, 默认为1) — 每个提示生成的图像数量。'
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 25) — The number
    of denoising steps for the prior. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps` (`int`, *optional*, 默认为25) — 先验的去噪步骤数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`decoder_num_inference_steps` (`int`, *optional*, defaults to 25) — The number
    of denoising steps for the decoder. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_num_inference_steps` (`int`, *optional*, 默认为25) — 解码器的去噪步骤数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`super_res_num_inference_steps` (`int`, *optional*, defaults to 7) — The number
    of denoising steps for super resolution. More denoising steps usually lead to
    a higher quality image at the expense of slower inference.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_num_inference_steps` (`int`, *optional*, 默认为7) — 超分辨率去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成结果确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`prior_latents` (`torch.FloatTensor` of shape (batch size, embeddings dimension),
    *optional*) — Pre-generated noisy latents to be used as inputs for the prior.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_latents` (`torch.FloatTensor`，形状为 (batch size, embeddings dimension)，*optional*)
    — 预先生成的嘈杂潜变量，用作先验的输入。'
- en: '`decoder_latents` (`torch.FloatTensor` of shape (batch size, channels, height,
    width), *optional*) — Pre-generated noisy latents to be used as inputs for the
    decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_latents` (`torch.FloatTensor`，形状为 (batch size, channels, height, width)，*optional*)
    — 预先生成的嘈杂潜变量，用作解码器的输入。'
- en: '`super_res_latents` (`torch.FloatTensor` of shape (batch size, channels, super
    res height, super res width), *optional*) — Pre-generated noisy latents to be
    used as inputs for the decoder.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_latents` (`torch.FloatTensor`，形状为 (batch size, channels, super res
    height, super res width)，*optional*) — 预先生成的嘈杂潜变量，用作解码器的输入。'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale` (`float`, *optional*, 默认为4.0) — 更高的引导比例值鼓励模型生成与文本 `prompt`
    密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。'
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 4.0) — A higher
    guidance scale value encourages the model to generate images closely linked to
    the text `prompt` at the expense of lower image quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_guidance_scale` (`float`, *optional*, 默认为4.0) — 更高的引导比例值鼓励模型生成与文本
    `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。'
- en: '`text_model_output` (`CLIPTextModelOutput`, *optional*) — Pre-defined `CLIPTextModel`
    outputs that can be derived from the text encoder. Pre-defined text outputs can
    be passed for tasks like text embedding interpolations. Make sure to also pass
    `text_attention_mask` in this case. `prompt` can the be left `None`.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_model_output` (`CLIPTextModelOutput`, *optional*) — 可从文本编码器派生的预定义 `CLIPTextModel`
    输出。预定义的文本输出可以用于诸如文本嵌入插值之类的任务。在这种情况下，确保还传递 `text_attention_mask`。`prompt` 可以留空。'
- en: '`text_attention_mask` (`torch.Tensor`, *optional*) — Pre-defined CLIP text
    attention mask that can be derived from the tokenizer. Pre-defined text attention
    masks are necessary when passing `text_model_output`.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_attention_mask` (`torch.Tensor`, *optional*) — 预定义的 CLIP 文本注意力掩码，可以从分词器中派生。当传递
    `text_model_output` 时，预定义的文本注意力掩码是必要的。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或
    `np.array` 之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    而不是一个普通的 tuple。'
- en: Returns
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，则返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则返回一个
    `tuple`，其中第一个元素是包含生成图像的列表。
- en: The call function to the pipeline for generation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: UnCLIPImageVariationPipeline
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UnCLIPImageVariationPipeline
- en: '### `class diffusers.UnCLIPImageVariationPipeline`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.UnCLIPImageVariationPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L39)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L39)'
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    — Frozen text-encoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（[CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)）—
    冻结的文本编码器。'
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    — A `CLIPTokenizer` to tokenize text.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)）—
    用于对文本进行标记化的 `CLIPTokenizer`。'
- en: '`feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    — Model that extracts features from generated images to be used as inputs for
    the `image_encoder`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)）—
    从生成的图像中提取特征的模型，用作 `image_encoder` 的输入。'
- en: '`image_encoder` ([CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection))
    — Frozen CLIP image-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_encoder`（[CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)）—
    冻结的 CLIP 图像编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`text_proj` (`UnCLIPTextProjModel`) — Utility class to prepare and combine
    the embeddings before they are passed to the decoder.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_proj`（`UnCLIPTextProjModel`）— 在传递给解码器之前准备和组合嵌入的实用类。'
- en: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — The decoder to invert the image embedding into an image.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）—
    将图像嵌入反转为图像的解码器。'
- en: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — Super resolution UNet. Used in all but the last step of the super resolution
    diffusion process.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_first`（[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)）—
    超分辨率 UNet。用于超分辨率扩散过程中的所有步骤，除了最后一步。'
- en: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — Super resolution UNet. Used in the last step of the super resolution diffusion
    process.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_last`（[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)）—
    超分辨率 UNet。用于超分辨率扩散过程的最后一步。'
- en: '`decoder_scheduler` (`UnCLIPScheduler`) — Scheduler used in the decoder denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_scheduler`（`UnCLIPScheduler`）— 用于解码器去噪过程的调度器（修改自 [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)）。'
- en: '`super_res_scheduler` (`UnCLIPScheduler`) — Scheduler used in the super resolution
    denoising process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_scheduler`（`UnCLIPScheduler`）— 用于超分辨率去噪过程的调度器（修改自 [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)）。'
- en: Pipeline to generate image variations from an input image using UnCLIP.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 UnCLIP 从输入图像生成图像变化的流水线。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L199)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L199)'
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image` (`PIL.Image.Image` or `List[PIL.Image.Image]` or `torch.FloatTensor`)
    — `Image` or tensor representing an image batch to be used as the starting point.
    If you provide a tensor, it needs to be compatible with the `CLIPImageProcessor`
    [configuration](https://huggingface.co/fusing/karlo-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json).
    Can be left as `None` only when `image_embeddings` are passed.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`（`PIL.Image.Image` 或 `List[PIL.Image.Image]` 或 `torch.FloatTensor`）—
    代表图像批次的 `Image` 或张量，用作起始点。如果提供张量，则需要与 `CLIPImageProcessor` 的 [配置](https://huggingface.co/fusing/karlo-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json)
    兼容。只有在传递 `image_embeddings` 时才能保持为 `None`。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`（`int`，*可选*，默认为 1）— 每个提示生成的图像数量。'
- en: '`decoder_num_inference_steps` (`int`, *optional*, defaults to 25) — The number
    of denoising steps for the decoder. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_num_inference_steps`（`int`，*可选*，默认为 25）— 解码器的去噪步骤数。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`super_res_num_inference_steps` (`int`, *optional*, defaults to 7) — The number
    of denoising steps for super resolution. More denoising steps usually lead to
    a higher quality image at the expense of slower inference.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_num_inference_steps`（`int`，*可选*，默认为 7）— 超分辨率的去噪步骤数。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`generator` (`torch.Generator`, *optional*) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（`torch.Generator`，*可选*）— 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`decoder_latents` (`torch.FloatTensor` of shape (batch size, channels, height,
    width), *optional*) — Pre-generated noisy latents to be used as inputs for the
    decoder.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_latents`（`torch.FloatTensor`，形状为（批量大小，通道数，高度，宽度），*可选*）— 预先生成的噪声潜变量，用作解码器的输入。'
- en: '`super_res_latents` (`torch.FloatTensor` of shape (batch size, channels, super
    res height, super res width), *optional*) — Pre-generated noisy latents to be
    used as inputs for the decoder.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_latents` (`torch.FloatTensor`，形状为(batch size, channels, super res
    height, super res width)，*optional*) — 预生成的噪声潜变量，用作解码器的输入。'
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 4.0) — A higher
    guidance scale value encourages the model to generate images closely linked to
    the text `prompt` at the expense of lower image quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_guidance_scale` (`float`, *optional*, 默认值为4.0) — 较高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。
    当`guidance_scale > 1`时启用引导比例。'
- en: '`image_embeddings` (`torch.Tensor`, *optional*) — Pre-defined image embeddings
    that can be derived from the image encoder. Pre-defined image embeddings can be
    passed for tasks like image interpolations. `image` can be left as `None`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeddings` (`torch.Tensor`, *optional*) — 可从图像编码器派生的预定义图像嵌入。 可以传递预定义的图像嵌入以用于诸如图像插值之类的任务。
    `image` 可以保持为 `None`。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认值为`"pil"`) — 生成图像的输出格式。 选择`PIL.Image`或`np.array`之间。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认值为`True`) — 是否返回一个[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则返回一个元组，其中第一个元素是包含生成图像的列表。
- en: The call function to the pipeline for generation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: ImagePipelineOutput
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImagePipelineOutput
- en: '### `class diffusers.ImagePipelineOutput`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.ImagePipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为`batch_size`的去噪PIL图像列表或形状为`(batch_size,
    height, width, num_channels)`的NumPy数组。'
- en: Output class for image pipelines.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图像管道的输出类。
