- en: Kandinsky
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åº·å®šæ–¯åŸº
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/kandinsky](https://huggingface.co/docs/diffusers/using-diffusers/kandinsky)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/kandinsky](https://huggingface.co/docs/diffusers/using-diffusers/kandinsky)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The Kandinsky models are a series of multilingual text-to-image generation models.
    The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those
    results for the UNet.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åº·å®šæ–¯åŸºæ¨¡å‹æ˜¯ä¸€ç³»åˆ—å¤šè¯­è¨€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚åº·å®šæ–¯åŸº2.0æ¨¡å‹ä½¿ç”¨ä¸¤ä¸ªå¤šè¯­è¨€æ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶å°†è¿™äº›ç»“æœè¿æ¥åˆ°UNetä¸­ã€‚
- en: '[Kandinsky 2.1](../api/pipelines/kandinsky) changes the architecture to include
    an image prior model ([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip))
    to generate a mapping between text and image embeddings. The mapping provides
    better text-image alignment and it is used with the text embeddings during training,
    leading to higher quality results. Finally, Kandinsky 2.1 uses a [Modulating Quantized
    Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) decoder - which adds
    a spatial conditional normalization layer to increase photorealism - to decode
    the latents into images.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[åº·å®šæ–¯åŸº2.1](../api/pipelines/kandinsky)æ”¹å˜äº†æ¶æ„ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå›¾åƒå…ˆå‰æ¨¡å‹ï¼ˆ[`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)ï¼‰ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥ä¹‹é—´çš„æ˜ å°„ã€‚è¯¥æ˜ å°„æä¾›æ›´å¥½çš„æ–‡æœ¬-å›¾åƒå¯¹é½ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´ä¸æ–‡æœ¬åµŒå…¥ä¸€èµ·ä½¿ç”¨ï¼Œä»è€Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„ç»“æœã€‚æœ€åï¼Œåº·å®šæ–¯åŸº2.1ä½¿ç”¨äº†ä¸€ä¸ª[è°ƒåˆ¶é‡åŒ–å‘é‡ï¼ˆMoVQï¼‰](https://huggingface.co/papers/2209.09002)è§£ç å™¨ï¼Œå®ƒæ·»åŠ äº†ä¸€ä¸ªç©ºé—´æ¡ä»¶å½’ä¸€åŒ–å±‚ä»¥å¢åŠ ç…§ç‰‡é€¼çœŸåº¦ï¼Œå°†æ½œåœ¨ç‰¹å¾è§£ç ä¸ºå›¾åƒã€‚'
- en: '[Kandinsky 2.2](../api/pipelines/kandinsky_v22) improves on the previous model
    by replacing the image encoder of the image prior model with a larger CLIP-ViT-G
    model to improve quality. The image prior model was also retrained on images with
    different resolutions and aspect ratios to generate higher-resolution images and
    different image sizes.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[åº·å®šæ–¯åŸº2.2](../api/pipelines/kandinsky_v22)é€šè¿‡ç”¨æ›´å¤§çš„CLIP-ViT-Gæ¨¡å‹æ›¿æ¢å›¾åƒå…ˆå‰æ¨¡å‹çš„å›¾åƒç¼–ç å™¨æ¥æ”¹è¿›å…ˆå‰æ¨¡å‹ã€‚å›¾åƒå…ˆå‰æ¨¡å‹è¿˜ç»è¿‡é‡æ–°è®­ç»ƒï¼Œç”¨ä¸åŒåˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡å’Œä¸åŒå°ºå¯¸çš„å›¾åƒã€‚'
- en: '[Kandinsky 3](../api/pipelines/kandinsky3) simplifies the architecture and
    shifts away from the two-stage generation process involving the prior model and
    diffusion model. Instead, Kandinsky 3 uses [Flan-UL2](https://huggingface.co/google/flan-ul2)
    to encode text, a UNet with [BigGan-deep](https://hf.co/papers/1809.11096) blocks,
    and [Sber-MoVQGAN](https://github.com/ai-forever/MoVQGAN) to decode the latents
    into images. Text understanding and generated image quality are primarily achieved
    by using a larger text encoder and UNet.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[åº·å®šæ–¯åŸº3](../api/pipelines/kandinsky3)ç®€åŒ–äº†æ¶æ„ï¼Œå¹¶æ‘†è„±äº†æ¶‰åŠå…ˆå‰æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ã€‚ç›¸åï¼Œåº·å®šæ–¯åŸº3ä½¿ç”¨[Flan-UL2](https://huggingface.co/google/flan-ul2)æ¥ç¼–ç æ–‡æœ¬ï¼Œä¸€ä¸ªå¸¦æœ‰[BigGan-deep](https://hf.co/papers/1809.11096)å—çš„UNetï¼Œä»¥åŠ[Sber-MoVQGAN](https://github.com/ai-forever/MoVQGAN)æ¥å°†æ½œåœ¨ç‰¹å¾è§£ç ä¸ºå›¾åƒã€‚é€šè¿‡ä½¿ç”¨æ›´å¤§çš„æ–‡æœ¬ç¼–ç å™¨å’ŒUNetï¼Œä¸»è¦å®ç°äº†æ–‡æœ¬ç†è§£å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„æé«˜ã€‚'
- en: This guide will show you how to use the Kandinsky models for text-to-image,
    image-to-image, inpainting, interpolation, and more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨åº·å®šæ–¯åŸºæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€ä¿®å¤ã€æ’å€¼ç­‰æ“ä½œã€‚
- en: 'Before you begin, make sure you have the following libraries installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Kandinsky 2.1 and 2.2 usage is very similar! The only difference is Kandinsky
    2.2 doesnâ€™t accept `prompt` as an input when decoding the latents. Instead, Kandinsky
    2.2 only accepts `image_embeds` during decoding.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åº·å®šæ–¯åŸº2.1å’Œ2.2çš„ç”¨æ³•éå¸¸ç›¸ä¼¼ï¼å”¯ä¸€çš„åŒºåˆ«æ˜¯åº·å®šæ–¯åŸº2.2åœ¨è§£ç æ½œåœ¨ç‰¹å¾æ—¶ä¸æ¥å—`prompt`ä½œä¸ºè¾“å…¥ã€‚ç›¸åï¼Œåº·å®šæ–¯åŸº2.2åœ¨è§£ç æ—¶åªæ¥å—`image_embeds`ã€‚
- en: Kandinsky 3 has a more concise architecture and it doesnâ€™t require a prior model.
    This means itâ€™s usage is identical to other diffusion models like [Stable Diffusion
    XL](sdxl).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åº·å®šæ–¯åŸº3å…·æœ‰æ›´ç®€æ´çš„æ¶æ„ï¼Œä¸éœ€è¦å…ˆå‰çš„æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒçš„ç”¨æ³•ä¸å…¶ä»–æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚[ç¨³å®šæ‰©æ•£XL](sdxl)ï¼‰ç›¸åŒã€‚
- en: Text-to-image
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒ
- en: To use the Kandinsky models for any task, you always start by setting up the
    prior pipeline to encode the prompt and generate the image embeddings. The prior
    pipeline also generates `negative_image_embeds` that correspond to the negative
    prompt `""`. For better results, you can pass an actual `negative_prompt` to the
    prior pipeline, but thisâ€™ll increase the effective batch size of the prior pipeline
    by 2x.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨åº·å®šæ–¯åŸºæ¨¡å‹æ‰§è¡Œä»»ä½•ä»»åŠ¡ï¼Œæ‚¨å§‹ç»ˆè¦å…ˆè®¾ç½®å…ˆå‰æµç¨‹ä»¥å¯¹æç¤ºè¿›è¡Œç¼–ç å¹¶ç”Ÿæˆå›¾åƒåµŒå…¥ã€‚å…ˆå‰æµç¨‹è¿˜ä¼šç”Ÿæˆä¸è´Ÿæç¤º`""`å¯¹åº”çš„`negative_image_embeds`ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œæ‚¨å¯ä»¥å‘å…ˆå‰æµç¨‹ä¼ é€’å®é™…çš„`negative_prompt`ï¼Œä½†è¿™ä¼šä½¿å…ˆå‰æµç¨‹çš„æœ‰æ•ˆæ‰¹é‡å¤§å°å¢åŠ 2å€ã€‚
- en: Kandinsky 2.1Kandinsky 2.2Kandinsky 3
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åº·å®šæ–¯åŸº2.1åº·å®šæ–¯åŸº2.2åº·å®šæ–¯åŸº3
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now pass all the prompts and embeddings to the [KandinskyPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyPipeline)
    to generate an image:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†æ‰€æœ‰æç¤ºå’ŒåµŒå…¥ä¼ é€’ç»™[KandinskyPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyPipeline)ä»¥ç”Ÿæˆå›¾åƒï¼š
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/7a49f6e1db66e1eebcfa846868181899.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a49f6e1db66e1eebcfa846868181899.png)'
- en: ğŸ¤— Diffusers also provides an end-to-end API with the [KandinskyCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyCombinedPipeline)
    and [KandinskyV22CombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22CombinedPipeline),
    meaning you donâ€™t have to separately load the prior and text-to-image pipeline.
    The combined pipeline automatically loads both the prior model and the decoder.
    You can still set different values for the prior pipeline with the `prior_guidance_scale`
    and `prior_num_inference_steps` parameters if you want.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—æ‰©æ•£å™¨è¿˜æä¾›äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„APIï¼Œå…¶ä¸­åŒ…æ‹¬[KandinskyCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyCombinedPipeline)å’Œ[KandinskyV22CombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22CombinedPipeline)ï¼Œè¿™æ„å‘³ç€æ‚¨ä¸å¿…å•ç‹¬åŠ è½½å…ˆå‰çš„æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒçš„æµç¨‹ã€‚åˆå¹¶æµç¨‹ä¼šè‡ªåŠ¨åŠ è½½å…ˆå‰æ¨¡å‹å’Œè§£ç å™¨ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨ä»ç„¶å¯ä»¥é€šè¿‡`prior_guidance_scale`å’Œ`prior_num_inference_steps`å‚æ•°ä¸ºå…ˆå‰æµç¨‹è®¾ç½®ä¸åŒçš„å€¼ã€‚
- en: 'Use the [AutoPipelineForText2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForText2Image)
    to automatically call the combined pipelines under the hood:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[AutoPipelineForText2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForText2Image)æ¥è‡ªåŠ¨è°ƒç”¨åº•å±‚çš„åˆå¹¶æµç¨‹ï¼š
- en: Kandinsky 2.1Kandinsky 2.2
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åº·å®šæ–¯åŸº2.1åº·å®šæ–¯åŸº2.2
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Image-to-image
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒ
- en: 'For image-to-image, pass the initial image and text prompt to condition the
    image to the pipeline. Start by loading the prior pipeline:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾åƒåˆ°å›¾åƒï¼Œå°†åˆå§‹å›¾åƒå’Œæ–‡æœ¬æç¤ºä¼ é€’ç»™ç®¡é“ä»¥å¯¹å›¾åƒè¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚é¦–å…ˆåŠ è½½å…ˆå‰ç®¡é“ï¼š
- en: Kandinsky 2.1Kandinsky 2.2Kandinsky 3
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2Kandinsky 3
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Download an image to condition on:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è½½ä¸€ä¸ªå›¾åƒè¿›è¡Œæ¡ä»¶è®¾ç½®ï¼š
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/f2a11ed9e52e5fabb2124e5fe7bba075.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2a11ed9e52e5fabb2124e5fe7bba075.png)'
- en: 'Generate the `image_embeds` and `negative_image_embeds` with the prior pipeline:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å…ˆå‰ç®¡é“ç”Ÿæˆ`image_embeds`å’Œ`negative_image_embeds`ï¼š
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now pass the original image, and all the prompts and embeddings to the pipeline
    to generate an image:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†åŸå§‹å›¾åƒå’Œæ‰€æœ‰æç¤ºå’ŒåµŒå…¥ä¼ é€’ç»™ç®¡é“ä»¥ç”Ÿæˆå›¾åƒï¼š
- en: Kandinsky 2.1Kandinsky 2.2Kandinsky 3
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2Kandinsky 3
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/8554b2d04f4b98afeb9692a00447ba5d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8554b2d04f4b98afeb9692a00447ba5d.png)'
- en: ğŸ¤— Diffusers also provides an end-to-end API with the [KandinskyImg2ImgCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyImg2ImgCombinedPipeline)
    and [KandinskyV22Img2ImgCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22Img2ImgCombinedPipeline),
    meaning you donâ€™t have to separately load the prior and image-to-image pipeline.
    The combined pipeline automatically loads both the prior model and the decoder.
    You can still set different values for the prior pipeline with the `prior_guidance_scale`
    and `prior_num_inference_steps` parameters if you want.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Diffusersè¿˜æä¾›äº†ç«¯åˆ°ç«¯çš„APIï¼Œä½¿ç”¨[KandinskyImg2ImgCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyImg2ImgCombinedPipeline)å’Œ[KandinskyV22Img2ImgCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22Img2ImgCombinedPipeline)ï¼Œè¿™æ„å‘³ç€æ‚¨ä¸å¿…å•ç‹¬åŠ è½½å…ˆå‰å’Œå›¾åƒåˆ°å›¾åƒç®¡é“ã€‚ç»„åˆç®¡é“ä¼šè‡ªåŠ¨åŠ è½½å…ˆå‰æ¨¡å‹å’Œè§£ç å™¨ã€‚å¦‚æœéœ€è¦ï¼Œä»ç„¶å¯ä»¥é€šè¿‡`prior_guidance_scale`å’Œ`prior_num_inference_steps`å‚æ•°ä¸ºå…ˆå‰ç®¡é“è®¾ç½®ä¸åŒçš„å€¼ã€‚
- en: 'Use the [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)
    to automatically call the combined pipelines under the hood:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)æ¥åœ¨å†…éƒ¨è‡ªåŠ¨è°ƒç”¨ç»„åˆç®¡é“ï¼š
- en: Kandinsky 2.1Kandinsky 2.2
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Inpainting
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿®å¤
- en: 'âš ï¸ The Kandinsky models use â¬œï¸ **white pixels** to represent the masked area
    now instead of black pixels. If you are using [KandinskyInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyInpaintPipeline)
    in production, you need to change the mask to use white pixels:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸ Kandinskyæ¨¡å‹ç°åœ¨ä½¿ç”¨â¬œï¸ **ç™½è‰²åƒç´ **æ¥ä»£è¡¨é®ç½©åŒºåŸŸï¼Œè€Œä¸æ˜¯é»‘è‰²åƒç´ ã€‚å¦‚æœæ‚¨æ­£åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨[KandinskyInpaintPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyInpaintPipeline)ï¼Œæ‚¨éœ€è¦å°†é®ç½©æ›´æ”¹ä¸ºä½¿ç”¨ç™½è‰²åƒç´ ï¼š
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For inpainting, youâ€™ll need the original image, a mask of the area to replace
    in the original image, and a text prompt of what to inpaint. Load the prior pipeline:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¿®å¤ï¼Œæ‚¨å°†éœ€è¦åŸå§‹å›¾åƒã€åŸå§‹å›¾åƒä¸­è¦æ›¿æ¢çš„åŒºåŸŸçš„é®ç½©ï¼Œä»¥åŠè¦ä¿®å¤çš„å†…å®¹çš„æ–‡æœ¬æç¤ºã€‚åŠ è½½å…ˆå‰ç®¡é“ï¼š
- en: Kandinsky 2.1Kandinsky 2.2
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Load an initial image and create a mask:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½åˆå§‹å›¾åƒå¹¶åˆ›å»ºé®ç½©ï¼š
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Generate the embeddings with the prior pipeline:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å…ˆå‰ç®¡é“ç”ŸæˆåµŒå…¥ï¼š
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now pass the initial image, mask, and prompt and embeddings to the pipeline
    to generate an image:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†åˆå§‹å›¾åƒã€é®ç½©ã€æç¤ºå’ŒåµŒå…¥ä¼ é€’ç»™ç®¡é“ä»¥ç”Ÿæˆå›¾åƒï¼š
- en: Kandinsky 2.1Kandinsky 2.2
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/06d9ede591c4c816c0afb1ac1756de88.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06d9ede591c4c816c0afb1ac1756de88.png)'
- en: 'You can also use the end-to-end [KandinskyInpaintCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyInpaintCombinedPipeline)
    and [KandinskyV22InpaintCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22InpaintCombinedPipeline)
    to call the prior and decoder pipelines together under the hood. Use the [AutoPipelineForInpainting](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForInpainting)
    for this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ç«¯åˆ°ç«¯çš„[KandinskyInpaintCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky#diffusers.KandinskyInpaintCombinedPipeline)å’Œ[KandinskyV22InpaintCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22InpaintCombinedPipeline)æ¥åœ¨å†…éƒ¨åŒæ—¶è°ƒç”¨å…ˆå‰å’Œè§£ç å™¨ç®¡é“ã€‚ä½¿ç”¨[AutoPipelineForInpainting](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForInpainting)æ¥å®ç°ï¼š
- en: Kandinsky 2.1Kandinsky 2.2
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Interpolation
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ’å€¼
- en: 'Interpolation allows you to explore the latent space between the image and
    text embeddings which is a cool way to see some of the prior modelâ€™s intermediate
    outputs. Load the prior pipeline and two images youâ€™d like to interpolate:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ’å€¼å…è®¸æ‚¨æ¢ç´¢å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„æ½œåœ¨ç©ºé—´ï¼Œè¿™æ˜¯ä¸€ç§å¾ˆé…·çš„æ–¹å¼æ¥æŸ¥çœ‹å…ˆå‰æ¨¡å‹çš„ä¸­é—´è¾“å‡ºã€‚åŠ è½½å…ˆå‰ç®¡é“å’Œæ‚¨æƒ³è¦æ’å€¼çš„ä¸¤ä¸ªå›¾åƒï¼š
- en: Kandinsky 2.1Kandinsky 2.2
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/c39ee2cbaacad2963f3842a301c122a7.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c39ee2cbaacad2963f3842a301c122a7.png)'
- en: a cat
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åªçŒ«
- en: '![](../Images/d28a21117fde9e1974b55867c4e29dea.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d28a21117fde9e1974b55867c4e29dea.png)'
- en: Van Gogh's Starry Night painting
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢µé«˜çš„ã€Šæ˜Ÿå¤œã€‹ç»˜ç”»
- en: Specify the text or images to interpolate, and set the weights for each text
    or image. Experiment with the weights to see how they affect the interpolation!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡å®šè¦æ’å€¼çš„æ–‡æœ¬æˆ–å›¾åƒï¼Œå¹¶ä¸ºæ¯ä¸ªæ–‡æœ¬æˆ–å›¾åƒè®¾ç½®æƒé‡ã€‚å°è¯•ä¸åŒçš„æƒé‡ä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ’å€¼ï¼
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Call the `interpolate` function to generate the embeddings, and then pass them
    to the pipeline to generate the image:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨`interpolate`å‡½æ•°ç”ŸæˆåµŒå…¥ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™ç®¡é“ç”Ÿæˆå›¾åƒï¼š
- en: Kandinsky 2.1Kandinsky 2.2
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1Kandinsky 2.2
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/59fe929dc7d1553d026875441e579c7b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59fe929dc7d1553d026875441e579c7b.png)'
- en: ControlNet
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ControlNet
- en: âš ï¸ ControlNet is only supported for Kandinsky 2.2!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸ ControlNetä»…æ”¯æŒKandinsky 2.2ï¼
- en: ControlNet enables conditioning large pretrained diffusion models with additional
    inputs such as a depth map or edge detection. For example, you can condition Kandinsky
    2.2 with a depth map so the model understands and preserves the structure of the
    depth image.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNetä½¿å¾—å¯ä»¥ä½¿ç”¨é¢å¤–çš„è¾“å…¥æ¡ä»¶åŒ–å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä¾‹å¦‚æ·±åº¦å›¾æˆ–è¾¹ç¼˜æ£€æµ‹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ·±åº¦å›¾å¯¹Kandinsky 2.2è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œä»¥ä¾¿æ¨¡å‹ç†è§£å¹¶ä¿ç•™æ·±åº¦å›¾åƒçš„ç»“æ„ã€‚
- en: 'Letâ€™s load an image and extract itâ€™s depth map:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åŠ è½½ä¸€å¼ å›¾åƒå¹¶æå–å…¶æ·±åº¦å›¾ï¼š
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/098067659c7d591c7d25aac963832424.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/098067659c7d591c7d25aac963832424.png)'
- en: 'Then you can use the `depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)
    from ğŸ¤— Transformers to process the image and retrieve the depth map:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— Transformersçš„`depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)å¤„ç†å›¾åƒå¹¶æ£€ç´¢æ·±åº¦å›¾ï¼š
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Text-to-image
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒ
- en: 'Load the prior pipeline and the [KandinskyV22ControlnetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetPipeline):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½å…ˆå‰çš„ç®¡é“å’Œ[KandinskyV22ControlnetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetPipeline)ï¼š
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Generate the image embeddings from a prompt and negative prompt:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æç¤ºå’Œè´Ÿæç¤ºç”Ÿæˆå›¾åƒåµŒå…¥ï¼š
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, pass the image embeddings and the depth image to the [KandinskyV22ControlnetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetPipeline)
    to generate an image:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå°†å›¾åƒåµŒå…¥å’Œæ·±åº¦å›¾åƒä¼ é€’ç»™[KandinskyV22ControlnetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetPipeline)ä»¥ç”Ÿæˆå›¾åƒï¼š
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/b044b806899a4ae7cdc6df24f2b71924.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b044b806899a4ae7cdc6df24f2b71924.png)'
- en: Image-to-image
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒ
- en: 'For image-to-image with ControlNet, youâ€™ll need to use the:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä½¿ç”¨ControlNetçš„å›¾åƒåˆ°å›¾åƒï¼Œæ‚¨éœ€è¦ä½¿ç”¨ï¼š
- en: '[KandinskyV22PriorEmb2EmbPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22PriorEmb2EmbPipeline)
    to generate the image embeddings from a text prompt and an image'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KandinskyV22PriorEmb2EmbPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22PriorEmb2EmbPipeline)ä»æ–‡æœ¬æç¤ºå’Œå›¾åƒç”Ÿæˆå›¾åƒåµŒå…¥'
- en: '[KandinskyV22ControlnetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetImg2ImgPipeline)
    to generate an image from the initial image and the image embeddings'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KandinskyV22ControlnetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetImg2ImgPipeline)ä»åˆå§‹å›¾åƒå’Œå›¾åƒåµŒå…¥ç”Ÿæˆå›¾åƒ'
- en: 'Process and extract a depth map of an initial image of a cat with the `depth-estimation`
    [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)
    from ğŸ¤— Transformers:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Transformersçš„`depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)å¤„ç†å’Œæå–çŒ«çš„åˆå§‹å›¾åƒçš„æ·±åº¦å›¾ï¼š
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Load the prior pipeline and the [KandinskyV22ControlnetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetImg2ImgPipeline):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½å…ˆå‰çš„ç®¡é“å’Œ[KandinskyV22ControlnetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetImg2ImgPipeline)ï¼š
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Pass a text prompt and the initial image to the prior pipeline to generate
    the image embeddings:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ é€’æ–‡æœ¬æç¤ºå’Œåˆå§‹å›¾åƒåˆ°å…ˆå‰çš„ç®¡é“ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥ï¼š
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now you can run the [KandinskyV22ControlnetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetImg2ImgPipeline)
    to generate an image from the initial image and the image embeddings:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥è¿è¡Œ[KandinskyV22ControlnetImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22ControlnetImg2ImgPipeline)ä»åˆå§‹å›¾åƒå’Œå›¾åƒåµŒå…¥ç”Ÿæˆå›¾åƒï¼š
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/3b9c48325073009cf6dc5d406bfe39d2.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b9c48325073009cf6dc5d406bfe39d2.png)'
- en: Optimizations
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–
- en: Kandinsky is unique because it requires a prior pipeline to generate the mappings,
    and a second pipeline to decode the latents into an image. Optimization efforts
    should be focused on the second pipeline because that is where the bulk of the
    computation is done. Here are some tips to improve Kandinsky during inference.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinskyæ˜¯ç‹¬ç‰¹çš„ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ä¸ªå…ˆå‰çš„ç®¡é“æ¥ç”Ÿæˆæ˜ å°„ï¼Œä»¥åŠç¬¬äºŒä¸ªç®¡é“æ¥å°†æ½œåœ¨å˜é‡è§£ç ä¸ºå›¾åƒã€‚ä¼˜åŒ–å·¥ä½œåº”è¯¥é›†ä¸­åœ¨ç¬¬äºŒä¸ªç®¡é“ä¸Šï¼Œå› ä¸ºé‚£é‡Œå®Œæˆäº†å¤§éƒ¨åˆ†è®¡ç®—ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ”¹è¿›Kandinskyæ¨ç†çš„æç¤ºã€‚
- en: 'Enable [xFormers](../optimization/xformers) if youâ€™re using PyTorch < 2.0:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯PyTorch < 2.0ï¼Œè¯·å¯ç”¨[xFormers](../optimization/xformers)ï¼š
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Enable `torch.compile` if youâ€™re using PyTorch >= 2.0 to automatically use
    scaled dot-product attention (SDPA):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯PyTorch >= 2.0ï¼Œè¯·å¯ç”¨`torch.compile`ä»¥è‡ªåŠ¨ä½¿ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰ï¼š
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is the same as explicitly setting the attention processor to use [AttnAddedKVProcessor2_0](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.AttnAddedKVProcessor2_0):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ˜¾å¼è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä¸º[AttnAddedKVProcessor2_0](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.AttnAddedKVProcessor2_0)ç›¸åŒï¼š
- en: '[PRE29]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Offload the model to the CPU with [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    to avoid out-of-memory errors:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)å°†æ¨¡å‹å¸è½½åˆ°CPUä»¥é¿å…å†…å­˜ä¸è¶³é”™è¯¯ï¼š
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'By default, the text-to-image pipeline uses the [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)
    but you can replace it with another scheduler like [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)
    to see how that affects the tradeoff between inference speed and image quality:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œæ–‡æœ¬åˆ°å›¾åƒç®¡é“ä½¿ç”¨[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œä½†æ‚¨å¯ä»¥å°†å…¶æ›¿æ¢ä¸ºå¦ä¸€ä¸ªè°ƒåº¦ç¨‹åºï¼Œå¦‚[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼Œä»¥æŸ¥çœ‹å®ƒå¦‚ä½•å½±å“æ¨ç†é€Ÿåº¦å’Œå›¾åƒè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼š
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
