- en: Quickstart
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå…¥é—¨
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/quickstart](https://huggingface.co/docs/optimum-neuron/quickstart)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/optimum-neuron/quickstart](https://huggingface.co/docs/optimum-neuron/quickstart)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'ğŸ¤— Optimum Neuron was designed with one goal in mind: **to make training and
    inference straightforward for any ğŸ¤— Transformers user while leveraging the complete
    power of AWS Accelerators**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimum Neuronçš„è®¾è®¡ç›®æ ‡æ˜¯ï¼š**ä½¿ä»»ä½•ğŸ¤— Transformersç”¨æˆ·èƒ½å¤Ÿè½»æ¾è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼ŒåŒæ—¶åˆ©ç”¨AWSåŠ é€Ÿå™¨çš„å…¨éƒ¨åŠŸèƒ½**ã€‚
- en: Training
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'There are two main classes one needs to know:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ä¸ªä¸»è¦çš„ç±»éœ€è¦äº†è§£ï¼š
- en: 'NeuronArgumentParser: inherits the original [HfArgumentParser](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.HfArgumentParser)
    in Transformers with additional checks on the argument values to make sure that
    they will work well with AWS Trainium instances.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NeuronArgumentParserï¼šç»§æ‰¿äº†Transformersä¸­åŸå§‹çš„[HfArgumentParser](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.HfArgumentParser)ï¼Œå¹¶å¯¹å‚æ•°å€¼è¿›è¡Œé¢å¤–æ£€æŸ¥ï¼Œä»¥ç¡®ä¿å®ƒä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°ä¸AWS
    Trainiumå®ä¾‹é…åˆä½¿ç”¨ã€‚
- en: '[NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer):
    the trainer class that takes care of compiling and distributing the model to run
    on Trainium Chips, and performing training and evaluation.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer)ï¼šè´Ÿè´£ç¼–è¯‘å’Œåˆ†å‘æ¨¡å‹ä»¥åœ¨TrainiumèŠ¯ç‰‡ä¸Šè¿è¡Œï¼Œå¹¶æ‰§è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„è®­ç»ƒå™¨ç±»ã€‚'
- en: The [NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer)
    is very similar to the [ğŸ¤— Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer),
    and adapting a script using the Trainer to make it work with Trainium will mostly
    consist in simply swapping the `Trainer` class for the `NeuronTrainer` one. Thatâ€™s
    how most of the [example scripts](https://github.com/huggingface/optimum-neuron/tree/main/examples)
    were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: NeuronTraineréå¸¸ç±»ä¼¼äºğŸ¤— Transformers Trainerï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨NeuronTraineræ¥è°ƒæ•´è„šæœ¬ä»¥ä½¿å…¶ä¸Trainiumä¸€èµ·å·¥ä½œï¼Œä¸»è¦æ˜¯ç®€å•åœ°å°†`Trainer`ç±»æ›¿æ¢ä¸º`NeuronTrainer`ç±»ã€‚è¿™å°±æ˜¯å¤§å¤šæ•°[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/optimum-neuron/tree/main/examples)æ˜¯å¦‚ä½•ä»å®ƒä»¬çš„[åŸå§‹å¯¹åº”ç‰©](https://github.com/huggingface/transformers/tree/main/examples/pytorch)è¿›è¡Œè°ƒæ•´çš„ã€‚
- en: 'modifications:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®æ”¹ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All Trainium instances come at least with 2 Neuron Cores. To leverage those
    we need to launch the training whith `torchrun`. Below you see and example of
    how to launch a training script on a `trn1.2xlarge` instance using a `bert-base-uncased`
    model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰Trainiumå®ä¾‹è‡³å°‘é…å¤‡2ä¸ªNeuronæ ¸å¿ƒã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›æ ¸å¿ƒï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`torchrun`å¯åŠ¨è®­ç»ƒã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•åœ¨`trn1.2xlarge`å®ä¾‹ä¸Šä½¿ç”¨`bert-base-uncased`æ¨¡å‹å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Inference
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: 'You can compile and export your ğŸ¤— Transformers models to a serialized format
    before inference on Neuron devices:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨Neuronè®¾å¤‡ä¸Šè¿›è¡Œæ¨ç†ä¹‹å‰ï¼Œå°†ğŸ¤— Transformersæ¨¡å‹ç¼–è¯‘å¹¶å¯¼å‡ºä¸ºåºåˆ—åŒ–æ ¼å¼ï¼š
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The command above will export `distilbert-base-uncased-finetuned-sst-2-english`
    with static shapes: `batch_size=1` and `sequence_length=32`, and cast all `matmul`
    operations from FP32 to BF16\. Check out the [exporter guide](https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-a-model-to-neuron-using-the-cli)
    for more compilation options.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„å‘½ä»¤å°†ä½¿ç”¨é™æ€å½¢çŠ¶å¯¼å‡º`distilbert-base-uncased-finetuned-sst-2-english`ï¼š`batch_size=1`å’Œ`sequence_length=32`ï¼Œå¹¶å°†æ‰€æœ‰`matmul`æ“ä½œä»FP32è½¬æ¢ä¸ºBF16ã€‚æŸ¥çœ‹[å¯¼å‡ºå™¨æŒ‡å—](https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-a-model-to-neuron-using-the-cli)ä»¥è·å–æ›´å¤šç¼–è¯‘é€‰é¡¹ã€‚
- en: 'Then you can run the exported Neuron model on Neuron devices with `NeuronModelForXXX`
    classes which are similar to `AutoModelForXXX` classes in ğŸ¤— Transformers:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ç±»ä¼¼äºğŸ¤— Transformersä¸­çš„`AutoModelForXXX`ç±»çš„`NeuronModelForXXX`ç±»åœ¨Neuronè®¾å¤‡ä¸Šè¿è¡Œå¯¼å‡ºçš„Neuronæ¨¡å‹ï¼š
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
