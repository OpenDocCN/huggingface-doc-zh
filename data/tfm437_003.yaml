- en: Quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速导览
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/quicktour](https://huggingface.co/docs/transformers/v4.37.2/en/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/quicktour](https://huggingface.co/docs/transformers/v4.37.2/en/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Get up and running with 🤗 Transformers! Whether you’re a developer or an everyday
    user, this quick tour will help you get started and show you how to use the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for inference, load a pretrained model and preprocessor with an [AutoClass](./model_doc/auto),
    and quickly train a model with PyTorch or TensorFlow. If you’re a beginner, we
    recommend checking out our tutorials or [course](https://huggingface.co/course/chapter1/1)
    next for more in-depth explanations of the concepts introduced here.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 快速上手 🤗 Transformers！无论您是开发人员还是日常用户，这个快速导览将帮助您入门，展示如何使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)进行推理，加载一个预训练模型和预处理器与[AutoClass](./model_doc/auto)，并快速使用PyTorch或TensorFlow训练模型。如果您是初学者，我们建议您查看我们的教程或[课程](https://huggingface.co/course/chapter1/1)以获取更深入的解释。
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保您已安装所有必要的库：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You’ll also need to install your preferred machine learning framework:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要安装您喜欢的机器学习框架：
- en: PytorchHide Pytorch content
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TensorFlowHide TensorFlow content
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Pipeline
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: '[https://www.youtube-nocookie.com/embed/tiZFewofSLM](https://www.youtube-nocookie.com/embed/tiZFewofSLM)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/tiZFewofSLM](https://www.youtube-nocookie.com/embed/tiZFewofSLM)'
- en: 'The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    is the easiest and fastest way to use a pretrained model for inference. You can
    use the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    out-of-the-box for many tasks across different modalities, some of which are shown
    in the table below:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)是使用预训练模型进行推理的最简单和最快速的方法。您可以直接使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)来处理许多不同模态的任务，其中一些显示在下表中：'
- en: For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看所有可用任务的完整列表，请查看[pipeline API参考](./main_classes/pipelines)。
- en: '| **Task** | **Description** | **Modality** | **Pipeline identifier** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **描述** | **模态** | **管道标识符** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Text classification | assign a label to a given sequence of text | NLP |
    pipeline(task=“sentiment-analysis”) |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | 为给定的文本序列分配一个标签 | NLP | pipeline(task=“sentiment-analysis”) |'
- en: '| Text generation | generate text given a prompt | NLP | pipeline(task=“text-generation”)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 文本生成 | 根据提示生成文本 | NLP | pipeline(task=“text-generation”) |'
- en: '| Summarization | generate a summary of a sequence of text or document | NLP
    | pipeline(task=“summarization”) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 | 生成文本或文档序列的摘要 | NLP | pipeline(task=“summarization”) |'
- en: '| Image classification | assign a label to an image | Computer vision | pipeline(task=“image-classification”)
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 图像分类 | 为图像分配一个标签 | 计算机视觉 | pipeline(task=“image-classification”) |'
- en: '| Image segmentation | assign a label to each individual pixel of an image
    (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=“image-segmentation”)
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 图像分割 | 为图像的每个像素分配一个标签（支持语义、全景和实例分割） | 计算机视觉 | pipeline(task=“image-segmentation”)
    |'
- en: '| Object detection | predict the bounding boxes and classes of objects in an
    image | Computer vision | pipeline(task=“object-detection”) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 物体检测 | 预测图像中物体的边界框和类别 | 计算机视觉 | pipeline(task=“object-detection”) |'
- en: '| Audio classification | assign a label to some audio data | Audio | pipeline(task=“audio-classification”)
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 音频分类 | 为一些音频数据分配一个标签 | 音频 | pipeline(task=“audio-classification”) |'
- en: '| Automatic speech recognition | transcribe speech into text | Audio | pipeline(task=“automatic-speech-recognition”)
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 自动语音识别 | 将语音转录为文本 | 音频 | pipeline(task=“automatic-speech-recognition”) |'
- en: '| Visual question answering | answer a question about the image, given an image
    and a question | Multimodal | pipeline(task=“vqa”) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 视觉问答 | 回答关于图像的问题，给定一个图像和一个问题 | 多模态 | pipeline(task=“vqa”) |'
- en: '| Document question answering | answer a question about the document, given
    a document and a question | Multimodal | pipeline(task=“document-question-answering”)
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 文档问答 | 回答关于文档的问题，给定一个文档和一个问题 | 多模态 | pipeline(task=“document-question-answering”)
    |'
- en: '| Image captioning | generate a caption for a given image | Multimodal | pipeline(task=“image-to-text”)
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 图像字幕 | 为给定图像生成字幕 | 多模态 | pipeline(task=“image-to-text”) |'
- en: 'Start by creating an instance of [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    and specifying a task you want to use it for. In this guide, you’ll use the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for sentiment analysis as an example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)实例，并指定要用它进行的任务。在本指南中，您将使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)进行情感分析作为示例：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)
    and tokenizer for sentiment analysis. Now you can use the `classifier` on your
    target text:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)会下载并缓存一个默认的[预训练模型](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)和情感分析的分词器。现在您可以在目标文本上使用`classifier`：'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you have more than one input, pass your inputs as a list to the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    to return a list of dictionaries:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有多个输入，请将输入作为列表传递给[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)以返回一个字典列表：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    can also iterate over an entire dataset for any task you like. For this example,
    let’s choose automatic speech recognition as our task:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)还可以迭代处理任何您喜欢的任务的整个数据集。在此示例中，让我们选择自动语音识别作为我们的任务：'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Load an audio dataset (see the 🤗 Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio)
    for more details) you’d like to iterate over. For example, load the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
    dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 加载音频数据集（有关更多详细信息，请参阅🤗数据集[快速入门](https://huggingface.co/docs/datasets/quickstart#audio)）。例如，加载[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)数据集：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You need to make sure the sampling rate of the dataset matches the sampling
    rate [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h)
    was trained on:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要确保数据集的采样率与[`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h)训练时的采样率匹配：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The audio files are automatically loaded and resampled when calling the `"audio"`
    column. Extract the raw waveform arrays from the first 4 samples and pass it as
    a list to the pipeline:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用“audio”列时，音频文件将自动加载并重新采样。从前 4 个样本中提取原始波形数组，并将其作为列表传递给管道：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For larger datasets where the inputs are big (like in speech or vision), you’ll
    want to pass a generator instead of a list to load all the inputs in memory. Take
    a look at the [pipeline API reference](./main_classes/pipelines) for more information.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入数据量较大的情况（比如语音或视觉），您将希望传递一个生成器而不是列表，以将所有输入加载到内存中。查看[pipeline API 参考](./main_classes/pipelines)以获取更多信息。
- en: Use another model and tokenizer in the pipeline
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在管道中使用另一个模型和分词器
- en: 'The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    can accommodate any model from the [Hub](https://huggingface.co/models), making
    it easy to adapt the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for other use-cases. For example, if you’d like a model capable of handling French
    text, use the tags on the Hub to filter for an appropriate model. The top filtered
    result returns a multilingual [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)
    finetuned for sentiment analysis you can use for French text:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)可以适应[Hub](https://huggingface.co/models)中的任何模型，从而可以轻松地调整[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)以适应其他用例。例如，如果您需要一个能够处理法语文本的模型，请使用
    Hub 上的标签来过滤适当的模型。顶部过滤结果返回一个针对情感分析进行微调的多语言[BERT 模型](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)，您可以用于法语文本：'
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: PytorchHide Pytorch content
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch 内容
- en: 'Use [AutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification)
    and [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    to load the pretrained model and it’s associated tokenizer (more on an `AutoClass`
    in the next section):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来加载预训练模型及其关联的分词器（关于`AutoClass`的更多信息请参见下一节）：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: TensorFlowHide TensorFlow content
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow 内容
- en: 'Use [TFAutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)
    and [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    to load the pretrained model and it’s associated tokenizer (more on an `TFAutoClass`
    in the next section):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[TFAutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来加载预训练模型及其关联的分词器（关于`TFAutoClass`的更多信息请参见下一节）：
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Specify the model and tokenizer in the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline),
    and now you can apply the `classifier` on French text:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中指定模型和分词器，现在您可以在法语文本上应用`classifier`：
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you can’t find a model for your use-case, you’ll need to finetune a pretrained
    model on your data. Take a look at our [finetuning tutorial](./training) to learn
    how. Finally, after you’ve finetuned your pretrained model, please consider [sharing](./model_sharing)
    the model with the community on the Hub to democratize machine learning for everyone!
    🤗
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找不到适合您用例的模型，您需要在您的数据上对预训练模型进行微调。查看我们的[微调教程](./training)以了解如何操作。最后，在微调预训练模型后，请考虑在
    Hub 上[共享](./model_sharing)该模型，以使机器学习民主化！🤗
- en: AutoClass
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoClass
- en: '[https://www.youtube-nocookie.com/embed/AhChOFRegn4](https://www.youtube-nocookie.com/embed/AhChOFRegn4)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/AhChOFRegn4](https://www.youtube-nocookie.com/embed/AhChOFRegn4)'
- en: Under the hood, the [AutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification)
    and [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    classes work together to power the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    you used above. An [AutoClass](./model_doc/auto) is a shortcut that automatically
    retrieves the architecture of a pretrained model from its name or path. You only
    need to select the appropriate `AutoClass` for your task and it’s associated preprocessing
    class.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，[AutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)类共同驱动您上面使用的[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)。[AutoClass](./model_doc/auto)是一个快捷方式，可以根据预训练模型的名称或路径自动检索架构。您只需要为您的任务选择适当的`AutoClass`及其关联的预处理类。
- en: Let’s return to the example from the previous section and see how you can use
    the `AutoClass` to replicate the results of the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到前一节的示例，看看如何使用`AutoClass`来复制[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)的结果。
- en: AutoTokenizer
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoTokenizer
- en: A tokenizer is responsible for preprocessing text into an array of numbers as
    inputs to a model. There are multiple rules that govern the tokenization process,
    including how to split a word and at what level words should be split (learn more
    about tokenization in the [tokenizer summary](./tokenizer_summary)). The most
    important thing to remember is you need to instantiate a tokenizer with the same
    model name to ensure you’re using the same tokenization rules a model was pretrained
    with.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器负责将文本预处理为输入模型的数字数组。有多个规则管理标记化过程，包括如何拆分单词以及单词应该在什么级别拆分（在[分词器摘要](./tokenizer_summary)中了解更多关于分词的信息）。最重要的是要记住，您需要使用相同模型名称实例化分词器，以确保您使用与模型预训练时相同的标记化规则。
- en: 'Load a tokenizer with [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)加载分词器：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Pass your text to the tokenizer:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本传递给分词器：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The tokenizer returns a dictionary containing:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器返回一个包含的字典：
- en: '[input_ids](./glossary#input-ids): numerical representations of your tokens.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[input_ids](./glossary#input-ids)：您的标记的数值表示。'
- en: '[attention_mask](.glossary#attention-mask): indicates which tokens should be
    attended to.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[attention_mask](.glossary#attention-mask)：指示应该关注哪些标记。'
- en: 'A tokenizer can also accept a list of inputs, and pad and truncate the text
    to return a batch with uniform length:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器还可以接受输入列表，并填充和截断文本以返回具有统一长度的批处理：
- en: PytorchHide Pytorch content
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: TensorFlowHide TensorFlow content
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Check out the [preprocess](./preprocessing) tutorial for more details about
    tokenization, and how to use an [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor),
    [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    and [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    to preprocess image, audio, and multimodal inputs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[预处理](./preprocessing)教程，了解有关分词以及如何使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)、[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)和[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)预处理图像、音频和多模态输入的更多详细信息。
- en: AutoModel
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoModel
- en: PytorchHide Pytorch content
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: '🤗 Transformers provides a simple and unified way to load pretrained instances.
    This means you can load an [AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)
    like you would load an [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    The only difference is selecting the correct [AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)
    for the task. For text (or sequence) classification, you should load [AutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers提供了一种简单而统一的方式来加载预训练实例。这意味着您可以加载一个[AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)，就像加载[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)一样。唯一的区别是选择正确的[AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)用于任务。对于文本（或序列）分类，您应该加载[AutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification)：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: See the [task summary](./task_summary) for tasks supported by an [AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)
    class.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[任务摘要](./task_summary)以了解[AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)类支持的任务。
- en: 'Now pass your preprocessed batch of inputs directly to the model. You just
    have to unpack the dictionary by adding `**`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在直接将预处理的输入批次传递给模型。您只需通过添加`**`来解包字典：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The model outputs the final activations in the `logits` attribute. Apply the
    softmax function to the `logits` to retrieve the probabilities:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在`logits`属性中输出最终激活值。将softmax函数应用于`logits`以检索概率：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TensorFlowHide TensorFlow content
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: '🤗 Transformers provides a simple and unified way to load pretrained instances.
    This means you can load an [TFAutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModel)
    like you would load an [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    The only difference is selecting the correct [TFAutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModel)
    for the task. For text (or sequence) classification, you should load [TFAutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers提供了一种简单而统一的方式来加载预训练实例。这意味着您可以加载一个[TFAutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModel)，就像加载[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)一样。唯一的区别是选择正确的[TFAutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModel)用于任务。对于文本（或序列）分类，您应该加载[TFAutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: See the [task summary](./task_summary) for tasks supported by an [AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)
    class.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[任务摘要](./task_summary)以了解[AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)类支持的任务。
- en: 'Now pass your preprocessed batch of inputs directly to the model. You can pass
    the tensors as-is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在直接将预处理的输入批次传递给模型。您可以直接传递张量：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The model outputs the final activations in the `logits` attribute. Apply the
    softmax function to the `logits` to retrieve the probabilities:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在`logits`属性中输出最终激活值。将softmax函数应用于`logits`以检索概率：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: All 🤗 Transformers models (PyTorch or TensorFlow) output the tensors *before*
    the final activation function (like softmax) because the final activation function
    is often fused with the loss. Model outputs are special dataclasses so their attributes
    are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary
    (you can index with an integer, a slice or a string) in which case, attributes
    that are None are ignored.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所有🤗 Transformers 模型（PyTorch 或 TensorFlow）在最终激活函数（如 softmax）之前输出张量，因为最终激活函数通常与损失融合在一起。模型输出是特殊的数据类，因此在
    IDE 中可以自动完成其属性。模型输出的行为类似于元组或字典（可以使用整数、切片或字符串进行索引），在这种情况下，空属性将被忽略。
- en: Save a model
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存模型
- en: PytorchHide Pytorch content
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch 内容
- en: 'Once your model is fine-tuned, you can save it with its tokenizer using [PreTrainedModel.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的模型微调完成，您可以使用[PreTrainedModel.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存模型及其分词器：
- en: '[PRE24]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When you are ready to use the model again, reload it with [PreTrainedModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当您准备再次使用模型时，请使用[PreTrainedModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)重新加载它：
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: TensorFlowHide TensorFlow content
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow 内容
- en: 'Once your model is fine-tuned, you can save it with its tokenizer using [TFPreTrainedModel.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的模型微调完成，您可以使用[TFPreTrainedModel.save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)保存模型及其分词器：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When you are ready to use the model again, reload it with [TFPreTrainedModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当您准备再次使用模型时，请使用[TFPreTrainedModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)重新加载它：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'One particularly cool 🤗 Transformers feature is the ability to save a model
    and reload it as either a PyTorch or TensorFlow model. The `from_pt` or `from_tf`
    parameter can convert the model from one framework to the other:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers 的一个特别酷的功能是能够将模型保存并重新加载为 PyTorch 或 TensorFlow 模型。`from_pt` 或 `from_tf`
    参数可以将模型从一个框架转换为另一个框架：
- en: PytorchHide Pytorch content
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch 内容
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: TensorFlowHide TensorFlow content
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow 内容
- en: '[PRE29]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Custom model builds
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义模型构建
- en: You can modify the model’s configuration class to change how a model is built.
    The configuration specifies a model’s attributes, such as the number of hidden
    layers or attention heads. You start from scratch when you initialize a model
    from a custom configuration class. The model attributes are randomly initialized,
    and you’ll need to train the model before you can use it to get meaningful results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以修改模型的配置类以更改模型的构建方式。配置指定模型的属性，例如隐藏层或注意力头的数量。当您从自定义配置类初始化模型时，您将从头开始。模型属性是随机初始化的，您需要在使用它以获得有意义的结果之前对模型进行训练。
- en: 'Start by importing [AutoConfig](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoConfig),
    and then load the pretrained model you want to modify. Within [AutoConfig.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoConfig.from_pretrained),
    you can specify the attribute you want to change, such as the number of attention
    heads:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入[AutoConfig](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoConfig)，然后加载您想要修改的预训练模型。在[AutoConfig.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoConfig.from_pretrained)中，您可以指定要更改的属性，比如注意力头的数量：
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: PytorchHide Pytorch content
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch 内容
- en: 'Create a model from your custom configuration with [AutoModel.from_config()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoModel.from_config()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config)从您的自定义配置创建模型：
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: TensorFlowHide TensorFlow content
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow 内容
- en: 'Create a model from your custom configuration with [TFAutoModel.from_config()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[TFAutoModel.from_config()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config)从您的自定义配置创建模型：
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Take a look at the [Create a custom architecture](./create_a_model) guide for
    more information about building custom configurations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[创建自定义架构](./create_a_model)指南，了解有关构建自定义配置的更多信息。
- en: Trainer - a PyTorch optimized training loop
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Trainer - 一个 PyTorch 优化的训练循环
- en: All models are a standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    so you can use them in any typical training loop. While you can write your own
    training loop, 🤗 Transformers provides a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class for PyTorch, which contains the basic training loop and adds additional
    functionality for features like distributed training, mixed precision, and more.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都是标准的[`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)，因此您可以在任何典型的训练循环中使用它们。虽然您可以编写自己的训练循环，🤗
    Transformers 提供了一个用于 PyTorch 的[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类，其中包含基本的训练循环，并添加了额外的功能，如分布式训练、混合精度等。
- en: 'Depending on your task, you’ll typically pass the following parameters to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的任务，通常会将以下参数传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)：
- en: 'You’ll start with a [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or a [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将从[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)开始：
- en: '[PRE33]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    contains the model hyperparameters you can change like learning rate, batch size,
    and the number of epochs to train for. The default values are used if you don’t
    specify any training arguments:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)包含您可以更改的模型超参数，如学习率、批量大小和训练的时代数。如果您不指定任何训练参数，将使用默认值：'
- en: '[PRE34]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Load a preprocessing class like a tokenizer, image processor, feature extractor,
    or processor:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个预处理类，比如分词器、图像处理器、特征提取器或处理器：
- en: '[PRE35]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Load a dataset:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE36]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create a function to tokenize the dataset:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来对数据集进行分词：
- en: '[PRE37]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then apply it over the entire dataset with [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map):'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后在整个数据集上应用它，使用[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)：
- en: '[PRE38]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'A [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)
    to create a batch of examples from your dataset:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个[DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)来从您的数据集中创建一批示例：
- en: '[PRE39]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now gather all these classes in [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将所有这些类聚集在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)中：
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'When you’re ready, call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to start training:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当你准备好时，调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)开始训练：
- en: '[PRE41]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: For tasks - like translation or summarization - that use a sequence-to-sequence
    model, use the [Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    and [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)
    classes instead.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用序列到序列模型的任务，比如翻译或摘要，使用[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)和[Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)类。
- en: You can customize the training loop behavior by subclassing the methods inside
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    This allows you to customize features such as the loss function, optimizer, and
    scheduler. Take a look at the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    reference for which methods can be subclassed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过对[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)中的方法进行子类化来自定义训练循环行为。这样可以自定义特性，如损失函数、优化器和调度器。查看[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)参考，了解哪些方法可以被子类化。
- en: The other way to customize the training loop is by using [Callbacks](./main_classes/callbacks).
    You can use callbacks to integrate with other libraries and inspect the training
    loop to report on progress or stop the training early. Callbacks do not modify
    anything in the training loop itself. To customize something like the loss function,
    you need to subclass the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    instead.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种自定义训练循环的方法是使用[Callbacks](./main_classes/callbacks)。您可以使用回调函数与其他库集成，并检查训练循环以报告进度或提前停止训练。回调函数不会修改训练循环本身。要自定义像损失函数这样的东西，您需要对[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)进行子类化。
- en: Train with TensorFlow
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行训练
- en: All models are a standard [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    so they can be trained in TensorFlow with the [Keras](https://keras.io/) API.
    🤗 Transformers provides the [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)
    method to easily load your dataset as a `tf.data.Dataset` so you can start training
    right away with Keras’ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method)
    and [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) methods.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都是标准的[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)，因此它们可以在TensorFlow中使用[Keras](https://keras.io/)
    API进行训练。🤗 Transformers提供了[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)方法，可以轻松将数据集加载为`tf.data.Dataset`，这样您就可以立即开始使用Keras的[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)和[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)方法进行训练。
- en: 'You’ll start with a [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    or a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model):'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将从[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)或[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)开始：
- en: '[PRE42]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Load a preprocessing class like a tokenizer, image processor, feature extractor,
    or processor:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个预处理类，比如分词器、图像处理器、特征提取器或处理器：
- en: '[PRE43]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create a function to tokenize the dataset:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来对数据集进行分词：
- en: '[PRE44]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Apply the tokenizer over the entire dataset with [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    and then pass the dataset and tokenizer to [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset).
    You can also change the batch size and shuffle the dataset here if you’d like:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)在整个数据集上应用分词器，然后将数据集和分词器传递给[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)。如果需要，您还可以在这里更改批量大小和对数据集进行洗牌：
- en: '[PRE45]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'When you’re ready, you can call `compile` and `fit` to start training. Note
    that Transformers models all have a default task-relevant loss function, so you
    don’t need to specify one unless you want to:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您准备好时，您可以调用`compile`和`fit`开始训练。请注意，Transformers模型都有一个默认的与任务相关的损失函数，所以除非您想要，否则不需要指定一个：
- en: '[PRE46]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: What’s next?
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: Now that you’ve completed the 🤗 Transformers quick tour, check out our guides
    and learn how to do more specific things like writing a custom model, fine-tuning
    a model for a task, and how to train a model with a script. If you’re interested
    in learning more about 🤗 Transformers core concepts, grab a cup of coffee and
    take a look at our Conceptual Guides!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经完成了🤗 Transformers的快速导览，请查看我们的指南，学习如何做更具体的事情，比如编写自定义模型，为任务微调模型，以及如何使用脚本训练模型。如果您对学习更多关于🤗
    Transformers核心概念感兴趣，请拿杯咖啡，看看我们的概念指南！
