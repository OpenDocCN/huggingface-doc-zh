- en: Preparing the Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备模型
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model](https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model](https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference improves the model in several aspects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成推断在几个方面改进了模型。
- en: Quantization
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: TGI supports [bits-and-bytes](https://github.com/TimDettmers/bitsandbytes#bitsandbytes),
    [GPT-Q](https://arxiv.org/abs/2210.17323) and [AWQ](https://arxiv.org/abs/2306.00978)
    quantization. To speed up inference with quantization, simply set `quantize` flag
    to `bitsandbytes`, `gptq` or `awq` depending on the quantization technique you
    wish to use. When using GPT-Q quantization, you need to point to one of the models
    [here](https://huggingface.co/models?search=gptq) when using AWQ quantization,
    you need to point to one of the models [here](https://huggingface.co/models?search=awq).
    To get more information about quantization, please refer to [quantization guide](./../conceptual/quantization)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TGI支持[bits-and-bytes](https://github.com/TimDettmers/bitsandbytes#bitsandbytes)，[GPT-Q](https://arxiv.org/abs/2210.17323)和[AWQ](https://arxiv.org/abs/2306.00978)量化。为了加速量化推断，只需将`quantize`标志设置为`bitsandbytes`，`gptq`或`awq`，具体取决于您希望使用的量化技术。当使用GPT-Q量化时，您需要指向其中一个模型[这里](https://huggingface.co/models?search=gptq)，当使用AWQ量化时，您需要指向其中一个模型[这里](https://huggingface.co/models?search=awq)。要获取有关量化的更多信息，请参考[量化指南](./../conceptual/quantization)
- en: RoPE Scaling
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RoPE缩放
- en: RoPE scaling can be used to increase the sequence length of the model during
    the inference time without necessarily fine-tuning it. To enable RoPE scaling,
    simply pass `--rope-scaling`, `--max-input-length` and `--rope-factors` flags
    when running through CLI. `--rope-scaling` can take the values `linear` or `dynamic`.
    If your model is not fine-tuned to a longer sequence length, use `dynamic`. `--rope-factor`
    is the ratio between the intended max sequence length and the model’s original
    max sequence length. Make sure to pass `--max-input-length` to provide maximum
    input length for extension.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: RoPE缩放可用于在推断时增加模型的序列长度，而无需必要地微调它。要启用RoPE缩放，只需在通过CLI运行时传递`--rope-scaling`，`--max-input-length`和`--rope-factors`标志。`--rope-scaling`可以取值`linear`或`dynamic`。如果您的模型没有微调到更长的序列长度，请使用`dynamic`。`--rope-factor`是预期最大序列长度与模型原始最大序列长度之间的比率。确保传递`--max-input-length`以提供扩展的最大输入长度。
- en: We recommend using `dynamic` RoPE scaling.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用`dynamic` RoPE缩放。
- en: Safetensors
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Safetensors
- en: '[Safetensors](https://github.com/huggingface/safetensors) is a fast and safe
    persistence format for deep learning models, and is required for tensor parallelism.
    TGI supports `safetensors` model loading under the hood. By default, given a repository
    with `safetensors` and `pytorch` weights, TGI will always load `safetensors`.
    If there’s no `pytorch` weights, TGI will convert the weights to `safetensors`
    format.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[Safetensors](https://github.com/huggingface/safetensors)是用于深度学习模型的快速且安全的持久化格式，对于张量并行性是必需的。TGI在底层支持`Safetensors`模型加载。默认情况下，给定一个带有`safetensors`和`pytorch`权重的存储库，TGI将始终加载`safetensors`。如果没有`pytorch`权重，TGI将把权重转换为`safetensors`格式。'
