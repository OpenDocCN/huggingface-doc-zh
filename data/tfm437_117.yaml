- en: Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Each framework has a generate method for text generation implemented in their
    respective `GenerationMixin` class:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 每个框架都有一个用于文本生成的`GenerationMixin`类中实现的生成方法：
- en: PyTorch [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    is implemented in [GenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch的[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)在[GenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin)中实现。
- en: TensorFlow [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)
    is implemented in [TFGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow的[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)在[TFGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin)中实现。
- en: Flax/JAX [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate)
    is implemented in [FlaxGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flax/JAX的[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate)在[FlaxGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin)中实现。
- en: Regardless of your framework of choice, you can parameterize the generate method
    with a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    class instance. Please refer to this class for the complete list of generation
    parameters, which control the behavior of the generation method.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪个框架，您都可以使用[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)类实例对生成方法进行参数化。请参考此类以获取完整的生成参数列表，这些参数控制生成方法的行为。
- en: To learn how to inspect a model’s generation configuration, what are the defaults,
    how to change the parameters ad hoc, and how to create and save a customized generation
    configuration, refer to the [text generation strategies guide](../generation_strategies).
    The guide also explains how to use related features, like token streaming.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何检查模型的生成配置，了解默认值，如何临时更改参数以及如何创建和保存自定义生成配置，请参考[文本生成策略指南](../generation_strategies)。该指南还解释了如何使用相关功能，如标记流。
- en: GenerationConfig
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GenerationConfig
- en: '### `class transformers.GenerationConfig`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GenerationConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L40)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L40)'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters that control the length of the output
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 控制输出长度的参数
- en: '`max_length` (`int`, *optional*, defaults to 20) — The maximum length the generated
    tokens can have. Corresponds to the length of the input prompt + `max_new_tokens`.
    Its effect is overridden by `max_new_tokens`, if also set.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*，默认为20）— 生成的标记可以具有的最大长度。对应于输入提示的长度+`max_new_tokens`。如果也设置了`max_new_tokens`，则其效果将被覆盖。'
- en: '`max_new_tokens` (`int`, *optional*) — The maximum numbers of tokens to generate,
    ignoring the number of tokens in the prompt.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens`（`int`，*可选*）— 生成的最大标记数，忽略提示中的标记数。'
- en: '`min_length` (`int`, *optional*, defaults to 0) — The minimum length of the
    sequence to be generated. Corresponds to the length of the input prompt + `min_new_tokens`.
    Its effect is overridden by `min_new_tokens`, if also set.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_length`（`int`，*可选*，默认为0）— 要生成的序列的最小长度。对应于输入提示的长度+`min_new_tokens`。如果也设置了`min_new_tokens`，则其效果将被覆盖。'
- en: '`min_new_tokens` (`int`, *optional*) — The minimum numbers of tokens to generate,
    ignoring the number of tokens in the prompt.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_new_tokens`（`int`，*可选*）— 生成的最小标记数，忽略提示中的标记数。'
- en: '`early_stopping` (`bool` or `str`, *optional*, defaults to `False`) — Controls
    the stopping condition for beam-based methods, like beam-search. It accepts the
    following values: `True`, where the generation stops as soon as there are `num_beams`
    complete candidates; `False`, where an heuristic is applied and the generation
    stops when is it very unlikely to find better candidates; `"never"`, where the
    beam search procedure only stops when there cannot be better candidates (canonical
    beam search algorithm).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping`（`bool`或`str`，*可选*，默认为`False`）— 控制基于束搜索的方法的停止条件，如束搜索。它接受以下值：`True`，表示一旦有`num_beams`个完整候选项就停止生成；`False`，应用启发式方法，当很难找到更好的候选项时停止生成；`"never"`，束搜索过程仅在不能有更好的候选项时停止（经典的束搜索算法）。'
- en: '`max_time(float,` *optional*) — The maximum amount of time you allow the computation
    to run for in seconds. generation will still finish the current pass after allocated
    time has been passed.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_time(float,` *可选*) — 允许计算运行的最长时间（以秒为单位）。在分配的时间已过后，生成仍将完成当前传递。'
- en: Parameters that control the generation strategy used
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 控制生成策略使用的参数
- en: '`do_sample` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    sampling ; use greedy decoding otherwise.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_sample`（`bool`，*可选*，默认为`False`）— 是否使用采样；否则使用贪婪解码。'
- en: '`num_beams` (`int`, *optional*, defaults to 1) — Number of beams for beam search.
    1 means no beam search.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams`（`int`，*可选*，默认为1）— 用于束搜索的束数。1表示不进行束搜索。'
- en: '`num_beam_groups` (`int`, *optional*, defaults to 1) — Number of groups to
    divide `num_beams` into in order to ensure diversity among different groups of
    beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beam_groups`（`int`，*可选*，默认为1）— 将`num_beams`分成多个组以确保不同组的束之间的多样性。有关更多详细信息，请参阅[此论文](https://arxiv.org/pdf/1610.02424.pdf)。'
- en: '`penalty_alpha` (`float`, *optional*) — The values balance the model confidence
    and the degeneration penalty in contrastive search decoding.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`penalty_alpha` (`float`, *optional*) — 这些值平衡了对比搜索解码中模型置信度和退化惩罚。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should use the past last key/values attentions (if applicable to the model) to
    speed up decoding.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应使用过去的最后键/值注意力（如果适用于模型）来加快解码速度。'
- en: Parameters for manipulation of the model output logits
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 用于操纵模型输出logits的参数
- en: '`temperature` (`float`, *optional*, defaults to 1.0) — The value used to modulate
    the next token probabilities.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` (`float`, *optional*, defaults to 1.0) — 用于调节下一个标记概率的值。'
- en: '`top_k` (`int`, *optional*, defaults to 50) — The number of highest probability
    vocabulary tokens to keep for top-k-filtering.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`, *optional*, defaults to 50) — 要保留的最高概率词汇标记的数量，用于进行top-k过滤。'
- en: '`top_p` (`float`, *optional*, defaults to 1.0) — If set to float < 1, only
    the smallest set of most probable tokens with probabilities that add up to `top_p`
    or higher are kept for generation.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p` (`float`, *optional*, defaults to 1.0) — 如果设置为小于1的浮点数，则仅保留概率加起来达到`top_p`或更高的最可能标记集合以进行生成。'
- en: '`typical_p` (`float`, *optional*, defaults to 1.0) — Local typicality measures
    how similar the conditional probability of predicting a target token next is to
    the expected conditional probability of predicting a random token next, given
    the partial text already generated. If set to float < 1, the smallest set of the
    most locally typical tokens with probabilities that add up to `typical_p` or higher
    are kept for generation. See [this paper](https://arxiv.org/pdf/2202.00666.pdf)
    for more details.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`typical_p` (`float`, *optional*, defaults to 1.0) — 本地典型性衡量了预测下一个目标标记的条件概率与预测随机下一个标记的条件概率之间的相似程度，给定已生成的部分文本。如果设置为小于1的浮点数，则保留概率加起来达到`typical_p`或更高的最典型标记集合以进行生成。有关更多详细信息，请参见[此论文](https://arxiv.org/pdf/2202.00666.pdf)。'
- en: '`epsilon_cutoff` (`float`, *optional*, defaults to 0.0) — If set to float strictly
    between 0 and 1, only tokens with a conditional probability greater than `epsilon_cutoff`
    will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending
    on the size of the model. See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191)
    for more details.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon_cutoff` (`float`, *optional*, defaults to 0.0) — 如果设置为0和1之间的浮点数，只有条件概率大于`epsilon_cutoff`的标记才会被采样。在论文中，建议的值范围从3e-4到9e-4，取决于模型的大小。有关更多详细信息，请参见[截断采样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。'
- en: '`eta_cutoff` (`float`, *optional*, defaults to 0.0) — Eta sampling is a hybrid
    of locally typical sampling and epsilon sampling. If set to float strictly between
    0 and 1, a token is only considered if it is greater than either `eta_cutoff`
    or `sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits)))`. The latter
    term is intuitively the expected next token probability, scaled by `sqrt(eta_cutoff)`.
    In the paper, suggested values range from 3e-4 to 2e-3, depending on the size
    of the model. See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191)
    for more details.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta_cutoff` (`float`, *optional*, defaults to 0.0) — Eta采样是局部典型采样和epsilon采样的混合。如果设置为0和1之间的浮点数，仅当一个标记大于`eta_cutoff`或`sqrt(eta_cutoff)
    * exp(-entropy(softmax(next_token_logits))`时才考虑。后一项直观上是下一个标记概率的期望，乘以`sqrt(eta_cutoff)`。在论文中，建议的值范围从3e-4到2e-3，取决于模型的大小。有关更多详细信息，请参见[截断采样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。'
- en: '`diversity_penalty` (`float`, *optional*, defaults to 0.0) — This value is
    subtracted from a beam’s score if it generates a token same as any beam from other
    group at a particular time. Note that `diversity_penalty` is only effective if
    `group beam search` is enabled.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_penalty` (`float`, *optional*, defaults to 0.0) — 如果一个beam在特定时间生成与其他组中的任何beam相同的标记，则从该beam的得分中减去此值。请注意，只有在启用`group
    beam search`时，`diversity_penalty`才有效。'
- en: '`repetition_penalty` (`float`, *optional*, defaults to 1.0) — The parameter
    for repetition penalty. 1.0 means no penalty. See [this paper](https://arxiv.org/pdf/1909.05858.pdf)
    for more details.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repetition_penalty` (`float`, *optional*, defaults to 1.0) — 重复惩罚的参数。1.0表示没有惩罚。有关更多详细信息，请参见[此论文](https://arxiv.org/pdf/1909.05858.pdf)。'
- en: '`encoder_repetition_penalty` (`float`, *optional*, defaults to 1.0) — The paramater
    for encoder_repetition_penalty. An exponential penalty on sequences that are not
    in the original input. 1.0 means no penalty.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_repetition_penalty` (`float`, *optional*, defaults to 1.0) — 编码器重复惩罚的参数。对于不在原始输入中的序列施加指数惩罚。1.0表示没有惩罚。'
- en: '`length_penalty` (`float`, *optional*, defaults to 1.0) — Exponential penalty
    to the length that is used with beam-based generation. It is applied as an exponent
    to the sequence length, which in turn is used to divide the score of the sequence.
    Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty`
    > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter
    sequences.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_penalty` (`float`, *optional*, defaults to 1.0) — 用于基于beam的生成的长度的指数惩罚。它作为指数应用于序列长度，然后用于分割序列的得分。由于得分是序列的对数似然（即负数），`length_penalty`
    > 0.0 会促进更长的序列，而`length_penalty` < 0.0 会鼓励更短的序列。'
- en: '`no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — If set to int >
    0, all ngrams of that size can only occur once.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — 如果设置为大于0的整数，那么该大小的所有ngram只能出现一次。'
- en: '`bad_words_ids(List[List[int]],` *optional*) — List of list of token ids that
    are not allowed to be generated. Check [NoBadWordsLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoBadWordsLogitsProcessor)
    for further documentation and examples.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bad_words_ids(List[List[int]],` *optional*) — 不允许生成的标记id列表。查看[NoBadWordsLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoBadWordsLogitsProcessor)以获取更多文档和示例。'
- en: '`force_words_ids(List[List[int]]` or `List[List[List[int]]]`, *optional*) —
    List of token ids that must be generated. If given a `List[List[int]]`, this is
    treated as a simple list of words that must be included, the opposite to `bad_words_ids`.
    If given `List[List[List[int]]]`, this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),
    where one can allow different forms of each word.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_words_ids(List[List[int]]` 或 `List[List[List[int]]]`, *optional*) —
    必须生成的标记id列表。如果给定 `List[List[int]]`，则将其视为必须包含的简单单词列表，与 `bad_words_ids` 相反。如果给定
    `List[List[List[int]]]`，这将触发一个[分离约束](https://github.com/huggingface/transformers/issues/14081)，其中可以允许每个单词的不同形式。'
- en: '`renormalize_logits` (`bool`, *optional*, defaults to `False`) — Whether to
    renormalize the logits after applying all the logits processors or warpers (including
    the custom ones). It’s highly recommended to set this flag to `True` as the search
    algorithms suppose the score logits are normalized but some logit processors or
    warpers break the normalization.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`renormalize_logits` (`bool`, *optional*, 默认为 `False`) — 在应用所有标记处理器或包装器（包括自定义的）后是否重新归一化标记。强烈建议将此标志设置为
    `True`，因为搜索算法假定得分标记已归一化，但某些标记处理器或包装器会破坏归一化。'
- en: '`constraints` (`List[Constraint]`, *optional*) — Custom constraints that can
    be added to the generation to ensure that the output will contain the use of certain
    tokens as defined by `Constraint` objects, in the most sensible way possible.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`constraints` (`List[Constraint]`, *optional*) — 可以添加到生成中的自定义约束，以确保输出将包含由 `Constraint`
    对象定义的某些标记的使用，以最合理的方式。'
- en: '`forced_bos_token_id` (`int`, *optional*, defaults to `model.config.forced_bos_token_id`)
    — The id of the token to force as the first generated token after the `decoder_start_token_id`.
    Useful for multilingual models like [mBART](../model_doc/mbart) where the first
    generated token needs to be the target language token.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forced_bos_token_id` (`int`, *optional*, 默认为 `model.config.forced_bos_token_id`)
    — 在 `decoder_start_token_id` 后强制作为第一个生成的标记的标记id。对于像 [mBART](../model_doc/mbart)
    这样的多语言模型很有用，其中第一个生成的标记需要是目标语言标记。'
- en: '`forced_eos_token_id` (`Union[int, List[int]]`, *optional*, defaults to `model.config.forced_eos_token_id`)
    — The id of the token to force as the last generated token when `max_length` is
    reached. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forced_eos_token_id` (`Union[int, List[int]]`, *optional*, 默认为 `model.config.forced_eos_token_id`)
    — 当达到 `max_length` 时，强制作为最后生成的标记的标记id。可选择使用列表设置多个 *end-of-sequence* 标记。'
- en: '`remove_invalid_values` (`bool`, *optional*, defaults to `model.config.remove_invalid_values`)
    — Whether to remove possible *nan* and *inf* outputs of the model to prevent the
    generation method to crash. Note that using `remove_invalid_values` can slow down
    generation.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_invalid_values` (`bool`, *optional*, 默认为 `model.config.remove_invalid_values`)
    — 是否删除模型可能的 *nan* 和 *inf* 输出，以防止生成方法崩溃。请注意，使用 `remove_invalid_values` 可以减慢生成速度。'
- en: '`exponential_decay_length_penalty` (`tuple(int, float)`, *optional*) — This
    Tuple adds an exponentially increasing length penalty, after a certain amount
    of tokens have been generated. The tuple shall consist of: `(start_index, decay_factor)`
    where `start_index` indicates where penalty starts and `decay_factor` represents
    the factor of exponential decay'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exponential_decay_length_penalty` (`tuple(int, float)`, *optional*) — 此元组在生成一定数量的标记后添加指数增长的长度惩罚。元组应包含：`(start_index,
    decay_factor)`，其中 `start_index` 表示惩罚开始的位置，`decay_factor` 表示指数衰减的因子。'
- en: '`suppress_tokens` (`List[int]`, *optional*) — A list of tokens that will be
    suppressed at generation. The `SupressTokens` logit processor will set their log
    probs to `-inf` so that they are not sampled.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suppress_tokens` (`List[int]`, *optional*) — 生成时将被抑制的标记列表。`SupressTokens`
    标记处理器将将它们的对数概率设置为 `-inf`，以便它们不被抽样。'
- en: '`begin_suppress_tokens` (`List[int]`, *optional*) — A list of tokens that will
    be suppressed at the beginning of the generation. The `SupressBeginTokens` logit
    processor will set their log probs to `-inf` so that they are not sampled.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`begin_suppress_tokens` (`List[int]`, *optional*) — 生成开始时将被抑制的标记列表。`SupressBeginTokens`
    标记处理器将将它们的对数概率设置为 `-inf`，以便它们不被抽样。'
- en: '`forced_decoder_ids` (`List[List[int]]`, *optional*) — A list of pairs of integers
    which indicates a mapping from generation indices to token indices that will be
    forced before sampling. For example, `[[1, 123]]` means the second generated token
    will always be a token of index 123.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forced_decoder_ids` (`List[List[int]]`, *optional*) — 一对整数的列表，指示在抽样之前将强制执行的生成索引到标记索引的映射。例如，`[[1,
    123]]` 表示第二个生成的标记将始终是索引为 123 的标记。'
- en: '`sequence_bias` (`Dict[Tuple[int], float]`, *optional*)) — Dictionary that
    maps a sequence of tokens to its bias term. Positive biases increase the odds
    of the sequence being selected, while negative biases do the opposite. Check [SequenceBiasLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SequenceBiasLogitsProcessor)
    for further documentation and examples.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_bias` (`Dict[Tuple[int], float]`, *optional*)) — 将标记序列映射到其偏差项的字典。正偏差增加选择该序列的几率，而负偏差则相反。查看[SequenceBiasLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SequenceBiasLogitsProcessor)以获取更多文档和示例。'
- en: '`guidance_scale` (`float`, *optional*) — The guidance scale for classifier
    free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`. Higher guidance
    scale encourages the model to generate samples that are more closely linked to
    the input prompt, usually at the expense of poorer quality.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*) — 分类器自由引导（CFG）的引导比例。通过设置 `guidance_scale
    > 1` 启用 CFG。更高的引导比例鼓励模型生成与输入提示更紧密相关的样本，通常以牺牲质量为代价。'
- en: '`low_memory` (`bool`, *optional*) — Switch to sequential topk for contrastive
    search to reduce peak memory. Used with contrastive search.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_memory` (`bool`, *optional*) — 用于对比搜索的顺序topk开关，以减少内存峰值。与对比搜索一起使用。'
- en: Parameters that define the output variables of `generate`
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate` 输出变量的定义参数'
- en: '`num_return_sequences(int,` *optional*, defaults to 1) — The number of independently
    computed returned sequences for each element in the batch.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_return_sequences(int,` *optional*, 默认为 1) — 每个批次中每个元素独立计算返回序列的数量。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*, defaults to `False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的
    `hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores` (`bool`, *optional*, defaults to `False`) — 是否返回预测分数。有关更多详细信息，请参阅返回张量中的
    `scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — 是否返回
    [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Special tokens that can be used at generation time
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 可在生成时使用的特殊标记
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*) — *填充*标记的 id。'
- en: '`bos_token_id` (`int`, *optional*) — The id of the *beginning-of-sequence*
    token.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*) — *序列开始*标记的 id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — *序列结束*标记的 id。可选择使用列表设置多个*序列结束*标记。'
- en: Generation parameters exclusive to encoder-decoder models
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 专属于编码器-解码器模型的生成参数
- en: '`encoder_no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — If set
    to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot
    occur in the `decoder_input_ids`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — 如果设置为大于
    0 的整数，则在 `encoder_input_ids` 中出现的该大小的所有 n 元组不能出现在 `decoder_input_ids` 中。'
- en: '`decoder_start_token_id` (`int`, *optional*) — If an encoder-decoder model
    starts decoding with a different token than *bos*, the id of that token.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_start_token_id` (`int`, *optional*) — 如果编码器-解码器模型开始解码时使用与 *bos* 不同的标记，则为该标记的
    id。'
- en: Generation parameters exclusive to [assistant generation](https
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 专属于 [助手生成](https
- en: '`num_assistant_tokens` (`int`, *optional*, defaults to 5) — Defines the number
    of *speculative tokens* that shall be generated by the assistant model before
    being checked by the target model at each iteration. Higher values for `num_assistant_tokens`
    make the generation more *speculative* : If the assistant model is performant
    larger speed-ups can be reached, if the assistant model requires lots of corrections,
    lower speed-ups are reached.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_assistant_tokens` (`int`, *optional*, defaults to 5) — 定义助手模型在每次迭代之前生成的*推测标记*数量。`num_assistant_tokens`
    的值越高，生成的*推测性*越强：如果助手模型表现良好，可以实现更大的加速，如果助手模型需要大量修正，则实现的加速度较低。'
- en: '`num_assistant_tokens_schedule` (`str`, *optional*, defaults to `"heuristic"`)
    — Defines the schedule at which max assistant tokens shall be changed during inference.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_assistant_tokens_schedule` (`str`, *optional*, defaults to `"heuristic"`)
    — 定义推断期间最大助手标记应该如何更改的计划。'
- en: '`"_heuristic_`: When all *speculative* tokens are correct, increase `num_assistant_tokens`
    by 2 else reduce by 1'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"_heuristic_`: 当所有*推测*标记都正确时，将 `num_assistant_tokens` 增加 2，否则减少 1'
- en: '`"constant"`: `num_assistant_tokens` stays unchanged during generation'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"constant"`: 在生成期间 `num_assistant_tokens` 保持不变'
- en: Wild card
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通配符
- en: 'Class that holds a configuration for a generation task. A `generate` call supports
    the following generation methods for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成任务配置的类。`generate` 调用支持以下文本解码器、文本到文本、语音到文本和视觉到文本模型的生成方法：
- en: '*greedy decoding* by calling [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)
    if `num_beams=1` and `do_sample=False`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用 [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)
    实现*贪婪解码*，如果 `num_beams=1` 且 `do_sample=False`
- en: '*contrastive search* by calling [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)
    if `penalty_alpha>0.` and `top_k>1`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用 [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)
    实现*对比搜索*，如果 `penalty_alpha>0.` 且 `top_k>1`
- en: '*multinomial sampling* by calling [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)
    if `num_beams=1` and `do_sample=True`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用 [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)
    实现*多项式采样*，如果 `num_beams=1` 且 `do_sample=True`
- en: '*beam-search decoding* by calling [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    if `num_beams>1` and `do_sample=False`'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用 [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    实现*束搜索解码*，如果 `num_beams>1` 且 `do_sample=False`
- en: '*beam-search multinomial sampling* by calling [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)
    if `num_beams>1` and `do_sample=True`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用 [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)
    实现*束搜索多项式采样*，如果 `num_beams>1` 且 `do_sample=True`
- en: '*diverse beam-search decoding* by calling [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search),
    if `num_beams>1` and `num_beam_groups>1`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用 [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)
    实现*多样束搜索解码*，如果 `num_beams>1` 且 `num_beam_groups>1`
- en: '*constrained beam-search decoding* by calling [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search),
    if `constraints!=None` or `force_words_ids!=None`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用 [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)
    实现*受限束搜索解码*，如果 `constraints!=None` 或 `force_words_ids!=None`
- en: '*assisted decoding* by calling `assisted_decoding()`, if `assistant_model`
    is passed to `.generate()`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将`assistant_model`传递给`.generate()`来进行*辅助解码*调用`assisted_decoding()`。
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘.generate()‘. To learn more about decoding strategies refer to the
    [text generation strategies guide](../generation_strategies).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要直接调用上述任何方法。将自定义参数值传递给‘.generate()‘。要了解更多关于解码策略的信息，请参考[文本生成策略指南](../generation_strategies)。
- en: A large number of these flags control the logits or the stopping criteria of
    the generation. Make sure you check the [generate-related classes](https://huggingface.co/docs/transformers/internal/generation_utils)
    for a full description of the possible manipulations, as well as examples of their
    usage.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标志中的大部分控制生成的对数或停止标准。确保您查看[生成相关类](https://huggingface.co/docs/transformers/internal/generation_utils)以获取可能操作的完整描述，以及它们用法的示例。
- en: '#### `from_pretrained`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L605)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L605)'
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model_name` (`str` or `os.PathLike`) — This can be either:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name` (`str`或`os.PathLike`) — 这可以是：'
- en: a string, the *model id* of a pretrained model configuration hosted inside a
    model repo on huggingface.co. Valid model ids can be located at the root-level,
    like `bert-base-uncased`, or namespaced under a user or organization name, like
    `dbmdz/bert-base-german-cased`.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，预训练模型配置的*模型ID*，托管在huggingface.co模型存储库中。有效的模型ID可以位于根级别，如`bert-base-uncased`，或命名空间在用户或组织名称下，如`dbmdz/bert-base-german-cased`。
- en: a path to a *directory* containing a configuration file saved using the [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)方法保存的配置文件，例如，`./my_model_directory/`。
- en: '`config_file_name` (`str` or `os.PathLike`, *optional*, defaults to `"generation_config.json"`)
    — Name of the generation configuration JSON file to be loaded from `pretrained_model_name`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_file_name` (`str`或`os.PathLike`，*可选*，默认为`"generation_config.json"`)
    — 要从`pretrained_model_name`加载的生成配置JSON文件的名称。'
- en: '`cache_dir` (`str` or `os.PathLike`, *optional*) — Path to a directory in which
    a downloaded pretrained model configuration should be cached if the standard cache
    should not be used.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir` (`str`或`os.PathLike`，*可选*) — 下载预训练模型配置文件时应缓存的目录路径，如果不应使用标准缓存。'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force to (re-)download the configuration files and override the cached versions
    if they exist.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download` (`bool`，*可选*，默认为`False`) — 是否强制（重新）下载配置文件并覆盖缓存版本（如果存在）。'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to delete incompletely received file. Attempts to resume the download if such
    a file exists.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download` (`bool`，*可选*，默认为`False`) — 是否删除接收不完整的文件。如果存在这样的文件，则尝试恢复下载。'
- en: '`proxies` (`Dict[str, str]`, *optional*) — A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}.` The proxies are used on each request.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies` (`Dict[str, str]`，*可选*) — 一个按协议或端点使用的代理服务器字典，例如，`{''http'': ''foo.bar:3128'',
    ''http://hostname'': ''foo.bar:4012''}`。代理服务器在每个请求上使用。'
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str`或`bool`，*可选*) — 用作远程文件的HTTP令牌的令牌。如果为`True`，或未指定，将使用运行`huggingface-cli
    login`时生成的令牌（存储在`~/.huggingface`中）。'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`，*可选*，默认为`"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交ID，因为我们在huggingface.co上使用基于git的系统来存储模型和其他工件，所以`revision`可以是git允许的任何标识符。'
- en: To test a pull request you made on the Hub, you can pass `revision=“refs/pr/<pr_number>“.</pr_number>
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要测试您在Hub上提交的拉取请求，可以传递`revision=“refs/pr/<pr_number>“。</pr_number>
- en: '`return_unused_kwargs` (`bool`, *optional*, defaults to `False`) — If `False`,
    then this function returns just the final configuration object.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_unused_kwargs` (`bool`，*可选*，默认为`False`) — 如果为`False`，则此函数仅返回最终的配置对象。'
- en: 'If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where
    *unused_kwargs* is a dictionary consisting of the key/value pairs whose keys are
    not configuration attributes: i.e., the part of `kwargs` which has not been used
    to update `config` and is otherwise ignored.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果为`True`，则此函数返回一个`Tuple(config, unused_kwargs)`，其中*unused_kwargs*是一个字典，其中键/值对的键不是配置属性：即，未使用来更新`config`的`kwargs`的部分，否则将被忽略。
- en: '`subfolder` (`str`, *optional*, defaults to `""`) — In case the relevant files
    are located inside a subfolder of the model repo on huggingface.co, you can specify
    the folder name here.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subfolder` (`str`，*可选*，默认为`""`) — 如果相关文件位于huggingface.co模型存储库的子文件夹中，您可以在此处指定文件夹名称。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — The values in kwargs of any keys
    which are configuration attributes will be used to override the loaded values.
    Behavior concerning key/value pairs whose keys are *not* configuration attributes
    is controlled by the `return_unused_kwargs` keyword parameter.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`，*可选*) — 任何键的kwargs值，这些键是配置属性，将用于覆盖加载的值。关于键/值对的行为，其键*不*是配置属性，由`return_unused_kwargs`关键字参数控制。'
- en: Returns
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)'
- en: The configuration object instantiated from this pretrained model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个预训练模型实例化的配置对象。
- en: Instantiate a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    from a generation configuration file.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成配置文件实例化[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。
- en: 'Examples:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `from_model_config`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_model_config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L927)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L927)'
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model_config` (`PretrainedConfig`) — The model config that will be used to
    instantiate the generation config.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_config` (`PretrainedConfig`) — 将用于实例化生成配置的模型配置。'
- en: Returns
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)'
- en: The configuration object instantiated from those parameters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些参数实例化的配置对象。
- en: Instantiates a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    from a [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).
    This function is useful to convert legacy [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    objects, which may contain generation parameters, into a stand-alone [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)实例化[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。此函数可用于将可能包含生成参数的旧[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)对象转换为独立的[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。
- en: '#### `save_pretrained`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L529)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L529)'
- en: '[PRE4]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str` or `os.PathLike`) — Directory where the configuration
    JSON file will be saved (will be created if it does not exist).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory` (`str`或`os.PathLike`) — 将保存配置JSON文件的目录（如果不存在，将创建）。'
- en: '`config_file_name` (`str` or `os.PathLike`, *optional*, defaults to `"generation_config.json"`)
    — Name of the generation configuration JSON file to be saved in `save_directory`.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_file_name` (`str`或`os.PathLike`, *可选*, 默认为`"generation_config.json"`)
    — 要保存在`save_directory`中的生成配置JSON文件的名称。'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *可选*, 默认为 `False`) — 在保存模型后是否将其推送到Hugging Face模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`的名称）。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 传递给[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法的额外关键字参数。'
- en: Save a generation configuration object to the directory `save_directory`, so
    that it can be re-loaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained)
    class method.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成配置对象保存到目录`save_directory`，以便可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained)类方法重新加载它。
- en: GenerationMixin
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GenerationMixin
- en: '### `class transformers.GenerationMixin`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GenerationMixin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L321)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L321)'
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A class containing all functions for auto-regressive text generation, to be
    used as a mixin in [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含自回归文本生成所有函数的类，可作为[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)中的混合类使用。
- en: 'The class exposes [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    which can be used for:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该类公开[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)，可用于：
- en: '*greedy decoding* by calling [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)
    if `num_beams=1` and `do_sample=False`'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用[greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)实现*贪婪解码*，如果`num_beams=1`且`do_sample=False`
- en: '*contrastive search* by calling [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)
    if `penalty_alpha>0` and `top_k>1`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用[contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)实现*对比搜索*，如果`penalty_alpha>0`且`top_k>1`
- en: '*multinomial sampling* by calling [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)
    if `num_beams=1` and `do_sample=True`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用[sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)实现*多项式采样*，如果`num_beams=1`且`do_sample=True`
- en: '*beam-search decoding* by calling [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    if `num_beams>1` and `do_sample=False`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)实现*束搜索解码*，如果`num_beams>1`且`do_sample=False`
- en: '*beam-search multinomial sampling* by calling [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)
    if `num_beams>1` and `do_sample=True`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)实现*束搜索多项式采样*，如果`num_beams>1`且`do_sample=True`
- en: '*diverse beam-search decoding* by calling [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search),
    if `num_beams>1` and `num_beam_groups>1`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用[group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)实现*多样化的束搜索解码*，如果`num_beams>1`且`num_beam_groups>1`
- en: '*constrained beam-search decoding* by calling [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search),
    if `constraints!=None` or `force_words_ids!=None`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用[constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)实现*受限束搜索解码*，如果`constraints!=None`或`force_words_ids!=None`
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘generate’ instead. To learn more about decoding strategies refer to
    the [text generation strategies guide](../generation_strategies).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要直接调用上述任何方法。而是将自定义参数值传递给“generate”。要了解更多有关解码策略的信息，请参考[文本生成策略指南](../generation_strategies)。
- en: '#### `generate`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1173)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1173)'
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`torch.Tensor` of varying shape depending on the modality, *optional*)
    — The sequence used as a prompt for the generation or as model inputs to the encoder.
    If `None` the method initializes it with `bos_token_id` and a batch size of 1\.
    For decoder-only models `inputs` should of in the format of `input_ids`. For encoder-decoder
    models *inputs* can represent any of `input_ids`, `input_values`, `input_features`,
    or `pixel_values`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（根据模态性质变化的`torch.Tensor`，*可选*）— 用作生成提示或模型输入到编码器的序列。如果为`None`，则该方法将使用`bos_token_id`和批量大小为1进行初始化。对于仅解码器模型，`inputs`应该是`input_ids`格式。对于编码器-解码器模型，*inputs*可以表示任何`input_ids`、`input_values`、`input_features`或`pixel_values`之一。'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config`（`~generation.GenerationConfig`，*可选*）— 用作生成调用的基本参数化的生成配置。传递给generate的`**kwargs`与`generation_config`的属性匹配将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`（`LogitsProcessorList`，*可选*）— 自定义logits处理器，用于补充从参数和生成配置构建的默认logits处理器。如果传递了已经使用参数或生成配置创建的logit处理器，则会引发错误。此功能适用于高级用户。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. If your stopping
    criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`. This feature is intended for advanced users.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria`（`StoppingCriteriaList`，*可选*）— 自定义停止标准，用于补充从参数和生成配置构建的默认停止标准。如果传递了已经使用参数或生成配置创建的停止标准，则会引发错误。如果您的停止标准取决于`scores`输入，请确保将`return_dict_in_generate=True,
    output_scores=True`传递给`generate`。此功能适用于高级用户。'
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_allowed_tokens_fn`（`Callable[[int, torch.Tensor], List[int]]`，*可选*）—
    如果提供了此函数，则将束搜索限制为每个步骤仅允许的令牌。如果未提供，则不应用任何约束。此函数接受2个参数：批次ID `batch_id` 和 `input_ids`。它必须返回一个列表，其中包含下一代步骤的允许令牌，条件是批次ID
    `batch_id` 和先前生成的令牌 `inputs_ids`。此参数对于基于前缀的受限生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。'
- en: '`synced_gpus` (`bool`, *optional*) — Whether to continue running the while
    loop until max_length. Unless overridden this flag will be set to `True` under
    DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished
    generating before other GPUs. Otherwise it’ll be set to `False`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus`（`bool`，*可选*）— 是否继续运行while循环直到max_length。除非被覆盖，否则在DeepSpeed ZeRO
    Stage 3多GPU环境下，此标志将设置为`True`，以避免在其他GPU生成之前一个GPU完成生成时挂起。否则将设置为`False`。'
- en: '`assistant_model` (`PreTrainedModel`, *optional*) — An assistant model that
    can be used to accelerate generation. The assistant model must have the exact
    same tokenizer. The acceleration is achieved when forecasting candidate tokens
    with the assistent model is much faster than running generation with the model
    you’re calling generate from. As such, the assistant model should be much smaller.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assistant_model`（`PreTrainedModel`，*可选*）— 一个可以用来加速生成的助手模型。助手模型必须具有完全相同的分词器。当使用助手模型预测候选令牌比使用您调用generate的模型进行生成要快得多时，加速就会实现。因此，助手模型应该要小得多。'
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`streamer`（`BaseStreamer`，*可选*）— 将用于流式传输生成的序列的Streamer对象。生成的令牌通过`streamer.put(token_ids)`传递，Streamer负责任何进一步的处理。'
- en: '`negative_prompt_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — The negative prompt needed for some processors such as CFG. The
    batch size must match the input batch size. This is an experimental feature, subject
    to breaking API changes in future versions.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    一些处理器（如CFG）需要的负提示。批量大小必须与输入批量大小匹配。这是一个实验性功能，可能在未来版本中会有破坏性的API更改。'
- en: '`negative_prompt_attention_mask` (`torch.LongTensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Attention_mask for `negative_prompt_ids`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_attention_mask`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    用于`negative_prompt_ids`的Attention_mask。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）— `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。如果模型是编码器-解码器模型，则编码器特定的kwargs不应该有前缀，解码器特定的kwargs应该以*decoder_*为前缀。'
- en: Returns
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)或`torch.LongTensor`'
- en: A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个`torch.FloatTensor`。
- en: 'If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型*不是*编码器-解码器模型（`model.config.is_encoder_decoder=False`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型有：
- en: '[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput),'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)'
- en: '[GenerateBeamDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamDecoderOnlyOutput)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateBeamDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamDecoderOnlyOutput)'
- en: 'If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型是一个编码器-解码器模型（`model.config.is_encoder_decoder=True`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型有：
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
- en: Generates sequences of token ids for models with a language modeling head.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为具有语言建模头的模型生成令牌id序列。
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数控制生成的参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应的参数传递给generate()来覆盖任何`generation_config`，例如`.generate(inputs,
    num_beams=4, do_sample=True)`。
- en: For an overview of generation strategies and code examples, check out the [following
    guide](../generation_strategies).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: '#### `compute_transition_scores`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_transition_scores`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L919)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L919)'
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`torch.LongTensor`) — The generated sequences. The second dimension
    (sequence_length) is either equal to `max_length` or shorter if all batches finished
    early due to the `eos_token_id`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（`torch.LongTensor`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。'
- en: '`scores` (`tuple(torch.FloatTensor)`) — Transition scores for each vocabulary
    token at each generation step. Beam transition scores consisting of log probabilities
    of tokens conditioned on log softmax of previously generated tokens Tuple of `torch.FloatTensor`
    with up to `max_new_tokens` elements (one element for each generated token), with
    each tensor of shape `(batch_size*num_beams, config.vocab_size)`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（`tuple(torch.FloatTensor)`）— 每个生成步骤中每个词汇令牌的转移分数。由条件于先前生成令牌的对数softmax的令牌的对数概率组成的波束转移分数元组，其中每个张量的形状为`(batch_size*num_beams,
    config.vocab_size)`，最多有`max_new_tokens`个元素（每个生成的令牌一个元素）。'
- en: '`beam_indices` (`torch.LongTensor`, *optional*) — Beam indices of generated
    token id at each generation step. `torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`. Only required if a `num_beams>1` at generate-time.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices`（`torch.LongTensor`，*可选*）— 在每个生成步骤生成的标记id的波束索引。形状为`(batch_size*num_return_sequences,
    sequence_length)`的`torch.LongTensor`。仅在生成时`num_beams>1`时需要。'
- en: '`normalize_logits` (`bool`, *optional*, defaults to `False`) — Whether to normalize
    the logits (which, for legacy reasons, may be unnormalized).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_logits`（`bool`，*可选*，默认为`False`）— 是否对logits进行归一化（由于历史原因，可能未归一化）。'
- en: Returns
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`
    containing the transition scores (logits)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(batch_size*num_return_sequences, sequence_length)`的`torch.Tensor`，包含转换分数（logits）
- en: Computes the transition scores of sequences given the generation scores (and
    beam indices, if beam search was used). This is a convenient method to quicky
    obtain the scores of the selected tokens at generation time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 计算给定生成分数的序列的转换分数（如果使用波束搜索，则还有波束索引）。这是一种方便的方法，在生成时快速获取所选标记的分数。
- en: 'Examples:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#### `greedy_search`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `greedy_search`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2172)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2172)'
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 用作生成提示的序列。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`（`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)类的实例列表，用于修改应用于每个生成步骤的语言建模头的预测分数。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria`（`StoppingCriteriaList`，*可选*）— [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)类的实例列表，用于告知生成循环是否应该停止。'
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*，默认为20）— **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成标记的数量。要生成的序列的最大长度。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id`（`int`，*可选*）— *填充*标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*，默认为`False`）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*，默认为`False`）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores`（`bool`，*可选*，默认为`False`）— 是否返回预测分数。有关更多详细信息，请参见返回的张量下的`scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate`（`bool`，*可选*，默认为`False`）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus`（`bool`，*可选*，默认为`False`）— 是否继续运行while循环直到max_length（对于ZeRO阶段3需要）'
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing. model_kwargs — Additional
    model specific keyword arguments will be forwarded to the `forward` function of
    the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`streamer`（`BaseStreamer`，*可选*）— 将用于流式传输生成序列的Streamer对象。生成的标记通过`streamer.put(token_ids)`传递，streamer负责任何进一步处理。model_kwargs
    — 附加的特定于模型的关键字参数将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。'
- en: Generates sequences of token ids for models with a language modeling head using
    **greedy decoding** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**贪婪解码**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。
- en: In most cases, you do not need to call [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要直接调用[greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: 'Examples:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `sample`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2433)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2433)'
- en: '[PRE11]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 用作生成提示的序列。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`（`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于修改应用于每个生成步骤的语言建模头的预测分数的类派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)的实例列表。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria`（`StoppingCriteriaList`，*可选*）— [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。用于告知生成循环是否应停止的类派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)的实例列表。'
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_warper`（`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于在每个生成步骤的多项式抽样之前应用于语言建模头的预测分数分布的类派生自[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)的实例列表。'
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*，默认为20）— **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成标记的数量。要生成的序列的最大长度。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id`（`int`，*可选*）— *填充*标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*，默认为`False`）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*，默认为`False`）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores`（`bool`，*可选*，默认为`False`）— 是否返回预测分数。有关更多详细信息，请参见返回张量下的`scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate`（`bool`，*可选*，默认为`False`）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus`（`bool`，*可选*，默认为`False`）— 是否继续运行while循环直到max_length（需要ZeRO阶段3）'
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing. model_kwargs — Additional
    model specific kwargs will be forwarded to the `forward` function of the model.
    If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`streamer`（`BaseStreamer`，*可选*）— 将用于流式传输生成的序列的Streamer对象。生成的标记通过`streamer.put(token_ids)`传递，streamer负责任何进一步处理。model_kwargs
    — 附加的模型特定kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。'
- en: Returns
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput),
    [GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)
    or `torch.LongTensor`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)，[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)或`torch.LongTensor`'
- en: A `torch.LongTensor` containing the generated tokens (default behaviour) or
    a [GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)
    if `model.config.is_encoder_decoder=False` and `return_dict_in_generate=True`
    or a [GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)
    if `model.config.is_encoder_decoder=True`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 包含生成标记的`torch.LongTensor`（默认行为）或一个[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)，如果`model.config.is_encoder_decoder=False`且`return_dict_in_generate=True`，或一个[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)，如果`model.config.is_encoder_decoder=True`。
- en: Generates sequences of token ids for models with a language modeling head using
    **multinomial sampling** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**多项式采样**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。
- en: In most cases, you do not need to call [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要直接调用[sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: 'Examples:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#### `beam_search`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `beam_search`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2743)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2743)'
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。'
- en: '`beam_scorer` (`BeamScorer`) — An derived instance of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation.
    For more information, the documentation of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    should be read.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_scorer` (`BeamScorer`) — 一个派生自[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的实例，定义了在生成过程中如何构建、存储和排序beam假设。有关更多信息，请阅读[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的文档。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor` (`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。从[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)派生的类的实例列表，用于修改应用于每个生成步骤的语言建模头的预测分数。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`StoppingCriteriaList`，*可选*）— [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。从[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)派生的类的实例列表，用于告知生成循环是否应该停止。'
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`，*可选*，默认为20) — **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成的标记数量。要生成的序列的最大长度。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`，*可选*）— *填充* 标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`，*可选*）— *序列结束* 标记的id。可选择使用列表设置多个*序列结束*
    标记。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*，默认为`False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*，默认为`False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores` (`bool`，*可选*，默认为`False`) — 是否返回预测分数。有关更多详细信息，请参阅返回张量下的`scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate` (`bool`，*可选*，默认为`False`) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3) model_kwargs
    — Additional model specific kwargs will be forwarded to the `forward` function
    of the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus` (`bool`，*可选*，默认为`False`) — 是否继续运行 while 循环直到达到 max_length（ZeRO
    阶段 3 所需）model_kwargs — 附加的模型特定kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。'
- en: Generates sequences of token ids for models with a language modeling head using
    **beam search decoding** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**beam search解码**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。
- en: In most cases, you do not need to call [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要直接调用[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)。请改用`generate()`。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: 'Examples:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#### `beam_sample`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `beam_sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3074)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3074)'
- en: '[PRE15]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`） — 用作生成提示的序列。'
- en: '`beam_scorer` (`BeamScorer`) — A derived instance of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation.
    For more information, the documentation of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    should be read.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_scorer` (`BeamScorer`) — [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的派生实例，定义了在生成过程中如何构建、存储和排序beam假设。有关更多信息，请阅读[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的文档。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于修改每个生成步骤应用的语言建模头的预测分数的从[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)派生类的实例列表。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。用于告知生成循环是否应该停止的从[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)派生类的实例列表。'
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_warper` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于在每个生成步骤的多项式抽样之前应用于语言建模头的预测分数分布的扭曲的从[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)派生类的实例列表。'
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*, defaults to 20) — **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成的标记数量。要生成的序列的最大长度。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*) — *填充*标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — *序列结束*标记的id。可选择使用列表设置多个*序列结束*标记。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*, defaults to `False`) — 是否返回所有注意力层的注意力张量。有关更多细节，请参见返回的张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — 是否返回所有层的隐藏状态。有关更多细节，请参见返回的张量中的`hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores` (`bool`, *optional*, defaults to `False`) — 是否返回预测分数。有关更多细节，请参见返回的张量中的`scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3) model_kwargs
    — Additional model specific kwargs will be forwarded to the `forward` function
    of the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — 是否继续运行while循环直到达到`max_length`（对于ZeRO阶段3需要）。`model_kwargs`
    — 额外的特定于模型的kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。'
- en: Generates sequences of token ids for models with a language modeling head using
    **beam search multinomial sampling** and can be used for text-decoder, text-to-text,
    speech-to-text, and vision-to-text models.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**波束搜索多项式抽样**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。
- en: In most cases, you do not need to call [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要直接调用[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: 'Examples:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#### `contrastive_search`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `对比搜索`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1715)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1715)'
- en: '[PRE17]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。'
- en: '`top_k` (`int`, *optional*, defaults to 1) — The size of the candidate set
    that is used to re-rank for contrastive search'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`, *可选*，默认为1) — 用于重新排名对比搜索的候选集的大小'
- en: '`penalty_alpha` (`float`, *optional*, defaults to 0) — The degeneration penalty
    for contrastive search; activate when it is larger than 0'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`penalty_alpha` (`float`, *可选*，默认为0) — 对比搜索的退化惩罚；当大于0时激活'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor` (`LogitsProcessorList`, *可选*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)类的实例列表，用于修改每个生成步骤中应用的语言建模头的预测分数。'
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_warper` (`LogitsProcessorList`, *可选*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。派生自[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)类的实例列表，用于在每个生成步骤之前应用于多项式抽样的语言建模头的预测分数分布。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`StoppingCriteriaList`, *可选*) — [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)类的实例列表，用于告知生成循环是否应该停止。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *可选*) — *填充*标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`, *可选*) — *序列结束*标记的id。可选择使用列表设置多个*序列结束*标记。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*，默认为`False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*，默认为`False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores` (`bool`, *可选*，默认为`False`) — 是否返回预测分数。有关更多详细信息，请查看返回张量下的`scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate` (`bool`, *可选*，默认为`False`) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus` (`bool`, *可选*，默认为`False`) — 是否继续运行while循环直到max_length（需要ZeRO阶段3）'
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`streamer` (`BaseStreamer`, *可选*) — 将用于流式传输生成序列的Streamer对象。生成的标记通过`streamer.put(token_ids)`传递，streamer负责任何进一步处理。'
- en: '`sequential` (`bool`, *optional*) — Switches topk hidden state computation
    from parallel to sequential to reduce memory if True. model_kwargs — Additional
    model specific keyword arguments will be forwarded to the `forward` function of
    the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequential` (`bool`, *optional*) — 如果为True，则将topk隐藏状态计算从并行切换到顺序以减少内存。model_kwargs
    — 附加的模型特定关键字参数将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。'
- en: Generates sequences of token ids for models with a language modeling head using
    **contrastive search** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**对比搜索**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。
- en: In most cases, you do not need to call [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要直接调用[contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: 'Examples:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE18]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#### `group_beam_search`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `group_beam_search`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3411)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3411)'
- en: '[PRE19]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。'
- en: '`beam_scorer` (`BeamScorer`) — An derived instance of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation.
    For more information, the documentation of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    should be read.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_scorer` (`BeamScorer`) — 一个派生自[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的实例，定义了在生成过程中如何构建、存储和排序beam假设。有关更多信息，请阅读[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的文档。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor` (`LogitsProcessorList`, *optional*) — 一个[LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于修改应用于每个生成步骤的语言建模头的预测分数的类派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)的实例列表。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — 一个[StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。用于告诉生成循环是否应该停止的类派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)的实例列表。'
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*, 默认为20) — **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成标记的数量。要生成的序列的最大长度。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*) — *填充*标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — *序列结束*标记的id。可选择使用列表设置多个*序列结束*标记。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*, 默认为`False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*, 默认为`False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores` (`bool`, *optional*, 默认为`False`) — 是否返回预测分数。有关更多详细信息，请参阅返回张量下的`scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate` (`bool`, *optional*, 默认为`False`) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus` (`bool`, *optional*, 默认为`False`) — 是否继续运行while循环直到max_length（需要ZeRO阶段3）'
- en: model_kwargs — Additional model specific kwargs that will be forwarded to the
    `forward` function of the model. If model is an encoder-decoder model the kwargs
    should include `encoder_outputs`.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: model_kwargs — 附加的模型特定kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。
- en: Generates sequences of token ids for models with a language modeling head using
    **diverse beam search decoding** and can be used for text-decoder, text-to-text,
    speech-to-text, and vision-to-text models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**多样化束搜索解码**为具有语言建模头的模型生成令牌id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。
- en: In most cases, you do not need to call [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要直接调用 [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)。请改用
    generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: 'Examples:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#### `constrained_beam_search`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `constrained_beam_search`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3796)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3796)'
- en: '[PRE21]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 用作生成提示的序列。'
- en: '`constrained_beam_scorer` (`ConstrainedBeamSearchScorer`) — A derived instance
    of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation,
    while satisfying a list of positive constraints. For more information, the documentation
    of [ConstrainedBeamSearchScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer)
    should be read.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`constrained_beam_scorer` (`ConstrainedBeamSearchScorer`) — 表示如何在生成过程中构建、存储和排序束假设的
    [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    的派生实例，同时满足一系列正面约束。有关更多信息，请阅读 [ConstrainedBeamSearchScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer)
    的文档。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)
    的实例。用于修改每个生成步骤应用的语言建模头的预测分数的类派生实例的列表。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)
    的实例。用于告知生成循环是否应该停止的类派生实例的列表。'
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_warper` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)
    的实例。用于在每个生成步骤的多项式抽样之前应用于语言建模头的预测分数分布的类派生实例的列表。'
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*, 默认为 20) — **已弃用**。直接使用 `logits_processor`
    或 `stopping_criteria` 来限制生成的令牌数量。要生成的序列的最大长度。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*) — *填充* 令牌的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — *结束序列* 令牌的id。可选择使用列表设置多个
    *结束序列* 令牌。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*, 默认为 `False`) — 是否返回所有注意力层的注意力张量。有关更多细节，请参见返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*, 默认为 `False`) — 是否返回所有层的隐藏状态。有关更多细节，请参见返回张量中的
    `hidden_states`。'
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_scores` (`bool`, *optional*, 默认为 `False`) — 是否返回预测分数。有关更多细节，请参见返回张量中的
    `scores`。'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate` (`bool`, *optional*, 默认为 `False`) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3) model_kwargs
    — Additional model specific kwargs will be forwarded to the `forward` function
    of the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus`（`bool`，*可选*，默认为`False`）- 是否继续运行while循环直到max_length（需要ZeRO阶段3）model_kwargs-其他特定于模型的kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。'
- en: Generates sequences of token ids for models with a language modeling head using
    **constrained beam search decoding** and can be used for text-decoder, text-to-text,
    speech-to-text, and vision-to-text models.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**受限束搜索解码**为具有语言建模头的模型生成令牌id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。
- en: In most cases, you do not need to call [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要直接调用[constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: 'Examples:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: TFGenerationMixin
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFGenerationMixin
- en: '### `class transformers.TFGenerationMixin`'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFGenerationMixin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L444)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L444)'
- en: '[PRE23]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A class containing all of the functions supporting generation, to be used as
    a mixin in [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 包含支持生成的所有函数的类，可用作[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)中的mixin。
- en: 'The class exposes [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate),
    which can be used for:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 该类公开[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)，可用于：
- en: '*greedy decoding* by calling `greedy_search()` if `num_beams=1` and `do_sample=False`'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*贪婪解码*，如果`num_beams=1`且`do_sample=False`，则调用`greedy_search()`'
- en: '*contrastive search* by calling `contrastive_search()` if `penalty_alpha>0`
    and `top_k>1`'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对比搜索*，如果`penalty_alpha>0`和`top_k>1`，则调用`contrastive_search()`'
- en: '*multinomial sampling* by calling `sample()` if `num_beams=1` and `do_sample=True`'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多项式采样*，如果`num_beams=1`且`do_sample=True`，则调用`sample()`'
- en: '*beam-search decoding* by calling `beam_search()` if `num_beams>1`'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*束搜索解码*，如果`num_beams>1`，则调用`beam_search()`'
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘generate’ instead. To learn more about decoding strategies refer to
    the [text generation strategies guide](../generation_strategies).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要直接调用上述任何方法。请将自定义参数值传递给“generate”而不是。要了解有关解码策略的更多信息，请参考[文本生成策略指南](../generation_strategies)。
- en: '#### `generate`'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L645)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L645)'
- en: '[PRE24]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`tf.Tensor` of varying shape depending on the modality, *optional*)
    — The sequence used as a prompt for the generation or as model inputs to the encoder.
    If `None` the method initializes it with `bos_token_id` and a batch size of 1\.
    For decoder-only models `inputs` should of in the format of `input_ids`. For encoder-decoder
    models *inputs* can represent any of `input_ids`, `input_values`, `input_features`,
    or `pixel_values`.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（`tf.Tensor`，根据模态性质而变化的形状，*可选*）- 用作生成提示或作为编码器的模型输入的序列。如果为`None`，则该方法将使用`bos_token_id`和批量大小为1进行初始化。对于仅解码器模型，`inputs`应为`input_ids`格式。对于编码器-解码器模型，*inputs*可以表示`input_ids`、`input_values`、`input_features`或`pixel_values`中的任何一个。'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config`（`~generation.GenerationConfig`，*可选*）- 用作生成调用的基本参数化的生成配置。传递给生成的`**kwargs`与`generation_config`的属性匹配将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`（`LogitsProcessorList`，*可选*）- 自定义logits处理器，补充从参数和生成配置构建的默认logits处理器。如果传递的logit处理器已经使用参数或生成配置创建，则会抛出错误。此功能适用于高级用户。'
- en: '`seed` (`List[int]`, *optional*) — Random seed to control sampling, containing
    two integers, used when `do_sample` is `True`. See the `seed` argument from stateless
    functions in `tf.random`.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed`（`List[int]`，*可选*）- 用于控制采样的随机种子，包含两个整数，在`do_sample`为`True`时使用。请参阅`tf.random`中无状态函数的`seed`参数。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *optional*) — `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。如果模型是编码器-解码器模型，则不应该为编码器特定kwargs添加前缀，而应该为解码器特定kwargs添加前缀*decoder_*。'
- en: Returns
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `tf.Tensor`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    或 `tf.Tensor`'
- en: A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `tf.Tensor`.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)（如果`return_dict_in_generate=True`或`config.return_dict_in_generate=True`）或一个`tf.Tensor`。
- en: 'If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型*不是*编码器-解码器模型（`model.config.is_encoder_decoder=False`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型为：
- en: '[TFGreedySearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchDecoderOnlyOutput),'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFGreedySearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchDecoderOnlyOutput),'
- en: '[TFSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleDecoderOnlyOutput),'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleDecoderOnlyOutput),'
- en: '[TFBeamSearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchDecoderOnlyOutput),'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFBeamSearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchDecoderOnlyOutput),'
- en: '[TFBeamSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleDecoderOnlyOutput)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFBeamSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleDecoderOnlyOutput)'
- en: 'If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型是编码器-解码器模型（`model.config.is_encoder_decoder=True`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型为：
- en: '[TFGreedySearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchEncoderDecoderOutput),'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFGreedySearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchEncoderDecoderOutput),'
- en: '[TFSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleEncoderDecoderOutput),'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleEncoderDecoderOutput),'
- en: '[TFBeamSearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchEncoderDecoderOutput),'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFBeamSearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchEncoderDecoderOutput),'
- en: '[TFBeamSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleEncoderDecoderOutput)'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFBeamSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleEncoderDecoderOutput)'
- en: Generates sequences of token ids for models with a language modeling head.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 为具有语言建模头的模型生成token id序列。
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate, e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数控制生成的参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应的参数传递给generate来覆盖任何`generation_config`，例如`.generate(inputs,
    num_beams=4, do_sample=True)`。
- en: For an overview of generation strategies and code examples, check out the [following
    guide](../generation_strategies).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。
- en: '#### `compute_transition_scores`'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_transition_scores`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L477)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L477)'
- en: '[PRE25]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor`) — The generated sequences. The second dimension (sequence_length)
    is either equal to `max_length` or shorter if all batches finished early due to
    the `eos_token_id`.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`) — 生成的序列。第二维（序列长度）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短一些。'
- en: '`scores` (`tuple(tf.Tensor)`) — Transition scores for each vocabulary token
    at each generation step. Beam transition scores consisting of log probabilities
    of tokens conditioned on log softmax of previously generated tokens Tuple of `tf.Tensor`
    with up to `max_new_tokens` elements (one element for each generated token), with
    each tensor of shape `(batch_size*num_beams, config.vocab_size)`.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)`) — 每个生成步骤中每个词汇标记的转移分数。Beam转移分数由tokens的log概率组成，条件是先前生成的tokens的log
    softmax。形状为`(batch_size*num_beams, config.vocab_size)`的`tf.Tensor`元组，最多包含`max_new_tokens`个元素（每个生成的token一个元素）。'
- en: '`beam_indices` (`tf.Tensor`, *optional*) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`. Only required if a `num_beams>1` at generate-time.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices` (`tf.Tensor`, *optional*) — 每个生成步骤中生成的token id的beam索引。形状为`(batch_size*num_return_sequences,
    sequence_length)`的`tf.Tensor`。在生成时如果`num_beams>1`则是必需的。'
- en: '`normalize_logits` (`bool`, *optional*, defaults to `False`) — Whether to normalize
    the logits (which, for legacy reasons, may be unnormalized).'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_logits` (`bool`, *optional*, defaults to `False`) — 是否对logits进行归一化（由于历史原因，可能未归一化）。'
- en: Returns
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tf.Tensor`'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Tensor`'
- en: A `tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`
    containing the transition scores (logits)
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一个形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`，包含转移分数（logits）
- en: Computes the transition scores of sequences given the generation scores (and
    beam indices, if beam search was used). This is a convenient method to quicky
    obtain the scores of the selected tokens at generation time.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 计算给定生成分数的序列的转移分数（以及如果使用了束搜索，则为束索引）。这是一个方便的方法，在生成时快速获取所选标记的分数。
- en: 'Examples:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: FlaxGenerationMixin
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxGenerationMixin
- en: '### `class transformers.FlaxGenerationMixin`'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxGenerationMixin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L129)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L129)'
- en: '[PRE27]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: A class containing all functions for auto-regressive text generation, to be
    used as a mixin in [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 包含用于自回归文本生成的所有函数的类，可作为[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)中的mixin使用。
- en: 'The class exposes [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate),
    which can be used for:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 该类公开了[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate)，可用于：
- en: '*greedy decoding* by calling `_greedy_search()` if `num_beams=1` and `do_sample=False`'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*贪婪解码*，如果`num_beams=1`且`do_sample=False`，则调用`_greedy_search()`'
- en: '*multinomial sampling* by calling `_sample()` if `num_beams=1` and `do_sample=True`'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多项式采样*，如果`num_beams=1`且`do_sample=True`，则调用`_sample()`。'
- en: '*beam-search decoding* by calling `_beam_search()` if `num_beams>1` and `do_sample=False`'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*束搜索解码*，如果`num_beams>1`且`do_sample=False`，则调用`_beam_search()`'
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘generate’ instead. To learn more about decoding strategies refer to
    the [text generation strategies guide](../generation_strategies).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 您无需直接调用上述任何方法。而是将自定义参数值传递给“generate”。要了解更多关于解码策略的信息，请参考[文本生成策略指南](../generation_strategies)。
- en: '#### `generate`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L267)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L267)'
- en: '[PRE28]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — The
    sequence used as a prompt for the generation.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`）—用作生成提示的序列。'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config`（`~generation.GenerationConfig`，*可选*）—用作生成调用的基本参数化的生成配置。传递给generate的`**kwargs`匹配`generation_config`的属性将覆盖它们。如果未提供`generation_config`，则将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。'
- en: '`trace` (`bool`, *optional*, defaults to `True`) — Whether to trace generation.
    Setting `trace=False` should only be used for debugging and will lead to a considerably
    slower runtime.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trace`（`bool`，*可选*，默认为`True`）—是否跟踪生成。设置`trace=False`仅用于调试，会导致运行时间明显变慢。'
- en: '`params` (`Dict[str, jnp.ndarray]`, *optional*) — Optionally the model parameters
    can be passed. Can be useful for parallelized generation.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（`Dict[str, jnp.ndarray]`，*可选*）—可选择传递模型参数。对于并行化生成可能会有用。'
- en: '`logits_processor` (`FlaxLogitsProcessorList` , *optional*) — Custom logits
    processors that complement the default logits processors built from arguments
    and generation config. If a logit processor is passed that is already created
    with the arguments or a generation config an error is thrown. This feature is
    intended for advanced users.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`（`FlaxLogitsProcessorList`，*可选*）—自定义logits处理器，补充了从参数和生成配置构建的默认logits处理器。如果传递了已经使用参数或生成配置创建的logit处理器，则会抛出错误。此功能旨在供高级用户使用。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）—`generate_config`的临时参数化和/或将转发给模型的`forward`函数的其他特定于模型的kwargs。如果模型是编码器-解码器模型，则不应添加编码器特定的kwargs前缀，而应为解码器特定的kwargs添加*decoder_*前缀。'
- en: Generates sequences of token ids for models with a language modeling head.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 为具有语言建模头的模型生成令牌id序列。
