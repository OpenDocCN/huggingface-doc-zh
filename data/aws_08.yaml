- en: Fine-tune BERT for Text Classification on AWS Trainium
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨AWS Trainiumä¸Šä¸ºæ–‡æœ¬åˆ†ç±»å¾®è°ƒBERT
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '*There is a notebook version of that tutorial [here](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/notebook.ipynb)*.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ä¸ªæ•™ç¨‹æœ‰ä¸€ä¸ªç¬”è®°æœ¬ç‰ˆæœ¬[åœ¨è¿™é‡Œ](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/notebook.ipynb)*ã€‚'
- en: This tutorial will help you to get started with [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls)
    and Hugging Face Transformers. It will cover how to set up a Trainium instance
    on AWS, load & fine-tune a transformers model for text-classification
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ•™ç¨‹å°†å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨[AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls)å’ŒHugging
    Face Transformersã€‚å®ƒå°†æ¶µç›–å¦‚ä½•åœ¨AWSä¸Šè®¾ç½®Trainiumå®ä¾‹ï¼ŒåŠ è½½å’Œå¾®è°ƒä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ†ç±»çš„transformersæ¨¡å‹
- en: 'You will learn how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š
- en: '[Setup AWS environment](#1-setup-aws-environment)'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è®¾ç½®AWSç¯å¢ƒ](#1-setup-aws-environment)'
- en: '[Load and process the dataset](#2-load-and-process-the-dataset)'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[åŠ è½½å’Œå¤„ç†æ•°æ®é›†](#2-load-and-process-the-dataset)'
- en: '[Fine-tune BERT using Hugging Face Transformers and Optimum Neuron](#3-fine-tune-bert-using-hugging-face-transformers)'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨Hugging Face Transformerså’ŒOptimum Neuronå¾®è°ƒBERT](#3-fine-tune-bert-using-hugging-face-transformers)'
- en: Before we can start, make sure you have a [Hugging Face Account](https://huggingface.co/join)
    to save artifacts and experiments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰ä¸€ä¸ª[Hugging Faceè´¦æˆ·](https://huggingface.co/join)ä»¥ä¿å­˜å·¥ä»¶å’Œå®éªŒã€‚
- en: 'Quick intro: AWS Trainium'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¿«é€Ÿä»‹ç»ï¼šAWS Trainium
- en: '[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is
    a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the
    successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls)
    focused on high-performance training workloads claiming up to 50% cost-to-train
    savings over comparable GPU-based instances.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/)æ˜¯ä¸“ä¸ºæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰è®­ç»ƒå·¥ä½œè´Ÿè½½è€Œæ„å»ºçš„EC2ã€‚Trainiumæ˜¯[AWS
    Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls)çš„ç»§ä»»è€…ï¼Œä¸“æ³¨äºé«˜æ€§èƒ½è®­ç»ƒå·¥ä½œè´Ÿè½½ï¼Œå£°ç§°ä¸å¯æ¯”è¾ƒçš„åŸºäºGPUçš„å®ä¾‹ç›¸æ¯”ï¼Œè®­ç»ƒæˆæœ¬èŠ‚çœé«˜è¾¾50%ã€‚'
- en: Trainium has been optimized for training natural language processing, computer
    vision, and recommender models used. The accelerator supports a wide range of
    data types, including FP32, TF32, BF16, FP16, UINT8, and configurable FP8.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Trainiumå·²ç»é’ˆå¯¹è®­ç»ƒè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œæ¨èæ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚è¯¥åŠ é€Ÿå™¨æ”¯æŒå¹¿æ³›çš„æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬FP32ã€TF32ã€BF16ã€FP16ã€UINT8å’Œå¯é…ç½®çš„FP8ã€‚
- en: 'The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of
    memory, making it easy to fine-tune ~10B parameter models on a single instance.
    Below you will find an overview of the available instance types. More details
    [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§çš„Trainiumå®ä¾‹ï¼Œ`trn1.32xlarge`æ‹¥æœ‰è¶…è¿‡500GBçš„å†…å­˜ï¼Œä½¿å¾—åœ¨å•ä¸ªå®ä¾‹ä¸Šè½»æ¾å¾®è°ƒçº¦10Bå‚æ•°æ¨¡å‹å˜å¾—å®¹æ˜“ã€‚ä¸‹é¢æ˜¯å¯ç”¨å®ä¾‹ç±»å‹çš„æ¦‚è¿°ã€‚æ›´å¤šç»†èŠ‚[åœ¨è¿™é‡Œ](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details)ï¼š
- en: '| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price
    per hour |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| å®ä¾‹å¤§å° | åŠ é€Ÿå™¨ | åŠ é€Ÿå™¨å†…å­˜ | vCPU | CPUå†…å­˜ | æ¯å°æ—¶ä»·æ ¼ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| trn1.2xlarge | 1 | 32 | 8 | 32 | $1.34 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| trn1.2xlarge | 1 | 32 | 8 | 32 | $1.34 |'
- en: '| trn1.32xlarge | 16 | 512 | 128 | 512 | $21.50 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| trn1.32xlarge | 16 | 512 | 128 | 512 | $21.50 |'
- en: '| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | $24.78 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | $24.78 |'
- en: '* * *'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Now we know what Trainium offers, letâ€™s get started. ğŸš€
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“Trainiumæä¾›äº†ä»€ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸš€
- en: '*Note: This tutorial was created on a trn1.2xlarge AWS EC2 Instance.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šè¿™ä¸ªæ•™ç¨‹æ˜¯åœ¨ä¸€ä¸ªtrn1.2xlargeçš„AWS EC2å®ä¾‹ä¸Šåˆ›å»ºçš„ã€‚*'
- en: 1\. Setup AWS environment
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. è®¾ç½®AWSç¯å¢ƒ
- en: In this example, we will use the `trn1.2xlarge` instance on AWS with 1 Accelerator,
    including two Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨AWSä¸Šä½¿ç”¨`trn1.2xlarge`å®ä¾‹ï¼ŒåŒ…æ‹¬1ä¸ªåŠ é€Ÿå™¨ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªNeuron Coreså’Œ[Hugging Face Neuron
    Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2)ã€‚
- en: This blog post doesnâ€™t cover how to create the instance in detail. You can check
    out my previous blog about [â€œSetting up AWS Trainium for Hugging Face Transformersâ€](https://www.philschmid.de/setup-aws-trainium),
    which includes a step-by-step guide on setting up the environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡åšæ–‡ä¸è¯¦ç»†ä»‹ç»å¦‚ä½•åˆ›å»ºå®ä¾‹ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹æˆ‘çš„ä¹‹å‰å…³äº[â€œä¸ºHugging Face Transformersè®¾ç½®AWS Trainiumâ€](https://www.philschmid.de/setup-aws-trainium)çš„åšå®¢ï¼Œå…¶ä¸­åŒ…æ‹¬å…³äºè®¾ç½®ç¯å¢ƒçš„é€æ­¥æŒ‡å—ã€‚
- en: Once the instance is up and running, we can ssh into it. But instead of developing
    inside a terminal we want to use a `Jupyter` environment, which we can use for
    preparing our dataset and launching the training. For this, we need to add a port
    for forwarding in the `ssh` command, which will tunnel our localhost traffic to
    the Trainium instance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®ä¾‹å¯åŠ¨è¿è¡Œï¼Œæˆ‘ä»¬å¯ä»¥sshè¿›å…¥å®ƒã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸æƒ³åœ¨ç»ˆç«¯å†…å¼€å‘ï¼Œè€Œæ˜¯æƒ³ä½¿ç”¨ä¸€ä¸ª`Jupyter`ç¯å¢ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¥å‡†å¤‡æ•°æ®é›†å’Œå¯åŠ¨è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨`ssh`å‘½ä»¤ä¸­æ·»åŠ ä¸€ä¸ªç”¨äºè½¬å‘çš„ç«¯å£ï¼Œè¿™å°†æŠŠæˆ‘ä»¬çš„æœ¬åœ°ä¸»æœºæµé‡éš§é“åˆ°Trainiumå®ä¾‹ã€‚
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can now start our **`jupyter`** server.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯åŠ¨æˆ‘ä»¬çš„**`jupyter`**æœåŠ¡å™¨ã€‚
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see a familiar **`jupyter`** output with a URL to the notebook.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªç†Ÿæ‚‰çš„**`jupyter`**è¾“å‡ºï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæŒ‡å‘ç¬”è®°æœ¬çš„URLã€‚
- en: '**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**'
- en: We can click on it, and a **`jupyter`** environment opens in our local browser.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç‚¹å‡»å®ƒï¼Œåœ¨æˆ‘ä»¬çš„æœ¬åœ°æµè§ˆå™¨ä¸­æ‰“å¼€ä¸€ä¸ª**`jupyter`**ç¯å¢ƒã€‚
- en: '![jupyter.webp](../Images/f3e7326719a8cc7f67122b89fb3e1dc1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![jupyter.webp](../Images/f3e7326719a8cc7f67122b89fb3e1dc1.png)'
- en: We are going to use the Jupyter environment only for preparing the dataset and
    then `torchrun` for launching our training script on both neuron cores for distributed
    training. Lets create a new notebook and get started.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»…ä½¿ç”¨Jupyterç¯å¢ƒå‡†å¤‡æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨`torchrun`åœ¨ä¸¤ä¸ªNeuron Coresä¸Šå¯åŠ¨æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬å¹¶å¼€å§‹å§ã€‚
- en: 2\. Load and process the dataset
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. åŠ è½½å’Œå¤„ç†æ•°æ®é›†
- en: 'We are training a Text Classification model on the [emotion](https://huggingface.co/datasets/philschmid/emotion)
    dataset to keep the example straightforward. The `emotion` is a dataset of English
    Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and
    surprise.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨å¯¹[æƒ…æ„Ÿ](https://huggingface.co/datasets/philschmid/emotion)æ•°æ®é›†ä¸Šçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¿æŒç¤ºä¾‹ç®€å•ã€‚`emotion`æ˜¯ä¸€ä¸ªåŒ…å«å…­ç§åŸºæœ¬æƒ…ç»ªï¼ˆæ„¤æ€’ã€ææƒ§ã€å–œæ‚¦ã€çˆ±ã€æ‚²ä¼¤å’ŒæƒŠè®¶ï¼‰çš„è‹±æ–‡Twitteræ¶ˆæ¯æ•°æ®é›†ã€‚
- en: We will use the `load_dataset()` method from the [ğŸ¤— Datasets](https://huggingface.co/docs/datasets/index)
    library to load the `emotion`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨[ğŸ¤— Datasets](https://huggingface.co/docs/datasets/index)åº“ä¸­çš„`load_dataset()`æ–¹æ³•åŠ è½½`emotion`ã€‚
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Letâ€™s check out an example of the dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†çš„ç¤ºä¾‹ã€‚
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We must convert our â€œNatural Languageâ€ to token IDs to train our model. This
    is done by a Tokenizer, which tokenizes the inputs (including converting the tokens
    to their corresponding IDs in the pre-trained vocabulary). if you want to learn
    more about this, out [chapter 6](https://huggingface.co/course/chapter6/1?fw=pt)
    of the [Hugging Face Course](https://huggingface.co/course/chapter1/1).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»å°†æˆ‘ä»¬çš„â€œè‡ªç„¶è¯­è¨€â€è½¬æ¢ä¸ºæ ‡è®°IDä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªåˆ†è¯å™¨å®Œæˆçš„ï¼Œå®ƒå¯¹è¾“å…¥è¿›è¡Œåˆ†è¯ï¼ˆåŒ…æ‹¬å°†æ ‡è®°è½¬æ¢ä¸ºé¢„è®­ç»ƒè¯æ±‡è¡¨ä¸­å¯¹åº”çš„IDï¼‰ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[Hugging
    Faceè¯¾ç¨‹](https://huggingface.co/course/chapter1/1)ä¸­çš„[ç¬¬6ç« ](https://huggingface.co/course/chapter6/1?fw=pt)ã€‚
- en: Our Neuron Accelerator expects a fixed shape of inputs. We need to truncate
    or pad all samples to the same length.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„NeuronåŠ é€Ÿå™¨æœŸæœ›è¾“å…¥å…·æœ‰å›ºå®šçš„å½¢çŠ¶ã€‚æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æˆªæ–­æˆ–å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ã€‚
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 3\. Fine-tune BERT using Hugging Face Transformers
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. ä½¿ç”¨Hugging Face Transformerså¾®è°ƒBERT
- en: Normally you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)
    and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)
    to fine-tune PyTorch-based transformer models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨ä¼šä½¿ç”¨[Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)å’Œ[TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)æ¥å¾®è°ƒåŸºäºPyTorchçš„transformeræ¨¡å‹ã€‚
- en: But together with AWS, we have developed a [NeuronTrainer](https://huggingface.co/docs/optimum-neuron/package_reference/trainer)
    to improve performance, robustness, and safety when training on Trainium or Inferentia2
    instances. The `NeuronTrainer` also comes with a [model cache](https://www.notion.so/Getting-started-with-AWS-Trainium-and-Hugging-Face-Transformers-8428c72556194aed9c393de101229dcf),
    which allows us to use precompiled models and configuration from Hugging Face
    Hub to skip the compilation step, which would be needed at the beginning of training.
    This can reduce the training time by ~3x.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œä¸AWSä¸€èµ·ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª[NeuronTrainer](https://huggingface.co/docs/optimum-neuron/package_reference/trainer)ï¼Œä»¥æé«˜åœ¨Trainiumæˆ–Inferentia2å®ä¾‹ä¸Šè®­ç»ƒæ—¶çš„æ€§èƒ½ã€ç¨³å¥æ€§å’Œå®‰å…¨æ€§ã€‚`NeuronTrainer`è¿˜é…å¤‡äº†ä¸€ä¸ª[æ¨¡å‹ç¼“å­˜](https://www.notion.so/Getting-started-with-AWS-Trainium-and-Hugging-Face-Transformers-8428c72556194aed9c393de101229dcf)ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨Hugging
    Face Hubä¸­çš„é¢„ç¼–è¯‘æ¨¡å‹å’Œé…ç½®ï¼Œè·³è¿‡è®­ç»ƒå¼€å§‹æ—¶éœ€è¦çš„ç¼–è¯‘æ­¥éª¤ã€‚è¿™å¯ä»¥å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­çº¦3å€ã€‚
- en: The `NeuronTrainer` is part of the `optimum-neuron` library and can be used
    as a 1-to-1 replacement for the `Trainer`. You only have to adjust the import
    in your training script.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`NeuronTrainer`æ˜¯`optimum-neuron`åº“çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥ä½œä¸º`Trainer`çš„ä¸€å¯¹ä¸€æ›¿ä»£å“ä½¿ç”¨ã€‚æ‚¨åªéœ€è°ƒæ•´è®­ç»ƒè„šæœ¬ä¸­çš„å¯¼å…¥å³å¯ã€‚'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We prepared a simple [train.py](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/scripts/train.py)
    training script based on the [â€œGetting started with Pytorch 2.0 and Hugging Face
    Transformersâ€](https://www.philschmid.de/getting-started-pytorch-2-0-transformers#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer)
    blog post with the `NeuronTrainer`. Below is an excerpt
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªç®€å•çš„[train.py](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/scripts/train.py)è®­ç»ƒè„šæœ¬ï¼ŒåŸºäº[â€œGetting
    started with Pytorch 2.0 and Hugging Face Transformersâ€](https://www.philschmid.de/getting-started-pytorch-2-0-transformers#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer)åšå®¢æ–‡ç« ï¼Œä½¿ç”¨`NeuronTrainer`ã€‚ä»¥ä¸‹æ˜¯æ‘˜å½•
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can load the training script into our environment using the `wget` command
    or manually copy it into the notebook from [here](https://github.com/huggingface/optimum-neuron/blob/notebooks/text-classification/scripts/train.py).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`wget`å‘½ä»¤å°†è®­ç»ƒè„šæœ¬åŠ è½½åˆ°æˆ‘ä»¬çš„ç¯å¢ƒä¸­ï¼Œä¹Ÿå¯ä»¥ä»[è¿™é‡Œ](https://github.com/huggingface/optimum-neuron/blob/notebooks/text-classification/scripts/train.py)æ‰‹åŠ¨å¤åˆ¶åˆ°ç¬”è®°æœ¬ä¸­ã€‚
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will use `torchrun` to launch our training script on both neuron cores for
    distributed training. `torchrun` is a tool that automatically distributes a PyTorch
    model across multiple accelerators. We can pass the number of accelerators as
    `nproc_per_node` arguments alongside our hyperparameters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨`torchrun`åœ¨ä¸¤ä¸ªç¥ç»å…ƒæ ¸å¿ƒä¸Šå¯åŠ¨æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚`torchrun`æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œå¯ä»¥è‡ªåŠ¨å°†PyTorchæ¨¡å‹åˆ†å¸ƒåˆ°å¤šä¸ªåŠ é€Ÿå™¨ä¸Šã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¶…å‚æ•°æ—è¾¹ä¼ é€’åŠ é€Ÿå™¨æ•°é‡ä½œä¸º`nproc_per_node`å‚æ•°ã€‚
- en: 'Weâ€™ll use the following command to launch training:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***Note**: If you see bad, bad accuracy, you might want to deactivate `bf16`
    for now.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ³¨æ„**ï¼šå¦‚æœæ‚¨çœ‹åˆ°ç³Ÿç³•çš„å‡†ç¡®ç‡ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æš‚æ—¶åœç”¨`bf16`ã€‚*'
- en: After 9 minutes the training was completed and achieved an excellent f1 score
    of `0.914`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡9åˆ†é’Ÿçš„è®­ç»ƒï¼Œè·å¾—äº†å‡ºè‰²çš„`0.914`çš„f1åˆ†æ•°ã€‚
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Last but not least, terminate the EC2 instance to avoid unnecessary charges.
    Looking at the price-performance, our training only cost **`20ct`** (**`1.34$/h
    * 0.15h = 0.20$`**)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç»ˆæ­¢EC2å®ä¾‹ä»¥é¿å…ä¸å¿…è¦çš„è´¹ç”¨ã€‚ä»æ€§ä»·æ¯”æ¥çœ‹ï¼Œæˆ‘ä»¬çš„è®­ç»ƒåªèŠ±è´¹äº†**`20ct`**ï¼ˆ**`1.34$/h * 0.15h = 0.20$`**ï¼‰
