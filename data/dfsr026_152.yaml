- en: MusicLDM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MusicLDM
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/musicldm](https://huggingface.co/docs/diffusers/api/pipelines/musicldm)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/musicldm](https://huggingface.co/docs/diffusers/api/pipelines/musicldm)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'MusicLDM was proposed in [MusicLDM: Enhancing Novelty in Text-to-Music Generation
    Using Beat-Synchronous Mixup Strategies](https://huggingface.co/papers/2308.01546)
    by Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick,
    Shlomo Dubnov. MusicLDM takes a text prompt as input and predicts the corresponding
    music sample.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 'MusicLDM是由Ke Chen、Yusong Wu、Haohe Liu、Marianna Nezhurina、Taylor Berg-Kirkpatrick、Shlomo
    Dubnov在[MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous
    Mixup Strategies](https://huggingface.co/papers/2308.01546)中提出的。MusicLDM以文本提示作为输入，并预测相应的音乐样本。'
- en: Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)
    and [AudioLDM](https://huggingface.co/docs/diffusers/api/pipelines/audioldm),
    MusicLDM is a text-to-music *latent diffusion model (LDM)* that learns continuous
    audio representations from [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)
    latents.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 受到[Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)和[AudioLDM](https://huggingface.co/docs/diffusers/api/pipelines/audioldm)的启发，MusicLDM是一个文本到音乐的*潜在扩散模型（LDM）*，从[CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)潜在中学习连续音频表示。
- en: MusicLDM is trained on a corpus of 466 hours of music data. Beat-synchronous
    data augmentation strategies are applied to the music samples, both in the time
    domain and in the latent space. Using beat-synchronous data augmentation strategies
    encourages the model to interpolate between the training samples, but stay within
    the domain of the training data. The result is generated music that is more diverse
    while staying faithful to the corresponding style.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: MusicLDM在466小时的音乐数据语料库上进行训练。节拍同步数据增强策略应用于音乐样本，既在时间域中，也在潜在空间中。使用节拍同步数据增强策略鼓励模型在训练样本之间插值，但仍保持在训练数据的领域内。结果是生成的音乐更加多样化，同时仍然忠实于相应的风格。
- en: 'The abstract of the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Diffusion models have shown promising results in cross-modal generation tasks,
    including text-to-image and text-to-audio generation. However, generating music,
    as a special type of audio, presents unique challenges due to limited availability
    of music data and sensitive issues related to copyright and plagiarism. In this
    paper, to tackle these challenges, we first construct a state-of-the-art text-to-music
    model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the
    music domain. We achieve this by retraining the contrastive language-audio pretraining
    model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection
    of music data samples. Then, to address the limitations of training data and to
    avoid plagiarism, we leverage a beat tracking model and propose two different
    mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous
    latent mixup, which recombine training audio directly or via a latent embeddings
    space, respectively. Such mixup strategies encourage the model to interpolate
    between musical training samples and generate new music within the convex hull
    of the training data, making the generated music more diverse while still staying
    faithful to the corresponding style. In addition to popular evaluation metrics,
    we design several new evaluation metrics based on CLAP score to demonstrate that
    our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality
    and novelty of generated music, as well as the correspondence between input text
    and generated music.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*扩散模型在跨模态生成任务中表现出有希望的结果，包括文本到图像和文本到音频的生成。然而，生成音乐作为一种特殊类型的音频，由于音乐数据的有限可用性以及与版权和抄袭相关的敏感问题，存在独特的挑战。在本文中，为了解决这些挑战，我们首先构建了一个最先进的文本到音乐模型MusicLDM，该模型将Stable
    Diffusion和AudioLDM架构调整到音乐领域。我们通过在一组音乐数据样本上重新训练对比语言-音频预训练模型（CLAP）和Hifi-GAN声码器，作为MusicLDM的组成部分。然后，为了解决训练数据的限制并避免抄袭，我们利用一个节拍跟踪模型，并提出了两种不同的数据增强混合策略：节拍同步音频混合和节拍同步潜在混合，分别直接或通过潜在嵌入空间重新组合训练音频。这些混合策略鼓励模型在音乐训练样本之间插值，并在训练数据的凸包内生成新音乐，使生成的音乐更加多样化，同时仍然忠实于相应的风格。除了流行的评估指标外，我们设计了几个基于CLAP分数的新评估指标，以证明我们提出的MusicLDM和节拍同步混合策略提高了生成音乐的质量和新颖性，以及输入文本与生成音乐之间的对应关系。*'
- en: This pipeline was contributed by [sanchit-gandhi](https://huggingface.co/sanchit-gandhi).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道由[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)贡献。
- en: Tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: 'When constructing a prompt, keep in mind:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建提示时，请记住：
- en: Descriptive prompt inputs work best; use adjectives to describe the sound (for
    example, “high quality” or “clear”) and make the prompt context specific where
    possible (e.g. “melodic techno with a fast beat and synths” works better than
    “techno”).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述性的提示输入效果最好；使用形容词描述声音（例如，“高质量”或“清晰”），尽可能使提示具体（例如，“旋律技术与快节奏和合成器”比“技术”更好）。
- en: Using a *negative prompt* can significantly improve the quality of the generated
    audio. Try using a negative prompt of “low quality, average quality”.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*负面提示*可以显著提高生成音频的质量。尝试使用“低质量，平均质量”的负面提示。
- en: 'During inference:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中：
- en: The *quality* of the generated audio sample can be controlled by the `num_inference_steps`
    argument; higher steps give higher quality audio at the expense of slower inference.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的音频样本的质量可以通过`num_inference_steps`参数进行控制；更高的步骤会提供更高质量的音频，但推理速度会变慢。
- en: 'Multiple waveforms can be generated in one go: set `num_waveforms_per_prompt`
    to a value greater than 1 to enable. Automatic scoring will be performed between
    the generated waveforms and prompt text, and the audios ranked from best to worst
    accordingly.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以一次生成多个波形：将`num_waveforms_per_prompt`设置为大于1的值以启用。将对生成的波形和提示文本进行自动评分，并相应地对音频进行排序。
- en: The *length* of the generated audio sample can be controlled by varying the
    `audio_length_in_s` argument.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的音频样本的*长度*可以通过变化`audio_length_in_s`参数来控制。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看调度器[指南](../../using-diffusers/schedulers)以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分以了解如何有效地将相同组件加载到多个管道中。
- en: MusicLDMPipeline
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicLDMPipeline
- en: '### `class diffusers.MusicLDMPipeline`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.MusicLDMPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L67)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L67)'
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。'
- en: '`text_encoder` ([ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel))
    — Frozen text-audio embedding model (`ClapTextModel`), specifically the [laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)
    variant.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` ([ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel))
    — 冻结的文本-音频嵌入模型（`ClapTextModel`），具体来说是[laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)变种。'
- en: '`tokenizer` (`PreTrainedTokenizer`) — A [RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    to tokenize text.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`PreTrainedTokenizer`) — 用于对文本进行标记化的[RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)。'
- en: '`feature_extractor` ([ClapFeatureExtractor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor))
    — Feature extractor to compute mel-spectrograms from audio waveforms.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` ([ClapFeatureExtractor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor))
    — 从音频波形计算梅尔频谱图的特征提取器。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A `UNet2DConditionModel` to denoise the encoded audio latents.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 用于降噪编码音频潜在空间的`UNet2DConditionModel`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded audio
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 与`unet`结合使用的调度器，用于降噪编码音频潜在空间。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: '`vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan))
    — Vocoder of class `SpeechT5HifiGan`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan))
    — 类`SpeechT5HifiGan`的声码器。'
- en: Pipeline for text-to-audio generation using MusicLDM.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MusicLDM进行文本到音频生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L434)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L434)'
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    audio generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`, *可选*) — 指导音频生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`audio_length_in_s` (`int`, *optional*, defaults to 10.24) — The length of
    the generated audio sample in seconds.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio_length_in_s` (`int`, *可选*, 默认为10.24) — 生成的音频样本的长度（秒）。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 200) — The number of
    denoising steps. More denoising steps usually lead to a higher quality audio at
    the expense of slower inference.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为200) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的音频，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 2.0) — A higher guidance
    scale value encourages the model to generate audio that is closely linked to the
    text `prompt` at the expense of lower sound quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为2.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的音频，但会降低声音质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in audio generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *可选*) — 指导音频生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。当不使用引导时（`guidance_scale
    < 1`）将被忽略。'
- en: '`num_waveforms_per_prompt` (`int`, *optional*, defaults to 1) — The number
    of waveforms to generate per prompt. If `num_waveforms_per_prompt > 1`, the text
    encoding model is a joint text-audio model ([ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)),
    and the tokenizer is a `[~transformers.ClapProcessor]`, then automatic scoring
    will be performed between the generated outputs and the input text. This scoring
    ranks the generated waveforms based on their cosine similarity to text input in
    the joint text-audio embedding space.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_waveforms_per_prompt` (`int`, *可选*, 默认为1) — 每个提示生成的波形数量。如果 `num_waveforms_per_prompt
    > 1`，文本编码模型是一个联合文本-音频模型（[ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)），并且分词器是一个
    `[~transformers.ClapProcessor]`，则将在生成的输出和输入文本之间执行自动评分。此评分根据它们在联合文本-音频嵌入空间中与文本输入的余弦相似度对生成的波形进行排名。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *可选*, 默认为0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数
    eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成的嘈杂潜在变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机
    `generator` 进行采样生成潜在变量张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`
    将从 `negative_prompt` 输入参数生成。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    instead of a plain tuple.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    而不是普通元组。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *可选*) — 在推断期间每 `callback_steps` 步调用一次的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *可选*, 默认为1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给 `AttentionProcessor` 的
    kwargs 字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。'
- en: '`output_type` (`str`, *optional*, defaults to `"np"`) — The output format of
    the generated audio. Choose between `"np"` to return a NumPy `np.ndarray` or `"pt"`
    to return a PyTorch `torch.Tensor` object. Set to `"latent"` to return the latent
    diffusion model (LDM) output.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为 `"np"`) — 生成的音频的输出格式。选择 `"np"` 返回一个 NumPy `np.ndarray`
    或 `"pt"` 返回一个 PyTorch `torch.Tensor` 对象。设置为 `"latent"` 以返回潜在扩散模型 (LDM) 输出。'
- en: Returns
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    or `tuple`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated audio.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，将返回 [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)，否则将返回一个
    `tuple`，其中第一个元素是包含生成音频的列表。
- en: The call function to the pipeline for generation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `disable_vae_slicing`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L125)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L125)'
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片 VAE 解码。如果先前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `enable_model_cpu_offload`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_model_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L400)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L400)'
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Offloads all models to CPU using accelerate, reducing memory usage with a low
    impact on performance. Compared to `enable_sequential_cpu_offload`, this method
    moves one whole model at a time to the GPU when its `forward` method is called,
    and the model remains in GPU until the next model runs. Memory savings are lower
    than with `enable_sequential_cpu_offload`, but performance is much better due
    to the iterative execution of the `unet`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加速器将所有模型转移到CPU，减少内存使用量，对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将一个完整模型移动到GPU，并且该模型保持在GPU中直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较少，但由于`unet`的迭代执行，性能要好得多。
- en: '#### `enable_vae_slicing`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L117)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L117)'
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。当启用此选项时，VAE将在几个步骤中将输入张量分割成片来计算解码。这对节省一些内存并允许更大的批量大小很有用。
