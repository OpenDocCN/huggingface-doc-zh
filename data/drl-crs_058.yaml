- en: Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/introduction](https://huggingface.co/learn/deep-rl-course/unit4/introduction)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit4/introduction](https://huggingface.co/learn/deep-rl-course/unit4/introduction)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![thumbnail](../Images/207886028f30a9a8c43010256f915e88.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![缩略图](../Images/207886028f30a9a8c43010256f915e88.png)'
- en: In the last unit, we learned about Deep Q-Learning. In this value-based deep
    reinforcement learning algorithm, we **used a deep neural network to approximate
    the different Q-values for each possible action at a state.**
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个单元中，我们学习了深度 Q 学习。在这种基于价值的深度强化学习算法中，我们使用深度神经网络来近似每个可能动作的不同 Q 值。
- en: Since the beginning of the course, we have only studied value-based methods, **where
    we estimate a value function as an intermediate step towards finding an optimal
    policy.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自课程开始以来，我们只研究了基于价值的方法，其中我们估计一个值函数作为找到最优策略的中间步骤。
- en: '![Link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![链接价值策略](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
- en: In value-based methods, the policy **(π) only exists because of the action value
    estimates since the policy is just a function** (for instance, greedy-policy)
    that will select the action with the highest value given a state.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，策略（π）仅存在是因为动作价值的估计，因为策略只是一个函数（例如，贪婪策略），它将在给定状态时选择价值最高的动作。
- en: With policy-based methods, we want to optimize the policy directly **without
    having an intermediate step of learning a value function.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基于策略的方法，我们希望直接优化策略，而不需要学习价值函数的中间步骤。
- en: So today, **we’ll learn about policy-based methods and study a subset of these
    methods called policy gradient**. Then we’ll implement our first policy gradient
    algorithm called Monte Carlo **Reinforce** from scratch using PyTorch. Then, we’ll
    test its robustness using the CartPole-v1 and PixelCopter environments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，今天，我们将学习基于策略的方法，并研究这些方法的一个子集，称为策略梯度。然后，我们将使用 PyTorch 从头开始实现我们的第一个策略梯度算法，称为蒙特卡罗强化。然后，我们将测试其在
    CartPole-v1 和 PixelCopter 环境中的稳健性。
- en: You’ll then be able to iterate and improve this implementation for more advanced
    environments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将能够迭代并改进此实现，以适用于更高级的环境。
- en: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![环境](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
- en: Let’s get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
