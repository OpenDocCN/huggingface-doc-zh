- en: Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/introduction](https://huggingface.co/learn/deep-rl-course/unit6/introduction)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit6/introduction](https://huggingface.co/learn/deep-rl-course/unit6/introduction)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail](../Images/9f49f2880784ef300d40b68768960852.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![缩略图](../Images/9f49f2880784ef300d40b68768960852.png)'
- en: In unit 4, we learned about our first Policy-Based algorithm called **Reinforce**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4单元，我们学习了我们的第一个基于策略的算法叫做 **Reinforce**。
- en: In Policy-Based methods, **we aim to optimize the policy directly without using
    a value function**. More precisely, Reinforce is part of a subclass of *Policy-Based
    Methods* called *Policy-Gradient methods*. This subclass optimizes the policy
    directly by **estimating the weights of the optimal policy using Gradient Ascent**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方法中，**我们旨在直接优化策略而不使用价值函数**。更准确地说，Reinforce是*基于策略方法* 中的一个子类，称为*策略梯度方法*。这个子类通过**使用梯度上升来估计最优策略的权重**
    直接优化策略。
- en: We saw that Reinforce worked well. However, because we use Monte-Carlo sampling
    to estimate return (we use an entire episode to calculate the return), **we have
    significant variance in policy gradient estimation**.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到Reinforce效果很好。然而，因为我们使用蒙特卡洛采样来估计回报（我们使用整个情节来计算回报），**我们在策略梯度估计中有显著的方差**。
- en: Remember that the policy gradient estimation is **the direction of the steepest
    increase in return**. In other words, how to update our policy weights so that
    actions that lead to good returns have a higher probability of being taken. The
    Monte Carlo variance, which we will further study in this unit, **leads to slower
    training since we need a lot of samples to mitigate it**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，策略梯度估计是 **回报增加最陡峭的方向**。换句话说，如何更新我们的策略权重，使导致良好回报的行动有更高的被采取的概率。蒙特卡洛方差，我们将在本单元进一步研究，**导致训练速度较慢，因为我们需要大量样本来减轻它**。
- en: 'So today we’ll study **Actor-Critic methods**, a hybrid architecture combining
    value-based and Policy-Based methods that helps to stabilize the training by reducing
    the variance using:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以今天我们将学习 **演员评论家方法**，这是一种混合架构，结合了基于价值和基于策略的方法，通过使用以下方法来减少方差，从而稳定训练：
- en: '*An Actor* that controls **how our agent behaves** (Policy-Based method)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个演员* 控制 **我们的代理如何行为**（基于策略的方法）'
- en: '*A Critic* that measures **how good the taken action is** (Value-Based method)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个评论家* 评估 **采取的行动有多好**（基于价值的方法）'
- en: 'We’ll study one of these hybrid methods, Advantage Actor Critic (A2C), **and
    train our agent using Stable-Baselines3 in robotic environments**. We’ll train:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究其中一种混合方法，优势演员评论家（A2C），**并在机器人环境中使用Stable-Baselines3训练我们的代理**。我们将训练：
- en: A robotic arm 🦾 to move to the correct position.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个机械臂 🦾 移动到正确的位置。
- en: Sound exciting? Let’s get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很激动人心吗？让我们开始吧！
