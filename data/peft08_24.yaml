- en: Adapters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适配器
- en: 'Original text: [https://huggingface.co/docs/peft/conceptual_guides/adapter](https://huggingface.co/docs/peft/conceptual_guides/adapter)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/conceptual_guides/adapter](https://huggingface.co/docs/peft/conceptual_guides/adapter)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Adapter-based methods add extra trainable parameters after the attention and
    fully-connected layers of a frozen pretrained model to reduce memory-usage and
    speed up training. The method varies depending on the adapter, it could simply
    be an extra added layer or it could be expressing the weight updates ∆W as a low-rank
    decomposition of the weight matrix. Either way, the adapters are typically small
    but demonstrate comparable performance to a fully finetuned model and enable training
    larger models with fewer resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 基于适配器的方法在冻结的预训练模型的注意力和全连接层之后添加额外的可训练参数，以减少内存使用量并加快训练速度。适配器的方法因适配器而异，它可能只是添加一个额外的层，也可能将权重更新∆W表示为权重矩阵的低秩分解。无论如何，适配器通常很小，但表现与完全微调模型相当，并且使得用更少资源训练更大模型成为可能。
- en: This guide will give you a brief overview of the adapter methods supported by
    PEFT (if you’re interested in learning more details about a specific method, take
    a look at the linked paper).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将简要介绍PEFT支持的适配器方法（如果您对学习特定方法的更多细节感兴趣，请查看链接的论文）。
- en: Low-Rank Adaptation (LoRA)
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）
- en: LoRA is one of the most popular PEFT methods and a good starting point if you’re
    just getting started with PEFT. It was originally developed for large language
    models but it is a tremendously popular training method for diffusion models because
    of its efficiency and effectiveness.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA是最受欢迎的PEFT方法之一，如果您刚开始使用PEFT，这是一个很好的起点。最初是为大型语言模型开发的，但由于其效率和有效性，它成为扩散模型的非常流行的训练方法。
- en: As mentioned briefly earlier, [LoRA](https://hf.co/papers/2106.09685) is a technique
    that accelerates finetuning large models while consuming less memory.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面简要提到的，[LoRA](https://hf.co/papers/2106.09685)是一种加速微调大型模型的技术，同时消耗更少内存。
- en: LoRA represents the weight updates ∆W with two smaller matrices (called *update
    matrices*) through low-rank decomposition. These new matrices can be trained to
    adapt to the new data while keeping the overall number of parameters low. The
    original weight matrix remains frozen and doesn’t receive any further updates.
    To produce the final results, the original and extra adapted weights are combined.
    You could also merge the adapter weights with the base model to eliminate inference
    latency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA通过低秩分解用两个较小的矩阵（称为*更新矩阵*）表示权重更新∆W。这些新矩阵可以训练以适应新数据，同时保持总参数数量较低。原始权重矩阵保持冻结，不再接收任何更新。为了生成最终结果，将原始和额外适应的权重组合在一起。您还可以将适配器权重与基础模型合并，以消除推理延迟。
- en: '![](../Images/cda06893d37810e4cf53c848b789ce47.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cda06893d37810e4cf53c848b789ce47.png)'
- en: 'This approach has a number of advantages:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有许多优点：
- en: LoRA makes finetuning more efficient by drastically reducing the number of trainable
    parameters.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA通过大幅减少可训练参数的数量，使微调更加高效。
- en: The original pretrained weights are kept frozen, which means you can have multiple
    lightweight and portable LoRA models for various downstream tasks built on top
    of them.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的预训练权重保持冻结，这意味着您可以基于它们构建多个轻量且便携的LoRA模型，用于各种下游任务。
- en: LoRA is orthogonal to other parameter-efficient methods and can be combined
    with many of them.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA与其他参数高效方法正交，并且可以与许多方法结合使用。
- en: Performance of models finetuned using LoRA is comparable to the performance
    of fully finetuned models.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LoRA微调的模型的性能与完全微调模型的性能相当。
- en: In principle, LoRA can be applied to any subset of weight matrices in a neural
    network to reduce the number of trainable parameters. However, for simplicity
    and further parameter efficiency, LoRA is typically only applied to the attention
    blocks in Transformer models. The resulting number of trainable parameters in
    a LoRA model depends on the size of the update matrices, which is determined mainly
    by the rank `r` and the shape of the original weight matrix.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，LoRA可以应用于神经网络中的任何权重矩阵子集，以减少可训练参数的数量。但是，为了简单起见和进一步的参数效率，LoRA通常仅应用于Transformer模型中的注意力块。LoRA模型中的可训练参数数量取决于更新矩阵的大小，主要由秩`r`和原始权重矩阵的形状确定。
- en: '![](../Images/9d68304d8b38683336e1572972c45e38.png)[Navigating Text-To-Image
    Customization: From LyCORIS Fine-Tuning to Model Evaluation](https://hf.co/papers/2103.10385)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d68304d8b38683336e1572972c45e38.png)[导航文本到图像定制：从LyCORIS微调到模型评估](https://hf.co/papers/2103.10385)'
- en: Low-Rank Hadamard Product (LoHa)
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩Hadamard乘积（LoHa）
- en: Low-rank decomposition can impact performance because the weight updates are
    limited to the low-rank space, which can constrain a model’s expressiveness. However,
    you don’t necessarily want to use a larger rank because it increases the number
    of trainable parameters. To address this, [LoHa](https://huggingface.co/papers/2108.06098)
    (a method originally developed for computer vision) was applied to diffusion models
    where the ability to generate diverse images is an important consideration. LoHa
    should also work with general model types, but the embedding layers aren’t currently
    implemented in PEFT.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩分解可能会影响性能，因为权重更新受限于低秩空间，这可能会限制模型的表达能力。但是，您不一定希望使用更大的秩，因为这会增加可训练参数的数量。为了解决这个问题，[LoHa](https://huggingface.co/papers/2108.06098)（最初为计算机视觉开发的方法）被应用于扩散模型，其中生成多样化图像的能力是一个重要考虑因素。LoHa也应该适用于一般的模型类型，但嵌入层目前尚未在PEFT中实现。
- en: LoHa uses the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
    (element-wise product) instead of the matrix product. ∆W is represented by four
    smaller matrices instead of two - like in LoRA - and each pair of these low-rank
    matrices are combined with the Hadamard product. As a result, ∆W can have the
    same number of trainable parameters but a higher rank and expressivity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LoHa使用[Hadamard乘积](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))（逐元素乘积）代替矩阵乘积。∆W由四个较小的矩阵代替两个
    - 就像在LoRA中一样 - 每对这些低秩矩阵通过Hadamard乘积组合。因此，∆W可以具有相同数量的可训练参数，但具有更高的秩和表达能力。
- en: Low-Rank Kronecker Product (LoKr)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩Kronecker乘积（LoKr）
- en: '[LoKr](https://hf.co/papers/2309.14859) is very similar to LoRA and LoHa, and
    it is also mainly applied to diffusion models, though you could also use it with
    other model types. LoKr replaces the matrix product with the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product)
    instead. The Kronecker product decomposition creates a block matrix which preserves
    the rank of the original weight matrix. Another benefit of the Kronecker product
    is that it can be vectorized by stacking the matrix columns. This can speed up
    the process because you’re avoiding fully reconstructing ∆W.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[LoKr](https://hf.co/papers/2309.14859)与LoRA和LoHa非常相似，主要应用于扩散模型，尽管您也可以将其用于其他模型类型。LoKr将矩阵乘积替换为[Kronecker乘积](https://en.wikipedia.org/wiki/Kronecker_product)。Kronecker乘积分解创建一个保留原始权重矩阵秩的块矩阵。Kronecker乘积的另一个好处是可以通过堆叠矩阵列来向量化。这可以加快过程，因为您避免了完全重建∆W。'
- en: Orthogonal Finetuning (OFT)
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正交微调（OFT）
- en: '![](../Images/856b08d13033d86928eab989e7cface4.png)[Controlling Text-to-Image
    Diffusion by Orthogonal Finetuning](https://hf.co/papers/2306.07280)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '[通过正交微调控制文本到图像扩散](https://hf.co/papers/2306.07280)'
- en: '[OFT](https://hf.co/papers/2306.07280) is a method that primarily focuses on
    preserving a pretrained model’s generative performance in the finetuned model.
    It tries to maintain the same cosine similarity (hyperspherical energy) between
    all pairwise neurons in a layer because this better captures the semantic information
    among neurons. This means OFT is more capable at preserving the subject and it
    is better for controllable generation (similar to [ControlNet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: OFT是一种主要关注在微调模型中保留预训练模型生成性能的方法。它试图保持层中所有成对神经元之间的余弦相似度（超球能量）相同，因为这更好地捕捉神经元之间的语义信息。这意味着OFT更擅长保留主题，对于可控生成更好（类似于[ControlNet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)）。
- en: OFT preserves the hyperspherical energy by learning an orthogonal transformation
    for neurons to keep the cosine similarity between them unchanged. In practice,
    this means taking the matrix product of an orthogonal matrix with the pretrained
    weight matrix. However, to be parameter-efficient, the orthogonal matrix is represented
    as a block-diagonal matrix with rank `r` blocks. Whereas LoRA reduces the number
    of trainable parameters with low-rank structures, OFT reduces the number of trainable
    parameters with a sparse block-diagonal matrix structure.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: OFT通过学习一个正交变换来保持神经元之间的余弦相似度不变来保留超球能量。在实践中，这意味着使用正交矩阵与预训练权重矩阵的矩阵乘积。然而，为了参数高效，正交矩阵被表示为具有秩`r`块的块对角矩阵。LoRA通过低秩结构减少可训练参数的数量，OFT通过稀疏块对角矩阵结构减少可训练参数的数量。
- en: Adaptive Low-Rank Adaptation (AdaLoRA)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应低秩适应（AdaLoRA）
- en: '[AdaLoRA](https://hf.co/papers/2303.10512) manages the parameter budget introduced
    from LoRA by allocating more parameters - in other words, a higher rank `r` -
    for important weight matrices that are better adapted for a task and pruning less
    important ones. The rank is controlled by a method similar to singular value decomposition
    (SVD). The ∆W is parameterized with two orthogonal matrices and a diagonal matrix
    which contains singular values. This parametrization method avoids iteratively
    applying SVD which is computationally expensive. Based on this method, the rank
    of ∆W is adjusted according to an importance score. ∆W is divided into triplets
    and each triplet is scored according to its contribution to model performance.
    Triplets with low importance scores are pruned and triplets with high importance
    scores are kept for finetuning.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdaLoRA](https://hf.co/papers/2303.10512)通过为更适合任务的重要权重矩阵分配更多参数（即更高的秩`r`），管理LoRA引入的参数预算，并修剪不太重要的权重矩阵。秩由类似于奇异值分解（SVD）的方法控制。∆W由两个正交矩阵和一个包含奇异值的对角矩阵参数化。这种参数化方法避免了计算昂贵的迭代应用SVD。根据这种方法，根据重要性评分调整∆W的秩。∆W被分成三元组，每个三元组根据其对模型性能的贡献进行评分。重要性评分低的三元组被修剪，重要性评分高的三元组被保留用于微调。'
- en: Llama-Adapter
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama-Adapter
- en: '[Llama-Adapter](https://hf.co/papers/2303.16199) is a method for adapting Llama
    into a instruction-following model. To help adapt the model for instruction-following,
    the adapter is trained with a 52K instruction-output dataset.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[Llama-Adapter](https://hf.co/papers/2303.16199)是一种将Llama调整为指令跟踪模型的方法。为了帮助调整模型以进行指令跟踪，适配器使用了一个包含52K个指令-输出数据集的训练。'
- en: A set of of learnable adaption prompts are prefixed to the input instruction
    tokens. These are inserted into the upper layers of the model because it is better
    to learn with the higher-level semantics of the pretrained model. The instruction-output
    tokens prefixed to the input guide the adaption prompt to generate a contextual
    response.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一组可学习的适应提示被添加到输入指令标记之前。这些提示被插入到模型的上层，因为最好使用预训练模型的更高级语义进行学习。添加到输入的指令-输出标记引导适应提示生成上下文响应。
- en: '![](../Images/e71baed69bac87945cc57a6635bafdbf.png)[LLaMA-Adapter: Efficient
    Fine-tuning of Language Models with Zero-init Attention](https://hf.co/papers/2303.16199)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '[LLaMA-Adapter：使用零初始化注意力高效微调语言模型](https://hf.co/papers/2303.16199)'
- en: To avoid adding noise to the tokens, the adapter uses zero-initialized attention.
    On top of this, the adapter adds a learnable gating factor (initialized with zeros)
    to progressively add information to the model during training. This prevents overwhelming
    the model’s pretrained knowledge with the newly learned instructions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免向标记添加噪音，适配器使用零初始化的注意力。除此之外，适配器还添加了一个可学习的门控因子（初始化为零），在训练过程中逐渐向模型添加信息。这可以防止新学习的指令过多地压倒模型的预训练知识。
