- en: Messages API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息API
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/messages_api](https://huggingface.co/docs/text-generation-inference/messages_api)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/text-generation-inference/messages_api](https://huggingface.co/docs/text-generation-inference/messages_api)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference (TGI) now supports the Messages API, which is fully
    compatible with the OpenAI Chat Completion API. This feature is available starting
    from version 1.4.0\. You can use OpenAI’s client libraries or third-party libraries
    expecting OpenAI schema to interact with TGI’s Messages API. Below are some examples
    of how to utilize this compatibility.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成推理（TGI）现在支持消息API，与OpenAI聊天完成API完全兼容。此功能从版本1.4.0开始提供。您可以使用OpenAI的客户端库或期望OpenAI模式的第三方库与TGI的消息API进行交互。以下是一些如何利用此兼容性的示例。
- en: '**Note:** The Messages API is supported from TGI version 1.4.0 and above. Ensure
    you are using a compatible version to access this feature.'
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**消息API支持TGI版本1.4.0及以上。确保您使用兼容版本以访问此功能。'
- en: Table of Contents
  id: totrans-5
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目录
- en: '[Making a Request](#making-a-request)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[发出请求](#making-a-request)'
- en: '[Streaming](#streaming)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[流式传输](#streaming)'
- en: '[Synchronous](#synchronous)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[同步](#synchronous)'
- en: '[Hugging Face Inference Endpoints](#hugging-face-inference-endpoints)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[拥抱脸推理端点](#hugging-face-inference-endpoints)'
- en: '[Cloud Providers](#cloud-providers)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[云服务提供商](#cloud-providers)'
- en: '[Amazon SageMaker](#amazon-sagemaker)'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[亚马逊SageMaker](#amazon-sagemaker)'
- en: Making a Request
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发出请求
- en: 'You can make a request to TGI’s Messages API using `curl`. Here’s an example:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`curl`向TGI的消息API发出请求。以下是一个示例：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Streaming
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式传输
- en: 'You can also use OpenAI’s Python client library to make a streaming request.
    Here’s how:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用OpenAI的Python客户端库发出流式请求。以下是如何操作的示例：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Synchronous
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步
- en: 'If you prefer to make a synchronous request, you can do so like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更喜欢发出同步请求，可以这样做：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Hugging Face Inference Endpoints
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拥抱脸推理端点
- en: The Messages API is integrated with [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 消息API已与[推理端点](https://huggingface.co/inference-endpoints/dedicated)集成。
- en: 'Every endpoint that uses “Text Generation Inference” with an LLM, which has
    a chat template can now be used. Below is an example of how to use IE with TGI
    using OpenAI’s Python client library:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用具有聊天模板的LLM的“文本生成推理”来使用每个端点。以下是如何使用OpenAI的Python客户端库与TGI一起使用IE的示例：
- en: '**Note:** Make sure to replace `base_url` with your endpoint URL and to include
    `v1/` at the end of the URL. The `api_key` should be replaced with your Hugging
    Face API key.'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**确保将`base_url`替换为您的端点URL，并在URL末尾包含`v1/`。`api_key`应替换为您的拥抱脸API密钥。'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Cloud Providers
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云服务提供商
- en: 'TGI can be deployed on various cloud providers for scalable and robust text
    generation. One such provider is Amazon SageMaker, which has recently added support
    for TGI. Here’s how you can deploy TGI on Amazon SageMaker:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TGI可以部署在各种云提供商上，用于可扩展和强大的文本生成。亚马逊SageMaker是其中之一，最近已添加对TGI的支持。以下是如何在亚马逊SageMaker上部署TGI的方法：
- en: Amazon SageMaker
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊SageMaker
- en: To enable the Messages API in Amazon SageMaker you need to set the environment
    variable `MESSAGES_API_ENABLED=true`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要在亚马逊SageMaker中启用消息API，您需要设置环境变量`MESSAGES_API_ENABLED=true`。
- en: This will modify the `/invocations` route to accept Messages dictonaries consisting
    out of role and content. See the example below on how to deploy Llama with the
    new Messages API.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这将修改`/invocations`路由以接受由角色和内容组成的消息字典。请参考下面的示例，了解如何使用新的消息API部署Llama。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
