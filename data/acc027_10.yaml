- en: Troubleshooting guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除指南
- en: 'Original text: [https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting](https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting](https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This guide aims to provide you the tools and knowledge required to navigate
    some common issues. However, as 🤗 Accelerate continuously evolves and the use
    cases and setups are diverse, you might encounter an issue not covered in this
    guide. If the suggestions listed in this guide do not cover your such situation,
    please refer to the final section of the guide, [Asking for Help](#ask-for-help),
    to learn where to find help with your specific issue.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南旨在为您提供导航一些常见问题所需的工具和知识。然而，由于🤗 Accelerate不断发展，用例和设置多种多样，您可能会遇到本指南未涵盖的问题。如果本指南中列出的建议未涵盖您的情况，请参考指南的最后一部分[寻求帮助](#ask-for-help)，了解如何在特定问题上寻求帮助。
- en: Logging
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: When facing an error, logging can help narrow down where it is coming from.
    In a distributed setup with multiple processes, logging can be a challenge, but
    🤗 Accelerate provides a utility that streamlines the logging process and ensures
    that logs are synchronized and managed effectively across the distributed setup.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 面对错误时，日志记录可以帮助缩小错误来源。在具有多个进程的分布式设置中，日志记录可能是一个挑战，但🤗 Accelerate提供了一个工具，简化了日志记录过程，并确保日志在分布式设置中同步和有效管理。
- en: 'To troubleshoot an issue, use `accelerate.logging` instead of the standard
    Python `logging` module:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决问题，请使用`accelerate.logging`而不是标准的Python `logging`模块：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To set the log level (`INFO`, `DEBUG`, `WARNING`, `ERROR`, `CRITICAL`), export
    it as the `ACCELERATE_LOG_LEVEL` environment, or pass as `log_level` to `get_logger`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置日志级别（`INFO`，`DEBUG`，`WARNING`，`ERROR`，`CRITICAL`），请将其导出为`ACCELERATE_LOG_LEVEL`环境，或将其作为`log_level`传递给`get_logger`：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By default, the log is called on main processes only. To call it on all processes,
    pass `main_process_only=False`. If a log should be called on all processes and
    in order, also pass `in_order=True`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，日志仅在主进程上调用。要在所有进程上调用它，请传递`main_process_only=False`。如果日志应在所有进程上按顺序调用，还要传递`in_order=True`。
- en: Hanging code and timeout errors
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挂起代码和超时错误
- en: Mismatched tensor shapes
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不匹配的张量形状
- en: If your code seems to be hanging for a significant amount time on a distributed
    setup, a common cause is mismatched shapes of tensors on different devices.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的代码在分布式设置上挂起了很长时间，一个常见原因是不同设备上张量的形状不匹配。
- en: When running scripts in a distributed fashion, functions such as [Accelerator.gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)
    and [Accelerator.reduce()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.reduce)
    are necessary to grab tensors across devices to perform operations on them collectively.
    These (and other) functions rely on `torch.distributed` performing a `gather`
    operation, which requires that tensors have the **exact same shape** across all
    processes. When the tensor shapes don’t match, you will experience handing code,
    and eventually hit a timeout exception.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在以分布式方式运行脚本时，诸如[Accelerator.gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)和[Accelerator.reduce()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.reduce)等函数是必要的，以跨设备获取张量以集体执行操作。这些（以及其他）函数依赖于`torch.distributed`执行`gather`操作，这要求张量在所有进程中具有**完全相同的形状**。当张量形状不匹配时，您将遇到挂起代码，并最终触发超时异常。
- en: If you suspect this to be the case, use Accelerate’s operational debug mode
    to immediately catch the issue.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您怀疑这种情况，请使用Accelerate的操作调试模式立即捕捉问题。
- en: 'The recommended way to enable Accelerate’s operational debug mode is during
    `accelerate config` setup. Alternative ways to enable debug mode are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 启用Accelerate的操作调试模式的推荐方法是在`accelerate config`设置期间。启用调试模式的替代方法包括：
- en: 'From the CLI:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从CLI：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As an environmental variable (which avoids the need for `accelerate launch`):'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为环境变量（避免需要`accelerate launch`）：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Manually changing the `config.yaml` file:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动更改`config.yaml`文件：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once you enable the debug mode, you should get a similar traceback that points
    to the tensor shape mismatch issue:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用调试模式，您应该获得类似的回溯，指向张量形状不匹配问题：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Early stopping leads to hanging
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早停止导致挂起
- en: When doing early stopping in distributed training, if each process has a specific
    stopping condition (e.g. validation loss), it may not be synchronized across all
    of them. As a result, a break can happen on process 0 but not on process 1. This
    will cause the code to hang indefinitely until a timeout occurs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练中进行早停止时，如果每个进程都有特定的停止条件（例如验证损失），可能不会在所有进程之间同步。结果，进程0上可能发生中断，但进程1上可能没有。这将导致代码无限期挂起，直到超时发生。
- en: 'If you have early stopping conditionals, use `set_breakpoint` and `check_breakpoint`
    methods to make sure all the processes are ended correctly:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有早停止条件，请使用`set_breakpoint`和`check_breakpoint`方法确保所有进程正确结束：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hanging on low kernel versions on Linux
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Linux低内核版本上挂起
- en: This is a known issue. On Linux with kernel version < 5.5, hanging processes
    have been reported. To avoid encountering this problem, we recommend upgrading
    your system to a later kernel version.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个已知问题。在Linux上，内核版本<5.5，已经报告了挂起进程。为了避免遇到这个问题，我们建议将系统升级到更高版本的内核。
- en: CUDA out of memory
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA内存不足
- en: One of the most frustrating errors when it comes to running training scripts
    is hitting “CUDA Out-of-Memory”, as the entire script needs to be restarted, progress
    is lost, and typically a developer would want to simply start their script and
    let it run.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行训练脚本时，最令人沮丧的错误之一是遇到“CUDA内存不足”，因为整个脚本需要重新启动，进度丢失，通常开发人员希望简单地启动他们的脚本并让其运行。
- en: To address this problem, `Accelerate` offers a utility `find_executable_batch_size`
    that is heavily based on [toma](https://github.com/BlackHC/toma). The utility
    retries code that fails due to OOM (out-of-memory) conditions and lowers batch
    sizes automatically.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决此问题，`Accelerate`提供了一个实用程序`find_executable_batch_size`，它在很大程度上基于[toma](https://github.com/BlackHC/toma)。该实用程序会重试由于OOM（内存不足）条件而失败的代码，并自动降低批量大小。
- en: find_executable_batch_size
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: find_executable_batch_size
- en: This algorithm operates with exponential decay, decreasing the batch size in
    half after each failed run on some training script. To use it, restructure your
    training function to include an inner function that includes this wrapper, and
    build your dataloaders inside it. At a minimum, this could look like 4 new lines
    of code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用指数衰减，在某些训练脚本的每次失败运行后将批量大小减半。要使用它，重构您的训练函数以包含一个包含此包装器的内部函数，并在其中构建您的数据加载器。至少，这可能看起来像4行新代码。
- en: The inner function *must* take in the batch size as the first parameter, but
    we do not pass one to it when called. The wrapper handles this for us.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 内部函数*必须*将批量大小作为第一个参数传入，但在调用时我们不会传递一个。包装器会为我们处理这个。
- en: It should also be noted that anything which will consume CUDA memory and passed
    to the `accelerator` **must** be declared inside the inner function, such as models
    and optimizers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 还应注意，任何将消耗CUDA内存并传递给`accelerator`的内容**必须**在内部函数中声明，例如模型和优化器。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To find out more, check the documentation [here](../package_reference/utilities#accelerate.find_executable_batch_size).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多，请查看此处的文档[here](../package_reference/utilities#accelerate.find_executable_batch_size)。
- en: Non-reproducible results between device setups
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设备设置之间的不可重现结果
- en: If you have changed the device setup and are observing different model performance,
    this is likely due to the fact that you have not updated your script when moving
    from one setup to another. The same script with the same batch size across TPU,
    multi-GPU, and single-GPU with Accelerate will have different results.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已更改设备设置并观察到不同的模型性能，这很可能是因为在从一个设置转移到另一个设置时，您没有更新脚本。在TPU、多GPU和单GPU与Accelerate下，相同批量大小的相同脚本将产生不同的结果。
- en: For example, if you were previously training on a single GPU with a batch size
    of 16, when moving to two GPU setup, you need to change the batch size to 8 to
    have the same effective batch size. This is because when training with Accelerate,
    the batch size passed to the dataloader is the **batch size per GPU**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您以前在单GPU上训练，批量大小为16，当转移到两个GPU设置时，您需要将批量大小更改为8，以获得相同的有效批量大小。这是因为在使用Accelerate进行训练时，传递给数据加载器的批量大小是**每个GPU的批量大小**。
- en: To make sure you can reproduce the results between the setups, make sure to
    use the same seed, adjust the batch size accordingly, consider scaling the learning
    rate.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保您可以在不同设置之间重现结果，请确保使用相同的种子，根据情况调整批量大小，考虑调整学习率。
- en: For more details and a quick reference for batch sizes, check out the [Comparing
    performance between different device setups](../concept_guides/performance) guide.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有关批量大小的更多详细信息和快速参考，请查看[比较不同设备设置之间性能](../concept_guides/performance)指南。
- en: Performance issues on different GPUs
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同GPU上的性能问题
- en: 'If your multi-GPU setup consists of different GPUs, you may hit some limitations:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的多GPU设置包含不同的GPU，可能会遇到一些限制：
- en: There may be an imbalance in GPU memory between the GPUs. In this case, the
    GPU with smaller memory will limit the batch size or the size of the model that
    can be loaded onto the GPUs.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU之间的显存可能存在不平衡。在这种情况下，显存较小的GPU将限制批量大小或可以加载到GPU上的模型大小。
- en: If you are using GPUs with different performance profiles, the performance will
    be driven by the slowest GPU that you are using as the other GPUs will have to
    wait for it to complete its workload.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用具有不同性能配置文件的GPU，性能将由您使用的最慢的GPU驱动，因为其他GPU将不得不等待其完成工作负载。
- en: Vastly different GPUs within the same setup can lead to performance bottlenecks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同一设置中极不同的GPU可能导致性能瓶颈。
- en: Ask for help
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻求帮助
- en: If the above troubleshooting tools and advice did not help you resolve your
    issue, reach out for help to the community and the team.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上述故障排除工具和建议无法帮助您解决问题，请向社区和团队寻求帮助。
- en: Forums
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论坛
- en: Ask for help on the Hugging Face forums - post your question in the [🤗Accelerate
    category](https://discuss.huggingface.co/c/accelerate/18) Make sure to write a
    descriptive post with relevant context about your setup and reproducible code
    to maximize the likelihood that your problem is solved!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face论坛上寻求帮助-在[🤗Accelerate类别](https://discuss.huggingface.co/c/accelerate/18)中发布您的问题。确保写一个描述性的帖子，提供有关您的设置和可重现代码的相关上下文，以最大程度地提高解决问题的可能性！
- en: Discord
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Discord
- en: Post a question on [Discord](http://hf.co/join/discord), and let the team and
    the community help you.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Discord](http://hf.co/join/discord)上提问，让团队和社区帮助您。
- en: GitHub Issues
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GitHub问题
- en: Create an Issue on the 🤗 Accelerate [GitHub repository](https://github.com/huggingface/accelerate/issues)
    if you suspect to have found a bug related to the library. Include context regarding
    the bug and details about your distributed setup to help us better figure out
    what’s wrong and how we can fix it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您怀疑发现与库相关的错误，请在🤗 Accelerate [GitHub存储库](https://github.com/huggingface/accelerate/issues)上创建一个问题。包括有关错误的上下文和有关您的分布式设置的详细信息，以帮助我们更好地找出问题所在以及如何解决它。
