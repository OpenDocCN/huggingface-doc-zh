- en: Quick Tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速浏览
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/quicktour](https://huggingface.co/docs/text-generation-inference/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/text-generation-inference/quicktour](https://huggingface.co/docs/text-generation-inference/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way of getting started is using the official Docker container. Install
    Docker following [their installation instructions](https://docs.docker.com/get-docker/).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 开始的最简单方法是使用官方的Docker容器。按照[它们的安装说明](https://docs.docker.com/get-docker/)安装Docker。
- en: 'Let’s say you want to deploy [Falcon-7B Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)
    model with TGI. Here is an example on how to do that:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要部署[Falcon-7B Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)模型与TGI。以下是如何执行的示例：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To use NVIDIA GPUs, you need to install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html).
    We also recommend using NVIDIA drivers with CUDA version 12.2 or higher.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用NVIDIA GPU，您需要安装[NVIDIA容器工具包](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)。我们还建议使用带有CUDA版本12.2或更高版本的NVIDIA驱动程序。
- en: 'TGI also supports ROCm-enabled AMD GPUs (only MI210 and MI250 are tested),
    details are available in the [Supported Hardware section](./supported_models#supported-hardware)
    and [AMD documentation](https://rocm.docs.amd.com/en/latest/deploy/docker.html).
    To launch TGI on ROCm GPUs, please use instead:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TGI还支持启用ROCm的AMD GPU（仅MI210和MI250经过测试），详细信息请参阅[支持的硬件部分](./supported_models#supported-hardware)和[AMD文档](https://rocm.docs.amd.com/en/latest/deploy/docker.html)。要在ROCm
    GPU上启动TGI，请改用：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once TGI is running, you can use the `generate` endpoint by doing requests.
    To learn more about how to query the endpoints, check the [Consuming TGI](./basic_tutorials/consuming_tgi)
    section, where we show examples with utility libraries and UIs. Below you can
    see a simple snippet to query the endpoint.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦TGI运行起来，您可以通过请求使用`generate`端点。要了解有关如何查询端点的更多信息，请查看[使用TGI](./basic_tutorials/consuming_tgi)部分，我们在那里展示了使用实用程序库和UI的示例。下面是一个查询端点的简单代码片段。
- en: PythonJavaScriptcURL
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To see all possible deploy flags and options, you can use the `--help` flag.
    It’s possible to configure the number of shards, quantization, generation parameters,
    and more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看所有可能的部署标志和选项，您可以使用`--help`标志。可以配置分片数、量化、生成参数等。
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
