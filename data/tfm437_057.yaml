- en: Export to ONNX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¼å‡ºä¸ºONNX
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/serialization](https://huggingface.co/docs/transformers/v4.37.2/en/serialization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/serialization](https://huggingface.co/docs/transformers/v4.37.2/en/serialization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Deploying ğŸ¤— Transformers models in production environments often requires, or
    can benefit from exporting the models into a serialized format that can be loaded
    and executed on specialized runtimes and hardware.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ğŸ¤— Transformersæ¨¡å‹é€šå¸¸éœ€è¦å°†æ¨¡å‹å¯¼å‡ºä¸ºå¯ä»¥åœ¨ä¸“ç”¨è¿è¡Œæ—¶å’Œç¡¬ä»¶ä¸ŠåŠ è½½å’Œæ‰§è¡Œçš„åºåˆ—åŒ–æ ¼å¼ã€‚
- en: ğŸ¤— Optimum is an extension of Transformers that enables exporting models from
    PyTorch or TensorFlow to serialized formats such as ONNX and TFLite through its
    `exporters` module. ğŸ¤— Optimum also provides a set of performance optimization
    tools to train and run models on targeted hardware with maximum efficiency.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimumæ˜¯Transformersçš„æ‰©å±•ï¼Œé€šè¿‡å…¶`exporters`æ¨¡å—ä½¿å¾—å¯ä»¥å°†æ¨¡å‹ä»PyTorchæˆ–TensorFlowå¯¼å‡ºä¸ºONNXå’ŒTFLiteç­‰åºåˆ—åŒ–æ ¼å¼ã€‚ğŸ¤—
    Optimumè¿˜æä¾›äº†ä¸€å¥—æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œä»¥åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šä»¥æœ€å¤§æ•ˆç‡è®­ç»ƒå’Œè¿è¡Œæ¨¡å‹ã€‚
- en: This guide demonstrates how you can export ğŸ¤— Transformers models to ONNX with
    ğŸ¤— Optimum, for the guide on exporting models to TFLite, please refer to the [Export
    to TFLite page](tflite).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ğŸ¤— Optimumå°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNXï¼Œæœ‰å…³å°†æ¨¡å‹å¯¼å‡ºä¸ºTFLiteçš„æŒ‡å—ï¼Œè¯·å‚è€ƒ[å¯¼å‡ºåˆ°TFLiteé¡µé¢](tflite)ã€‚
- en: Export to ONNX
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å‡ºä¸ºONNX
- en: '[ONNX (Open Neural Network eXchange)](http://onnx.ai) is an open standard that
    defines a common set of operators and a common file format to represent deep learning
    models in a wide variety of frameworks, including PyTorch and TensorFlow. When
    a model is exported to the ONNX format, these operators are used to construct
    a computational graph (often called an *intermediate representation*) which represents
    the flow of data through the neural network.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[ONNXï¼ˆOpen Neural Network eXchangeï¼‰](http://onnx.ai)æ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†ï¼Œå®šä¹‰äº†ä¸€ç»„é€šç”¨æ“ä½œç¬¦å’Œä¸€ç§é€šç”¨æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºåœ¨å„ç§æ¡†æ¶ä¸­è¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬PyTorchå’ŒTensorFlowã€‚å½“æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼æ—¶ï¼Œè¿™äº›æ“ä½œç¬¦ç”¨äºæ„å»ºè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸º*ä¸­é—´è¡¨ç¤º*ï¼‰ï¼Œè¡¨ç¤ºæ•°æ®é€šè¿‡ç¥ç»ç½‘ç»œçš„æµåŠ¨ã€‚'
- en: By exposing a graph with standardized operators and data types, ONNX makes it
    easy to switch between frameworks. For example, a model trained in PyTorch can
    be exported to ONNX format and then imported in TensorFlow (and vice versa).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–æ“ä½œç¬¦å’Œæ•°æ®ç±»å‹çš„å›¾ï¼ŒONNXä½¿å¾—åœ¨ä¸åŒæ¡†æ¶ä¹‹é—´è½»æ¾åˆ‡æ¢å˜å¾—å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œåœ¨PyTorchä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼Œç„¶ååœ¨TensorFlowä¸­å¯¼å…¥ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚
- en: 'Once exported to ONNX format, a model can be:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼åï¼Œå¯ä»¥ï¼š
- en: optimized for inference via techniques such as [graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization)
    and [quantization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡è¯¸å¦‚[å›¾ä¼˜åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization)å’Œ[é‡åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)ç­‰æŠ€æœ¯è¿›è¡Œæ¨ç†ä¼˜åŒ–ã€‚
- en: run with ONNX Runtime via [`ORTModelForXXX` classes](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort),
    which follow the same `AutoModel` API as the one you are used to in ğŸ¤— Transformers.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ONNX Runtimeè¿è¡Œï¼Œé€šè¿‡[`ORTModelForXXX`ç±»](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort)ï¼Œå…¶éµå¾ªä¸ğŸ¤—
    Transformersä¸­æ‚¨ä¹ æƒ¯çš„`AutoModel` APIç›¸åŒã€‚
- en: run with [optimized inference pipelines](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines),
    which has the same API as the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    function in ğŸ¤— Transformers.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[ä¼˜åŒ–æ¨ç†æµæ°´çº¿](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines)è¿è¡Œï¼Œå…¶APIä¸ğŸ¤—
    Transformersä¸­çš„[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)å‡½æ•°ç›¸åŒã€‚
- en: ğŸ¤— Optimum provides support for the ONNX export by leveraging configuration objects.
    These configuration objects come ready-made for a number of model architectures,
    and are designed to be easily extendable to other architectures.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimumé€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æä¾›å¯¹ONNXå¯¼å‡ºçš„æ”¯æŒã€‚è¿™äº›é…ç½®å¯¹è±¡å·²ç»ä¸ºè®¸å¤šæ¨¡å‹æ¶æ„å‡†å¤‡å¥½ï¼Œå¹¶ä¸”è®¾è®¡ä¸ºæ˜“äºæ‰©å±•åˆ°å…¶ä»–æ¶æ„ã€‚
- en: For the list of ready-made configurations, please refer to [ğŸ¤— Optimum documentation](https://huggingface.co/docs/optimum/exporters/onnx/overview).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³ç°æˆé…ç½®åˆ—è¡¨ï¼Œè¯·å‚é˜…[ğŸ¤— Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/overview)ã€‚
- en: 'There are two ways to export a ğŸ¤— Transformers model to ONNX, here we show both:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§å°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNXçš„æ–¹æ³•ï¼Œè¿™é‡Œæˆ‘ä»¬å±•ç¤ºä¸¤ç§ï¼š
- en: export with ğŸ¤— Optimum via CLI.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡CLIä½¿ç”¨ğŸ¤— Optimumå¯¼å‡ºã€‚
- en: export with ğŸ¤— Optimum with `optimum.onnxruntime`.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`optimum.onnxruntime`ä¸ğŸ¤— Optimumå¯¼å‡ºã€‚
- en: Exporting a ğŸ¤— Transformers model to ONNX with CLI
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨CLIå°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNX
- en: 'To export a ğŸ¤— Transformers model to ONNX, first install an extra dependency:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNXï¼Œé¦–å…ˆå®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To check out all available arguments, refer to the [ğŸ¤— Optimum docs](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli),
    or view help in command line:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°ï¼Œè¯·å‚è€ƒ[ğŸ¤— Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ï¼Œæˆ–åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹å¸®åŠ©ï¼š
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To export a modelâ€™s checkpoint from the ğŸ¤— Hub, for example, `distilbert-base-uncased-distilled-squad`,
    run the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä»ğŸ¤— Hubå¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚`distilbert-base-uncased-distilled-squad`ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see the logs indicating progress and showing where the resulting
    `model.onnx` is saved, like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥çœ‹åˆ°æŒ‡ç¤ºè¿›åº¦å¹¶æ˜¾ç¤ºç»“æœ`model.onnx`ä¿å­˜ä½ç½®çš„æ—¥å¿—ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The example above illustrates exporting a checkpoint from ğŸ¤— Hub. When exporting
    a local model, first make sure that you saved both the modelâ€™s weights and tokenizer
    files in the same directory (`local_path`). When using CLI, pass the `local_path`
    to the `model` argument instead of the checkpoint name on ğŸ¤— Hub and provide the
    `--task` argument. You can review the list of supported tasks in the [ğŸ¤— Optimum
    documentation](https://huggingface.co/docs/optimum/exporters/task_manager). If
    `task` argument is not provided, it will default to the model architecture without
    any task specific head.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„ç¤ºä¾‹è¯´æ˜äº†ä» ğŸ¤— Hub å¯¼å‡ºæ£€æŸ¥ç‚¹ã€‚åœ¨å¯¼å‡ºæœ¬åœ°æ¨¡å‹æ—¶ï¼Œé¦–å…ˆç¡®ä¿æ‚¨å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ä¸­ï¼ˆ`local_path`ï¼‰ã€‚åœ¨ä½¿ç”¨
    CLI æ—¶ï¼Œå°† `local_path` ä¼ é€’ç»™ `model` å‚æ•°ï¼Œè€Œä¸æ˜¯åœ¨ ğŸ¤— Hub ä¸Šæä¾›æ£€æŸ¥ç‚¹åç§°ï¼Œå¹¶æä¾› `--task` å‚æ•°ã€‚æ‚¨å¯ä»¥åœ¨ [ğŸ¤—
    Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/task_manager) ä¸­æŸ¥çœ‹æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚å¦‚æœæœªæä¾›
    `task` å‚æ•°ï¼Œå®ƒå°†é»˜è®¤ä¸ºæ²¡æœ‰ä»»ä½•ç‰¹å®šä»»åŠ¡å¤´çš„æ¨¡å‹æ¶æ„ã€‚
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The resulting `model.onnx` file can then be run on one of the [many accelerators](https://onnx.ai/supported-tools.html#deployModel)
    that support the ONNX standard. For example, we can load and run the model with
    [ONNX Runtime](https://onnxruntime.ai/) as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼å‡ºçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒONNXæ ‡å‡†çš„ [è®¸å¤šåŠ é€Ÿå™¨](https://onnx.ai/supported-tools.html#deployModel)
    ä¸­è¿è¡Œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [ONNX Runtime](https://onnxruntime.ai/) åŠ è½½å’Œè¿è¡Œæ¨¡å‹å¦‚ä¸‹ï¼š
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The process is identical for TensorFlow checkpoints on the Hub. For instance,
    hereâ€™s how you would export a pure TensorFlow checkpoint from the [Keras organization](https://huggingface.co/keras-io):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹å¯¹äº Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯å¦‚ä½•ä» [Keras ç»„ç»‡](https://huggingface.co/keras-io)
    å¯¼å‡ºä¸€ä¸ªçº¯ TensorFlow æ£€æŸ¥ç‚¹çš„ï¼š
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exporting a ğŸ¤— Transformers model to ONNX with optimum.onnxruntime
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ optimum.onnxruntime å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNX
- en: 'Alternative to CLI, you can export a ğŸ¤— Transformers model to ONNX programmatically
    like so:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ CLI çš„æ›¿ä»£æ–¹æ³•æ˜¯ï¼Œæ‚¨å¯ä»¥åƒè¿™æ ·ä»¥ç¼–ç¨‹æ–¹å¼å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNXï¼š
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Exporting a model for an unsupported architecture
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹å¯¼å‡ºåˆ°ä¸å—æ”¯æŒçš„æ¶æ„
- en: If you wish to contribute by adding support for a model that cannot be currently
    exported, you should first check if it is supported in [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview),
    and if it is not, [contribute to ğŸ¤— Optimum](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)
    directly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¸Œæœ›é€šè¿‡ä¸ºå½“å‰æ— æ³•å¯¼å‡ºçš„æ¨¡å‹æ·»åŠ æ”¯æŒæ¥åšå‡ºè´¡çŒ®ï¼Œæ‚¨åº”è¯¥é¦–å…ˆæ£€æŸ¥å®ƒæ˜¯å¦åœ¨ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview)
    ä¸­å—æ”¯æŒï¼Œå¦‚æœä¸æ˜¯ï¼Œå¯ä»¥ç›´æ¥ [ä¸º ğŸ¤— Optimum åšå‡ºè´¡çŒ®](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã€‚
- en: Exporting a model with transformers.onnx
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ transformers.onnx å¯¼å‡ºæ¨¡å‹
- en: '`tranformers.onnx` is no longer maintained, please export models with ğŸ¤— Optimum
    as described above. This section will be removed in the future versions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`tranformers.onnx` ä¸å†ç»´æŠ¤ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°ä½¿ç”¨ ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹çš„æ–¹æ³•ã€‚è¿™ä¸€éƒ¨åˆ†å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­è¢«ç§»é™¤ã€‚'
- en: 'To export a ğŸ¤— Transformers model to ONNX with `tranformers.onnx`, install extra
    dependencies:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ `transformers.onnx` å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNXï¼Œéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–ï¼š
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Use `transformers.onnx` package as a Python module to export a checkpoint using
    a ready-made configuration:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `transformers.onnx` åŒ…ä½œä¸º Python æ¨¡å—ï¼Œä½¿ç”¨ç°æˆçš„é…ç½®å¯¼å‡ºæ£€æŸ¥ç‚¹ï¼š
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This exports an ONNX graph of the checkpoint defined by the `--model` argument.
    Pass any checkpoint on the ğŸ¤— Hub or one thatâ€™s stored locally. The resulting `model.onnx`
    file can then be run on one of the many accelerators that support the ONNX standard.
    For example, load and run the model with ONNX Runtime as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å¯¼å‡ºç”± `--model` å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ ONNX å›¾ã€‚ä¼ é€’ä»»ä½•åœ¨ ğŸ¤— Hub ä¸Šæˆ–æœ¬åœ°å­˜å‚¨çš„æ£€æŸ¥ç‚¹ã€‚å¯¼å‡ºçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒONNXæ ‡å‡†çš„è®¸å¤šåŠ é€Ÿå™¨ä¸­è¿è¡Œã€‚ä¾‹å¦‚ï¼ŒåŠ è½½å¹¶ä½¿ç”¨
    ONNX Runtime è¿è¡Œæ¨¡å‹å¦‚ä¸‹ï¼š
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The required output names (like `["last_hidden_state"]`) can be obtained by
    taking a look at the ONNX configuration of each model. For example, for DistilBERT
    we have:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€éœ€çš„è¾“å‡ºåç§°ï¼ˆå¦‚ `["last_hidden_state"]`ï¼‰å¯ä»¥é€šè¿‡æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„ ONNX é…ç½®æ¥è·å¾—ã€‚ä¾‹å¦‚ï¼Œå¯¹äº DistilBERTï¼Œæˆ‘ä»¬æœ‰ï¼š
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The process is identical for TensorFlow checkpoints on the Hub. For example,
    export a pure TensorFlow checkpoint like so:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹å¯¹äº Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œåƒè¿™æ ·å¯¼å‡ºä¸€ä¸ªçº¯ TensorFlow æ£€æŸ¥ç‚¹ï¼š
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To export a model thatâ€™s stored locally, save the modelâ€™s weights and tokenizer
    files in the same directory (e.g. `local-pt-checkpoint`), then export it to ONNX
    by pointing the `--model` argument of the `transformers.onnx` package to the desired
    directory:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯¼å‡ºå­˜å‚¨åœ¨æœ¬åœ°çš„æ¨¡å‹ï¼Œè¯·å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ä¸­ï¼ˆä¾‹å¦‚ `local-pt-checkpoint`ï¼‰ï¼Œç„¶åé€šè¿‡å°† `transformers.onnx`
    åŒ…çš„ `--model` å‚æ•°æŒ‡å‘æ‰€éœ€ç›®å½•æ¥å°†å…¶å¯¼å‡ºåˆ° ONNXï¼š
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
