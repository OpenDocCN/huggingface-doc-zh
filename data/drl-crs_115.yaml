- en: (Automatic) Curriculum Learning for RL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ï¼ˆè‡ªåŠ¨ï¼‰å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning](https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning](https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'While most of the RL methods seen in this course work well in practice, there
    are some cases where using them alone fails. This can happen, for instance, when:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æœ¬è¯¾ç¨‹ä¸­ä»‹ç»çš„å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å®è·µä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æœ‰äº›æƒ…å†µä¸‹ä»…ä½¿ç”¨å®ƒä»¬ä¼šå¤±è´¥ã€‚ä¾‹å¦‚ï¼Œå½“ï¼š
- en: the task to learn is hard and requires an **incremental acquisition of skills**
    (for instance when one wants to make a bipedal agent learn to go through hard
    obstacles, it must first learn to stand, then walk, then maybe jumpâ€¦)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„ä»»åŠ¡å¾ˆè‰°å·¨ï¼Œéœ€è¦**é€æ­¥ä¹ å¾—æŠ€èƒ½**ï¼ˆä¾‹å¦‚ï¼Œå½“ä¸€ä¸ªäººæƒ³è®©ä¸€ä¸ªåŒè¶³æœºå™¨äººå­¦ä¼šç©¿è¿‡å›°éš¾éšœç¢ç‰©æ—¶ï¼Œå®ƒå¿…é¡»é¦–å…ˆå­¦ä¼šç«™ç«‹ï¼Œç„¶åè¡Œèµ°ï¼Œç„¶åå¯èƒ½è·³è·ƒâ€¦ï¼‰
- en: there are variations in the environment (that affect the difficulty) and one
    wants its agent to be **robust** to them
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¯å¢ƒä¸­å­˜åœ¨å˜åŒ–ï¼ˆå½±å“éš¾åº¦ï¼‰ï¼Œäººä»¬å¸Œæœ›ä»–ä»¬çš„ä»£ç†èƒ½å¤Ÿ**å¯¹å…¶å…·æœ‰é²æ£’æ€§**
- en: '![Bipedal](../Images/e675d3cbd1fb1e896c36e92b88b2a7dd.png) ![Movable creepers](../Images/515895e0a85621d86aab110f7a7f5398.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![åŒè¶³](../Images/e675d3cbd1fb1e896c36e92b88b2a7dd.png) ![å¯ç§»åŠ¨çš„çˆ¬è¡Œè€…](../Images/515895e0a85621d86aab110f7a7f5398.png)'
- en: '[TeachMyAgent](https://developmentalsystems.org/TeachMyAgent/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[TeachMyAgent](https://developmentalsystems.org/TeachMyAgent/)'
- en: In such cases, it seems needed to propose different tasks to our RL agent and
    organize them such that the agent progressively acquires skills. This approach
    is called **Curriculum Learning** and usually implies a hand-designed curriculum
    (or set of tasks organized in a specific order). In practice, one can, for instance,
    control the generation of the environment, the initial states, or use Self-Play
    and control the level of opponents proposed to the RL agent.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¼¼ä¹éœ€è¦å‘æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ ä»£ç†æå‡ºä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶ç»„ç»‡è¿™äº›ä»»åŠ¡ï¼Œä½¿ä»£ç†é€æ¸ä¹ å¾—æŠ€èƒ½ã€‚è¿™ç§æ–¹æ³•ç§°ä¸º**è¯¾ç¨‹å­¦ä¹ **ï¼Œé€šå¸¸æ„å‘³ç€æ‰‹åŠ¨è®¾è®¡çš„è¯¾ç¨‹ï¼ˆæˆ–æŒ‰ç‰¹å®šé¡ºåºç»„ç»‡çš„ä»»åŠ¡é›†ï¼‰ã€‚åœ¨å®è·µä¸­ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥æ§åˆ¶ç¯å¢ƒçš„ç”Ÿæˆï¼Œåˆå§‹çŠ¶æ€ï¼Œæˆ–è€…ä½¿ç”¨è‡ªæˆ‘å¯¹å¼ˆå¹¶æ§åˆ¶æä¾›ç»™å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å¯¹æ‰‹çš„æ°´å¹³ã€‚
- en: 'As designing such a curriculum is not always trivial, the field of **Automatic
    Curriculum Learning (ACL) proposes to design approaches that learn to create such
    an organization of tasks in order to maximize the RL agentâ€™s performances**. Portelas
    et al. proposed to define ACL as:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè®¾è®¡è¿™æ ·çš„è¯¾ç¨‹å¹¶ä¸æ€»æ˜¯ç®€å•çš„ï¼Œ**è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼ˆACLï¼‰é¢†åŸŸæå‡ºè®¾è®¡æ–¹æ³•ï¼Œå­¦ä¹ åˆ›å»ºè¿™æ ·çš„ä»»åŠ¡ç»„ç»‡ï¼Œä»¥æœ€å¤§åŒ–å¼ºåŒ–å­¦ä¹ ä»£ç†çš„è¡¨ç°**ã€‚Portelasç­‰äººæå‡ºå°†ACLå®šä¹‰ä¸ºï¼š
- en: â€¦ a family of mechanisms that automatically adapt the distribution of training
    data by learning to adjust the selection of learning situations to the capabilities
    of RL agents.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€¦ ä¸€ç³»åˆ—æœºåˆ¶ï¼Œé€šè¿‡å­¦ä¹ è°ƒæ•´å­¦ä¹ æƒ…å¢ƒçš„é€‰æ‹©ï¼Œè‡ªåŠ¨è°ƒæ•´è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œä»¥é€‚åº”å¼ºåŒ–å­¦ä¹ ä»£ç†çš„èƒ½åŠ›ã€‚
- en: As an example, OpenAI used **Domain Randomization** (they applied random variations
    on the environment) to make a robot hand solve Rubikâ€™s Cubes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¾‹æ¥è¯´ï¼ŒOpenAIä½¿ç”¨**é¢†åŸŸéšæœºåŒ–**ï¼ˆä»–ä»¬åœ¨ç¯å¢ƒä¸­åº”ç”¨éšæœºå˜åŒ–ï¼‰è®©ä¸€ä¸ªæœºå™¨äººæ‰‹è§£å†³é­”æ–¹ã€‚
- en: '![Dr](../Images/375a72dcd7269a70641cfd815a1e9467.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![åšå£«](../Images/375a72dcd7269a70641cfd815a1e9467.png)'
- en: '[OpenAI - Solving Rubikâ€™s Cube with a Robot Hand](https://openai.com/blog/solving-rubiks-cube/)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI - ç”¨æœºå™¨äººæ‰‹è§£å†³é­”æ–¹](https://openai.com/blog/solving-rubiks-cube/)'
- en: Finally, you can play with the robustness of agents trained in the [TeachMyAgent](https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo)
    benchmark by controlling environment variations or even drawing the terrain ğŸ‘‡
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å¯ä»¥é€šè¿‡æ§åˆ¶ç¯å¢ƒå˜åŒ–æˆ–ç”šè‡³ç»˜åˆ¶åœ°å½¢ğŸ‘‡æ¥ç©å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨[TeachMyAgent](https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo)åŸºå‡†æµ‹è¯•ä¸­è®­ç»ƒçš„é²æ£’æ€§
- en: '![Demo](../Images/5c2270050486e329736af710094b7c28.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![æ¼”ç¤º](../Images/5c2270050486e329736af710094b7c28.png)'
- en: '[https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo](https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo](https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo)'
- en: Further reading
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: 'For more information, we recommend that you check out the following resources:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹èµ„æºï¼š
- en: Overview of the field
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢†åŸŸæ¦‚è¿°
- en: '[Automatic Curriculum Learning For Deep RL: A Short Survey](https://arxiv.org/pdf/2003.04664.pdf)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼šç®€çŸ­è°ƒæŸ¥](https://arxiv.org/pdf/2003.04664.pdf)'
- en: '[Curriculum for Reinforcement Learning](https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹](https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/)'
- en: Recent methods
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€æ–°æ–¹æ³•
- en: '[Evolving Curricula with Regret-Based Environment Design](https://arxiv.org/abs/2203.01302)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é€šè¿‡åŸºäºé—æ†¾çš„ç¯å¢ƒè®¾è®¡æ¼”å˜è¯¾ç¨‹](https://arxiv.org/abs/2203.01302)'
- en: '[Curriculum Reinforcement Learning via Constrained Optimal Transport](https://proceedings.mlr.press/v162/klink22a.html)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é€šè¿‡å—é™æœ€ä¼˜ä¼ è¾“è¿›è¡Œè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ](https://proceedings.mlr.press/v162/klink22a.html)'
- en: '[Prioritized Level Replay](https://arxiv.org/abs/2010.03934)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä¼˜å…ˆçº§æ°´å¹³é‡æ’­](https://arxiv.org/abs/2010.03934)'
- en: Author
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½œè€…
- en: This section was written by [ClÃ©ment Romac](https://twitter.com/ClementRomac)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ç”±[ClÃ©ment Romac](https://twitter.com/ClementRomac)æ’°å†™
