- en: Data2Vec
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Data2Vec
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/data2vec)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The Data2Vec model was proposed in [data2vec: A General Framework for Self-supervised
    Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by
    Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.
    Data2Vec proposes a unified framework for self-supervised learning across different
    data modalities - text, audio and images. Importantly, predicted targets for pre-training
    are contextualized latent representations of the inputs, rather than modality-specific,
    context-independent targets.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2Vecæ¨¡å‹æ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Guå’ŒMichael
    Auliåœ¨[æ•°æ®2vec: è¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€ä¸­çš„è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚Data2Vecæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè·¨ä¸åŒæ•°æ®æ¨¡æ€çš„è‡ªç›‘ç£å­¦ä¹ 
    - æ–‡æœ¬ã€éŸ³é¢‘å’Œå›¾åƒã€‚é‡è¦çš„æ˜¯ï¼Œé¢„è®­ç»ƒçš„é¢„æµ‹ç›®æ ‡æ˜¯è¾“å…¥çš„ä¸Šä¸‹æ–‡åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç‰¹å®šäºæ¨¡æ€çš„ã€ä¸Šä¸‹æ–‡æ— å…³çš„ç›®æ ‡ã€‚'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*While the general idea of self-supervised learning is identical across modalities,
    the actual algorithms and objectives differ widely because they were developed
    with a single modality in mind. To get us closer to general self-supervised learning,
    we present data2vec, a framework that uses the same learning method for either
    speech, NLP or computer vision. The core idea is to predict latent representations
    of the full input data based on a masked view of the input in a selfdistillation
    setup using a standard Transformer architecture. Instead of predicting modality-specific
    targets such as words, visual tokens or units of human speech which are local
    in nature, data2vec predicts contextualized latent representations that contain
    information from the entire input. Experiments on the major benchmarks of speech
    recognition, image classification, and natural language understanding demonstrate
    a new state of the art or competitive performance to predominant approaches. Models
    and code are available at [www.github.com/pytorch/fairseq/tree/master/examples/data2vec](http://www.github.com/pytorch/fairseq/tree/master/examples/data2vec).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*å°½ç®¡è‡ªç›‘ç£å­¦ä¹ çš„ä¸€èˆ¬æ€æƒ³åœ¨å„ç§æ¨¡æ€ä¹‹é—´æ˜¯ç›¸åŒçš„ï¼Œä½†å®é™…çš„ç®—æ³•å’Œç›®æ ‡å·®å¼‚å¾ˆå¤§ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é’ˆå¯¹å•ä¸€æ¨¡æ€å¼€å‘çš„ã€‚ä¸ºäº†è®©æˆ‘ä»¬æ›´æ¥è¿‘äºä¸€èˆ¬çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºäº†data2vecï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒä½¿ç”¨ç›¸åŒçš„å­¦ä¹ æ–¹æ³•æ¥å¤„ç†è¯­éŸ³ã€NLPæˆ–è®¡ç®—æœºè§†è§‰ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäºè¾“å…¥æ•°æ®çš„é®è”½è§†å›¾æ¥é¢„æµ‹å®Œæ•´è¾“å…¥æ•°æ®çš„æ½œåœ¨è¡¨ç¤ºï¼Œä½¿ç”¨æ ‡å‡†çš„Transformeræ¶æ„è¿›è¡Œè‡ªè’¸é¦è®¾ç½®ã€‚data2vecä¸æ˜¯é¢„æµ‹ç‰¹å®šäºæ¨¡æ€çš„ç›®æ ‡ï¼Œæ¯”å¦‚å•è¯ã€è§†è§‰æ ‡è®°æˆ–äººç±»è¯­éŸ³å•å…ƒï¼Œè€Œæ˜¯é¢„æµ‹åŒ…å«æ¥è‡ªæ•´ä¸ªè¾“å…¥çš„ä¿¡æ¯çš„ä¸Šä¸‹æ–‡åŒ–æ½œåœ¨è¡¨ç¤ºã€‚å¯¹è¯­éŸ³è¯†åˆ«ã€å›¾åƒåˆ†ç±»å’Œè‡ªç„¶è¯­è¨€ç†è§£çš„ä¸»è¦åŸºå‡†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¸»æµæ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆ–å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ¨¡å‹å’Œä»£ç å¯åœ¨[www.github.com/pytorch/fairseq/tree/master/examples/data2vec](http://www.github.com/pytorch/fairseq/tree/master/examples/data2vec)ä¸Šæ‰¾åˆ°ã€‚*'
- en: This model was contributed by [edugp](https://huggingface.co/edugp) and [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    [sayakpaul](https://github.com/sayakpaul) and [Rocketknight1](https://github.com/Rocketknight1)
    contributed Data2Vec for vision in TensorFlow.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[edugp](https://huggingface.co/edugp)å’Œ[patrickvonplaten](https://huggingface.co/patrickvonplaten)è´¡çŒ®çš„ã€‚[sayakpaul](https://github.com/sayakpaul)å’Œ[Rocketknight1](https://github.com/Rocketknight1)ä¸ºTensorFlowä¸­çš„è§†è§‰è´¡çŒ®äº†Data2Vecã€‚
- en: The original code (for NLP and Speech) can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/data2vec).
    The original code for vision can be found [here](https://github.com/facebookresearch/data2vec_vision/tree/main/beit).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ä»£ç ï¼ˆç”¨äºNLPå’Œè¯­éŸ³ï¼‰å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/pytorch/fairseq/tree/main/examples/data2vec)æ‰¾åˆ°ã€‚è§†è§‰çš„åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/data2vec_vision/tree/main/beit)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: Data2VecAudio, Data2VecText, and Data2VecVision have all been trained using
    the same self-supervised learning method.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Data2VecAudioã€Data2VecTextå’ŒData2VecVisionéƒ½æ˜¯ä½¿ç”¨ç›¸åŒçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒçš„ã€‚
- en: For Data2VecAudio, preprocessing is identical to [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model),
    including feature extraction
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºData2VecAudioï¼Œé¢„å¤„ç†ä¸[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)ç›¸åŒï¼ŒåŒ…æ‹¬ç‰¹å¾æå–ã€‚
- en: For Data2VecText, preprocessing is identical to [RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel),
    including tokenization.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºData2VecTextï¼Œé¢„å¤„ç†ä¸[RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel)ç›¸åŒï¼ŒåŒ…æ‹¬æ ‡è®°åŒ–ã€‚
- en: For Data2VecVision, preprocessing is identical to [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel),
    including feature extraction.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºData2VecVisionï¼Œé¢„å¤„ç†ä¸[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)ç›¸åŒï¼ŒåŒ…æ‹¬ç‰¹å¾æå–ã€‚
- en: Resources
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with Data2Vec.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨Data2Vecã€‚
- en: Image Classification
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: '[Data2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Data2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification)ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚'
- en: To fine-tune [TFData2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification)
    on a custom dataset, see [this notebook](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒ[TFData2VecVisionForImageClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification)ï¼Œè¯·å‚é˜…[æ­¤ç¬”è®°æœ¬](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb)ã€‚
- en: '**Data2VecText documentation resources**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Data2VecTextæ–‡æ¡£èµ„æº**'
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ ‡è®°åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)'
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/language_modeling)'
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/masked_language_modeling)'
- en: '[Multiple choice task guide](../tasks/multiple_choice)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¤šé¡¹é€‰æ‹©ä»»åŠ¡æŒ‡å—](../tasks/multiple_choice)'
- en: '**Data2VecAudio documentation resources**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Data2VecAudioæ–‡æ¡£èµ„æº**'
- en: '[Audio classification task guide](../tasks/audio_classification)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[éŸ³é¢‘åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/audio_classification)'
- en: '[Automatic speech recognition task guide](../tasks/asr)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡æŒ‡å—](../tasks/asr)'
- en: '**Data2VecVision documentation resources**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**Data2VecVisionæ–‡æ¡£èµ„æº**'
- en: '[Image classification](../tasks/image_classification)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åƒåˆ†ç±»](../tasks/image_classification)'
- en: '[Semantic segmentation](../tasks/semantic_segmentation)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¯­ä¹‰åˆ†å‰²](../tasks/semantic_segmentation)'
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: Data2VecTextConfig
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecTextConfig
- en: '### `class transformers.Data2VecTextConfig`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecTextConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_text.py#L31)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_text.py#L31)'
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) â€” Vocabulary size of the
    DATA2VEC model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling `Data2VecModel`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º30522) â€” DATA2VECæ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨`Data2VecModel`æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º12) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º12) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º3072) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`æˆ–`Callable`, *å¯é€‰*, é»˜è®¤ä¸º`"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) â€” The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚512ã€1024æˆ–2048ï¼‰ã€‚'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) â€” The vocabulary size
    of the `token_type_ids` passed when calling `Data2VecModel`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º2) â€” åœ¨è°ƒç”¨`Data2VecModel`æ—¶ä¼ é€’çš„`token_type_ids`çš„è¯æ±‡é‡ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) â€” Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"absolute"`) â€” ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚é€‰æ‹©`"absolute"`ä¹‹ä¸€ã€‚æœ‰å…³`"relative_key"`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[ä½¿ç”¨ç›¸å¯¹ä½ç½®è¡¨ç¤ºçš„è‡ªæ³¨æ„åŠ›ï¼ˆShawç­‰äººï¼‰](https://arxiv.org/abs/1803.02155)ã€‚æœ‰å…³`"relative_key_query"`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[ä½¿ç”¨æ›´å¥½çš„ç›¸å¯¹ä½ç½®åµŒå…¥æ”¹è¿›Transformeræ¨¡å‹ï¼ˆHuangç­‰äººï¼‰](https://arxiv.org/abs/2009.13658)ä¸­çš„*æ–¹æ³•4*ã€‚'
- en: '`is_decoder` (`bool`, *optional*, defaults to `False`) â€” Whether the model
    is used as a decoder or not. If `False`, the model is used as an encoder.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_decoder` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ¨¡å‹æ˜¯å¦ç”¨ä½œè§£ç å™¨ã€‚å¦‚æœä¸º`False`ï¼Œåˆ™æ¨¡å‹ç”¨ä½œç¼–ç å™¨ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨`config.is_decoder=True`æ—¶ç›¸å…³ã€‚'
- en: '`classifier_dropout` (`float`, *optional*) â€” The dropout ratio for the classification
    head.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout` (`float`, *optional*) â€” åˆ†ç±»å¤´çš„ dropout æ¯”ç‡ã€‚'
- en: This is the configuration class to store the configuration of a [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel)
    and [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel).
    It is used to instantiate a Data2VecText model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Data2VecText [facebook/data2vec-text-base](https://huggingface.co/facebook/data2vec-text-base)
    architecture.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨ [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel)
    å’Œ [Data2VecTextModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecTextModel)
    é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª Data2VecText æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Data2VecText [facebook/data2vec-text-base](https://huggingface.co/facebook/data2vec-text-base)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Examples:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data2VecAudioConfig
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioConfig
- en: '### `class transformers.Data2VecAudioConfig`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_audio.py#L31)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_audio.py#L31)'
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 32) â€” Vocabulary size of the Data2VecAudio
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    or `TFData2VecAudioModel`. Vocabulary size of the model. Defines the different
    tokens that can be represented by the *inputs_ids* passed to the forward method
    of [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 32) â€” Data2VecAudio æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨
    [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    æˆ– `TFData2VecAudioModel` æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†ä¼ é€’ç»™ [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    çš„ *inputs_ids* å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ
    `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„
    dropout æ¦‚ç‡ã€‚'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) â€” å…¨è¿æ¥å±‚å†…æ¿€æ´»çš„ dropout
    æ¯”ç‡ã€‚'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout ratio
    for the attention probabilities.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout
    æ¯”ç‡ã€‚'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for the final projection layer of [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_dropout` (`float`, *optional*, defaults to 0.1) â€” [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)
    çš„æœ€ç»ˆæŠ•å½±å±‚çš„ dropout æ¦‚ç‡ã€‚'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) â€” The LayerDrop probability.
    See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.1) â€” LayerDrop æ¦‚ç‡ã€‚æ›´å¤šç»†èŠ‚è¯·å‚é˜… [LayerDrop
    è®ºæ–‡](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    for output of the feature encoder.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) â€” ç‰¹å¾ç¼–ç å™¨è¾“å‡ºçš„ dropout
    æ¦‚ç‡ã€‚'
- en: '`feat_extract_activation` (`str,` optional`, defaults to` â€œgeluâ€`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` â€œgeluâ€`,` â€œreluâ€`,` â€œseluâ€`and`â€œgelu_newâ€` are
    supported.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_activation` (`str,` optional`, defaults to` â€œgeluâ€`) -- ç‰¹å¾æå–å™¨ä¸­1Då·ç§¯å±‚çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`
    â€œgeluâ€`ã€` â€œreluâ€`ã€` â€œseluâ€`å’Œ`â€œgelu_newâ€`ã€‚'
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) â€” A tuple of integers defining the number of input
    and output channels of each 1D convolutional layer in the feature encoder. The
    length of *conv_dim* defines the number of 1D convolutional layers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) â€” å®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°çš„æ•´æ•°å…ƒç»„ã€‚*conv_dim*çš„é•¿åº¦å®šä¹‰äº†1Då·ç§¯å±‚çš„æ•°é‡ã€‚'
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) â€” A tuple of integers defining the stride of each 1D convolutional
    layer in the feature encoder. The length of *conv_stride* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) â€” å®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ­¥å¹…çš„æ•´æ•°å…ƒç»„ã€‚*conv_stride*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚'
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) â€” A tuple of integers defining the kernel size of each 1D convolutional
    layer in the feature encoder. The length of *conv_kernel* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) â€” å®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„å†…æ ¸å¤§å°çš„æ•´æ•°å…ƒç»„ã€‚*conv_kernel*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚'
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) â€” Whether the 1D convolutional
    layers have a bias.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_bias` (`bool`, *optional*, defaults to `False`) â€” 1Då·ç§¯å±‚æ˜¯å¦æœ‰åç½®ã€‚'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) â€” Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) â€” å·ç§¯ä½ç½®åµŒå…¥çš„æ•°é‡ã€‚å®šä¹‰äº†1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„å†…æ ¸å¤§å°ã€‚'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) â€” Number
    of groups of 1D convolutional positional embeddings layer.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) â€” 1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„ç»„æ•°ã€‚'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates â€mask_time_prob*len(time_axis)/mask_time_lengthâ€ independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” æ²¿æ—¶é—´è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_time_prob*len(time_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*
    mask_time_prob *åº”è¯¥æ˜¯`prob_vector_start*mask_time_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) â€” Length of vector span
    along the time axis.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *optional*, defaults to 10) â€” æ²¿æ—¶é—´è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if â€mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masksâ€'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” æ²¿æ—¶é—´è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masksâ€æ—¶ç›¸å…³'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates â€mask_feature_prob*len(feature_axis)/mask_time_lengthâ€
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” æ²¿ç‰¹å¾è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_feature_prob*len(feature_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*
    mask_feature_prob *åº”è¯¥æ˜¯`prob_vector_start*mask_feature_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½æ©ç›–å‘é‡çš„å®é™…ç™¾åˆ†æ¯”ã€‚ä»…åœ¨`apply_spec_augmentä¸ºTrue`æ—¶ç›¸å…³ã€‚'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) â€” Length of vector
    span along the feature axis.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *optional*, defaults to 10) â€” æ²¿ç‰¹å¾è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” The minimum
    number of masks of length `mask_feature_length` generated along the feature axis,
    each time step, irrespectively of `mask_feature_prob`. Only relevant if â€mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masksâ€'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” æ²¿ç‰¹å¾è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masksâ€æ—¶ç›¸å…³'
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) â€” Specifies the
    reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training
    an instance of [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) â€” æŒ‡å®šåº”ç”¨äº`torch.nn.CTCLoss`è¾“å‡ºçš„å‡å°‘æ–¹å¼ã€‚ä»…åœ¨è®­ç»ƒ[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) â€” Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å°†`torch.nn.CTCLoss`çš„æ— é™æŸå¤±å’Œç›¸å…³æ¢¯åº¦ç½®é›¶ã€‚å½“è¾“å…¥å¤ªçŸ­æ— æ³•ä¸ç›®æ ‡å¯¹é½æ—¶ï¼Œä¸»è¦ä¼šå‡ºç°æ— é™æŸå¤±ã€‚ä»…åœ¨è®­ç»ƒ[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) â€” Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [Data2VecAudioForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨å¸¦æœ‰å­¦ä¹ æƒé‡çš„å±‚è¾“å‡ºçš„åŠ æƒå¹³å‡ã€‚ä»…åœ¨ä½¿ç”¨[Data2VecAudioForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification)å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) â€” Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *optional*, defaults to 256) â€” åˆ†ç±»å‰çš„æŠ•å½±ç»´åº¦ï¼Œç”¨äºæ ‡è®°å‡å€¼æ± åŒ–ã€‚'
- en: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) â€” A tuple of integers defining the number of output channels
    of each 1D convolutional layer in the *TDNN* module of the *XVector* model. The
    length of *tdnn_dim* defines the number of *TDNN* layers.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰*XVector*æ¨¡å‹ä¸­*TDNN*æ¨¡å—ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚*tdnn_dim*çš„é•¿åº¦å®šä¹‰äº†*TDNN*å±‚æ•°ã€‚'
- en: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) â€” A tuple of integers defining the kernel size of each 1D convolutional
    layer in the *TDNN* module of the *XVector* model. The length of *tdnn_kernel*
    has to match the length of *tdnn_dim*.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰*XVector*æ¨¡å‹ä¸­*TDNN*æ¨¡å—ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ ¸å¤§å°ã€‚*tdnn_kernel*çš„é•¿åº¦å¿…é¡»ä¸*tdnn_dim*çš„é•¿åº¦ç›¸åŒ¹é…ã€‚'
- en: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) â€” A tuple of integers defining the dilation factor of each 1D convolutional
    layer in *TDNN* module of the *XVector* model. The length of *tdnn_dilation* has
    to match the length of *tdnn_dim*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰*XVector*æ¨¡å‹ä¸­*TDNN*æ¨¡å—ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ‰©å¼ å› å­ã€‚*tdnn_dilation*çš„é•¿åº¦å¿…é¡»ä¸*tdnn_dim*çš„é•¿åº¦ç›¸åŒ¹é…ã€‚'
- en: '`xvector_output_dim` (`int`, *optional*, defaults to 512) â€” Dimensionality
    of the *XVector* embedding vectors.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xvector_output_dim` (`int`, *optional*, defaults to 512) â€” *XVector*åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚'
- en: '`add_adapter` (`bool`, *optional*, defaults to `False`) â€” Whether a convolutional
    network should be stacked on top of the Data2VecAudio Encoder. Can be very useful
    for warm-starting Data2VecAudio for SpeechEncoderDecoder models.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_adapter` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨Data2VecAudioç¼–ç å™¨é¡¶éƒ¨å †å å·ç§¯ç½‘ç»œã€‚å¯¹äºå¯åŠ¨Data2VecAudioç”¨äºSpeechEncoderDecoderæ¨¡å‹éå¸¸æœ‰ç”¨ã€‚'
- en: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) â€” Kernel size of the
    convolutional layers in the adapter network. Only relevant if `add_adapter is
    True`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„æ ¸å¤§å°ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚'
- en: '`adapter_stride` (`int`, *optional*, defaults to 2) â€” Stride of the convolutional
    layers in the adapter network. Only relevant if `add_adapter is True`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_stride` (`int`, *optional*, defaults to 2) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„æ­¥å¹…ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚'
- en: '`num_adapter_layers` (`int`, *optional*, defaults to 3) â€” Number of convolutional
    layers that should be used in the adapter network. Only relevant if `add_adapter
    is True`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_adapter_layers` (`int`, *optional*, defaults to 3) â€” é€‚é…å™¨ç½‘ç»œä¸­åº”ä½¿ç”¨çš„å·ç§¯å±‚æ•°é‡ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚'
- en: '`output_hidden_size` (`int`, *optional*) â€” Dimensionality of the encoder output
    layer. If not defined, this defaults to *hidden-size*. Only relevant if `add_adapter
    is True`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_size` (`int`, *optional*) â€” ç¼–ç å™¨è¾“å‡ºå±‚çš„ç»´åº¦ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™é»˜è®¤ä¸º*hidden-size*ã€‚ä»…åœ¨`add_adapter`ä¸ºTrueæ—¶ç›¸å…³ã€‚'
- en: This is the configuration class to store the configuration of a [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel).
    It is used to instantiate an Data2VecAudio model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Data2VecAudio [facebook/data2vec-audio-base-960h](https://huggingface.co/facebook/data2vec-audio-base-960h)
    architecture.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªData2VecAudioæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºData2VecAudio
    [facebook/data2vec-audio-base-960h](https://huggingface.co/facebook/data2vec-audio-base-960h)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Data2VecVisionConfig
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecVisionConfig
- en: '### `class transformers.Data2VecVisionConfig`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecVisionConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_vision.py#L35)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/configuration_data2vec_vision.py#L35)'
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ
    `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚'
- en: '`image_size` (`int`, *optional*, defaults to 224) â€” The size (resolution) of
    each image.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 224) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`patch_size` (`int`, *optional*, defaults to 16) â€” The size (resolution) of
    each patch.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 16) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“çš„æ•°é‡ã€‚'
- en: '`use_mask_token` (`bool`, *optional*, defaults to `False`) â€” Whether to use
    a mask token for masked image modeling.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mask_token` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æ©è”½å›¾åƒå»ºæ¨¡ä¸­ä½¿ç”¨æ©è”½æ ‡è®°ã€‚'
- en: '`use_absolute_position_embeddings` (`bool`, *optional*, defaults to `False`)
    â€” Whether to use BERT-style absolute position embeddings.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_absolute_position_embeddings` (`bool`, *optional*, defaults to `False`)
    â€” æ˜¯å¦ä½¿ç”¨ç±»ä¼¼ BERT çš„ç»å¯¹ä½ç½®åµŒå…¥ã€‚'
- en: '`use_relative_position_bias` (`bool`, *optional*, defaults to `False`) â€” Whether
    to use T5-style relative position embeddings in the self-attention layers.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_relative_position_bias` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨
    T5 é£æ ¼çš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚'
- en: '`use_shared_relative_position_bias` (`bool`, *optional*, defaults to `False`)
    â€” Whether to use the same relative position embeddings across all self-attention
    layers of the Transformer.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_shared_relative_position_bias` (`bool`, *optional*, defaults to `False`)
    â€” æ˜¯å¦åœ¨ Transformer çš„æ‰€æœ‰è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨ç›¸åŒçš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚'
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.1) â€” Scale to
    use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable
    layer scale.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.1) â€” è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨çš„æ¯”ä¾‹ã€‚åŸºç¡€ä¸º
    0.1ï¼Œå¤§å‹ä¸º 1e-5ã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨å±‚æ¯”ä¾‹ã€‚'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) â€” Stochastic depth
    rate per sample (when applied in the main path of residual layers).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) â€” æ¯ä¸ªæ ·æœ¬çš„éšæœºæ·±åº¦ç‡ï¼ˆå½“åº”ç”¨äºæ®‹å·®å±‚çš„ä¸»è·¯å¾„æ—¶ï¼‰ã€‚'
- en: '`use_mean_pooling` (`bool`, *optional*, defaults to `True`) â€” Whether to mean
    pool the final hidden states of the patches instead of using the final hidden
    state of the CLS token, before applying the classification head.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mean_pooling` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦å¯¹è¡¥ä¸çš„æœ€ç»ˆéšè—çŠ¶æ€è¿›è¡Œå¹³å‡æ± åŒ–ï¼Œè€Œä¸æ˜¯ä½¿ç”¨
    CLS æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ååº”ç”¨åˆ†ç±»å¤´ã€‚'
- en: '`out_indices` (`List[int]`, *optional*, defaults to `[3, 5, 7, 11]`) â€” Indices
    of the feature maps to use for semantic segmentation.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *optional*, defaults to `[3, 5, 7, 11]`) â€” ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç‰¹å¾å›¾çš„ç´¢å¼•ã€‚'
- en: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) â€” Pooling
    scales used in Pooling Pyramid Module applied on the last feature map.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) â€” åº”ç”¨äºæœ€åç‰¹å¾å›¾çš„æ± åŒ–é‡‘å­—å¡”æ¨¡å—ä¸­ä½¿ç”¨çš„æ± åŒ–æ¯”ä¾‹ã€‚'
- en: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” Whether to
    use an auxiliary head during training.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨è¾…åŠ©å¤´ã€‚'
- en: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” Weight of
    the cross-entropy loss of the auxiliary head.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” è¾…åŠ©å¤´çš„äº¤å‰ç†µæŸå¤±çš„æƒé‡ã€‚'
- en: '`auxiliary_channels` (`int`, *optional*, defaults to 256) â€” Number of channels
    to use in the auxiliary head.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_channels` (`int`, *optional*, defaults to 256) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„é€šé“æ•°ã€‚'
- en: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) â€” Number of convolutional
    layers to use in the auxiliary head.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„å·ç§¯å±‚æ•°é‡ã€‚'
- en: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) â€” Whether
    to concatenate the output of the auxiliary head with the input before the classification
    layer.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨åˆ†ç±»å±‚ä¹‹å‰å°†è¾…åŠ©å¤´çš„è¾“å‡ºä¸è¾“å…¥è¿æ¥èµ·æ¥ã€‚'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚'
- en: This is the configuration class to store the configuration of a [Data2VecVisionModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionModel).
    It is used to instantiate an Data2VecVision model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Data2VecVision [facebook/data2vec-vision-base](https://huggingface.co/facebook/data2vec-vision-base)
    architecture.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å­˜å‚¨[Data2VecVisionModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecVisionModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªData2VecVisionæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºData2VecVision
    [facebook/data2vec-vision-base](https://huggingface.co/facebook/data2vec-vision-base)æ¶æ„çš„é…ç½®ã€‚
- en: 'Example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: PytorchHide Pytorch content
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: Data2VecAudioModel
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioModel
- en: '### `class transformers.Data2VecAudioModel`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L812)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L812)'
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰-
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: 'The bare Data2VecAudio Model transformer outputting raw hidden-states without
    any specific head on top. Data2VecAudio was proposed in [data2vec: A General Framework
    for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555)
    by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael
    Auli.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸Data2VecAudioæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚Data2VecAudioæ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong
    Xuã€Arun Babuã€Jiatao Guå’ŒMichael Auliåœ¨[æ•°æ®2vecï¼šè¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L887)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L887)'
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type *List[float]* or a *numpy.ndarray*,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into *input_values*, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†*.flac*æˆ–*.wav*éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°*List[float]*æˆ–*numpy.ndarray*ç±»å‹çš„æ•°ç»„ä¸­è·å¾—å€¼ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ*pip
    install soundfile*ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ*input_values*ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸ºç±»å‹ä¸º*torch.FloatTensor*çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºé¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤º`æœªè¢«æ©ç `çš„ä»¤ç‰Œï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤º`è¢«æ©ç `çš„ä»¤ç‰Œã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should be passed if the corresponding processor has `config.return_attention_mask
    == True`, which is the case for all pre-trained Data2Vec Audio models. Be aware
    that that even with `attention_mask`, zero-padded inputs will have slightly different
    outputs compared to non-padded inputs because there are more than one convolutional
    layer in the positional encodings. For a more detailed explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`ï¼Œåˆ™åº”ä¼ é€’`attention_mask`ï¼Œè¿™é€‚ç”¨äºæ‰€æœ‰é¢„è®­ç»ƒçš„Data2VecéŸ³é¢‘æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿ä½¿ç”¨`attention_mask`ï¼Œé›¶å¡«å……çš„è¾“å…¥ä¸éå¡«å……çš„è¾“å…¥å°†å…·æœ‰ç•¥æœ‰ä¸åŒçš„è¾“å‡ºï¼Œå› ä¸ºåœ¨ä½ç½®ç¼–ç ä¸­æœ‰å¤šä¸ªå·ç§¯å±‚ã€‚æœ‰å…³æ›´è¯¦ç»†çš„è§£é‡Šï¼Œè¯·å‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    and inputs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) â€” Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚æå–çš„ç‰¹å¾å‘é‡åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” `torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)
    forward method, overrides the `__call__` special method.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[Data2VecAudioModel](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioModel)çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE8]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Data2VecAudioForAudioFrameClassification
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioForAudioFrameClassification
- en: '### `class transformers.Data2VecAudioForAudioFrameClassification`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioForAudioFrameClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1196)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1196)'
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Data2VecAudio Model with a frame classification head on top for tasks like Speaker
    Diarization.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Data2VecAudioæ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ç”¨äºè¯´è¯äººåˆ†ç¦»ç­‰ä»»åŠ¡çš„å¸§åˆ†ç±»å¤´ã€‚
- en: 'Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised
    Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by
    Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2VecAudioæ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Guå’ŒMichael
    Auliåœ¨[æ•°æ®2vec: è¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1247)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1247)'
- en: '[PRE10]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type *List[float]* or a *numpy.ndarray*,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into *input_values*, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°† *.flac* æˆ– *.wav* éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° *List[float]* ç±»å‹çš„æ•°ç»„æˆ– *numpy.ndarray*
    ä¸­è·å¾—è¿™äº›å€¼ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (*pip install soundfile*)ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ *input_values*ï¼Œåº”è¯¥ä½¿ç”¨
    [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º *torch.FloatTensor* ç±»å‹çš„å¼ é‡ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§ [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` èŒƒå›´å†…ã€‚'
- en: 1 for tokens that are `not masked`,
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢« `masked` çš„æ ‡è®°ä¸º 1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢« `masked` çš„æ ‡è®°ä¸º 0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should be passed if the corresponding processor has `config.return_attention_mask
    == True`, which is the case for all pre-trained Data2Vec Audio models. Be aware
    that that even with `attention_mask`, zero-padded inputs will have slightly different
    outputs compared to non-padded inputs because there are more than one convolutional
    layer in the positional encodings. For a more detailed explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == True`ï¼Œåˆ™åº”ä¼ é€’ `attention_mask`ï¼Œè¿™é€‚ç”¨äºæ‰€æœ‰é¢„è®­ç»ƒçš„
    Data2Vec éŸ³é¢‘æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿ä½¿ç”¨äº† `attention_mask`ï¼Œé›¶å¡«å……çš„è¾“å…¥ä¸éå¡«å……çš„è¾“å…¥ä¼šæœ‰ç¨å¾®ä¸åŒçš„è¾“å‡ºï¼Œå› ä¸ºåœ¨ä½ç½®ç¼–ç ä¸­æœ‰å¤šä¸ªå·ç§¯å±‚ã€‚æœ‰å…³æ›´è¯¦ç»†çš„è§£é‡Šï¼Œè¯·å‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨
    `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤± (å‡æ–¹æŸå¤±)ï¼Œå¦‚æœ
    `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤± (äº¤å‰ç†µ)ã€‚'
- en: Returns
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    and inputs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ (å¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶) åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½® ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification loss.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾› `labels` æ—¶è¿”å›)
    â€” åˆ†ç±»æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    â€” Classification scores (before SoftMax).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    â€” åˆ†ç±»åˆ†æ•° (SoftMax ä¹‹å‰)ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True`
    æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_attentions=True`
    æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length,
    sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Data2VecAudioForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[Data2VecAudioForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE11]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Data2VecAudioForCTC
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioForCTC
- en: '### `class transformers.Data2VecAudioForCTC`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L948)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L948)'
- en: '[PRE12]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: 'Data2VecAudio Model with a `language modeling` head on top for Connectionist
    Temporal Classification (CTC). Data2VecAudio was proposed in [data2vec: A General
    Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555)
    by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael
    Auli.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2VecAudio æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰ `è¯­è¨€å»ºæ¨¡` å¤´éƒ¨ï¼Œç”¨äº Connectionist Temporal Classification (CTC)ã€‚Data2VecAudio
    æ˜¯ç”± Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Gu å’Œ Michael Auli
    åœ¨ [data2vec: A General Framework for Self-supervised Learning in Speech, Vision
    and Language](https://arxiv.org/pdf/2202.03555) ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L993)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L993)'
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type *List[float]* or a *numpy.ndarray*,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into *input_values*, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type *torch.FloatTensor*.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°† *.flac* æˆ– *.wav* éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° *List[float]* æˆ– *numpy.ndarray*
    ç±»å‹çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (*pip install soundfile*)ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ *input_values*ï¼Œåº”ä½¿ç”¨
    [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º *torch.FloatTensor* ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 ç”¨äº `æœªè¢«æ©ç ` çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 ç”¨äº `è¢«æ©ç ` çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should be passed if the corresponding processor has `config.return_attention_mask
    == True`, which is the case for all pre-trained Data2Vec Audio models. Be aware
    that that even with `attention_mask`, zero-padded inputs will have slightly different
    outputs compared to non-padded inputs because there are more than one convolutional
    layer in the positional encodings. For a more detailed explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`ï¼Œåˆ™åº”ä¼ é€’`attention_mask`ï¼Œè¿™å¯¹äºæ‰€æœ‰é¢„è®­ç»ƒçš„Data2Vec
    Audioæ¨¡å‹éƒ½æ˜¯å¦‚æ­¤ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿ä½¿ç”¨`attention_mask`ï¼Œé›¶å¡«å……çš„è¾“å…¥ä¸éå¡«å……çš„è¾“å…¥å°†å…·æœ‰ç¨æœ‰ä¸åŒçš„è¾“å‡ºï¼Œå› ä¸ºåœ¨ä½ç½®ç¼–ç ä¸­æœ‰å¤šä¸ªå·ç§¯å±‚ã€‚æœ‰å…³æ›´è¯¦ç»†çš„è§£é‡Šï¼Œè¯·å‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349)ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    â€” Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_length)`ï¼Œ*optional*) â€”
    è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œ`target_length`å¿…é¡»å°äºæˆ–ç­‰äºè¾“å‡ºlogitsçš„åºåˆ—é•¿åº¦ã€‚ç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size
    - 1]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—åœ¨`[0, ..., config.vocab_size - 1]`ä¸­çš„æ ‡ç­¾ã€‚'
- en: Returns
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    and inputs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*optional*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ª+æ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[Data2VecAudioForCTC](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE14]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Data2VecAudioForSequenceClassification
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data2VecAudioForSequenceClassification
- en: '### `class transformers.Data2VecAudioForSequenceClassification`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Data2VecAudioForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1074)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1074)'
- en: '[PRE15]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[Data2VecAudioConfig](/docs/transformers/v4.37.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig)ï¼‰-æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Data2VecAudio Model with a sequence classification head on top (a linear layer
    over the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Data2VecAudioæ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰ä¸€ä¸ªåºåˆ—åˆ†ç±»å¤´ï¼ˆä¸€ä¸ªçº¿æ€§å±‚åœ¨æ± åŒ–è¾“å‡ºä¹‹ä¸Šï¼‰ç”¨äºç±»ä¼¼SUPERBå…³é”®è¯æ£€æµ‹çš„ä»»åŠ¡ã€‚
- en: 'Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised
    Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by
    Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 'Data2VecAudioæ˜¯ç”±Alexei Baevskiã€Wei-Ning Hsuã€Qiantong Xuã€Arun Babuã€Jiatao Guå’ŒMichael
    Auliåœ¨[æ•°æ®2vec: è¯­éŸ³ã€è§†è§‰å’Œè¯­è¨€è‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨æ¡†æ¶](https://arxiv.org/pdf/2202.03555)ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1126)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/data2vec/modeling_data2vec_audio.py#L1126)'
- en: '[PRE16]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
