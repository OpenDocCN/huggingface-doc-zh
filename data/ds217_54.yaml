- en: Differences between Dataset and IterableDataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和IterableDataset之间的区别
- en: 'Original text: [https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable](https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable](https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of dataset objects, a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    and an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset).
    Whichever type of dataset you choose to use or create depends on the size of the
    dataset. In general, an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and
    speed advantages, while a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    is great for everything else. This page will compare the differences between a
    [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    and an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    to help you pick the right dataset object for you.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集对象有两种类型，一个是[数据集](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)，另一个是[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)。您选择使用或创建哪种类型的数据集取决于数据集的大小。一般来说，由于其惰性行为和速度优势，[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)非常适合大型数据集（想想数百GB！），而[数据集](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)非常适合其他情况。本页将比较[数据集](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)和[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)之间的区别，以帮助您选择适合您的正确数据集对象。
- en: Downloading and streaming
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和流式传输
- en: 'When you have a regular [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset),
    you can access it using `my_dataset[0]`. This provides random access to the rows.
    Such datasets are also called “map-style” datasets. For example you can download
    ImageNet-1k like this and access any row:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当您拥有一个常规的[数据集](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)时，您可以使用`my_dataset[0]`来访问它。这提供了对行的随机访问。这种数据集也被称为“映射样式”数据集。例如，您可以像这样下载ImageNet-1k并访问任何行：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But one caveat is that you must have the entire dataset stored on your disk
    or in memory, which blocks you from accessing datasets bigger than the disk. Because
    it can become inconvenient for big datasets, there exists another type of dataset,
    the [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset).
    When you have an `IterableDataset`, you can access it using a `for` loop to load
    the data progressively as you iterate over the dataset. This way, only a small
    fraction of examples is loaded in memory, and you don’t write anything on disk.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但需要注意的是，您必须将整个数据集存储在磁盘或内存中，这会阻止您访问大于磁盘的数据集。因为对于大型数据集来说这可能会变得不方便，存在另一种类型的数据集，即[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)。当您拥有一个`IterableDataset`时，您可以使用`for`循环来逐步加载数据，只有一小部分示例被加载到内存中，而且您不会在磁盘上写入任何内容。
- en: 'For example, you can stream the ImageNet-1k dataset without downloading it
    on disk:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以在不下载到磁盘上的情况下流式传输ImageNet-1k数据集：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Streaming can read online data without writing any file to disk. For example,
    you can stream datasets made out of multiple shards, each of which is hundreds
    of gigabytes like [C4](https://huggingface.co/datasets/c4), [OSCAR](https://huggingface.co/datasets/oscar)
    or [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en). Learn more about
    how to stream a dataset in the [Dataset Streaming Guide](./stream).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 流式传输可以在不将任何文件写入磁盘的情况下读取在线数据。例如，您可以流式传输由多个分片组成的数据集，每个分片都有数百GB，比如[C4](https://huggingface.co/datasets/c4)、[OSCAR](https://huggingface.co/datasets/oscar)或[LAION-2B](https://huggingface.co/datasets/laion/laion2B-en)。了解有关如何在[数据集流式传输指南](./stream)中流式传输数据集的更多信息。
- en: This is not the only difference though, because the “lazy” behavior of an `IterableDataset`
    is also present when it comes to dataset creation and processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是唯一的区别，因为在数据集的创建和处理方面，`IterableDataset`的“惰性”行为也存在。
- en: Creating map-style datasets and iterable datasets
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建映射样式数据集和可迭代数据集
- en: 'You can create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    using lists or dictionaries, and the data is entirely converted to Arrow so you
    can easily access any row:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用列表或字典创建一个[数据集](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)，数据完全转换为Arrow，因此您可以轻松访问任何行：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To create an `IterableDataset` on the other hand, you must provide a “lazy”
    way to load the data. In Python, we generally use generator functions. These functions
    `yield` one example at a time, which means you can’t access a row by slicing it
    like a regular `Dataset`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，要创建一个`IterableDataset`，您必须提供一种“惰性”加载数据的方式。在Python中，我们通常使用生成器函数。这些函数每次`yield`一个示例，这意味着您无法像常规`数据集`那样通过切片访问行：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Loading local files entirely and progressively
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全和逐步加载本地文件
- en: 'It is possible to convert local or remote data files to an Arrow [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    using [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)将本地或远程数据文件转换为Arrow
    [数据集](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, this requires a conversion step from CSV to Arrow format, which takes
    time and disk space if your dataset is big.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这需要从CSV转换为Arrow格式，如果您的数据集很大，这将需要时间和磁盘空间。
- en: 'To save disk space and skip the conversion step, you can define an `IterableDataset`
    by streaming from the local files directly. This way, the data is read progressively
    from the local files as you iterate over the dataset:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省磁盘空间并跳过转换步骤，您可以通过直接从本地文件流式传输来定义一个`IterableDataset`。这样，当您在数据集上进行迭代时，数据会逐步从本地文件中读取：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Many file formats are supported, like CSV, JSONL, and Parquet, as well as image
    and audio files. You can find more information in the corresponding guides for
    loading [tabular](./tabular_load), [text](./nlp_load), [vision](./image_load),
    and [audio](./audio_load%5D) datasets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 支持许多文件格式，如CSV、JSONL和Parquet，以及图像和音频文件。您可以在相应的指南中找到有关加载[表格](./tabular_load)、[文本](./nlp_load)、[视觉](./image_load)和[音频](./audio_load%5D)数据集的更多信息。
- en: Eager data processing and lazy data processing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 急切数据处理和惰性数据处理
- en: When you process a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object using [Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map),
    the entire dataset is processed immediately and returned. This is similar to how
    `pandas` works for example.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用[Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)处理[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象时，整个数据集会立即被处理并返回。这类似于例如`pandas`的工作方式。
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: On the other hand, due to the “lazy” nature of an `IterableDataset`, calling
    [IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)
    does not apply your `map` function over the full dataset. Instead, your `map`
    function is applied on-the-fly.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于“惰性”`IterableDataset`的特性，调用[IterableDataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.map)不会将您的`map`函数应用于整个数据集。相反，您的`map`函数是实时应用的。
- en: 'Because of that, you can chain multiple processing steps and they will all
    run at once when you start iterating over the dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以链接多个处理步骤，当您开始迭代数据集时，它们将同时运行：
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Exact and fast approximate shuffling
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确和快速近似洗牌
- en: 'When you shuffle a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    using [Dataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle),
    you apply an exact shuffling of the dataset. It works by taking a list of indices
    `[0, 1, 2, ... len(my_dataset) - 1]` and shuffling this list. Then, accessing
    `my_dataset[0]` returns the row and index defined by the first element of the
    indices mapping that has been shuffled:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用[Dataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)对[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)进行洗牌时，您会对数据集进行精确的洗牌。它通过获取索引列表`[0,
    1, 2, ... len(my_dataset) - 1]`并对该列表进行洗牌来工作。然后，访问`my_dataset[0]`将返回由已洗牌的索引映射的第一个元素定义的行和索引：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Since we don’t have random access to the rows in the case of an `IterableDataset`,
    we can’t use a shuffled list of indices and access a row at an arbitrary position.
    This prevents the use of exact shuffling. Instead, a fast approximate shuffling
    is used in [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle).
    It uses a shuffle buffer to sample random examples iteratively from the dataset.
    Since the dataset is still read iteratively, it provides excellent speed performance:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在`IterableDataset`的情况下无法随机访问行，因此无法使用洗牌后的索引列表并在任意位置访问行。这阻止了使用精确的洗牌。相反，在[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)中使用了快速近似洗牌。它使用洗牌缓冲区从数据集中迭代地抽样随机示例。由于数据集仍然是迭代读取的，因此提供了出色的速度性能：
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'But using a shuffle buffer is not enough to provide a satisfactory shuffling
    for machine learning model training. So [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)
    also shuffles the dataset shards if your dataset is made of multiple files or
    sources:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，仅使用洗牌缓冲区无法为机器学习模型训练提供令人满意的洗牌。因此，如果您的数据集由多个文件或来源组成，则[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)也会对数据集片段进行洗牌：
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Speed differences
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度差异
- en: Regular [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    objects are based on Arrow which provides fast random access to the rows. Thanks
    to memory mapping and the fact that Arrow is an in-memory format, reading data
    from disk doesn’t do expensive system calls and deserialization. It provides even
    faster data loading when iterating using a `for` loop by iterating on contiguous
    Arrow record batches.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 常规[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象基于Arrow，提供对行的快速随机访问。由于内存映射和Arrow是内存格式，从磁盘读取数据不会进行昂贵的系统调用和反序列化。通过在连续的Arrow记录批次上进行迭代，使用`for`循环进行迭代时，甚至可以提供更快的数据加载。
- en: 'However as soon as your [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    has an indices mapping (via [Dataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)
    for example), the speed can become 10x slower. This is because there is an extra
    step to get the row index to read using the indices mapping, and most importantly,
    you aren’t reading contiguous chunks of data anymore. To restore the speed, you’d
    need to rewrite the entire dataset on your disk again using [Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices),
    which removes the indices mapping. This may take a lot of time depending of the
    size of your dataset though:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，一旦您的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)具有索引映射（例如通过[Dataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)），速度可能会变慢10倍。这是因为需要额外的步骤来获取要使用索引映射读取的行索引，最重要的是，您不再读取连续的数据块。要恢复速度，您需要再次使用[Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices)在磁盘上重新编写整个数据集，以删除索引映射。不过，这可能需要很长时间，具体取决于数据集的大小：
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this case, we recommend switching to an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    and leveraging its fast approximate shuffling method [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle).
    It only shuffles the shards order and adds a shuffle buffer to your dataset, which
    keeps the speed of your dataset optimal. You can also reshuffle the dataset easily:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们建议切换到[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)，并利用其快速近似洗牌方法[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)。它只会对碎片顺序进行洗牌，并向您的数据集添加一个洗牌缓冲区，从而保持数据集的速度最佳。您也可以轻松地重新洗牌数据集：
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you’re using your dataset on multiple epochs, the effective seed to shuffle
    the shards order in the shuffle buffer is `seed + epoch`. It makes it easy to
    reshuffle a dataset between epochs:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在多个时期使用数据集，则用于在洗牌缓冲区中洗牌碎片顺序的有效种子是`seed + epoch`。这样可以很容易地在时期之间重新洗牌数据集：
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Switch from map-style to iterable
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从映射样式切换到可迭代样式
- en: 'If you want to benefit from the “lazy” behavior of an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    or their speed advantages, you can switch your map-style [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    to an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要从[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)的“惰性”行为或其速度优势中受益，可以将您的映射样式[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)切换为[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)：
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you want to shuffle your dataset or [use it with a PyTorch DataLoader](./use_with_pytorch#stream-data),
    we recommend generating a sharded [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要洗牌您的数据集或[将其与PyTorch DataLoader一起使用](./use_with_pytorch#stream-data)，我们建议生成一个分片的[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)：
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
