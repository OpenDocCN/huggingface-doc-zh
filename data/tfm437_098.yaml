- en: Summary of the tokenizers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨æ€»ç»“
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tokenizer_summary](https://huggingface.co/docs/transformers/v4.37.2/en/tokenizer_summary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tokenizer_summary](https://huggingface.co/docs/transformers/v4.37.2/en/tokenizer_summary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: On this page, we will have a closer look at tokenization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé¡µé¢ä¸Šï¼Œæˆ‘ä»¬å°†æ›´ä»”ç»†åœ°çœ‹ä¸€ä¸‹åˆ†è¯ã€‚
- en: '[https://www.youtube-nocookie.com/embed/VFp38yj8h3A](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/VFp38yj8h3A](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)'
- en: 'As we saw in [the preprocessing tutorial](preprocessing), tokenizing a text
    is splitting it into words or subwords, which then are converted to ids through
    a look-up table. Converting words or subwords to ids is straightforward, so in
    this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing
    a text). More specifically, we will look at the three main types of tokenizers
    used in ğŸ¤— Transformers: [Byte-Pair Encoding (BPE)](#byte-pair-encoding), [WordPiece](#wordpiece),
    and [SentencePiece](#sentencepiece), and show examples of which tokenizer type
    is used by which model.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨[é¢„å¤„ç†æ•™ç¨‹](preprocessing)ä¸­çœ‹åˆ°çš„ï¼Œå°†æ–‡æœ¬åˆ†è¯æ˜¯å°†å…¶åˆ†å‰²æˆå•è¯æˆ–å­è¯ï¼Œç„¶åé€šè¿‡æŸ¥æ‰¾è¡¨å°†å…¶è½¬æ¢ä¸ºidã€‚å°†å•è¯æˆ–å­è¯è½¬æ¢ä¸ºidæ˜¯ç›´æ¥çš„ï¼Œå› æ­¤åœ¨æœ¬æ‘˜è¦ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºå°†æ–‡æœ¬åˆ†å‰²æˆå•è¯æˆ–å­è¯ï¼ˆå³æ–‡æœ¬åˆ†è¯ï¼‰ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å°†çœ‹ä¸€ä¸‹ğŸ¤—
    Transformersä¸­ä½¿ç”¨çš„ä¸‰ç§ä¸»è¦åˆ†è¯å™¨ç±»å‹ï¼š[Byte-Pair Encoding (BPE)](#byte-pair-encoding)ã€[WordPiece](#wordpiece)å’Œ[SentencePiece](#sentencepiece)ï¼Œå¹¶å±•ç¤ºå“ªç§æ¨¡å‹ä½¿ç”¨äº†å“ªç§åˆ†è¯å™¨ç±»å‹çš„ç¤ºä¾‹ã€‚
- en: Note that on each model page, you can look at the documentation of the associated
    tokenizer to know which tokenizer type was used by the pretrained model. For instance,
    if we look at [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer),
    we can see that the model uses [WordPiece](#wordpiece).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨æ¯ä¸ªæ¨¡å‹é¡µé¢ä¸Šï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹ç›¸å…³åˆ†è¯å™¨çš„æ–‡æ¡£ï¼Œä»¥äº†è§£é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨äº†å“ªç§åˆ†è¯å™¨ç±»å‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥æ¨¡å‹ä½¿ç”¨äº†[WordPiece](#wordpiece)ã€‚
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Splitting a text into smaller chunks is a task that is harder than it looks,
    and there are multiple ways of doing so. For instance, letâ€™s look at the sentence
    `"Don't you love ğŸ¤— Transformers? We sure do."`
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„å—æ˜¯ä¸€é¡¹çœ‹èµ·æ¥æ¯”è¾ƒå›°éš¾çš„ä»»åŠ¡ï¼Œæœ‰å¤šç§æ–¹æ³•å¯ä»¥å®ç°ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å¥å­`"Don't you love ğŸ¤— Transformers?
    We sure do."`
- en: '[https://www.youtube-nocookie.com/embed/nhJxYji1aho](https://www.youtube-nocookie.com/embed/nhJxYji1aho)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/nhJxYji1aho](https://www.youtube-nocookie.com/embed/nhJxYji1aho)'
- en: 'A simple way of tokenizing this text is to split it by spaces, which would
    give:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™æ®µæ–‡æœ¬ç®€å•åœ°æŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œä¼šå¾—åˆ°ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is a sensible first step, but if we look at the tokens `"Transformers?"`
    and `"do."`, we notice that the punctuation is attached to the words `"Transformer"`
    and `"do"`, which is suboptimal. We should take the punctuation into account so
    that a model does not have to learn a different representation of a word and every
    possible punctuation symbol that could follow it, which would explode the number
    of representations the model has to learn. Taking punctuation into account, tokenizing
    our exemplary text would give:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ˜æ™ºçš„ç¬¬ä¸€æ­¥ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹æ ‡è®°`"Transformers?"`å’Œ`"do."`ï¼Œæˆ‘ä»¬ä¼šæ³¨æ„åˆ°æ ‡ç‚¹ç¬¦å·é™„åŠ åœ¨å•è¯`"Transformer"`å’Œ`"do"`ä¸Šï¼Œè¿™æ˜¯ä¸å¤Ÿç†æƒ³çš„ã€‚æˆ‘ä»¬åº”è¯¥è€ƒè™‘æ ‡ç‚¹ç¬¦å·ï¼Œè¿™æ ·æ¨¡å‹å°±ä¸å¿…å­¦ä¹ ä¸€ä¸ªå•è¯çš„ä¸åŒè¡¨ç¤ºä»¥åŠå¯èƒ½è·Ÿéšå®ƒçš„æ¯ä¸ªå¯èƒ½çš„æ ‡ç‚¹ç¬¦å·ï¼Œè¿™å°†å¯¼è‡´æ¨¡å‹éœ€è¦å­¦ä¹ çš„è¡¨ç¤ºæ•°é‡æ¿€å¢ã€‚è€ƒè™‘æ ‡ç‚¹ç¬¦å·ï¼Œå¯¹æˆ‘ä»¬çš„ç¤ºä¾‹æ–‡æœ¬è¿›è¡Œåˆ†è¯ä¼šå¾—åˆ°ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Better. However, it is disadvantageous, how the tokenization dealt with the
    word `"Don't"`. `"Don't"` stands for `"do not"`, so it would be better tokenized
    as `["Do", "n't"]`. This is where things start getting complicated, and part of
    the reason each model has its own tokenizer type. Depending on the rules we apply
    for tokenizing a text, a different tokenized output is generated for the same
    text. A pretrained model only performs properly if you feed it an input that was
    tokenized with the same rules that were used to tokenize its training data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¥½ã€‚ç„¶è€Œï¼Œåˆ†è¯å¤„ç†å•è¯`"Don't"`çš„æ–¹å¼æ˜¯ä¸åˆ©çš„ã€‚`"Don't"`ä»£è¡¨`"do not"`ï¼Œæ‰€ä»¥æœ€å¥½å°†å…¶åˆ†è¯ä¸º`["Do", "n't"]`ã€‚è¿™å°±æ˜¯äº‹æƒ…å¼€å§‹å˜å¾—å¤æ‚çš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯æ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±çš„åˆ†è¯å™¨ç±»å‹çš„åŸå› ä¹‹ä¸€ã€‚æ ¹æ®æˆ‘ä»¬åº”ç”¨äºæ–‡æœ¬åˆ†è¯çš„è§„åˆ™ï¼Œç›¸åŒæ–‡æœ¬ä¼šç”Ÿæˆä¸åŒçš„åˆ†è¯è¾“å‡ºã€‚é¢„è®­ç»ƒæ¨¡å‹åªæœ‰åœ¨è¾“å…¥ä¸è®­ç»ƒæ•°æ®åˆ†è¯æ—¶ä½¿ç”¨çš„è§„åˆ™ç›¸åŒçš„æƒ…å†µä¸‹æ‰èƒ½æ­£å¸¸è¿è¡Œã€‚
- en: '[spaCy](https://spacy.io/) and [Moses](http://www.statmt.org/moses/?n=Development.GetStarted)
    are two popular rule-based tokenizers. Applying them on our example, *spaCy* and
    *Moses* would output something like:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[spaCy](https://spacy.io/)å’Œ[Moses](http://www.statmt.org/moses/?n=Development.GetStarted)æ˜¯ä¸¤ç§æµè¡Œçš„åŸºäºè§„åˆ™çš„åˆ†è¯å™¨ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸Šåº”ç”¨å®ƒä»¬ï¼Œ*spaCy*å’Œ*Moses*å¯èƒ½ä¼šè¾“å‡ºç±»ä¼¼ä»¥ä¸‹å†…å®¹ï¼š'
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As can be seen space and punctuation tokenization, as well as rule-based tokenization,
    is used here. Space and punctuation tokenization and rule-based tokenization are
    both examples of word tokenization, which is loosely defined as splitting sentences
    into words. While itâ€™s the most intuitive way to split texts into smaller chunks,
    this tokenization method can lead to problems for massive text corpora. In this
    case, space and punctuation tokenization usually generates a very big vocabulary
    (the set of all unique words and tokens used). *E.g.*, [Transformer XL](model_doc/transformerxl)
    uses space and punctuation tokenization, resulting in a vocabulary size of 267,735!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥çœ‹åˆ°ï¼Œè¿™é‡Œä½¿ç”¨äº†ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†è¯ï¼Œä»¥åŠåŸºäºè§„åˆ™çš„åˆ†è¯ã€‚ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†è¯ä»¥åŠåŸºäºè§„åˆ™çš„åˆ†è¯éƒ½æ˜¯å•è¯åˆ†è¯çš„ç¤ºä¾‹ï¼Œå®ƒä»¬è¢«å®½æ³›åœ°å®šä¹‰ä¸ºå°†å¥å­åˆ†å‰²æˆå•è¯ã€‚è™½ç„¶è¿™æ˜¯å°†æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°å—çš„æœ€ç›´è§‚çš„æ–¹æ³•ï¼Œä½†è¿™ç§åˆ†è¯æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“å‡ºç°é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç©ºæ ¼å’Œæ ‡ç‚¹åˆ†è¯é€šå¸¸ä¼šç”Ÿæˆä¸€ä¸ªéå¸¸åºå¤§çš„è¯æ±‡è¡¨ï¼ˆæ‰€æœ‰ä½¿ç”¨çš„å”¯ä¸€å•è¯å’Œæ ‡è®°çš„é›†åˆï¼‰ã€‚*ä¾‹å¦‚*ï¼Œ[Transformer
    XL](model_doc/transformerxl)ä½¿ç”¨ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†è¯ï¼Œå¯¼è‡´è¯æ±‡é‡ä¸º267,735ï¼
- en: Such a big vocabulary size forces the model to have an enormous embedding matrix
    as the input and output layer, which causes both an increased memory and time
    complexity. In general, transformers models rarely have a vocabulary size greater
    than 50,000, especially if they are pretrained only on a single language.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ­¤åºå¤§çš„è¯æ±‡é‡è¿«ä½¿æ¨¡å‹å…·æœ‰å·¨å¤§çš„åµŒå…¥çŸ©é˜µä½œä¸ºè¾“å…¥å’Œè¾“å‡ºå±‚ï¼Œè¿™ä¼šå¯¼è‡´å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦å¢åŠ ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œtransformersæ¨¡å‹å¾ˆå°‘æœ‰è¯æ±‡é‡è¶…è¿‡50,000çš„æƒ…å†µï¼Œå°¤å…¶æ˜¯å¦‚æœå®ƒä»¬åªåœ¨å•ä¸€è¯­è¨€ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚
- en: So if simple space and punctuation tokenization is unsatisfactory, why not simply
    tokenize on characters?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœç®€å•çš„ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†è¯ä¸å°½å¦‚äººæ„ï¼Œä¸ºä»€ä¹ˆä¸ç®€å•åœ°åœ¨å­—ç¬¦ä¸Šè¿›è¡Œåˆ†è¯å‘¢ï¼Ÿ
- en: '[https://www.youtube-nocookie.com/embed/ssLq_EK2jLE](https://www.youtube-nocookie.com/embed/ssLq_EK2jLE)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/ssLq_EK2jLE](https://www.youtube-nocookie.com/embed/ssLq_EK2jLE)'
- en: While character tokenization is very simple and would greatly reduce memory
    and time complexity it makes it much harder for the model to learn meaningful
    input representations. *E.g.* learning a meaningful context-independent representation
    for the letter `"t"` is much harder than learning a context-independent representation
    for the word `"today"`. Therefore, character tokenization is often accompanied
    by a loss of performance. So to get the best of both worlds, transformers models
    use a hybrid between word-level and character-level tokenization called **subword**
    tokenization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å­—ç¬¦åˆ†è¯éå¸¸ç®€å•ä¸”å¯ä»¥å¤§å¤§å‡å°‘å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦ï¼Œä½†è¿™ä½¿å¾—æ¨¡å‹æ›´éš¾å­¦ä¹ æœ‰æ„ä¹‰çš„è¾“å…¥è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œå­¦ä¹ å­—æ¯`"t"`çš„æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡æ— å…³è¡¨ç¤ºè¦æ¯”å­¦ä¹ å•è¯`"today"`çš„ä¸Šä¸‹æ–‡æ— å…³è¡¨ç¤ºå›°éš¾å¾—å¤šã€‚å› æ­¤ï¼Œå­—ç¬¦åˆ†è¯é€šå¸¸ä¼´éšç€æ€§èƒ½æŸå¤±ã€‚å› æ­¤ï¼Œä¸ºäº†å…¼é¡¾ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œtransformersæ¨¡å‹ä½¿ç”¨äº†ä»‹äºå•è¯çº§å’Œå­—ç¬¦çº§åˆ†è¯ä¹‹é—´çš„æ··åˆç§°ä¸º**å­è¯**åˆ†è¯çš„æ–¹æ³•ã€‚
- en: Subword tokenization
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­è¯åˆ†è¯
- en: '[https://www.youtube-nocookie.com/embed/zHvTiHr506c](https://www.youtube-nocookie.com/embed/zHvTiHr506c)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/zHvTiHr506c](https://www.youtube-nocookie.com/embed/zHvTiHr506c)'
- en: Subword tokenization algorithms rely on the principle that frequently used words
    should not be split into smaller subwords, but rare words should be decomposed
    into meaningful subwords. For instance `"annoyingly"` might be considered a rare
    word and could be decomposed into `"annoying"` and `"ly"`. Both `"annoying"` and
    `"ly"` as stand-alone subwords would appear more frequently while at the same
    time the meaning of `"annoyingly"` is kept by the composite meaning of `"annoying"`
    and `"ly"`. This is especially useful in agglutinative languages such as Turkish,
    where you can form (almost) arbitrarily long complex words by stringing together
    subwords.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å­è¯åˆ†è¯ç®—æ³•ä¾èµ–äºè¿™æ ·ä¸€ä¸ªåŸåˆ™ï¼Œå³ç»å¸¸ä½¿ç”¨çš„å•è¯ä¸åº”è¯¥è¢«åˆ†å‰²ä¸ºæ›´å°çš„å­è¯ï¼Œä½†ç½•è§çš„å•è¯åº”è¯¥è¢«åˆ†è§£ä¸ºæœ‰æ„ä¹‰çš„å­è¯ã€‚ä¾‹å¦‚ï¼Œ`"annoyingly"`å¯èƒ½è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªç½•è§å•è¯ï¼Œå¯ä»¥è¢«åˆ†è§£ä¸º`"annoying"`å’Œ`"ly"`ã€‚`"annoying"`å’Œ`"ly"`ä½œä¸ºç‹¬ç«‹çš„å­è¯ä¼šæ›´é¢‘ç¹åœ°å‡ºç°ï¼ŒåŒæ—¶`"annoyingly"`çš„å«ä¹‰ç”±`"annoying"`å’Œ`"ly"`çš„ç»„åˆå«ä¹‰ä¿ç•™ã€‚è¿™åœ¨åƒåœŸè€³å…¶è¿™æ ·çš„èšåˆè¯­è¨€ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¸²è”å­è¯å½¢æˆï¼ˆå‡ ä¹ï¼‰ä»»æ„é•¿çš„å¤æ‚å•è¯ã€‚
- en: 'Subword tokenization allows the model to have a reasonable vocabulary size
    while being able to learn meaningful context-independent representations. In addition,
    subword tokenization enables the model to process words it has never seen before,
    by decomposing them into known subwords. For instance, the [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    tokenizes `"I have a new GPU!"` as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å­è¯åˆ†è¯ä½¿æ¨¡å‹èƒ½å¤Ÿæ‹¥æœ‰åˆç†çš„è¯æ±‡é‡ï¼ŒåŒæ—¶èƒ½å¤Ÿå­¦ä¹ æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡æ— å…³è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå­è¯åˆ†è¯ä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†å®ƒä»¥å‰ä»æœªè§è¿‡çš„å•è¯ï¼Œé€šè¿‡å°†å®ƒä»¬åˆ†è§£ä¸ºå·²çŸ¥çš„å­è¯ã€‚ä¾‹å¦‚ï¼Œ[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)å°†`"I
    have a new GPU!"`åˆ†è¯å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Because we are considering the uncased model, the sentence was lowercased first.
    We can see that the words `["i", "have", "a", "new"]` are present in the tokenizerâ€™s
    vocabulary, but the word `"gpu"` is not. Consequently, the tokenizer splits `"gpu"`
    into known subwords: `["gp" and "##u"]`. `"##"` means that the rest of the token
    should be attached to the previous one, without space (for decoding or reversal
    of the tokenization).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬è€ƒè™‘çš„æ˜¯ä¸åŒºåˆ†å¤§å°å†™çš„æ¨¡å‹ï¼Œæ‰€ä»¥é¦–å…ˆå°†å¥å­è½¬æ¢ä¸ºå°å†™ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å•è¯`["i", "have", "a", "new"]`å­˜åœ¨äºåˆ†è¯å™¨çš„è¯æ±‡è¡¨ä¸­ï¼Œä½†å•è¯`"gpu"`ä¸åœ¨å…¶ä¸­ã€‚å› æ­¤ï¼Œåˆ†è¯å™¨å°†`"gpu"`åˆ†å‰²ä¸ºå·²çŸ¥çš„å­è¯ï¼š`["gp"
    å’Œ "##u"]`ã€‚`"##"`è¡¨ç¤ºå‰©ä½™çš„æ ‡è®°åº”è¯¥é™„åŠ åˆ°å‰ä¸€ä¸ªæ ‡è®°ä¸Šï¼Œæ²¡æœ‰ç©ºæ ¼ï¼ˆç”¨äºè§£ç æˆ–åå‘åˆ†è¯ï¼‰ã€‚
- en: 'As another example, [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)
    tokenizes our previously exemplary text as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºå¦ä¸€ä¸ªä¾‹å­ï¼Œ[XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)å°†æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹æ–‡æœ¬åˆ†è¯å¦‚ä¸‹ï¼š
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Weâ€™ll get back to the meaning of those `"â–"` when we look at [SentencePiece](#sentencepiece).
    As one can see, the rare word `"Transformers"` has been split into the more frequent
    subwords `"Transform"` and `"ers"`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æŸ¥çœ‹[SentencePiece](#sentencepiece)æ—¶ï¼Œæˆ‘ä»¬å°†å›åˆ°é‚£äº›`"â–"`çš„å«ä¹‰ã€‚æ­£å¦‚å¤§å®¶æ‰€çœ‹åˆ°çš„ï¼Œç½•è§å•è¯`"Transformers"`å·²è¢«åˆ†å‰²ä¸ºæ›´å¸¸è§çš„å­è¯`"Transform"`å’Œ`"ers"`ã€‚
- en: Letâ€™s now look at how the different subword tokenization algorithms work. Note
    that all of those tokenization algorithms rely on some form of training which
    is usually done on the corpus the corresponding model will be trained on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä¸åŒçš„å­è¯åˆ†è¯ç®—æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚è¯·æ³¨æ„ï¼Œæ‰€æœ‰è¿™äº›åˆ†è¯ç®—æ³•éƒ½ä¾èµ–äºæŸç§å½¢å¼çš„è®­ç»ƒï¼Œé€šå¸¸æ˜¯åœ¨ç›¸åº”æ¨¡å‹å°†è¢«è®­ç»ƒçš„è¯­æ–™åº“ä¸Šè¿›è¡Œçš„ã€‚
- en: Byte-Pair Encoding (BPE)
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰
- en: Byte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare
    Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909).
    BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization
    can be as simple as space tokenization, e.g. [GPT-2](model_doc/gpt2), [RoBERTa](model_doc/roberta).
    More advanced pre-tokenization include rule-based tokenization, e.g. [XLM](model_doc/xlm),
    [FlauBERT](model_doc/flaubert) which uses Moses for most languages, or [GPT](model_doc/gpt)
    which uses Spacy and ftfy, to count the frequency of each word in the training
    corpus.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æ˜¯åœ¨[Neural Machine Translation of Rare Words with Subword Units (Sennrich
    et al., 2015)](https://arxiv.org/abs/1508.07909)ä¸­å¼•å…¥çš„ã€‚BPEä¾èµ–äºä¸€ä¸ªé¢„åˆ†è¯å™¨ï¼Œå°†è®­ç»ƒæ•°æ®åˆ†å‰²æˆå•è¯ã€‚é¢„åˆ†è¯å¯ä»¥ç®€å•åˆ°ç©ºæ ¼åˆ†è¯ï¼Œä¾‹å¦‚[GPT-2](model_doc/gpt2)ï¼Œ[RoBERTa](model_doc/roberta)ã€‚æ›´é«˜çº§çš„é¢„åˆ†è¯åŒ…æ‹¬åŸºäºè§„åˆ™çš„åˆ†è¯ï¼Œä¾‹å¦‚[XLM](model_doc/xlm)ï¼Œ[FlauBERT](model_doc/flaubert)ä½¿ç”¨Mosesç”¨äºå¤§å¤šæ•°è¯­è¨€ï¼Œæˆ–è€…[GPT](model_doc/gpt)ä½¿ç”¨Spacyå’Œftfyï¼Œæ¥è®¡ç®—è®­ç»ƒè¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ã€‚
- en: After pre-tokenization, a set of unique words has been created and the frequency
    with which each word occurred in the training data has been determined. Next,
    BPE creates a base vocabulary consisting of all symbols that occur in the set
    of unique words and learns merge rules to form a new symbol from two symbols of
    the base vocabulary. It does so until the vocabulary has attained the desired
    vocabulary size. Note that the desired vocabulary size is a hyperparameter to
    define before training the tokenizer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¢„åˆ†è¯ä¹‹åï¼Œå·²åˆ›å»ºäº†ä¸€ç»„å”¯ä¸€çš„å•è¯ï¼Œå¹¶ç¡®å®šäº†æ¯ä¸ªå•è¯åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„é¢‘ç‡ã€‚æ¥ä¸‹æ¥ï¼ŒBPEåˆ›å»ºä¸€ä¸ªåŸºæœ¬è¯æ±‡ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰å‡ºç°åœ¨å”¯ä¸€å•è¯é›†åˆä¸­çš„ç¬¦å·ï¼Œå¹¶å­¦ä¹ åˆå¹¶è§„åˆ™ï¼Œä»¥ä»åŸºæœ¬è¯æ±‡çš„ä¸¤ä¸ªç¬¦å·å½¢æˆä¸€ä¸ªæ–°ç¬¦å·ã€‚å®ƒä¼šä¸€ç›´è¿™æ ·åšï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°æ‰€éœ€çš„è¯æ±‡é‡ã€‚è¯·æ³¨æ„ï¼Œæ‰€éœ€çš„è¯æ±‡é‡æ˜¯åœ¨è®­ç»ƒåˆ†è¯å™¨ä¹‹å‰å®šä¹‰çš„ä¸€ä¸ªè¶…å‚æ•°ã€‚
- en: 'As an example, letâ€™s assume that after pre-tokenization, the following set
    of words including their frequency has been determined:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾åœ¨é¢„åˆ†è¯ä¹‹åï¼Œå·²ç¡®å®šäº†ä»¥ä¸‹åŒ…å«é¢‘ç‡çš„å•è¯é›†åˆï¼š
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Consequently, the base vocabulary is `["b", "g", "h", "n", "p", "s", "u"]`.
    Splitting all words into symbols of the base vocabulary, we obtain:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒåŸºæœ¬è¯æ±‡æ˜¯`["b", "g", "h", "n", "p", "s", "u"]`ã€‚å°†æ‰€æœ‰å•è¯åˆ†å‰²ä¸ºåŸºæœ¬è¯æ±‡çš„ç¬¦å·ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: BPE then counts the frequency of each possible symbol pair and picks the symbol
    pair that occurs most frequently. In the example above `"h"` followed by `"u"`
    is present *10 + 5 = 15* times (10 times in the 10 occurrences of `"hug"`, 5 times
    in the 5 occurrences of `"hugs"`). However, the most frequent symbol pair is `"u"`
    followed by `"g"`, occurring *10 + 5 + 5 = 20* times in total. Thus, the first
    merge rule the tokenizer learns is to group all `"u"` symbols followed by a `"g"`
    symbol together. Next, `"ug"` is added to the vocabulary. The set of words then
    becomes
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BPEç„¶åè®¡ç®—æ¯å¯¹å¯èƒ½ç¬¦å·çš„é¢‘ç‡ï¼Œå¹¶é€‰æ‹©å‡ºç°æœ€é¢‘ç¹çš„ç¬¦å·å¯¹ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ`"h"`åè·Ÿ`"u"`å‡ºç°äº†*10 + 5 = 15*æ¬¡ï¼ˆåœ¨10æ¬¡`"hug"`å‡ºç°ä¸­çš„10æ¬¡ï¼Œä»¥åŠåœ¨5æ¬¡`"hugs"`å‡ºç°ä¸­çš„5æ¬¡ï¼‰ã€‚ç„¶è€Œï¼Œæœ€é¢‘ç¹çš„ç¬¦å·å¯¹æ˜¯`"u"`åè·Ÿ`"g"`ï¼Œæ€»å…±å‡ºç°äº†*10
    + 5 + 5 = 20*æ¬¡ã€‚å› æ­¤ï¼Œåˆ†è¯å™¨å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯å°†æ‰€æœ‰è·Ÿåœ¨`"u"`ç¬¦å·åé¢çš„`"g"`ç¬¦å·ç»„åˆåœ¨ä¸€èµ·ã€‚æ¥ä¸‹æ¥ï¼Œ`"ug"`è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚ç„¶åå•è¯é›†åˆå˜ä¸º
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: BPE then identifies the next most common symbol pair. Itâ€™s `"u"` followed by
    `"n"`, which occurs 16 times. `"u"`, `"n"` is merged to `"un"` and added to the
    vocabulary. The next most frequent symbol pair is `"h"` followed by `"ug"`, occurring
    15 times. Again the pair is merged and `"hug"` can be added to the vocabulary.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: BPEç„¶åè¯†åˆ«ä¸‹ä¸€ä¸ªæœ€å¸¸è§çš„ç¬¦å·å¯¹ã€‚å®ƒæ˜¯`"u"`åè·Ÿ`"n"`ï¼Œå‡ºç°äº†16æ¬¡ã€‚`"u"`ã€`"n"`è¢«åˆå¹¶ä¸º`"un"`å¹¶æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚ä¸‹ä¸€ä¸ªæœ€é¢‘ç¹çš„ç¬¦å·å¯¹æ˜¯`"h"`åè·Ÿ`"ug"`ï¼Œå‡ºç°äº†15æ¬¡ã€‚å†æ¬¡åˆå¹¶è¿™å¯¹ï¼Œå¹¶ä¸”`"hug"`å¯ä»¥è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚
- en: At this stage, the vocabulary is `["b", "g", "h", "n", "p", "s", "u", "ug",
    "un", "hug"]` and our set of unique words is represented as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé˜¶æ®µï¼Œè¯æ±‡è¡¨æ˜¯`["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]`ï¼Œæˆ‘ä»¬çš„å”¯ä¸€å•è¯é›†åˆè¡¨ç¤ºä¸º
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Assuming, that the Byte-Pair Encoding training would stop at this point, the
    learned merge rules would then be applied to new words (as long as those new words
    do not include symbols that were not in the base vocabulary). For instance, the
    word `"bug"` would be tokenized to `["b", "ug"]` but `"mug"` would be tokenized
    as `["<unk>", "ug"]` since the symbol `"m"` is not in the base vocabulary. In
    general, single letters such as `"m"` are not replaced by the `"<unk>"` symbol
    because the training data usually includes at least one occurrence of each letter,
    but it is likely to happen for very special characters like emojis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾å­—èŠ‚å¯¹ç¼–ç è®­ç»ƒåœ¨è¿™ä¸€ç‚¹åœæ­¢ï¼Œé‚£ä¹ˆå­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™å°†è¢«åº”ç”¨äºæ–°å•è¯ï¼ˆåªè¦è¿™äº›æ–°å•è¯ä¸åŒ…å«åŸºæœ¬è¯æ±‡ä¸­æ²¡æœ‰çš„ç¬¦å·ï¼‰ã€‚ä¾‹å¦‚ï¼Œå•è¯`"bug"`å°†è¢«åˆ†è¯ä¸º`["b",
    "ug"]`ï¼Œä½†`"mug"`å°†è¢«åˆ†è¯ä¸º`["<unk>", "ug"]`ï¼Œå› ä¸ºç¬¦å·`"m"`ä¸åœ¨åŸºæœ¬è¯æ±‡ä¸­ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œåƒ`"m"`è¿™æ ·çš„å•ä¸ªå­—æ¯ä¸ä¼šè¢«`"<unk>"`ç¬¦å·æ›¿æ¢ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®é€šå¸¸è‡³å°‘åŒ…å«æ¯ä¸ªå­—æ¯çš„ä¸€ä¸ªå‡ºç°ï¼Œä½†å¯¹äºéå¸¸ç‰¹æ®Šçš„å­—ç¬¦ï¼Œæ¯”å¦‚è¡¨æƒ…ç¬¦å·ï¼Œå¯èƒ½ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚
- en: As mentioned earlier, the vocabulary size, *i.e.* the base vocabulary size +
    the number of merges, is a hyperparameter to choose. For instance [GPT](model_doc/gpt)
    has a vocabulary size of 40,478 since they have 478 base characters and chose
    to stop training after 40,000 merges.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¯æ±‡é‡ï¼Œå³åŸºæœ¬è¯æ±‡é‡+åˆå¹¶æ¬¡æ•°ï¼Œæ˜¯ä¸€ä¸ªéœ€è¦é€‰æ‹©çš„è¶…å‚æ•°ã€‚ä¾‹å¦‚[GPT](model_doc/gpt)çš„è¯æ±‡é‡ä¸º40,478ï¼Œå› ä¸ºå®ƒä»¬æœ‰478ä¸ªåŸºæœ¬å­—ç¬¦ï¼Œå¹¶é€‰æ‹©åœ¨40,000æ¬¡åˆå¹¶ååœæ­¢è®­ç»ƒã€‚
- en: Byte-level BPE
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å­—èŠ‚çº§BPE
- en: A base vocabulary that includes all possible base characters can be quite large
    if *e.g.* all unicode characters are considered as base characters. To have a
    better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    uses bytes as the base vocabulary, which is a clever trick to force the base vocabulary
    to be of size 256 while ensuring that every base character is included in the
    vocabulary. With some additional rules to deal with punctuation, the GPT2â€™s tokenizer
    can tokenize every text without the need for the <unk> symbol. [GPT-2](model_doc/gpt)
    has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens,
    a special end-of-text token and the symbols learned with 50,000 merges.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°†æ‰€æœ‰Unicodeå­—ç¬¦è§†ä¸ºåŸºæœ¬å­—ç¬¦ï¼Œé‚£ä¹ˆåŒ…å«æ‰€æœ‰å¯èƒ½åŸºæœ¬å­—ç¬¦çš„åŸºæœ¬è¯æ±‡å¯èƒ½ä¼šéå¸¸åºå¤§ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„åŸºæœ¬è¯æ±‡ï¼Œ[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    ä½¿ç”¨å­—èŠ‚ä½œä¸ºåŸºæœ¬è¯æ±‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå·§å¦™çš„æŠ€å·§ï¼Œå¯ä»¥å¼ºåˆ¶åŸºæœ¬è¯æ±‡çš„å¤§å°ä¸º256ï¼ŒåŒæ—¶ç¡®ä¿æ¯ä¸ªåŸºæœ¬å­—ç¬¦éƒ½åŒ…å«åœ¨è¯æ±‡ä¸­ã€‚é€šè¿‡ä¸€äº›é¢å¤–çš„è§„åˆ™æ¥å¤„ç†æ ‡ç‚¹ç¬¦å·ï¼ŒGPT2çš„åˆ†è¯å™¨å¯ä»¥å¯¹æ¯ä¸ªæ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè€Œæ— éœ€ä½¿ç”¨<unk>ç¬¦å·ã€‚[GPT-2](model_doc/gpt)
    çš„è¯æ±‡é‡ä¸º50,257ï¼Œå¯¹åº”äº256ä¸ªå­—èŠ‚åŸºæœ¬æ ‡è®°ã€ä¸€ä¸ªç‰¹æ®Šçš„æ–‡æœ¬ç»“æŸæ ‡è®°å’Œé€šè¿‡50,000æ¬¡åˆå¹¶å­¦ä¹ çš„ç¬¦å·ã€‚
- en: WordPiece
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WordPiece
- en: WordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert),
    [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm
    was outlined in [Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
    and is very similar to BPE. WordPiece first initializes the vocabulary to include
    every character present in the training data and progressively learns a given
    number of merge rules. In contrast to BPE, WordPiece does not choose the most
    frequent symbol pair, but the one that maximizes the likelihood of the training
    data once added to the vocabulary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: WordPieceæ˜¯ç”¨äº[BERT](model_doc/bert)ã€[DistilBERT](model_doc/distilbert)å’Œ[Electra](model_doc/electra)çš„å­è¯åˆ†è¯ç®—æ³•ã€‚è¯¥ç®—æ³•åœ¨[Japanese
    and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)ä¸­æ¦‚è¿°ï¼Œä¸BPEéå¸¸ç›¸ä¼¼ã€‚WordPieceé¦–å…ˆå°†è¯æ±‡è¡¨åˆå§‹åŒ–ä¸ºåŒ…å«è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªå­—ç¬¦ï¼Œå¹¶é€æ¸å­¦ä¹ ä¸€å®šæ•°é‡çš„åˆå¹¶è§„åˆ™ã€‚ä¸BPEä¸åŒï¼ŒWordPieceä¸é€‰æ‹©æœ€é¢‘ç¹çš„ç¬¦å·å¯¹ï¼Œè€Œæ˜¯é€‰æ‹©ä¸€æ—¦æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­å°±æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§çš„ç¬¦å·å¯¹ã€‚
- en: So what does this mean exactly? Referring to the previous example, maximizing
    the likelihood of the training data is equivalent to finding the symbol pair,
    whose probability divided by the probabilities of its first symbol followed by
    its second symbol is the greatest among all symbol pairs. *E.g.* `"u"`, followed
    by `"g"` would have only been merged if the probability of `"ug"` divided by `"u"`,
    `"g"` would have been greater than for any other symbol pair. Intuitively, WordPiece
    is slightly different to BPE in that it evaluates what it *loses* by merging two
    symbols to ensure itâ€™s *worth it*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¿™åˆ°åº•æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿå‚è€ƒå‰é¢çš„ä¾‹å­ï¼Œæœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§ç­‰åŒäºæ‰¾åˆ°ç¬¦å·å¯¹ï¼Œå…¶æ¦‚ç‡é™¤ä»¥å…¶ç¬¬ä¸€ä¸ªç¬¦å·åè·Ÿç¬¬äºŒä¸ªç¬¦å·çš„æ¦‚ç‡åœ¨æ‰€æœ‰ç¬¦å·å¯¹ä¸­æœ€å¤§ã€‚ä¾‹å¦‚ï¼Œ`"u"`åè·Ÿ`"g"`åªæœ‰åœ¨`"ug"`çš„æ¦‚ç‡é™¤ä»¥`"u"`ã€`"g"`çš„æ¦‚ç‡å¤§äºä»»ä½•å…¶ä»–ç¬¦å·å¯¹æ—¶æ‰ä¼šè¢«åˆå¹¶ã€‚ç›´è§‰ä¸Šï¼ŒWordPieceä¸BPEç•¥æœ‰ä¸åŒï¼Œå› ä¸ºå®ƒè¯„ä¼°åˆå¹¶ä¸¤ä¸ªç¬¦å·ä¼šæŸå¤±ä»€ä¹ˆï¼Œä»¥ç¡®ä¿å€¼å¾—ã€‚
- en: Unigram
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Unigram
- en: 'Unigram is a subword tokenization algorithm introduced in [Subword Regularization:
    Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo,
    2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or WordPiece,
    Unigram initializes its base vocabulary to a large number of symbols and progressively
    trims down each symbol to obtain a smaller vocabulary. The base vocabulary could
    for instance correspond to all pre-tokenized words and the most common substrings.
    Unigram is not used directly for any of the models in the transformers, but itâ€™s
    used in conjunction with [SentencePiece](#sentencepiece).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Unigramæ˜¯ä¸€ç§å­è¯åˆ†è¯ç®—æ³•ï¼Œç”±[Kudo, 2018](https://arxiv.org/pdf/1804.10959.pdf)å¼•å…¥ã€‚ä¸BPEæˆ–WordPieceç›¸æ¯”ï¼ŒUnigramå°†å…¶åŸºæœ¬è¯æ±‡åˆå§‹åŒ–ä¸ºå¤§é‡ç¬¦å·ï¼Œå¹¶é€æ¸ä¿®å‰ªæ¯ä¸ªç¬¦å·ä»¥è·å¾—è¾ƒå°çš„è¯æ±‡è¡¨ã€‚åŸºæœ¬è¯æ±‡è¡¨å¯ä»¥å¯¹åº”äºæ‰€æœ‰é¢„åˆ†è¯çš„å•è¯å’Œæœ€å¸¸è§çš„å­å­—ç¬¦ä¸²ã€‚Unigramä¸ç›´æ¥ç”¨äºtransformersä¸­çš„ä»»ä½•æ¨¡å‹ï¼Œä½†ä¸[SentencePiece](#sentencepiece)ä¸€èµ·ä½¿ç”¨ã€‚
- en: At each training step, the Unigram algorithm defines a loss (often defined as
    the log-likelihood) over the training data given the current vocabulary and a
    unigram language model. Then, for each symbol in the vocabulary, the algorithm
    computes how much the overall loss would increase if the symbol was to be removed
    from the vocabulary. Unigram then removes p (with p usually being 10% or 20%)
    percent of the symbols whose loss increase is the lowest, *i.e.* those symbols
    that least affect the overall loss over the training data. This process is repeated
    until the vocabulary has reached the desired size. The Unigram algorithm always
    keeps the base characters so that any word can be tokenized.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼ŒUnigramç®—æ³•æ ¹æ®å½“å‰è¯æ±‡è¡¨å’Œunigramè¯­è¨€æ¨¡å‹å®šä¹‰äº†ä¸€ä¸ªæŸå¤±ï¼ˆé€šå¸¸å®šä¹‰ä¸ºå¯¹æ•°ä¼¼ç„¶ï¼‰ã€‚ç„¶åï¼Œå¯¹äºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªç¬¦å·ï¼Œè¯¥ç®—æ³•è®¡ç®—å¦‚æœå°†è¯¥ç¬¦å·ä»è¯æ±‡è¡¨ä¸­ç§»é™¤ä¼šå¯¼è‡´æ•´ä½“æŸå¤±å¢åŠ å¤šå°‘ã€‚Unigramç„¶ååˆ é™¤æŸå¤±å¢åŠ æœ€ä½çš„pï¼ˆé€šå¸¸ä¸º10%æˆ–20%ï¼‰ç™¾åˆ†æ¯”çš„ç¬¦å·ï¼Œå³é‚£äº›å¯¹è®­ç»ƒæ•°æ®æ•´ä½“æŸå¤±å½±å“æœ€å°çš„ç¬¦å·ã€‚è¿™ä¸ªè¿‡ç¨‹é‡å¤è¿›è¡Œï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°æ‰€éœ€å¤§å°ã€‚Unigramç®—æ³•å§‹ç»ˆä¿ç•™åŸºæœ¬å­—ç¬¦ï¼Œä»¥ä¾¿ä»»ä½•å•è¯éƒ½å¯ä»¥è¢«åˆ†è¯ã€‚
- en: 'Because Unigram is not based on merge rules (in contrast to BPE and WordPiece),
    the algorithm has several ways of tokenizing new text after training. As an example,
    if a trained Unigram tokenizer exhibits the vocabulary:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºUnigramä¸åŸºäºåˆå¹¶è§„åˆ™ï¼ˆä¸BPEå’ŒWordPieceç›¸åï¼‰ï¼Œè¯¥ç®—æ³•åœ¨è®­ç»ƒåæœ‰å‡ ç§åˆ†è¯æ–°æ–‡æœ¬çš„æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç»è¿‡è®­ç»ƒçš„Unigramåˆ†è¯å™¨å±•ç¤ºä»¥ä¸‹è¯æ±‡è¡¨ï¼š
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`"hugs"` could be tokenized both as `["hug", "s"]`, `["h", "ug", "s"]` or `["h",
    "u", "g", "s"]`. So which one to choose? Unigram saves the probability of each
    token in the training corpus on top of saving the vocabulary so that the probability
    of each possible tokenization can be computed after training. The algorithm simply
    picks the most likely tokenization in practice, but also offers the possibility
    to sample a possible tokenization according to their probabilities.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`"hugs"`å¯ä»¥è¢«åˆ†è¯ä¸º`["hug", "s"]`ã€`["h", "ug", "s"]`æˆ–`["h", "u", "g", "s"]`ã€‚é‚£ä¹ˆåº”è¯¥é€‰æ‹©å“ªä¸€ä¸ªï¼ŸUnigramåœ¨ä¿å­˜è¯æ±‡çš„åŒæ—¶è¿˜ä¿å­˜äº†è®­ç»ƒè¯­æ–™åº“ä¸­æ¯ä¸ªæ ‡è®°çš„æ¦‚ç‡ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒåè®¡ç®—æ¯ç§å¯èƒ½çš„åˆ†è¯çš„æ¦‚ç‡ã€‚è¯¥ç®—æ³•å®é™…ä¸Šåªé€‰æ‹©æœ€æœ‰å¯èƒ½çš„åˆ†è¯ï¼Œä½†ä¹Ÿæä¾›äº†æ ¹æ®å®ƒä»¬çš„æ¦‚ç‡å¯¹å¯èƒ½çš„åˆ†è¯è¿›è¡ŒæŠ½æ ·çš„å¯èƒ½æ€§ã€‚'
- en: Those probabilities are defined by the loss the tokenizer is trained on. Assuming
    that the training data consists of the words<math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo
    separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>N</mi></msub></mrow><annotation
    encoding="application/x-tex">x_{1}, \dots, x_{N}</annotation></semantics></math>x1â€‹,â€¦,xNâ€‹
    and that the set of all possible tokenizations for a word<math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">x_{i}</annotation></semantics></math>xiâ€‹ is defined
    as<math><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S(x_{i})</annotation></semantics></math>S(xiâ€‹),
    then the overall loss is defined as <math display="block"><semantics><mrow><mi
    mathvariant="script">L</mi><mo>=</mo><mo>âˆ’</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>â¡</mo><mrow><mo
    fence="true">(</mo><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false">)</mo></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo
    stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}
    = -\sum_{i=1}^{N} \log \left ( \sum_{x \in S(x_{i})} p(x) \right )</annotation></semantics></math>L=âˆ’i=1âˆ‘Nâ€‹logâ€‹xâˆˆS(xiâ€‹)âˆ‘â€‹p(x)â€‹
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¦‚ç‡ç”±æ ‡è®°å™¨è®­ç»ƒæ—¶å®šä¹‰çš„æŸå¤±æ¥ç¡®å®šã€‚å‡è®¾è®­ç»ƒæ•°æ®ç”±å•è¯ x1â€‹,â€¦,xNâ€‹ ç»„æˆï¼Œå¹¶ä¸”å¯¹äºå•è¯ xiâ€‹ çš„æ‰€æœ‰å¯èƒ½æ ‡è®°åŒ–é›†åˆå®šä¹‰ä¸º S(xiâ€‹)ï¼Œåˆ™æ€»æŸå¤±å®šä¹‰ä¸º
    L=âˆ’i=1âˆ‘Nâ€‹logâ€‹xâˆˆS(xiâ€‹)âˆ‘â€‹p(x)â€‹ã€‚
- en: SentencePiece
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SentencePiece
- en: 'All tokenization algorithms described so far have the same problem: It is assumed
    that the input text uses spaces to separate words. However, not all languages
    use spaces to separate words. One possible solution is to use language specific
    pre-tokenizers, *e.g.* [XLM](model_doc/xlm) uses a specific Chinese, Japanese,
    and Thai pre-tokenizer). To solve this problem more generally, [SentencePiece:
    A simple and language independent subword tokenizer and detokenizer for Neural
    Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats
    the input as a raw input stream, thus including the space in the set of characters
    to use. It then uses the BPE or unigram algorithm to construct the appropriate
    vocabulary.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰æè¿°çš„æ ‡è®°åŒ–ç®—æ³•éƒ½æœ‰åŒæ ·çš„é—®é¢˜ï¼šå‡è®¾è¾“å…¥æ–‡æœ¬ä½¿ç”¨ç©ºæ ¼æ¥åˆ†éš”å•è¯ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰è¯­è¨€éƒ½ä½¿ç”¨ç©ºæ ¼æ¥åˆ†éš”å•è¯ã€‚ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ç‰¹å®šè¯­è¨€çš„é¢„åˆ†è¯å™¨ï¼Œä¾‹å¦‚
    [XLM](model_doc/xlm) ä½¿ç”¨ç‰¹å®šçš„ä¸­æ–‡ã€æ—¥æ–‡å’Œæ³°æ–‡é¢„åˆ†è¯å™¨ã€‚ä¸ºäº†æ›´æ™®éåœ°è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ[SentencePiece: A simple and
    language independent subword tokenizer and detokenizer for Neural Text Processing
    (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) å°†è¾“å…¥è§†ä¸ºåŸå§‹è¾“å…¥æµï¼Œå› æ­¤åŒ…æ‹¬ç©ºæ ¼åœ¨è¦ä½¿ç”¨çš„å­—ç¬¦é›†ä¸­ã€‚ç„¶åä½¿ç”¨
    BPE æˆ– unigram ç®—æ³•æ„å»ºé€‚å½“çš„è¯æ±‡è¡¨ã€‚'
- en: The [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)
    uses SentencePiece for example, which is also why in the example earlier the `"â–"`
    character was included in the vocabulary. Decoding with SentencePiece is very
    easy since all tokens can just be concatenated and `"â–"` is replaced by a space.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)
    ä¾‹å¦‚ä½¿ç”¨äº† SentencePieceï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåœ¨å‰é¢çš„ä¾‹å­ä¸­åŒ…å«äº† `"â–"` å­—ç¬¦åœ¨è¯æ±‡è¡¨ä¸­ã€‚ä½¿ç”¨ SentencePiece è¿›è¡Œè§£ç éå¸¸å®¹æ˜“ï¼Œå› ä¸ºæ‰€æœ‰æ ‡è®°åªéœ€è¿æ¥åœ¨ä¸€èµ·ï¼Œè€Œ
    `"â–"` è¢«æ›¿æ¢ä¸ºä¸€ä¸ªç©ºæ ¼ã€‚'
- en: All transformers models in the library that use SentencePiece use it in combination
    with unigram. Examples of models using SentencePiece are [ALBERT](model_doc/albert),
    [XLNet](model_doc/xlnet), [Marian](model_doc/marian), and [T5](model_doc/t5).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åº“ä¸­æ‰€æœ‰ä½¿ç”¨ SentencePiece çš„å˜å‹å™¨æ¨¡å‹éƒ½ä¸ unigram ç»“åˆä½¿ç”¨ã€‚ä½¿ç”¨ SentencePiece çš„æ¨¡å‹ç¤ºä¾‹åŒ…æ‹¬ [ALBERT](model_doc/albert),
    [XLNet](model_doc/xlnet), [Marian](model_doc/marian) å’Œ [T5](model_doc/t5)ã€‚
