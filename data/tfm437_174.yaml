- en: OpenAI GPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI GPT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/openai-gpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/openai-gpt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/openai-gpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/openai-gpt)
- en: '[![Models](../Images/65c2999b59581465a08a2c29063c6d24.png)](https://huggingface.co/models?filter=openai-gpt)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/openai-gpt)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![æ¨¡å‹](../Images/65c2999b59581465a08a2c29063c6d24.png)](https://huggingface.co/models?filter=openai-gpt)
    [![ç©ºé—´](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/openai-gpt)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: OpenAI GPT model was proposed in [Improving Language Understanding by Generative
    Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
    by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. Itâ€™s a causal
    (unidirectional) transformer pre-trained using language modeling on a large corpus
    will long range dependencies, the Toronto Book Corpus.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPTæ¨¡å‹æ˜¯ç”±Alec Radfordã€Karthik Narasimhanã€Tim Salimanså’ŒIlya Sutskeveråœ¨[é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒæ”¹è¿›è¯­è¨€ç†è§£](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªä½¿ç”¨è¯­è¨€å»ºæ¨¡åœ¨å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„å› æœï¼ˆå•å‘ï¼‰å˜å‹å™¨ï¼Œå…·æœ‰é•¿è·ç¦»ä¾èµ–æ€§ï¼Œå¤šä¼¦å¤šå›¾ä¹¦è¯­æ–™åº“ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Natural language understanding comprises a wide range of diverse tasks such
    as textual entailment, question answering, semantic similarity assessment, and
    document classification. Although large unlabeled text corpora are abundant, labeled
    data for learning these specific tasks is scarce, making it challenging for discriminatively
    trained models to perform adequately. We demonstrate that large gains on these
    tasks can be realized by generative pretraining of a language model on a diverse
    corpus of unlabeled text, followed by discriminative fine-tuning on each specific
    task. In contrast to previous approaches, we make use of task-aware input transformations
    during fine-tuning to achieve effective transfer while requiring minimal changes
    to the model architecture. We demonstrate the effectiveness of our approach on
    a wide range of benchmarks for natural language understanding. Our general task-agnostic
    model outperforms discriminatively trained models that use architectures specifically
    crafted for each task, significantly improving upon the state of the art in 9
    out of the 12 tasks studied.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*è‡ªç„¶è¯­è¨€ç†è§£åŒ…æ‹¬å„ç§ä¸åŒçš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬è•´æ¶µã€é—®é¢˜å›ç­”ã€è¯­ä¹‰ç›¸ä¼¼æ€§è¯„ä¼°å’Œæ–‡æ¡£åˆ†ç±»ã€‚å°½ç®¡å¤§é‡æœªæ ‡è®°çš„æ–‡æœ¬è¯­æ–™åº“ä¸°å¯Œï¼Œä½†ç”¨äºå­¦ä¹ è¿™äº›ç‰¹å®šä»»åŠ¡çš„æ ‡è®°æ•°æ®ç¨€ç¼ºï¼Œè¿™ä½¿å¾—ç»è¿‡åŒºåˆ†è®­ç»ƒçš„æ¨¡å‹éš¾ä»¥è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡åœ¨å¤šæ ·åŒ–çš„æœªæ ‡è®°æ–‡æœ¬è¯­æ–™åº“ä¸Šå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œç”Ÿæˆé¢„è®­ç»ƒï¼Œç„¶ååœ¨æ¯ä¸ªç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡ŒåŒºåˆ†å¾®è°ƒï¼Œå¯ä»¥å®ç°è¿™äº›ä»»åŠ¡çš„å¤§å¹…æå‡ã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸åï¼Œæˆ‘ä»¬åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åˆ©ç”¨ä»»åŠ¡æ„ŸçŸ¥çš„è¾“å…¥è½¬æ¢ï¼Œå®ç°æœ‰æ•ˆçš„è¿ç§»ï¼ŒåŒæ—¶å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œæœ€å°çš„æ›´æ”¹ã€‚æˆ‘ä»¬åœ¨è‡ªç„¶è¯­è¨€ç†è§£çš„å¹¿æ³›åŸºå‡†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é€šç”¨ä»»åŠ¡ä¸å¯çŸ¥æ¨¡å‹åœ¨9ä¸ªç ”ç©¶çš„12ä¸ªä»»åŠ¡ä¸­ä¼˜äºä½¿ç”¨ä¸“é—¨ä¸ºæ¯ä¸ªä»»åŠ¡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„çš„ç»è¿‡åŒºåˆ†è®­ç»ƒçš„æ¨¡å‹ï¼Œæ˜¾è‘—æ”¹è¿›äº†æŠ€æœ¯æ°´å¹³ã€‚*'
- en: '[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp
    created and hosted by Hugging Face showcasing the generative capabilities of several
    models. GPT is one of them.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨Transformerå†™ä½œ](https://transformer.huggingface.co/doc/gpt)æ˜¯ç”±Hugging Faceåˆ›å»ºå’Œæ‰˜ç®¡çš„ç½‘ç»œåº”ç”¨ç¨‹åºï¼Œå±•ç¤ºäº†å‡ ç§æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚GPTæ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚'
- en: This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The
    original code can be found [here](https://github.com/openai/finetune-transformer-lm).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[thomwolf](https://huggingface.co/thomwolf)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/openai/finetune-transformer-lm)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: GPT is a model with absolute position embeddings so itâ€™s usually advised to
    pad the inputs on the right rather than the left.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPTæ˜¯ä¸€ä¸ªå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤é€šå¸¸å»ºè®®åœ¨å³ä¾§è€Œä¸æ˜¯å·¦ä¾§å¡«å……è¾“å…¥ã€‚
- en: GPT was trained with a causal language modeling (CLM) objective and is therefore
    powerful at predicting the next token in a sequence. Leveraging this feature allows
    GPT-2 to generate syntactically coherent text as it can be observed in the *run_generation.py*
    example script.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPTæ˜¯é€šè¿‡å› æœè¯­è¨€å»ºæ¨¡ï¼ˆCLMï¼‰ç›®æ ‡è¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤åœ¨é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ—¶éå¸¸å¼ºå¤§ã€‚åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ä½¿GPT-2èƒ½å¤Ÿç”Ÿæˆå¥æ³•è¿è´¯çš„æ–‡æœ¬ï¼Œå¯ä»¥åœ¨*run_generation.py*ç¤ºä¾‹è„šæœ¬ä¸­è§‚å¯Ÿåˆ°ã€‚
- en: 'Note:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼š
- en: 'If you want to reproduce the original tokenization process of the *OpenAI GPT*
    paper, you will need to install `ftfy` and `SpaCy`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³å¤åˆ¶*OpenAI GPT*è®ºæ–‡çš„åŸå§‹æ ‡è®°åŒ–è¿‡ç¨‹ï¼Œæ‚¨éœ€è¦å®‰è£…`ftfy`å’Œ`SpaCy`ï¼š
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you donâ€™t install `ftfy` and `SpaCy`, the [OpenAIGPTTokenizer](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer)
    will default to tokenize using BERTâ€™s `BasicTokenizer` followed by Byte-Pair Encoding
    (which should be fine for most usage, donâ€™t worry).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ²¡æœ‰å®‰è£…`ftfy`å’Œ`SpaCy`ï¼Œ[OpenAIGPTTokenizer](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer)å°†é»˜è®¤ä½¿ç”¨BERTçš„`BasicTokenizer`è¿›è¡Œæ ‡è®°åŒ–ï¼Œç„¶åæ˜¯å­—èŠ‚å¯¹ç¼–ç ï¼ˆå¯¹äºå¤§å¤šæ•°ç”¨é€”æ¥è¯´åº”è¯¥æ²¡é—®é¢˜ï¼Œä¸ç”¨æ‹…å¿ƒï¼‰ã€‚
- en: Resources
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with OpenAI GPT. If youâ€™re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and weâ€™ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨OpenAI GPTã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: Text Classification
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ†ç±»
- en: A blog post on [outperforming OpenAI GPT-3 with SetFit for text-classification](https://www.philschmid.de/getting-started-setfit).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äº[ä½¿ç”¨SetFitåœ¨æ–‡æœ¬åˆ†ç±»ä¸­èƒœè¿‡OpenAI GPT-3çš„åšå®¢æ–‡ç« ](https://www.philschmid.de/getting-started-setfit)ã€‚
- en: 'See also: [Text classification task guide](../tasks/sequence_classification)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)
- en: Text Generation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç”Ÿæˆ
- en: A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•[ä½¿ç”¨Hugging Faceå¯¹éè‹±è¯­GPT-2æ¨¡å‹è¿›è¡Œå¾®è°ƒ](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface)çš„åšå®¢ã€‚
- en: 'A blog on [How to generate text: using different decoding methods for language
    generation with Transformers](https://huggingface.co/blog/how-to-generate) with
    GPT-2.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨ GPT-2 è¿›è¡Œæ–‡æœ¬ç”Ÿæˆçš„åšå®¢ï¼š[ä½¿ç”¨ä¸åŒè§£ç æ–¹æ³•è¿›è¡Œè¯­è¨€ç”Ÿæˆä¸ Transformers](https://huggingface.co/blog/how-to-generate)ã€‚
- en: A blog on [Training CodeParrot ğŸ¦œ from Scratch](https://huggingface.co/blog/codeparrot),
    a large GPT-2 model.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äºä»å¤´å¼€å§‹è®­ç»ƒ [CodeParrot ğŸ¦œ çš„åšå®¢](https://huggingface.co/blog/codeparrot)ï¼Œä¸€ä¸ªå¤§å‹çš„
    GPT-2 æ¨¡å‹ã€‚
- en: A blog on [Faster Text Generation with TensorFlow and XLA](https://huggingface.co/blog/tf-xla-generate)
    with GPT-2.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨ GPT-2 è¿›è¡Œ [æ›´å¿«çš„æ–‡æœ¬ç”Ÿæˆä¸ TensorFlow å’Œ XLA](https://huggingface.co/blog/tf-xla-generate)
    çš„åšå®¢ã€‚
- en: A blog on [How to train a Language Model with Megatron-LM](https://huggingface.co/blog/megatron-training)
    with a GPT-2 model.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨ GPT-2 æ¨¡å‹ [è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸ Megatron-LM](https://huggingface.co/blog/megatron-training)
    çš„åšå®¢ã€‚
- en: A notebook on how to [finetune GPT2 to generate lyrics in the style of your
    favorite artist](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb).
    ğŸŒ
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äºå¦‚ä½• [å¾®è°ƒ GPT2 ä»¥ç”Ÿæˆæ‚¨æœ€å–œçˆ±è‰ºæœ¯å®¶é£æ ¼çš„æ­Œè¯](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)
    çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- en: A notebook on how to [finetune GPT2 to generate tweets in the style of your
    favorite Twitter user](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb).
    ğŸŒ
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äºå¦‚ä½• [å¾®è°ƒ GPT2 ä»¥ç”Ÿæˆæ‚¨æœ€å–œçˆ± Twitter ç”¨æˆ·é£æ ¼çš„æ¨æ–‡](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- en: '[Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch)
    chapter of the ğŸ¤— Hugging Face Course.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ¤— Hugging Face è¯¾ç¨‹çš„ [å› æœè¯­è¨€å»ºæ¨¡](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch)
    ç« èŠ‚ã€‚
- en: '[OpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel)
    is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling),
    [text generation example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel)
    åœ¨è¿™ä¸ª [å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling)ã€[æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py)
    å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
    ä¸­å¾—åˆ°æ”¯æŒã€‚'
- en: '[TFOpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel)
    is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFOpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel)
    åœ¨è¿™ä¸ª [å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy)
    å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)
    ä¸­å¾—åˆ°æ”¯æŒã€‚'
- en: 'See also: [Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/language_modeling)
- en: Token Classification
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åˆ†ç±»
- en: A course material on [Byte-Pair Encoding tokenization](https://huggingface.co/course/en/chapter6/5).
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰å…³ [å­—èŠ‚å¯¹ç¼–ç æ ‡è®°åŒ–](https://huggingface.co/course/en/chapter6/5) çš„è¯¾ç¨‹ææ–™ã€‚
- en: OpenAIGPTConfig
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIGPTConfig
- en: '### `class transformers.OpenAIGPTConfig`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OpenAIGPTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/configuration_openai.py#L27)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/configuration_openai.py#L27)'
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 40478) â€” Vocabulary size of the
    GPT-2 model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)
    or [TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 40478) â€” GPT-2 æ¨¡å‹çš„è¯æ±‡å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ [OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)
    æˆ– [TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel)
    æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`n_positions` (`int`, *optional*, defaults to 512) â€” The maximum sequence length
    that this model might ever be used with. Typically set this to something large
    just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_positions` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512ã€1024
    æˆ– 2048ï¼‰ã€‚'
- en: '`n_embd` (`int`, *optional*, defaults to 768) â€” Dimensionality of the embeddings
    and hidden states.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_embd` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 768) â€” åµŒå…¥å’Œéšè—çŠ¶æ€çš„ç»´åº¦ã€‚'
- en: '`n_layer` (`int`, *optional*, defaults to 12) â€” Number of hidden layers in
    the Transformer encoder.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚'
- en: '`n_head` (`int`, *optional*, defaults to 12) â€” Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`afn` (`str` or `Callable`, *optional*, defaults to `"gelu"`) â€” The non-linear
    activation function (function or string) in the encoder and pooler. If string,
    `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`afn` (`str` æˆ– `Callable`, *å¯é€‰*, é»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ
    `"gelu"`, `"relu"`, `"silu"` å’Œ `"gelu_new"`ã€‚'
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resid_pdrop` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” åµŒå…¥ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) â€” The dropout ratio for the
    embeddings.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embd_pdrop` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” åµŒå…¥çš„ä¸¢å¼ƒæ¯”ç‡ã€‚'
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) â€” The dropout ratio for
    the attention.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_pdrop` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” æ³¨æ„åŠ›çš„ä¸¢å¼ƒæ¯”ç‡ã€‚'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) â€” The epsilon
    to use in the layer normalization layers'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1e-05) â€” åœ¨å±‚å½’ä¸€åŒ–å±‚ä¸­ä½¿ç”¨çš„ epsilon'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`summary_type` (`str`, *optional*, defaults to `"cls_index"`) â€” Argument used
    when doing sequence summary, used in the models [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)
    and [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"cls_index"`) â€” åœ¨è¿›è¡Œåºåˆ—æ‘˜è¦æ—¶ä½¿ç”¨çš„å‚æ•°ï¼Œåœ¨æ¨¡å‹[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)å’Œ[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)ä¸­ä½¿ç”¨ã€‚'
- en: 'Has to be one of the following options:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¿…é¡»æ˜¯ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š
- en: '`"last"`: Take the last token hidden state (like XLNet).'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"last"`: è·å–æœ€åä¸€ä¸ªæ ‡è®°çš„éšè—çŠ¶æ€ï¼ˆç±»ä¼¼äºXLNetï¼‰ã€‚'
- en: '`"first"`: Take the first token hidden state (like BERT).'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"first"`: è·å–ç¬¬ä¸€ä¸ªæ ‡è®°çš„éšè—çŠ¶æ€ï¼ˆç±»ä¼¼äºBERTï¼‰ã€‚'
- en: '`"mean"`: Take the mean of all tokens hidden states.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mean"`: è·å–æ‰€æœ‰æ ‡è®°çš„éšè—çŠ¶æ€çš„å¹³å‡å€¼ã€‚'
- en: '`"cls_index"`: Supply a Tensor of classification token position (like GPT/GPT-2).'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cls_index"`: æä¾›ä¸€ä¸ªåˆ†ç±»æ ‡è®°ä½ç½®çš„å¼ é‡ï¼ˆç±»ä¼¼äºGPT/GPT-2ï¼‰ã€‚'
- en: '`"attn"`: Not implemented now, use multi-head attention.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"attn"`: ç›®å‰æœªå®ç°ï¼Œä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ã€‚'
- en: '`summary_use_proj` (`bool`, *optional*, defaults to `True`) â€” Argument used
    when doing sequence summary, used in the models [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)
    and [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_use_proj` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” åœ¨è¿›è¡Œåºåˆ—æ‘˜è¦æ—¶ä½¿ç”¨çš„å‚æ•°ï¼Œåœ¨æ¨¡å‹[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)å’Œ[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)ä¸­ä½¿ç”¨ã€‚'
- en: Whether or not to add a projection after the vector extraction.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ˜¯å¦åœ¨å‘é‡æå–åæ·»åŠ æŠ•å½±ã€‚
- en: '`summary_activation` (`str`, *optional*) â€” Argument used when doing sequence
    summary, used in the models [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)
    and [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_activation` (`str`, *å¯é€‰*) â€” åœ¨è¿›è¡Œåºåˆ—æ‘˜è¦æ—¶ä½¿ç”¨çš„å‚æ•°ï¼Œåœ¨æ¨¡å‹[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)å’Œ[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)ä¸­ä½¿ç”¨ã€‚'
- en: Pass `"tanh"` for a tanh activation to the output, any other value will result
    in no activation.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å°† `"tanh"` ä¼ é€’ç»™è¾“å‡ºä»¥è·å¾—åŒæ›²æ­£åˆ‡æ¿€æ´»ï¼Œå…¶ä»–ä»»ä½•å€¼éƒ½å°†å¯¼è‡´æ— æ¿€æ´»ã€‚
- en: '`summary_proj_to_labels` (`bool`, *optional*, defaults to `True`) â€” Argument
    used when doing sequence summary, used in the models [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)
    and [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_proj_to_labels` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” åœ¨è¿›è¡Œåºåˆ—æ‘˜è¦æ—¶ä½¿ç”¨çš„å‚æ•°ï¼Œåœ¨æ¨¡å‹[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)å’Œ[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)ä¸­ä½¿ç”¨ã€‚'
- en: Whether the projection outputs should have `config.num_labels` or `config.hidden_size`
    classes.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æŠ•å½±è¾“å‡ºåº”å…·æœ‰ `config.num_labels` æˆ– `config.hidden_size` ç±»ã€‚
- en: '`summary_first_dropout` (`float`, *optional*, defaults to 0.1) â€” Argument used
    when doing sequence summary, used in the models [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)
    and [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_first_dropout` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” åœ¨è¿›è¡Œåºåˆ—æ‘˜è¦æ—¶ä½¿ç”¨çš„å‚æ•°ï¼Œåœ¨æ¨¡å‹[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)å’Œ[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)ä¸­ä½¿ç”¨ã€‚'
- en: The dropout ratio to be used after the projection and activation.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æŠ•å½±å’Œæ¿€æ´»ä¹‹åä½¿ç”¨çš„è¾å­¦æ¯”ä¾‹ã€‚
- en: This is the configuration class to store the configuration of a [OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)
    or a [TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel).
    It is used to instantiate a GPT model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the GPT [openai-gpt](https://huggingface.co/openai-gpt)
    architecture from OpenAI.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)æˆ–[TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªGPTæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºOpenAIçš„GPT
    [openai-gpt](https://huggingface.co/openai-gpt)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Examples:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: OpenAIGPTTokenizer
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIGPTTokenizer
- en: '### `class transformers.OpenAIGPTTokenizer`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OpenAIGPTTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai.py#L245)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai.py#L245)'
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” Path to the vocabulary file.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`merges_file` (`str`) â€” Path to the merges file.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) â€” The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰- æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: 'Construct a GPT Tokenizer. Based on Byte-Pair-Encoding with the following peculiarities:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªGPTåˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚å¯¹ç¼–ç ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
- en: lowercases all inputs,
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥è½¬æ¢ä¸ºå°å†™ï¼Œ
- en: uses `SpaCy` tokenizer and `ftfy` for pre-BPE tokenization if they are installed,
    fallback to BERTâ€™s `BasicTokenizer` if not.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†`SpaCy`åˆ†è¯å™¨å’Œ`ftfy`ï¼Œåˆ™ä½¿ç”¨å®ƒä»¬è¿›è¡ŒBPEæ ‡è®°åŒ–ï¼Œå¦åˆ™å›é€€åˆ°BERTçš„`BasicTokenizer`ã€‚
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `save_vocabulary`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai.py#L378)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai.py#L378)'
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: OpenAIGPTTokenizerFast
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIGPTTokenizerFast
- en: '### `class transformers.OpenAIGPTTokenizerFast`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OpenAIGPTTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai_fast.py#L40)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai_fast.py#L40)'
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” Path to the vocabulary file.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`ï¼ˆ`str`ï¼‰- è¯æ±‡è¡¨æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`merges_file` (`str`) â€” Path to the merges file.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file`ï¼ˆ`str`ï¼‰- åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) â€” The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰- æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: 'Construct a â€œfastâ€ GPT Tokenizer (backed by HuggingFaceâ€™s *tokenizers* library).
    Based on Byte-Pair-Encoding with the following peculiarities:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªâ€œå¿«é€Ÿâ€GPTåˆ†è¯å™¨ï¼ˆç”±HuggingFaceçš„*tokenizers*åº“æ”¯æŒï¼‰ã€‚åŸºäºå­—èŠ‚å¯¹ç¼–ç ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
- en: lower case all inputs
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥è½¬æ¢ä¸ºå°å†™
- en: uses BERTâ€™s BasicTokenizer for pre-BPE tokenization
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨BERTçš„BasicTokenizerè¿›è¡ŒBPEæ ‡è®°åŒ–
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: OpenAI specific outputs
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIç‰¹å®šè¾“å‡º
- en: '### `class transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L297)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L297)'
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰- è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`mc_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mc_labels`
    is provided) â€” Multiple choice classification loss.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`mc_labels`æ—¶è¿”å›ï¼‰- å¤šé¡¹é€‰æ‹©åˆ†ç±»æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length,
    config.vocab_size)`) â€” Prediction scores of the language modeling head (scores
    for each vocabulary token before SoftMax).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_choices, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰-
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`mc_logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) â€” Prediction
    scores of the multiple choice classification head (scores for each choice before
    SoftMax).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_choices)`çš„`torch.FloatTensor`ï¼‰- å¤šé¡¹é€‰æ‹©åˆ†ç±»å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªé€‰æ‹©çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: Base class for outputs of models predicting if two sentences are consecutive
    or not.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºé¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­çš„æ¨¡å‹è¾“å‡ºçš„åŸºç±»ã€‚
- en: '### `class transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L413)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L413)'
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_choices, sequence_length,
    config.vocab_size)`) â€” Prediction scores of the language modeling head (scores
    for each vocabulary token before SoftMax).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_choices, sequence_length, config.vocab_size)`çš„`tf.Tensor`ï¼‰-
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`mc_logits` (`tf.Tensor` of shape `(batch_size, num_choices)`) â€” Prediction
    scores of the multiple choice classification head (scores for each choice before
    SoftMax).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_choices)`) â€” å¤šé¡¹é€‰æ‹©åˆ†ç±»å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax
    ä¹‹å‰æ¯ä¸ªé€‰æ‹©çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–
    `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`
    çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True`
    æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor`
    å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: Base class for outputs of models predicting if two sentences are consecutive
    or not.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¾“å‡ºçš„åŸºç±»ï¼Œç”¨äºé¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­ã€‚
- en: PytorchHide Pytorch content
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch å†…å®¹
- en: OpenAIGPTModel
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIGPTModel
- en: '### `class transformers.OpenAIGPTModel`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OpenAIGPTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L398)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L398)'
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare OpenAI GPT transformer model outputting raw hidden-states without any
    specific head on top.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„ OpenAI GPT å˜å‹å™¨æ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥äº†è§£ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L428)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L428)'
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢« `masked` çš„æ ‡è®°ä¸º 1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢« `masked` çš„æ ‡è®°ä¸º 0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 å¯¹åº”äºä¸€ä¸ª *å¥å­ A* æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äºä¸€ä¸ª *å¥å­ B* æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º `[0, config.max_position_embeddings - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*)
    â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºå…³è”å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))å’Œè¾“å…¥ã€‚'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¯å±‚è¾“å‡ºçš„æ¨¡å‹çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: OpenAIGPTLMHeadModel
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIGPTLMHeadModel
- en: '### `class transformers.OpenAIGPTLMHeadModel`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OpenAIGPTLMHeadModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L526)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L526)'
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: OpenAI GPT Model transformer with a language modeling head on top (linear layer
    with weights tied to the input embeddings).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPTæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ï¼ˆçº¿æ€§å±‚ï¼Œå…¶æƒé‡ä¸è¾“å…¥åµŒå…¥ç»‘å®šï¼‰ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L550)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L550)'
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—ä»¤ç‰Œçš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºé¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«`masked`çš„ä»¤ç‰Œï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«`masked`çš„ä»¤ç‰Œã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ®µä»¤ç‰Œç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*ä»¤ç‰Œï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*ä»¤ç‰Œã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—ä»¤ç‰Œåœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*)
    â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹å†…éƒ¨çš„æ ‡ç­¾**å·²ç»è¢«ç§»ä½**ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels
    = input_ids`ã€‚ç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—åœ¨`[0,
    ..., config.vocab_size]`ä¸­çš„æ ‡ç­¾ã€‚'
- en: Returns
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€”
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¯å±‚æ¨¡å‹çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [OpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE13]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: OpenAIGPTDoubleHeadsModel
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIGPTDoubleHeadsModel
- en: '### `class transformers.OpenAIGPTDoubleHeadsModel`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OpenAIGPTDoubleHeadsModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L615)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L615)'
- en: '[PRE14]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: OpenAI GPT Model transformer with a language modeling and a multiple-choice
    classification head on top e.g. for RocStories/SWAG tasks. The two heads are two
    linear layers. The language modeling head has its weights tied to the input embeddings,
    the classification head takes as input the input of a specified classification
    token index in the input sequence).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPTæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å’Œå¤šé€‰åˆ†ç±»å¤´ï¼Œä¾‹å¦‚ç”¨äºRocStories/SWAGä»»åŠ¡ã€‚è¿™ä¸¤ä¸ªå¤´æ˜¯ä¸¤ä¸ªçº¿æ€§å±‚ã€‚è¯­è¨€å»ºæ¨¡å¤´çš„æƒé‡ä¸è¾“å…¥åµŒå…¥ç»‘å®šï¼Œåˆ†ç±»å¤´ä»¥æŒ‡å®šçš„åˆ†ç±»æ ‡è®°ç´¢å¼•çš„è¾“å…¥ä½œä¸ºè¾“å…¥åºåˆ—çš„è¾“å…¥ï¼‰ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L644)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L644)'
- en: '[PRE15]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«æ©ç›–çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«æ©ç›–çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`mc_token_ids` (`torch.LongTensor` of shape `(batch_size, num_choices)`, *optional*,
    default to index of the last token of the input) â€” Index of the classification
    token in each input sequence. Selected in the range `[0, input_ids.size(-1) -
    1]`.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_token_ids` (`torch.LongTensor` of shape `(batch_size, num_choices)`, *optional*,
    é»˜è®¤ä¸ºè¾“å…¥çš„æœ€åä¸€ä¸ªæ ‡è®°çš„ç´¢å¼•) â€” æ¯ä¸ªè¾“å…¥åºåˆ—ä¸­åˆ†ç±»æ ‡è®°çš„ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, input_ids.size(-1) - 1]`ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-1, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” ç”¨äºè¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹å†…éƒ¨çš„æ ‡ç­¾**å·²ç»ç§»ä½**ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels = input_ids`ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[-1, 0, ...,
    config.vocab_size]`ä¸­ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆæ©ç›–ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æ ‡ç­¾ã€‚'
- en: '`mc_labels` (`torch.LongTensor` of shape `(batch_size)`, *optional*) â€” Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices]` where *num_choices* is the size of the second dimension of
    the input tensors. (see *input_ids* above)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_labels` (`torch.LongTensor` of shape `(batch_size)`, *optional*) â€” ç”¨äºè®¡ç®—å¤šé¡¹é€‰æ‹©åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., num_choices]`ä¸­ï¼Œå…¶ä¸­*num_choices*æ˜¯è¾“å…¥å¼ é‡ç¬¬äºŒç»´çš„å¤§å°ã€‚ï¼ˆå‚è§ä¸Šé¢çš„*input_ids*ï¼‰'
- en: Returns
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰å’Œè¾“å…¥ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`mc_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mc_labels`
    is provided) â€” Multiple choice classification loss.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`mc_labels`æ—¶è¿”å›ï¼‰â€” å¤šé¡¹é€‰æ‹©åˆ†ç±»æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length,
    config.vocab_size)`) â€” Prediction scores of the language modeling head (scores
    for each vocabulary token before SoftMax).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_choices, sequence_length,
    config.vocab_size)`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`mc_logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) â€” Prediction
    scores of the multiple choice classification head (scores for each choice before
    SoftMax).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_choices)`ï¼‰â€” å¤šé¡¹é€‰æ‹©åˆ†ç±»å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªé€‰æ‹©çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)
    forward method, overrides the `__call__` special method.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE16]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: OpenAIGPTForSequenceClassification
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAIGPTForSequenceClassification
- en: '### `class transformers.OpenAIGPTForSequenceClassification`'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OpenAIGPTForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L740)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L740)'
- en: '[PRE17]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The Original OpenAI GPT Model transformer with a sequence classification head
    on top (linear layer). [OpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do. Since it does classification on the last token, it requires to
    know the position of the last token. If a `pad_token_id` is defined in the configuration,
    it finds the last token that is not a padding token in each row. If no `pad_token_id`
    is defined, it simply takes the last value in each row of the batch. Since it
    cannot guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹çš„OpenAI GPTæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰åºåˆ—åˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰ã€‚[OpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification)ä½¿ç”¨æœ€åä¸€ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼Œå°±åƒå…¶ä»–å› æœæ¨¡å‹ï¼ˆä¾‹å¦‚GPT-2ï¼‰ä¸€æ ·ã€‚ç”±äºå®ƒåœ¨æœ€åä¸€ä¸ªæ ‡è®°ä¸Šè¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤éœ€è¦çŸ¥é“æœ€åä¸€ä¸ªæ ‡è®°çš„ä½ç½®ã€‚å¦‚æœåœ¨é…ç½®ä¸­å®šä¹‰äº†`pad_token_id`ï¼Œåˆ™æ‰¾åˆ°æ¯è¡Œä¸­ä¸æ˜¯å¡«å……æ ‡è®°çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚å¦‚æœæœªå®šä¹‰`pad_token_id`ï¼Œåˆ™ç®€å•åœ°å–æ‰¹æ¬¡ä¸­æ¯è¡Œçš„æœ€åä¸€ä¸ªå€¼ã€‚ç”±äºåœ¨ä¼ é€’`inputs_embeds`è€Œä¸æ˜¯`input_ids`æ—¶æ— æ³•çŒœæµ‹å¡«å……æ ‡è®°ï¼Œå› æ­¤å®ƒæ‰§è¡Œç›¸åŒæ“ä½œï¼ˆå–æ‰¹æ¬¡ä¸­æ¯è¡Œçš„æœ€åä¸€ä¸ªå€¼ï¼‰ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L762)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L762)'
- en: '[PRE18]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)æ¥è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºâ€œæœªè¢«æ©ç›–â€çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºâ€œè¢«æ©ç›–â€çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*æ ‡è®°ã€‚
- en: 1 corresponds to a *sentence B* token.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨â€œæœªè¢«æ©ç›–â€ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨â€œè¢«æ©ç›–â€ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`ä¸­ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`, *å¯é€‰çš„*, å½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [OpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example of single-label classification:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: å•æ ‡ç­¾åˆ†ç±»ç¤ºä¾‹ï¼š
- en: '[PRE19]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Example of multi-label classification:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ ‡ç­¾åˆ†ç±»ç¤ºä¾‹ï¼š
- en: '[PRE20]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TensorFlowHide TensorFlow content
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: TFOpenAIGPTModel
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFOpenAIGPTModel
- en: '### `class transformers.TFOpenAIGPTModel`'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFOpenAIGPTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L540)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L540)'
- en: '[PRE21]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•æ¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare OpenAI GPT transformer model outputting raw hidden-states without any
    specific head on top.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„OpenAI GPTå˜æ¢å™¨æ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§æ ¼å¼å¾—åˆ°æ”¯æŒçš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKerasæ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œåœ¨ä½¿ç”¨è¯¸å¦‚`model.fit()`ä¹‹ç±»çš„æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯¹æ‚¨â€œåªéœ€å·¥ä½œâ€
    - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Keras`Functional`
    APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªæœ‰ä¸€ä¸ªåŒ…å«`input_ids`çš„å¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L549)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L549)'
- en: '[PRE22]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy array`æˆ–`tf.Tensor`ï¼‰ -
    è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—ä»¤ç‰Œçš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)å’Œ[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`Numpy array`ï¼Œ*å¯é€‰*ï¼‰
    - ç”¨äºé¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«`masked`çš„ä»¤ç‰Œï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«`masked`çš„ä»¤ç‰Œã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`Numpy array`ï¼Œ*å¯é€‰*ï¼‰
    - æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*ä»¤ç‰Œï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*ä»¤ç‰Œã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`Numpy array`ï¼Œ*å¯é€‰*ï¼‰
    - æ¯ä¸ªè¾“å…¥åºåˆ—ä»¤ç‰Œåœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`tf.Tensor`æˆ–`Numpy
    array`ï¼Œ*å¯é€‰*ï¼‰ - ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨æ˜¯`masked`ã€‚
- en: '`inputs_embeds` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`æˆ–`Numpy
    array`ï¼Œ*å¯é€‰*ï¼‰ - å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ - æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨
    eager æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥åœ¨ eager æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `False`) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚ dropout æ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: Returns
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    æˆ– `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    æˆ–ä¸€ä¸ª `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ å…¥ `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    â€” Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`)
    â€” æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ å…¥ `output_hidden_states=True`
    æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `tf.Tensor` å…ƒç»„ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ å…¥ `output_attentions=True` æˆ–å½“ `config.output_attentions=True`
    æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor`
    å…ƒç»„ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE23]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: TFOpenAIGPTLMHeadModel
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFOpenAIGPTLMHeadModel
- en: '### `class transformers.TFOpenAIGPTLMHeadModel`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.TFOpenAIGPTLMHeadModel` ç±»'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L592)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L592)'
- en: '[PRE24]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: OpenAI GPT Model transformer with a language modeling head on top (linear layer
    with weights tied to the input embeddings).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„ OpenAI GPT æ¨¡å‹å˜æ¢å™¨ï¼ˆçº¿æ€§å±‚ï¼Œå…¶æƒé‡ä¸è¾“å…¥åµŒå…¥ç›¸å…³è”ï¼‰ã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§è¾“å…¥æ ¼å¼ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€
    - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Keras `Functional`
    APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥Tensorï¼š
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»…å…·æœ‰`input_ids`çš„å•ä¸ªTensorï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…·æœ‰ä¸åŒé•¿åº¦çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„è¾“å…¥Tensorï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥Tensorï¼š`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L612)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L612)'
- en: '[PRE25]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)å’Œ[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`Numpy`æ•°ç»„ï¼Œ*å¯é€‰*ï¼‰-
    é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤º`not masked`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤º`masked`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`Numpy`æ•°ç»„ï¼Œ*å¯é€‰*ï¼‰-
    æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*çš„æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*çš„æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆ`tf.Tensor`æˆ–å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„ï¼Œ*å¯é€‰*ï¼‰-
    æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®çš„ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`tf.Tensor`æˆ–`Numpy`æ•°ç»„ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æ˜¯`not masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨æ˜¯`masked`ã€‚
- en: '`inputs_embeds` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`æˆ–`Numpy
    array`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºå…³è”å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `False`) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸­ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the cross entropy classification loss. Indices should be
    in `[0, ..., config.vocab_size - 1]`.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—äº¤å‰ç†µåˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.vocab_size - 1]`èŒƒå›´å†…ã€‚'
- en: Returns
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)æˆ–`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼Œåˆ™æ ¹æ®é…ç½®ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰å’Œè¾“å…¥åŒ…å«å„ç§å…ƒç´ ã€‚
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) â€” Language modeling loss (for next-token
    prediction).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(n,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼Œå…¶ä¸­næ˜¯æœªå±è”½æ ‡ç­¾çš„æ•°é‡ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`tf.Tensor`ï¼‰â€”
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFOpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFOpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE26]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: TFOpenAIGPTDoubleHeadsModel
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFOpenAIGPTDoubleHeadsModel
- en: '### `class transformers.TFOpenAIGPTDoubleHeadsModel`'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFOpenAIGPTDoubleHeadsModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L685)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L685)'
- en: '[PRE27]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: OpenAI GPT Model transformer with a language modeling and a multiple-choice
    classification head on top e.g. for RocStories/SWAG tasks. The two heads are two
    linear layers. The language modeling head has its weights tied to the input embeddings,
    the classification head takes as input the input of a specified classification
    token index in the input sequence).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPTæ¨¡å‹å˜å‹å™¨ï¼Œå…·æœ‰è¯­è¨€å»ºæ¨¡å’Œé¡¶éƒ¨çš„å¤šé€‰åˆ†ç±»å¤´ï¼Œä¾‹å¦‚ç”¨äºRocStories/SWAGä»»åŠ¡ã€‚è¿™ä¸¤ä¸ªå¤´æ˜¯ä¸¤ä¸ªçº¿æ€§å±‚ã€‚è¯­è¨€å»ºæ¨¡å¤´çš„æƒé‡ä¸è¾“å…¥åµŒå…¥ç»‘å®šï¼Œåˆ†ç±»å¤´ä»¥è¾“å…¥åºåˆ—ä¸­æŒ‡å®šåˆ†ç±»æ ‡è®°ç´¢å¼•çš„è¾“å…¥ä¸ºè¾“å…¥ï¼‰ã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKerasæ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥â€œåªéœ€å·¥ä½œâ€
    - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨Keras `Functional`
    APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä»…åŒ…å«`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦ä¸åŒçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L703)'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L703)'
- en: '[PRE28]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy array`æˆ–`tf.Tensor`ï¼‰â€”
    è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)å’Œ[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆ`tf.Tensor`æˆ–å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy array`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`tf.Tensor`æˆ–`Numpy array`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 å¯¹åº”äº *sentence A* æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äº *sentence B* æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯token type IDsï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`tf.Tensor`æˆ–`Numpy array`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯position IDsï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`tf.Tensor`æˆ–`Numpy array`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*optional*)
    â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`inputs_embeds` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor`æˆ–`Numpy array`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å¾ˆæœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `False`) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: '`mc_token_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, num_choices)`,
    *optional*, default to index of the last token of the input) â€” Index of the classification
    token in each input sequence. Selected in the range `[0, input_ids.size(-1) -
    1]`.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_token_ids` (`tf.Tensor`æˆ–`Numpy array`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_choices)`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸ºè¾“å…¥åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°çš„ç´¢å¼•)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—ä¸­åˆ†ç±»æ ‡è®°çš„ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, input_ids.size(-1) - 1]`ä¸­é€‰æ‹©ã€‚'
- en: Returns
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput)
    æˆ– `tuple(tf.Tensor)`'
- en: A [transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_choices, sequence_length,
    config.vocab_size)`) â€” Prediction scores of the language modeling head (scores
    for each vocabulary token before SoftMax).'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_choices, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`mc_logits` (`tf.Tensor` of shape `(batch_size, num_choices)`) â€” Prediction
    scores of the multiple choice classification head (scores for each choice before
    SoftMax).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mc_logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_choices)`) â€” å¤šé¡¹é€‰æ‹©åˆ†ç±»å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªé€‰æ‹©çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFOpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTDoubleHeadsModel)
    forward method, overrides the `__call__` special method.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFOpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTDoubleHeadsModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE29]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: TFOpenAIGPTForSequenceClassification
  id: totrans-482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFOpenAIGPTForSequenceClassification
- en: '### `class transformers.TFOpenAIGPTForSequenceClassification`'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFOpenAIGPTForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L817)'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L817)'
- en: '[PRE30]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰-
    åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The OpenAI GPT Model transformer with a sequence classification head on top
    (linear layer).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¡¶éƒ¨å¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„OpenAI GPTæ¨¡å‹å˜å‹å™¨ï¼ˆçº¿æ€§å±‚ï¼‰ã€‚
- en: '[TFOpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFOpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification)ä½¿ç”¨æœ€åä¸€ä¸ªä»¤ç‰Œæ¥è¿›è¡Œåˆ†ç±»ï¼Œå°±åƒå…¶ä»–å› æœæ¨¡å‹ï¼ˆä¾‹å¦‚GPT-2ï¼‰ä¸€æ ·ã€‚'
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå®ƒå¯¹æœ€åä¸€ä¸ªä»¤ç‰Œè¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤éœ€è¦çŸ¥é“æœ€åä¸€ä¸ªä»¤ç‰Œçš„ä½ç½®ã€‚å¦‚æœåœ¨é…ç½®ä¸­å®šä¹‰äº†`pad_token_id`ï¼Œåˆ™å®ƒä¼šæ‰¾åˆ°æ¯è¡Œä¸­ä¸æ˜¯å¡«å……ä»¤ç‰Œçš„æœ€åä¸€ä¸ªä»¤ç‰Œã€‚å¦‚æœæ²¡æœ‰å®šä¹‰`pad_token_id`ï¼Œåˆ™å®ƒä¼šç®€å•åœ°å–æ¯è¡Œæ‰¹æ¬¡ä¸­çš„æœ€åä¸€ä¸ªå€¼ã€‚ç”±äºåœ¨ä¼ é€’`inputs_embeds`è€Œä¸æ˜¯`input_ids`æ—¶æ— æ³•çŒœæµ‹å¡«å……ä»¤ç‰Œï¼Œå› æ­¤å®ƒä¼šæ‰§è¡Œç›¸åŒçš„æ“ä½œï¼ˆå–æ¯è¡Œæ‰¹æ¬¡ä¸­çš„æœ€åä¸€ä¸ªå€¼ï¼‰ã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKerasæ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œæ‚¨åº”è¯¥å¯ä»¥â€œåªéœ€ä¼ é€’â€æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ï¼Œä»¥ä»»ä½•`model.fit()`æ”¯æŒçš„æ ¼å¼ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨Keras`Functional`APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªæœ‰`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])` æˆ–
    `model([input_ids, attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨ [å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›å†…å®¹ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L845)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L845)'
- en: '[PRE31]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`)
    â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    å’Œ [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`tf.Tensor` æˆ– `Numpy array`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]`ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºæœªè¢« `masked` çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºè¢« `masked` çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`tf.Tensor` æˆ– `Numpy array`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰å®šåœ¨ `[0, 1]`ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-513
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-514
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`tf.Tensor` æˆ– `Numpy array`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰å®šèŒƒå›´ä¸º `[0, config.max_position_embeddings - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`tf.Tensor` æˆ– `Numpy array`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers,
    num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]`ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-519
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-520
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚
- en: '`inputs_embeds` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor` æˆ– `Numpy array`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°† `input_ids`
    ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„ `attentions`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸­å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„ `hidden_states`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸­å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸­è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `False`) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—ï¼Œå¦‚ dropout æ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the cross entropy classification loss. Indices should be
    in `[0, ..., config.vocab_size - 1]`.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—äº¤å‰ç†µåˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.vocab_size - 1]`èŒƒå›´å†…ã€‚'
- en: Returns
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFSequenceClassifierOutput` æˆ– `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))
    and inputs.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, )`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºçš„éšè—çŠ¶æ€ã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFOpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFOpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE32]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
