- en: CodeLlama
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CodeLlama
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/code_llama](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/code_llama)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/code_llama](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/code_llama)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Code Llama model was proposed in [Code Llama: Open Foundation Models for
    Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)
    by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing
    Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov,
    Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori,
    Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis
    Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Code Llama模型是由Baptiste Rozière，Jonas Gehring，Fabian Gloeckle，Sten Sootla，Itai
    Gat，Xiaoqing Ellen Tan，Yossi Adi，Jingyu Liu，Tal Remez，Jérémy Rapin，Artyom Kozhevnikov，Ivan
    Evtimov，Joanna Bitton，Manish Bhatt，Cristian Canton Ferrer，Aaron Grattafiori，Wenhan
    Xiong，Alexandre Défossez，Jade Copet，Faisal Azhar，Hugo Touvron，Louis Martin，Nicolas
    Usunier，Thomas Scialom，Gabriel Synnaeve提出的，详见[Code Llama: Open Foundation Models
    for Code](https://ai.meta.com/research/publications/code-llama/)。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We release Code Llama, a family of large language models for code based on
    Llama 2 providing state-of-the-art performance among open models, infilling capabilities,
    support for large input contexts, and zero-shot instruction following ability
    for programming tasks. We provide multiple flavors to cover a wide range of applications:
    foundation models (Code Llama), Python specializations (Code Llama - Python),
    and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B
    parameters each. All models are trained on sequences of 16k tokens and show improvements
    on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct
    variants support infilling based on surrounding content. Code Llama reaches state-of-the-art
    performance among open models on several code benchmarks, with scores of up to
    53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
    7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
    every other publicly available model on MultiPL-E. We release Code Llama under
    a permissive license that allows for both research and commercial use.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们发布了Code Llama，这是基于Llama 2的一系列大型代码语言模型，提供了在开放模型中最先进的性能，填充能力，支持大型输入上下文以及编程任务的零-shot指令跟随能力。我们提供多种风格以涵盖广泛的应用：基础模型（Code
    Llama），Python专业化模型（Code Llama - Python）和指令跟随模型（Code Llama - Instruct），分别具有7B，13B和34B参数。所有模型都是在16k标记序列上训练的，并在最多100k标记的输入上显示出改进。7B和13B的Code
    Llama和Code Llama - Instruct变体支持基于周围内容的填充。Code Llama在几个代码基准测试中达到了开放模型的最先进性能，分别在HumanEval和MBPP上达到了53%和55%的分数。值得注意的是，Code
    Llama - Python 7B在HumanEval和MBPP上优于Llama 2 70B，而我们所有的模型都优于MultiPL-E上的其他任何公开可用模型。我们以一种允许研究和商业使用的宽松许可证发布了Code
    Llama。*'
- en: Check out all Code Llama model checkpoints [here](https://huggingface.co/models?search=code_llama)
    and the officially released ones in the [codellama org](https://huggingface.co/codellama).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 查看所有Code Llama模型检查点[这里](https://huggingface.co/models?search=code_llama)，以及在[codellama
    org](https://huggingface.co/codellama)上正式发布的模型。
- en: This model was contributed by [ArthurZucker](https://huggingface.co/ArthurZ).
    The original code of the authors can be found [here](https://github.com/facebookresearch/llama).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[ArthurZucker](https://huggingface.co/ArthurZ)贡献。作者的原始代码可以在[这里](https://github.com/facebookresearch/llama)找到。
- en: Usage tips and examples
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法提示和示例
- en: 'The `Llama2` family models, on which Code Llama is based, were trained using
    `bfloat16`, but the original inference uses `float16`. Let’s look at the different
    precisions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama基于的`Llama2`系列模型是使用`bfloat16`训练的，但原始推断使用`float16`。让我们看看不同的精度：
- en: '`float32`: PyTorch convention on model initialization is to load models in
    `float32`, no matter with which `dtype` the model weights were stored. `transformers`
    also follows this convention for consistency with PyTorch. This will be picked
    by default. If you want the `AutoModel` API to cast the load the checkpoints with
    the storage weights type, you must specify `torch_dtype="auto"`, e.g. `model =
    AutoModelForCausalLM.from_pretrained("path", torch_dtype = "auto")`.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float32`：PyTorch在模型初始化时的惯例是以`float32`加载模型，无论模型权重存储时使用的是哪种`dtype`。`transformers`也遵循这种惯例，以保持与PyTorch的一致性。这将被默认选择。如果您希望`AutoModel`
    API将加载检查点时的存储权重类型转换为`dtype`，则必须指定`torch_dtype="auto"`，例如`model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`。'
- en: '`bfloat16`: Code Llama was trained with this precision, so we recommend using
    it for further training or fine-tuning.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bfloat16`：Code Llama是使用这种精度训练的，因此我们建议在进一步训练或微调时使用它。'
- en: '`float16`: We recommend running inference using this precision, as it’s usually
    faster than `bfloat16`, and evaluation metrics show no discernible degradation
    with respect to `bfloat16`. You can also run inference using `bfloat16`, and we
    recommend you check inference results with both `float16` and `bfloat16` after
    fine-tuning.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float16`：我们建议使用这种精度来运行推断，因为通常比`bfloat16`更快，并且评估指标显示与`bfloat16`相比没有明显的降级。您也可以使用`bfloat16`来运行推断，我们建议您在微调后使用`float16`和`bfloat16`检查推断结果。'
- en: As mentioned above, the `dtype` of the storage weights is mostly irrelevant
    unless you are using `torch_dtype="auto"` when initializing a model using. The
    reason is that the model will first be downloaded (using the `dtype` of the checkpoints
    online) and then will be casted to the default `dtype` of `torch` (becomes `torch.float32`).
    If there is a specified `torch_dtype`, it will be used instead.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，存储权重的`dtype`大多数情况下并不重要，除非您在初始化模型时使用`torch_dtype="auto"`。原因是模型将首先被下载（使用在线检查点的`dtype`），然后将被转换为`torch`的默认`dtype`（变为`torch.float32`）。如果有指定的`torch_dtype`，则将使用该指定值。
- en: 'Tips:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: The infilling task is supported out of the box. You should be using the `tokenizer.fill_token`
    where you want your input to be filled.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充任务是开箱即用的。您应该在想要填充输入的地方使用`tokenizer.fill_token`。
- en: 'The model conversion script is the same as for the `Llama2` family:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型转换脚本与`Llama2`系列相同：
- en: 'Here is a sample usage:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例用法：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that executing the script requires enough CPU RAM to host the whole model
    in float16 precision (even if the biggest versions come in several checkpoints
    they each contain a part of each weight of the model, so we need to load them
    all in RAM).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，执行脚本需要足够的CPU RAM来承载整个模型的float16精度（即使最大版本分为几个检查点，每个检查点都包含模型的每个权重的一部分，因此我们需要将它们全部加载到RAM中）。
- en: 'After conversion, the model and tokenizer can be loaded via:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后，可以通过以下方式加载模型和分词器：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you only want the infilled part:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只想要填充部分：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Under the hood, the tokenizer [automatically splits by `<FILL_ME>`](https://huggingface.co/docs/transformers/main/model_doc/code_llama#transformers.CodeLlamaTokenizer.fill_token)
    to create a formatted input string that follows [the original training pattern](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402).
    This is more robust than preparing the pattern yourself: it avoids pitfalls, such
    as token glueing, that are very hard to debug. To see how much CPU and GPU memory
    you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)
    which can help determine that value.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，分词器[自动通过`<FILL_ME>`进行拆分](https://huggingface.co/docs/transformers/main/model_doc/code_llama#transformers.CodeLlamaTokenizer.fill_token)以创建一个格式化的输入字符串，遵循[原始训练模式](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402)。这比自己准备模式更加健壮：它避免了很难调试的诸如标记粘合等问题。要查看此模型或其他模型所需的CPU和GPU内存量，请尝试使用[此计算器](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)，该计算器可以帮助确定该值。
- en: The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece).
    One quirk of sentencepiece is that when decoding a sequence, if the first token
    is the start of the word (e.g. “Banana”), the tokenizer does not prepend the prefix
    space to the string.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA分词器是基于[sentencepiece](https://github.com/google/sentencepiece)的BPE模型。SentencePiece的一个特点是，在解码序列时，如果第一个标记是单词的开头（例如“Banana”），分词器不会在字符串前面添加前缀空格。
- en: Code Llama has the same architecture as the `Llama2` models, refer to [Llama2’s
    documentation page](llama2) for the API reference. Find Code Llama tokenizer reference
    below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama与`Llama2`模型具有相同的架构，请参考[Llama2的文档页面](llama2)获取API参考。以下是Code Llama分词器的参考。
- en: CodeLlamaTokenizer
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CodeLlamaTokenizer
- en: '### `class transformers.CodeLlamaTokenizer`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CodeLlamaTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L59)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L59)'
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`（`str`）— 词汇表文件的路径。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可用作序列分类器标记。'
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*，默认为`"</s>"`）— 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在构建使用特殊标记的序列时，这不是用于表示序列结束的标记。使用的标记是`sep_token`。
- en: '`prefix_token` (`str`, *optional*, defaults to `"▁<PRE>"`) — Prefix token used
    for infilling.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_token`（`str`，*可选*，默认为`"▁<PRE>"`）— 用于填充的前缀标记。'
- en: '`middle_token` (`str`, *optional*, defaults to `"▁<MID>"`) — Middle token used
    for infilling.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`middle_token`（`str`，*可选*，默认为`"▁<MID>"`）— 用于填充的中间标记。'
- en: '`suffix_token` (`str`, *optional*, defaults to `"▁<SUF>"`) — Suffix token used
    for infilling.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffix_token`（`str`，*可选*，默认为`"▁<SUF>"`）— 用于填充的后缀标记。'
- en: '`eot_token` (`str`, *optional*, defaults to `"▁<EOT>"`) — End of text token
    used for infilling.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eot_token`（`str`，*可选*，默认为`"▁<EOT>"`）— 用于填充的文本结束标记。'
- en: '`fill_token` (`str`, *optional*, defaults to `"<FILL_ME>"`) — The token used
    to split the input between the prefix and suffix.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill_token`（`str`，*可选*，默认为`"<FILL_ME>"`）— 用于在前缀和后缀之间拆分输入的标记。'
- en: '`suffix_first` (`bool`, *optional*, defaults to `False`) — Whether the input
    prompt and suffix should be formatted with the suffix first.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffix_first`（`bool`，*可选*，默认为`False`）— 输入提示和后缀是否应该首先格式化后缀。'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs`（`dict`，*可选*）— 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`：启用子词规范化。'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`：unigram的抽样参数。对于BPE-Dropout无效。'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`：不执行抽样。'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`：从nbest_size结果中进行抽样。'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`：假设nbest_size是无限的，并使用前向过滤和后向抽样算法从所有假设（格子）中进行抽样。'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：unigram抽样的平滑参数，以及BPE-dropout合并操作的丢弃概率。'
- en: '`add_bos_token` (`bool`, *optional*, defaults to `True`) — Whether to add a
    beginning of sequence token at the start of sequences.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_bos_token`（`bool`，*可选*，默认为`True`）— 是否在序列开头添加一个序列开始标记。'
- en: '`add_eos_token` (`bool`, *optional*, defaults to `False`) — Whether to add
    an end of sequence token at the end of sequences.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_eos_token`（`bool`，*可选*，默认为`False`）— 是否在序列末尾添加一个序列结束标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the tokenization spaces.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*，默认为`False`）— 是否清除分词空格。'
- en: '`additional_special_tokens` (`List[str]`, *optional*) — Additional special
    tokens used by the tokenizer.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens`（`List[str]`，*可选*）— 分词器使用的额外特殊标记。'
- en: '`use_default_system_prompt` (`bool`, *optional*, defaults to `False`) — Whether
    or not the default system prompt for Llama should be used.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_default_system_prompt`（`bool`，*可选*，默认为`False`）— 是否应使用Llama的默认系统提示。'
- en: Construct a CodeLlama tokenizer. Based on byte-level Byte-Pair-Encoding. The
    default padding token is unset as there is no padding token in the original model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个CodeLlama分词器。基于字节级字节对编码。默认填充标记未设置，因为原始模型中没有填充标记。
- en: The default configuration match that of [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)
    which supports prompt infilling.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 默认配置与[sentencepiece/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)匹配，支持提示填充。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L369)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L369)'
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `get_special_tokens_mask`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L381)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L381)'
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经使用特殊标记格式化为模型。'
- en: Returns
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用分词器的`prepare_for_model`方法添加特殊标记时调用此方法。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L419)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L419)'
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of ids.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。
- en: Creates a mask from the two sequences passed to be used in a sequence-pair classification
    task. An ALBERT
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。一个ALBERT
- en: 'sequence pair mask has the following format:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 序列对掩码具有以下格式：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: if token_ids_1 is None, only returns the first portion of the mask (0s).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`token_ids_1`为`None`，则仅返回掩码的第一部分（0s）。
- en: '#### `save_vocabulary`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L341)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama.py#L341)'
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str`) — The directory in which to save the vocabulary.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`（`str`）— 保存词汇表的目录。'
- en: Returns
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Tuple(str)`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuple(str)`'
- en: Paths to the files saved.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的文件路径。
- en: Save the vocabulary and special tokens file to a directory.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将词汇表和特殊标记文件保存到目录中。
- en: CodeLlamaTokenizerFast
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CodeLlamaTokenizerFast
- en: '### `class transformers.CodeLlamaTokenizerFast`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CodeLlamaTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L52)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L52)'
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`, *optional*) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .model extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`（`str`，*可选*）— [SentencePiece](https://github.com/google/sentencepiece)文件（通常具有`.model`扩展名），其中包含实例化分词器所需的词汇表。'
- en: '`tokenizer_file` (`str`, *optional*) — [tokenizers](https://github.com/huggingface/tokenizers)
    file (generally has a .json extension) that contains everything needed to load
    the tokenizer.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file`（`str`，*可选*）— [tokenizers](https://github.com/huggingface/tokenizers)文件（通常具有`.json`扩展名），其中包含加载分词器所需的所有内容。'
- en: '`clean_up_tokenization_spaces` (`str`, *optional*, defaults to `False`) — Wether
    to cleanup spaces after decoding, cleanup consists in removing potential artifacts
    like extra spaces.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`str`，*可选*，默认为`False`）— 解码后是否清除空格，清除包括删除额外的空格等潜在工件。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: '`prefix_token` (`str`, *optional*, defaults to `"▁<PRE>"`) — Prefix token used
    for infilling.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_token` (`str`, *optional*, defaults to `"▁<PRE>"`) — 用于填充的前缀标记。'
- en: '`middle_token` (`str`, *optional*, defaults to `"▁<MID>"`) — Middle token used
    for infilling.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`middle_token` (`str`, *optional*, defaults to `"▁<MID>"`) — 用于填充的中间标记。'
- en: '`suffix_token` (`str`, *optional*, defaults to `"▁<SUF>"`) — Suffix token used
    for infilling.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffix_token` (`str`, *optional*, defaults to `"▁<SUF>"`) — 用于填充的后缀标记。'
- en: '`eot_token` (`str`, *optional*, defaults to `"▁<EOT>"`) — End of text token
    used for infilling.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eot_token` (`str`, *optional*, defaults to `"▁<EOT>"`) — 用于填充的文本结束标记。'
- en: '`fill_token` (`str`, *optional*, defaults to `"<FILL_ME>"`) — The token used
    to split the input between the prefix and suffix.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill_token` (`str`, *optional*, defaults to `"<FILL_ME>"`) — 用于在前缀和后缀之间分割输入的标记。'
- en: '`additional_special_tokens` (`List[str]`, *optional*) — Additional special
    tokens used by the tokenizer.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`, *optional*) — 分词器使用的额外特殊标记。'
- en: '`add_bos_token` (`bool`, *optional*, defaults to `True`) — Whether to add a
    beginning of sequence token at the start of sequences.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_bos_token` (`bool`, *optional*, defaults to `True`) — 是否在序列开头添加一个起始标记。'
- en: '`add_eos_token` (`bool`, *optional*, defaults to `False`) — Whether to add
    an end of sequence token at the end of sequences.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_eos_token` (`bool`, *optional*, defaults to `False`) — 是否在序列末尾添加一个序列结束标记。'
- en: '`use_default_system_prompt` (`bool`, *optional*, defaults to `False`) — Whether
    or not the default system prompt for Llama should be used.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_default_system_prompt` (`bool`, *optional*, defaults to `False`) — 是否使用
    Llama 的默认系统提示。'
- en: Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 Llama 分词器。基于字节级字节对编码。
- en: This uses notably ByteFallback and no normalization.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里特别使用了 ByteFallback 和无规范化。
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you want to change the `bos_token` or the `eos_token`, make sure to specify
    them when initializing the model, or call `tokenizer.update_post_processor()`
    to make sure that the post-processing is correctly done (otherwise the values
    of the first token and final token of an encoded sequence will not be correct).
    For more details, checkout [post-processors] ([https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors))
    documentation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要更改 `bos_token` 或 `eos_token`，请确保在初始化模型时指定它们，或调用 `tokenizer.update_post_processor()`
    确保后处理正确完成（否则编码序列的第一个标记和最后一个标记的值将不正确）。有关更多详细信息，请查看 [post-processors] ([https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors))
    文档。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods. The default configuration match
    that of [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)
    which supports prompt infilling.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。默认配置与
    [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)
    匹配，支持提示填充。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L413)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L413)'
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 第二个序列的可选 ID 列表。'
- en: Returns
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊标记的 [输入 ID](../glossary#input-ids) 列表。
- en: Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. The special tokens depend on
    calling set_lang.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，为序列分类任务构建模型输入的序列或序列对。特殊标记取决于调用 set_lang。
- en: 'An NLLB sequence has the following format, where `X` represents the sequence:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: NLLB 序列具有以下格式，其中 `X` 表示序列：
- en: '`input_ids` (for encoder) `X [eos, src_lang_code]`'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（用于编码器）`X [eos, src_lang_code]`'
- en: '`decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`: (用于解码器) `X [eos, tgt_lang_code]`'
- en: BOS is never used. Pairs of sequences are not the expected use case, but they
    will be handled without a separator.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: BOS 从不使用。序列对不是预期的用例，但它们将在没有分隔符的情况下处理。
- en: '#### `get_special_tokens_mask`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)'
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of ids of the first sequence.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 第一个序列的 ID 列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — List of ids of the second sequence.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 第二个序列的 ID 列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — 标记列表是否已经使用模型的特殊标记格式化。'
- en: Returns
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list of integers in the range [0, 1]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在范围[0, 1]内的整数列表
- en: 1 for a special token, 0 for a sequence token.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 1 表示特殊标记，0 表示序列标记。
- en: Retrieves sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    or `encode_plus` methods.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列 ID。当使用分词器的 `prepare_for_model` 或 `encode_plus` 方法添加特殊标记时，将调用此方法。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — The first tokenized sequence.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 第一个标记化序列。'
- en: '`token_ids_1` (`List[int]`, *optional*) — The second tokenized sequence.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 第二个标记化序列。'
- en: Returns
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: The token type ids.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 标记类型ID。
- en: Create the token type IDs corresponding to the sequences passed. [What are token
    type IDs?](../glossary#token-type-ids)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 创建与传递的序列相对应的标记类型ID。[什么是标记类型ID？](../glossary#token-type-ids)
- en: Should be overridden in a subclass if the model has a special way of building
    those.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型有一种特殊的构建方式，则应该在子类中重写这个方法。
- en: '#### `update_post_processor`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update_post_processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L179)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L179)'
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Updates the underlying post processor with the current `bos_token` and `eos_token`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用当前的`bos_token`和`eos_token`更新底层后处理器。
- en: '#### `save_vocabulary`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L333)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/code_llama/tokenization_code_llama_fast.py#L333)'
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
