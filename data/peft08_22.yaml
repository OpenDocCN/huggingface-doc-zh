- en: Fully Sharded Data Parallel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ
- en: 'Original text: [https://huggingface.co/docs/peft/accelerate/fsdp](https://huggingface.co/docs/peft/accelerate/fsdp)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/peft/accelerate/fsdp](https://huggingface.co/docs/peft/accelerate/fsdp)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP)
    is developed for distributed training of large pretrained models up to 1T parameters.
    FSDP achieves this by sharding the model parameters, gradients, and optimizer
    states across data parallel processes and it can also offload sharded model parameters
    to a CPU. The memory efficiency afforded by FSDP allows you to scale training
    to larger batch or model sizes.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ](https://pytorch.org/docs/stable/fsdp.html)ï¼ˆFSDPï¼‰æ˜¯ä¸ºäº†åˆ†å¸ƒå¼è®­ç»ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ€å¤šå¯è¾¾1Tå‚æ•°ã€‚FSDPé€šè¿‡åœ¨æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¹‹é—´åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿˜å¯ä»¥å°†åˆ†ç‰‡æ¨¡å‹å‚æ•°å¸è½½åˆ°CPUã€‚FSDPæä¾›çš„å†…å­˜æ•ˆç‡ä½¿æ‚¨å¯ä»¥å°†è®­ç»ƒæ‰©å±•åˆ°æ›´å¤§çš„æ‰¹æ¬¡æˆ–æ¨¡å‹å¤§å°ã€‚'
- en: Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with
    CPU offload actually consumes 1.65x more GPU memory during training. You can track
    this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any
    updates.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒFSDPå¹¶ä¸ä¼šå‡å°‘GPUå†…å­˜çš„ä½¿ç”¨é‡ï¼Œè€Œå…·æœ‰CPUå¸è½½çš„FSDPåœ¨è®­ç»ƒæœŸé—´å®é™…ä¸Šä¼šæ¶ˆè€—1.65å€çš„GPUå†…å­˜ã€‚æ‚¨å¯ä»¥è·Ÿè¸ªè¿™ä¸ªPyTorch [é—®é¢˜](https://github.com/pytorch/pytorch/issues/91165)
    è·å–ä»»ä½•æ›´æ–°ã€‚
- en: FSDP is supported in ğŸ¤— Accelerate, and you can use it with ğŸ¤— PEFT. This guide
    will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py).
    Youâ€™ll configure the script to train a large model for conditional generation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: FSDPåœ¨ğŸ¤— Accelerateä¸­å—åˆ°æ”¯æŒï¼Œæ‚¨å¯ä»¥ä¸ğŸ¤— PEFTä¸€èµ·ä½¿ç”¨ã€‚æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å­¦ä¹ å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„FSDP[è®­ç»ƒè„šæœ¬](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py)ã€‚æ‚¨å°†é…ç½®è„šæœ¬ä»¥è®­ç»ƒä¸€ä¸ªå¤§å‹æ¨¡å‹ç”¨äºæ¡ä»¶ç”Ÿæˆã€‚
- en: Configuration
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®
- en: Begin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp)
    with ğŸ¤— Accelerate. Use the `--config_file` flag to save the configuration file
    to a specific location, otherwise it is saved as a `default_config.yaml` file
    in the ğŸ¤— Accelerate cache.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥[åˆ›å»ºä¸€ä¸ªFSDPé…ç½®æ–‡ä»¶](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp)ä¸ğŸ¤—
    Accelerateä¸€èµ·ã€‚ä½¿ç”¨`--config_file`æ ‡å¿—å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç‰¹å®šä½ç½®ï¼Œå¦åˆ™å®ƒå°†ä¿å­˜ä¸ºğŸ¤— Accelerateç¼“å­˜ä¸­çš„`default_config.yaml`æ–‡ä»¶ã€‚
- en: The configuration file is used to set the default options when you launch the
    training script.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®æ–‡ä»¶ç”¨äºåœ¨å¯åŠ¨è®­ç»ƒè„šæœ¬æ—¶è®¾ç½®é»˜è®¤é€‰é¡¹ã€‚
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Youâ€™ll be asked a few questions about your setup, and configure the following
    arguments. For this example, make sure you fully shard the model parameters, gradients,
    optimizer states, leverage the CPU for offloading, and wrap model layers based
    on the Transformer layer class name.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†è¢«é—®åŠæœ‰å…³æ‚¨çš„è®¾ç½®çš„å‡ ä¸ªé—®é¢˜ï¼Œå¹¶é…ç½®ä»¥ä¸‹å‚æ•°ã€‚åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œè¯·ç¡®ä¿å®Œå…¨åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œåˆ©ç”¨CPUè¿›è¡Œå¸è½½ï¼Œå¹¶æ ¹æ®Transformerå±‚ç±»ååŒ…è£…æ¨¡å‹å±‚ã€‚
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example, your FSDP configuration file may look like the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæ‚¨çš„FSDPé…ç½®æ–‡ä»¶å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The important parts
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡è¦éƒ¨åˆ†
- en: Letâ€™s dig a bit deeper into the training script to understand how it works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥äº†è§£è®­ç»ƒè„šæœ¬çš„å·¥ä½œåŸç†ã€‚
- en: The [`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14)
    function begins with initializing an [Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    class which handles everything for distributed training, such as automatically
    detecting your training environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14)
    å‡½æ•°ä»åˆå§‹åŒ–ä¸€ä¸ª[Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ç±»å¼€å§‹ï¼Œè¯¥ç±»å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒçš„æ‰€æœ‰å†…å®¹ï¼Œä¾‹å¦‚è‡ªåŠ¨æ£€æµ‹æ‚¨çš„è®­ç»ƒç¯å¢ƒã€‚'
- en: ğŸ’¡ Feel free to change the model and dataset inside the `main` function. If your
    dataset format is different from the one in the script, you may also need to write
    your own preprocessing function.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ éšæ„æ›´æ”¹`main`å‡½æ•°ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†æ ¼å¼ä¸è„šæœ¬ä¸­çš„ä¸åŒï¼Œæ‚¨å¯èƒ½è¿˜éœ€è¦ç¼–å†™è‡ªå·±çš„é¢„å¤„ç†å‡½æ•°ã€‚
- en: The script also creates a configuration corresponding to the ğŸ¤— PEFT method youâ€™re
    using. For LoRA, youâ€™ll use [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    to specify the task type, and several other important parameters such as the dimension
    of the low-rank matrices, the matrices scaling factor, and the dropout probability
    of the LoRA layers. If you want to use a different ğŸ¤— PEFT method, replace `LoraConfig`
    with the appropriate [class](../package_reference/tuners).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªä¸æ‚¨æ­£åœ¨ä½¿ç”¨çš„ğŸ¤— PEFTæ–¹æ³•ç›¸å¯¹åº”çš„é…ç½®ã€‚å¯¹äºLoRAï¼Œæ‚¨å°†ä½¿ç”¨[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)æ¥æŒ‡å®šä»»åŠ¡ç±»å‹ï¼Œä»¥åŠå…¶ä»–ä¸€äº›é‡è¦å‚æ•°ï¼Œå¦‚ä½ç§©çŸ©é˜µçš„ç»´åº¦ã€çŸ©é˜µç¼©æ”¾å› å­å’ŒLoRAå±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸åŒçš„ğŸ¤—
    PEFTæ–¹æ³•ï¼Œè¯·å°†`LoraConfig`æ›¿æ¢ä¸ºé€‚å½“çš„[ç±»](../package_reference/tuners)ã€‚
- en: Next, the script wraps the base model and `peft_config` with the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè„šæœ¬ä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)å‡½æ•°å°†åŸºç¡€æ¨¡å‹å’Œ`peft_config`åŒ…è£…èµ·æ¥ï¼Œä»¥åˆ›å»ºä¸€ä¸ª[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ã€‚
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Throughout the script, youâ€™ll see the [main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)
    and [wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)
    functions which help control and synchronize when processes are executed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªè„šæœ¬ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°[main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)å’Œ[wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)å‡½æ•°ï¼Œè¿™äº›å‡½æ•°æœ‰åŠ©äºæ§åˆ¶å’ŒåŒæ­¥è¿›ç¨‹çš„æ‰§è¡Œæ—¶é—´ã€‚
- en: After your dataset is prepared, and all the necessary training components are
    loaded, the script checks if youâ€™re using the `fsdp_plugin`. PyTorch offers two
    ways for wrapping model layers in FSDP, automatically or manually. The simplest
    method is to allow FSDP to automatically recursively wrap model layers without
    changing any other code. You can choose to wrap the model layers based on the
    layer name or on the size (number of parameters). In the FSDP configuration file,
    it uses the `TRANSFORMER_BASED_WRAP` option to wrap the `T5Block` layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡å¥½æ•°æ®é›†ï¼Œå¹¶åŠ è½½æ‰€æœ‰å¿…è¦çš„è®­ç»ƒç»„ä»¶åï¼Œè„šæœ¬ä¼šæ£€æŸ¥æ‚¨æ˜¯å¦åœ¨ä½¿ç”¨`fsdp_plugin`ã€‚PyTorchæä¾›äº†ä¸¤ç§åœ¨FSDPä¸­åŒ…è£…æ¨¡å‹å±‚çš„æ–¹æ³•ï¼Œè‡ªåŠ¨æˆ–æ‰‹åŠ¨ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯å…è®¸FSDPè‡ªåŠ¨é€’å½’åŒ…è£…æ¨¡å‹å±‚ï¼Œè€Œæ— éœ€æ›´æ”¹ä»»ä½•å…¶ä»–ä»£ç ã€‚æ‚¨å¯ä»¥é€‰æ‹©æ ¹æ®å±‚åç§°æˆ–å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰æ¥åŒ…è£…æ¨¡å‹å±‚ã€‚åœ¨FSDPé…ç½®æ–‡ä»¶ä¸­ï¼Œå®ƒä½¿ç”¨`TRANSFORMER_BASED_WRAP`é€‰é¡¹æ¥åŒ…è£…`T5Block`å±‚ã€‚
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, use ğŸ¤— Accelerateâ€™s [prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    function to prepare the model, datasets, optimizer, and scheduler for training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ğŸ¤— Accelerateçš„[prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)å‡½æ•°æ¥å‡†å¤‡æ¨¡å‹ã€æ•°æ®é›†ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è¿›è¡Œè®­ç»ƒã€‚
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From here, the remainder of the script handles the training loop, evaluation,
    and sharing your model to the Hub.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œè„šæœ¬çš„å…¶ä½™éƒ¨åˆ†å¤„ç†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ï¼Œå¹¶å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ°Hubã€‚
- en: Train
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'Run the following command to launch the training script. Earlier, you saved
    the configuration file to `fsdp_config.yaml`, so youâ€™ll need to pass the path
    to the launcher with the `--config_file` argument like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚ä¹‹å‰ï¼Œæ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸º`fsdp_config.yaml`ï¼Œå› æ­¤æ‚¨éœ€è¦é€šè¿‡`--config_file`å‚æ•°ä¼ é€’è·¯å¾„ç»™å¯åŠ¨å™¨ï¼Œå°±åƒè¿™æ ·ï¼š
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once training is complete, the script returns the accuracy and compares the
    predictions to the labels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œè„šæœ¬å°†è¿”å›å‡†ç¡®æ€§å¹¶å°†é¢„æµ‹ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚
