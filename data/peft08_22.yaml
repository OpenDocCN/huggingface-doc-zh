- en: Fully Sharded Data Parallel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全分片数据并行
- en: 'Original text: [https://huggingface.co/docs/peft/accelerate/fsdp](https://huggingface.co/docs/peft/accelerate/fsdp)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/peft/accelerate/fsdp](https://huggingface.co/docs/peft/accelerate/fsdp)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP)
    is developed for distributed training of large pretrained models up to 1T parameters.
    FSDP achieves this by sharding the model parameters, gradients, and optimizer
    states across data parallel processes and it can also offload sharded model parameters
    to a CPU. The memory efficiency afforded by FSDP allows you to scale training
    to larger batch or model sizes.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[完全分片数据并行](https://pytorch.org/docs/stable/fsdp.html)（FSDP）是为了分布式训练大型预训练模型，最多可达1T参数。FSDP通过在数据并行进程之间分片模型参数、梯度和优化器状态来实现这一点，还可以将分片模型参数卸载到CPU。FSDP提供的内存效率使您可以将训练扩展到更大的批次或模型大小。'
- en: Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with
    CPU offload actually consumes 1.65x more GPU memory during training. You can track
    this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any
    updates.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，FSDP并不会减少GPU内存的使用量，而具有CPU卸载的FSDP在训练期间实际上会消耗1.65倍的GPU内存。您可以跟踪这个PyTorch [问题](https://github.com/pytorch/pytorch/issues/91165)
    获取任何更新。
- en: FSDP is supported in 🤗 Accelerate, and you can use it with 🤗 PEFT. This guide
    will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py).
    You’ll configure the script to train a large model for conditional generation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: FSDP在🤗 Accelerate中受到支持，您可以与🤗 PEFT一起使用。本指南将帮助您学习如何使用我们的FSDP[训练脚本](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py)。您将配置脚本以训练一个大型模型用于条件生成。
- en: Configuration
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: Begin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp)
    with 🤗 Accelerate. Use the `--config_file` flag to save the configuration file
    to a specific location, otherwise it is saved as a `default_config.yaml` file
    in the 🤗 Accelerate cache.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先运行以下命令来[创建一个FSDP配置文件](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp)与🤗
    Accelerate一起。使用`--config_file`标志将配置文件保存到特定位置，否则它将保存为🤗 Accelerate缓存中的`default_config.yaml`文件。
- en: The configuration file is used to set the default options when you launch the
    training script.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件用于在启动训练脚本时设置默认选项。
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll be asked a few questions about your setup, and configure the following
    arguments. For this example, make sure you fully shard the model parameters, gradients,
    optimizer states, leverage the CPU for offloading, and wrap model layers based
    on the Transformer layer class name.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被问及有关您的设置的几个问题，并配置以下参数。在本示例中，请确保完全分片模型参数、梯度、优化器状态，利用CPU进行卸载，并根据Transformer层类名包装模型层。
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example, your FSDP configuration file may look like the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您的FSDP配置文件可能如下所示：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The important parts
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要部分
- en: Let’s dig a bit deeper into the training script to understand how it works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解训练脚本的工作原理。
- en: The [`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14)
    function begins with initializing an [Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    class which handles everything for distributed training, such as automatically
    detecting your training environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14)
    函数从初始化一个[Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)类开始，该类处理分布式训练的所有内容，例如自动检测您的训练环境。'
- en: 💡 Feel free to change the model and dataset inside the `main` function. If your
    dataset format is different from the one in the script, you may also need to write
    your own preprocessing function.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 随意更改`main`函数中的模型和数据集。如果您的数据集格式与脚本中的不同，您可能还需要编写自己的预处理函数。
- en: The script also creates a configuration corresponding to the 🤗 PEFT method you’re
    using. For LoRA, you’ll use [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    to specify the task type, and several other important parameters such as the dimension
    of the low-rank matrices, the matrices scaling factor, and the dropout probability
    of the LoRA layers. If you want to use a different 🤗 PEFT method, replace `LoraConfig`
    with the appropriate [class](../package_reference/tuners).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本还创建了一个与您正在使用的🤗 PEFT方法相对应的配置。对于LoRA，您将使用[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)来指定任务类型，以及其他一些重要参数，如低秩矩阵的维度、矩阵缩放因子和LoRA层的丢失概率。如果您想使用不同的🤗
    PEFT方法，请将`LoraConfig`替换为适当的[类](../package_reference/tuners)。
- en: Next, the script wraps the base model and `peft_config` with the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，脚本使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数将基础模型和`peft_config`包装起来，以创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Throughout the script, you’ll see the [main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)
    and [wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)
    functions which help control and synchronize when processes are executed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个脚本中，您将看到[main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)和[wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)函数，这些函数有助于控制和同步进程的执行时间。
- en: After your dataset is prepared, and all the necessary training components are
    loaded, the script checks if you’re using the `fsdp_plugin`. PyTorch offers two
    ways for wrapping model layers in FSDP, automatically or manually. The simplest
    method is to allow FSDP to automatically recursively wrap model layers without
    changing any other code. You can choose to wrap the model layers based on the
    layer name or on the size (number of parameters). In the FSDP configuration file,
    it uses the `TRANSFORMER_BASED_WRAP` option to wrap the `T5Block` layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好数据集，并加载所有必要的训练组件后，脚本会检查您是否在使用`fsdp_plugin`。PyTorch提供了两种在FSDP中包装模型层的方法，自动或手动。最简单的方法是允许FSDP自动递归包装模型层，而无需更改任何其他代码。您可以选择根据层名称或大小（参数数量）来包装模型层。在FSDP配置文件中，它使用`TRANSFORMER_BASED_WRAP`选项来包装`T5Block`层。
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, use 🤗 Accelerate’s [prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    function to prepare the model, datasets, optimizer, and scheduler for training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用🤗 Accelerate的[prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)函数来准备模型、数据集、优化器和调度器进行训练。
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From here, the remainder of the script handles the training loop, evaluation,
    and sharing your model to the Hub.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，脚本的其余部分处理训练循环、评估，并将您的模型分享到Hub。
- en: Train
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Run the following command to launch the training script. Earlier, you saved
    the configuration file to `fsdp_config.yaml`, so you’ll need to pass the path
    to the launcher with the `--config_file` argument like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令启动训练脚本。之前，您将配置文件保存为`fsdp_config.yaml`，因此您需要通过`--config_file`参数传递路径给启动器，就像这样：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once training is complete, the script returns the accuracy and compares the
    predictions to the labels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，脚本将返回准确性并将预测与标签进行比较。
