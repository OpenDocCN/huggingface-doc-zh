- en: PyTorch training on Apple silicon
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Apple硅上进行PyTorch训练
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_special](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_special)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_special](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_special)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Previously, training models on a Mac was limited to the CPU only. With the release
    of PyTorch v1.12, you can take advantage of training models with Apple’s silicon
    GPUs for significantly faster performance and training. This is powered in PyTorch
    by integrating Apple’s Metal Performance Shaders (MPS) as a backend. The [MPS
    backend](https://pytorch.org/docs/stable/notes/mps.html) implements PyTorch operations
    as custom Metal shaders and places these modules on a `mps` device.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，在Mac上训练模型仅限于CPU。随着PyTorch v1.12的发布，您可以利用使用Apple的硅GPU训练模型，以获得更快的性能和训练速度。在PyTorch中，这是通过将Apple的Metal
    Performance Shaders（MPS）集成为后端来实现的。[MPS后端](https://pytorch.org/docs/stable/notes/mps.html)将PyTorch操作实现为自定义的Metal着色器，并将这些模块放置在`mps`设备上。
- en: Some PyTorch operations are not implemented in MPS yet and will throw an error.
    To avoid this, you should set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1`
    to use the CPU kernels instead (you’ll still see a `UserWarning`).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一些PyTorch操作尚未在MPS中实现，将会引发错误。为了避免这种情况，您应该设置环境变量`PYTORCH_ENABLE_MPS_FALLBACK=1`来使用CPU内核（仍会看到`UserWarning`）。
- en: If you run into any other errors, please open an issue in the [PyTorch](https://github.com/pytorch/pytorch/issues)
    repository because the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    only integrates the MPS backend.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到其他错误，请在[PyTorch](https://github.com/pytorch/pytorch/issues)存储库中打开问题，因为[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)仅集成了MPS后端。
- en: 'With the `mps` device set, you can:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`mps`设备后，您可以：
- en: train larger networks or batch sizes locally
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地训练更大的网络或批量大小
- en: reduce data retrieval latency because the GPU’s unified memory architecture
    allows direct access to the full memory store
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少数据检索延迟，因为GPU的统一内存架构允许直接访问完整的内存存储
- en: reduce costs because you don’t need to train on cloud-based GPUs or add additional
    local GPUs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少成本，因为您不需要在基于云的GPU上进行训练或添加额外的本地GPU
- en: Get started by making sure you have PyTorch installed. MPS acceleration is supported
    on macOS 12.3+.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先确保您已安装PyTorch。MPS加速支持macOS 12.3+。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    uses the `mps` device by default if it’s available which means you don’t need
    to explicitly set the device. For example, you can run the [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py)
    script with the MPS backend automatically enabled without making any changes.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)默认使用`mps`设备，如果可用的话，这意味着您不需要显式设置设备。例如，您可以在不进行任何更改的情况下自动启用MPS后端运行[run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py)脚本。'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Backends for [distributed setups](https://pytorch.org/docs/stable/distributed.html#backends)
    like `gloo` and `nccl` are not supported by the `mps` device which means you can
    only train on a single GPU with the MPS backend.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 像`gloo`和`nccl`这样的[分布式设置](https://pytorch.org/docs/stable/distributed.html#backends)的后端不受`mps`设备支持，这意味着您只能在具有MPS后端的单个GPU上进行训练。
- en: You can learn more about the MPS backend in the [Introducing Accelerated PyTorch
    Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)
    blog post.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[在Mac上介绍加速PyTorch训练](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)博客文章中了解更多关于MPS后端的信息。
