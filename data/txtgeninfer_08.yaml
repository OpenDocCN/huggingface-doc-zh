- en: Consuming Text Generation Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消费文本生成推理
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways you can consume Text Generation Inference server in your
    applications. After launching, you can use the `/generate` route and make a `POST`
    request to get results from the server. You can also use the `/generate_stream`
    route if you want TGI to return a stream of tokens. You can make the requests
    using the tool of your preference, such as curl, Python or TypeScrpt. For a final
    end-to-end experience, we also open-sourced ChatUI, a chat interface for open-source
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以在应用程序中使用文本生成推理服务器。启动后，您可以使用`/generate`路由并发出`POST`请求以从服务器获取结果。如果希望TGI返回标记流，则还可以使用`/generate_stream`路由。您可以使用您喜欢的工具（如curl、Python或TypeScrpt）发出请求。为了获得最终的端到端体验，我们还开源了ChatUI，这是一个用于开源模型的聊天界面。
- en: curl
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: curl
- en: 'After the launch, you can query the model using either the `/generate` or `/generate_stream`
    routes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 启动后，可以使用`/generate`或`/generate_stream`路由查询模型：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inference Client
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理客户端
- en: '[`huggingface-hub`](https://huggingface.co/docs/huggingface_hub/main/en/index)
    is a Python library to interact with the Hugging Face Hub, including its endpoints.
    It provides a nice high-level class, [`~huggingface_hub.InferenceClient`], which
    makes it easy to make calls to a TGI endpoint. `InferenceClient` also takes care
    of parameter validation and provides a simple to-use interface. You can simply
    install `huggingface-hub` package with pip.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[`huggingface-hub`](https://huggingface.co/docs/huggingface_hub/main/en/index)是一个用于与Hugging
    Face Hub交互的Python库，包括其端点。它提供了一个很好的高级类[`~huggingface_hub.InferenceClient`]，使得调用TGI端点变得容易。`InferenceClient`还负责参数验证并提供一个简单易用的接口。您可以通过pip简单安装`huggingface-hub`包。'
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you start the TGI server, instantiate `InferenceClient()` with the URL
    to the endpoint serving the model. You can then call `text_generation()` to hit
    the endpoint through Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动TGI服务器，请使用提供模型的端点的URL实例化`InferenceClient()`。然后可以通过Python调用`text_generation()`来访问端点。
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can do streaming with `InferenceClient` by passing `stream=True`. Streaming
    will return tokens as they are being generated in the server. To use streaming,
    you can do as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递`stream=True`，您可以使用`InferenceClient`进行流式处理。流式处理将在服务器生成标记时返回标记。要使用流式处理，可以按照以下步骤操作：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Another parameter you can use with TGI backend is `details`. You can get more
    details on generation (tokens, probabilities, etc.) by setting `details` to `True`.
    When it’s specified, TGI will return a `TextGenerationResponse` or `TextGenerationStreamResponse`
    rather than a string or stream.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在TGI后端中使用的另一个参数是`details`。通过将`details`设置为`True`，可以获取有关生成（标记、概率等）的更多详细信息。当指定时，TGI将返回`TextGenerationResponse`或`TextGenerationStreamResponse`而不是字符串或流。
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see how to stream below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看下面的流式处理方式。
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can check out the details of the function [here](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation).
    There is also an async version of the client, `AsyncInferenceClient`, based on
    `asyncio` and `aiohttp`. You can find docs for it [here](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[此处](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)查看该函数的详细信息。还有一个基于`asyncio`和`aiohttp`的客户端的异步版本`AsyncInferenceClient`。您可以在[此处](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)找到其文档。
- en: ChatUI
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatUI
- en: ChatUI is an open-source interface built for LLM serving. It offers many customization
    options, such as web search with SERP API and more. ChatUI can automatically consume
    the TGI server and even provides an option to switch between different TGI endpoints.
    You can try it out at [Hugging Chat](https://huggingface.co/chat/), or use the
    [ChatUI Docker Space](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)
    to deploy your own Hugging Chat to Spaces.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ChatUI是为LLM服务构建的开源界面。它提供许多自定义选项，例如使用SERP API进行网络搜索等。ChatUI可以自动消费TGI服务器，甚至提供在不同TGI端点之间切换的选项。您可以在[Hugging
    Chat](https://huggingface.co/chat/)尝试它，或使用[ChatUI Docker Space](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)部署自己的Hugging
    Chat到Spaces。
- en: To serve both ChatUI and TGI in same environment, simply add your own endpoints
    to the `MODELS` variable in `.env.local` file inside the `chat-ui` repository.
    Provide the endpoints pointing to where TGI is served.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要在同一环境中同时提供ChatUI和TGI服务，只需将您自己的端点添加到`chat-ui`存储库中的`.env.local`文件中的`MODELS`变量中。提供指向TGI服务位置的端点。
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![ChatUI](../Images/ce268cbdae69aa7842502b673821bebb.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![ChatUI](../Images/ce268cbdae69aa7842502b673821bebb.png)'
- en: Gradio
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gradio
- en: Gradio is a Python library that helps you build web applications for your machine
    learning models with a few lines of code. It has a `ChatInterface` wrapper that
    helps create neat UIs for chatbots. Let’s take a look at how to create a chatbot
    with streaming mode using TGI and Gradio. Let’s install Gradio and Hub Python
    library first.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Gradio是一个Python库，可以帮助您使用几行代码为机器学习模型构建Web应用程序。它具有一个`ChatInterface`包装器，可帮助创建聊天机器人的整洁UI。让我们先安装Gradio和Hub
    Python库。
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Assume you are serving your model on port 8080, we will query through [InferenceClient](consuming_tgi#inference-client).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在端口8080上提供模型，我们将通过[InferenceClient](consuming_tgi#inference-client)进行查询。
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The UI looks like this 👇
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: UI如下👇
- en: '![](../Images/145df95e03868e6e18c9588c0878b82a.png) ![](../Images/af0024df3a67614ca6a101e75f4cdb9c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/145df95e03868e6e18c9588c0878b82a.png) ![](../Images/af0024df3a67614ca6a101e75f4cdb9c.png)'
- en: You can try the demo directly here 👇
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接在此处尝试演示👇
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=light](https://merve-gradio-tgi-2.hf.space?__theme=light)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://merve-gradio-tgi-2.hf.space?__theme=light](https://merve-gradio-tgi-2.hf.space?__theme=light)'
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)'
- en: You can disable streaming mode using `return` instead of `yield` in your inference
    function, like below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在推断函数中使用`return`而不是`yield`来禁用流式模式，就像下面这样。
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can read more about how to customize a `ChatInterface` [here](https://www.gradio.app/guides/creating-a-chatbot-fast).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以阅读有关如何自定义`ChatInterface`的更多信息[这里](https://www.gradio.app/guides/creating-a-chatbot-fast)。
- en: API documentation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API文档
- en: You can consult the OpenAPI documentation of the `text-generation-inference`
    REST API using the `/docs` route. The Swagger UI is also available [here](https://huggingface.github.io/text-generation-inference).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`/docs`路由查阅`text-generation-inference` REST API的OpenAPI文档。Swagger UI也可在[这里](https://huggingface.github.io/text-generation-inference)找到。
