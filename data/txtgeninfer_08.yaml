- en: Consuming Text Generation Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¶ˆè´¹æ–‡æœ¬ç”Ÿæˆæ¨ç†
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways you can consume Text Generation Inference server in your
    applications. After launching, you can use the `/generate` route and make a `POST`
    request to get results from the server. You can also use the `/generate_stream`
    route if you want TGI to return a stream of tokens. You can make the requests
    using the tool of your preference, such as curl, Python or TypeScrpt. For a final
    end-to-end experience, we also open-sourced ChatUI, a chat interface for open-source
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥åœ¨åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨ç†æœåŠ¡å™¨ã€‚å¯åŠ¨åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`/generate`è·¯ç”±å¹¶å‘å‡º`POST`è¯·æ±‚ä»¥ä»æœåŠ¡å™¨è·å–ç»“æœã€‚å¦‚æœå¸Œæœ›TGIè¿”å›æ ‡è®°æµï¼Œåˆ™è¿˜å¯ä»¥ä½¿ç”¨`/generate_stream`è·¯ç”±ã€‚æ‚¨å¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„å·¥å…·ï¼ˆå¦‚curlã€Pythonæˆ–TypeScrptï¼‰å‘å‡ºè¯·æ±‚ã€‚ä¸ºäº†è·å¾—æœ€ç»ˆçš„ç«¯åˆ°ç«¯ä½“éªŒï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†ChatUIï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¼€æºæ¨¡å‹çš„èŠå¤©ç•Œé¢ã€‚
- en: curl
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: curl
- en: 'After the launch, you can query the model using either the `/generate` or `/generate_stream`
    routes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¯åŠ¨åï¼Œå¯ä»¥ä½¿ç”¨`/generate`æˆ–`/generate_stream`è·¯ç”±æŸ¥è¯¢æ¨¡å‹ï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inference Client
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†å®¢æˆ·ç«¯
- en: '[`huggingface-hub`](https://huggingface.co/docs/huggingface_hub/main/en/index)
    is a Python library to interact with the Hugging Face Hub, including its endpoints.
    It provides a nice high-level class, [`~huggingface_hub.InferenceClient`], which
    makes it easy to make calls to a TGI endpoint. `InferenceClient` also takes care
    of parameter validation and provides a simple to-use interface. You can simply
    install `huggingface-hub` package with pip.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[`huggingface-hub`](https://huggingface.co/docs/huggingface_hub/main/en/index)æ˜¯ä¸€ä¸ªç”¨äºä¸Hugging
    Face Hubäº¤äº’çš„Pythonåº“ï¼ŒåŒ…æ‹¬å…¶ç«¯ç‚¹ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„é«˜çº§ç±»[`~huggingface_hub.InferenceClient`]ï¼Œä½¿å¾—è°ƒç”¨TGIç«¯ç‚¹å˜å¾—å®¹æ˜“ã€‚`InferenceClient`è¿˜è´Ÿè´£å‚æ•°éªŒè¯å¹¶æä¾›ä¸€ä¸ªç®€å•æ˜“ç”¨çš„æ¥å£ã€‚æ‚¨å¯ä»¥é€šè¿‡pipç®€å•å®‰è£…`huggingface-hub`åŒ…ã€‚'
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you start the TGI server, instantiate `InferenceClient()` with the URL
    to the endpoint serving the model. You can then call `text_generation()` to hit
    the endpoint through Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å¯åŠ¨TGIæœåŠ¡å™¨ï¼Œè¯·ä½¿ç”¨æä¾›æ¨¡å‹çš„ç«¯ç‚¹çš„URLå®ä¾‹åŒ–`InferenceClient()`ã€‚ç„¶åå¯ä»¥é€šè¿‡Pythonè°ƒç”¨`text_generation()`æ¥è®¿é—®ç«¯ç‚¹ã€‚
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can do streaming with `InferenceClient` by passing `stream=True`. Streaming
    will return tokens as they are being generated in the server. To use streaming,
    you can do as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¼ é€’`stream=True`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`InferenceClient`è¿›è¡Œæµå¼å¤„ç†ã€‚æµå¼å¤„ç†å°†åœ¨æœåŠ¡å™¨ç”Ÿæˆæ ‡è®°æ—¶è¿”å›æ ‡è®°ã€‚è¦ä½¿ç”¨æµå¼å¤„ç†ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Another parameter you can use with TGI backend is `details`. You can get more
    details on generation (tokens, probabilities, etc.) by setting `details` to `True`.
    When itâ€™s specified, TGI will return a `TextGenerationResponse` or `TextGenerationStreamResponse`
    rather than a string or stream.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨TGIåç«¯ä¸­ä½¿ç”¨çš„å¦ä¸€ä¸ªå‚æ•°æ˜¯`details`ã€‚é€šè¿‡å°†`details`è®¾ç½®ä¸º`True`ï¼Œå¯ä»¥è·å–æœ‰å…³ç”Ÿæˆï¼ˆæ ‡è®°ã€æ¦‚ç‡ç­‰ï¼‰çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚å½“æŒ‡å®šæ—¶ï¼ŒTGIå°†è¿”å›`TextGenerationResponse`æˆ–`TextGenerationStreamResponse`è€Œä¸æ˜¯å­—ç¬¦ä¸²æˆ–æµã€‚
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see how to stream below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„æµå¼å¤„ç†æ–¹å¼ã€‚
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can check out the details of the function [here](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation).
    There is also an async version of the client, `AsyncInferenceClient`, based on
    `asyncio` and `aiohttp`. You can find docs for it [here](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)æŸ¥çœ‹è¯¥å‡½æ•°çš„è¯¦ç»†ä¿¡æ¯ã€‚è¿˜æœ‰ä¸€ä¸ªåŸºäº`asyncio`å’Œ`aiohttp`çš„å®¢æˆ·ç«¯çš„å¼‚æ­¥ç‰ˆæœ¬`AsyncInferenceClient`ã€‚æ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)æ‰¾åˆ°å…¶æ–‡æ¡£ã€‚
- en: ChatUI
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatUI
- en: ChatUI is an open-source interface built for LLM serving. It offers many customization
    options, such as web search with SERP API and more. ChatUI can automatically consume
    the TGI server and even provides an option to switch between different TGI endpoints.
    You can try it out at [Hugging Chat](https://huggingface.co/chat/), or use the
    [ChatUI Docker Space](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)
    to deploy your own Hugging Chat to Spaces.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ChatUIæ˜¯ä¸ºLLMæœåŠ¡æ„å»ºçš„å¼€æºç•Œé¢ã€‚å®ƒæä¾›è®¸å¤šè‡ªå®šä¹‰é€‰é¡¹ï¼Œä¾‹å¦‚ä½¿ç”¨SERP APIè¿›è¡Œç½‘ç»œæœç´¢ç­‰ã€‚ChatUIå¯ä»¥è‡ªåŠ¨æ¶ˆè´¹TGIæœåŠ¡å™¨ï¼Œç”šè‡³æä¾›åœ¨ä¸åŒTGIç«¯ç‚¹ä¹‹é—´åˆ‡æ¢çš„é€‰é¡¹ã€‚æ‚¨å¯ä»¥åœ¨[Hugging
    Chat](https://huggingface.co/chat/)å°è¯•å®ƒï¼Œæˆ–ä½¿ç”¨[ChatUI Docker Space](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)éƒ¨ç½²è‡ªå·±çš„Hugging
    Chatåˆ°Spacesã€‚
- en: To serve both ChatUI and TGI in same environment, simply add your own endpoints
    to the `MODELS` variable in `.env.local` file inside the `chat-ui` repository.
    Provide the endpoints pointing to where TGI is served.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨åŒä¸€ç¯å¢ƒä¸­åŒæ—¶æä¾›ChatUIå’ŒTGIæœåŠ¡ï¼Œåªéœ€å°†æ‚¨è‡ªå·±çš„ç«¯ç‚¹æ·»åŠ åˆ°`chat-ui`å­˜å‚¨åº“ä¸­çš„`.env.local`æ–‡ä»¶ä¸­çš„`MODELS`å˜é‡ä¸­ã€‚æä¾›æŒ‡å‘TGIæœåŠ¡ä½ç½®çš„ç«¯ç‚¹ã€‚
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![ChatUI](../Images/ce268cbdae69aa7842502b673821bebb.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![ChatUI](../Images/ce268cbdae69aa7842502b673821bebb.png)'
- en: Gradio
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gradio
- en: Gradio is a Python library that helps you build web applications for your machine
    learning models with a few lines of code. It has a `ChatInterface` wrapper that
    helps create neat UIs for chatbots. Letâ€™s take a look at how to create a chatbot
    with streaming mode using TGI and Gradio. Letâ€™s install Gradio and Hub Python
    library first.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Gradioæ˜¯ä¸€ä¸ªPythonåº“ï¼Œå¯ä»¥å¸®åŠ©æ‚¨ä½¿ç”¨å‡ è¡Œä»£ç ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æ„å»ºWebåº”ç”¨ç¨‹åºã€‚å®ƒå…·æœ‰ä¸€ä¸ª`ChatInterface`åŒ…è£…å™¨ï¼Œå¯å¸®åŠ©åˆ›å»ºèŠå¤©æœºå™¨äººçš„æ•´æ´UIã€‚è®©æˆ‘ä»¬å…ˆå®‰è£…Gradioå’ŒHub
    Pythonåº“ã€‚
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Assume you are serving your model on port 8080, we will query through [InferenceClient](consuming_tgi#inference-client).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ‚¨åœ¨ç«¯å£8080ä¸Šæä¾›æ¨¡å‹ï¼Œæˆ‘ä»¬å°†é€šè¿‡[InferenceClient](consuming_tgi#inference-client)è¿›è¡ŒæŸ¥è¯¢ã€‚
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The UI looks like this ğŸ‘‡
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: UIå¦‚ä¸‹ğŸ‘‡
- en: '![](../Images/145df95e03868e6e18c9588c0878b82a.png) ![](../Images/af0024df3a67614ca6a101e75f4cdb9c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/145df95e03868e6e18c9588c0878b82a.png) ![](../Images/af0024df3a67614ca6a101e75f4cdb9c.png)'
- en: You can try the demo directly here ğŸ‘‡
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ç›´æ¥åœ¨æ­¤å¤„å°è¯•æ¼”ç¤ºğŸ‘‡
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=light](https://merve-gradio-tgi-2.hf.space?__theme=light)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://merve-gradio-tgi-2.hf.space?__theme=light](https://merve-gradio-tgi-2.hf.space?__theme=light)'
- en: '[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)'
- en: You can disable streaming mode using `return` instead of `yield` in your inference
    function, like below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨æ¨æ–­å‡½æ•°ä¸­ä½¿ç”¨`return`è€Œä¸æ˜¯`yield`æ¥ç¦ç”¨æµå¼æ¨¡å¼ï¼Œå°±åƒä¸‹é¢è¿™æ ·ã€‚
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can read more about how to customize a `ChatInterface` [here](https://www.gradio.app/guides/creating-a-chatbot-fast).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é˜…è¯»æœ‰å…³å¦‚ä½•è‡ªå®šä¹‰`ChatInterface`çš„æ›´å¤šä¿¡æ¯[è¿™é‡Œ](https://www.gradio.app/guides/creating-a-chatbot-fast)ã€‚
- en: API documentation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: APIæ–‡æ¡£
- en: You can consult the OpenAPI documentation of the `text-generation-inference`
    REST API using the `/docs` route. The Swagger UI is also available [here](https://huggingface.github.io/text-generation-inference).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`/docs`è·¯ç”±æŸ¥é˜…`text-generation-inference` REST APIçš„OpenAPIæ–‡æ¡£ã€‚Swagger UIä¹Ÿå¯åœ¨[è¿™é‡Œ](https://huggingface.github.io/text-generation-inference)æ‰¾åˆ°ã€‚
