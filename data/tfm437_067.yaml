- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/quantization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Quantization techniques focus on representing data with less information while
    also trying to not lose too much accuracy. This often means converting a data
    type to represent the same information with fewer bits. For example, if your model
    weights are stored as 32-bit floating points and they’re quantized to 16-bit floating
    points, this halves the model size which makes it easier to store and reduces
    memory-usage. Lower precision can also speedup inference because it takes less
    time to perform calculations with fewer bits.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术专注于用更少的信息表示数据，同时也试图不丢失太多准确性。这通常意味着将数据类型转换为用更少的位表示相同信息。例如，如果您的模型权重存储为32位浮点数，并且它们被量化为16位浮点数，这将使模型大小减半，使其更容易存储并减少内存使用。较低的精度也可以加快推理速度，因为使用更少的位进行计算需要更少的时间。
- en: Transformers supports several quantization schemes to help you run inference
    with large language models (LLMs) and finetune adapters on quantized models. This
    guide will show you how to use Activation-aware Weight Quantization (AWQ), AutoGPTQ,
    and bitsandbytes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers支持几种量化方案，帮助您在大型语言模型（LLMs）上运行推理和在量化模型上微调适配器。本指南将向您展示如何使用激活感知权重量化（AWQ）、AutoGPTQ和bitsandbytes。
- en: AWQ
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWQ
- en: Try AWQ quantization with this [notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用这个[notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)进行AWQ量化！
- en: '[Activation-aware Weight Quantization (AWQ)](https://hf.co/papers/2306.00978)
    doesn’t quantize all the weights in a model, and instead, it preserves a small
    percentage of weights that are important for LLM performance. This significantly
    reduces quantization loss such that you can run models in 4-bit precision without
    experiencing any performance degradation.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[激活感知权重量化（AWQ）](https://hf.co/papers/2306.00978)不会量化模型中的所有权重，而是保留对LLM性能重要的一小部分权重。这显著减少了量化损失，使您可以在不经历任何性能降级的情况下以4位精度运行模型。'
- en: There are several libraries for quantizing models with the AWQ algorithm, such
    as [llm-awq](https://github.com/mit-han-lab/llm-awq), [autoawq](https://github.com/casper-hansen/AutoAWQ)
    or [optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc).
    Transformers supports loading models quantized with the llm-awq and autoawq libraries.
    This guide will show you how to load models quantized with autoawq, but the processs
    is similar for llm-awq quantized models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个库可用于使用AWQ算法量化模型，例如[llm-awq](https://github.com/mit-han-lab/llm-awq)、[autoawq](https://github.com/casper-hansen/AutoAWQ)或[optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)。
    Transformers支持加载使用llm-awq和autoawq库量化的模型。本指南将向您展示如何加载使用autoawq量化的模型，但对于使用llm-awq量化的模型，过程类似。
- en: 'Make sure you have autoawq installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已安装autoawq：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'AWQ-quantized models can be identified by checking the `quantization_config`
    attribute in the model’s [config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json)
    file:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过检查模型的[config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json)文件中的`quantization_config`属性来识别AWQ量化模型：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A quantized model is loaded with the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method. If you loaded your model on the CPU, make sure to move it to a GPU device
    first. Use the `device_map` parameter to specify where to place the model:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法加载量化模型。如果您在CPU上加载了模型，请确保首先将其移动到GPU设备上。使用`device_map`参数指定模型放置的位置：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Loading an AWQ-quantized model automatically sets other weights to fp16 by
    default for performance reasons. If you want to load these other weights in a
    different format, use the `torch_dtype` parameter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 加载AWQ量化模型会自动将其他权重默认设置为fp16以提高性能。如果您想以不同格式加载这些其他权重，请使用`torch_dtype`参数：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'AWQ quantization can also be combined with [FlashAttention-2](perf_infer_gpu_one#flashattention-2)
    to further accelerate inference:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AWQ量化也可以与[FlashAttention-2](perf_infer_gpu_one#flashattention-2)结合，以进一步加速推理：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Fused modules
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 融合模块
- en: Fused modules offers improved accuracy and performance and it is supported out-of-the-box
    for AWQ modules for [Llama](https://huggingface.co/meta-llama) and [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    architectures, but you can also fuse AWQ modules for unsupported architectures.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 融合模块提供了改进的准确性和性能，并且对于[Llama](https://huggingface.co/meta-llama)和[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)架构的AWQ模块支持开箱即用，但您也可以为不支持的架构融合AWQ模块。
- en: Fused modules cannot be combined with other optimization techniques such as
    FlashAttention-2.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 融合模块不能与FlashAttention-2等其他优化技术结合使用。
- en: supported architecturesunsupported architectures
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的架构不支持的架构
- en: To enable fused modules for supported architectures, create an [AwqConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.AwqConfig)
    and set the parameters `fuse_max_seq_len` and `do_fuse=True`. The `fuse_max_seq_len`
    parameter is the total sequence length and it should include the context length
    and the expected generation length. You can set it to a larger value to be safe.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要为支持的架构启用融合模块，请创建一个[AwqConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.AwqConfig)并设置参数`fuse_max_seq_len`和`do_fuse=True`。`fuse_max_seq_len`参数是总序列长度，应包括上下文长度和预期生成长度。您可以将其设置为较大的值以确保安全。
- en: For example, to fuse the AWQ modules of the [TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)
    model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要融合[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)模型的AWQ模块。
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: AutoGPTQ
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoGPTQ
- en: Try GPTQ quantization with PEFT in this [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)
    and learn more about it’s details in this [blog post](https://huggingface.co/blog/gptq-integration)!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个[notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)中尝试使用PEFT进行GPTQ量化，并在这篇[博客文章](https://huggingface.co/blog/gptq-integration)中了解更多细节！
- en: The [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) library implements the
    GPTQ algorithm, a post-training quantization technique where each row of the weight
    matrix is quantized independently to find a version of the weights that minimizes
    the error. These weights are quantized to int4, but they’re restored to fp16 on
    the fly during inference. This can save your memory-usage by 4x because the int4
    weights are dequantized in a fused kernel rather than a GPU’s global memory, and
    you can also expect a speedup in inference because using a lower bitwidth takes
    less time to communicate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)库实现了GPTQ算法，这是一种后训练量化技术，其中权重矩阵的每一行都独立量化，以找到最小化误差的权重版本。这些权重被量化为int4，但在推理过程中会动态恢复为fp16。这可以通过4倍减少内存使用，因为int4权重在融合内核中而不是GPU的全局内存中被解量化，您还可以期望推理速度提升，因为使用较低的位宽需要更少的通信时间。'
- en: 'Before you begin, make sure the following libraries are installed:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 开始之前，请确保安装了以下库：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To quantize a model (currently only supported for text models), you need to
    create a [GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)
    class and set the number of bits to quantize to, a dataset to calibrate the weights
    for quantization, and a tokenizer to prepare the dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要量化一个模型（目前仅支持文本模型），您需要创建一个[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)类，并设置要量化的位数、用于校准权重的数据集以及准备数据集的分词器。
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You could also pass your own dataset as a list of strings, but it is highly
    recommended to use the same dataset from the GPTQ paper.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将自己的数据集作为字符串列表传递，但强烈建议使用GPTQ论文中相同的数据集。
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Load a model to quantize and pass the `gptq_config` to the [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    method. Set `device_map="auto"` to automatically offload the model to a CPU to
    help fit the model in memory, and allow the model modules to be moved between
    the CPU and GPU for quantization.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一个要量化的模型，并将`gptq_config`传递给[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)方法。设置`device_map="auto"`以自动将模型卸载到CPU，以帮助将模型适配到内存中，并允许模型模块在CPU和GPU之间移动以进行量化。
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you’re running out of memory because a dataset is too large, disk offloading
    is not supported. If this is the case, try passing the `max_memory` parameter
    to allocate the amount of memory to use on your device (GPU and CPU):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于数据集过大而导致内存不足，不支持磁盘卸载。如果是这种情况，请尝试传递`max_memory`参数来分配设备（GPU和CPU）上要使用的内存量：
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Depending on your hardware, it can take some time to quantize a model from scratch.
    It can take ~5 minutes to quantize the faceboook/opt-350m model on a free-tier
    Google Colab GPU, but it’ll take ~4 hours to quantize a 175B parameter model on
    a NVIDIA A100\. Before you quantize a model, it is a good idea to check the Hub
    if a GPTQ-quantized version of the model already exists.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的硬件，从头开始量化一个模型可能需要一些时间。在免费的Google Colab GPU上，量化faceboook/opt-350m模型可能需要约5分钟，但在NVIDIA
    A100上，量化一个175B参数模型可能需要约4小时。在量化模型之前，最好先检查Hub，看看模型的GPTQ量化版本是否已经存在。
- en: 'Once your model is quantized, you can push the model and tokenizer to the Hub
    where it can be easily shared and accessed. Use the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method to save the [GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的模型被量化，您可以将模型和分词器推送到Hub，这样可以轻松共享和访问。使用[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法保存[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)：
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You could also save your quantized model locally with the [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    method. If the model was quantized with the `device_map` parameter, make sure
    to move the entire model to a GPU or CPU before saving it. For example, to save
    the model on a CPU:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)方法将您的量化模型保存在本地。如果模型是使用`device_map`参数量化的，请确保在保存之前将整个模型移动到GPU或CPU。例如，要在CPU上保存模型：
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Reload a quantized model with the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method, and set `device_map="auto"` to automatically distribute the model on all
    available GPUs to load the model faster without using more memory than needed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法重新加载一个量化模型，并设置`device_map="auto"`以自动将模型分布在所有可用的GPU上，以便更快地加载模型而不使用比所需更多的内存。
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ExLlama
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExLlama
- en: '[ExLlama](https://github.com/turboderp/exllama) is a Python/C++/CUDA implementation
    of the [Llama](model_doc/llama) model that is designed for faster inference with
    4-bit GPTQ weights (check out these [benchmarks](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)).
    The ExLlama kernel is activated by default when you create a [GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)
    object. To boost inference speed even further, use the [ExLlamaV2](https://github.com/turboderp/exllamav2)
    kernels by configuring the `exllama_config` parameter:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[ExLlama](https://github.com/turboderp/exllama)是[Llama](model_doc/llama)模型的Python/C++/CUDA实现，旨在通过4位GPTQ权重实现更快的推理（查看这些[基准测试](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)）。当您创建一个[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)对象时，默认情况下会激活ExLlama内核。为了进一步提高推理速度，请配置`exllama_config`参数使用[ExLlamaV2](https://github.com/turboderp/exllamav2)内核：'
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Only 4-bit models are supported, and we recommend deactivating the ExLlama kernels
    if you’re finetuning a quantized model with PEFT.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 仅支持4位模型，并且我们建议在对量化模型进行微调时停用ExLlama内核。
- en: The ExLlama kernels are only supported when the entire model is on the GPU.
    If you’re doing inference on a CPU with AutoGPTQ (version > 0.4.2), then you’ll
    need to disable the ExLlama kernel. This overwrites the attributes related to
    the ExLlama kernels in the quantization config of the config.json file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当整个模型在GPU上时才支持ExLlama内核。如果您在CPU上使用AutoGPTQ（版本>0.4.2）进行推理，则需要禁用ExLlama内核。这将覆盖config.json文件中与ExLlama内核相关的属性。
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: bitsandbytes
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: bitsandbytes
- en: '[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) is the easiest
    option for quantizing a model to 8 and 4-bit. 8-bit quantization multiplies outliers
    in fp16 with non-outliers in int8, converts the non-outlier values back to fp16,
    and then adds them together to return the weights in fp16\. This reduces the degradative
    effect outlier values have on a model’s performance. 4-bit quantization compresses
    a model even further, and it is commonly used with [QLoRA](https://hf.co/papers/2305.14314)
    to finetune quantized LLMs.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)是将模型量化为8位和4位的最简单选择。8位量化将fp16中的异常值与int8中的非异常值相乘，将非异常值转换回fp16，然后将它们相加以返回fp16中的权重。这减少了异常值对模型性能的降级影响。4位量化进一步压缩模型，通常与[QLoRA](https://hf.co/papers/2305.14314)一起用于微调量化的LLM。'
- en: 'To use bitsandbytes, make sure you have the following libraries installed:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用bitsandbytes，请确保已安装以下库：
- en: 8-bit4-bit
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 8位4位
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now you can quantize a model with the `load_in_8bit` or `load_in_4bit` parameters
    in the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method. This works for any model in any modality, as long as it supports loading
    with Accelerate and contains `torch.nn.Linear` layers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法中的`load_in_8bit`或`load_in_4bit`参数来量化模型。只要模型支持使用Accelerate加载并包含`torch.nn.Linear`层，这对任何模态的模型都适用。
- en: 8-bit4-bit
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 8位4位
- en: 'Quantizing a model in 8-bit halves the memory-usage, and for large models,
    set `device_map="auto"` to efficiently use the GPUs available:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型量化为8位可以减半内存使用量，对于大型模型，设置`device_map="auto"`以有效地使用可用的GPU：
- en: '[PRE17]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'By default, all the other modules such as `torch.nn.LayerNorm` are converted
    to `torch.float16`. You can change the data type of these modules with the `torch_dtype`
    parameter if you want:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有其他模块（如`torch.nn.LayerNorm`）都会转换为`torch.float16`。如果需要，您可以使用`torch_dtype`参数更改这些模块的数据类型：
- en: '[PRE18]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Once a model is quantized to 8-bit, you can’t push the quantized weights to
    the Hub unless you’re using the latest version of Transformers and bitsandbytes.
    If you have the latest versions, then you can push the 8-bit model to the Hub
    with the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method. The quantization config.json file is pushed first, followed by the quantized
    model weights.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被量化为8位，除非您使用最新版本的Transformers和bitsandbytes，否则无法将量化的权重推送到Hub。如果您有最新版本，则可以使用[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法将8位模型推送到Hub。首先推送量化的config.json文件，然后是量化的模型权重。
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Training with 8-bit and 4-bit weights are only supported for training *extra*
    parameters.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 仅支持使用8位和4位权重进行训练*额外*参数。
- en: 'You can check your memory footprint with the `get_memory_footprint` method:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`get_memory_footprint`方法检查内存占用情况：
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Quantized models can be loaded from the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method without needing to specify the `load_in_8bit` or `load_in_4bit` parameters:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法中加载量化模型，无需指定`load_in_8bit`或`load_in_4bit`参数：
- en: '[PRE21]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 8-bit
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8位
- en: Learn more about the details of 8-bit quantization in this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration)!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇[博客文章](https://huggingface.co/blog/hf-bitsandbytes-integration)中了解更多关于8位量化的细节！
- en: This section explores some of the specific features of 8-bit models, such as
    offloading, outlier thresholds, skipping module conversion, and finetuning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了8位模型的一些特定功能，如转移、异常值阈值、跳过模块转换和微调。
- en: Offloading
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转移
- en: '8-bit models can offload weights between the CPU and GPU to support fitting
    very large models into memory. The weights dispatched to the CPU are actually
    stored in **float32**, and aren’t converted to 8-bit. For example, to enable offloading
    for the [bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7) model,
    start by creating a [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 8位模型可以在CPU和GPU之间转移权重，以支持将非常大的模型适配到内存中。发送到CPU的权重实际上是以**float32**存储的，并且不会转换为8位。例如，要为[bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)模型启用转移，请首先创建一个[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)：
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Design a custom device map to fit everything on your GPU except for the `lm_head`,
    which you’ll dispatch to the CPU:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个自定义设备映射，将所有内容都适配到GPU上，除了`lm_head`，这部分将发送到CPU：
- en: '[PRE23]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now load your model with the custom `device_map` and `quantization_config`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用自定义的`device_map`和`quantization_config`加载您的模型：
- en: '[PRE24]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Outlier threshold
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常值阈值
- en: An “outlier” is a hidden state value greater than a certain threshold, and these
    values are computed in fp16\. While the values are usually normally distributed
    ([-3.5, 3.5]), this distribution can be very different for large models ([-60,
    6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that,
    there is a significant performance penalty. A good default threshold value is
    6, but a lower threshold may be needed for more unstable models (small models
    or finetuning).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: “异常值”是大于某个阈值的隐藏状态值，这些值是在fp16中计算的。虽然这些值通常是正态分布的（[-3.5, 3.5]），但对于大型模型（[-60, 6]或[6,
    60]），这种分布可能会有很大不同。8位量化适用于值约为5，但超过这个值，会有显著的性能损失。一个很好的默认阈值是6，但对于更不稳定的模型（小模型或微调），可能需要更低的阈值。
- en: 'To find the best threshold for your model, we recommend experimenting with
    the `llm_int8_threshold` parameter in [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到您的模型的最佳阈值，我们建议尝试在[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中使用`llm_int8_threshold`参数进行实验：
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Skip module conversion
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跳过模块转换
- en: 'For some models, like [Jukebox](model_doc/jukebox), you don’t need to quantize
    every module to 8-bit which can actually cause instability. With Jukebox, there
    are several `lm_head` modules that should be skipped using the `llm_int8_skip_modules`
    parameter in [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些模型，如[Jukebox](model_doc/jukebox)，您不需要将每个模块量化为8位，这实际上可能会导致不稳定性。对于Jukebox，有几个`lm_head`模块应该使用[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中的`llm_int8_skip_modules`参数跳过：
- en: '[PRE26]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Finetuning
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调
- en: With the [PEFT](https://github.com/huggingface/peft) library, you can finetune
    large models like [flan-t5-large](https://huggingface.co/google/flan-t5-large)
    and [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b) with 8-bit quantization.
    You don’t need to pass the `device_map` parameter for training because it’ll automatically
    load your model on a GPU. However, you can still customize the device map with
    the `device_map` parameter if you want to (`device_map="auto"` should only be
    used for inference).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[PEFT](https://github.com/huggingface/peft)库，您可以使用8位量化微调大型模型，如[flan-t5-large](https://huggingface.co/google/flan-t5-large)和[facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)。在训练时不需要传递`device_map`参数，因为它会自动将您的模型加载到GPU上。但是，如果您想要，仍然可以使用`device_map`参数自定义设备映射（`device_map="auto"`仅应用于推断）。
- en: 4-bit
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4位
- en: Try 4-bit quantization in this [notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)
    and learn more about it’s details in this [blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个[notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)中尝试4位量化，并在这篇[博客文章](https://huggingface.co/blog/4bit-transformers-bitsandbytes)中了解更多细节。
- en: This section explores some of the specific features of 4-bit models, such as
    changing the compute data type, using the Normal Float 4 (NF4) data type, and
    using nested quantization.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了4位模型的一些特定功能，如更改计算数据类型、使用Normal Float 4 (NF4)数据类型和使用嵌套量化。
- en: Compute data type
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算数据类型
- en: 'To speedup computation, you can change the data type from float32 (the default
    value) to bf16 using the `bnb_4bit_compute_dtype` parameter in [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速计算，您可以将数据类型从float32（默认值）更改为bf16，使用[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中的`bnb_4bit_compute_dtype`参数：
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Normal Float 4 (NF4)
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Normal Float 4 (NF4)
- en: 'NF4 is a 4-bit data type from the [QLoRA](https://hf.co/papers/2305.14314)
    paper, adapted for weights initialized from a normal distribution. You should
    use NF4 for training 4-bit base models. This can be configured with the `bnb_4bit_quant_type`
    parameter in the [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: NF4是来自[QLoRA](https://hf.co/papers/2305.14314)论文的4位数据类型，适用于从正态分布初始化的权重。您应该使用NF4来训练4位基础模型。这可以通过[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)中的`bnb_4bit_quant_type`参数进行配置：
- en: '[PRE28]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For inference, the `bnb_4bit_quant_type` does not have a huge impact on performance.
    However, to remain consistent with the model weights, you should use the `bnb_4bit_compute_dtype`
    and `torch_dtype` values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推断，`bnb_4bit_quant_type`对性能没有太大影响。但是，为了保持与模型权重一致，您应该使用`bnb_4bit_compute_dtype`和`torch_dtype`值。
- en: Nested quantization
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌套量化
- en: Nested quantization is a technique that can save additional memory at no additional
    performance cost. This feature performs a second quantization of the already quantized
    weights to save an addition 0.4 bits/parameter. For example, with nested quantization,
    you can finetune a [Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b)
    model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of
    1, and enabling gradient accumulation with 4 steps.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套量化是一种技术，可以在不增加性能成本的情况下节省额外的内存。此功能对已经量化的权重执行第二次量化，以节省额外的0.4位/参数。例如，使用嵌套量化，您可以在16GB的NVIDIA
    T4 GPU上微调[Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b)模型，序列长度为1024，批量大小为1，并启用梯度累积4步。
- en: '[PRE29]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Optimum
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optimum
- en: The [Optimum](https://huggingface.co/docs/optimum/index) library supports quantization
    for Intel, Furiosa, ONNX Runtime, GPTQ, and lower-level PyTorch quantization functions.
    Consider using Optimum for quantization if you’re using specific and optimized
    hardware like Intel CPUs, Furiosa NPUs or a model accelerator like ONNX Runtime.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[Optimum](https://huggingface.co/docs/optimum/index)库支持Intel、Furiosa、ONNX Runtime、GPTQ和较低级别的PyTorch量化功能。如果您正在使用像Intel
    CPU、Furiosa NPU或像ONNX Runtime这样的模型加速器这样的特定和优化的硬件，请考虑使用Optimum进行量化。'
- en: Benchmarks
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: To compare the speed, throughput, and latency of each quantization scheme, check
    the following benchmarks obtained from the [optimum-benchmark](https://github.com/huggingface/optimum-benchmark)
    library. The benchmark was run on a NVIDIA A1000 for the [TheBloke/Mistral-7B-v0.1-AWQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)
    and [TheBloke/Mistral-7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)
    models. These were also tested against the bitsandbytes quantization methods as
    well as a native fp16 model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较每种量化方案的速度、吞吐量和延迟，请查看从[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)库获得的以下基准测试。该基准测试在NVIDIA
    A1000上运行，用于[TheBloke/Mistral-7B-v0.1-AWQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)和[TheBloke/Mistral-7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)模型。这些还与bitsandbytes量化方法以及本机fp16模型进行了测试。
- en: '![forward peak memory per batch size](../Images/7f7a45cc2b5704faa291fb7c3ca255d6.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![每批前向峰值内存](../Images/7f7a45cc2b5704faa291fb7c3ca255d6.png)'
- en: forward peak memory/batch size
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前向峰值内存/批处理大小
- en: '![generate peak memory per batch size](../Images/c53e9d1c99d4b8ce87793021dc7f7393.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![每批前向峰值内存](../Images/c53e9d1c99d4b8ce87793021dc7f7393.png)'
- en: generate peak memory/batch size
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 每批生成峰值内存/批处理大小
- en: '![generate throughput per batch size](../Images/40d57cb1a4c4282ec140a27de593156a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![每批生成吞吐量](../Images/40d57cb1a4c4282ec140a27de593156a.png)'
- en: generate throughput/batch size
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 每批生成吞吐量/批处理大小
- en: '![forward latency per batch size](../Images/67c3f5ce2fa384e77579473e9efb77fa.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![每批前向延迟](../Images/67c3f5ce2fa384e77579473e9efb77fa.png)'
- en: forward latency/batch size
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前向延迟/批处理大小
- en: The benchmarks indicate AWQ quantization is the fastest for inference, text
    generation, and has the lowest peak memory for text generation. However, AWQ has
    the largest forward latency per batch size. For a more detailed discussion about
    the pros and cons of each quantization method, read the [Overview of natively
    supported quantization schemes in 🤗 Transformers](https://huggingface.co/blog/overview-quantization-transformers)
    blog post.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试表明，AWQ量化在推理、文本生成方面是最快的，并且在文本生成方面具有最低的峰值内存。然而，AWQ在每个批处理大小上具有最大的前向延迟。要了解每种量化方法的优缺点的更详细讨论，请阅读[🤗
    Transformers中本地支持的量化方案概述](https://huggingface.co/blog/overview-quantization-transformers)博客文章。
- en: Fused AWQ modules
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 融合AWQ模块
- en: The [TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)
    model was benchmarked with `batch_size=1` with and without fused modules.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)模型在`batch_size=1`下进行了基准测试，有无融合模块。'
- en: Unfused module
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 未融合模块
- en: '| Batch Size | Prefill Length | Decode Length | Prefill tokens/s | Decode tokens/s
    | Memory (VRAM) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: 批处理大小 | 预填充长度 | 解码长度 | 预填充标记/秒 | 解码标记/秒 | 内存（VRAM） |
- en: '| --: | --: | --: | --: | --: | :-- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --: | --: | --: | --: | --: | :-- |'
- en: '| 1 | 32 | 32 | 60.0984 | 38.4537 | 4.50 GB (5.68%) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 32 | 32 | 60.0984 | 38.4537 | 4.50 GB (5.68%) |'
- en: '| 1 | 64 | 64 | 1333.67 | 31.6604 | 4.50 GB (5.68%) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 64 | 64 | 1333.67 | 31.6604 | 4.50 GB (5.68%) |'
- en: '| 1 | 128 | 128 | 2434.06 | 31.6272 | 4.50 GB (5.68%) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 128 | 128 | 2434.06 | 31.6272 | 4.50 GB (5.68%) |'
- en: '| 1 | 256 | 256 | 3072.26 | 38.1731 | 4.50 GB (5.68%) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 256 | 256 | 3072.26 | 38.1731 | 4.50 GB (5.68%) |'
- en: '| 1 | 512 | 512 | 3184.74 | 31.6819 | 4.59 GB (5.80%) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 512 | 512 | 3184.74 | 31.6819 | 4.59 GB (5.80%) |'
- en: '| 1 | 1024 | 1024 | 3148.18 | 36.8031 | 4.81 GB (6.07%) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1024 | 1024 | 3148.18 | 36.8031 | 4.81 GB (6.07%) |'
- en: '| 1 | 2048 | 2048 | 2927.33 | 35.2676 | 5.73 GB (7.23%) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2048 | 2048 | 2927.33 | 35.2676 | 5.73 GB (7.23%) |'
- en: Fused module
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 融合模块
- en: '| Batch Size | Prefill Length | Decode Length | Prefill tokens/s | Decode tokens/s
    | Memory (VRAM) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: 批处理大小 | 预填充长度 | 解码长度 | 预填充标记/秒 | 解码标记/秒 | 内存（VRAM） |
- en: '| --: | --: | --: | --: | --: | :-- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --: | --: | --: | --: | --: | :-- |'
- en: '| 1 | 32 | 32 | 81.4899 | 80.2569 | 4.00 GB (5.05%) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 32 | 32 | 81.4899 | 80.2569 | 4.00 GB (5.05%) |'
- en: '| 1 | 64 | 64 | 1756.1 | 106.26 | 4.00 GB (5.05%) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 64 | 64 | 1756.1 | 106.26 | 4.00 GB (5.05%) |'
- en: '| 1 | 128 | 128 | 2479.32 | 105.631 | 4.00 GB (5.06%) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 128 | 128 | 2479.32 | 105.631 | 4.00 GB (5.06%) |'
- en: '| 1 | 256 | 256 | 1813.6 | 85.7485 | 4.01 GB (5.06%) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 256 | 256 | 1813.6 | 85.7485 | 4.01 GB (5.06%) |'
- en: '| 1 | 512 | 512 | 2848.9 | 97.701 | 4.11 GB (5.19%) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 512 | 512 | 2848.9 | 97.701 | 4.11 GB (5.19%) |'
- en: '| 1 | 1024 | 1024 | 3044.35 | 87.7323 | 4.41 GB (5.57%) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1024 | 1024 | 3044.35 | 87.7323 | 4.41 GB (5.57%) |'
- en: '| 1 | 2048 | 2048 | 2715.11 | 89.4709 | 5.57 GB (7.04%) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2048 | 2048 | 2715.11 | 89.4709 | 5.57 GB (7.04%) |'
- en: The speed and throughput of fused and unfused modules were also tested with
    the [optimum-benchmark](https://github.com/huggingface/optimum-benchmark) library.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 融合和未融合模块的速度和吞吐量也经过了[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)库的测试。
- en: '![generate throughput per batch size](../Images/c73fad074a31449cad262be148000a7b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![每批生成吞吐量](../Images/c73fad074a31449cad262be148000a7b.png)'
- en: foward peak memory/batch size
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 前向峰值内存/批处理大小
- en: '![forward latency per batch size](../Images/a6ca88db796d9aca9bc439edc51f861d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![每批前向延迟](../Images/a6ca88db796d9aca9bc439edc51f861d.png)'
- en: generate throughput/batch size
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 每批生成吞吐量/批处理大小
