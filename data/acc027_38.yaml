- en: Accelerator
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速器
- en: 'Original text: [https://huggingface.co/docs/accelerate/package_reference/accelerator](https://huggingface.co/docs/accelerate/package_reference/accelerator)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/accelerate/package_reference/accelerator](https://huggingface.co/docs/accelerate/package_reference/accelerator)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    is the main class provided by 🤗 Accelerate. It serves at the main entry point
    for the API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)是🤗
    Accelerate提供的主要类。它作为API的主要入口点。'
- en: Quick adaptation of your code
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码的快速适应
- en: 'To quickly adapt your script to work on any kind of setup with 🤗 Accelerate
    just:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 将脚本快速适应任何类型的设置与🤗 Accelerate一起工作只需：
- en: Initialize an [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    object (that we will call `accelerator` throughout this page) as early as possible
    in your script.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽早在脚本中初始化一个[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)对象（我们将在本页中称之为`accelerator`）。
- en: Pass your dataloader(s), model(s), optimizer(s), and scheduler(s) to the [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的数据加载器、模型、优化器和调度器传递给[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)方法。
- en: Remove all the `.cuda()` or `.to(device)` from your code and let the `accelerator`
    handle the device placement for you.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从您的代码中删除所有`.cuda()`或`.to(device)`，让`accelerator`为您处理设备放置。
- en: Step three is optional, but considered a best practice.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是可选的，但被认为是最佳实践。
- en: Replace `loss.backward()` in your code with `accelerator.backward(loss)`
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的代码中用`accelerator.backward(loss)`替换`loss.backward()`
- en: Gather your predictions and labels before storing them or using them for metric
    computation using [gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在存储或使用预测和标签之前，使用[gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)来收集它们，以进行度量计算。
- en: Step five is mandatory when using distributed evaluation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行分布式评估时，第五步是强制的
- en: 'In most cases this is all that is needed. The next section lists a few more
    advanced use cases and nice features you should search for and replace by the
    corresponding methods of your `accelerator`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，这就是所需的。下一节列出了一些更高级的用例和您应该搜索并替换为`accelerator`相应方法的一些不错的功能：
- en: Advanced recommendations
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级建议
- en: Printing
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打印
- en: '`print` statements should be replaced by [print()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.print)
    to be printed once per process:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 将`print`语句替换为[print()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.print)以每个进程打印一次：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Executing processes
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行进程
- en: Once on a single server
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一次在单个服务器上
- en: 'For statements that should be executed once per server, use `is_local_main_process`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应在每台服务器上执行一次的语句，使用`is_local_main_process`：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A function can be wrapped using the [on_local_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_local_main_process)
    function to achieve the same behavior on a function’s execution:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用[on_local_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_local_main_process)函数来包装函数，以在函数执行时实现相同的行为：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Only ever once across all servers
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在所有服务器上仅一次
- en: 'For statements that should only ever be executed once, use `is_main_process`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应仅执行一次的语句，使用`is_main_process`：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A function can be wrapped using the [on_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_main_process)
    function to achieve the same behavior on a function’s execution:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用[on_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_main_process)函数来包装函数，以在函数执行时实现相同的行为：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: On specific processes
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在特定进程上
- en: 'If a function should be ran on a specific overall or local process index, there
    are similar decorators to achieve this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数应在特定的整体或本地进程索引上运行，有类似的装饰器可实现此目的：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Synchronicity control
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步控制
- en: Use [wait_for_everyone()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)
    to make sure all processes join that point before continuing. (Useful before a
    model save for instance).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[wait_for_everyone()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)确保所有进程在继续之前加入该点。（例如，在保存模型之前很有用）。
- en: Saving and loading
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存和加载
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Use [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    instead of `torch.save` to save a model. It will remove all model wrappers added
    during the distributed process, get the state_dict of the model and save it. The
    state_dict will be in the same precision as the model being trained.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)代替`torch.save`来保存模型。它将删除在分布式过程中添加的所有模型包装器，获取模型的state_dict并保存它。state_dict将与正在训练的模型具有相同的精度。
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    can also save a model into sharded checkpoints or with safetensors format. Here
    is an example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)还可以将模型保存为分片检查点或使用safetensors格式。这里是一个示例：'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 🤗 Transformers models
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 🤗 Transformers模型
- en: If you are using models from the [🤗 Transformers](https://huggingface.co/docs/transformers/)
    library, you can use the `.save_pretrained()` method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用[🤗 Transformers](https://huggingface.co/docs/transformers/)库中的模型，可以使用`.save_pretrained()`方法。
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will ensure your model stays compatible with other 🤗 Transformers functionality
    like the `.from_pretrained()` method.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保您的模型与其他🤗 Transformers功能保持兼容，如`.from_pretrained()`方法。
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Operations
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作
- en: Use [clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)
    instead of `torch.nn.utils.clip_grad_norm_` and [clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)
    instead of `torch.nn.utils.clip_grad_value`
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)代替`torch.nn.utils.clip_grad_norm_`，并使用[clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)代替`torch.nn.utils.clip_grad_value`
- en: Gradient Accumulation
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度累积
- en: 'To perform gradient accumulation use [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    and specify a gradient_accumulation_steps. This will also automatically ensure
    the gradients are synced or unsynced when on multi-device training, check if the
    step should actually be performed, and auto-scale the loss:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行梯度累积，请使用[accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)并指定gradient_accumulation_steps。这也将自动确保在多设备训练时同步或异步梯度，检查是否实际执行步骤，并自动缩放损失：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: GradientAccumulationPlugin
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GradientAccumulationPlugin
- en: '### `class accelerate.utils.GradientAccumulationPlugin`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.GradientAccumulationPlugin`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)'
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: A plugin to configure gradient accumulation behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于配置梯度累积行为的插件。
- en: Instead of passing `gradient_accumulation_steps` you can instantiate a GradientAccumulationPlugin
    and pass it to the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)’s
    `__init__` as `gradient_accumulation_plugin`. You can only pass either one of
    `gradient_accumulation_plugin` or `gradient_accumulation_steps` passing both will
    raise an error.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不要传递`gradient_accumulation_steps`，您可以实例化一个GradientAccumulationPlugin并将其传递给[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)的`__init__`作为`gradient_accumulation_plugin`。您只能传递`gradient_accumulation_plugin`或`gradient_accumulation_steps`中的一个，传递两者将引发错误。
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In addition to the number of steps, this also lets you configure whether or
    not you adjust your learning rate scheduler to account for the change in steps
    due to accumulation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了步数之外，这还让您配置是否调整学习率调度程序以考虑由于累积而导致的步骤变化。
- en: 'Overall API documentation:'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总体API文档：
- en: '### `class accelerate.Accelerator`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.Accelerator`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L153)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L153)'
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`device_placement` (`bool`, *optional*, defaults to `True`) — Whether or not
    the accelerator should put objects on device (tensors yielded by the dataloader,
    model, etc…).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *optional*, 默认为`True`) — 加速器是否应该将对象放在设备上（数据加载器产生的张量，模型等）。'
- en: '`split_batches` (`bool`, *optional*, defaults to `False`) — Whether or not
    the accelerator should split the batches yielded by the dataloaders across the
    devices. If `True` the actual batch size used will be the same on any kind of
    distributed processes, but it must be a round multiple of the `num_processes`
    you are using. If `False`, actual batch size used will be the one set in your
    script multiplied by the number of processes.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_batches` (`bool`, *optional*, 默认为`False`) — 加速器是否应该将数据加载器产生的批次分配到不同的设备上。如果为`True`，实际使用的批次大小将在任何类型的分布式进程上相同，但必须是您使用的`num_processes`的圆倍数。如果为`False`，实际使用的批次大小将是脚本中设置的批次大小乘以进程数。'
- en: '`mixed_precision` (`str`, *optional*) — Whether or not to use mixed precision
    training. Choose from ‘no’,‘fp16’,‘bf16 or ‘fp8’. Will default to the value in
    the environment variable `ACCELERATE_MIXED_PRECISION`, which will use the default
    value in the accelerate config of the current system or the flag passed with the
    `accelerate.launch` command. ‘fp8’ requires the installation of transformers-engine.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mixed_precision` (`str`, *optional*) — 是否使用混合精度训练。选择‘no’、‘fp16’、‘bf16’或‘fp8’。将默认为环境变量`ACCELERATE_MIXED_PRECISION`中的值，该值将使用当前系统的加速配置中的默认值或使用`accelerate.launch`命令传递的标志。‘fp8’需要安装transformers-engine。'
- en: '`gradient_accumulation_steps` (`int`, *optional*, default to 1) — The number
    of steps that should pass before gradients are accumulated. A number > 1 should
    be combined with `Accelerator.accumulate`. If not passed, will default to the
    value in the environment variable `ACCELERATE_GRADIENT_ACCUMULATION_STEPS`. Can
    also be configured through a `GradientAccumulationPlugin`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_steps` (`int`, *optional*, default to 1) — 在梯度累积之前应该经过的步数。数字大于1应该与`Accelerator.accumulate`结合使用。如果未传递，则默认为环境变量`ACCELERATE_GRADIENT_ACCUMULATION_STEPS`中的值。也可以通过`GradientAccumulationPlugin`进行配置。'
- en: '`cpu` (`bool`, *optional*) — Whether or not to force the script to execute
    on CPU. Will ignore GPU available if set to `True` and force the execution on
    one process only.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu` (`bool`, *optional*) — 是否强制脚本在CPU上执行。如果设置为`True`，将忽略GPU的可用性，并强制在一个进程上执行。'
- en: '`deepspeed_plugin` (`DeepSpeedPlugin`, *optional*) — Tweak your DeepSpeed related
    args using this argument. This argument is optional and can be configured directly
    using *accelerate config*'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepspeed_plugin` (`DeepSpeedPlugin`, *optional*) — 使用此参数调整您的DeepSpeed相关参数。此参数是可选的，可以直接使用*accelerate
    config*进行配置。'
- en: '`fsdp_plugin` (`FullyShardedDataParallelPlugin`, *optional*) — Tweak your FSDP
    related args using this argument. This argument is optional and can be configured
    directly using *accelerate config*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp_plugin` (`FullyShardedDataParallelPlugin`, *optional*) — 使用此参数调整您的FSDP相关参数。此参数是可选的，可以直接使用*accelerate
    config*进行配置。'
- en: '`megatron_lm_plugin` (`MegatronLMPlugin`, *optional*) — Tweak your MegatronLM
    related args using this argument. This argument is optional and can be configured
    directly using *accelerate config*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`megatron_lm_plugin` (`MegatronLMPlugin`, *optional*) — 使用此参数调整您的MegatronLM相关参数。此参数是可选的，可以直接使用*accelerate
    config*进行配置。'
- en: '`rng_types` (list of `str` or [RNGType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.RNGType))
    — The list of random number generators to synchronize at the beginning of each
    iteration in your prepared dataloaders. Should be one or several of:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rng_types`（`str`列表或[RNGType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.RNGType)）—
    在准备好的数据加载器的每次迭代开始时要同步的随机数生成器列表。应该是以下之一或几个：'
- en: '`"torch"`: the base torch random number generator'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"torch"`：基本torch随机数生成器'
- en: '`"cuda"`: the CUDA random number generator (GPU only)'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cuda"`：CUDA随机数生成器（仅限GPU）'
- en: '`"xla"`: the XLA random number generator (TPU only)'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"xla"`：XLA随机数生成器（仅限TPU）'
- en: '`"generator"`: the `torch.Generator` of the sampler (or batch sampler if there
    is no sampler in your dataloader) or of the iterable dataset (if it exists) if
    the underlying dataset is of that type.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"generator"`：采样器的`torch.Generator`（如果数据加载器中没有采样器，则为批次采样器）或可迭代数据集（如果存在）的生成器（如果基础数据集是该类型）。'
- en: Will default to `["torch"]` for PyTorch versions <=1.5.1 and `["generator"]`
    for PyTorch versions >= 1.6.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于PyTorch版本<=1.5.1，默认为`["torch"]`，对于PyTorch版本>= 1.6，默认为`["generator"]`。
- en: '`log_with` (list of `str`, [LoggerType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.LoggerType)
    or [GeneralTracker](/docs/accelerate/v0.27.2/en/package_reference/tracking#accelerate.tracking.GeneralTracker),
    *optional*) — A list of loggers to be setup for experiment tracking. Should be
    one or several of:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_with`（`str`列表，[LoggerType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.LoggerType)或[GeneralTracker](/docs/accelerate/v0.27.2/en/package_reference/tracking#accelerate.tracking.GeneralTracker)，*可选*）—
    要为实验跟踪设置的记录器列表。应该是以下之一或几个：'
- en: '`"all"`'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all"`'
- en: '`"tensorboard"`'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"tensorboard"`'
- en: '`"wandb"`'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"wandb"`'
- en: '`"comet_ml"` If `"all"` is selected, will pick up all available trackers in
    the environment and initialize them. Can also accept implementations of `GeneralTracker`
    for custom trackers, and can be combined with `"all"`.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"comet_ml"` 如果选择`"all"`，将捡起环境中所有可用的跟踪器并初始化它们。还可以接受`GeneralTracker`的自定义跟踪器的实现，并且可以与`"all"`组合使用。'
- en: '`project_config` (`ProjectConfiguration`, *optional*) — A configuration for
    how saving the state can be handled.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`project_config`（`ProjectConfiguration`，*可选*）— 用于处理状态保存的配置。'
- en: '`project_dir` (`str`, `os.PathLike`, *optional*) — A path to a directory for
    storing data such as logs of locally-compatible loggers and potentially saved
    checkpoints.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`project_dir`（`str`，`os.PathLike`，*可选*）— 用于存储数据的目录路径，例如本地兼容日志记录器的日志和可能的保存检查点。'
- en: '`dispatch_batches` (`bool`, *optional*) — If set to `True`, the dataloader
    prepared by the Accelerator is only iterated through on the main process and then
    the batches are split and broadcast to each process. Will default to `True` for
    `DataLoader` whose underlying dataset is an `IterableDataset`, `False` otherwise.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dispatch_batches`（`bool`，*可选*）— 如果设置为`True`，则由加速器准备的数据加载器仅在主进程上进行迭代，然后将批次分割并广播到每个进程。对于其基础数据集为`IterableDataset`的`DataLoader`，默认为`True`，否则为`False`。'
- en: '`even_batches` (`bool`, *optional*, defaults to `True`) — If set to `True`,
    in cases where the total batch size across all processes does not exactly divide
    the dataset, samples at the start of the dataset will be duplicated so the batch
    can be divided equally among all workers.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`even_batches`（`bool`，*可选*，默认为`True`）— 如果设置为`True`，在所有进程的总批次大小不能完全整除数据集的情况下，数据集开头的样本将被复制，以便批次可以在所有工作进程之间均匀分配。'
- en: '`use_seedable_sampler` (`bool`, *optional*, defaults to `False`) — Whether
    or not use a fully seedable random sampler (`SeedableRandomSampler`). Ensures
    training results are fully reproducable using a different sampling technique.
    While seed-to-seed results may differ, on average the differences are neglible
    when using multiple different seeds to compare. Should also be ran with [set_seed()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.set_seed)
    each time for the best results.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_seedable_sampler`（`bool`，*可选*，默认为`False`）— 是否使用完全可种子的随机采样器（`SeedableRandomSampler`）。确保使用不同的采样技术时训练结果是完全可重现的。虽然种子到种子的结果可能不同，但平均而言，使用多个不同的种子进行比较时差异微不足道。每次都应该与[set_seed()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.set_seed)一起运行以获得最佳结果。'
- en: '`step_scheduler_with_optimizer` (`bool`, *optional`, defaults to` True`) --
    Set` True`if the learning rate scheduler is stepped at the same time as the optimizer,`False`
    if only done under certain circumstances (at the end of each epoch, for instance).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`step_scheduler_with_optimizer`（`bool`，*可选*，默认为`True`）-- 如果学习率调度程序与优化器同时进行步进，则设置为`True`，如果仅在某些情况下进行（例如在每个时期结束时），则设置为`False`。'
- en: '`kwargs_handlers` (`list[KwargHandler]`, *optional*) — A list of `KwargHandler`
    to customize how the objects related to distributed training or mixed precision
    are created. See [kwargs](kwargs) for more information.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs_handlers`（`list[KwargHandler]`，*可选*）— 用于自定义与分布式训练或混合精度相关的对象如何创建的`KwargHandler`列表。有关更多信息，请参阅[kwargs](kwargs)。'
- en: '`dynamo_backend` (`str` or `DynamoBackend`, *optional*, defaults to `"no"`)
    — Set to one of the possible dynamo backends to optimize your training with torch
    dynamo.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo_backend`（`str`或`DynamoBackend`，*可选*，默认为`"no"`）— 设置为可能的dynamo后端之一，以优化使用torch
    dynamo进行训练。'
- en: '`gradient_accumulation_plugin` (`GradientAccumulationPlugin`, *optional*) —
    A configuration for how gradient accumulation should be handled, if more tweaking
    than just the `gradient_accumulation_steps` is needed.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_plugin`（`GradientAccumulationPlugin`，*可选*）— 用于处理梯度累积的配置，如果需要比仅使用`gradient_accumulation_steps`更多的调整。'
- en: Creates an instance of an accelerator for distributed training (on multi-GPU,
    TPU) or mixed precision training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个用于分布式训练（在多GPU、TPU上）或混合精度训练的加速器实例。
- en: '**Available attributes:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**可用属性：**'
- en: '`device` (`torch.device`) — The device to use.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`torch.device`）— 要使用的设备。'
- en: '`distributed_type` ([DistributedType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.DistributedType))
    — The distributed training configuration.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distributed_type`（[DistributedType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.DistributedType)）—
    分布式训练配置。'
- en: '`local_process_index` (`int`) — The process index on the current machine.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_process_index`（`int`）— 当前机器上的进程索引。'
- en: '`mixed_precision` (`str`) — The configured mixed precision mode.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mixed_precision`（`str`）-配置的混合精度模式。'
- en: '`num_processes` (`int`) — The total number of processes used for training.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_processes`（`int`）-用于训练的进程总数。'
- en: '`optimizer_step_was_skipped` (`bool`) — Whether or not the optimizer update
    was skipped (because of gradient overflow in mixed precision), in which case the
    learning rate should not be changed.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer_step_was_skipped`（`bool`）-是否跳过了优化器更新（因为在混合精度中梯度溢出），在这种情况下学习率不应该改变。'
- en: '`process_index` (`int`) — The overall index of the current process among all
    processes.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process_index`（`int`）-当前进程在所有进程中的总体索引。'
- en: '`state` ([AcceleratorState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.AcceleratorState))
    — The distributed setup state.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`（[AcceleratorState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.AcceleratorState)）-分布式设置状态。'
- en: '`sync_gradients` (`bool`) — Whether the gradients are currently being synced
    across all processes.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sync_gradients`（`bool`）-当前是否正在跨所有进程同步梯度。'
- en: '`use_distributed` (`bool`) — Whether the current configuration is for distributed
    training.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_distributed`（`bool`）-当前配置是否用于分布式训练。'
- en: '#### `accumulate`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accumulate`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L967)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L967)'
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`*models` (list of `torch.nn.Module`) — PyTorch Modules that were prepared
    with `Accelerator.prepare`. Models passed to `accumulate()` will skip gradient
    syncing during backward pass in distributed training'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*models`（`torch.nn.Module`列表）-使用`Accelerator.prepare`准备的PyTorch模块。传递给`accumulate()`的模型将在分布式训练中的反向传递期间跳过梯度同步'
- en: A context manager that will lightly wrap around and perform gradient accumulation
    automatically
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一个上下文管理器，将轻松包装并自动执行梯度累积
- en: 'Example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#### `autocast`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `autocast`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3109)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3109)'
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Will apply automatic mixed-precision inside the block inside this context manager,
    if it is enabled. Nothing different will happen otherwise.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，将在此上下文管理器内部的块中应用自动混合精度。否则不会发生任何不同。
- en: A different `autocast_handler` can be passed in to override the one set in the
    `Accelerator` object. This is useful in blocks under `autocast` where you want
    to revert to fp32.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可以传入不同的`autocast_handler`来覆盖`Accelerator`对象中设置的处理程序。这在`autocast`下的块中很有用，您想要恢复到fp32。
- en: 'Example:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#### `backward`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `backward`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1938)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1938)'
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Scales the gradients in accordance to the `GradientAccumulationPlugin` and calls
    the correct `backward()` based on the configuration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`GradientAccumulationPlugin`缩放梯度，并根据配置调用正确的`backward()`。
- en: Should be used in lieu of `loss.backward()`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 应该用来代替`loss.backward()`。
- en: 'Example:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#### `check_trigger`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `check_trigger`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1994)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1994)'
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Checks if the internal trigger tensor has been set to 1 in any of the processes.
    If so, will return `True` and reset the trigger tensor to 0.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 检查内部触发张量是否在任何进程中设置为1。如果是，则将返回`True`并将触发张量重置为0。
- en: 'Note: Does not require `wait_for_everyone()`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不需要`wait_for_everyone()`
- en: 'Example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#### `clear`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `clear`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2973)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2973)'
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Alias for `Accelerate.free_memory`, releases all references to the internal
    objects stored and call the garbage collector. You should call this method between
    two trainings with different models/optimizers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 别名为`Accelerate.free_memory`，释放所有对存储的内部对象的引用并调用垃圾回收器。您应该在两个具有不同模型/优化器的训练之间调用此方法。
- en: 'Example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#### `clip_grad_norm_`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `clip_grad_norm_`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2066)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2066)'
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Returns
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: Total norm of the parameter gradients (viewed as a single vector).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 参数梯度的总范数（视为单个向量）。
- en: Should be used in place of `torch.nn.utils.clip_grad_norm_`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 应该用来代替`torch.nn.utils.clip_grad_norm_`。
- en: 'Example:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#### `clip_grad_value_`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `clip_grad_value_`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2104)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2104)'
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Should be used in place of `torch.nn.utils.clip_grad_value_`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 应该用来代替`torch.nn.utils.clip_grad_value_`。
- en: 'Example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#### `free_memory`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `free_memory`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2948)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2948)'
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Will release all references to the internal objects stored and call the garbage
    collector. You should call this method between two trainings with different models/optimizers.
    Also will reset `Accelerator.step` to 0.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将释放所有对存储的内部对象的引用并调用垃圾回收器。您应该在两个具有不同模型/优化器的训练之间调用此方法。还将`Accelerator.step`重置为0。
- en: 'Example:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#### `gather`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `gather`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2131)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2131)'
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tensor` (`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`)
    — The tensors to gather across all processes.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`（`torch.Tensor`，或`torch.Tensor`的嵌套元组/列表/字典）-要在所有进程中收集的张量。'
- en: Returns
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`，或`torch.Tensor`的嵌套元组/列表/字典'
- en: The gathered tensor(s). Note that the first dimension of the result is *num_processes*
    multiplied by the first dimension of the input tensors.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的张量。请注意，结果的第一个维度是* num_processes *乘以输入张量的第一个维度。
- en: Gather the values in *tensor* across all processes and concatenate them on the
    first dimension. Useful to regroup the predictions from all processes when doing
    evaluation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 收集*张量*在所有进程中的值，并在第一维上将它们连接起来。在进行评估时，有助于重新组合所有进程的预测。
- en: 'Note: This gather happens in all processes.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此收集在所有进程中发生。
- en: 'Example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#### `gather_for_metrics`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `gather_for_metrics`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2163)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2163)'
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input` (`torch.Tensor`, `object`, a nested tuple/list/dictionary of `torch.Tensor`,
    or a nested tuple/list/dictionary of `object`) — The tensors or objects for calculating
    metrics across all processes'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input` (`torch.Tensor`, `object`, `torch.Tensor`的嵌套元组/列表/字典，或`object`的嵌套元组/列表/字典)
    — 用于计算所有进程间指标的张量或对象'
- en: Gathers `input_data` and potentially drops duplicates in the last batch if on
    a distributed system. Should be used for gathering the inputs and targets for
    metric calculation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 收集`input_data`并在分布式系统上可能删除最后一批中的重复项。应用于收集用于指标计算的输入和目标。
- en: 'Example:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '#### `get_state_dict`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_state_dict`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3017)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3017)'
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — A PyTorch model sent through [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) — 通过[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)发送的PyTorch模型'
- en: '`unwrap` (`bool`, *optional*, defaults to `True`) — Whether to return the original
    underlying state_dict of `model` or to return the wrapped state_dict'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unwrap` (`bool`, *可选*, 默认为`True`) — 是否返回`model`的原始底层`state_dict`或返回包装的`state_dict`'
- en: Returns
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`dict`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`dict`'
- en: The state dictionary of the model potentially without full precision.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的状态字典，可能不是完全精确。
- en: Returns the state dictionary of a model sent through [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    potentially without full precision.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 返回通过[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)发送的模型的状态字典，可能不是完全精确。
- en: 'Example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE37]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#### `get_tracker`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_tracker`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2400)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2400)'
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`name` (`str`) — The name of a tracker, corresponding to the `.name` property.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str`) — 一个跟踪器的名称，对应于`.name`属性。'
- en: '`unwrap` (`bool`) — Whether to return the internal tracking mechanism or to
    return the wrapped tracker instead (recommended).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unwrap` (`bool`) — 是否返回内部跟踪机制或返回包装的跟踪器（推荐）。'
- en: Returns
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`GeneralTracker`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`GeneralTracker`'
- en: The tracker corresponding to `name` if it exists.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在，则返回与`name`对应的跟踪器。
- en: Returns a `tracker` from `self.trackers` based on `name` on the main process
    only.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在主进程上基于`name`从`self.trackers`返回一个`tracker`。
- en: 'Example:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#### `join_uneven_inputs`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `join_uneven_inputs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1001)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1001)'
- en: '[PRE40]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`joinables` (`list[torch.distributed.algorithms.Joinable]`) — A list of models
    or optimizers that subclass `torch.distributed.algorithms.Joinable`. Most commonly,
    a PyTorch Module that was prepared with `Accelerator.prepare` for DistributedDataParallel
    training.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`joinables` (`list[torch.distributed.algorithms.Joinable]`) — 一个子类为`torch.distributed.algorithms.Joinable`的模型或优化器列表。最常见的是，使用`Accelerator.prepare`为分布式数据并行训练准备的PyTorch模块。'
- en: '`even_batches` (`bool`, *optional*) — If set, this will override the value
    of `even_batches` set in the `Accelerator`. If it is not provided, the default
    `Accelerator` value wil be used.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`even_batches` (`bool`, *可选*) — 如果设置，这将覆盖在`Accelerator`中设置的`even_batches`的值。如果未提供，则将使用默认的`Accelerator`值。'
- en: A context manager that facilitates distributed training or evaluation on uneven
    inputs, which acts as a wrapper around `torch.distributed.algorithms.join`. This
    is useful when the total batch size does not evenly divide the length of the dataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一个上下文管理器，用于在不均匀输入上进行分布式训练或评估，作为`torch.distributed.algorithms.join`周围的包装器。当总批量大小不能完全整除数据集的长度时，这是有用的。
- en: '`join_uneven_inputs` is only supported for Distributed Data Parallel training
    on multiple GPUs. For any other configuration, this method will have no effect.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`join_uneven_inputs`仅支持在多个GPU上进行分布式数据并行训练。对于任何其他配置，此方法将不起作用。'
- en: Overidding `even_batches` will not affect iterable-style data loaders.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖`even_batches`不会影响可迭代样式的数据加载器。
- en: 'Example:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '#### `load_state`'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_state`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2816)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2816)'
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_dir` (`str` or `os.PathLike`) — The name of the folder all relevant
    weights and states were saved in. Can be `None` if `automatic_checkpoint_naming`
    is used, and will pick up from the latest checkpoint.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_dir` (`str`或`os.PathLike`) — 所有相关权重和状态保存在的文件夹的名称。如果使用`automatic_checkpoint_naming`，则可以为`None`，并将从最新的检查点中获取。'
- en: '`load_model_func_kwargs` (`dict`, *optional*) — Additional keyword arguments
    for loading model which can be passed to the underlying load function, such as
    optional arguments for DeepSpeed’s `load_checkpoint` function or a `map_location`
    to load the model and optimizer on.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_model_func_kwargs` (`dict`, *可选*) — 用于加载模型的附加关键字参数，可以传递给底层加载函数，例如DeepSpeed的`load_checkpoint`函数的可选参数或`map_location`以加载模型和优化器。'
- en: Loads the current states of the model, optimizer, scaler, RNG generators, and
    registered objects.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型、优化器、缩放器、RNG生成器和已注册对象的当前状态。
- en: Should only be used in conjunction with [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state).
    If a file is not registered for checkpointing, it will not be loaded if stored
    in the directory.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 应该与 [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    结合使用。如果未注册用于检查点的文件，则如果存储在目录中，则不会加载。
- en: 'Example:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#### `local_main_process_first`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `local_main_process_first`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L830)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L830)'
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Lets the local main process go inside a with block.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让本地主进程在 with 块内执行。
- en: The other processes will enter the with block after the main process exits.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其他进程将在主进程退出后进入 with 块。
- en: 'Example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#### `main_process_first`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `main_process_first`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L808)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L808)'
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Lets the main process go first inside a with block.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让主进程在 with 块内先执行。
- en: The other processes will enter the with block after the main process exits.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其他进程将在主进程退出后进入 with 块。
- en: 'Example:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '#### `no_sync`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `no_sync`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L852)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L852)'
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — PyTorch Module that was prepared with `Accelerator.prepare`'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) — 使用 `Accelerator.prepare` 准备的 PyTorch 模块。'
- en: A context manager to disable gradient synchronizations across DDP processes
    by calling `torch.nn.parallel.DistributedDataParallel.no_sync`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一个上下文管理器，通过调用 `torch.nn.parallel.DistributedDataParallel.no_sync` 来禁用 DDP 进程之间的梯度同步。
- en: If `model` is not in DDP, this context manager does nothing
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `model` 不在 DDP 中，则此上下文管理器不起作用
- en: 'Example:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE49]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#### `on_last_process`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_last_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L676)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L676)'
- en: '[PRE50]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`function` (`Callable`) — The function to decorate.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`) — 要装饰的函数。'
- en: A decorator that will run the decorated function on the last process only. Can
    also be called using the `PartialState` class.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一个装饰器，只会在最后一个进程上运行装饰的函数。也可以使用 `PartialState` 类调用。
- en: 'Example:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE51]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `on_local_main_process`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_local_main_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L634)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L634)'
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`function` (`Callable`) — The function to decorate.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`) — 要装饰的函数。'
- en: A decorator that will run the decorated function on the local main process only.
    Can also be called using the `PartialState` class.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一个装饰器，只会在本地主进程上运行装饰的函数。也可以使用 `PartialState` 类调用。
- en: 'Example:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE53]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '#### `on_local_process`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_local_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L760)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L760)'
- en: '[PRE54]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`function` (`Callable`, *optional*) — The function to decorate.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`, *可选*) — 要装饰的函数。'
- en: '`local_process_index` (`int`, *optional*) — The index of the local process
    on which to run the function.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_process_index` (`int`, *可选*) — 要运行函数的本地进程的索引。'
- en: A decorator that will run the decorated function on a given local process index
    only. Can also be called using the `PartialState` class.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一个装饰器，只会在给定的本地进程索引上运行装饰的函数。也可以使用 `PartialState` 类调用。
- en: 'Example:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE55]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '#### `on_main_process`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_main_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L595)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L595)'
- en: '[PRE56]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`function` (`Callable`) — The function to decorate.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`) — 要装饰的函数。'
- en: A decorator that will run the decorated function on the main process only. Can
    also be called using the `PartialState` class.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一个装饰器，只会在主进程上运行装饰的函数。也可以使用 `PartialState` 类调用。
- en: 'Example:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE57]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `on_process`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L715)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L715)'
- en: '[PRE58]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`function` (`Callable`, `optional`) — The function to decorate.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`, `可选`) — 要装饰的函数。'
- en: '`process_index` (`int`, `optional`) — The index of the process on which to
    run the function.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process_index` (`int`, `可选`) — 要运行函数的进程的索引。'
- en: A decorator that will run the decorated function on a given process index only.
    Can also be called using the `PartialState` class.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一个装饰器，只会在给定的进程索引上运行装饰的函数。也可以使用 `PartialState` 类调用。
- en: 'Example:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE59]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '#### `pad_across_processes`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pad_across_processes`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2261)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2261)'
- en: '[PRE60]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) — The data to gather.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`（`torch.Tensor`的嵌套列表/元组/字典） — 要收集的数据。'
- en: '`dim` (`int`, *optional*, defaults to 0) — The dimension on which to pad.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim` (`int`, *可选*, 默认为 0) — 要填充的维度。'
- en: '`pad_index` (`int`, *optional*, defaults to 0) — The value with which to pad.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_index` (`int`, *可选*, 默认为 0) — 用于填充的值。'
- en: '`pad_first` (`bool`, *optional*, defaults to `False`) — Whether to pad at the
    beginning or the end.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_first` (`bool`, *可选*, 默认为 `False`) — 是否在开头或结尾填充。'
- en: Returns
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`，或 `torch.Tensor` 的嵌套元组/列表/字典'
- en: The padded tensor(s).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 填充后的张量。
- en: Recursively pad the tensors in a nested list/tuple/dictionary of tensors from
    all devices to the same size so they can safely be gathered.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 递归填充嵌套列表/元组/张量字典中的张量，使它们可以安全地被收集到相同的大小。
- en: 'Example:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE61]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#### `prepare`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1116)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1116)'
- en: '[PRE62]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`*args` (list of objects) — Any of the following type of objects:'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*args`（对象列表） — 以下类型的任何对象：'
- en: '`torch.utils.data.DataLoader`: PyTorch Dataloader'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.utils.data.DataLoader`: PyTorch数据加载器'
- en: '`torch.nn.Module`: PyTorch Module'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`: PyTorch模块'
- en: '`torch.optim.Optimizer`: PyTorch Optimizer'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.Optimizer`: PyTorch优化器'
- en: '`torch.optim.lr_scheduler.LRScheduler`: PyTorch LR Scheduler'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.lr_scheduler.LRScheduler`: PyTorch LR调度程序'
- en: '`device_placement` (`list[bool]`, *optional*) — Used to customize whether automatic
    device placement should be performed for each object passed. Needs to be a list
    of the same length as `args`. Not compatible with DeepSpeed or FSDP.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`list[bool]`, *可选*) — 用于自定义是否应为传递的每个对象执行自动设备放置。需要与 `args`
    长度相同的列表。与DeepSpeed或FSDP不兼容。'
- en: Prepare all objects passed in `args` for distributed training and mixed precision,
    then return them in the same order.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 准备传递的所有对象进行分布式训练和混合精度，然后以相同顺序返回它们。
- en: You don’t need to prepare a model if you only use it for inference without any
    kind of mixed precision
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仅用于推断而不涉及任何混合精度，则无需准备模型
- en: 'Examples:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE63]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '#### `prepare_data_loader`'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_data_loader`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1812)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1812)'
- en: '[PRE65]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`data_loader` (`torch.utils.data.DataLoader`) — A vanilla PyTorch DataLoader
    to prepare'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_loader` (`torch.utils.data.DataLoader`) — 一个普通的PyTorch数据加载器要准备'
- en: '`device_placement` (`bool`, *optional*) — Whether or not to place the batches
    on the proper device in the prepared dataloader. Will default to `self.device_placement`.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *可选*) — 是否将批次放置在准备好的数据加载器中的正确设备上。默认为 `self.device_placement`。'
- en: '`slice_fn_for_dispatch` (`Callable`, *optional*`) -- If passed, this function
    will be used to slice tensors across` num_processes`. Will default to [slice_tensors()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.slice_tensors).
    This argument is used only when` dispatch_batches`is set to`True` and will be
    ignored otherwise.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_fn_for_dispatch` (`Callable`, *可选*`) -- 如果传递，将使用此函数在` num_processes`上切片张量。默认为[slice_tensors()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.slice_tensors)。此参数仅在`
    dispatch_batches`设置为`True`时使用，否则将被忽略。'
- en: Prepares a PyTorch DataLoader for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为在任何分布式设置中训练准备一个PyTorch数据加载器。建议使用[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)代替。
- en: 'Example:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE66]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '#### `prepare_model`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1252)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1252)'
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — A PyTorch model to prepare. You don’t need to
    prepare a model if it is used only for inference without any kind of mixed precision'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) — 一个要准备的PyTorch模型。如果仅用于推断而不涉及任何混合精度，则无需准备模型'
- en: '`device_placement` (`bool`, *optional*) — Whether or not to place the model
    on the proper device. Will default to `self.device_placement`.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *可选*) — 是否将模型放置在正确的设备上。默认为 `self.device_placement`。'
- en: '`evaluation_mode` (`bool`, *optional*, defaults to `False`) — Whether or not
    to set the model for evaluation only, by just applying mixed precision and `torch.compile`
    (if configured in the `Accelerator` object).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluation_mode` (`bool`, *可选*, 默认为 `False`) — 是否仅将模型设置为评估模式，只需应用混合精度和 `torch.compile`（如果在
    `Accelerator` 对象中配置）。'
- en: Prepares a PyTorch model for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 为在任何分布式设置中训练准备一个PyTorch模型。建议使用[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)代替。
- en: 'Example:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE68]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '#### `prepare_optimizer`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_optimizer`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1864)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1864)'
- en: '[PRE69]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Parameters
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`torch.optim.Optimizer`) — A vanilla PyTorch optimizer to prepare'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`torch.optim.Optimizer`) — 一个普通的PyTorch优化器要准备'
- en: '`device_placement` (`bool`, *optional*) — Whether or not to place the optimizer
    on the proper device. Will default to `self.device_placement`.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *可选*) — 是否将优化器放置在正确的设备上。默认为 `self.device_placement`。'
- en: Prepares a PyTorch Optimizer for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为在任何分布式设置中训练准备一个PyTorch优化器。建议使用[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)代替。
- en: 'Example:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE70]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '#### `prepare_scheduler`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_scheduler`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1897)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1897)'
- en: '[PRE71]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Parameters
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`scheduler` (`torch.optim.lr_scheduler.LRScheduler`) — A vanilla PyTorch scheduler
    to prepare'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`torch.optim.lr_scheduler.LRScheduler`) — 要准备的一个普通的PyTorch调度程序'
- en: Prepares a PyTorch Scheduler for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为在任何分布式设置中训练准备一个PyTorch调度程序。建议使用[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)代替。
- en: 'Example:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE72]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '#### `print`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `print`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1084)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1084)'
- en: '[PRE73]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Drop in replacement of `print()` to only print once per server.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`print()`的替代方法，每个服务器只打印一次。'
- en: 'Example:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE74]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '#### `reduce`'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reduce`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2225)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2225)'
- en: '[PRE75]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Parameters
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tensor` (`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`)
    — The tensors to reduce across all processes.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`（`torch.Tensor`，或`torch.Tensor`的嵌套元组/列表/字典）— 要在所有进程中减少的张量。'
- en: '`reduction` (`str`, *optional*, defaults to “sum”) — A reduction type, can
    be one of ‘sum’, ‘mean’, or ‘none’. If ‘none’, will not perform any operation.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduction`（`str`，*可选*，默认为“sum”）— 减少类型，可以是“sum”、“mean”或“none”中的一个。如果是“none”，则不执行任何操作。'
- en: '`scale` (`float`, *optional*, defaults to 1.0) — A default scaling value to
    be applied after the reduce, only valied on XLA.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`（`float`，*可选*，默认为1.0）— 在减少后应用的默认缩放值，仅在XLA上有效。'
- en: Returns
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`，或`torch.Tensor`的嵌套元组/列表/字典'
- en: The reduced tensor(s).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 减少的张量。
- en: Reduce the values in *tensor* across all processes based on *reduction*.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*reduction*在*张量*中减少所有进程中的值。
- en: 'Note: All processes get the reduced value.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：所有进程都会得到减少的值。
- en: 'Example:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE76]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '#### `register_for_checkpointing`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_for_checkpointing`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3073)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3073)'
- en: '[PRE77]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Makes note of `objects` and will save or load them in during `save_state` or
    `load_state`.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`objects`并将在`save_state`或`load_state`期间保存或加载它们。
- en: These should be utilized when the state is being loaded or saved in the same
    script. It is not designed to be used in different scripts.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应该在相同脚本中加载或保存状态时使用。不应该在不同脚本中使用。
- en: Every `object` must have a `load_state_dict` and `state_dict` function to be
    stored.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`object`必须有一个`load_state_dict`和`state_dict`函数以进行存储。
- en: 'Example:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE78]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '#### `register_load_state_pre_hook`'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_load_state_pre_hook`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2785)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2785)'
- en: '[PRE79]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Parameters
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hook` (`Callable`) — A function to be called in [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)
    before `load_checkpoint`.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hook`（`Callable`）— 在[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)之前调用的函数，用于`load_checkpoint`。'
- en: Returns
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.utils.hooks.RemovableHandle`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.utils.hooks.RemovableHandle`'
- en: a handle that can be used to remove the added hook by calling `handle.remove()`
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 一个句柄，可以通过调用`handle.remove()`来移除添加的钩子
- en: Registers a pre hook to be run before `load_checkpoint` is called in [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 注册一个预钩子，在[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)中调用`load_checkpoint`之前运行。
- en: 'The hook should have the following signature:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '钩子应该具有以下签名:'
- en: '`hook(models: list[torch.nn.Module], input_dir: str) -> None`'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '`hook(models: list[torch.nn.Module], input_dir: str) -> None`'
- en: The `models` argument are the models as saved in the accelerator state under
    `accelerator._models`, and the `input_dir` argument is the `input_dir` argument
    passed to [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`models`参数是保存在加速器状态下的模型`accelerator._models`，`input_dir`参数是传递给[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)的`input_dir`参数。'
- en: Should only be used in conjunction with [Accelerator.register_save_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_save_state_pre_hook).
    Can be useful to load configurations in addition to model weights. Can also be
    used to overwrite model loading with a customized method. In this case, make sure
    to remove already loaded models from the models list.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 应该只与[Accelerator.register_save_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_save_state_pre_hook)一起使用。可以用于加载配置以及模型权重。也可以用于使用自定义方法覆盖模型加载。在这种情况下，请确保从模型列表中删除已加载的模型。
- en: '#### `register_save_state_pre_hook`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_save_state_pre_hook`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2619)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2619)'
- en: '[PRE80]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Parameters
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hook` (`Callable`) — A function to be called in [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    before `save_checkpoint`.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hook`（`Callable`）— 在[Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)之前调用的函数，用于`save_checkpoint`。'
- en: Returns
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.utils.hooks.RemovableHandle`'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.utils.hooks.RemovableHandle`'
- en: a handle that can be used to remove the added hook by calling `handle.remove()`
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 一个句柄，可以通过调用`handle.remove()`来移除添加的钩子
- en: Registers a pre hook to be run before `save_checkpoint` is called in [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 注册一个预钩子，在[Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)中调用`save_checkpoint`之前运行。
- en: 'The hook should have the following signature:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 钩子应该具有以下签名：
- en: '`hook(models: list[torch.nn.Module], weights: list[dict[str, torch.Tensor]],
    input_dir: str) -> None`'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '`hook(models: list[torch.nn.Module], weights: list[dict[str, torch.Tensor]],
    input_dir: str) -> None`'
- en: The `models` argument are the models as saved in the accelerator state under
    `accelerator._models`, `weigths` argument are the state dicts of the `models`,
    and the `input_dir` argument is the `input_dir` argument passed to [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`models`参数是保存在加速器状态下的模型`accelerator._models`，`weights`参数是`models`的状态字典，`input_dir`参数是传递给[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)的`input_dir`参数。'
- en: Should only be used in conjunction with [Accelerator.register_load_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_load_state_pre_hook).
    Can be useful to save configurations in addition to model weights. Can also be
    used to overwrite model saving with a customized method. In this case, make sure
    to remove already loaded weights from the weights list.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 应仅与[Accelerator.register_load_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_load_state_pre_hook)一起使用。除了模型权重外，保存配置也很有用。也可以用于使用自定义方法覆盖模型保存。在这种情况下，请确保从权重列表中删除已加载的权重。
- en: '#### `save`'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2482)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2482)'
- en: '[PRE81]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Parameters
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`obj` (`object`) — The object to save.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`obj`（`object`）— 要保存的对象。'
- en: '`f` (`str` or `os.PathLike`) — Where to save the content of `obj`.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f`（`str`或`os.PathLike`）— 要保存`obj`内容的位置。'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) — Whether to
    save `obj` using `safetensors`'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`（`bool`，*可选*，默认为`False`）— 是否使用`safetensors`保存`obj`'
- en: Save the object passed to disk once per machine. Use in place of `torch.save`.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 将传递给磁盘的对象每台机器保存一次。用于替代`torch.save`。
- en: 'Note: If `save_on_each_node` was passed in as a `ProjectConfiguration`, will
    save the object once per node, rather than only once on the main node.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果`save_on_each_node`作为`ProjectConfiguration`传入，则会在每个节点上保存对象一次，而不仅在主节点上保存一次。
- en: 'Example:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE82]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '#### `save_model`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2512)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2512)'
- en: '[PRE83]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Parameters
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str` or `os.PathLike`) — Directory to which to save. Will
    be created if it doesn’t exist.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`（`str`或`os.PathLike`）— 要保存到的目录。如果不存在，将会创建。'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) — The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size`（`int`或`str`，*可选*，默认为`"10GB"`）— 在分片之前的检查点的最大大小。然后，检查点分片将每个大小都小于此大小。如果表示为字符串，则需要是数字后跟一个单位（如`"5MB"`）。'
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型的单个权重大于`max_shard_size`，则它将在自己的检查点分片中，该分片将大于`max_shard_size`。
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether to
    save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`（`bool`，*可选*，默认为`True`）— 是否使用`safetensors`或传统的 PyTorch
    方式（使用`pickle`）保存模型。'
- en: Save a model so that it can be re-loaded using load_checkpoint_in_model
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型，以便可以使用`load_checkpoint_in_model`重新加载
- en: 'Example:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE84]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '#### `save_state`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_state`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2651)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2651)'
- en: '[PRE85]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Parameters
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`output_dir` (`str` or `os.PathLike`) — The name of the folder to save all
    relevant weights and states.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir`（`str`或`os.PathLike`）— 保存所有相关权重和状态的文件夹的名称。'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether to
    save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`（`bool`，*可选*，默认为`True`）— 是否使用`safetensors`或传统的 PyTorch
    方式（使用`pickle`）保存模型。'
- en: '`save_model_func_kwargs` (`dict`, *optional*) — Additional keyword arguments
    for saving model which can be passed to the underlying save function, such as
    optional arguments for DeepSpeed’s `save_checkpoint` function.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_model_func_kwargs`（`dict`，*可选*）— 用于保存模型的额外关键字参数，可以传递给底层保存函数，例如 DeepSpeed
    的`save_checkpoint`函数的可选参数。'
- en: Saves the current states of the model, optimizer, scaler, RNG generators, and
    registered objects to a folder.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型、优化器、缩放器、RNG 生成器和已注册对象的当前状态保存到一个文件夹中。
- en: If a `ProjectConfiguration` was passed to the `Accelerator` object with `automatic_checkpoint_naming`
    enabled then checkpoints will be saved to `self.project_dir/checkpoints`. If the
    number of current saves is greater than `total_limit` then the oldest save is
    deleted. Each checkpoint is saved in seperate folders named `checkpoint_<iteration>`.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将`ProjectConfiguration`传递给启用了`automatic_checkpoint_naming`的`Accelerator`对象，则检查点将保存在`self.project_dir/checkpoints`中。如果当前保存的数量大于`total_limit`，则将删除最旧的保存。每个检查点都保存在名为`checkpoint_<iteration>`的单独文件夹中。
- en: Otherwise they are just saved to `output_dir`.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 否则它们只保存到`output_dir`。
- en: Should only be used when wanting to save a checkpoint during training and restoring
    the state in the same environment.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在希望在训练期间保存检查点并在相同环境中恢复状态时使用。
- en: 'Example:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE86]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '#### `set_trigger`'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_trigger`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1968)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1968)'
- en: '[PRE87]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Sets the internal trigger tensor to 1 on the current process. A latter check
    should follow using this which will check across all processes.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 将当前进程的内部触发张量设置为1。随后应该使用此检查，该检查将跨所有进程进行检查。
- en: 'Note: Does not require `wait_for_everyone()`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不需要`wait_for_everyone()`
- en: 'Example:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE88]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '#### `skip_first_batches`'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `skip_first_batches`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3156)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3156)'
- en: '[PRE89]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Parameters
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`dataloader` (`torch.utils.data.DataLoader`) — The data loader in which to
    skip batches.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader`（`torch.utils.data.DataLoader`）— 要跳过批次的数据加载器。'
- en: '`num_batches` (`int`, *optional*, defaults to 0) — The number of batches to
    skip'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_batches`（`int`，*可选*，默认为0）— 要跳过的批次数'
- en: Creates a new `torch.utils.data.DataLoader` that will efficiently skip the first
    `num_batches`.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的`torch.utils.data.DataLoader`，它将高效地跳过前`num_batches`。
- en: 'Example:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE90]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '#### `split_between_processes`'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `split_between_processes`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L553)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L553)'
- en: '[PRE91]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Parameters
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`list`, `tuple`, `torch.Tensor`, or `dict` of `list`/`tuple`/`torch.Tensor`)
    — The input to split between processes.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（`list`，`tuple`，`torch.Tensor`或`dict`的`list`/`tuple`/`torch.Tensor`）—
    在进程之间拆分的输入。'
- en: '`apply_padding` (`bool`, `optional`, defaults to `False`) — Whether to apply
    padding by repeating the last element of the input so that all processes have
    the same number of elements. Useful when trying to perform actions such as `Accelerator.gather()`
    on the outputs or passing in less inputs than there are processes. If so, just
    remember to drop the padded elements afterwards.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_padding`（`bool`，`可选`，默认为`False`）— 是否通过重复输入的最后一个元素来应用填充，以便所有进程具有相同数量的元素。在尝试对输出执行`Accelerator.gather()`等操作或传入少于进程数的输入时很有用。如果是这样，请记得之后删除填充的元素。'
- en: Splits `input` between `self.num_processes` quickly and can be then used on
    that process. Useful when doing distributed inference, such as with different
    prompts.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 快速将`input`在`self.num_processes`之间拆分，然后可以在该进程上使用。在进行分布式推理时很有用，例如使用不同的提示。
- en: Note that when using a `dict`, all keys need to have the same number of elements.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用`dict`时，所有键都需要具有相同数量的元素。
- en: 'Example:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE92]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '#### `trigger_sync_in_backward`'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `trigger_sync_in_backward`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L893)'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L893)'
- en: '[PRE93]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Parameters
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — The model for which to trigger the gradient synchronization.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.nn.Module`）— 要触发梯度同步的模型。'
- en: Trigger the sync of the gradients in the next backward pass of the model after
    multiple forward passes under `Accelerator.no_sync` (only applicable in multi-GPU
    scenarios).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Accelerator.no_sync`下的多次前向传递后，触发模型在下一个反向传递中同步梯度（仅适用于多GPU场景）。
- en: If the script is not launched in distributed mode, this context manager does
    nothing.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 如果脚本未在分布式模式下启动，则此上下文管理器不起作用。
- en: 'Example:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE94]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '#### `unscale_gradients`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unscale_gradients`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2027)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2027)'
- en: '[PRE95]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Parameters
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`torch.optim.Optimizer` or `list[torch.optim.Optimizer]`, *optional*)
    — The optimizer(s) for which to unscale gradients. If not set, will unscale gradients
    on all optimizers that were passed to [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`（`torch.optim.Optimizer`或`list[torch.optim.Optimizer]`，*可选*）— 要对梯度进行不缩放的优化器。如果未设置，将对传递给[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)的所有优化器进行梯度不缩放。'
- en: Unscale the gradients in mixed precision training with AMP. This is a noop in
    all other settings.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在AMP混合精度训练中不缩放梯度。在所有其他设置中，这是一个空操作。
- en: Likely should be called through [Accelerator.clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)
    or [Accelerator.clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 可能应该通过[Accelerator.clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)或[Accelerator.clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)来调用
- en: 'Example:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE96]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '#### `unwrap_model`'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unwrap_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2296)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2296)'
- en: '[PRE97]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Parameters
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — The model to unwrap.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.nn.Module`）— 要取消包装的模型。'
- en: '`keep_fp32_wrapper` (`bool`, *optional*, defaults to `True`) — Whether to not
    remove the mixed precision hook if it was added.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_fp32_wrapper`（`bool`，*可选*，默认为`True`）— 是否在添加时不删除混合精度钩子。'
- en: Returns
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.nn.Module`'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The unwrapped model.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 未包装的模型。
- en: Unwraps the `model` from the additional layer possible added by [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare).
    Useful before saving the model.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 从[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)可能添加的额外层中取消包装`model`。在保存模型之前很有用。
- en: 'Example:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE98]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '#### `verify_device_map`'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `verify_device_map`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3192)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3192)'
- en: '[PRE99]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Verifies that `model` has not been prepared with big model inference with a
    device-map resembling `auto`.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 验证`model`是否未准备好使用类似`auto`的设备映射进行大型模型推理。
- en: '#### `wait_for_everyone`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `wait_for_everyone`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2329)'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2329)'
- en: '[PRE100]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Will stop the execution of the current process until every other process has
    reached that point (so this does nothing when the script is only run in one process).
    Useful to do before saving a model.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 将停止当前进程的执行，直到每个其他进程都达到该点（因此当脚本仅在一个进程中运行时，此操作无效）。在保存模型之前执行此操作很有用。
- en: 'Example:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE101]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
