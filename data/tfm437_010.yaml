- en: Train with a script
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è„šæœ¬è¿›è¡Œè®­ç»ƒ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts](https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts](https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Along with the ğŸ¤— Transformers [notebooks](./noteboks/README), there are also
    example scripts demonstrating how to train a model for a task with [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch),
    [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow),
    or [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ğŸ¤— Transformersçš„[notebooks](./noteboks/README)ä¹‹å¤–ï¼Œè¿˜æœ‰ç¤ºä¾‹è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ã€[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow)æˆ–[JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚
- en: You will also find scripts weâ€™ve used in our [research projects](https://github.com/huggingface/transformers/tree/main/examples/research_projects)
    and [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy)
    which are mostly community contributed. These scripts are not actively maintained
    and require a specific version of ğŸ¤— Transformers that will most likely be incompatible
    with the latest version of the library.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜ä¼šå‘ç°æˆ‘ä»¬åœ¨[ç ”ç©¶é¡¹ç›®](https://github.com/huggingface/transformers/tree/main/examples/research_projects)å’Œ[é—ç•™ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy)ä¸­ä½¿ç”¨çš„è„šæœ¬ï¼Œè¿™äº›è„šæœ¬å¤§å¤šæ˜¯ç¤¾åŒºè´¡çŒ®çš„ã€‚è¿™äº›è„šæœ¬ç›®å‰æ²¡æœ‰å¾—åˆ°ç§¯æç»´æŠ¤ï¼Œå¹¶ä¸”éœ€è¦ç‰¹å®šç‰ˆæœ¬çš„ğŸ¤—
    Transformersï¼Œè¿™å¾ˆå¯èƒ½ä¸åº“çš„æœ€æ–°ç‰ˆæœ¬ä¸å…¼å®¹ã€‚
- en: The example scripts are not expected to work out-of-the-box on every problem,
    and you may need to adapt the script to the problem youâ€™re trying to solve. To
    help you with this, most of the scripts fully expose how data is preprocessed,
    allowing you to edit it as necessary for your use case.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹è„šæœ¬ä¸æ˜¯æœŸæœ›åœ¨æ¯ä¸ªé—®é¢˜ä¸Šç«‹å³è¿è¡Œï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´è„šæœ¬ä»¥é€‚åº”æ‚¨è¦è§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†å¸®åŠ©æ‚¨ï¼Œå¤§å¤šæ•°è„šæœ¬å®Œå…¨æš´éœ²äº†æ•°æ®é¢„å¤„ç†çš„æ–¹å¼ï¼Œå…è®¸æ‚¨æ ¹æ®éœ€è¦è¿›è¡Œç¼–è¾‘ä»¥é€‚åº”æ‚¨çš„ç”¨ä¾‹ã€‚
- en: For any feature youâ€™d like to implement in an example script, please discuss
    it on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues)
    before submitting a Pull Request. While we welcome bug fixes, it is unlikely we
    will merge a Pull Request that adds more functionality at the cost of readability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‚¨æƒ³åœ¨ç¤ºä¾‹è„šæœ¬ä¸­å®ç°çš„ä»»ä½•åŠŸèƒ½ï¼Œè¯·åœ¨æäº¤æ‹‰å–è¯·æ±‚ä¹‹å‰åœ¨[è®ºå›](https://discuss.huggingface.co/)æˆ–[é—®é¢˜](https://github.com/huggingface/transformers/issues)ä¸­è®¨è®ºã€‚è™½ç„¶æˆ‘ä»¬æ¬¢è¿é”™è¯¯ä¿®å¤ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸å¤ªå¯èƒ½åˆå¹¶å¢åŠ æ›´å¤šåŠŸèƒ½ä½†ç‰ºç‰²å¯è¯»æ€§çš„æ‹‰å–è¯·æ±‚ã€‚
- en: This guide will show you how to run an example summarization training script
    in [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)
    and [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization).
    All examples are expected to work with both frameworks unless otherwise specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åœ¨[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)å’Œ[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)ä¸­è¿è¡Œä¸€ä¸ªç¤ºä¾‹æ‘˜è¦è®­ç»ƒè„šæœ¬ã€‚é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰ç¤ºä¾‹éƒ½é¢„è®¡èƒ½å¤Ÿåœ¨ä¸¤ä¸ªæ¡†æ¶ä¸­è¿è¡Œã€‚
- en: Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®
- en: 'To successfully run the latest version of the example scripts, you have to
    **install ğŸ¤— Transformers from source** in a new virtual environment:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æˆåŠŸè¿è¡Œç¤ºä¾‹è„šæœ¬çš„æœ€æ–°ç‰ˆæœ¬ï¼Œæ‚¨å¿…é¡»åœ¨æ–°çš„è™šæ‹Ÿç¯å¢ƒä¸­**ä»æºä»£ç å®‰è£…ğŸ¤— Transformers**ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For older versions of the example scripts, click on the toggle below:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ—§ç‰ˆæœ¬çš„ç¤ºä¾‹è„šæœ¬ï¼Œè¯·ç‚¹å‡»ä¸‹é¢çš„åˆ‡æ¢ï¼š
- en: <details data-svelte-h="svelte-145d76l"><summary>Examples for older versions
    of ğŸ¤— Transformers</summary>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-145d76l"><summary>æ—§ç‰ˆæœ¬ğŸ¤— Transformersçš„ç¤ºä¾‹</summary>
- en: '[v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)'
- en: '[v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)'
- en: '[v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)'
- en: '[v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)'
- en: '[v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)'
- en: '[v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)'
- en: '[v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)'
- en: '[v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)'
- en: '[v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)'
- en: '[v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)'
- en: '[v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)'
- en: '[v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)'
- en: '[v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)'
- en: '[v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)'
- en: '[v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)'
- en: '[v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)'
- en: '[v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)'
- en: '[v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)'
- en: '[v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)'
- en: '[v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)'
- en: '[v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)'
- en: '[v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)'
- en: '[v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)'
- en: '[v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)'
- en: '[v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)'
- en: '[v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)'
- en: '[v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)</details>'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)</details>'
- en: 'Then switch your current clone of ğŸ¤— Transformers to a specific version, like
    v3.5.1 for example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Then switch your current clone of ğŸ¤— Transformers to a specific version, like
    v3.5.1 for example:'
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After youâ€™ve setup the correct library version, navigate to the example folder
    of your choice and install the example specific requirements:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'After youâ€™ve setup the correct library version, navigate to the example folder
    of your choice and install the example specific requirements:'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Run a script
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Run a script
- en: PytorchHide Pytorch content
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: The example script downloads and preprocesses a dataset from the ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    on an architecture that supports summarization. The following example shows how
    to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)
    dataset. The T5 model requires an additional `source_prefix` argument due to how
    it was trained. This prompt lets T5 know this is a summarization task.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: The example script downloads and preprocesses a dataset from the ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    on an architecture that supports summarization. The following example shows how
    to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)
    dataset. The T5 model requires an additional `source_prefix` argument due to how
    it was trained. This prompt lets T5 know this is a summarization task.
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TensorFlowHide TensorFlow content
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow content
- en: The example script downloads and preprocesses a dataset from the ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset using Keras on an architecture that
    supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small)
    on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset.
    The T5 model requires an additional `source_prefix` argument due to how it was
    trained. This prompt lets T5 know this is a summarization task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: The example script downloads and preprocesses a dataset from the ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset using Keras on an architecture that
    supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small)
    on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset.
    The T5 model requires an additional `source_prefix` argument due to how it was
    trained. This prompt lets T5 know this is a summarization task.
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Distributed training and mixed precision
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Distributed training and mixed precision
- en: 'The [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    supports distributed training and mixed precision, which means you can also use
    it in a script. To enable both of these features:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'The [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    supports distributed training and mixed precision, which means you can also use
    it in a script. To enable both of these features:'
- en: Add the `fp16` argument to enable mixed precision.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Add the `fp16` argument to enable mixed precision.
- en: Set the number of GPUs to use with the `nproc_per_node` argument.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Set the number of GPUs to use with the `nproc_per_node` argument.
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy)
    for distributed training, and you donâ€™t need to add any additional arguments to
    the training script. The TensorFlow script will use multiple GPUs by default if
    they are available.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy)
    for distributed training, and you donâ€™t need to add any additional arguments to
    the training script. The TensorFlow script will use multiple GPUs by default if
    they are available.
- en: Run a script on a TPU
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Run a script on a TPU
- en: PytorchHide Pytorch content
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning
    compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for
    more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores`
    argument to set the number of TPU cores you want to use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning
    compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for
    more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores`
    argument to set the number of TPU cores you want to use.
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: TensorFlowHide TensorFlow content
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow content
- en: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)
    for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu`
    argument.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)
    for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu`
    argument.
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Run a script with ğŸ¤— Accelerate
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Run a script with ğŸ¤— Accelerate
- en: 'ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library
    that offers a unified method for training a model on several types of setups (CPU-only,
    multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training
    loop. Make sure you have ğŸ¤— Accelerate installed if you donâ€™t already have it:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library
    that offers a unified method for training a model on several types of setups (CPU-only,
    multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training
    loop. Make sure you have ğŸ¤— Accelerate installed if you donâ€™t already have it:'
- en: 'Note: As Accelerate is rapidly developing, the git version of accelerate must
    be installed to run the scripts'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Note: As Accelerate is rapidly developing, the git version of accelerate must
    be installed to run the scripts'
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Instead of the `run_summarization.py` script, you need to use the `run_summarization_no_trainer.py`
    script. ğŸ¤— Accelerate supported scripts will have a `task_no_trainer.py` file in
    the folder. Begin by running the following command to create and save a configuration
    file:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦ä½¿ç”¨`run_summarization_no_trainer.py`è„šæœ¬ï¼Œè€Œä¸æ˜¯`run_summarization.py`è„šæœ¬ã€‚ğŸ¤— åŠ é€Ÿæ”¯æŒçš„è„šæœ¬å°†åœ¨æ–‡ä»¶å¤¹ä¸­æœ‰ä¸€ä¸ª`task_no_trainer.py`æ–‡ä»¶ã€‚é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºå¹¶ä¿å­˜ä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼š
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Test your setup to make sure it is configured correctly:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•ä½ çš„è®¾ç½®ä»¥ç¡®ä¿é…ç½®æ­£ç¡®ï¼š
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now you are ready to launch the training:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒäº†ï¼š
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Use a custom dataset
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
- en: 'The summarization script supports custom datasets as long as they are a CSV
    or JSON Line file. When you use your own dataset, you need to specify several
    additional arguments:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ‘˜è¦è„šæœ¬æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼Œåªè¦å®ƒä»¬æ˜¯CSVæˆ–JSON Lineæ–‡ä»¶ã€‚å½“ä½ ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†æ—¶ï¼Œä½ éœ€è¦æŒ‡å®šå‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼š
- en: '`train_file` and `validation_file` specify the path to your training and validation
    files.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_file`å’Œ`validation_file`æŒ‡å®šäº†ä½ çš„è®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`text_column` is the input text to summarize.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_column`æ˜¯è¦æ€»ç»“çš„è¾“å…¥æ–‡æœ¬ã€‚'
- en: '`summary_column` is the target text to output.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_column`æ˜¯è¦è¾“å‡ºçš„ç›®æ ‡æ–‡æœ¬ã€‚'
- en: 'A summarization script using a custom dataset would look like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†çš„æ‘˜è¦è„šæœ¬å°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Test a script
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµ‹è¯•ä¸€ä¸ªè„šæœ¬
- en: 'It is often a good idea to run your script on a smaller number of dataset examples
    to ensure everything works as expected before committing to an entire dataset
    which may take hours to complete. Use the following arguments to truncate the
    dataset to a maximum number of samples:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰¿è¯ºå®Œæ•´æ•°æ®é›†ä¹‹å‰ï¼Œæœ€å¥½å…ˆåœ¨è¾ƒå°‘æ•°é‡çš„æ•°æ®é›†ç¤ºä¾‹ä¸Šè¿è¡Œä½ çš„è„šæœ¬ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸå·¥ä½œã€‚ä½¿ç”¨ä»¥ä¸‹å‚æ•°å°†æ•°æ®é›†æˆªæ–­ä¸ºæœ€å¤§æ ·æœ¬æ•°ï¼š
- en: '`max_train_samples`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_train_samples`'
- en: '`max_eval_samples`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_eval_samples`'
- en: '`max_predict_samples`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_predict_samples`'
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Not all example scripts support the `max_predict_samples` argument. If you
    arenâ€™t sure whether your script supports this argument, add the `-h` argument
    to check:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶éæ‰€æœ‰ç¤ºä¾‹è„šæœ¬éƒ½æ”¯æŒ`max_predict_samples`å‚æ•°ã€‚å¦‚æœä½ ä¸ç¡®å®šä½ çš„è„šæœ¬æ˜¯å¦æ”¯æŒè¿™ä¸ªå‚æ•°ï¼Œæ·»åŠ `-h`å‚æ•°è¿›è¡Œæ£€æŸ¥ï¼š
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Resume training from checkpoint
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
- en: Another helpful option to enable is resuming training from a previous checkpoint.
    This will ensure you can pick up where you left off without starting over if your
    training gets interrupted. There are two methods to resume training from a checkpoint.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰ç”¨çš„é€‰é¡¹æ˜¯ä»å…ˆå‰çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚è¿™å°†ç¡®ä¿ä½ å¯ä»¥åœ¨ä¸­æ–­è®­ç»ƒåç»§ç»­è¿›è¡Œï¼Œè€Œä¸å¿…é‡æ–°å¼€å§‹ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚
- en: 'The first method uses the `output_dir previous_output_dir` argument to resume
    training from the latest checkpoint stored in `output_dir`. In this case, you
    should remove `overwrite_output_dir`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ç§æ–¹æ³•ä½¿ç”¨`output_dir previous_output_dir`å‚æ•°ä»`output_dir`ä¸­å­˜å‚¨çš„æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥åˆ é™¤`overwrite_output_dir`ï¼š
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The second method uses the `resume_from_checkpoint path_to_specific_checkpoint`
    argument to resume training from a specific checkpoint folder.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨`resume_from_checkpoint path_to_specific_checkpoint`å‚æ•°ä»ç‰¹å®šæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹æ¢å¤è®­ç»ƒã€‚
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Share your model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†äº«ä½ çš„æ¨¡å‹
- en: 'All scripts can upload your final model to the [Model Hub](https://huggingface.co/models).
    Make sure you are logged into Hugging Face before you begin:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è„šæœ¬éƒ½å¯ä»¥å°†ä½ çš„æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ°[æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)ã€‚ç¡®ä¿åœ¨å¼€å§‹ä¹‹å‰å·²ç»ç™»å½•åˆ°Hugging Faceï¼š
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Then add the `push_to_hub` argument to the script. This argument will create
    a repository with your Hugging Face username and the folder name specified in
    `output_dir`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨è„šæœ¬ä¸­æ·»åŠ `push_to_hub`å‚æ•°ã€‚è¿™ä¸ªå‚æ•°å°†åˆ›å»ºä¸€ä¸ªå­˜å‚¨åº“ï¼Œå…¶ä¸­åŒ…å«ä½ çš„Hugging Faceç”¨æˆ·åå’Œ`output_dir`ä¸­æŒ‡å®šçš„æ–‡ä»¶å¤¹åç§°ã€‚
- en: To give your repository a specific name, use the `push_to_hub_model_id` argument
    to add it. The repository will be automatically listed under your namespace.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™ä½ çš„å­˜å‚¨åº“èµ·ä¸€ä¸ªç‰¹å®šçš„åç§°ï¼Œä½¿ç”¨`push_to_hub_model_id`å‚æ•°æ·»åŠ å®ƒã€‚å­˜å‚¨åº“å°†è‡ªåŠ¨åˆ—åœ¨ä½ çš„å‘½åç©ºé—´ä¸‹ã€‚
- en: 'The following example shows how to upload a model with a specific repository
    name:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä¸Šä¼ å…·æœ‰ç‰¹å®šå­˜å‚¨åº“åç§°çš„æ¨¡å‹ï¼š
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
