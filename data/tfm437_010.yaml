- en: Train with a script
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用脚本进行训练
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts](https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts](https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Along with the 🤗 Transformers [notebooks](./noteboks/README), there are also
    example scripts demonstrating how to train a model for a task with [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch),
    [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow),
    or [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了🤗 Transformers的[notebooks](./noteboks/README)之外，还有示例脚本演示如何使用[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)、[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow)或[JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)训练模型的方法。
- en: You will also find scripts we’ve used in our [research projects](https://github.com/huggingface/transformers/tree/main/examples/research_projects)
    and [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy)
    which are mostly community contributed. These scripts are not actively maintained
    and require a specific version of 🤗 Transformers that will most likely be incompatible
    with the latest version of the library.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会发现我们在[研究项目](https://github.com/huggingface/transformers/tree/main/examples/research_projects)和[遗留示例](https://github.com/huggingface/transformers/tree/main/examples/legacy)中使用的脚本，这些脚本大多是社区贡献的。这些脚本目前没有得到积极维护，并且需要特定版本的🤗
    Transformers，这很可能与库的最新版本不兼容。
- en: The example scripts are not expected to work out-of-the-box on every problem,
    and you may need to adapt the script to the problem you’re trying to solve. To
    help you with this, most of the scripts fully expose how data is preprocessed,
    allowing you to edit it as necessary for your use case.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 示例脚本不是期望在每个问题上立即运行，您可能需要调整脚本以适应您要解决的问题。为了帮助您，大多数脚本完全暴露了数据预处理的方式，允许您根据需要进行编辑以适应您的用例。
- en: For any feature you’d like to implement in an example script, please discuss
    it on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues)
    before submitting a Pull Request. While we welcome bug fixes, it is unlikely we
    will merge a Pull Request that adds more functionality at the cost of readability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您想在示例脚本中实现的任何功能，请在提交拉取请求之前在[论坛](https://discuss.huggingface.co/)或[问题](https://github.com/huggingface/transformers/issues)中讨论。虽然我们欢迎错误修复，但是我们不太可能合并增加更多功能但牺牲可读性的拉取请求。
- en: This guide will show you how to run an example summarization training script
    in [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)
    and [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization).
    All examples are expected to work with both frameworks unless otherwise specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将向您展示如何在[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)和[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)中运行一个示例摘要训练脚本。除非另有说明，所有示例都预计能够在两个框架中运行。
- en: Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: 'To successfully run the latest version of the example scripts, you have to
    **install 🤗 Transformers from source** in a new virtual environment:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功运行示例脚本的最新版本，您必须在新的虚拟环境中**从源代码安装🤗 Transformers**：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For older versions of the example scripts, click on the toggle below:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旧版本的示例脚本，请点击下面的切换：
- en: <details data-svelte-h="svelte-145d76l"><summary>Examples for older versions
    of 🤗 Transformers</summary>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-145d76l"><summary>旧版本🤗 Transformers的示例</summary>
- en: '[v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)'
- en: '[v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)'
- en: '[v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)'
- en: '[v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)'
- en: '[v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)'
- en: '[v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)'
- en: '[v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)'
- en: '[v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)'
- en: '[v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)'
- en: '[v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)'
- en: '[v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)'
- en: '[v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)'
- en: '[v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)'
- en: '[v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)'
- en: '[v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)'
- en: '[v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)'
- en: '[v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)'
- en: '[v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)'
- en: '[v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)'
- en: '[v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)'
- en: '[v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)'
- en: '[v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)'
- en: '[v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)'
- en: '[v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)'
- en: '[v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)'
- en: '[v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)'
- en: '[v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)</details>'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)</details>'
- en: 'Then switch your current clone of 🤗 Transformers to a specific version, like
    v3.5.1 for example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Then switch your current clone of 🤗 Transformers to a specific version, like
    v3.5.1 for example:'
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After you’ve setup the correct library version, navigate to the example folder
    of your choice and install the example specific requirements:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'After you’ve setup the correct library version, navigate to the example folder
    of your choice and install the example specific requirements:'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Run a script
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Run a script
- en: PytorchHide Pytorch content
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: The example script downloads and preprocesses a dataset from the 🤗 [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    on an architecture that supports summarization. The following example shows how
    to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)
    dataset. The T5 model requires an additional `source_prefix` argument due to how
    it was trained. This prompt lets T5 know this is a summarization task.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: The example script downloads and preprocesses a dataset from the 🤗 [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    on an architecture that supports summarization. The following example shows how
    to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)
    dataset. The T5 model requires an additional `source_prefix` argument due to how
    it was trained. This prompt lets T5 know this is a summarization task.
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TensorFlowHide TensorFlow content
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow content
- en: The example script downloads and preprocesses a dataset from the 🤗 [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset using Keras on an architecture that
    supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small)
    on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset.
    The T5 model requires an additional `source_prefix` argument due to how it was
    trained. This prompt lets T5 know this is a summarization task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: The example script downloads and preprocesses a dataset from the 🤗 [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset using Keras on an architecture that
    supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small)
    on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset.
    The T5 model requires an additional `source_prefix` argument due to how it was
    trained. This prompt lets T5 know this is a summarization task.
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Distributed training and mixed precision
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Distributed training and mixed precision
- en: 'The [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    supports distributed training and mixed precision, which means you can also use
    it in a script. To enable both of these features:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'The [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    supports distributed training and mixed precision, which means you can also use
    it in a script. To enable both of these features:'
- en: Add the `fp16` argument to enable mixed precision.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Add the `fp16` argument to enable mixed precision.
- en: Set the number of GPUs to use with the `nproc_per_node` argument.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Set the number of GPUs to use with the `nproc_per_node` argument.
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy)
    for distributed training, and you don’t need to add any additional arguments to
    the training script. The TensorFlow script will use multiple GPUs by default if
    they are available.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy)
    for distributed training, and you don’t need to add any additional arguments to
    the training script. The TensorFlow script will use multiple GPUs by default if
    they are available.
- en: Run a script on a TPU
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Run a script on a TPU
- en: PytorchHide Pytorch content
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning
    compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for
    more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores`
    argument to set the number of TPU cores you want to use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning
    compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for
    more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores`
    argument to set the number of TPU cores you want to use.
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: TensorFlowHide TensorFlow content
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow content
- en: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)
    for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu`
    argument.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)
    for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu`
    argument.
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Run a script with 🤗 Accelerate
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Run a script with 🤗 Accelerate
- en: '🤗 [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library
    that offers a unified method for training a model on several types of setups (CPU-only,
    multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training
    loop. Make sure you have 🤗 Accelerate installed if you don’t already have it:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '🤗 [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library
    that offers a unified method for training a model on several types of setups (CPU-only,
    multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training
    loop. Make sure you have 🤗 Accelerate installed if you don’t already have it:'
- en: 'Note: As Accelerate is rapidly developing, the git version of accelerate must
    be installed to run the scripts'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Note: As Accelerate is rapidly developing, the git version of accelerate must
    be installed to run the scripts'
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Instead of the `run_summarization.py` script, you need to use the `run_summarization_no_trainer.py`
    script. 🤗 Accelerate supported scripts will have a `task_no_trainer.py` file in
    the folder. Begin by running the following command to create and save a configuration
    file:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用`run_summarization_no_trainer.py`脚本，而不是`run_summarization.py`脚本。🤗 加速支持的脚本将在文件夹中有一个`task_no_trainer.py`文件。首先运行以下命令创建并保存一个配置文件：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Test your setup to make sure it is configured correctly:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 测试你的设置以确保配置正确：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now you are ready to launch the training:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好开始训练了：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Use a custom dataset
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自定义数据集
- en: 'The summarization script supports custom datasets as long as they are a CSV
    or JSON Line file. When you use your own dataset, you need to specify several
    additional arguments:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要脚本支持自定义数据集，只要它们是CSV或JSON Line文件。当你使用自己的数据集时，你需要指定几个额外的参数：
- en: '`train_file` and `validation_file` specify the path to your training and validation
    files.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_file`和`validation_file`指定了你的训练和验证文件的路径。'
- en: '`text_column` is the input text to summarize.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_column`是要总结的输入文本。'
- en: '`summary_column` is the target text to output.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_column`是要输出的目标文本。'
- en: 'A summarization script using a custom dataset would look like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义数据集的摘要脚本将如下所示：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Test a script
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试一个脚本
- en: 'It is often a good idea to run your script on a smaller number of dataset examples
    to ensure everything works as expected before committing to an entire dataset
    which may take hours to complete. Use the following arguments to truncate the
    dataset to a maximum number of samples:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在承诺完整数据集之前，最好先在较少数量的数据集示例上运行你的脚本，以确保一切按预期工作。使用以下参数将数据集截断为最大样本数：
- en: '`max_train_samples`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_train_samples`'
- en: '`max_eval_samples`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_eval_samples`'
- en: '`max_predict_samples`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_predict_samples`'
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Not all example scripts support the `max_predict_samples` argument. If you
    aren’t sure whether your script supports this argument, add the `-h` argument
    to check:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有示例脚本都支持`max_predict_samples`参数。如果你不确定你的脚本是否支持这个参数，添加`-h`参数进行检查：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Resume training from checkpoint
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从检查点恢复训练
- en: Another helpful option to enable is resuming training from a previous checkpoint.
    This will ensure you can pick up where you left off without starting over if your
    training gets interrupted. There are two methods to resume training from a checkpoint.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的选项是从先前的检查点恢复训练。这将确保你可以在中断训练后继续进行，而不必重新开始。有两种方法可以从检查点恢复训练。
- en: 'The first method uses the `output_dir previous_output_dir` argument to resume
    training from the latest checkpoint stored in `output_dir`. In this case, you
    should remove `overwrite_output_dir`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法使用`output_dir previous_output_dir`参数从`output_dir`中存储的最新检查点恢复训练。在这种情况下，你应该删除`overwrite_output_dir`：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The second method uses the `resume_from_checkpoint path_to_specific_checkpoint`
    argument to resume training from a specific checkpoint folder.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法使用`resume_from_checkpoint path_to_specific_checkpoint`参数从特定检查点文件夹恢复训练。
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Share your model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分享你的模型
- en: 'All scripts can upload your final model to the [Model Hub](https://huggingface.co/models).
    Make sure you are logged into Hugging Face before you begin:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所有脚本都可以将你的最终模型上传到[模型中心](https://huggingface.co/models)。确保在开始之前已经登录到Hugging Face：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Then add the `push_to_hub` argument to the script. This argument will create
    a repository with your Hugging Face username and the folder name specified in
    `output_dir`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在脚本中添加`push_to_hub`参数。这个参数将创建一个存储库，其中包含你的Hugging Face用户名和`output_dir`中指定的文件夹名称。
- en: To give your repository a specific name, use the `push_to_hub_model_id` argument
    to add it. The repository will be automatically listed under your namespace.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 给你的存储库起一个特定的名称，使用`push_to_hub_model_id`参数添加它。存储库将自动列在你的命名空间下。
- en: 'The following example shows how to upload a model with a specific repository
    name:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何上传具有特定存储库名称的模型：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
