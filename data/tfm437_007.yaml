- en: Load pretrained instances with an AutoClass
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AutoClass加载预训练实例
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/autoclass_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/autoclass_tutorial)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/autoclass_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/autoclass_tutorial)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: With so many different Transformer architectures, it can be challenging to create
    one for your checkpoint. As a part of 🤗 Transformers core philosophy to make the
    library easy, simple and flexible to use, an `AutoClass` automatically infers
    and loads the correct architecture from a given checkpoint. The `from_pretrained()`
    method lets you quickly load a pretrained model for any architecture so you don’t
    have to devote time and resources to train a model from scratch. Producing this
    type of checkpoint-agnostic code means if your code works for one checkpoint,
    it will work with another checkpoint - as long as it was trained for a similar
    task - even if the architecture is different.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有这么多不同的Transformer架构，为您的检查点创建一个可能是具有挑战性的。作为🤗 Transformers核心理念的一部分，使库易于使用、简单灵活，`AutoClass`会自动推断并从给定的检查点加载正确的架构。`from_pretrained()`方法让您快速加载任何架构的预训练模型，这样您就不必花时间和资源从头开始训练模型。生成这种与检查点无关的代码意味着，如果您的代码适用于一个检查点，它将适用于另一个检查点
    - 只要它是为类似任务训练的 - 即使架构不同。
- en: Remember, architecture refers to the skeleton of the model and checkpoints are
    the weights for a given architecture. For example, [BERT](https://huggingface.co/bert-base-uncased)
    is an architecture, while `bert-base-uncased` is a checkpoint. Model is a general
    term that can mean either architecture or checkpoint.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，架构指的是模型的骨架，检查点是给定架构的权重。例如，[BERT](https://huggingface.co/bert-base-uncased)是一个架构，而`bert-base-uncased`是一个检查点。模型是一个通用术语，可以指代架构或检查点。
- en: 'In this tutorial, learn to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，学习：
- en: Load a pretrained tokenizer.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一个预训练分词器。
- en: Load a pretrained image processor
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一个预训练图像处理器
- en: Load a pretrained feature extractor.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一个预训练特征提取器。
- en: Load a pretrained processor.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一个预训练处理器。
- en: Load a pretrained model.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一个预训练模型。
- en: Load a model as a backbone.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一个作为骨干的模型。
- en: AutoTokenizer
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoTokenizer
- en: Nearly every NLP task begins with a tokenizer. A tokenizer converts your input
    into a format that can be processed by the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个NLP任务都以分词器开始。分词器将您的输入转换为模型可以处理的格式。
- en: 'Load a tokenizer with [AutoTokenizer.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoTokenizer.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)加载一个分词器：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then tokenize your input as shown below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照下面所示对您的输入进行标记化：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: AutoImageProcessor
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoImageProcessor
- en: For vision tasks, an image processor processes the image into the correct input
    format.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视觉任务，图像处理器将图像处理成正确的输入格式。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: AutoFeatureExtractor
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoFeatureExtractor
- en: For audio tasks, a feature extractor processes the audio signal the correct
    input format.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音频任务，特征提取器将音频信号处理成正确的输入格式。
- en: 'Load a feature extractor with [AutoFeatureExtractor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoFeatureExtractor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained)加载一个特征提取器：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: AutoProcessor
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoProcessor
- en: Multimodal tasks require a processor that combines two types of preprocessing
    tools. For example, the [LayoutLMV2](model_doc/layoutlmv2) model requires an image
    processor to handle images and a tokenizer to handle text; a processor combines
    both of them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态任务需要一个结合两种预处理工具的处理器。例如，[LayoutLMV2](model_doc/layoutlmv2)模型需要一个图像处理器来处理图像，一个分词器来处理文本；处理器将两者结合起来。
- en: 'Load a processor with [AutoProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained)加载一个处理器：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: AutoModel
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoModel
- en: PytorchHide Pytorch content
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: 'The `AutoModelFor` classes let you load a pretrained model for a given task
    (see [here](model_doc/auto) for a complete list of available tasks). For example,
    load a model for sequence classification with [AutoModelForSequenceClassification.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`AutoModelFor`类让您加载给定任务的预训练模型（请参阅[此处](model_doc/auto)以获取可用任务的完整列表）。例如，使用[AutoModelForSequenceClassification.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)加载一个用于序列分类的模型：'
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Easily reuse the same checkpoint to load an architecture for a different task:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 轻松重用相同的检查点来加载不同任务的架构：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For PyTorch models, the `from_pretrained()` method uses `torch.load()` which
    internally uses `pickle` and is known to be insecure. In general, never load a
    model that could have come from an untrusted source, or that could have been tampered
    with. This security risk is partially mitigated for public models hosted on the
    Hugging Face Hub, which are [scanned for malware](https://huggingface.co/docs/hub/security-malware)
    at each commit. See the [Hub documentation](https://huggingface.co/docs/hub/security)
    for best practices like [signed commit verification](https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg)
    with GPG.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PyTorch模型，`from_pretrained()`方法使用`torch.load()`，内部使用`pickle`，已知存在安全风险。一般来说，永远不要加载可能来自不受信任来源或可能被篡改的模型。对于在Hugging
    Face Hub上托管的公共模型，这种安全风险部分得到缓解，这些模型在每次提交时都会进行[恶意软件扫描](https://huggingface.co/docs/hub/security-malware)。查看[Hub文档](https://huggingface.co/docs/hub/security)以获取最佳实践，如使用GPG进行[签名提交验证](https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg)。
- en: TensorFlow and Flax checkpoints are not affected, and can be loaded within PyTorch
    architectures using the `from_tf` and `from_flax` kwargs for the `from_pretrained`
    method to circumvent this issue.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和Flax检查点不受影响，可以在PyTorch架构中使用`from_tf`和`from_flax`参数加载，以绕过此问题。
- en: Generally, we recommend using the `AutoTokenizer` class and the `AutoModelFor`
    class to load pretrained instances of models. This will ensure you load the correct
    architecture every time. In the next [tutorial](preprocessing), learn how to use
    your newly loaded tokenizer, image processor, feature extractor and processor
    to preprocess a dataset for fine-tuning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们建议使用`AutoTokenizer`类和`AutoModelFor`类来加载模型的预训练实例。这将确保您每次加载正确的架构。在下一个[教程](preprocessing)中，学习如何使用新加载的分词器、图像处理器、特征提取器和处理器来预处理数据集进行微调。
- en: TensorFlowHide TensorFlow content
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: 'Finally, the `TFAutoModelFor` classes let you load a pretrained model for a
    given task (see [here](model_doc/auto) for a complete list of available tasks).
    For example, load a model for sequence classification with [TFAutoModelForSequenceClassification.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`TFAutoModelFor`类让您加载给定任务的预训练模型（请参阅[此处](model_doc/auto)以获取可用任务的完整列表）。例如，使用[TFAutoModelForSequenceClassification.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)加载用于序列分类的模型：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Easily reuse the same checkpoint to load an architecture for a different task:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 轻松地重复使用相同的检查点来加载不同任务的架构：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Generally, we recommend using the `AutoTokenizer` class and the `TFAutoModelFor`
    class to load pretrained instances of models. This will ensure you load the correct
    architecture every time. In the next [tutorial](preprocessing), learn how to use
    your newly loaded tokenizer, image processor, feature extractor and processor
    to preprocess a dataset for fine-tuning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们建议使用`AutoTokenizer`类和`TFAutoModelFor`类来加载模型的预训练实例。这将确保您每次加载正确的架构。在下一个[教程](preprocessing)中，学习如何使用新加载的分词器、图像处理器、特征提取器和处理器来预处理数据集进行微调。
- en: AutoBackbone
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoBackbone
- en: '`AutoBackbone` lets you use pretrained models as backbones and get feature
    maps as outputs from different stages of the models. Below you can see how to
    get feature maps from a [Swin](model_doc/swin) checkpoint.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`AutoBackbone`允许您将预训练模型用作骨干，并从模型的不同阶段获得特征图作为输出。下面您可以看到如何从[Swin](model_doc/swin)检查点获取特征图。'
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
