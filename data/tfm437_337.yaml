- en: BROS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BROS
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bros](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bros)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bros](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bros)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The BROS model was proposed in [BROS: A Pre-trained Language Model Focusing
    on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539)
    by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'BROS模型是由Teakgyu Hong、Donghyun Kim、Mingi Ji、Wonseok Hwang、Daehyun Nam、Sungrae
    Park在[BROS: A Pre-trained Language Model Focusing on Text and Layout for Better
    Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539)中提出的。'
- en: BROS stands for *BERT Relying On Spatiality*. It is an encoder-only Transformer
    model that takes a sequence of tokens and their bounding boxes as inputs and outputs
    a sequence of hidden states. BROS encode relative spatial information instead
    of using absolute spatial information.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: BROS代表*BERT依赖空间性*。它是一个仅编码器的Transformer模型，接受一系列标记和它们的边界框作为输入，并输出一系列隐藏状态。BROS编码相对空间信息而不是使用绝对空间信息。
- en: 'It is pre-trained with two objectives: a token-masked language modeling objective
    (TMLM) used in BERT, and a novel area-masked language modeling objective (AMLM)
    In TMLM, tokens are randomly masked, and the model predicts the masked tokens
    using spatial information and other unmasked tokens. AMLM is a 2D version of TMLM.
    It randomly masks text tokens and predicts with the same information as TMLM,
    but it masks text blocks (areas).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过两个目标进行预训练：BERT中使用的标记掩码语言建模目标（TMLM）和一种新颖的区域掩码语言建模目标（AMLM）。在TMLM中，标记被随机掩码，模型使用空间信息和其他未掩码的标记来预测掩码的标记。AMLM是TMLM的二维版本。它随机掩码文本标记，并使用与TMLM相同的信息进行预测，但它掩码文本块（区域）。
- en: '`BrosForTokenClassification` has a simple linear layer on top of BrosModel.
    It predicts the label of each token. `BrosSpadeEEForTokenClassification` has an
    `initial_token_classifier` and `subsequent_token_classifier` on top of BrosModel.
    `initial_token_classifier` is used to predict the first token of each entity,
    and `subsequent_token_classifier` is used to predict the next token of within
    entity. `BrosSpadeELForTokenClassification` has an `entity_linker` on top of BrosModel.
    `entity_linker` is used to predict the relation between two entities.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`BrosForTokenClassification`在BrosModel之上有一个简单的线性层。它预测每个标记的标签。`BrosSpadeEEForTokenClassification`在BrosModel之上有一个`initial_token_classifier`和`subsequent_token_classifier`。`initial_token_classifier`用于预测每个实体的第一个标记，`subsequent_token_classifier`用于预测实体内的下一个标记。`BrosSpadeELForTokenClassification`在BrosModel之上有一个`entity_linker`。`entity_linker`用于预测两个实体之间的关系。'
- en: '`BrosForTokenClassification` and `BrosSpadeEEForTokenClassification` essentially
    perform the same job. However, `BrosForTokenClassification` assumes input tokens
    are perfectly serialized (which is very challenging task since they exist in a
    2D space), while `BrosSpadeEEForTokenClassification` allows for more flexibility
    in handling serialization errors as it predicts next connection tokens from one
    token.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`BrosForTokenClassification`和`BrosSpadeEEForTokenClassification`本质上执行相同的任务。然而，`BrosForTokenClassification`假设输入标记是完全串行化的（这是一个非常具有挑战性的任务，因为它们存在于二维空间），而`BrosSpadeEEForTokenClassification`允许更灵活地处理串行化错误，因为它从一个标记预测下一个连接标记。'
- en: '`BrosSpadeELForTokenClassification` perform the intra-entity linking task.
    It predicts relation from one token (of one entity) to another token (of another
    entity) if these two entities share some relation.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`BrosSpadeELForTokenClassification`执行实体内链接任务。如果这两个实体共享某种关系，则它预测一个标记（一个实体）到另一个标记（另一个实体）的关系。'
- en: BROS achieves comparable or better result on Key Information Extraction (KIE)
    benchmarks such as FUNSD, SROIE, CORD and SciTSR, without relying on explicit
    visual features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BROS在关键信息提取（KIE）基准测试中取得了可比较或更好的结果，如FUNSD、SROIE、CORD和SciTSR，而不依赖于显式的视觉特征。
- en: 'The abstract from the paper is the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Key information extraction (KIE) from document images requires understanding
    the contextual and spatial semantics of texts in two-dimensional (2D) space. Many
    recent studies try to solve the task by developing pre-trained language models
    focusing on combining visual features from document images with texts and their
    layout. On the other hand, this paper tackles the problem by going back to the
    basic: effective combination of text and layout. Specifically, we propose a pre-trained
    language model, named BROS (BERT Relying On Spatiality), that encodes relative
    positions of texts in 2D space and learns from unlabeled documents with area-masking
    strategy. With this optimized training scheme for understanding texts in 2D space,
    BROS shows comparable or better performance compared to previous methods on four
    KIE benchmarks (FUNSD, SROIE*, CORD, and SciTSR) without relying on visual features.
    This paper also reveals two real-world challenges in KIE tasks-(1) minimizing
    the error from incorrect text ordering and (2) efficient learning from fewer downstream
    examples-and demonstrates the superiority of BROS over previous methods.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*从文档图像中提取关键信息（KIE）需要理解二维空间中文本的上下文和空间语义。许多最近的研究尝试通过开发专注于将文档图像的视觉特征与文本及其布局结合的预训练语言模型来解决该任务。另一方面，本文通过回归基本问题来解决问题：文本和布局的有效组合。具体而言，我们提出了一个名为BROS（BERT依赖空间性）的预训练语言模型，它编码了二维空间中文本的相对位置，并通过区域掩码策略从未标记的文档中学习。通过这种针对理解二维空间中文本的优化训练方案，BROS在四个KIE基准测试（FUNSD、SROIE、CORD和SciTSR）上显示出与先前方法相当或更好的性能，而不依赖于视觉特征。本文还揭示了KIE任务中的两个现实挑战-(1)减少由于不正确的文本排序而产生的错误和(2)有效地从更少的下游示例中学习-并展示了BROS相对于先前方法的优越性。*'
- en: This model was contributed by [jinho8345](https://huggingface.co/jinho8345).
    The original code can be found [here](https://github.com/clovaai/bros).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[jinho8345](https://huggingface.co/jinho8345)贡献的。原始代码可以在[这里](https://github.com/clovaai/bros)找到。
- en: Usage tips and examples
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法提示和示例
- en: '[forward()](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel.forward)
    requires `input_ids` and `bbox` (bounding box). Each bounding box should be in
    (x0, y0, x1, y1) format (top-left corner, bottom-right corner). Obtaining of Bounding
    boxes depends on external OCR system. The `x` coordinate should be normalized
    by document image width, and the `y` coordinate should be normalized by document
    image height.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[forward()](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel.forward)
    需要 `input_ids` 和 `bbox`（边界框）。每个边界框应该以 (x0, y0, x1, y1) 格式（左上角，右下角）表示。边界框的获取取决于外部
    OCR 系统。`x` 坐标应该通过文档图像宽度进行归一化，`y` 坐标应该通过文档图像高度进行归一化。'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`,
    `~transformers.BrosSpadeEEForTokenClassification.forward`] require not only `input_ids`
    and `bbox` but also `box_first_token_mask` for loss calculation. It is a mask
    to filter out non-first tokens of each box. You can obtain this mask by saving
    start token indices of bounding boxes when creating `input_ids` from words. You
    can make `box_first_token_mask` with following code,'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`,
    `~transformers.BrosSpadeEEForTokenClassification.forward`] 需要不仅 `input_ids` 和
    `bbox`，还需要 `box_first_token_mask` 用于损失计算。这是一个用于过滤每个框的非第一个标记的掩码。您可以通过保存从单词创建 `input_ids`
    时的边界框的起始标记索引来获得此掩码。您可以使用以下代码生成 `box_first_token_mask`，'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Resources
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: Demo scripts can be found [here](https://github.com/clovaai/bros).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示脚本可以在 [这里](https://github.com/clovaai/bros) 找到。
- en: BrosConfig
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BrosConfig
- en: '### `class transformers.BrosConfig`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BrosConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/configuration_bros.py#L29)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/configuration_bros.py#L29)'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    Bros model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)
    or `TFBrosModel`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 30522) — Bros 模型的词汇表大小。定义了在调用
    [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)
    或 `TFBrosModel` 时可以由 `inputs_ids` 表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数量。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`, `"relu"`, `"silu"` 和 `"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢失比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024或2048）。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)
    or `TFBrosModel`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *optional*, defaults to 2) — 在调用 [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)
    或 `TFBrosModel` 时传递的 `token_type_ids` 的词汇表大小。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The index of the padding
    token in the token vocabulary.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, defaults to 0) — 令牌词汇表中填充令牌的索引。'
- en: '`dim_bbox` (`int`, *optional*, defaults to 8) — The dimension of the bounding
    box coordinates. (x0, y1, x1, y0, x1, y1, x0, y1)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_bbox` (`int`, *optional*, defaults to 8) — 边界框坐标的维度。 (x0, y1, x1, y0,
    x1, y1, x0, y1)'
- en: '`bbox_scale` (`float`, *optional*, defaults to 100.0) — The scale factor of
    the bounding box coordinates.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_scale` (`float`, *optional*, defaults to 100.0) — 边界框坐标的缩放因子。'
- en: '`n_relations` (`int`, *optional*, defaults to 1) — The number of relations
    for SpadeEE(entity extraction), SpadeEL(entity linking) head.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_relations` (`int`, *optional*, defaults to 1) — SpadeEE（实体提取）、SpadeEL（实体链接）头部的关系数量。'
- en: '`classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    ratio for the classifier head.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) — 分类器头部的丢失比率。'
- en: This is the configuration class to store the configuration of a [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)
    or a `TFBrosModel`. It is used to instantiate a Bros model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the Bros [jinho8345/bros-base-uncased](https://huggingface.co/jinho8345/bros-base-uncased)
    architecture.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)或`TFBrosModel`配置的配置类。根据指定的参数实例化一个Bros模型，定义模型架构。使用默认值实例化配置将产生类似于Bros
    [jinho8345/bros-base-uncased](https://huggingface.co/jinho8345/bros-base-uncased)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: BrosProcessor
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BrosProcessor
- en: '### `class transformers.BrosProcessor`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BrosProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/processing_bros.py#L26)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/processing_bros.py#L26)'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`BertTokenizerFast`, *optional*) — An instance of [‘BertTokenizerFast`].
    The tokenizer is a required input.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`BertTokenizerFast`, *可选*) — 一个[‘BertTokenizerFast`]的实例。这是一个必需的输入。'
- en: Constructs a Bros processor which wraps a BERT tokenizer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个包装了BERT tokenizer的Bros处理器。
- en: '[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)
    offers all the functionalities of [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor.__call__)
    and `decode()` for more information.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)提供了[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)的所有功能。查看[**call**()](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor.__call__)和`decode()`的文档字符串获取更多信息。'
- en: '#### `__call__`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/processing_bros.py#L47)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/processing_bros.py#L47)'
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This method uses [BertTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    to prepare text for the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法使用[BertTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)准备文本以供模型使用。
- en: Please refer to the docstring of the above two methods for more information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考上述两种方法的文档字符串获取更多信息。
- en: BrosModel
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BrosModel
- en: '### `class transformers.BrosModel`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BrosModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L784)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L784)'
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Bros Model transformer outputting raw hidden-states without any specific
    head on top. This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Bros模型变换器输出原始隐藏状态，没有特定的头部。这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L815)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L815)'
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`bbox` (‘torch.FloatTensor’ of shape ‘(batch_size, num_boxes, 4)’) — Bounding
    box coordinates for each token in the input sequence. Each bounding box is a list
    of four values (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2,
    y2) is the bottom right corner of the bounding box.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox`（形状为`(batch_size, num_boxes, 4)`的‘torch.FloatTensor’） — 输入序列中每个标记的边界框坐标。每个边界框是四个值的列表（x1,
    y1, x2, y2），其中（x1, y1）是左上角，（x2, y2）是右下角的边界框。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充令牌索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被“掩盖”的令牌，
- en: 0 for tokens that are `masked`.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩盖”的令牌为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`bbox_first_token_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to indicate the first token of each bounding box. Mask values
    selected in `[0, 1]`:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_first_token_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于指示每个边界框的第一个令牌的掩码。选择的掩码值为`[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被“掩盖”的令牌，
- en: 0 for tokens that are `masked`.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被“掩盖”的令牌。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    段令牌索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*令牌，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*令牌。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是令牌类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    每个输入序列令牌在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。选择的掩码值为`[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”。
- en: 0 indicates the head is `masked`.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这是很有用的，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    and inputs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的隐藏状态序列的输出。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后，序列的第一个令牌（分类令牌）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回通过线性层和tanh激活函数处理后的分类令牌。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值，以及在交叉注意力块中，如果`config.is_encoder_decoder=True`，还可以使用）可用（见`past_key_values`输入）以加速顺序解码。
- en: The [BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)
    forward method, overrides the `__call__` special method.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[BrosModel](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: BrosForTokenClassification
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BrosForTokenClassification
- en: '### `class transformers.BrosForTokenClassification`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BrosForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L959)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L959)'
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Bros Model with a token classification head on top (a linear layer on top of
    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有标记分类头的Bros模型（隐藏状态输出的线性层），例如用于命名实体识别（NER）任务。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L982)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L982)'
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`bbox` (‘torch.FloatTensor’ of shape ‘(batch_size, num_boxes, 4)’) — Bounding
    box coordinates for each token in the input sequence. Each bounding box is a list
    of four values (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2,
    y2) is the bottom right corner of the bounding box.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` (‘torch.FloatTensor’ of shape ‘(batch_size, num_boxes, 4)’) — 输入序列中每个标记的边界框坐标。每个边界框都是四个值的列表（x1,
    y1, x2, y2），其中 (x1, y1) 是左上角，(x2, y2) 是右下角的边界框。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被 `masked` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `masked` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`bbox_first_token_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to indicate the first token of each bounding box. Mask values
    selected in `[0, 1]`:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_first_token_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于指示每个边界框的第一个标记的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被 `masked` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `masked` 的标记。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记的位置嵌入的位置索引。在范围 `[0, config.max_position_embeddings - 1]`
    中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids`
    索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Returns
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    and inputs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回)
    — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — 分类分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为一个，+
    每一层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [BrosForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[BrosForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: BrosSpadeEEForTokenClassification
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BrosSpadeEEForTokenClassification
- en: '### `class transformers.BrosSpadeEEForTokenClassification`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BrosSpadeEEForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1063)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1063)'
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）-
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Bros Model with a token classification head on top (initial_token_layers and
    subsequent_token_layer on top of the hidden-states output) e.g. for Named-Entity-Recognition
    (NER) tasks. The initial_token_classifier is used to predict the first token of
    each entity, and the subsequent_token_classifier is used to predict the subsequent
    tokens within an entity. Compared to BrosForTokenClassification, this model is
    more robust to serialization errors since it predicts next token from one token.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Bros模型在顶部带有一个标记分类头（在隐藏状态输出的顶部有初始标记层和后续标记层），例如用于命名实体识别（NER）任务。初始标记分类器用于预测每个实体的第一个标记，后续标记分类器用于预测实体内的后续标记。与BrosForTokenClassification相比，该模型对序列化错误更加稳健，因为它从一个标记预测下一个标记。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1101)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1101)'
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`bbox` (‘torch.FloatTensor’ of shape ‘(batch_size, num_boxes, 4)’) — Bounding
    box coordinates for each token in the input sequence. Each bounding box is a list
    of four values (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2,
    y2) is the bottom right corner of the bounding box.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox`（形状为`(batch_size, num_boxes, 4)`的`torch.FloatTensor`）- 输入序列中每个标记的边界框坐标。每个边界框是四个值的列表（x1，y1，x2，y2），其中（x1，y1）是左上角，（x2，y2）是边界框的右下角。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`已屏蔽`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`bbox_first_token_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to indicate the first token of each bounding box. Mask values
    selected in `[0, 1]`:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_first_token_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*optional*)
    — 用于指示每个边界框的第一个标记的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被 `masked` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `masked` 的标记。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*)
    — 段标记索引，用于指示输入的第一部分和第二部分。索引选在 `[0, 1]`：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*)
    — 每个输入序列标记在位置嵌入中的位置索引。选在范围 `[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*optional*)
    — 用于使自注意力模块的选定头部失效的掩码。掩码值选在 `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Returns
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.bros.modeling_bros.BrosSpadeOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.bros.modeling_bros.BrosSpadeOutput` 或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.bros.modeling_bros.BrosSpadeOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    and inputs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.bros.modeling_bros.BrosSpadeOutput` 或一个 `torch.FloatTensor`
    元组（如果传递 `return_dict=False` 或 `config.return_dict=False` 时）包含根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为 `(1,)`，*optional*，当提供 `labels` 时返回） — 分类损失。'
- en: '`initial_token_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.num_labels)`) — Classification scores for entity initial tokens (before
    SoftMax).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial_token_logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length,
    config.num_labels)`) — 实体初始标记的分类分数（SoftMax 之前）。'
- en: '`subsequent_token_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    sequence_length+1)`) — Classification scores for entity sequence tokens (before
    SoftMax).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsequent_token_logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length,
    sequence_length+1)`) — 实体序列标记的分类分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [BrosSpadeEEForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosSpadeEEForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[BrosSpadeEEForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosSpadeEEForTokenClassification)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: BrosSpadeELForTokenClassification
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BrosSpadeELForTokenClassification
- en: '### `class transformers.BrosSpadeELForTokenClassification`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BrosSpadeELForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1209)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1209)'
- en: '[PRE15]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Bros Model with a token classification head on top (a entity_linker layer on
    top of the hidden-states output) e.g. for Entity-Linking. The entity_linker is
    used to predict intra-entity links (one entity to another entity).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐藏状态输出的顶部有一个标记分类头的Bros模型（在隐藏状态输出的顶部有一个实体链接层），例如用于实体链接。实体链接器用于预测实体之间的内部实体链接（一个实体到另一个实体）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1233)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bros/modeling_bros.py#L1233)'
- en: '[PRE16]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[BrosProcessor](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosProcessor)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入ID是什么？
- en: '`bbox` (‘torch.FloatTensor’ of shape ‘(batch_size, num_boxes, 4)’) — Bounding
    box coordinates for each token in the input sequence. Each bounding box is a list
    of four values (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2,
    y2) is the bottom right corner of the bounding box.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox`（形状为`(batch_size, num_boxes, 4)`的‘torch.FloatTensor’）- 输入序列中每个标记的边界框坐标。每个边界框是一个包含四个值（x1，y1，x2，y2）的列表，其中（x1，y1）是左上角，（x2，y2）是边界框的右下角。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被屏蔽的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被屏蔽的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意掩码是什么？
- en: '`bbox_first_token_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to indicate the first token of each bounding box. Mask values
    selected in `[0, 1]`:'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox_first_token_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于指示每个边界框的第一个标记的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被屏蔽的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被屏蔽的标记为0。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令牌类型ID是什么？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    位置嵌入中每个输入序列标记的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置ID是什么？
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被屏蔽。
- en: 0 indicates the head is `masked`.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被屏蔽。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig))
    and inputs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[BrosConfig](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosConfig)）和输入不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.num_labels)`）
    — 分类得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [BrosSpadeELForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosSpadeELForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[BrosSpadeELForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bros#transformers.BrosSpadeELForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
