- en: Reference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒ
- en: 'Original text: [https://huggingface.co/docs/sagemaker/reference](https://huggingface.co/docs/sagemaker/reference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/sagemaker/reference](https://huggingface.co/docs/sagemaker/reference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Container
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ å®¹å™¨
- en: Below you can find a version table of currently available Hugging Face DLCs.
    The table doesnâ€™t include the full `image_uri` here are two examples on how to
    construct those if needed.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯å½“å‰å¯ç”¨çš„Hugging Face DLCçš„ç‰ˆæœ¬è¡¨ã€‚è¯¥è¡¨ä¸åŒ…æ‹¬å®Œæ•´çš„`image_uri`ï¼Œå¦‚æœéœ€è¦ï¼Œè¿™é‡Œæœ‰ä¸¤ä¸ªæ„å»ºçš„ç¤ºä¾‹ã€‚
- en: '**Manually construction the `image_uri`**'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰‹åŠ¨æ„å»º`image_uri`**'
- en: '`{dlc-aws-account-id}.dkr.ecr.{region}.amazonaws.com/huggingface-{framework}-{(training
    | inference)}:{framework-version}-transformers{transformers-version}-{device}-{python-version}-{device-tag}`'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`{dlc-aws-account-id}.dkr.ecr.{region}.amazonaws.com/huggingface-{framework}-{(training
    | inference)}:{framework-version}-transformers{transformers-version}-{device}-{python-version}-{device-tag}`'
- en: '`dlc-aws-account-id`: The AWS account ID of the account that owns the ECR repository.
    You can find them in the [here](https://github.com/aws/sagemaker-python-sdk/blob/e0b9d38e1e3b48647a02af23c4be54980e53dc61/src/sagemaker/image_uri_config/huggingface.json#L21)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dlc-aws-account-id`: æ‹¥æœ‰ECRå­˜å‚¨åº“çš„å¸æˆ·çš„AWSå¸æˆ·IDã€‚æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/aws/sagemaker-python-sdk/blob/e0b9d38e1e3b48647a02af23c4be54980e53dc61/src/sagemaker/image_uri_config/huggingface.json#L21)æ‰¾åˆ°å®ƒä»¬'
- en: '`region`: The AWS region where you want to use it.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`region`: æ‚¨æƒ³è¦ä½¿ç”¨çš„AWSåŒºåŸŸã€‚'
- en: '`framework`: The framework you want to use, either `pytorch` or `tensorflow`.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`: æ‚¨æƒ³è¦ä½¿ç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥æ˜¯`pytorch`æˆ–`tensorflow`ã€‚'
- en: '`(training | inference)`: The training or inference mode.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(training | inference)`: è®­ç»ƒæˆ–æ¨ç†æ¨¡å¼ã€‚'
- en: '`framework-version`: The version of the framework you want to use.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework-version`: æ‚¨æƒ³è¦ä½¿ç”¨çš„æ¡†æ¶çš„ç‰ˆæœ¬ã€‚'
- en: '`transformers-version`: The version of the transformers library you want to
    use.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-version`: æ‚¨æƒ³è¦ä½¿ç”¨çš„transformersåº“çš„ç‰ˆæœ¬ã€‚'
- en: '`device`: The device you want to use, either `cpu` or `gpu`.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`: æ‚¨æƒ³è¦ä½¿ç”¨çš„è®¾å¤‡ï¼Œå¯ä»¥æ˜¯`cpu`æˆ–`gpu`ã€‚'
- en: '`python-version`: The version of the python of the DLC.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python-version`: DLCçš„Pythonç‰ˆæœ¬ã€‚'
- en: '`device-tag`: The device tag you want to use. The device tag can include os
    version and cuda version'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device-tag`: æ‚¨æƒ³è¦ä½¿ç”¨çš„è®¾å¤‡æ ‡ç­¾ã€‚è®¾å¤‡æ ‡ç­¾å¯ä»¥åŒ…æ‹¬æ“ä½œç³»ç»Ÿç‰ˆæœ¬å’Œcudaç‰ˆæœ¬'
- en: '**Example 1: PyTorch Training:** `763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.6.0-transformers4.4.2-gpu-py36-cu110-ubuntu18.04`
    **Example 2: Tensorflow Inference:** `763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-tensorflow-inference:2.4.1-transformers4.6.1-cpu-py37-ubuntu18.04`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹1ï¼šPyTorchè®­ç»ƒï¼š** `763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.6.0-transformers4.4.2-gpu-py36-cu110-ubuntu18.04`
    **ç¤ºä¾‹2ï¼šTensorflowæ¨ç†ï¼š** `763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-tensorflow-inference:2.4.1-transformers4.6.1-cpu-py37-ubuntu18.04`'
- en: Training DLC Overview
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒDLCæ¦‚è¿°
- en: The Training DLC overview includes all released and available Hugging Face Training
    DLCs. It includes PyTorch and TensorFlow flavored versions for GPU.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒDLCæ¦‚è¿°åŒ…æ‹¬æ‰€æœ‰å‘å¸ƒå’Œå¯ç”¨çš„Hugging Faceè®­ç»ƒDLCã€‚å®ƒåŒ…æ‹¬ç”¨äºGPUçš„PyTorchå’ŒTensorFlowç‰ˆæœ¬ã€‚
- en: '| ğŸ¤— Transformers version | ğŸ¤— Datasets version | PyTorch/TensorFlow version
    | type | device | Python Version |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| ğŸ¤— Transformersç‰ˆæœ¬ | ğŸ¤— æ•°æ®é›†ç‰ˆæœ¬ | PyTorch/TensorFlowç‰ˆæœ¬ | ç±»å‹ | è®¾å¤‡ | Pythonç‰ˆæœ¬ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 4.4.2 | 1.5.0 | PyTorch 1.6.0 | training | GPU | 3.6 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 4.4.2 | 1.5.0 | PyTorch 1.6.0 | è®­ç»ƒ | GPU | 3.6 |'
- en: '| 4.4.2 | 1.5.0 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 4.4.2 | 1.5.0 | TensorFlow 2.4.1 | è®­ç»ƒ | GPU | 3.7 |'
- en: '| 4.5.0 | 1.5.0 | PyTorch 1.6.0 | training | GPU | 3.6 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 4.5.0 | 1.5.0 | PyTorch 1.6.0 | è®­ç»ƒ | GPU | 3.6 |'
- en: '| 4.5.0 | 1.5.0 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 4.5.0 | 1.5.0 | TensorFlow 2.4.1 | è®­ç»ƒ | GPU | 3.7 |'
- en: '| 4.6.1 | 1.6.2 | PyTorch 1.6.0 | training | GPU | 3.6 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 4.6.1 | 1.6.2 | PyTorch 1.6.0 | è®­ç»ƒ | GPU | 3.6 |'
- en: '| 4.6.1 | 1.6.2 | PyTorch 1.7.1 | training | GPU | 3.6 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 4.6.1 | 1.6.2 | PyTorch 1.7.1 | è®­ç»ƒ | GPU | 3.6 |'
- en: '| 4.6.1 | 1.6.2 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 4.6.1 | 1.6.2 | TensorFlow 2.4.1 | è®­ç»ƒ | GPU | 3.7 |'
- en: '| 4.10.2 | 1.11.0 | PyTorch 1.8.1 | training | GPU | 3.6 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | 1.11.0 | PyTorch 1.8.1 | è®­ç»ƒ | GPU | 3.6 |'
- en: '| 4.10.2 | 1.11.0 | PyTorch 1.9.0 | training | GPU | 3.8 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | 1.11.0 | PyTorch 1.9.0 | è®­ç»ƒ | GPU | 3.8 |'
- en: '| 4.10.2 | 1.11.0 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | 1.11.0 | TensorFlow 2.4.1 | è®­ç»ƒ | GPU | 3.7 |'
- en: '| 4.10.2 | 1.11.0 | TensorFlow 2.5.1 | training | GPU | 3.7 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | 1.11.0 | TensorFlow 2.5.1 | è®­ç»ƒ | GPU | 3.7 |'
- en: '| 4.11.0 | 1.12.1 | PyTorch 1.9.0 | training | GPU | 3.8 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 4.11.0 | 1.12.1 | PyTorch 1.9.0 | è®­ç»ƒ | GPU | 3.8 |'
- en: '| 4.11.0 | 1.12.1 | TensorFlow 2.5.1 | training | GPU | 3.7 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 4.11.0 | 1.12.1 | TensorFlow 2.5.1 | è®­ç»ƒ | GPU | 3.7 |'
- en: '| 4.12.3 | 1.15.1 | PyTorch 1.9.1 | training | GPU | 3.8 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 4.12.3 | 1.15.1 | PyTorch 1.9.1 | è®­ç»ƒ | GPU | 3.8 |'
- en: '| 4.12.3 | 1.15.1 | TensorFlow 2.5.1 | training | GPU | 3.7 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 4.12.3 | 1.15.1 | TensorFlow 2.5.1 | è®­ç»ƒ | GPU | 3.7 |'
- en: '| 4.17.0 | 1.18.4 | PyTorch 1.10.2 | training | GPU | 3.8 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 4.17.0 | 1.18.4 | PyTorch 1.10.2 | è®­ç»ƒ | GPU | 3.8 |'
- en: '| 4.17.0 | 1.18.4 | TensorFlow 2.6.3 | training | GPU | 3.8 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 4.17.0 | 1.18.4 | TensorFlow 2.6.3 | è®­ç»ƒ | GPU | 3.8 |'
- en: '| 4.26.0 | 2.9.0 | PyTorch 1.13.1 | training | GPU | 3.9 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 4.26.0 | 2.9.0 | PyTorch 1.13.1 | è®­ç»ƒ | GPU | 3.9 |'
- en: Inference DLC Overview
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†DLCæ¦‚è¿°
- en: The Inference DLC overview includes all released and available Hugging Face
    Inference DLCs. It includes PyTorch and TensorFlow flavored versions for CPU,
    GPU & AWS Inferentia.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†DLCæ¦‚è¿°åŒ…æ‹¬æ‰€æœ‰å‘å¸ƒå’Œå¯ç”¨çš„Hugging Faceæ¨ç†DLCã€‚å®ƒåŒ…æ‹¬ç”¨äºCPUã€GPUå’ŒAWS Inferentiaçš„PyTorchå’ŒTensorFlowç‰ˆæœ¬ã€‚
- en: '| ğŸ¤— Transformers version | PyTorch/TensorFlow version | type | device | Python
    Version |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ğŸ¤— Transformersç‰ˆæœ¬ | PyTorch/TensorFlowç‰ˆæœ¬ | ç±»å‹ | è®¾å¤‡ | Pythonç‰ˆæœ¬ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 4.6.1 | PyTorch 1.7.1 | inference | CPU | 3.6 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 4.6.1 | PyTorch 1.7.1 | æ¨ç† | CPU | 3.6 |'
- en: '| 4.6.1 | PyTorch 1.7.1 | inference | GPU | 3.6 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 4.6.1 | PyTorch 1.7.1 | æ¨ç† | GPU | 3.6 |'
- en: '| 4.6.1 | TensorFlow 2.4.1 | inference | CPU | 3.7 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4.6.1 | TensorFlow 2.4.1 | æ¨ç† | CPU | 3.7 |'
- en: '| 4.6.1 | TensorFlow 2.4.1 | inference | GPU | 3.7 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 4.6.1 | TensorFlow 2.4.1 | æ¨ç† | GPU | 3.7 |'
- en: '| 4.10.2 | PyTorch 1.8.1 | inference | GPU | 3.6 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | PyTorch 1.8.1 | æ¨ç† | GPU | 3.6 |'
- en: '| 4.10.2 | PyTorch 1.9.0 | inference | GPU | 3.8 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | PyTorch 1.9.0 | æ¨ç† | GPU | 3.8 |'
- en: '| 4.10.2 | TensorFlow 2.4.1 | inference | GPU | 3.7 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | TensorFlow 2.4.1 | æ¨ç† | GPU | 3.7 |'
- en: '| 4.10.2 | TensorFlow 2.5.1 | inference | GPU | 3.7 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | TensorFlow 2.5.1 | æ¨ç† | GPU | 3.7 |'
- en: '| 4.10.2 | PyTorch 1.8.1 | inference | CPU | 3.6 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | PyTorch 1.8.1 | æ¨ç† | CPU | 3.6 |'
- en: '| 4.10.2 | PyTorch 1.9.0 | inference | CPU | 3.8 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | PyTorch 1.9.0 | æ¨ç† | CPU | 3.8 |'
- en: '| 4.10.2 | TensorFlow 2.4.1 | inference | CPU | 3.7 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | TensorFlow 2.4.1 | æ¨ç† | CPU | 3.7 |'
- en: '| 4.10.2 | TensorFlow 2.5.1 | inference | CPU | 3.7 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 4.10.2 | TensorFlow 2.5.1 | æ¨ç† | CPU | 3.7 |'
- en: '| 4.11.0 | PyTorch 1.9.0 | inference | GPU | 3.8 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 4.11.0 | PyTorch 1.9.0 | æ¨ç† | GPU | 3.8 |'
- en: '| 4.11.0 | TensorFlow 2.5.1 | inference | GPU | 3.7 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 4.11.0 | TensorFlow 2.5.1 | æ¨ç† | GPU | 3.7 |'
- en: '| 4.11.0 | PyTorch 1.9.0 | inference | CPU | 3.8 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 4.11.0 | PyTorch 1.9.0 | æ¨ç† | CPU | 3.8 |'
- en: '| 4.11.0 | TensorFlow 2.5.1 | inference | CPU | 3.7 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 4.11.0 | TensorFlow 2.5.1 | æ¨ç† | CPU | 3.7 |'
- en: '| 4.12.3 | PyTorch 1.9.1 | inference | GPU | 3.8 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 4.12.3 | PyTorch 1.9.1 | æ¨ç† | GPU | 3.8 |'
- en: '| 4.12.3 | TensorFlow 2.5.1 | inference | GPU | 3.7 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 4.12.3 | TensorFlow 2.5.1 | æ¨ç† | GPU | 3.7 |'
- en: '| 4.12.3 | PyTorch 1.9.1 | inference | CPU | 3.8 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 4.12.3 | PyTorch 1.9.1 | æ¨ç† | CPU | 3.8 |'
- en: '| 4.12.3 | TensorFlow 2.5.1 | inference | CPU | 3.7 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 4.12.3 | TensorFlow 2.5.1 | æ¨ç† | CPU | 3.7 |'
- en: '| 4.12.3 | PyTorch 1.9.1 | inference | Inferentia | 3.7 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 4.12.3 | PyTorch 1.9.1 | æ¨ç† | Inferentia | 3.7 |'
- en: '| 4.17.0 | PyTorch 1.10.2 | inference | GPU | 3.8 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 4.17.0 | PyTorch 1.10.2 | æ¨ç† | GPU | 3.8 |'
- en: '| 4.17.0 | TensorFlow 2.6.3 | inference | GPU | 3.8 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 4.17.0 | TensorFlow 2.6.3 | æ¨ç† | GPU | 3.8 |'
- en: '| 4.17.0 | PyTorch 1.10.2 | inference | CPU | 3.8 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 4.17.0 | PyTorch 1.10.2 | æ¨ç† | CPU | 3.8 |'
- en: '| 4.17.0 | TensorFlow 2.6.3 | inference | CPU | 3.8 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 4.17.0 | TensorFlow 2.6.3 | æ¨ç† | CPU | 3.8 |'
- en: '| 4.26.0 | PyTorch 1.13.1 | inference | CPU | 3.9 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 4.26.0 | PyTorch 1.13.1 | æ¨ç† | CPU | 3.9 |'
- en: '| 4.26.0 | PyTorch 1.13.1 | inference | GPU | 3.9 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 4.26.0 | PyTorch 1.13.1 | æ¨ç† | GPU | 3.9 |'
- en: Hugging Face Transformers Amazon SageMaker Examples
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face Transformers Amazon SageMaker ç¤ºä¾‹
- en: Example Jupyter notebooks that demonstrate how to build, train, and deploy [Hugging
    Face Transformers](https://github.com/huggingface/transformers) using [Amazon
    SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html) and the
    [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Amazon SageMaker å’Œ Amazon SageMaker Python SDK æ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½² Hugging Face
    Transformers çš„ç¤ºä¾‹ Jupyter ç¬”è®°æœ¬
- en: '| Notebook | Type | Description |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ç¬”è®°æœ¬ | ç±»å‹ | æè¿° |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [01 Getting started with PyTorch](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb)
    | Training | Getting started end-to-end example on how to fine-tune a pre-trained
    Hugging Face Transformer for Text-Classification using PyTorch |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| [01 PyTorch å…¥é—¨](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨ PyTorch å¯¹é¢„è®­ç»ƒçš„ Hugging Face Transformer è¿›è¡Œæ–‡æœ¬åˆ†ç±»å¾®è°ƒçš„å…¥é—¨ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [02 getting started with TensorFlow](https://github.com/huggingface/notebooks/blob/main/sagemaker/02_getting_started_tensorflow/sagemaker-notebook.ipynb)
    | Training | Getting started end-to-end example on how to fine-tune a pre-trained
    Hugging Face Transformer for Text-Classification using TensorFlow |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| [02 TensorFlow å…¥é—¨](https://github.com/huggingface/notebooks/blob/main/sagemaker/02_getting_started_tensorflow/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨ TensorFlow å¯¹é¢„è®­ç»ƒçš„ Hugging Face Transformer è¿›è¡Œæ–‡æœ¬åˆ†ç±»å¾®è°ƒçš„å…¥é—¨ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [03 Distributed Training: Data Parallelism](https://github.com/huggingface/notebooks/blob/main/sagemaker/03_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use distributed training with data-parallelism
    strategy for fine-tuning a pre-trained Hugging Face Transformer for Question-Answering
    using Amazon SageMaker Data Parallelism |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| [03 åˆ†å¸ƒå¼è®­ç»ƒï¼šæ•°æ®å¹¶è¡Œ](https://github.com/huggingface/notebooks/blob/main/sagemaker/03_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨æ•°æ®å¹¶è¡Œç­–ç•¥å¯¹é¢„è®­ç»ƒçš„ Hugging Face Transformer è¿›è¡Œé—®ç­”å¾®è°ƒçš„åˆ†å¸ƒå¼è®­ç»ƒç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œä½¿ç”¨ Amazon SageMaker
    æ•°æ®å¹¶è¡Œ |'
- en: '| [04 Distributed Training: Model Parallelism](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use distributed training with model-parallelism
    strategy to pre-trained Hugging Face Transformer using Amazon SageMaker Model
    Parallelism |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| [04 åˆ†å¸ƒå¼è®­ç»ƒï¼šæ¨¡å‹å¹¶è¡Œ](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨æ¨¡å‹å¹¶è¡Œç­–ç•¥å¯¹é¢„è®­ç»ƒçš„ Hugging Face Transformer è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œä½¿ç”¨ Amazon SageMaker
    æ¨¡å‹å¹¶è¡Œ |'
- en: '| [05 How to use Spot Instances & Checkpointing](https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use Spot Instances and Checkpointing
    to reduce training cost |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| [05 å¦‚ä½•ä½¿ç”¨ Spot å®ä¾‹å’Œæ£€æŸ¥ç‚¹](https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨ Spot å®ä¾‹å’Œæ£€æŸ¥ç‚¹æ¥é™ä½è®­ç»ƒæˆæœ¬çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [06 Experiment Tracking with SageMaker Metrics](https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use SageMaker metrics to track your
    experiments and training jobs |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| [06 ä½¿ç”¨ SageMaker Metrics è¿›è¡Œå®éªŒè·Ÿè¸ª](https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨ SageMaker æŒ‡æ ‡è·Ÿè¸ªå®éªŒå’Œè®­ç»ƒä½œä¸šçš„ç«¯åˆ°ç«¯ç¤ºä¾‹'
- en: '| [07 Distributed Training: Data Parallelism](https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use Amazon SageMaker Data Parallelism
    with TensorFlow |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [07 åˆ†å¸ƒå¼è®­ç»ƒï¼šæ•°æ®å¹¶è¡Œ](https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨ TensorFlow å’Œ Amazon SageMaker æ•°æ®å¹¶è¡Œçš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [08 Distributed Training: Summarization with T5/BART](https://github.com/huggingface/notebooks/blob/main/sagemaker/08_distributed_summarization_bart_t5/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to fine-tune BART/T5 for Summarization
    using Amazon SageMaker Data Parallelism |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [08 åˆ†å¸ƒå¼è®­ç»ƒï¼šT5/BART æ‘˜è¦](https://github.com/huggingface/notebooks/blob/main/sagemaker/08_distributed_summarization_bart_t5/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨ Amazon SageMaker æ•°æ®å¹¶è¡Œå¯¹ BART/T5 è¿›è¡Œæ‘˜è¦å¾®è°ƒçš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [09 Vision: Fine-tune ViT](https://github.com/huggingface/notebooks/blob/main/sagemaker/09_image_classification_vision_transformer/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to fine-tune Vision Transformer for Image-Classification
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [09 è§†è§‰ï¼šå¾®è°ƒ ViT](https://github.com/huggingface/notebooks/blob/main/sagemaker/09_image_classification_vision_transformer/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | ä½¿ç”¨ Vision Transformer è¿›è¡Œå›¾åƒåˆ†ç±»çš„ç«¯åˆ°ç«¯å¾®è°ƒç¤ºä¾‹ |'
- en: '| [10 Deploy HF Transformer from Amazon S3](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)
    | Inference | End-to-end example on how to deploy a model from Amazon S3 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [10 ä»Amazon S3éƒ¨ç½²HF Transformer](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)
    | æ¨ç† | æœ‰å…³å¦‚ä½•ä»Amazon S3éƒ¨ç½²æ¨¡å‹çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [11 Deploy HF Transformer from Hugging Face Hub](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)
    | Inference | End-to-end example on how to deploy a model from the Hugging Face
    Hub |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [11 ä»Hugging Face Hubéƒ¨ç½²HF Transformer](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)
    | æ¨ç† | æœ‰å…³å¦‚ä½•ä»Hugging Face Hubéƒ¨ç½²æ¨¡å‹çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [12 Batch Processing with Amazon SageMaker Batch Transform](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to do batch processing with Amazon SageMaker
    Batch Transform |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [12 ä½¿ç”¨Amazon SageMakeræ‰¹é‡è½¬æ¢è¿›è¡Œæ‰¹å¤„ç†å¤„ç†](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb)
    | æ¨ç† | æœ‰å…³å¦‚ä½•ä½¿ç”¨Amazon SageMakeræ‰¹é‡è½¬æ¢è¿›è¡Œæ‰¹å¤„ç†å¤„ç†çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [13 Autoscaling SageMaker Endpoints](https://github.com/huggingface/notebooks/blob/main/sagemaker/13_deploy_and_autoscaling_transformers/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to do use autoscaling for a HF Endpoint
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [13 è‡ªåŠ¨ç¼©æ”¾SageMakerç«¯ç‚¹](https://github.com/huggingface/notebooks/blob/main/sagemaker/13_deploy_and_autoscaling_transformers/sagemaker-notebook.ipynb)
    | æ¨ç† | æœ‰å…³å¦‚ä½•ä¸ºHFç«¯ç‚¹ä½¿ç”¨è‡ªåŠ¨ç¼©æ”¾çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [14 Fine-tune and push to Hub](https://github.com/huggingface/notebooks/blob/main/sagemaker/14_train_and_push_to_hub/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to do use the Hugging Face Hub as MLOps
    backend for saving checkpoints during training |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [14 å¾®è°ƒå¹¶æ¨é€åˆ°Hub](https://github.com/huggingface/notebooks/blob/main/sagemaker/14_train_and_push_to_hub/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | æœ‰å…³å¦‚ä½•å°†Hugging Face Hubç”¨ä½œMLOpsåç«¯ä»¥åœ¨è®­ç»ƒæœŸé—´ä¿å­˜æ£€æŸ¥ç‚¹çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [15 Training Compiler](https://github.com/huggingface/notebooks/blob/main/sagemaker/15_training_compiler/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to do use Amazon SageMaker Training Compiler
    to speed up training time |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [15 è®­ç»ƒç¼–è¯‘å™¨](https://github.com/huggingface/notebooks/blob/main/sagemaker/15_training_compiler/sagemaker-notebook.ipynb)
    | è®­ç»ƒ | æœ‰å…³å¦‚ä½•ä½¿ç”¨Amazon SageMakerè®­ç»ƒç¼–è¯‘å™¨åŠ å¿«è®­ç»ƒæ—¶é—´çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [16 Asynchronous Inference](https://github.com/huggingface/notebooks/blob/main/sagemaker/16_async_inference_hf_hub/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to do use Amazon SageMaker Asynchronous
    Inference endpoints with Hugging Face Transformers |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [16 å¼‚æ­¥æ¨ç†](https://github.com/huggingface/notebooks/blob/main/sagemaker/16_async_inference_hf_hub/sagemaker-notebook.ipynb)
    | æ¨ç† | æœ‰å…³å¦‚ä½•ä½¿ç”¨Amazon SageMakerå¼‚æ­¥æ¨ç†ç«¯ç‚¹ä¸Hugging Face Transformersçš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [17 Custom inference.py script](https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to create a custom inference.py for Sentence
    Transformers and sentence embeddings |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [17 è‡ªå®šä¹‰æ¨ç†.pyè„šæœ¬](https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb)
    | æ¨ç† | æœ‰å…³å¦‚ä½•ä¸ºå¥å­è½¬æ¢å™¨å’Œå¥å­åµŒå…¥åˆ›å»ºè‡ªå®šä¹‰æ¨ç†.pyçš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: '| [18 AWS Inferentia](https://github.com/huggingface/notebooks/blob/main/sagemaker/18_inferentia_inference/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to AWS Inferentia to speed up inference
    time |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| [18 AWS Inferentia](https://github.com/huggingface/notebooks/blob/main/sagemaker/18_inferentia_inference/sagemaker-notebook.ipynb)
    | æ¨ç† | æœ‰å…³å¦‚ä½•ä½¿ç”¨AWS InferentiaåŠ å¿«æ¨ç†æ—¶é—´çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ |'
- en: Inference Toolkit API
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†å·¥å…·åŒ…API
- en: The Inference Toolkit accepts inputs in the `inputs` key, and supports additional
    [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines)
    parameters in the `parameters` key. You can provide any of the supported `kwargs`
    from `pipelines` as `parameters`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†å·¥å…·åŒ…æ¥å—`inputs`é”®ä¸­çš„è¾“å…¥ï¼Œå¹¶åœ¨`parameters`é”®ä¸­æ”¯æŒé¢å¤–çš„[`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines)å‚æ•°ã€‚æ‚¨å¯ä»¥å°†`pipelines`ä¸­æ”¯æŒçš„ä»»ä½•`kwargs`ä½œä¸º`parameters`æä¾›ã€‚
- en: 'Tasks supported by the Inference Toolkit API include:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†å·¥å…·åŒ…APIæ”¯æŒçš„ä»»åŠ¡åŒ…æ‹¬ï¼š
- en: '**`text-classification`**'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`text-classification`**'
- en: '**`sentiment-analysis`**'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`sentiment-analysis`**'
- en: '**`token-classification`**'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`token-classification`**'
- en: '**`feature-extraction`**'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`feature-extraction`**'
- en: '**`fill-mask`**'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`fill-mask`**'
- en: '**`summarization`**'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`summarization`**'
- en: '**`translation_xx_to_yy`**'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`translation_xx_to_yy`**'
- en: '**`text2text-generation`**'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`text2text-generation`**'
- en: '**`text-generation`**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`text-generation`**'
- en: '**`audio-classificatin`**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`audio-classificatin`**'
- en: '**`automatic-speech-recognition`**'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`automatic-speech-recognition`**'
- en: '**`conversational`**'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`conversational`**'
- en: '**`image-classification`**'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`image-classification`**'
- en: '**`image-segmentation`**'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`image-segmentation`**'
- en: '**`object-detection`**'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`object-detection`**'
- en: '**`table-question-answering`**'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`table-question-answering`**'
- en: '**`zero-shot-classification`**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`zero-shot-classification`**'
- en: '**`zero-shot-image-classification`**'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`zero-shot-image-classification`**'
- en: 'See the following request examples for some of the tasks:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ä»¥ä¸‹è¯·æ±‚ç¤ºä¾‹ï¼Œäº†è§£ä¸€äº›ä»»åŠ¡ï¼š
- en: '**`text-classification`**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**`text-classification`**'
- en: '[PRE0]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**`sentiment-analysis`**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**`sentiment-analysis`**'
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**`token-classification`**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**`token-classification`**'
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**`question-answering`**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**`question-answering`**'
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**`zero-shot-classification`**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**`zero-shot-classification`**'
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**`table-question-answering`**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**`table-question-answering`**'
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**`parameterized-request`**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**`parameterized-request`**'
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Inference Toolkit environment variables
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†å·¥å…·åŒ…ç¯å¢ƒå˜é‡
- en: 'The Inference Toolkit implements various additional environment variables to
    simplify deployment. A complete list of Hugging Face specific environment variables
    is shown below:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†å·¥å…·åŒ…å®ç°äº†å„ç§é¢å¤–çš„ç¯å¢ƒå˜é‡ä»¥ç®€åŒ–éƒ¨ç½²ã€‚ä¸‹é¢æ˜¾ç¤ºäº†Hugging Faceç‰¹å®šç¯å¢ƒå˜é‡çš„å®Œæ•´åˆ—è¡¨ï¼š
- en: '**`HF_TASK`**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**`HF_TASK`**'
- en: '`HF_TASK` defines the task for the ğŸ¤— Transformers pipeline used . See [here](https://huggingface.co/docs/transformers/main_classes/pipelines)
    for a complete list of tasks.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`HF_TASK`å®šä¹‰äº†ğŸ¤— Transformersç®¡é“ä½¿ç”¨çš„ä»»åŠ¡ã€‚è¯·å‚é˜…[æ­¤å¤„](https://huggingface.co/docs/transformers/main_classes/pipelines)è·å–ä»»åŠ¡çš„å®Œæ•´åˆ—è¡¨ã€‚'
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**`HF_MODEL_ID`**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**`HF_MODEL_ID`**'
- en: '`HF_MODEL_ID` defines the model ID which is automatically loaded from [hf.co/models](https://huggingface.co/models)
    when creating a SageMaker endpoint. All of the ğŸ¤— Hubâ€™s 10,000+ models are available
    through this environment variable.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`HF_MODEL_ID` å®šä¹‰äº†æ¨¡å‹ IDï¼Œåœ¨åˆ›å»º SageMaker ç«¯ç‚¹æ—¶ä¼šè‡ªåŠ¨ä» [hf.co/models](https://huggingface.co/models)
    åŠ è½½ã€‚æ‰€æœ‰ ğŸ¤— Hub çš„ 10,000 å¤šä¸ªæ¨¡å‹éƒ½å¯ä»¥é€šè¿‡è¿™ä¸ªç¯å¢ƒå˜é‡è·å¾—ã€‚'
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**`HF_MODEL_REVISION`**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**`HF_MODEL_REVISION`**'
- en: '`HF_MODEL_REVISION` is an extension to `HF_MODEL_ID` and allows you to define
    or pin a model revision to make sure you always load the same model on your SageMaker
    endpoint.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`HF_MODEL_REVISION` æ˜¯ `HF_MODEL_ID` çš„æ‰©å±•ï¼Œå…è®¸æ‚¨å®šä¹‰æˆ–å›ºå®šä¸€ä¸ªæ¨¡å‹ä¿®è®¢ç‰ˆæœ¬ï¼Œä»¥ç¡®ä¿æ‚¨å§‹ç»ˆåœ¨ SageMaker
    ç«¯ç‚¹ä¸ŠåŠ è½½ç›¸åŒçš„æ¨¡å‹ã€‚'
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**`HF_API_TOKEN`**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**`HF_API_TOKEN`**'
- en: '`HF_API_TOKEN` defines your Hugging Face authorization token. The `HF_API_TOKEN`
    is used as a HTTP bearer authorization for remote files like private models. You
    can find your token under [Settings](https://huggingface.co/settings/tokens) of
    your Hugging Face account.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`HF_API_TOKEN` å®šä¹‰äº†æ‚¨çš„ Hugging Face æˆæƒä»¤ç‰Œã€‚`HF_API_TOKEN` è¢«ç”¨ä½œè¿œç¨‹æ–‡ä»¶ï¼ˆå¦‚ç§æœ‰æ¨¡å‹ï¼‰çš„ HTTP
    bearer æˆæƒã€‚æ‚¨å¯ä»¥åœ¨æ‚¨çš„ Hugging Face è´¦æˆ·çš„ [è®¾ç½®](https://huggingface.co/settings/tokens)
    ä¸‹æ‰¾åˆ°æ‚¨çš„ä»¤ç‰Œã€‚'
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
