- en: Train a diffusion model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ‰©æ•£æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/diffusers/tutorials/basic_training](https://huggingface.co/docs/diffusers/tutorials/basic_training)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/tutorials/basic_training](https://huggingface.co/docs/diffusers/tutorials/basic_training)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Unconditional image generation is a popular application of diffusion models
    that generates images that look like those in the dataset used for training. Typically,
    the best results are obtained from finetuning a pretrained model on a specific
    dataset. You can find many of these checkpoints on the [Hub](https://huggingface.co/search/full-text?q=unconditional-image-generation&type=model),
    but if you canâ€™t find one you like, you can always train your own!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ç§æµè¡Œåº”ç”¨ï¼Œå®ƒç”Ÿæˆçœ‹èµ·æ¥åƒè®­ç»ƒæ•°æ®é›†ä¸­çš„å›¾åƒçš„å›¾åƒã€‚é€šå¸¸ï¼Œé€šè¿‡åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥è·å¾—æœ€ä½³ç»“æœã€‚æ‚¨å¯ä»¥åœ¨[Hub](https://huggingface.co/search/full-text?q=unconditional-image-generation&type=model)ä¸Šæ‰¾åˆ°è®¸å¤šè¿™äº›æ£€æŸ¥ç‚¹ï¼Œä½†å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„ï¼Œæ‚¨ä¹Ÿå¯ä»¥è‡ªå·±è®­ç»ƒï¼
- en: This tutorial will teach you how to train a [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)
    from scratch on a subset of the [Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset)
    dataset to generate your own ğŸ¦‹ butterflies ğŸ¦‹.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å°†æ•™æ‚¨å¦‚ä½•ä»å¤´å¼€å§‹åœ¨[Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset)æ•°æ®é›†çš„å­é›†ä¸Šè®­ç»ƒä¸€ä¸ª[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)ä»¥ç”Ÿæˆæ‚¨è‡ªå·±çš„ğŸ¦‹è´è¶ğŸ¦‹ã€‚
- en: ğŸ’¡ This training tutorial is based on the [Training with ğŸ§¨ Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)
    notebook. For additional details and context about diffusion models like how they
    work, check out the notebook!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ è¿™ä¸ªè®­ç»ƒæ•™ç¨‹åŸºäº[ä½¿ç”¨ğŸ§¨æ‰©æ•£å™¨è¿›è¡Œè®­ç»ƒ](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)ç¬”è®°æœ¬ã€‚æœ‰å…³æ‰©æ•£æ¨¡å‹çš„æ›´å¤šç»†èŠ‚å’ŒèƒŒæ™¯ï¼Œæ¯”å¦‚å®ƒä»¬çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹ç¬”è®°æœ¬ï¼
- en: Before you begin, make sure you have ğŸ¤— Datasets installed to load and preprocess
    image datasets, and ğŸ¤— Accelerate, to simplify training on any number of GPUs.
    The following command will also install [TensorBoard](https://www.tensorflow.org/tensorboard)
    to visualize training metrics (you can also use [Weights & Biases](https://docs.wandb.ai/)
    to track your training).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ğŸ¤— æ•°æ®é›†ä»¥åŠ è½½å’Œé¢„å¤„ç†å›¾åƒæ•°æ®é›†ï¼Œå¹¶å®‰è£…ğŸ¤— Accelerateï¼Œä»¥ç®€åŒ–åœ¨ä»»æ„æ•°é‡çš„GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚ä»¥ä¸‹å‘½ä»¤è¿˜å°†å®‰è£…[TensorBoard](https://www.tensorflow.org/tensorboard)ä»¥å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡ï¼ˆæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨[Weights
    & Biases](https://docs.wandb.ai/)æ¥è·Ÿè¸ªæ‚¨çš„è®­ç»ƒï¼‰ã€‚
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We encourage you to share your model with the community, and in order to do
    that, youâ€™ll need to login to your Hugging Face account (create one [here](https://hf.co/join)
    if you donâ€™t already have one!). You can login from a notebook and enter your
    token when prompted. Make sure your token has the write role.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæ‚¨éœ€è¦ç™»å½•åˆ°æ‚¨çš„Hugging Faceå¸æˆ·ï¼ˆå¦‚æœæ‚¨è¿˜æ²¡æœ‰å¸æˆ·ï¼Œè¯·åœ¨[æ­¤å¤„](https://hf.co/join)åˆ›å»ºä¸€ä¸ªï¼ï¼‰ã€‚æ‚¨å¯ä»¥ä»ç¬”è®°æœ¬ç™»å½•ï¼Œå¹¶åœ¨æç¤ºæ—¶è¾“å…¥æ‚¨çš„ä»¤ç‰Œã€‚ç¡®ä¿æ‚¨çš„ä»¤ç‰Œå…·æœ‰å†™å…¥æƒé™ã€‚
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or login in from the terminal:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä»ç»ˆç«¯ç™»å½•ï¼š
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since the model checkpoints are quite large, install [Git-LFS](https://git-lfs.com/)
    to version these large files:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¨¡å‹æ£€æŸ¥ç‚¹éå¸¸å¤§ï¼Œå®‰è£…[Git-LFS](https://git-lfs.com/)æ¥å¯¹è¿™äº›å¤§æ–‡ä»¶è¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼š
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training configuration
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒé…ç½®
- en: 'For convenience, create a `TrainingConfig` class containing the training hyperparameters
    (feel free to adjust them):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ–¹ä¾¿èµ·è§ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«è®­ç»ƒè¶…å‚æ•°çš„`TrainingConfig`ç±»ï¼ˆéšæ—¶è°ƒæ•´ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Load the dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®é›†
- en: 'You can easily load the [Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset)
    dataset with the ğŸ¤— Datasets library:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— æ•°æ®é›†åº“è½»æ¾åŠ è½½[Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset)æ•°æ®é›†ï¼š
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ğŸ’¡ You can find additional datasets from the [HugGan Community Event](https://huggingface.co/huggan)
    or you can use your own dataset by creating a local [`ImageFolder`](https://huggingface.co/docs/datasets/image_dataset#imagefolder).
    Set `config.dataset_name` to the repository id of the dataset if it is from the
    HugGan Community Event, or `imagefolder` if youâ€™re using your own images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ æ‚¨å¯ä»¥ä»[HugGanç¤¾åŒºæ´»åŠ¨](https://huggingface.co/huggan)ä¸­æ‰¾åˆ°å…¶ä»–æ•°æ®é›†ï¼Œæˆ–è€…é€šè¿‡åˆ›å»ºæœ¬åœ°[`ImageFolder`](https://huggingface.co/docs/datasets/image_dataset#imagefolder)æ¥ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†ã€‚å¦‚æœæ•°æ®é›†æ¥è‡ªHugGanç¤¾åŒºæ´»åŠ¨ï¼Œè¯·å°†`config.dataset_name`è®¾ç½®ä¸ºæ•°æ®é›†çš„å­˜å‚¨åº“IDï¼Œå¦‚æœæ‚¨ä½¿ç”¨è‡ªå·±çš„å›¾åƒï¼Œåˆ™è®¾ç½®ä¸º`imagefolder`ã€‚
- en: 'ğŸ¤— Datasets uses the [Image](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    feature to automatically decode the image data and load it as a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html)
    which we can visualize:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— æ•°æ®é›†ä½¿ç”¨[Image](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)åŠŸèƒ½è‡ªåŠ¨è§£ç å›¾åƒæ•°æ®å¹¶å°†å…¶åŠ è½½ä¸º[`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html)ï¼Œæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–ï¼š
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/a492ec80a6f208c57e9bd240df33193d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a492ec80a6f208c57e9bd240df33193d.png)'
- en: 'The images are all different sizes though, so youâ€™ll need to preprocess them
    first:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å›¾åƒå¤§å°å„ä¸ç›¸åŒï¼Œä½†æ‚¨éœ€è¦é¦–å…ˆå¯¹å…¶è¿›è¡Œé¢„å¤„ç†ï¼š
- en: '`Resize` changes the image size to the one defined in `config.image_size`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Resize`å°†å›¾åƒå¤§å°æ›´æ”¹ä¸º`config.image_size`ä¸­å®šä¹‰çš„å¤§å°ã€‚'
- en: '`RandomHorizontalFlip` augments the dataset by randomly mirroring the images.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomHorizontalFlip`é€šè¿‡éšæœºé•œåƒå›¾åƒæ¥å¢å¼ºæ•°æ®é›†ã€‚'
- en: '`Normalize` is important to rescale the pixel values into a [-1, 1] range,
    which is what the model expects.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normalize`å¯¹äºå°†åƒç´ å€¼é‡æ–°ç¼©æ”¾ä¸º[-1, 1]èŒƒå›´éå¸¸é‡è¦ï¼Œè¿™æ˜¯æ¨¡å‹æ‰€æœŸæœ›çš„ã€‚'
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Use ğŸ¤— Datasetsâ€™ [set_transform](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)
    method to apply the `preprocess` function on the fly during training:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— æ•°æ®é›†çš„[set_transform](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€åº”ç”¨`preprocess`å‡½æ•°ï¼š
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Feel free to visualize the images again to confirm that theyâ€™ve been resized.
    Now youâ€™re ready to wrap the dataset in a [DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader)
    for training!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ—¶å†æ¬¡å¯è§†åŒ–å›¾åƒä»¥ç¡®è®¤å®ƒä»¬å·²è¢«è°ƒæ•´å¤§å°ã€‚ç°åœ¨æ‚¨å·²å‡†å¤‡å¥½å°†æ•°æ®é›†åŒ…è£…åœ¨[DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader)ä¸­è¿›è¡Œè®­ç»ƒï¼
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Create a UNet2DModel
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªUNet2DModel
- en: 'Pretrained models in ğŸ§¨ Diffusers are easily created from their model class
    with the parameters you want. For example, to create a [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ§¨ Diffusersä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥æ ¹æ®æ‚¨æƒ³è¦çš„å‚æ•°è½»æ¾åˆ›å»ºã€‚ä¾‹å¦‚ï¼Œè¦åˆ›å»ºä¸€ä¸ª[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)ï¼š
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It is often a good idea to quickly check the sample image shape matches the
    model output shape:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«é€Ÿæ£€æŸ¥æ ·æœ¬å›¾åƒå½¢çŠ¶æ˜¯å¦ä¸æ¨¡å‹è¾“å‡ºå½¢çŠ¶åŒ¹é…é€šå¸¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼š
- en: '[PRE11]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Great! Next, youâ€™ll need a scheduler to add some noise to the image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªæ£’äº†ï¼æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªè°ƒåº¦å™¨æ¥å‘å›¾åƒæ·»åŠ ä¸€äº›å™ªå£°ã€‚
- en: Create a scheduler
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªè°ƒåº¦å™¨
- en: The scheduler behaves differently depending on whether youâ€™re using the model
    for training or inference. During inference, the scheduler generates image from
    the noise. During training, the scheduler takes a model output - or a sample -
    from a specific point in the diffusion process and applies noise to the image
    according to a *noise schedule* and an *update rule*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨çš„è¡Œä¸ºå–å†³äºæ‚¨æ˜¯å°†æ¨¡å‹ç”¨äºè®­ç»ƒè¿˜æ˜¯æ¨æ–­ã€‚åœ¨æ¨æ–­æœŸé—´ï¼Œè°ƒåº¦å™¨ä»å™ªå£°ä¸­ç”Ÿæˆå›¾åƒã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œè°ƒåº¦å™¨æ¥å—æ¨¡å‹è¾“å‡º - æˆ–æ ·æœ¬ - ä»æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç‰¹å®šç‚¹ï¼Œå¹¶æ ¹æ®*å™ªå£°è®¡åˆ’*å’Œ*æ›´æ–°è§„åˆ™*å‘å›¾åƒåº”ç”¨å™ªå£°ã€‚
- en: 'Letâ€™s take a look at the [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)
    and use the `add_noise` method to add some random noise to the `sample_image`
    from before:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼Œå¹¶ä½¿ç”¨`add_noise`æ–¹æ³•å‘ä¹‹å‰çš„`sample_image`æ·»åŠ ä¸€äº›éšæœºå™ªå£°ï¼š
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/9c3cadba02429fff4abe5f63e4e8c1d9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c3cadba02429fff4abe5f63e4e8c1d9.png)'
- en: 'The training objective of the model is to predict the noise added to the image.
    The loss at this step can be calculated by:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„è®­ç»ƒç›®æ ‡æ˜¯é¢„æµ‹æ·»åŠ åˆ°å›¾åƒä¸­çš„å™ªå£°ã€‚åœ¨è¿™ä¸€æ­¥çš„æŸå¤±å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Train the model
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹
- en: By now, you have most of the pieces to start training the model and all thatâ€™s
    left is putting everything together.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²ç»æœ‰äº†å¼€å§‹è®­ç»ƒæ¨¡å‹çš„å¤§éƒ¨åˆ†ç»„ä»¶ï¼Œç°åœ¨å‰©ä¸‹çš„å°±æ˜¯å°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ã€‚
- en: 'First, youâ€™ll need an optimizer and a learning rate scheduler:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ‚¨éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨å’Œä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, youâ€™ll need a way to evaluate the model. For evaluation, you can use
    the [DDPMPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/ddpm#diffusers.DDPMPipeline)
    to generate a batch of sample images and save it as a grid:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨éœ€è¦ä¸€ç§è¯„ä¼°æ¨¡å‹çš„æ–¹æ³•ã€‚å¯¹äºè¯„ä¼°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[DDPMPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/ddpm#diffusers.DDPMPipeline)ç”Ÿæˆä¸€æ‰¹æ ·æœ¬å›¾åƒå¹¶å°†å…¶ä¿å­˜ä¸ºç½‘æ ¼ï¼š
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now you can wrap all these components together in a training loop with ğŸ¤— Accelerate
    for easy TensorBoard logging, gradient accumulation, and mixed precision training.
    To upload the model to the Hub, write a function to get your repository name and
    information and then push it to the Hub.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— Accelerateå°†æ‰€æœ‰è¿™äº›ç»„ä»¶åŒ…è£…åœ¨ä¸€ä¸ªè®­ç»ƒå¾ªç¯ä¸­ï¼Œä»¥ä¾¿è¿›è¡Œç®€å•çš„TensorBoardæ—¥å¿—è®°å½•ã€æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚è¦å°†æ¨¡å‹ä¸Šä¼ åˆ°Hubï¼Œè¯·ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è·å–æ‚¨çš„å­˜å‚¨åº“åç§°å’Œä¿¡æ¯ï¼Œç„¶åå°†å…¶æ¨é€åˆ°Hubã€‚
- en: ğŸ’¡ The training loop below may look intimidating and long, but itâ€™ll be worth
    it later when you launch your training in just one line of code! If you canâ€™t
    wait and want to start generating images, feel free to copy and run the code below.
    You can always come back and examine the training loop more closely later, like
    when youâ€™re waiting for your model to finish training. ğŸ¤—
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä¸‹é¢çš„è®­ç»ƒå¾ªç¯å¯èƒ½çœ‹èµ·æ¥ä»¤äººç”Ÿç•ä¸”å†—é•¿ï¼Œä½†å½“æ‚¨åªéœ€ä¸€è¡Œä»£ç å¯åŠ¨è®­ç»ƒæ—¶ï¼Œè¿™å°†æ˜¯å€¼å¾—çš„ï¼å¦‚æœæ‚¨ç­‰ä¸åŠæƒ³è¦å¼€å§‹ç”Ÿæˆå›¾åƒï¼Œè¯·éšæ—¶å¤åˆ¶å¹¶è¿è¡Œä¸‹é¢çš„ä»£ç ã€‚æ‚¨éšæ—¶å¯ä»¥å›æ¥æ›´ä»”ç»†åœ°æ£€æŸ¥è®­ç»ƒå¾ªç¯ï¼Œæ¯”å¦‚å½“æ‚¨åœ¨ç­‰å¾…æ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ã€‚ğŸ¤—
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Phew, that was quite a bit of code! But youâ€™re finally ready to launch the
    training with ğŸ¤— Accelerateâ€™s [notebook_launcher](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/launchers#accelerate.notebook_launcher)
    function. Pass the function the training loop, all the training arguments, and
    the number of processes (you can change this value to the number of GPUs available
    to you) to use for training:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å“‡ï¼Œé‚£æ˜¯ç›¸å½“å¤šçš„ä»£ç ï¼ä½†æ˜¯æ‚¨ç»ˆäºå‡†å¤‡å¥½ä½¿ç”¨ğŸ¤— Accelerateçš„[notebook_launcher](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/launchers#accelerate.notebook_launcher)å‡½æ•°å¯åŠ¨è®­ç»ƒäº†ã€‚å°†è®­ç»ƒå¾ªç¯ã€æ‰€æœ‰è®­ç»ƒå‚æ•°ä»¥åŠè¦ç”¨äºè®­ç»ƒçš„è¿›ç¨‹æ•°ï¼ˆæ‚¨å¯ä»¥å°†æ­¤å€¼æ›´æ”¹ä¸ºæ‚¨å¯ç”¨çš„GPUæ•°é‡ï¼‰ä¼ é€’ç»™è¯¥å‡½æ•°ï¼š
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Once training is complete, take a look at the final ğŸ¦‹ images ğŸ¦‹ generated by
    your diffusion model!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼ŒæŸ¥çœ‹æ‚¨çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æœ€ç»ˆğŸ¦‹å›¾åƒğŸ¦‹ï¼
- en: '[PRE18]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/3c1b49414fd4ace41217daf06b50b326.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c1b49414fd4ace41217daf06b50b326.png)'
- en: Next steps
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'Unconditional image generation is one example of a task that can be trained.
    You can explore other tasks and training techniques by visiting the [ğŸ§¨ Diffusers
    Training Examples](../training/overview) page. Here are some examples of what
    you can learn:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ˜¯ä¸€ä¸ªå¯ä»¥è®­ç»ƒçš„ä»»åŠ¡ç¤ºä¾‹ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¿é—®[ğŸ§¨ Diffusers Training Examples](../training/overview)é¡µé¢æ¥æ¢ç´¢å…¶ä»–ä»»åŠ¡å’Œè®­ç»ƒæŠ€æœ¯ã€‚ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥å­¦åˆ°çš„ä¸€äº›ç¤ºä¾‹ï¼š
- en: '[Textual Inversion](../training/text_inversion), an algorithm that teaches
    a model a specific visual concept and integrates it into the generated image.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬åè½¬](../training/text_inversion)ï¼Œä¸€ç§æ•™å¯¼æ¨¡å‹ç‰¹å®šè§†è§‰æ¦‚å¿µå¹¶å°†å…¶æ•´åˆåˆ°ç”Ÿæˆçš„å›¾åƒä¸­çš„ç®—æ³•ã€‚'
- en: '[DreamBooth](../training/dreambooth), a technique for generating personalized
    images of a subject given several input images of the subject.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DreamBooth](../training/dreambooth)ï¼Œä¸€ç§æ ¹æ®ä¸»é¢˜çš„å‡ å¼ è¾“å…¥å›¾åƒç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒçš„æŠ€æœ¯ã€‚'
- en: '[Guide](../training/text2image) to finetuning a Stable Diffusion model on your
    own dataset.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æŒ‡å—](../training/text2image)æ¥åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒç¨³å®šæ‰©æ•£æ¨¡å‹ã€‚'
- en: '[Guide](../training/lora) to using LoRA, a memory-efficient technique for finetuning
    really large models faster.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æŒ‡å—](../training/lora)æ¥ä½¿ç”¨LoRAï¼Œä¸€ç§ç”¨äºæ›´å¿«å¾®è°ƒéå¸¸å¤§æ¨¡å‹çš„å†…å­˜é«˜æ•ˆæŠ€æœ¯ã€‚'
