- en: Vision Encoder Decoder Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉编码器解码器模型
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    can be used to initialize an image-to-text model with any pretrained Transformer-based
    vision model as the encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit), [Swin](swin))
    and any pretrained language model as the decoder (*e.g.* [RoBERTa](roberta), [GPT2](gpt2),
    [BERT](bert), [DistilBERT](distilbert)).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)可用于使用任何预训练的基于Transformer的视觉模型作为编码器（例如[ViT](vit)、[BEiT](beit)、[DeiT](deit)、[Swin](swin)）和任何预训练语言模型作为解码器（例如[RoBERTa](roberta)、[GPT2](gpt2)、[BERT](bert)、[DistilBERT](distilbert)）初始化图像到文本模型。'
- en: 'The effectiveness of initializing image-to-text-sequence models with pretrained
    checkpoints has been shown in (for example) [TrOCR: Transformer-based Optical
    Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)
    by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun
    Li, Furu Wei.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '使用预训练检查点初始化图像到文本序列模型的有效性已在Minghao Li、Tengchao Lv、Lei Cui、Yijuan Lu、Dinei Florencio、Cha
    Zhang、Zhoujun Li、Furu Wei的文章[TrOCR: Transformer-based Optical Character Recognition
    with Pre-trained Models](https://arxiv.org/abs/2109.10282)中得到展示。'
- en: After such a [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    has been trained/fine-tuned, it can be saved/loaded just like any other models
    (see the examples below for more information).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练/微调了这样一个[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)之后，它可以像其他模型一样保存/加载（有关更多信息，请参见下面的示例）。
- en: An example application is image captioning, in which the encoder is used to
    encode the image, after which an autoregressive language model generates the caption.
    Another example is optical character recognition. Refer to [TrOCR](trocr), which
    is an instance of [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例应用是图像字幕，其中编码器用于对图像进行编码，之后自回归语言模型生成字幕。另一个示例是光学字符识别。请参考[TrOCR](trocr)，这是[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)的一个实例。
- en: Randomly initializing VisionEncoderDecoderModel from model configurations.
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从模型配置随机初始化VisionEncoderDecoderModel。
- en: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    can be randomly initialized from an encoder and a decoder config. In the following
    example, we show how to do this using the default [ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)
    configuration for the encoder and the default `BertForCausalLM` configuration
    for the decoder.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)可以从编码器和解码器配置随机初始化。在以下示例中，我们展示了如何使用编码器的默认[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)配置和解码器的默认`BertForCausalLM`配置来实现这一点。'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Initialising VisionEncoderDecoderModel from a pretrained encoder and a pretrained
    decoder.
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从预训练的编码器和预训练的解码器初始化VisionEncoderDecoderModel。
- en: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    can be initialized from a pretrained encoder checkpoint and a pretrained decoder
    checkpoint. Note that any pretrained Transformer-based vision model, *e.g.* [Swin](swin),
    can serve as the encoder and both pretrained auto-encoding models, *e.g.* BERT,
    pretrained causal language models, *e.g.* GPT2, as well as the pretrained decoder
    part of sequence-to-sequence models, *e.g.* decoder of BART, can be used as the
    decoder. Depending on which architecture you choose as the decoder, the cross-attention
    layers might be randomly initialized. Initializing [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned
    on a downstream task, as has been shown in [the *Warm-starting-encoder-decoder
    blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder). To do
    so, the `VisionEncoderDecoderModel` class provides a [VisionEncoderDecoderModel.from_encoder_decoder_pretrained()](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained)
    method.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)可以从预训练的编码器检查点和预训练的解码器检查点初始化。请注意，任何预训练的基于Transformer的视觉模型，例如[Swin](swin)，都可以作为编码器，而预训练的自编码模型，例如BERT，预训练的因果语言模型，例如GPT2，以及序列到序列模型的预训练解码器部分，例如BART的解码器，都可以作为解码器。根据您选择的解码器架构，交叉注意力层可能会被随机初始化。从预训练的编码器和解码器检查点初始化[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)需要对模型进行下游任务的微调，正如在*Warm-starting-encoder-decoder
    blog post*中所示。为此，`VisionEncoderDecoderModel`类提供了一个[VisionEncoderDecoderModel.from_encoder_decoder_pretrained()](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained)方法。'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading an existing VisionEncoderDecoderModel checkpoint and perform inference.
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载现有的VisionEncoderDecoderModel检查点并执行推理。
- en: To load fine-tuned checkpoints of the `VisionEncoderDecoderModel` class, [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    provides the `from_pretrained(...)` method just like any other model architecture
    in Transformers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载`VisionEncoderDecoderModel`类的微调检查点，[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)提供了`from_pretrained(...)`方法，就像Transformers中的任何其他模型架构一样。
- en: To perform inference, one uses the `generate` method, which allows to autoregressively
    generate text. This method supports various forms of decoding, such as greedy,
    beam search and multinomial sampling.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行推断，可以使用 `generate` 方法，该方法允许自回归生成文本。此方法支持各种解码形式，如贪婪、束搜索和多项式采样。
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Loading a PyTorch checkpoint into TFVisionEncoderDecoderModel .
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 PyTorch checkpoint 加载到 TFVisionEncoderDecoderModel 中。
- en: '[TFVisionEncoderDecoderModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    currently doesn’t support initializing the model from a PyTorch checkpoint. Passing
    `from_pt=True` to this method will throw an exception. If there are only PyTorch
    checkpoints for a particular vision encoder-decoder model, a workaround is:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFVisionEncoderDecoderModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    目前不支持从 PyTorch checkpoint 初始化模型。将 `from_pt=True` 传递给此方法将引发异常。如果特定视觉编码器-解码器模型仅有
    PyTorch checkpoints，可以使用以下解决方法：'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Once the model is created, it can be fine-tuned similar to BART, T5 or any
    other encoder-decoder model on a dataset of (image, text) pairs. As you can see,
    only 2 inputs are required for the model in order to compute a loss: `pixel_values`
    (which are the images) and `labels` (which are the `input_ids` of the encoded
    target sequence).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型后，可以类似于 BART、T5 或任何其他编码器-解码器模型在（图像，文本）对数据集上进行微调。正如您所看到的，为了计算损失，模型只需要 2 个输入：`pixel_values`（即图像）和
    `labels`（即编码目标序列的 `input_ids`）。
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This model was contributed by [nielsr](https://github.com/nielsrogge). This
    model’s TensorFlow and Flax versions were contributed by [ydshieh](https://github.com/ydshieh).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由 [nielsr](https://github.com/nielsrogge) 贡献。此模型的 TensorFlow 和 Flax 版本由 [ydshieh](https://github.com/ydshieh)
    贡献。
- en: VisionEncoderDecoderConfig
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VisionEncoderDecoderConfig
- en: '### `class transformers.VisionEncoderDecoderConfig`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VisionEncoderDecoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L33)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L33)'
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments. Notably:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（*可选*）— 关键字参数字典。特别包括：'
- en: '`encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — An instance of a configuration object that defines the encoder config.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，*可选*）—
    定义编码器配置的配置对象实例。'
- en: '`decoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — An instance of a configuration object that defines the decoder config.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，*可选*）—
    定义解码器配置的配置对象实例。'
- en: '[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)
    is the configuration class to store the configuration of a [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel).
    It is used to instantiate a Vision-Encoder-Text-Decoder model according to the
    specified arguments, defining the encoder and decoder configs.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)
    是用于存储 [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    配置的配置类。根据指定的参数实例化 Vision-Encoder-Text-Decoder 模型，定义编码器和解码器配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `from_encoder_decoder_configs`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_encoder_decoder_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L100)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L100)'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Returns
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)'
- en: An instance of a configuration object
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象实例
- en: Instantiate a [VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)
    (or a derived class) from a pre-trained encoder model configuration and decoder
    model configuration.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练编码器模型配置和解码器模型配置实例化一个 [VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)（或派生类）。
- en: PytorchHide Pytorch content
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: VisionEncoderDecoderModel
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VisionEncoderDecoderModel
- en: '### `class transformers.VisionEncoderDecoderModel`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VisionEncoderDecoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L150)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L150)'
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: This class can be used to initialize an image-to-text-sequence model with any
    pretrained vision autoencoding model as the encoder and any pretrained text autoregressive
    model as the decoder. The encoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function and the decoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function. Cross-attention layers are automatically added to the decoder and should
    be fine-tuned on a downstream generative task, like image captioning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可以用来初始化一个图像到文本序列模型，其中预训练的视觉自编码模型作为编码器，预训练的文本自回归模型作为解码器。编码器通过 [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    函数加载，解码器通过 [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如图像字幕）上进行微调。
- en: The effectiveness of initializing sequence-to-sequence models with pretrained
    checkpoints for sequence generation tasks was shown in [Leveraging Pre-trained
    Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by
    Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei
    Li, Peter J. Liu.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461)
    中，Sascha Rothe、Shashi Narayan、Aliaksei Severyn、Michael Matena、Yanqi Zhou、Wei Li、Peter
    J. Liu 展示了使用预训练检查点初始化序列到序列模型进行序列生成任务的有效性。
- en: 'Additionally, in [TrOCR: Transformer-based Optical Character Recognition with
    Pre-trained Models](https://arxiv.org/abs/2109.10282) it is shown how leveraging
    large pretrained vision models for optical character recognition (OCR) yields
    a significant performance improvement.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在 [TrOCR: Transformer-based Optical Character Recognition with Pre-trained
    Models](https://arxiv.org/abs/2109.10282) 中，展示了如何利用大型预训练视觉模型进行光学字符识别（OCR）可以显著提高性能。'
- en: After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned,
    it can be saved/loaded just like any other models (see the examples for more information).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练/微调了这样一个视觉-编码器-文本-解码器模型后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    is a generic model class that will be instantiated as a transformer architecture
    with one of the base vision model classes of the library as encoder and another
    one as decoder when created with the :meth*~transformers.AutoModel.from_pretrained*
    class method for the encoder and :meth*~transformers.AutoModelForCausalLM.from_pretrained*
    class method for the decoder.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    是一个通用的模型类，当使用 :meth*~transformers.AutoModel.from_pretrained* 类方法为编码器创建一个基础视觉模型类，并为解码器创建另一个基础视觉模型类时，将实例化为一个变压器架构。'
- en: '#### `forward`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L515)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L515)'
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using an image processor
    (e.g. if you use ViT as the encoder, you should use [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以通过图像处理器获得（例如，如果您使用 ViT 作为编码器，应该使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参阅
    [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了 `past_key_values`，可选择仅输入最后的 `decoder_input_ids`（参见 `past_key_values`）。
- en: For training, `decoder_input_ids` are automatically created by the model by
    shifting the `labels` to the right, replacing -100 by the `pad_token_id` and prepending
    them with the `decoder_start_token_id`.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于训练，`decoder_input_ids` 会被模型自动创建，通过将 `labels` 向右移动，用 `pad_token_id` 替换 -100，并在前面加上
    `decoder_start_token_id`。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 默认行为：生成一个张量，忽略 `decoder_input_ids` 中的填充标记。因果掩码也将默认使用。'
- en: '`encoder_outputs` (`tuple(torch.FloatTensor)`, *optional*) — This tuple must
    consist of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) is a tensor of hidden-states at the output of the last layer of
    the encoder. Used in the cross-attention of the decoder.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(torch.FloatTensor)`, *optional*) — 此元组必须包含 (`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` (`torch.FloatTensor`
    of shape `(batch_size, sequence_length, hidden_size)`) 是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为 `config.n_layers`，每个元组包含
    4 个形状为 `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)` 的张量）
    — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了 `past_key_values`，用户可以选择仅输入形状为 `(batch_size, 1)` 的最后的 `decoder_input_ids`（即没有将过去的键值状态提供给该模型的那些）而不是形状为
    `(batch_size, sequence_length)` 的所有 `decoder_input_ids`。
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `decoder_input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `decoder_input_ids`。如果您想要更多控制如何将
    `decoder_input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss for the decoder. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算解码器的掩码语言建模损失的标签。索引应在 `[-100, 0, ..., config.vocab_size]` 范围内（参见 `input_ids`
    文档字符串）。索引设置为 `-100` 的标记将被忽略（掩码），损失仅计算具有标签在 `[0, ..., config.vocab_size]` 范围内的标记。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为 `True`，将返回 `past_key_values` 键值状态，可用于加速解码（参见
    `past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a `~utils.Seq2SeqLMOutput` instead of a plain tuple.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 如果设置为 `True`，模型将返回一个 `~utils.Seq2SeqLMOutput`
    而不是一个普通元组。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments. Keyword
    arguments come in two flavors:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 剩余的关键字参数字典。关键字参数有两种类型：'
- en: Without a prefix which will be input as `**encoder_kwargs` for the encoder forward
    function.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有前缀，将作为 `**encoder_kwargs` 输入到编码器前向函数中。
- en: With a *decoder_* prefix which will be input as `**decoder_kwargs` for the decoder
    forward function.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 *decoder_* 前缀，将作为 `**decoder_kwargs` 输入到解码器前向函数中。
- en: Returns
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    and inputs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）和输入而异的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，*optional*，当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 一个长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 一个元组，包含形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`（如果模型有嵌入层，则为嵌入层输出的一个+每层输出的一个）。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 一个元组，包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 一个元组，包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 一个元组，包含形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`（如果模型有嵌入层，则为嵌入层输出的一个+每层输出的一个）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 一个元组，包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `from_encoder_decoder_pretrained`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_encoder_decoder_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L361)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L361)'
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_pretrained_model_name_or_path` (`str`, *optional*) — Information necessary
    to initiate the image encoder. Can be either:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_pretrained_model_name_or_path`（`str`，*可选*）- 启动图像编码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. An example is `google/vit-base-patch16-224-in21k`.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，预训练模型的*模型ID*，托管在huggingface.co上的模型存储库内。一个示例是`google/vit-base-patch16-224-in21k`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存的模型权重的目录*的路径，例如，`./my_model_directory/`。
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*指向tensorflow索引检查点文件*的路径或url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用此加载路径比使用提供的转换脚本将TensorFlow检查点转换为PyTorch模型并加载PyTorch模型要慢。
- en: '`decoder_pretrained_model_name_or_path` (`str`, *optional*, defaults to `None`)
    — Information necessary to initiate the text decoder. Can be either:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_pretrained_model_name_or_path`（`str`，*可选*，默认为`None`）- 启动文本解码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，预训练模型的*模型ID*，托管在huggingface.co上的模型存储库内。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存的模型权重的目录*的路径，例如，`./my_model_directory/`。
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*指向tensorflow索引检查点文件*的路径或url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用此加载路径比使用提供的转换脚本将TensorFlow检查点转换为PyTorch模型并加载PyTorch模型要慢。
- en: '`model_args` (remaining positional arguments, *optional*) — All remaning positional
    arguments will be passed to the underlying model’s `__init__` method.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`（剩余的位置参数，*可选*）- 所有剩余的位置参数将传递给底层模型的`__init__`方法。'
- en: '`kwargs` (remaining dictionary of keyword arguments, *optional*) — Can be used
    to update the configuration object (after it being loaded) and initiate the model
    (e.g., `output_attentions=True`).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（剩余的关键字参数字典，*可选*）- 可用于更新配置对象（在加载后）并启动模型（例如，`output_attentions=True`）。'
- en: To update the encoder configuration, use the prefix *encoder_* for each configuration
    parameter.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新编码器配置，请为每个配置参数使用前缀*encoder_*。
- en: To update the decoder configuration, use the prefix *decoder_* for each configuration
    parameter.
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新解码器配置，请为每个配置参数使用前缀*decoder_*。
- en: To update the parent model configuration, do not use a prefix for each configuration
    parameter.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新父模型配置，请不要为每个配置参数使用前缀。
- en: Behaves differently depending on whether a `config` is provided or automatically
    loaded.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据是否提供`config`而表现不同。
- en: Instantiate an encoder and a decoder from one or two base classes of the library
    from pretrained model checkpoints.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型检查点中的一个或两个基类库中实例化一个编码器和一个解码器。
- en: The model is set in evaluation mode by default using `model.eval()` (Dropout
    modules are deactivated). To train the model, you need to first set it back in
    training mode with `model.train()`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，使用`model.eval()`将模型设置为评估模式（Dropout模块被停用）。要训练模型，您需要首先使用`model.train()`将其设置回训练模式。
- en: 'Example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: TensorFlowHide TensorFlow content
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: TFVisionEncoderDecoderModel
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFVisionEncoderDecoderModel
- en: '### `class transformers.TFVisionEncoderDecoderModel`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFVisionEncoderDecoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L175)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L175)'
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）—
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: This class can be used to initialize an image-to-text-sequence model with any
    pretrained vision autoencoding model as the encoder and any pretrained text autoregressive
    model as the decoder. The encoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function and the decoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function. Cross-attention layers are automatically added to the decoder and should
    be fine-tuned on a downstream generative task, like image captioning.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可用于使用任何预训练的视觉自编码模型作为编码器和任何预训练的文本自回归模型作为解码器来初始化一个图像到文本序列模型。编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载，解码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如图像字幕）上进行微调。
- en: The effectiveness of initializing sequence-to-sequence models with pretrained
    checkpoints for sequence generation tasks was shown in [Leveraging Pre-trained
    Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by
    Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei
    Li, Peter J. Liu.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou,
    Wei Li, Peter J. Liu的《利用预训练检查点进行序列生成任务》](https://arxiv.org/abs/1907.12461)中展示了使用预训练检查点初始化序列生成任务的序列到序列模型的有效性。
- en: 'Additionally, in [TrOCR: Transformer-based Optical Character Recognition with
    Pre-trained Models](https://arxiv.org/abs/2109.10282) it is shown how leveraging
    large pretrained vision models for optical character recognition (OCR) yields
    a significant performance improvement.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在[TrOCR: 基于Transformer的预训练模型的光学字符识别](https://arxiv.org/abs/2109.10282)中展示了如何利用大型预训练的视觉模型进行光学字符识别（OCR）可以显著提高性能。'
- en: After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned,
    it can be saved/loaded just like any other models (see the examples for more information).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练/微调了这样一个Vision-Encoder-Text-Decoder模型之后，它可以像任何其他模型一样保存/加载（查看示例以获取更多信息）。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: '[TFVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel)
    is a generic model class that will be instantiated as a transformer architecture
    with one of the base vision model classes of the library as encoder and another
    one of the base model classes as decoder when created with the [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    class method for the encoder and [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    class method for the decoder.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel)是一个通用模型类，当使用[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)类方法为编码器创建一个库中的基础视觉模型类，并为解码器创建另一个基础模型类时，将实例化为一个变压器架构。'
- en: '#### `call`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L458)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L458)'
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    the vision’s model’s image processor. For example, using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用视觉模型的图像处理器获得。例如，使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`decoder_input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如果使用了 `past_key_values`，可以选择仅输入最后的 `decoder_input_ids`（参见 `past_key_values`）。 '
- en: Provide for sequence to sequence training to the decoder. Indices can be obtained
    using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为解码器提供序列到序列训练。可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '`decoder_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    target_sequence_length)`, *optional*) — Default behavior: generate a tensor that
    ignores pad tokens in `decoder_input_ids`. Causal mask will also be used by default.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    target_sequence_length)`, *optional*) — 默认行为：生成一个张量，忽略 `decoder_input_ids` 中的填充标记。因果掩码也将默认使用。'
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — This tuple must
    consist of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    is a tensor of hidden-states at the output of the last layer of the encoder. Used
    in the cross-attention of the decoder.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — 这个元组必须包含 (`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` (`tf.Tensor`
    of shape `(batch_size, sequence_length, hidden_size)`) 是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))` of length `config.n_layers` with
    each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(tf.Tensor))` of length `config.n_layers` with
    each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（这些没有将它们的过去键值状态提供给此模型）的形状为
    `(batch_size, 1)`，而不是所有形状为 `(batch_size, sequence_length)` 的 `decoder_input_ids`。
- en: '`decoder_inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    target_sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing
    `decoder_input_ids` you can choose to directly pass an embedded representation.
    This is useful if you want more control over how to convert `decoder_input_ids`
    indices into associated vectors than the model’s internal embedding lookup matrix.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    target_sequence_length, hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递 `decoder_input_ids`。如果您想要更多控制权来将
    `decoder_input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`labels` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the masked language modeling loss for the decoder.
    Indices should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring)
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于计算解码器的掩码语言建模损失的标签。索引应在 `[-100, 0, ..., config.vocab_size]`（参见
    `input_ids` 文档字符串）。索引设置为 `-100` 的标记将被忽略（掩码），损失仅计算具有标签在 `[0, ..., config.vocab_size]`
    中的标记。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为 `True`，将返回 `past_key_values` 键值状态，并可用于加速解码（参见
    `past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a `~utils.Seq2SeqLMOutput` instead of a plain tuple.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 如果设置为 `True`，模型将返回一个 `~utils.Seq2SeqLMOutput`
    而不是一个普通元组。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments. Keyword
    arguments come in two flavors:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 剩余的关键字参数字典。关键字参数有两种类型：'
- en: Without a prefix which will be input as `**encoder_kwargs` for the encoder forward
    function.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有前缀，将作为编码器前向函数的`**encoder_kwargs`输入。
- en: With a *decoder_* prefix which will be input as `**decoder_kwargs` for the decoder
    forward function.
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有*decoder_*前缀，将作为解码器前向函数的`**decoder_kwargs`输入。
- en: Returns
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    and inputs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) — Language modeling loss.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, 其中n是未屏蔽标签的数量，在提供`labels`时返回)
    — 语言建模损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[tf.Tensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。
- en: '`decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`decoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力SoftMax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力SoftMax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [TFVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#### `from_encoder_decoder_pretrained`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_encoder_decoder_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L310)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L310)'
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_pretrained_model_name_or_path` (`str`, *optional*) — Information necessary
    to initiate the encoder. Can be either:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_pretrained_model_name_or_path`（`str`，*可选*） — 初始化编码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. An example is `google/vit-base-patch16-224-in21k`.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型的*模型id*，托管在huggingface.co上的模型存储库中。例如，`google/vit-base-patch16-224-in21k`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指向使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)保存的模型权重的*目录*的路径，例如，`./my_model_directory/`。
- en: A path or url to a *pytorch index checkpoint file* (e.g, `./pt_model/`). In
    this case, `encoder_from_pt` should be set to `True`.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指向*pytorch索引检查点文件*的路径或url（例如，`./pt_model/`）。在这种情况下，`encoder_from_pt`应设置为`True`。
- en: '`decoder_pretrained_model_name_or_path` (`str`, *optional*, defaults to *None*)
    — Information necessary to initiate the decoder. Can be either:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_pretrained_model_name_or_path`（`str`，*可选*，默认为*None*） — 初始化解码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型的*模型id*，托管在huggingface.co上的模型存储库中。有效的模型id可以位于根级别，如`bert-base-uncased`，或在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指向包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)保存的模型权重的*目录*的路径，例如，`./my_model_directory/`。
- en: A path or url to a *pytorch checkpoint file* (e.g, `./pt_model/`). In this case,
    `decoder_from_pt` should be set to `True`.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指向*pytorch检查点文件*的路径或url（例如，`./pt_model/`）。在这种情况下，`decoder_from_pt`应设置为`True`。
- en: '`model_args` (remaining positional arguments, *optional*) — All remaning positional
    arguments will be passed to the underlying model’s `__init__` method.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`（剩余的位置参数，*可选*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。'
- en: '`kwargs` (remaining dictionary of keyword arguments, *optional*) — Can be used
    to update the configuration object (after it being loaded) and initiate the model
    (e.g., `output_attentions=True`).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（剩余的关键字参数字典，*可选*） — 可用于更新配置对象（加载后）并初始化模型（例如，`output_attentions=True`）。'
- en: To update the encoder configuration, use the prefix *encoder_* for each configuration
    parameter.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新编码器配置时，对每个配置参数使用前缀*encoder_*。
- en: To update the decoder configuration, use the prefix *decoder_* for each configuration
    parameter.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新解码器配置时，对每个配置参数使用前缀*decoder_*。
- en: To update the parent model configuration, do not use a prefix for each configuration
    parameter.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新父模型配置，请不要对每个配置参数使用前缀。
- en: Behaves differently depending on whether a `config` is provided or automatically
    loaded.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据是否提供`config`或自动加载而表现不同。
- en: Instantiate an encoder and a decoder from one or two base classes of the library
    from pretrained model checkpoints.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型检查点实例化一个编码器和一个解码器，可以是库中一个或两个基类的预训练模型检查点。
- en: 'Example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: JAXHide JAX content
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: JAXHide JAX content
- en: FlaxVisionEncoderDecoderModel
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxVisionEncoderDecoderModel
- en: '### `class transformers.FlaxVisionEncoderDecoderModel`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxVisionEncoderDecoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L267)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L267)'
- en: '[PRE18]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`） — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可以用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的`dtype`执行。
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`请注意，这仅指定计算的dtype，不影响模型参数的dtype。`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您希望更改模型参数的dtype，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。
- en: This class can be used to initialize an image-to-text-sequence model with any
    pretrained vision autoencoding model as the encoder and any pretrained text autoregressive
    model as the decoder. The encoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function and the decoder is loaded via [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    function. Cross-attention layers are automatically added to the decoder and should
    be fine-tuned on a downstream generative task, like image captioning.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可以用来初始化一个图像到文本序列模型，其中编码器是任何预训练的视觉自编码模型，解码器是任何预训练的文本自回归模型。编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载，解码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载。交叉注意力层会自动添加到解码器上，并应该在下游生成任务（如图像字幕）上进行微调。
- en: The effectiveness of initializing sequence-to-sequence models with pretrained
    checkpoints for sequence generation tasks was shown in [Leveraging Pre-trained
    Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by
    Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei
    Li, Peter J. Liu.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou,
    Wei Li, Peter J. Liu](https://arxiv.org/abs/1907.12461)的研究中展示了使用预训练检查点初始化序列生成任务的序列到序列模型的有效性。
- en: 'Additionally, in [TrOCR: Transformer-based Optical Character Recognition with
    Pre-trained Models](https://arxiv.org/abs/2109.10282) it is shown how leveraging
    large pretrained vision models for optical character recognition (OCR) yields
    a significant performance improvement.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在[TrOCR: Transformer-based Optical Character Recognition with Pre-trained
    Models](https://arxiv.org/abs/2109.10282)中展示了如何利用大型预训练视觉模型进行光学字符识别（OCR）可以显著提高性能。'
- en: After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned,
    it can be saved/loaded just like any other models (see the examples for more information).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 训练/微调了这样一个视觉-编码器-文本-解码器模型后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规Flax模块，并参考Flax文档以了解与一般用法和行为相关的所有事项。
- en: '[FlaxVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel)
    is a generic model class that will be instantiated as a transformer architecture
    with the module (flax.nn.Module) of one of the base vision model classes of the
    library as encoder module and another one as decoder module when created with
    the :meth*~transformers.FlaxAutoModel.from_pretrained* class method for the encoder
    and :meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained* class method
    for the decoder.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel)是一个通用的模型类，当使用:meth*~transformers.FlaxAutoModel.from_pretrained*类方法为编码器创建模块（flax.nn.Module）时，会实例化为一个transformer架构，库中的一个基本视觉模型类作为编码器模块，另一个作为解码器模块，并使用:meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained*类方法为解码器创建模块。'
- en: '#### `__call__`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L599)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L599)'
- en: '[PRE19]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`jnp.ndarray` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using the vision model’s
    image processor. For example, using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`jnp.ndarray`，形状为`(batch_size, num_channels, height, width)`）—
    像素值。像素值可以使用视觉模型的图像处理器获得。例如，使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`decoder_input_ids` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（`jnp.ndarray`，形状为`(batch_size, target_sequence_length)`，*可选*）—
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: '`decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`jnp.ndarray`，形状为`(batch_size, target_sequence_length)`，*可选*)
    — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: '`decoder_position_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each decoder input sequence tokens in the
    position embeddings. Selected in the range `[0, config.decoder.max_position_embeddings
    - 1]`.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_position_ids` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`，*可选*)
    — 每个解码器输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.decoder.max_position_embeddings - 1]`。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a `~utils.FlaxSeq2SeqLMOutput` instead of a plain tuple.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 如果设置为`True`，模型将返回一个`~utils.FlaxSeq2SeqLMOutput`而不是一个普通元组。'
- en: Returns
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    and inputs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）和输入而异的各种元素。
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — Tuple of `tuple(jnp.ndarray)` of
    length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`decoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`jnp.ndarray`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 +
    一个用于每层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每层输出的隐藏状态加上初始嵌入输出。
- en: '`encoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [FlaxVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#### `from_encoder_decoder_pretrained`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_encoder_decoder_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L724)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L724)'
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *optional*)
    — Information necessary to initiate the encoder. Can be either:'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *可选*) —
    初始化编码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. An example is `google/vit-base-patch16-224-in21k`.
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，托管在huggingface.co上的模型存储库中的预训练模型的*模型ID*。一个示例是`google/vit-base-patch16-224-in21k`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained)保存的模型权重的*目录*路径，例如，`./my_model_directory/`。
- en: '`decoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *optional*,
    defaults to `None`) — Information necessary to initiate the decoder. Can be either:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *可选*, 默认为`None`)
    — 初始化解码器所需的信息。可以是：'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，预训练模型的*模型ID*，托管在huggingface.co上的模型存储库中。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained)保存的模型权重的*目录*路径，例如，`./my_model_directory/`。
- en: '`model_args` (remaining positional arguments, *optional*) — All remaning positional
    arguments will be passed to the underlying model’s `__init__` method.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`（剩余的位置参数，*可选*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。'
- en: '`kwargs` (remaining dictionary of keyword arguments, *optional*) — Can be used
    to update the configuration object (after it being loaded) and initiate the model
    (e.g., `output_attentions=True`).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（剩余的关键字参数字典，*可选*） — 可用于更新配置对象（在加载后）并初始化模型（例如，`output_attentions=True`）。'
- en: To update the encoder configuration, use the prefix *encoder_* for each configuration
    parameter.
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新编码器配置，请为每个配置参数使用前缀*encoder_*。
- en: To update the decoder configuration, use the prefix *decoder_* for each configuration
    parameter.
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新解码器配置，请为每个配置参数使用前缀*decoder_*。
- en: To update the parent model configuration, do not use a prefix for each configuration
    parameter.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新父模型配置时，不要为每个配置参数使用前缀。
- en: Behaves differently depending on whether a `config` is provided or automatically
    loaded.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据是否提供`config`或自动加载而表现不同。
- en: Instantiate an encoder and a decoder from one or two base classes of the library
    from pretrained model checkpoints.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型检查点实例化一个编码器和一个解码器，可以是库中一个或两个基类。
- en: 'Example:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
