- en: FLAN-UL2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FLAN-UL2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses
    the same configuration as the [UL2](ul2) model released earlier last year. It
    was fine tuned using the “Flan” prompt tuning and dataset collection. Similar
    to `Flan-T5`, one can directly use FLAN-UL2 weights without finetuning the model:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Flan-UL2是基于T5架构的编码器解码器模型。它使用与去年早些时候发布的UL2模型相同的配置。它经过“Flan”提示调整和数据集收集进行微调。与`Flan-T5`类似，可以直接使用FLAN-UL2权重而无需微调模型：
- en: 'According to the original blog here are the notable improvements:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据原始博客，以下是显著的改进：
- en: The original UL2 model was only trained with receptive field of 512, which made
    it non-ideal for N-shot prompting where N is large.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的UL2模型只使用了512的感受野进行训练，这使得它对于大量N-shot提示不理想。
- en: The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable
    for few-shot in-context learning.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flan-UL2检查点使用2048的感受野，使其更适用于少样本上下文学习。
- en: 'The original UL2 model also had mode switch tokens that was rather mandatory
    to get good performance. However, they were a little cumbersome as this requires
    often some changes during inference or finetuning. In this update/change, we continue
    training UL2 20B for an additional 100k steps (with small batch) to forget “mode
    tokens” before applying Flan instruction tuning. This Flan-UL2 checkpoint does
    not require mode tokens anymore. Google has released the following variants:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的UL2模型还有模式切换令牌，这对于获得良好性能是相当必要的。然而，它们有点繁琐，因为这经常需要在推理或微调过程中进行一些更改。在这次更新/更改中，我们继续训练UL2
    20B额外的100k步（使用小批量）来忘记“模式令牌”，然后应用Flan指令调整。这个Flan-UL2检查点不再需要模式令牌。Google发布了以下变体：
- en: The original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 原始检查点可以在[这里](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints)找到。
- en: Running on low resource devices
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在资源有限的设备上运行
- en: The model is pretty heavy (~40GB in half precision) so if you just want to run
    the model, make sure you load your model in 8bit, and use `device_map="auto"`
    to make sure you don’t have any OOM issue!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型非常庞大（半精度约40GB），因此如果您只想运行模型，请确保以8位加载您的模型，并使用`device_map="auto"`确保您没有任何OOM问题！
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Refer to [T5’s documentation page](t5) for API reference, tips, code examples
    and notebooks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[T5的文档页面](t5)获取API参考、提示、代码示例和笔记本。
