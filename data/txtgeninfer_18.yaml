- en: PagedAttention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PagedAttention
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention](https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention](https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: LLMs struggle with memory limitations during generation. In the decoding part
    of generation, all the attention keys and values generated for previous tokens
    are stored in GPU memory for reuse. This is called *KV cache*, and it may take
    up a large amount of memory for large models and long sequences.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成过程中，LLMs面临内存限制的挑战。在生成的解码部分，为以前的标记生成的所有注意力键和值存储在GPU内存中以便重复使用。这被称为*KV缓存*，对于大型模型和长序列可能占用大量内存。
- en: PagedAttention attempts to optimize memory use by partitioning the KV cache
    into blocks that are accessed through a lookup table. Thus, the KV cache does
    not need to be stored in contiguous memory, and blocks are allocated as needed.
    The memory efficiency can increase GPU utilization on memory-bound workloads,
    so more inference batches can be supported.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: PagedAttention尝试通过将KV缓存分成通过查找表访问的块来优化内存使用。因此，KV缓存不需要存储在连续的内存中，块会根据需要分配。内存效率可以增加GPU在内存受限工作负载上的利用率，从而支持更多的推理批次。
- en: The use of a lookup table to access the memory blocks can also help with KV
    sharing across multiple generations. This is helpful for techniques such as *parallel
    sampling*, where multiple outputs are generated simultaneously for the same prompt.
    In this case, the cached KV blocks can be shared among the generations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用查找表访问内存块也有助于在多代之间共享KV。这对于诸如*并行采样*之类的技术非常有帮助，其中多个输出同时为相同提示生成。在这种情况下，缓存的KV块可以在各代之间共享。
- en: TGI’s PagedAttention implementation leverages the custom cuda kernels developed
    by the [vLLM Project](https://github.com/vllm-project/vllm). You can learn more
    about this technique in the [project’s page](https://vllm.ai/).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: TGI的PagedAttention实现利用了[vLLM项目](https://github.com/vllm-project/vllm)开发的自定义cuda内核。您可以在[项目页面](https://vllm.ai/)了解更多关于这种技术的信息。
