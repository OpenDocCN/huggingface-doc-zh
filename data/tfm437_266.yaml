- en: DPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DPT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpt)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: The DPT model was proposed in [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413)
    by RenÃ© Ranftl, Alexey Bochkovskiy, Vladlen Koltun. DPT is a model that leverages
    the [Vision Transformer (ViT)](vit) as backbone for dense prediction tasks like
    semantic segmentation and depth estimation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DPT æ¨¡å‹ç”± RenÃ© Ranftlã€Alexey Bochkovskiyã€Vladlen Koltun åœ¨ [Vision Transformers
    for Dense Prediction](https://arxiv.org/abs/2103.13413) ä¸­æå‡ºã€‚DPT æ˜¯ä¸€ä¸ªåˆ©ç”¨ [Vision
    Transformer (ViT)](vit) ä½œä¸ºå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼‰çš„éª¨å¹²çš„æ¨¡å‹ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*We introduce dense vision transformers, an architecture that leverages vision
    transformers in place of convolutional networks as a backbone for dense prediction
    tasks. We assemble tokens from various stages of the vision transformer into image-like
    representations at various resolutions and progressively combine them into full-resolution
    predictions using a convolutional decoder. The transformer backbone processes
    representations at a constant and relatively high resolution and has a global
    receptive field at every stage. These properties allow the dense vision transformer
    to provide finer-grained and more globally coherent predictions when compared
    to fully-convolutional networks. Our experiments show that this architecture yields
    substantial improvements on dense prediction tasks, especially when a large amount
    of training data is available. For monocular depth estimation, we observe an improvement
    of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional
    network. When applied to semantic segmentation, dense vision transformers set
    a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture
    can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context
    where it also sets the new state of the art.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬ä»‹ç»äº†å¯†é›†è§†è§‰å˜æ¢å™¨ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è§†è§‰å˜æ¢å™¨ä»£æ›¿å·ç§¯ç½‘ç»œä½œä¸ºå¯†é›†é¢„æµ‹ä»»åŠ¡éª¨å¹²çš„æ¶æ„ã€‚æˆ‘ä»¬ä»è§†è§‰å˜æ¢å™¨çš„å„ä¸ªé˜¶æ®µæ±‡é›†ä»¤ç‰Œï¼Œå°†å®ƒä»¬ç»„åˆæˆå„ç§åˆ†è¾¨ç‡çš„å›¾åƒè¡¨ç¤ºï¼Œå¹¶é€æ¸å°†å®ƒä»¬ç»“åˆæˆä½¿ç”¨å·ç§¯è§£ç å™¨è¿›è¡Œå…¨åˆ†è¾¨ç‡é¢„æµ‹ã€‚å˜æ¢å™¨éª¨å¹²ä»¥æ’å®šä¸”ç›¸å¯¹è¾ƒé«˜çš„åˆ†è¾¨ç‡å¤„ç†è¡¨ç¤ºï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶æ®µå…·æœ‰å…¨å±€æ„Ÿå—é‡ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—å¯†é›†è§†è§‰å˜æ¢å™¨åœ¨ä¸å®Œå…¨å·ç§¯ç½‘ç»œç›¸æ¯”æä¾›æ›´ç²¾ç»†å’Œæ›´å…¨å±€ä¸€è‡´çš„é¢„æµ‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ¶æ„åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯å½“æœ‰å¤§é‡è®­ç»ƒæ•°æ®å¯ç”¨æ—¶ã€‚å¯¹äºå•ç›®æ·±åº¦ä¼°è®¡ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸æœ€å…ˆè¿›çš„å®Œå…¨å·ç§¯ç½‘ç»œç›¸æ¯”ï¼Œæ€§èƒ½ç›¸å¯¹æé«˜äº†é«˜è¾¾
    28%ã€‚å½“åº”ç”¨äºè¯­ä¹‰åˆ†å‰²æ—¶ï¼Œå¯†é›†è§†è§‰å˜æ¢å™¨åœ¨ ADE20K ä¸Šå–å¾—äº† 49.02% mIoU çš„æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºï¼Œè¯¥æ¶æ„å¯ä»¥åœ¨è¾ƒå°çš„æ•°æ®é›†ï¼ˆå¦‚
    NYUv2ã€KITTI å’Œ Pascal Contextï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¹Ÿåœ¨è¿™äº›æ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚*'
- en: '![drawing](../Images/a0659410be29958ad5c55dce63aa3e01.png) DPT architecture.
    Taken from the [original paper](https://arxiv.org/abs/2103.13413).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![drawing](../Images/a0659410be29958ad5c55dce63aa3e01.png) DPT æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2103.13413)ã€‚'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/isl-org/DPT).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”± [nielsr](https://huggingface.co/nielsr) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/isl-org/DPT)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: 'DPT is compatible with the `AutoBackbone` class. This allows to use the DPT
    framework with various computer vision backbones available in the library, such
    as `VitDetBackbone` or `Dinov2Backbone`. One can create it as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DPT å…¼å®¹ `AutoBackbone` ç±»ã€‚è¿™å…è®¸ä½¿ç”¨åº“ä¸­æä¾›çš„å„ç§è®¡ç®—æœºè§†è§‰éª¨å¹²ï¼ˆå¦‚ `VitDetBackbone` æˆ– `Dinov2Backbone`ï¼‰ä¸
    DPT æ¡†æ¶ä¸€èµ·ä½¿ç”¨ã€‚å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼åˆ›å»ºå®ƒï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with DPT.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆğŸŒ æ ‡å¿—ï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ DPTã€‚
- en: Demo notebooks for [DPTForDepthEstimation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForDepthEstimation)
    can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DPT).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DPTForDepthEstimation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForDepthEstimation)
    çš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DPT)æ‰¾åˆ°ã€‚'
- en: '[Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æŒ‡å—](../tasks/semantic_segmentation)'
- en: '[Monocular depth estimation task guide](../tasks/monocular_depth_estimation)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡æŒ‡å—](../tasks/monocular_depth_estimation)'
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: DPTConfig
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPTConfig
- en: '### `class transformers.DPTConfig`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.DPTConfig` ç±»'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/configuration_dpt.py#L33)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/configuration_dpt.py#L33)'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` æˆ– `function`, *å¯é€‰*, é»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ
    `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” åµŒå…¥ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`image_size` (`int`, *optional*, defaults to 384) â€” The size (resolution) of
    each image.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 384) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`patch_size` (`int`, *optional*, defaults to 16) â€” The size (resolution) of
    each patch.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 16) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“æ•°ã€‚'
- en: '`is_hybrid` (`bool`, *optional*, defaults to `False`) â€” Whether to use a hybrid
    backbone. Useful in the context of loading DPT-Hybrid models.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_hybrid` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨æ··åˆä¸»å¹²ã€‚åœ¨åŠ è½½DPT-Hybridæ¨¡å‹çš„æƒ…å†µä¸‹å¾ˆæœ‰ç”¨ã€‚'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” Whether to add a bias
    to the queries, keys and values.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åç½®ã€‚'
- en: '`backbone_out_indices` (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`)
    â€” Indices of the intermediate hidden states to use from backbone.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_out_indices` (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`)
    â€” è¦ä»ä¸»å¹²ä½¿ç”¨çš„ä¸­é—´éšè—çŠ¶æ€çš„ç´¢å¼•ã€‚'
- en: '`readout_type` (`str`, *optional*, defaults to `"project"`) â€” The readout type
    to use when processing the readout token (CLS token) of the intermediate hidden
    states of the ViT backbone. Can be one of [`"ignore"`, `"add"`, `"project"`].'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readout_type` (`str`, *optional*, defaults to `"project"`) â€” å¤„ç†ViTä¸»å¹²ä¸­é—´éšè—çŠ¶æ€çš„è¯»å‡ºæ ‡è®°ï¼ˆCLSæ ‡è®°ï¼‰æ—¶è¦ä½¿ç”¨çš„è¯»å‡ºç±»å‹ã€‚å¯ä»¥æ˜¯[`"ignore"`,
    `"add"`, `"project"`]ä¹‹ä¸€ã€‚'
- en: â€œignoreâ€ simply ignores the CLS token.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œignoreâ€ ç®€å•åœ°å¿½ç•¥CLSæ ‡è®°ã€‚
- en: â€œaddâ€ passes the information from the CLS token to all other tokens by adding
    the representations.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œaddâ€ é€šè¿‡å°†CLSæ ‡è®°çš„ä¿¡æ¯æ·»åŠ åˆ°æ‰€æœ‰å…¶ä»–æ ‡è®°ä¸­ä¼ é€’è¡¨ç¤ºã€‚
- en: â€œprojectâ€ passes information to the other tokens by concatenating the readout
    to all other tokens before projecting the representation to the original feature
    dimension D using a linear layer followed by a GELU non-linearity.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œprojectâ€ é€šè¿‡å°†è¯»å‡ºè¿æ¥åˆ°æ‰€æœ‰å…¶ä»–æ ‡è®°ï¼Œç„¶åä½¿ç”¨çº¿æ€§å±‚å°†è¡¨ç¤ºæŠ•å½±åˆ°åŸå§‹ç‰¹å¾ç»´åº¦Dï¼Œæ¥ç€ä½¿ç”¨GELUéçº¿æ€§ä¼ é€’ä¿¡æ¯ç»™å…¶ä»–æ ‡è®°ã€‚
- en: '`reassemble_factors` (`List[int]`, *optional*, defaults to `[4, 2, 1, 0.5]`)
    â€” The up/downsampling factors of the reassemble layers.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reassemble_factors` (`List[int]`, *optional*, defaults to `[4, 2, 1, 0.5]`)
    â€” é‡ç»„å±‚çš„ä¸Š/ä¸‹é‡‡æ ·å› å­ã€‚'
- en: '`neck_hidden_sizes` (`List[str]`, *optional*, defaults to `[96, 192, 384, 768]`)
    â€” The hidden sizes to project to for the feature maps of the backbone.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neck_hidden_sizes` (`List[str]`, *optional*, defaults to `[96, 192, 384, 768]`)
    â€” è¦æŠ•å½±åˆ°ä¸»å¹²ç‰¹å¾å›¾çš„éšè—å¤§å°ã€‚'
- en: '`fusion_hidden_size` (`int`, *optional*, defaults to 256) â€” The number of channels
    before fusion.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fusion_hidden_size` (`int`, *optional*, defaults to 256) â€” èåˆå‰çš„é€šé“æ•°ã€‚'
- en: '`head_in_index` (`int`, *optional*, defaults to -1) â€” The index of the features
    to use in the heads.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_in_index` (`int`, *optional*, defaults to -1) â€” åœ¨å¤´éƒ¨ä¸­è¦ä½¿ç”¨çš„ç‰¹å¾çš„ç´¢å¼•ã€‚'
- en: '`use_batch_norm_in_fusion_residual` (`bool`, *optional*, defaults to `False`)
    â€” Whether to use batch normalization in the pre-activate residual units of the
    fusion blocks.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_batch_norm_in_fusion_residual` (`bool`, *optional*, defaults to `False`)
    â€” æ˜¯å¦åœ¨èåˆå—çš„é¢„æ¿€æ´»æ®‹å·®å•å…ƒä¸­ä½¿ç”¨æ‰¹å½’ä¸€åŒ–ã€‚'
- en: '`use_bias_in_fusion_residual` (`bool`, *optional*, defaults to `True`) â€” Whether
    to use bias in the pre-activate residual units of the fusion blocks.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_bias_in_fusion_residual` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨èåˆå—çš„é¢„æ¿€æ´»æ®‹å·®å•å…ƒä¸­ä½¿ç”¨åç½®ã€‚'
- en: '`add_projection` (`bool`, *optional*, defaults to `False`) â€” Whether to add
    a projection layer before the depth estimation head.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_projection` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æ·±åº¦ä¼°è®¡å¤´ä¹‹å‰æ·»åŠ æŠ•å½±å±‚ã€‚'
- en: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” Whether to
    use an auxiliary head during training.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” è®­ç»ƒæ—¶æ˜¯å¦ä½¿ç”¨è¾…åŠ©å¤´ã€‚'
- en: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” Weight of
    the cross-entropy loss of the auxiliary head.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” è¾…åŠ©å¤´çš„äº¤å‰ç†µæŸå¤±æƒé‡ã€‚'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚'
- en: '`semantic_classifier_dropout` (`float`, *optional*, defaults to 0.1) â€” The
    dropout ratio for the semantic classification head.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_classifier_dropout` (`float`, *optional*, defaults to 0.1) â€” è¯­ä¹‰åˆ†ç±»å¤´çš„dropoutæ¯”ç‡ã€‚'
- en: '`backbone_featmap_shape` (`List[int]`, *optional*, defaults to `[1, 1024, 24,
    24]`) â€” Used only for the `hybrid` embedding type. The shape of the feature maps
    of the backbone.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_featmap_shape` (`List[int]`, *optional*, defaults to `[1, 1024, 24,
    24]`) â€” ä»…ç”¨äº`hybrid`åµŒå…¥ç±»å‹ã€‚ä¸»å¹²ç‰¹å¾å›¾çš„å½¢çŠ¶ã€‚'
- en: '`neck_ignore_stages` (`List[int]`, *optional*, defaults to `[0, 1]`) â€” Used
    only for the `hybrid` embedding type. The stages of the readout layers to ignore.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neck_ignore_stages` (`List[int]`, *optional*, defaults to `[0, 1]`) â€” ä»…ç”¨äº`hybrid`åµŒå…¥ç±»å‹ã€‚è¦å¿½ç•¥çš„è¯»å‡ºå±‚é˜¶æ®µã€‚'
- en: '`backbone_config` (`Union[Dict[str, Any], PretrainedConfig]`, *optional*) â€”
    The configuration of the backbone model. Only used in case `is_hybrid` is `True`
    or in case you want to leverage the `AutoBackbone` API.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` (`Union[Dict[str, Any], PretrainedConfig]`, *optional*) â€”
    ä¸»å¹²æ¨¡å‹çš„é…ç½®ã€‚ä»…åœ¨`is_hybrid`ä¸º`True`æˆ–è€…æƒ³è¦åˆ©ç”¨`AutoBackbone` APIæ—¶ä½¿ç”¨ã€‚'
- en: This is the configuration class to store the configuration of a [DPTModel](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTModel).
    It is used to instantiate an DPT model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the DPT [Intel/dpt-large](https://huggingface.co/Intel/dpt-large)
    architecture.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[DPTModel](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTModel)çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªDPTæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº[DPT
    Intel/dpt-large](https://huggingface.co/Intel/dpt-large)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `to_dict`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/configuration_dpt.py#L247)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/configuration_dpt.py#L247)'
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Serializes this instance to a Python dictionary. Override the default [to_dict()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict).
    Returns: `Dict[str, any]`: Dictionary of all the attributes that make up this
    configuration instance,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å®ä¾‹åºåˆ—åŒ–ä¸ºPythonå­—å…¸ã€‚è¦†ç›–é»˜è®¤çš„[to_dict()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict)ã€‚è¿”å›ï¼š`Dict[str,
    any]`ï¼šæ„æˆæ­¤é…ç½®å®ä¾‹çš„æ‰€æœ‰å±æ€§çš„å­—å…¸ï¼Œ
- en: DPTFeatureExtractor
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPTFeatureExtractor
- en: '### `class transformers.DPTFeatureExtractor`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPTFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/feature_extraction_dpt.py#L26)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/feature_extraction_dpt.py#L26)'
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `__call__`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Preprocess an image or a batch of images.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L422)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L422)'
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`ï¼ˆ[DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)ï¼‰â€”
    æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) â€” List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`ï¼ˆé•¿åº¦ä¸º`batch_size`çš„`List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: semantic_segmentation
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åˆ†å‰²
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿åº¦ä¸º`batch_size`çš„`List[torch.Tensor]`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº`target_sizes`æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†`target_sizes`ï¼‰ã€‚æ¯ä¸ª`torch.Tensor`çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºè¯­ä¹‰ç±»åˆ«IDã€‚
- en: Converts the output of [DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: DPTImageProcessor
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPTImageProcessor
- en: '### `class transformers.DPTImageProcessor`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPTImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L94)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L94)'
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Whether to resize the
    imageâ€™s (height, width) dimensions. Can be overidden by `do_resize` in `preprocess`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è°ƒæ•´å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_resize`è¦†ç›–ã€‚'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 384, "width":
    384}`): Size of the image after resizing. Can be overidden by `size` in `preprocess`.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size`ï¼ˆ`Dict[str, int]` *å¯é€‰*ï¼Œé»˜è®¤ä¸º`{"height" -- 384, "width": 384}`ï¼‰ï¼šè°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`size`è¦†ç›–ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`)
    â€” Defines the resampling filter to use if resizing the image. Can be overidden
    by `resample` in `preprocess`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample`ï¼ˆ`PILImageResampling`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`Resampling.BICUBIC`ï¼‰â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™å®šä¹‰è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`resample`è¦†ç›–ã€‚'
- en: '`keep_aspect_ratio` (`bool`, *optional*, defaults to `False`) â€” If `True`,
    the image is resized to the largest possible size such that the aspect ratio is
    preserved. Can be overidden by `keep_aspect_ratio` in `preprocess`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_aspect_ratio`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” å¦‚æœä¸º`True`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´ä¸ºä¿æŒçºµæ¨ªæ¯”çš„æœ€å¤§å¯èƒ½å°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`keep_aspect_ratio`è¦†ç›–ã€‚'
- en: '`ensure_multiple_of` (`int`, *optional*, defaults to 1) â€” If `do_resize` is
    `True`, the image is resized to a size that is a multiple of this value. Can be
    overidden by `ensure_multiple_of` in `preprocess`.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ensure_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” å¦‚æœ`do_resize`ä¸º`True`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´ä¸ºæ­¤å€¼çš„å€æ•°ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`ensure_multiple_of`è¦†ç›–ã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) â€” Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overidden by `do_rescale`
    in `preprocess`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹`rescale_factor`é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_rescale`è¦†ç›–ã€‚'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) â€” Scale
    factor to use if rescaling the image. Can be overidden by `rescale_factor` in
    `preprocess`.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int`æˆ–`float`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`rescale_factor`è¦†ç›–ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) â€” Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_normalize`å‚æ•°è¦†ç›–ã€‚'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    â€” Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`IMAGENET_STANDARD_MEAN`)
    â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_mean`å‚æ•°è¦†ç›–ã€‚'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    â€” Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`IMAGENET_STANDARD_STD`) â€”
    å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_std`å‚æ•°è¦†ç›–ã€‚'
- en: '`do_pad` (`bool`, *optional*, defaults to `False`) â€” Whether to apply center
    padding. This was introduced in the DINOv2 paper, which uses the model in combination
    with DPT.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_pad` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åº”ç”¨ä¸­å¿ƒå¡«å……ã€‚è¿™åœ¨DINOv2è®ºæ–‡ä¸­å¼•å…¥ï¼Œè¯¥è®ºæ–‡å°†è¯¥æ¨¡å‹ä¸DPTç»“åˆä½¿ç”¨ã€‚'
- en: '`size_divisor` (`int`, *optional*) â€” If `do_pad` is `True`, pads the image
    dimensions to be divisible by this value. This was introduced in the DINOv2 paper,
    which uses the model in combination with DPT.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size_divisor` (`int`ï¼Œ*optional*) â€” å¦‚æœ`do_pad`ä¸º`True`ï¼Œåˆ™å¡«å……å›¾åƒå°ºå¯¸ä½¿å…¶å¯è¢«è¯¥å€¼æ•´é™¤ã€‚è¿™åœ¨DINOv2è®ºæ–‡ä¸­å¼•å…¥ï¼Œè¯¥è®ºæ–‡å°†è¯¥æ¨¡å‹ä¸DPTç»“åˆä½¿ç”¨ã€‚'
- en: Constructs a DPT image processor.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ„é€ ä¸€ä¸ªDPTå›¾åƒå¤„ç†å™¨ã€‚
- en: '#### `preprocess`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L267)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L267)'
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`ImageInput`) â€” Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º0åˆ°255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_rescale=False`ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) â€” Whether to
    resize the image.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” Size of the
    image after reszing. If `keep_aspect_ratio` is `True`, the image is resized to
    the largest possible size such that the aspect ratio is preserved. If `ensure_multiple_of`
    is set, the image is resized to a size that is a multiple of this value.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¦‚æœ`keep_aspect_ratio`ä¸º`True`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä¸ºä¿æŒçºµæ¨ªæ¯”çš„æœ€å¤§å¯èƒ½å°ºå¯¸ã€‚å¦‚æœè®¾ç½®äº†`ensure_multiple_of`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä¸ºè¯¥å€¼çš„å€æ•°ã€‚'
- en: '`keep_aspect_ratio` (`bool`, *optional*, defaults to `self.keep_aspect_ratio`)
    â€” Whether to keep the aspect ratio of the image. If False, the image will be resized
    to (size, size). If True, the image will be resized to keep the aspect ratio and
    the size will be the maximum possible.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_aspect_ratio` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.keep_aspect_ratio`) â€” æ˜¯å¦ä¿æŒå›¾åƒçš„çºµæ¨ªæ¯”ã€‚å¦‚æœä¸ºFalseï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä¸ºï¼ˆsizeï¼Œsizeï¼‰ã€‚å¦‚æœä¸ºTrueï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä»¥ä¿æŒçºµæ¨ªæ¯”ï¼Œå¤§å°å°†æ˜¯æœ€å¤§å¯èƒ½çš„ã€‚'
- en: '`ensure_multiple_of` (`int`, *optional*, defaults to `self.ensure_multiple_of`)
    â€” Ensure that the image size is a multiple of this value.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ensure_multiple_of` (`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.ensure_multiple_of`) â€” ç¡®ä¿å›¾åƒå¤§å°æ˜¯è¯¥å€¼çš„å€æ•°ã€‚'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) â€” Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚è¿™å¯ä»¥æ˜¯æšä¸¾`PILImageResampling`ä¹‹ä¸€ï¼Œä»…åœ¨`do_resize`è®¾ç½®ä¸º`True`æ—¶æœ‰æ•ˆã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) â€” Whether
    to rescale the image values between [0 - 1].'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒå€¼é‡æ–°ç¼©æ”¾åœ¨[0 - 1]ä¹‹é—´ã€‚'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) â€”
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.rescale_factor`) â€” å¦‚æœ`do_rescale`è®¾ç½®ä¸º`True`ï¼Œåˆ™ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒçš„ç¼©æ”¾å› å­ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) â€” Whether
    to normalize the image.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    â€” Image mean.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.image_mean`) â€” å›¾åƒå‡å€¼ã€‚'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    â€” Image standard deviation.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.image_std`) â€” å›¾åƒæ ‡å‡†å·®ã€‚'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) â€” The type of tensors
    to return. Can be one of:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`æˆ–`TensorType`ï¼Œ*optional*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª`np.ndarray`åˆ—è¡¨ã€‚
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW`æˆ–`''tf''`ï¼šè¿”å›ç±»å‹ä¸º`tf.Tensor`çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH`æˆ–`''pt''`ï¼šè¿”å›ç±»å‹ä¸º`torch.Tensor`çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY`æˆ–`''np''`ï¼šè¿”å›ç±»å‹ä¸º`np.ndarray`çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX`æˆ–`''jax''`ï¼šè¿”å›ç±»å‹ä¸º`jax.numpy.ndarray`çš„æ‰¹å¤„ç†ã€‚'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” The channel dimension format for the output image. Can be one of:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension`æˆ–`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`ChannelDimension.FIRST`)
    â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆnum_channelsï¼Œheightï¼Œwidthï¼‰æ ¼å¼ã€‚'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) â€” The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format`ï¼ˆ`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` æˆ– `ChannelDimension.NONE`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: Preprocess an image or batch of images.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡è¿›è¡Œé¢„å¤„ç†ã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L422)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L422)'
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`ï¼ˆ[DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)ï¼‰â€”
    æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) â€” List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`ï¼ˆé•¿åº¦ä¸º`batch_size`çš„ `List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: semantic_segmentation
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: semantic_segmentation
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿åº¦ä¸º`batch_size`çš„ `List[torch.Tensor]`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº `target_sizes`
    æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº† `target_sizes`ï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚
- en: Converts the output of [DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å°† [DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)
    çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚
- en: DPTModel
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPTModel
- en: '### `class transformers.DPTModel`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L869)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L869)'
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare DPT Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„ DPT æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹çš„éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L905)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L905)'
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„ `torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [DPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„ `torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨ä¸º`æœªå±è”½`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨ä¸º`å·²å±è”½`ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.dpt.modeling_dpt.BaseModelOutputWithPoolingAndIntermediateActivations`
    or `tuple(torch.FloatTensor)`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.dpt.modeling_dpt.BaseModelOutputWithPoolingAndIntermediateActivations`
    æˆ– `tuple(torch.FloatTensor)`'
- en: A `transformers.models.dpt.modeling_dpt.BaseModelOutputWithPoolingAndIntermediateActivations`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPTConfig](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTConfig))
    and inputs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.models.dpt.modeling_dpt.BaseModelOutputWithPoolingAndIntermediateActivations`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[DPTConfig](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” åºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼ˆç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼‰çš„è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œå¯¹äºBERTç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œtanhæ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚æƒé‡æ˜¯ä»é¢„è®­ç»ƒæœŸé—´çš„ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`intermediate_activations` (`tuple(torch.FloatTensor)`, *optional*) â€” Intermediate
    activations that can be used to compute hidden states of the model at various
    layers.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_activations` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*) â€” å¯ç”¨äºè®¡ç®—å„å±‚æ¨¡å‹éšè—çŠ¶æ€çš„ä¸­é—´æ¿€æ´»ã€‚'
- en: The [DPTModel](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPTModel](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE12]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: DPTForDepthEstimation
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPTForDepthEstimation
- en: '### `class transformers.DPTForDepthEstimation`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPTForDepthEstimation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1073)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1073)'
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: DPT Model with a depth estimation head on top (consisting of 3 convolutional
    layers) e.g. for KITTI, NYUv2.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰æ·±åº¦ä¼°è®¡å¤´éƒ¨çš„DPTæ¨¡å‹ï¼ˆåŒ…å«3ä¸ªå·ç§¯å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºKITTIã€NYUv2ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1098)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1098)'
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[DPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Ground truth depth estimation maps for computing the loss.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®æ·±åº¦ä¼°è®¡å›¾ã€‚'
- en: Returns
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.DepthEstimatorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.DepthEstimatorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.DepthEstimatorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPTConfig](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTConfig))
    and inputs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.DepthEstimatorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[DPTConfig](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`predicted_depth` (`torch.FloatTensor` of shape `(batch_size, height, width)`)
    â€” Predicted depth for each pixel.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predicted_depth`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.FloatTensor`ï¼‰â€” æ¯ä¸ªåƒç´ çš„é¢„æµ‹æ·±åº¦ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºä¸€ä¸ªï¼Œ+
    æ¯ä¸€å±‚çš„è¾“å‡ºä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ›softmaxä¹‹åã€‚
- en: The [DPTForDepthEstimation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForDepthEstimation)
    forward method, overrides the `__call__` special method.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPTForDepthEstimation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForDepthEstimation)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE15]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: DPTForSemanticSegmentation
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPTForSemanticSegmentation
- en: '### `class transformers.DPTForSemanticSegmentation`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPTForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1259)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1259)'
- en: '[PRE16]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ViTConfig](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: DPT Model with a semantic segmentation head on top e.g. for ADE20k, CityScapes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰è¯­ä¹‰åˆ†å‰²å¤´çš„DPTæ¨¡å‹ï¼Œä¾‹å¦‚ADE20kï¼ŒCityScapesã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1281)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1281)'
- en: '[PRE17]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [DPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[DPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®ç½©ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«é®ç½©ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, height, width)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`ä¸­ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPTConfig](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTConfig))
    and inputs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[DPTConfig](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ`config.num_labels==1`ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” Classification scores for each pixel.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” æ¯ä¸ªåƒç´ çš„åˆ†ç±»åˆ†æ•°ã€‚'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">è¿”å›çš„logitsä¸ä¸€å®šä¸ä½œä¸ºè¾“å…¥ä¼ é€’çš„`pixel_values`å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡æ’å€¼å¹¶åœ¨ç”¨æˆ·éœ€è¦å°†logitsè°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°æ—¶ä¸¢å¤±ä¸€äº›è´¨é‡ã€‚æ‚¨åº”è¯¥å§‹ç»ˆæ£€æŸ¥æ‚¨çš„logitså½¢çŠ¶å¹¶æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚</tip>
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, patch_size, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºçš„ä¸€ä¸ª+æ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºæ—¶çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPTForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE18]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
