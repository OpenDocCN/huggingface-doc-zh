- en: Create reproducible pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºå¯å¤ç°çš„æµæ°´çº¿
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/reproducibility](https://huggingface.co/docs/diffusers/using-diffusers/reproducibility)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/reproducibility](https://huggingface.co/docs/diffusers/using-diffusers/reproducibility)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility is important for testing, replicating results, and can even
    be used to [improve image quality](reusing_seeds). However, the randomness in
    diffusion models is a desired property because it allows the pipeline to generate
    different images every time it is run. While you canâ€™t expect to get the exact
    same results across platforms, you can expect results to be reproducible across
    releases and platforms within a certain tolerance range. Even then, tolerance
    varies depending on the diffusion pipeline and checkpoint.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¯å¤ç°æ€§å¯¹äºæµ‹è¯•ã€å¤åˆ¶ç»“æœç”šè‡³ç”¨äº[æ”¹å–„å›¾åƒè´¨é‡](reusing_seeds)éƒ½å¾ˆé‡è¦ã€‚ç„¶è€Œï¼Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„éšæœºæ€§æ˜¯ä¸€ç§æœŸæœ›çš„å±æ€§ï¼Œå› ä¸ºå®ƒå…è®¸æµæ°´çº¿æ¯æ¬¡è¿è¡Œæ—¶ç”Ÿæˆä¸åŒçš„å›¾åƒã€‚è™½ç„¶æ‚¨ä¸èƒ½æœŸæœ›åœ¨ä¸åŒå¹³å°ä¸Šè·å¾—å®Œå…¨ç›¸åŒçš„ç»“æœï¼Œä½†æ‚¨å¯ä»¥æœŸæœ›åœ¨ä¸€å®šå®¹å·®èŒƒå›´å†…åœ¨å‘å¸ƒå’Œå¹³å°ä¹‹é—´å¤ç°ç»“æœã€‚å³ä½¿å¦‚æ­¤ï¼Œå®¹å·®ä¹Ÿå–å†³äºæ‰©æ•£æµæ°´çº¿å’Œæ£€æŸ¥ç‚¹ã€‚
- en: This is why itâ€™s important to understand how to control sources of randomness
    in diffusion models or use deterministic algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé‡è¦äº†è§£å¦‚ä½•æ§åˆ¶æ‰©æ•£æ¨¡å‹ä¸­çš„éšæœºæºæˆ–ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ã€‚
- en: 'ğŸ’¡ We strongly recommend reading PyTorchâ€™s [statement about reproducibility](https://pytorch.org/docs/stable/notes/randomness.html):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ æˆ‘ä»¬å¼ºçƒˆå»ºè®®é˜…è¯»PyTorchå…³äºå¯å¤ç°æ€§çš„[å£°æ˜](https://pytorch.org/docs/stable/notes/randomness.html)ï¼š
- en: Completely reproducible results are not guaranteed across PyTorch releases,
    individual commits, or different platforms. Furthermore, results may not be reproducible
    between CPU and GPU executions, even when using identical seeds.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ— æ³•ä¿è¯åœ¨PyTorchå‘å¸ƒã€å•ä¸ªæäº¤æˆ–ä¸åŒå¹³å°ä¹‹é—´å®Œå…¨å¯å¤ç°çš„ç»“æœã€‚æ­¤å¤–ï¼Œå³ä½¿ä½¿ç”¨ç›¸åŒçš„ç§å­ï¼Œåœ¨CPUå’ŒGPUæ‰§è¡Œä¹‹é—´ä¹Ÿå¯èƒ½æ— æ³•å¤ç°ç»“æœã€‚
- en: Control randomness
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ§åˆ¶éšæœºæ€§
- en: During inference, pipelines rely heavily on random sampling operations which
    include creating the Gaussian noise tensors to denoise and adding noise to the
    scheduling step.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæµæ°´çº¿ä¸¥é‡ä¾èµ–éšæœºæŠ½æ ·æ“ä½œï¼ŒåŒ…æ‹¬åˆ›å»ºé«˜æ–¯å™ªå£°å¼ é‡ä»¥å»å™ªå’Œåœ¨è°ƒåº¦æ­¥éª¤ä¸­æ·»åŠ å™ªå£°ã€‚
- en: 'Take a look at the tensor values in the [DDIMPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/ddim#diffusers.DDIMPipeline)
    after two inference steps:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸¤ä¸ªæ¨ç†æ­¥éª¤ä¹‹åæŸ¥çœ‹[DDIMPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/ddim#diffusers.DDIMPipeline)ä¸­çš„å¼ é‡å€¼ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running the code above prints one value, but if you run it again you get a different
    value. What is going on here?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œä¸Šé¢çš„ä»£ç ä¼šæ‰“å°ä¸€ä¸ªå€¼ï¼Œä½†å¦‚æœå†æ¬¡è¿è¡Œï¼Œä¼šå¾—åˆ°ä¸åŒçš„å€¼ã€‚è¿™æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ
- en: Every time the pipeline is run, [`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html)
    uses a different random seed to create Gaussian noise which is denoised stepwise.
    This leads to a different result each time it is run, which is great for diffusion
    pipelines since it generates a different random image each time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡è¿è¡Œæµæ°´çº¿æ—¶ï¼Œ[`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html)ä½¿ç”¨ä¸åŒçš„éšæœºç§å­åˆ›å»ºé«˜æ–¯å™ªå£°ï¼Œé€æ­¥å»å™ªã€‚è¿™å¯¼è‡´æ¯æ¬¡è¿è¡Œæ—¶éƒ½ä¼šäº§ç”Ÿä¸åŒçš„ç»“æœï¼Œè¿™å¯¹äºæ‰©æ•£æµæ°´çº¿æ˜¯å¾ˆå¥½çš„ï¼Œå› ä¸ºå®ƒæ¯æ¬¡ç”Ÿæˆä¸åŒçš„éšæœºå›¾åƒã€‚
- en: But if you need to reliably generate the same image, thatâ€™ll depend on whether
    youâ€™re running the pipeline on a CPU or GPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå¦‚æœæ‚¨éœ€è¦å¯é åœ°ç”Ÿæˆç›¸åŒçš„å›¾åƒï¼Œåˆ™å–å†³äºæ‚¨æ˜¯åœ¨CPUè¿˜æ˜¯GPUä¸Šè¿è¡Œæµæ°´çº¿ã€‚
- en: CPU
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU
- en: 'To generate reproducible results on a CPU, youâ€™ll need to use a PyTorch [`Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    and set a seed:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨CPUä¸Šç”Ÿæˆå¯å¤ç°çš„ç»“æœï¼Œæ‚¨éœ€è¦ä½¿ç”¨PyTorch [`Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)å¹¶è®¾ç½®ä¸€ä¸ªç§å­ï¼š
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now when you run the code above, it always prints a value of `1491.1711` no
    matter what because the `Generator` object with the seed is passed to all the
    random functions of the pipeline.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å½“æ‚¨è¿è¡Œä¸Šé¢çš„ä»£ç æ—¶ï¼Œæ— è®ºå¦‚ä½•éƒ½ä¼šæ‰“å°ä¸€ä¸ªå€¼`1491.1711`ï¼Œå› ä¸º`Generator`å¯¹è±¡ä¸ç§å­ä¸€èµ·ä¼ é€’ç»™æµæ°´çº¿çš„æ‰€æœ‰éšæœºå‡½æ•°ã€‚
- en: If you run this code example on your specific hardware and PyTorch version,
    you should get a similar, if not the same, result.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨æ‚¨çš„ç‰¹å®šç¡¬ä»¶å’ŒPyTorchç‰ˆæœ¬ä¸Šè¿è¡Œæ­¤ä»£ç ç¤ºä¾‹ï¼Œæ‚¨åº”è¯¥è·å¾—ç±»ä¼¼çš„ç»“æœï¼Œå¦‚æœä¸æ˜¯ç›¸åŒçš„è¯ã€‚
- en: ğŸ’¡ It might be a bit unintuitive at first to pass `Generator` objects to the
    pipeline instead of just integer values representing the seed, but this is the
    recommended design when dealing with probabilistic models in PyTorch, as `Generator`s
    are *random states* that can be passed to multiple pipelines in a sequence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä¸€å¼€å§‹å¯èƒ½æœ‰ç‚¹ä¸ç›´è§‚ï¼Œå°†`Generator`å¯¹è±¡ä¼ é€’ç»™æµæ°´çº¿è€Œä¸ä»…ä»…æ˜¯è¡¨ç¤ºç§å­çš„æ•´æ•°å€¼ï¼Œä½†è¿™æ˜¯å¤„ç†PyTorchä¸­æ¦‚ç‡æ¨¡å‹æ—¶æ¨èçš„è®¾è®¡ï¼Œå› ä¸º`Generator`æ˜¯å¯ä»¥ä¼ é€’ç»™å¤šä¸ªæµæ°´çº¿çš„*éšæœºçŠ¶æ€*ã€‚
- en: GPU
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU
- en: 'Writing a reproducible pipeline on a GPU is a bit trickier, and full reproducibility
    across different hardware is not guaranteed because matrix multiplication - which
    diffusion pipelines require a lot of - is less deterministic on a GPU than a CPU.
    For example, if you run the same code example above on a GPU:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GPUä¸Šç¼–å†™å¯å¤ç°çš„æµæ°´çº¿æœ‰ç‚¹æ£˜æ‰‹ï¼Œä¸ä¿è¯åœ¨ä¸åŒç¡¬ä»¶ä¸Šå®Œå…¨å¯å¤ç°ï¼Œå› ä¸ºçŸ©é˜µä¹˜æ³• - æ‰©æ•£æµæ°´çº¿éœ€è¦å¤§é‡çš„çŸ©é˜µä¹˜æ³• - åœ¨GPUä¸Šæ¯”åœ¨CPUä¸Šä¸å¤ªç¡®å®šã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨åœ¨GPUä¸Šè¿è¡Œä¸Šé¢çš„ç›¸åŒä»£ç ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result is not the same even though youâ€™re using an identical seed because
    the GPU uses a different random number generator than the CPU.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ä½¿ç”¨ç›¸åŒçš„ç§å­ï¼Œç»“æœä¹Ÿä¸ç›¸åŒï¼Œå› ä¸ºGPUä½¿ç”¨çš„éšæœºæ•°ç”Ÿæˆå™¨ä¸CPUä¸åŒã€‚
- en: To circumvent this problem, ğŸ§¨ Diffusers has a `randn_tensor()` function for
    creating random noise on the CPU, and then moving the tensor to a GPU if necessary.
    The `randn_tensor` function is used everywhere inside the pipeline, allowing the
    user to **always** pass a CPU `Generator` even if the pipeline is run on a GPU.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒğŸ§¨ Diffusersæœ‰ä¸€ä¸ª`randn_tensor()`å‡½æ•°ï¼Œç”¨äºåœ¨CPUä¸Šåˆ›å»ºéšæœºå™ªå£°ï¼Œç„¶åå°†å¼ é‡ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚`randn_tensor`å‡½æ•°åœ¨æµæ°´çº¿å†…çš„å„å¤„éƒ½è¢«ä½¿ç”¨ï¼Œå…è®¸ç”¨æˆ·**å§‹ç»ˆ**ä¼ é€’ä¸€ä¸ªCPU
    `Generator`ï¼Œå³ä½¿æµæ°´çº¿åœ¨GPUä¸Šè¿è¡Œã€‚
- en: Youâ€™ll see the results are much closer now!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨ä¼šçœ‹åˆ°ç»“æœç°åœ¨æ›´æ¥è¿‘äº†ï¼
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ğŸ’¡ If reproducibility is important, we recommend always passing a CPU generator.
    The performance loss is often neglectable, and youâ€™ll generate much more similar
    values than if the pipeline had been run on a GPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å¦‚æœå¯å¤ç°æ€§å¾ˆé‡è¦ï¼Œæˆ‘ä»¬å»ºè®®å§‹ç»ˆä¼ é€’ä¸€ä¸ªCPUç”Ÿæˆå™¨ã€‚æ€§èƒ½æŸå¤±é€šå¸¸å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå¹¶ä¸”æ‚¨å°†ç”Ÿæˆæ¯”åœ¨GPUä¸Šè¿è¡Œæµæ°´çº¿æ—¶æ›´ç›¸ä¼¼çš„å€¼ã€‚
- en: Finally, for more complex pipelines such as [UnCLIPPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/unclip#diffusers.UnCLIPPipeline),
    these are often extremely susceptible to precision error propagation. Donâ€™t expect
    similar results across different GPU hardware or PyTorch versions. In this case,
    youâ€™ll need to run exactly the same hardware and PyTorch version for full reproducibility.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´å¤æ‚çš„æµæ°´çº¿ï¼Œæ¯”å¦‚[UnCLIPPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/unclip#diffusers.UnCLIPPipeline)ï¼Œè¿™äº›é€šå¸¸ææ˜“å—åˆ°ç²¾åº¦è¯¯å·®ä¼ æ’­çš„å½±å“ã€‚ä¸è¦æœŸæœ›åœ¨ä¸åŒçš„GPUç¡¬ä»¶æˆ–PyTorchç‰ˆæœ¬ä¸Šè·å¾—ç±»ä¼¼çš„ç»“æœã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦åœ¨å®Œå…¨ç›¸åŒçš„ç¡¬ä»¶å’ŒPyTorchç‰ˆæœ¬ä¸Šè¿è¡Œä»¥å®ç°å®Œå…¨çš„å¯é‡ç°æ€§ã€‚
- en: Deterministic algorithms
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¡®å®šæ€§ç®—æ³•
- en: You can also configure PyTorch to use deterministic algorithms to create a reproducible
    pipeline. However, you should be aware that deterministic algorithms may be slower
    than nondeterministic ones and you may observe a decrease in performance. But
    if reproducibility is important to you, then this is the way to go!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é…ç½®PyTorchä½¿ç”¨ç¡®å®šæ€§ç®—æ³•æ¥åˆ›å»ºå¯é‡ç°çš„æµæ°´çº¿ã€‚ä½†æ˜¯ï¼Œæ‚¨åº”è¯¥æ„è¯†åˆ°ç¡®å®šæ€§ç®—æ³•å¯èƒ½æ¯”éç¡®å®šæ€§ç®—æ³•æ…¢ï¼Œæ‚¨å¯èƒ½ä¼šè§‚å¯Ÿåˆ°æ€§èƒ½ä¸‹é™ã€‚ä½†å¦‚æœå¯¹æ‚¨æ¥è¯´å¯é‡ç°æ€§å¾ˆé‡è¦ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯æ­£ç¡®çš„æ–¹å¼ï¼
- en: Nondeterministic behavior occurs when operations are launched in more than one
    CUDA stream. To avoid this, set the environment variable [`CUBLAS_WORKSPACE_CONFIG`](https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility)
    to `:16:8` to only use one buffer size during runtime.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ“ä½œåœ¨å¤šä¸ªCUDAæµä¸­å¯åŠ¨æ—¶ï¼Œä¼šå‘ç”Ÿéç¡®å®šæ€§è¡Œä¸ºã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œå°†ç¯å¢ƒå˜é‡[`CUBLAS_WORKSPACE_CONFIG`](https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility)è®¾ç½®ä¸º`:16:8`ï¼Œä»¥åœ¨è¿è¡Œæ—¶ä»…ä½¿ç”¨ä¸€ä¸ªç¼“å†²åŒºå¤§å°ã€‚
- en: PyTorch typically benchmarks multiple algorithms to select the fastest one,
    but if you want reproducibility, you should disable this feature because the benchmark
    may select different algorithms each time. Lastly, pass `True` to [`torch.use_deterministic_algorithms`](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)
    to enable deterministic algorithms.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorché€šå¸¸ä¼šå¯¹å¤šä¸ªç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥é€‰æ‹©æœ€å¿«çš„ç®—æ³•ï¼Œä½†å¦‚æœæ‚¨å¸Œæœ›å®ç°å¯é‡ç°æ€§ï¼Œåº”è¯¥ç¦ç”¨æ­¤åŠŸèƒ½ï¼Œå› ä¸ºåŸºå‡†æµ‹è¯•å¯èƒ½ä¼šæ¯æ¬¡é€‰æ‹©ä¸åŒçš„ç®—æ³•ã€‚æœ€åï¼Œå°†`True`ä¼ é€’ç»™[`torch.use_deterministic_algorithms`](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)ä»¥å¯ç”¨ç¡®å®šæ€§ç®—æ³•ã€‚
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now when you run the same pipeline twice, youâ€™ll get identical results.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å½“æ‚¨ä¸¤æ¬¡è¿è¡Œç›¸åŒçš„æµæ°´çº¿æ—¶ï¼Œæ‚¨å°†è·å¾—ç›¸åŒçš„ç»“æœã€‚
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
