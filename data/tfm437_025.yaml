- en: Multiple choice
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项选择
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/multiple_choice](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/multiple_choice)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/multiple_choice](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/multiple_choice)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: A multiple choice task is similar to question answering, except several candidate
    answers are provided along with a context and the model is trained to select the
    correct answer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 多项选择任务类似于问题回答，不同之处在于提供了几个候选答案以及上下文，模型经过训练后会选择正确的答案。
- en: 'This guide will show you how to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将向您展示如何：
- en: Finetune [BERT](https://huggingface.co/bert-base-uncased) on the `regular` configuration
    of the [SWAG](https://huggingface.co/datasets/swag) dataset to select the best
    answer given multiple options and some context.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[SWAG](https://huggingface.co/datasets/swag)数据集的`regular`配置上对[BERT](https://huggingface.co/bert-base-uncased)进行微调，以在给定多个选项和一些上下文的情况下选择最佳答案。
- en: Use your finetuned model for inference.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您微调过的模型进行推理。
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中演示的任务由以下模型架构支持：
- en: '[ALBERT](../model_doc/albert), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird),
    [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine), [ConvBERT](../model_doc/convbert),
    [Data2VecText](../model_doc/data2vec-text), [DeBERTa-v2](../model_doc/deberta-v2),
    [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie),
    [ErnieM](../model_doc/ernie_m), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet),
    [Funnel Transformer](../model_doc/funnel), [I-BERT](../model_doc/ibert), [Longformer](../model_doc/longformer),
    [LUKE](../model_doc/luke), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert),
    [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MRA](../model_doc/mra),
    [Nezha](../model_doc/nezha), [Nyströmformer](../model_doc/nystromformer), [QDQBert](../model_doc/qdqbert),
    [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm),
    [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert),
    [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl),
    [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[ALBERT](../model_doc/albert)、[BERT](../model_doc/bert)、[BigBird](../model_doc/big_bird)、[CamemBERT](../model_doc/camembert)、[CANINE](../model_doc/canine)、[ConvBERT](../model_doc/convbert)、[Data2VecText](../model_doc/data2vec-text)、[DeBERTa-v2](../model_doc/deberta-v2)、[DistilBERT](../model_doc/distilbert)、[ELECTRA](../model_doc/electra)、[ERNIE](../model_doc/ernie)、[ErnieM](../model_doc/ernie_m)、[FlauBERT](../model_doc/flaubert)、[FNet](../model_doc/fnet)、[Funnel
    Transformer](../model_doc/funnel)、[I-BERT](../model_doc/ibert)、[Longformer](../model_doc/longformer)、[LUKE](../model_doc/luke)、[MEGA](../model_doc/mega)、[Megatron-BERT](../model_doc/megatron-bert)、[MobileBERT](../model_doc/mobilebert)、[MPNet](../model_doc/mpnet)、[MRA](../model_doc/mra)、[Nezha](../model_doc/nezha)、[Nyströmformer](../model_doc/nystromformer)、[QDQBert](../model_doc/qdqbert)、[RemBERT](../model_doc/rembert)、[RoBERTa](../model_doc/roberta)、[RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm)、[RoCBert](../model_doc/roc_bert)、[RoFormer](../model_doc/roformer)、[SqueezeBERT](../model_doc/squeezebert)、[XLM](../model_doc/xlm)、[XLM-RoBERTa](../model_doc/xlm-roberta)、[XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl)、[XLNet](../model_doc/xlnet)、[X-MOD](../model_doc/xmod)、[YOSO](../model_doc/yoso)'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保已安装所有必要的库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您登录您的Hugging Face帐户，这样您就可以上传和与社区分享您的模型。在提示时，输入您的令牌以登录：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load SWAG dataset
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载SWAG数据集
- en: 'Start by loading the `regular` configuration of the SWAG dataset from the 🤗
    Datasets library:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载🤗 Datasets库中SWAG数据集的`regular`配置：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then take a look at an example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后看一个例子：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'While it looks like there are a lot of fields here, it is actually pretty straightforward:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这里看起来有很多字段，但实际上非常简单：
- en: '`sent1` and `sent2`: these fields show how a sentence starts, and if you put
    the two together, you get the `startphrase` field.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sent1`和`sent2`：这些字段显示句子如何开始，如果将它们放在一起，您将得到`startphrase`字段。'
- en: '`ending`: suggests a possible ending for how a sentence can end, but only one
    of them is correct.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ending`：建议一个可能的句子结尾，但只有一个是正确的。'
- en: '`label`: identifies the correct sentence ending.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`：标识正确的句子结尾。'
- en: Preprocess
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理
- en: 'The next step is to load a BERT tokenizer to process the sentence starts and
    the four possible endings:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载BERT分词器，以处理句子开头和四个可能的结尾：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preprocessing function you want to create needs to:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您要创建的预处理函数需要：
- en: Make four copies of the `sent1` field and combine each of them with `sent2`
    to recreate how a sentence starts.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制`sent1`字段四次，并将每个字段与`sent2`组合，以重新创建句子的开头方式。
- en: Combine `sent2` with each of the four possible sentence endings.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`sent2`与四个可能的句子结尾中的每一个结合起来。
- en: Flatten these two lists so you can tokenize them, and then unflatten them afterward
    so each example has a corresponding `input_ids`, `attention_mask`, and `labels`
    field.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个列表展平，以便对它们进行标记化，然后在之后将它们展开，以便每个示例都有相应的`input_ids`、`attention_mask`和`labels`字段。
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To apply the preprocessing function over the entire dataset, use 🤗 Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    method. You can speed up the `map` function by setting `batched=True` to process
    multiple elements of the dataset at once:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要在整个数据集上应用预处理函数，请使用🤗 Datasets [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)方法。您可以通过设置`batched=True`来加速`map`函数，以一次处理数据集的多个元素：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 🤗 Transformers doesn’t have a data collator for multiple choice, so you’ll need
    to adapt the [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)
    to create a batch of examples. It’s more efficient to *dynamically pad* the sentences
    to the longest length in a batch during collation, instead of padding the whole
    dataset to the maximum length.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers没有用于多项选择的数据整理器，因此您需要调整[DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)以创建一批示例。在整理期间，将句子动态填充到批次中的最长长度，而不是将整个数据集填充到最大长度，这样更有效。
- en: '`DataCollatorForMultipleChoice` flattens all the model inputs, applies padding,
    and then unflattens the results:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataCollatorForMultipleChoice`将所有模型输入展平，应用填充，然后展平结果：'
- en: PytorchHide Pytorch content
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Evaluate
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: 'Including a metric during training is often helpful for evaluating your model’s
    performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)
    metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中包含一个度量通常有助于评估模型的性能。您可以使用🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)库快速加载评估方法。对于此任务，加载[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)度量（请参阅🤗
    Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算度量）：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then create a function that passes your predictions and labels to `compute`
    to calculate the accuracy:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建一个函数，将您的预测和标签传递给`compute`来计算准确性：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Your `compute_metrics` function is ready to go now, and you’ll return to it
    when you setup your training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您的`compute_metrics`函数现在已经准备就绪，在设置训练时将返回到它。
- en: Train
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: PytorchHide Pytorch content
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: If you aren’t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)微调模型，请查看这里的基本教程[here](../training#train-with-pytorch-trainer)!
- en: 'You’re ready to start training your model now! Load BERT with [AutoModelForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMultipleChoice):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以开始训练您的模型了！使用[AutoModelForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMultipleChoice)加载BERT：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At this point, only three steps remain:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，只剩下三个步骤：
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. You’ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model). At the end of each epoch,
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the accuracy and save the training checkpoint.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中定义您的训练超参数。唯一必需的参数是`output_dir`，它指定保存模型的位置。您将通过设置`push_to_hub=True`将此模型推送到Hub（您需要登录Hugging
    Face才能上传模型）。在每个时代结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将评估准确性并保存训练检查点。
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练参数传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，同时还包括模型、数据集、标记器、数据整理器和`compute_metrics`函数。
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)来微调您的模型。
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将您的模型共享到Hub，以便每个人都可以使用您的模型：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: TensorFlowHide TensorFlow content
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: If you aren’t familiar with finetuning a model with Keras, take a look at the
    basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉使用Keras微调模型，请查看这里的基本教程[here](../training#train-a-tensorflow-model-with-keras)!
- en: 'To finetune a model in TensorFlow, start by setting up an optimizer function,
    learning rate schedule, and some training hyperparameters:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要在TensorFlow中微调模型，请首先设置优化器函数、学习率调度和一些训练超参数：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then you can load BERT with [TFAutoModelForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForMultipleChoice):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用[TFAutoModelForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForMultipleChoice)加载BERT：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Convert your datasets to the `tf.data.Dataset` format with [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)将数据集转换为`tf.data.Dataset`格式：
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method).
    Note that Transformers models all have a default task-relevant loss function,
    so you don’t need to specify one unless you want to:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)配置模型进行训练。请注意，Transformers模型都具有默认的与任务相关的损失函数，因此除非您想要指定一个，否则不需要：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The last two things to setup before you start training is to compute the accuracy
    from the predictions, and provide a way to push your model to the Hub. Both are
    done by using [Keras callbacks](../main_classes/keras_callbacks).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，设置最后两件事是从预测中计算准确性，并提供一种将模型推送到Hub的方法。这两个都可以使用[Keras callbacks](../main_classes/keras_callbacks)来完成。
- en: 'Pass your `compute_metrics` function to [KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的`compute_metrics`函数传递给[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)：
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Specify where to push your model and tokenizer in the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 指定在[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)中推送您的模型和分词器的位置：
- en: '[PRE19]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then bundle your callbacks together:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将您的回调捆绑在一起：
- en: '[PRE20]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, you’re ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)
    with your training and validation datasets, the number of epochs, and your callbacks
    to finetune the model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您已经准备好开始训练您的模型了！调用[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)，使用您的训练和验证数据集、时代数和回调来微调模型：
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once training is completed, your model is automatically uploaded to the Hub
    so everyone can use it!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，您的模型将自动上传到Hub，以便每个人都可以使用它！
- en: For a more in-depth example of how to finetune a model for multiple choice,
    take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)
    or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何为多项选择微调模型的更深入示例，请查看相应的[PyTorch笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)或[TensorFlow笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)。
- en: Inference
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: Great, now that you’ve finetuned a model, you can use it for inference!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在您已经对模型进行了微调，可以用于推理！
- en: 'Come up with some text and two candidate answers:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 想出一些文本和两个候选答案：
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: PytorchHide Pytorch content
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: 'Tokenize each prompt and candidate answer pair and return PyTorch tensors.
    You should also create some `labels`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个提示和候选答案对进行标记化，并返回PyTorch张量。您还应该创建一些`标签`：
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Pass your inputs and labels to the model and return the `logits`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的输入和标签传递给模型，并返回`logits`：
- en: '[PRE24]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Get the class with the highest probability:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 获取具有最高概率的类：
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: TensorFlowHide TensorFlow content
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: 'Tokenize each prompt and candidate answer pair and return TensorFlow tensors:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个提示和候选答案对进行标记化，并返回TensorFlow张量：
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Pass your inputs to the model and return the `logits`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的输入传递给模型，并返回`logits`：
- en: '[PRE27]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Get the class with the highest probability:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 获取具有最高概率的类：
- en: '[PRE28]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
