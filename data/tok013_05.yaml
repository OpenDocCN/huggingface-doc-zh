- en: The tokenization pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–ç®¡é“
- en: 'Original text: [https://huggingface.co/docs/tokenizers/pipeline](https://huggingface.co/docs/tokenizers/pipeline)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/tokenizers/pipeline](https://huggingface.co/docs/tokenizers/pipeline)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'When calling `Tokenizer.encode` or `Tokenizer.encode_batch`, the input text(s)
    go through the following pipeline:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨`Tokenizer.encode`æˆ–`Tokenizer.encode_batch`æ—¶ï¼Œè¾“å…¥æ–‡æœ¬å°†é€šè¿‡ä»¥ä¸‹ç®¡é“è¿›è¡Œå¤„ç†ï¼š
- en: '`normalization`'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`å½’ä¸€åŒ–`'
- en: '`pre-tokenization`'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`é¢„åˆ†è¯`'
- en: '`model`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ¨¡å‹`'
- en: '`post-processing`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`åå¤„ç†`'
- en: Weâ€™ll see in details what happens during each of those steps in detail, as well
    as when you want to `decode <decoding>` some token ids, and how the ğŸ¤— Tokenizers
    library allows you to customize each of those steps to your needs. If youâ€™re already
    familiar with those steps and want to learn by seeing some code, jump to `our
    BERT from scratch example <example>`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¯¦ç»†äº†è§£æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†æƒ…å†µï¼Œä»¥åŠå½“æ‚¨æƒ³è¦`è§£ç <è§£ç >`ä¸€äº›æ ‡è®°IDæ—¶ï¼Œä»¥åŠğŸ¤— Tokenizersåº“å¦‚ä½•å…è®¸æ‚¨æ ¹æ®éœ€è¦è‡ªå®šä¹‰æ¯ä¸ªæ­¥éª¤ã€‚å¦‚æœæ‚¨å·²ç»ç†Ÿæ‚‰è¿™äº›æ­¥éª¤ï¼Œå¹¶å¸Œæœ›é€šè¿‡æŸ¥çœ‹ä¸€äº›ä»£ç æ¥å­¦ä¹ ï¼Œè¯·è·³è½¬åˆ°`æˆ‘ä»¬çš„BERTä»å¤´å¼€å§‹ç¤ºä¾‹<ç¤ºä¾‹>`ã€‚
- en: 'For the examples that require a `Tokenizer` we will use the tokenizer we trained
    in the `quicktour`, which you can load with:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºéœ€è¦`Tokenizer`çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åœ¨`å¿«é€Ÿæµè§ˆ`ä¸­è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½ï¼š
- en: PythonRustNode
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Normalization
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å½’ä¸€åŒ–
- en: Normalization is, in a nutshell, a set of operations you apply to a raw string
    to make it less random or â€œcleanerâ€. Common operations include stripping whitespace,
    removing accented characters or lowercasing all text. If youâ€™re familiar with
    [Unicode normalization](https://unicode.org/reports/tr15), it is also a very common
    normalization operation applied in most tokenizers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å½’ä¸€åŒ–ï¼Œç®€è€Œè¨€ä¹‹ï¼Œæ˜¯å¯¹åŸå§‹å­—ç¬¦ä¸²åº”ç”¨çš„ä¸€ç»„æ“ä½œï¼Œä½¿å…¶æ›´å°‘éšæœºæˆ–â€œæ›´å¹²å‡€â€ã€‚å¸¸è§æ“ä½œåŒ…æ‹¬å»é™¤ç©ºæ ¼ã€å»é™¤é‡éŸ³å­—ç¬¦æˆ–å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ã€‚å¦‚æœæ‚¨ç†Ÿæ‚‰[Unicodeè§„èŒƒåŒ–](https://unicode.org/reports/tr15)ï¼Œé‚£ä¹ˆå¤§å¤šæ•°åˆ†è¯å™¨ä¸­ä¹Ÿä¼šåº”ç”¨éå¸¸å¸¸è§çš„è§„èŒƒåŒ–æ“ä½œã€‚
- en: 'Each normalization operation is represented in the ğŸ¤— Tokenizers library by
    a `Normalizer`, and you can combine several of those by using a `normalizers.Sequence`.
    Here is a normalizer applying NFD Unicode normalization and removing accents as
    an example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè§„èŒƒåŒ–æ“ä½œåœ¨ğŸ¤— Tokenizersåº“ä¸­ç”±ä¸€ä¸ª`Normalizer`è¡¨ç¤ºï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`normalizers.Sequence`ç»„åˆå…¶ä¸­çš„å‡ ä¸ªã€‚è¿™æ˜¯ä¸€ä¸ªåº”ç”¨NFD
    Unicodeè§„èŒƒåŒ–å¹¶å»é™¤é‡éŸ³çš„è§„èŒƒåŒ–å™¨ç¤ºä¾‹ï¼š
- en: PythonRustNode
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can manually test that normalizer by applying it to any string:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡å°†å…¶åº”ç”¨äºä»»ä½•å­—ç¬¦ä¸²æ¥æ‰‹åŠ¨æµ‹è¯•è¯¥è§„èŒƒåŒ–å™¨ï¼š
- en: PythonRustNode
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When building a `Tokenizer`, you can customize its normalizer by just changing
    the corresponding attribute:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»º`Tokenizer`æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ›´æ”¹ç›¸åº”å±æ€§æ¥è‡ªå®šä¹‰å…¶è§„èŒƒåŒ–ï¼š
- en: PythonRustNode
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of course, if you change the way a tokenizer applies normalization, you should
    probably retrain it from scratch afterward.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå¦‚æœæ›´æ”¹äº†åˆ†è¯å™¨åº”ç”¨å½’ä¸€åŒ–çš„æ–¹å¼ï¼Œé‚£ä¹ˆä¹‹åå¯èƒ½éœ€è¦ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒå®ƒã€‚
- en: Pre-Tokenization
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„åˆ†è¯
- en: Pre-tokenization is the act of splitting a text into smaller objects that give
    an upper bound to what your tokens will be at the end of training. A good way
    to think of this is that the pre-tokenizer will split your text into â€œwordsâ€ and
    then, your final tokens will be parts of those words.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„åˆ†è¯åŒ–æ˜¯å°†æ–‡æœ¬åˆ†å‰²ä¸ºè¾ƒå°å¯¹è±¡çš„è¡Œä¸ºï¼Œè¿™äº›å¯¹è±¡ç»™å‡ºäº†åœ¨è®­ç»ƒç»“æŸæ—¶æ‚¨çš„æ ‡è®°å°†æ˜¯ä»€ä¹ˆçš„ä¸Šé™ã€‚ä¸€ä¸ªå¥½çš„æ€è€ƒæ–¹å¼æ˜¯ï¼Œé¢„åˆ†è¯å™¨å°†æŠŠæ‚¨çš„æ–‡æœ¬åˆ†å‰²æˆâ€œå•è¯â€ï¼Œç„¶åï¼Œæ‚¨çš„æœ€ç»ˆæ ‡è®°å°†æ˜¯è¿™äº›å•è¯çš„ä¸€éƒ¨åˆ†ã€‚
- en: 'An easy way to pre-tokenize inputs is to split on spaces and punctuations,
    which is done by the `pre_tokenizers.Whitespace` pre-tokenizer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„é¢„åˆ†è¯è¾“å…¥çš„æ–¹æ³•æ˜¯åœ¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ä¸Šè¿›è¡Œåˆ†å‰²ï¼Œè¿™æ˜¯ç”±`pre_tokenizers.Whitespace`é¢„åˆ†è¯å™¨å®Œæˆçš„ï¼š
- en: PythonRustNode
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output is a list of tuples, with each tuple containing one word and its
    span in the original sentence (which is used to determine the final `offsets`
    of our `Encoding`). Note that splitting on punctuation will split contractions
    like `"I'm"` in this example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«ä¸€ä¸ªå•è¯åŠå…¶åœ¨åŸå§‹å¥å­ä¸­çš„è·¨åº¦ï¼ˆç”¨äºç¡®å®šæˆ‘ä»¬çš„`Encoding`çš„æœ€ç»ˆ`offsets`ï¼‰ã€‚è¯·æ³¨æ„ï¼Œæ ¹æ®æ ‡ç‚¹ç¬¦å·åˆ†å‰²å°†åˆ†å‰²åƒ`"I'm"`è¿™ä¸ªä¾‹å­ä¸­çš„ç¼©å†™ã€‚
- en: 'You can combine together any `PreTokenizer` together. For instance, here is
    a pre-tokenizer that will split on space, punctuation and digits, separating numbers
    in their individual digits:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å°†ä»»ä½•`PreTokenizer`ç»„åˆåœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢„åˆ†è¯å™¨ï¼Œå°†åœ¨ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—ä¸Šè¿›è¡Œåˆ†å‰²ï¼Œå°†æ•°å­—åˆ†éš”ä¸ºå„è‡ªçš„æ•°å­—ï¼š
- en: PythonRustNode
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we saw in the `quicktour`, you can customize the pre-tokenizer of a `Tokenizer`
    by just changing the corresponding attribute:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨`å¿«é€Ÿæµè§ˆ`ä¸­çœ‹åˆ°çš„ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ›´æ”¹ç›¸åº”å±æ€§æ¥è‡ªå®šä¹‰`Tokenizer`çš„é¢„åˆ†è¯å™¨ï¼š
- en: PythonRustNode
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Of course, if you change the way the pre-tokenizer, you should probably retrain
    your tokenizer from scratch afterward.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå¦‚æœæ›´æ”¹äº†é¢„åˆ†è¯å™¨çš„æ–¹å¼ï¼Œé‚£ä¹ˆä¹‹åå¯èƒ½éœ€è¦ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒæ‚¨çš„åˆ†è¯å™¨ã€‚
- en: Model
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: Once the input texts are normalized and pre-tokenized, the `Tokenizer` applies
    the model on the pre-tokens. This is the part of the pipeline that needs training
    on your corpus (or that has been trained if you are using a pretrained tokenizer).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è¾“å…¥æ–‡æœ¬è¢«å½’ä¸€åŒ–å’Œé¢„åˆ†è¯ï¼Œ`Tokenizer`å°†åœ¨é¢„åˆ†è¯ä¸Šåº”ç”¨æ¨¡å‹ã€‚è¿™æ˜¯ç®¡é“ä¸­éœ€è¦åœ¨æ‚¨çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒçš„éƒ¨åˆ†ï¼ˆæˆ–è€…å¦‚æœæ‚¨ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œåˆ™å·²ç»è®­ç»ƒè¿‡ï¼‰ã€‚
- en: The role of the model is to split your â€œwordsâ€ into tokens, using the rules
    it has learned. Itâ€™s also responsible for mapping those tokens to their corresponding
    IDs in the vocabulary of the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„ä½œç”¨æ˜¯æ ¹æ®å…¶å­¦ä¹ çš„è§„åˆ™å°†æ‚¨çš„â€œå•è¯â€åˆ†å‰²ä¸ºæ ‡è®°ã€‚å®ƒè¿˜è´Ÿè´£å°†è¿™äº›æ ‡è®°æ˜ å°„åˆ°æ¨¡å‹è¯æ±‡è¡¨ä¸­çš„ç›¸åº”IDã€‚
- en: 'This model is passed along when intializing the `Tokenizer` so you already
    know how to customize this part. Currently, the ğŸ¤— Tokenizers library supports:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹åœ¨åˆå§‹åŒ–`Tokenizer`æ—¶ä¼ é€’ï¼Œæ‰€ä»¥æ‚¨å·²ç»çŸ¥é“å¦‚ä½•è‡ªå®šä¹‰è¿™éƒ¨åˆ†ã€‚ç›®å‰ï¼ŒğŸ¤— Tokenizersåº“æ”¯æŒï¼š
- en: '`models.BPE`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.BPE`'
- en: '`models.Unigram`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.Unigram`'
- en: '`models.WordLevel`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.WordLevel`'
- en: '`models.WordPiece`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.WordPiece`'
- en: For more details about each model and its behavior, you can check [here](components#models)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ¯ä¸ªæ¨¡å‹åŠå…¶è¡Œä¸ºçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥åœ¨[æ­¤å¤„](components#models)æŸ¥çœ‹
- en: Post-Processing
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå¤„ç†
- en: Post-processing is the last step of the tokenization pipeline, to perform any
    additional transformation to the `Encoding` before itâ€™s returned, like adding
    potential special tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åå¤„ç†æ˜¯åˆ†è¯ç®¡é“çš„æœ€åä¸€æ­¥ï¼Œç”¨äºåœ¨è¿”å›ä¹‹å‰å¯¹`Encoding`æ‰§è¡Œä»»ä½•é¢å¤–çš„è½¬æ¢ï¼Œä¾‹å¦‚æ·»åŠ æ½œåœ¨çš„ç‰¹æ®Šæ ‡è®°ã€‚
- en: 'As we saw in the quick tour, we can customize the post processor of a `Tokenizer`
    by setting the corresponding attribute. For instance, here is how we can post-process
    to make the inputs suitable for the BERT model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å¿«é€Ÿæµè§ˆä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®ç›¸åº”çš„å±æ€§æ¥è‡ªå®šä¹‰`Tokenizer`çš„åå¤„ç†å™¨ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯æˆ‘ä»¬å¦‚ä½•è¿›è¡Œåå¤„ç†ä»¥ä½¿è¾“å…¥é€‚åˆBERTæ¨¡å‹ï¼š
- en: PythonRustNode
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that contrarily to the pre-tokenizer or the normalizer, you donâ€™t need
    to retrain a tokenizer after changing its post-processor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸é¢„åˆ†è¯å™¨æˆ–è§„èŒƒåŒ–å™¨ç›¸åï¼Œæ›´æ”¹åå¤„ç†å™¨åä¸éœ€è¦é‡æ–°è®­ç»ƒåˆ†è¯å™¨ã€‚
- en: 'All together: a BERT tokenizer from scratch'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰€æœ‰åœ¨ä¸€èµ·ï¼šä»å¤´å¼€å§‹çš„BERTåˆ†è¯å™¨
- en: 'Letâ€™s put all those pieces together to build a BERT tokenizer. First, BERT
    relies on WordPiece, so we instantiate a new `Tokenizer` with this model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æŠŠæ‰€æœ‰è¿™äº›éƒ¨åˆ†æ”¾åœ¨ä¸€èµ·æ¥æ„å»ºä¸€ä¸ªBERTåˆ†è¯å™¨ã€‚é¦–å…ˆï¼ŒBERTä¾èµ–äºWordPieceï¼Œå› æ­¤æˆ‘ä»¬ç”¨è¿™ä¸ªæ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„`Tokenizer`ï¼š
- en: PythonRustNode
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we know that BERT preprocesses texts by removing accents and lowercasing.
    We also use a unicode normalizer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬çŸ¥é“BERTé€šè¿‡å»é™¤é‡éŸ³ç¬¦å·å’Œå°å†™æ¥é¢„å¤„ç†æ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†ä¸€ä¸ªUnicodeè§„èŒƒåŒ–å™¨ï¼š
- en: PythonRustNode
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The pre-tokenizer is just splitting on whitespace and punctuation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„åˆ†è¯å™¨åªæ˜¯åœ¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ä¸Šè¿›è¡Œåˆ†å‰²ï¼š
- en: PythonRustNode
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the post-processing uses the template we saw in the previous section:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åå¤„ç†ä½¿ç”¨äº†æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­çœ‹åˆ°çš„æ¨¡æ¿ï¼š
- en: PythonRustNode
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can use this tokenizer and train on it on wikitext like in the `quicktour`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªåˆ†è¯å™¨ï¼Œå¹¶åœ¨wikitextä¸Šè¿›è¡Œè®­ç»ƒï¼Œå°±åƒåœ¨`quicktour`ä¸­ä¸€æ ·ï¼š
- en: PythonRustNode
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Decoding
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£ç 
- en: On top of encoding the input texts, a `Tokenizer` also has an API for decoding,
    that is converting IDs generated by your model back to a text. This is done by
    the methods `Tokenizer.decode` (for one predicted text) and `Tokenizer.decode_batch`
    (for a batch of predictions).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç å¤–ï¼Œ`Tokenizer`è¿˜å…·æœ‰ç”¨äºè§£ç çš„APIï¼Œå³å°†æ¨¡å‹ç”Ÿæˆçš„IDè½¬æ¢å›æ–‡æœ¬ã€‚è¿™æ˜¯é€šè¿‡æ–¹æ³•`Tokenizer.decode`ï¼ˆç”¨äºä¸€ä¸ªé¢„æµ‹æ–‡æœ¬ï¼‰å’Œ`Tokenizer.decode_batch`ï¼ˆç”¨äºä¸€æ‰¹é¢„æµ‹ï¼‰æ¥å®Œæˆçš„ã€‚
- en: 'The `decoder` will first convert the IDs back to tokens (using the tokenizerâ€™s
    vocabulary) and remove all special tokens, then join those tokens with spaces:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoder`é¦–å…ˆå°†IDè½¬æ¢å›æ ‡è®°ï¼ˆä½¿ç”¨åˆ†è¯å™¨çš„è¯æ±‡è¡¨ï¼‰ï¼Œç„¶ååˆ é™¤æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œç„¶åç”¨ç©ºæ ¼è¿æ¥è¿™äº›æ ‡è®°ï¼š'
- en: PythonRustNode
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you used a model that added special characters to represent subtokens of
    a given â€œwordâ€ (like the `"##"` in WordPiece) you will need to customize the `decoder`
    to treat them properly. If we take our previous `bert_tokenizer` for instance
    the default decoding will give:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ·»åŠ äº†ç‰¹æ®Šå­—ç¬¦æ¥è¡¨ç¤ºç»™å®šâ€œå•è¯â€çš„å­æ ‡è®°ï¼ˆä¾‹å¦‚WordPieceä¸­çš„`"##"`ï¼‰ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰`decoder`ä»¥æ­£ç¡®å¤„ç†å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä»¥å‰çš„`bert_tokenizer`ï¼Œé»˜è®¤è§£ç å°†ä¼šç»™å‡ºï¼š
- en: PythonRustNode
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'But by changing it to a proper decoder, we get:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é€šè¿‡å°†å…¶æ›´æ”¹ä¸ºé€‚å½“çš„è§£ç å™¨ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: PythonRustNode
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
