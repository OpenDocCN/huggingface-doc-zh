- en: RWKV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RWKV
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The RWKV model was proposed in [this repo](https://github.com/BlinkDL/RWKV-LM)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: RWKV模型是在[此存储库](https://github.com/BlinkDL/RWKV-LM)中提出的。
- en: 'It suggests a tweak in the traditional Transformer attention to make it linear.
    This way, the model can be used as recurrent network: passing inputs for timestamp
    0 and timestamp 1 together is the same as passing inputs at timestamp 0, then
    inputs at timestamp 1 along with the state of timestamp 0 (see example below).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它建议对传统Transformer注意力进行微调，使其线性化。这样，模型可以用作循环网络：同时传递时间戳0和时间戳1的输入与在时间戳0传递输入，然后在时间戳1传递输入以及时间戳0的状态是相同的（见下面的示例）。
- en: This can be more efficient than a regular Transformer and can deal with sentence
    of any length (even if the model uses a fixed context length for training).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这比常规Transformer更有效，并且可以处理任意长度的句子（即使模型在训练时使用固定的上下文长度）。
- en: This model was contributed by [sgugger](https://huggingface.co/sgugger). The
    original code can be found [here](https://github.com/BlinkDL/RWKV-LM).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[sgugger](https://huggingface.co/sgugger)贡献的。原始代码可以在[这里](https://github.com/BlinkDL/RWKV-LM)找到。
- en: Usage example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法示例
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to make sure the model stops generating when `''\n\n''` is detected,
    we recommend using the following stopping criteria:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要确保模型在检测到`'\n\n'`时停止生成，我们建议使用以下停止标准：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: RwkvConfig
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RwkvConfig
- en: '### `class transformers.RwkvConfig`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RwkvConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/configuration_rwkv.py#L38)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/configuration_rwkv.py#L38)'
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 50277) — Vocabulary size of the
    RWKV model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`，*可选*，默认为50277) — RWKV模型的词汇量。定义了在调用[RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel)时可以表示的不同标记数量。'
- en: '`context_length` (`int`, *optional*, defaults to 1024) — The maximum sequence
    length that this model can be be used with in a single forward (using it in RNN
    mode lets use any sequence length).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_length` (`int`，*可选*，默认为1024) — 此模型可以在单个前向传播中使用的最大序列长度（在RNN模式中使用任何序列长度）。'
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) — Dimensionality of the
    embeddings and hidden states.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`，*可选*，默认为4096) — 嵌入和隐藏状态的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 32) — Number of hidden
    layers in the model.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`，*可选*，默认为32) — 模型中的隐藏层数量。'
- en: '`attention_hidden_size` (`int`, *optional*) — Dimensionality of the attention
    hidden states. Will default to `hidden_size` if unset.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_hidden_size` (`int`，*可选*) — 注意力隐藏状态的维度。如果未设置，将默认为`hidden_size`。'
- en: '`intermediate_size` (`int`, *optional*) — Dimensionality of the inner feed-forward
    layers. Will default to 4 times `hidden_size` if unset.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`，*可选*) — 内部前馈层的维度。如果未设置，将默认为`hidden_size`的4倍。'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) — The epsilon
    to use in the layer normalization layers.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`，*可选*，默认为1e-05) — 在层归一化层中使用的epsilon。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 0) — The id of the beginning
    of sentence token in the vocabulary. Defaults to 0 as RWKV uses the same tokenizer
    as GPTNeoX.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`，*可选*，默认为0) — 词汇表中句子开头标记的id。默认为0，因为RWKV使用与GPTNeoX相同的分词器。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 0) — The id of the end of sentence
    token in the vocabulary. Defaults to 0 as RWKV uses the same tokenizer as GPTNeoX.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`，*可选*，默认为0) — 词汇表中句子结尾标记的id。默认为0，因为RWKV使用与GPTNeoX相同的分词器。'
- en: '`rescale_every` (`int`, *optional*, defaults to 6) — At inference, the hidden
    states (and weights of the correponding output layers) are divided by 2 every
    `rescale_every` layer. If set to 0 or a negative number, no rescale is done.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_every` (`int`，*可选*，默认为6) — 推理时，隐藏状态（以及相应输出层的权重）每`rescale_every`层除以2。如果设置为0或负数，则不进行重新缩放。'
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether or
    not to tie the word embeddings with the input token embeddings.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tie_word_embeddings` (`bool`，*可选*，默认为`False`) — 是否将单词嵌入与输入标记嵌入相结合。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last state.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`，*可选*，默认为`True`) — 模型是否应返回最后一个状态。'
- en: This is the configuration class to store the configuration of a [RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel).
    It is used to instantiate a RWKV model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the RWVK-4 [RWKV/rwkv-4-169m-pile](https://huggingface.co/RWKV/rwkv-4-169m-pile)
    architecture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是存储[RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel)配置的配置类。它用于根据指定的参数实例化一个RWKV模型，定义模型架构。使用默认值实例化配置将产生类似于RWVK-4
    [RWKV/rwkv-4-169m-pile](https://huggingface.co/RWKV/rwkv-4-169m-pile)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: RwkvModel
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RwkvModel
- en: '### `class transformers.RwkvModel`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RwkvModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L592)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L592)'
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare RWKV Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的RWKV模型变压器输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `前进`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L617)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L617)'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）— 如果`past_key_values`为`None`，则`input_ids_length`
    = `sequence_length`，否则为`past_key_values[0][0].shape[-2]`（输入过去关键值状态的序列长度）。词汇表中输入序列标记的索引。'
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则只有那些没有计算过去的`input_ids`应该作为`input_ids`传递。
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“掩码”的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩码”的标记为0。
- en: This is currently not used by `RwkvModel`, but will be supported in the future.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前`RwkvModel`不使用这个，但将在未来支持。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`state` (tuple of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`, *optional*) — If passed along, the model uses the previous
    state in all the blocks (which will give the output for the `input_ids` provided
    as if the model add `state_input_ids + input_ids` as context).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`（五个形状为`(batch_size, hidden_size, num_hidden_layers)`的`torch.FloatTensor`元组，*可选*）—
    如果传递，模型将在所有块中使用先前的状态（这将为提供的`input_ids`提供输出，就好像模型将`state_input_ids + input_ids`作为上下文）。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, the last state is returned
    and can be used to quickly generate the next logits.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回上一个状态，并可用于快速生成下一个对数。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.rwkv.modeling_rwkv.RwkvOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.rwkv.modeling_rwkv.RwkvOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.rwkv.modeling_rwkv.RwkvOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    and inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.rwkv.modeling_rwkv.RwkvOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`state` (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`) — The state of the model at the last time step. Can be used
    in a forward method with the next `input_ids` to avoid providing the old `input_ids`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`（形状为`(batch_size, hidden_size, num_hidden_layers)`的五个`torch.FloatTensor`列表）-
    模型在最后一个时间步的状态。可以在前向方法中与下一个`input_ids`一起使用，以避免提供旧的`input_ids`。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+
    一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel)
    forward method, overrides the `__call__` special method.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[RwkvModel](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: RwkvLMHeadModel
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RwkvLMHeadModel
- en: '### `class transformers.RwkvForCausalLM`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RwkvForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L757)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L757)'
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The RWKV Model transformer with a language modeling head on top (linear layer
    with weights tied to the input embeddings).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 带有语言建模头部的RWKV模型变压器（线性层，其权重与输入嵌入绑定）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L795)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L795)'
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）- 如果`past_key_values`为`None`，则`input_ids_length`=`sequence_length`，否则`input_ids_length`=`past_key_values[0][0].shape[-2]`（输入过去关键值状态的序列长度）。词汇表中输入序列标记的索引。'
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则只应将未计算其过去的`input_ids`作为`input_ids`传递。
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`，形状为`(batch_size, input_ids_length)`，*可选*)
    — 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩盖`的标记。
- en: This is currently not used by `RwkvModel`, but will be supported in the future.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这目前`RwkvModel`没有使用，但将来会支持。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`state` (tuple of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`, *optional*) — If passed along, the model uses the previous
    state in all the blocks (which will give the output for the `input_ids` provided
    as if the model add `state_input_ids + input_ids` as context).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`（五个形状为`(batch_size, hidden_size, num_hidden_layers)`的`torch.FloatTensor`元组，*可选*）
    — 如果传递，模型将在所有块中使用先前的状态（这将为提供的`input_ids`产生输出，就好像模型将`state_input_ids + input_ids`作为上下文）。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, the last state is returned
    and can be used to quickly generate the next logits.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`，*可选*) — 如果设置为`True`，则返回上一个状态，并可用于快速生成下一个logits。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 语言建模的标签。请注意，标签**在模型内部被移位**，即您可以设置`labels
    = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（掩盖），损失仅计算在`[0,
    ..., config.vocab_size]`中的标签。'
- en: Returns
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig))
    and inputs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[RwkvConfig](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`state` (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size,
    num_hidden_layers)`) — The state of the model at the last time step. Can be used
    in a forward method with the next `input_ids` to avoid providing the old `input_ids`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`（五个形状为`(batch_size, hidden_size, num_hidden_layers)`的`torch.FloatTensor`列表）
    — 模型在最后一个时间步的状态。可以在前向方法中与下一个`input_ids`一起使用，以避免提供旧的`input_ids`。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出一个，+每个层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [RwkvForCausalLM](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[RwkvForCausalLM](/docs/transformers/v4.37.2/en/model_doc/rwkv#transformers.RwkvForCausalLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Rwkv attention and the recurrent formulas
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rwkv注意力和循环公式
- en: In a traditional auto-regressive Transformer, attention is written as <math
    display="block"><semantics><mrow><mi>O</mi><mo>=</mo><mrow><mtext>softmax</mtext></mrow><mo
    stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt><mo
    stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">O
    = \hbox{softmax}(QK^{T} / \sqrt{d}) V</annotation></semantics></math>O=softmax(QKT/d​)V
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的自回归Transformer中，注意力写为<math display="block"><semantics><mrow><mi>O</mi><mo>=</mo><mrow><mtext>softmax</mtext></mrow><mo
    stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt><mo
    stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">O
    = \hbox{softmax}(QK^{T} / \sqrt{d}) V</annotation></semantics></math>O=softmax(QKT/d​)V
- en: with<math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math>Q,<math><semantics><mrow><mi>K</mi></mrow><annotation
    encoding="application/x-tex">K</annotation></semantics></math>K and<math><semantics><mrow><mi>V</mi></mrow><annotation
    encoding="application/x-tex">V</annotation></semantics></math>V are matrices of
    shape `seq_len x hidden_size` named query, key and value (they are actually bigger
    matrices with a batch dimension and an attention head dimension but we’re only
    interested in the last two, which is where the matrix product is taken, so for
    the sake of simplicity we only consider those two). The product<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^{T}</annotation></semantics></math>QKT then has
    shape `seq_len x seq_len` and we can take the maxtrix product with<math><semantics><mrow><mi>V</mi></mrow><annotation
    encoding="application/x-tex">V</annotation></semantics></math>V to get the output<math><semantics><mrow><mi>O</mi></mrow><annotation
    encoding="application/x-tex">O</annotation></semantics></math>O of the same shape
    as the others.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math>Q，<math><semantics><mrow><mi>K</mi></mrow><annotation
    encoding="application/x-tex">K</annotation></semantics></math>K和<math><semantics><mrow><mi>V</mi></mrow><annotation
    encoding="application/x-tex">V</annotation></semantics></math>V是形状为`seq_len x
    hidden_size`的矩阵，分别命名为查询、键和值（实际上它们是带有批处理维度和注意力头维度的更大矩阵，但我们只关心最后两个，这是矩阵乘积发生的地方，所以为了简单起见，我们只考虑这两个）。乘积<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^{T}</annotation></semantics></math>QKT然后具有形状`seq_len
    x seq_len`，我们可以将其与<math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math>V进行矩阵乘积，得到与其他相同形状的输出<math><semantics><mrow><mi>O</mi></mrow><annotation
    encoding="application/x-tex">O</annotation></semantics></math>O。
- en: 'Replacing the softmax by its value gives: <math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi
    mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi
    mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">O_{i} = \frac{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} /
    \sqrt{d}} V_{j}}{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \sqrt{d}}}</annotation></semantics></math>Oi​=∑j=1i​eQi​KjT​/d​∑j=1i​eQi​KjT​/d​Vj​​'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 用其值替换softmax得到：<math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi
    mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi
    mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">O_{i} = \frac{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} /
    \sqrt{d}} V_{j}}{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \sqrt{d}}}</annotation></semantics></math>Oi​=∑j=1i​eQi​KjT​/d​∑j=1i​eQi​KjT​/d​Vj​​
- en: Note that the entries in<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^{T}</annotation></semantics></math>QKT corresponding
    to<math><semantics><mrow><mi>j</mi><mo>></mo><mi>i</mi></mrow><annotation encoding="application/x-tex">j
    > i</annotation></semantics></math>j>i are masked (the sum stops at j) because
    the attention is not allowed to look at future tokens (only past ones).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，QKT中与j>i对应的条目被屏蔽（总和在j处停止），因为注意力不允许查看未来的令牌（只能查看过去的令牌）。
- en: In comparison, the RWKV attention is given by <math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mi>σ</mi><mo
    stretchy="false">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>K</mi><mi>j</mi></msub></mrow></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>K</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">O_{i} = \sigma(R_{i}) \frac{\sum_{j=1}^{i} e^{W_{i-j}
    + K_{j}} V_{j}}{\sum_{j=1}^{i} e^{W_{i-j} + K_{j}}}</annotation></semantics></math>Oi​=σ(Ri​)∑j=1i​eWi−j​+Kj​∑j=1i​eWi−j​+Kj​Vj​​
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，RWKV注意力由Oi = σ(Ri)（∑j=1i eWi−j + Kj Vj）/（∑j=1i eWi−j + Kj）给出。
- en: where<math><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math>R
    is a new matrix called receptance by the author,<math><semantics><mrow><mi>K</mi></mrow><annotation
    encoding="application/x-tex">K</annotation></semantics></math>K and<math><semantics><mrow><mi>V</mi></mrow><annotation
    encoding="application/x-tex">V</annotation></semantics></math>V are still the
    key and value (\(\sigma\) here is the sigmoid function).<math><semantics><mrow><mi>W</mi></mrow><annotation
    encoding="application/x-tex">W</annotation></semantics></math>W is a new vector
    that represents the position of the token and is given by <math display="block"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>=</mo><mi>u</mi><mrow><mtext> and </mtext></mrow><msub><mi>W</mi><mi>k</mi></msub><mo>=</mo><mo
    stretchy="false">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi>w</mi><mrow><mtext> for </mtext></mrow><mi>k</mi><mo>≥</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">W_{0} = u \hbox{ and } W_{k} = (k-1)w \hbox{ for
    } k \geq 1</annotation></semantics></math>W0​=u and Wk​=(k−1)w for k≥1
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作者称之为接受度的新矩阵R，K和V仍然是关键和值（这里σ是Sigmoid函数）。W是代表令牌位置的新向量，由W0 = u和Wk = (k-1)w（对于k≥1）给出。
- en: 'with<math><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math>u
    and<math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math>w
    learnable parameters called in the code `time_first` and `time_decay` respectively.
    The numerator and denominator can both be expressed recursively. Naming them<math><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">N_{i}</annotation></semantics></math>Ni​ and<math><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">D_{i}</annotation></semantics></math>Di​ we have:
    <math display="block"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub></mrow></msup><msub><mi>V</mi><mi>i</mi></msub><mo>+</mo><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></msup><msub><mi>V</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow></msup><msub><mi>V</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub><mo>⋯</mo><mo>+</mo><msup><mi>e</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>K</mi><mn>1</mn></msub></mrow></msup><msub><mi>V</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">N_{i} = e^{u + K_{i}} V_{i} + \hat{N}_{i} \hbox{
    where } \hat{N}_{i} = e^{K_{i-1}} V_{i-1} + e^{w + K_{i-2}} V_{i-2} \cdots + e^{(i-2)w
    + K_{1}} V_{1}</annotation></semantics></math>Ni​=eu+Ki​Vi​+N^i​ where N^i​=eKi−1​Vi−1​+ew+Ki−2​Vi−2​⋯+e(i−2)w+K1​V1​'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: u和w是可学习的参数，分别在代码中称为`time_first`和`time_decay`。分子和分母都可以递归表示。将它们命名为Ni和Di，我们有：Ni
    = e^(u + Ki)Vi + N^i 其中N^i = e^(Ki-1)Vi-1 + e^(w + Ki-2)Vi-2 + ... + e^((i-2)w
    + K1)Vi-1
- en: so<math><semantics><mrow><msub><mover accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{N}_{i}</annotation></semantics></math>N^i​ (called
    `numerator_state` in the code) satistfies <math display="block"><semantics><mrow><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mi>j</mi></msub></msup><msub><mi>V</mi><mi>j</mi></msub><mo>+</mo><msup><mi>e</mi><mi>w</mi></msup><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{N}_{0} = 0 \hbox{ and } \hat{N}_{j+1} = e^{K_{j}}
    V_{j} + e^{w} \hat{N}_{j}</annotation></semantics></math>N^0​=0 and N^j+1​=eKj​Vj​+ewN^j​
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所以N^i（在代码中称为`numerator_state`）满足N^0 = 0 和 N^j+1 = e^Kj Vj + e^w N^j
- en: and <math display="block"><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub></mrow></msup><mo>+</mo><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>K</mi><mrow><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow></msup><mo>⋯</mo><mo>+</mo><msup><mi>e</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>K</mi><mn>1</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">D_{i} = e^{u + K_{i}} + \hat{D}_{i} \hbox{ where
    } \hat{D}_{i} = e^{K_{i-1}} + e^{w + K_{i-2}} \cdots + e^{(i-2)w + K_{1}}</annotation></semantics></math>Di​=eu+Ki​+D^i​ where D^i​=eKi−1​+ew+Ki−2​⋯+e(i−2)w+K1​
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 和Di = e^(u + Ki) + D^i 其中D^i = e^(Ki-1) + e^(w + Ki-2) + ... + e^((i-2)w + K1)
- en: so<math><semantics><mrow><msub><mover accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{D}_{i}</annotation></semantics></math>D^i​ (called
    `denominator_state` in the code) satistfies <math display="block"><semantics><mrow><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><msub><mi>K</mi><mi>j</mi></msub></msup><mo>+</mo><msup><mi>e</mi><mi>w</mi></msup><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">\hat{D}_{0} = 0 \hbox{ and } \hat{D}_{j+1} = e^{K_{j}}
    + e^{w} \hat{D}_{j}</annotation></semantics></math>D^0​=0 and D^j+1​=eKj​+ewD^j​
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(\hat{D}^{i}\)（在代码中称为`denominator_state`）满足\(\hat{D}_{0} = 0\)和\(\hat{D}_{j+1}
    = e^{K_{j}} + e^{w} \hat{D}_{j}\)。
- en: 'The actual recurrent formula used are a tiny bit more complex, as for numerical
    stability we don’t want to compute exponentials of big numbers. Usually the softmax
    is not computed as is, but the exponential of the maximum term is divided of the
    numerator and denominator: <math display="block"><semantics><mrow><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>M</mi></mrow></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><mi>M</mi></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\frac{e^{x_{i}}}{\sum_{j=1}^{n} e^{x_{j}}} = \frac{e^{x_{i}
    - M}}{\sum_{j=1}^{n} e^{x_{j} - M}}</annotation></semantics></math>∑j=1n​exj​exi​​=∑j=1n​exj​−Mexi​−M​'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 实际使用的递归公式稍微复杂一些，因为为了数值稳定性，我们不希望计算大数的指数。通常，softmax不是按原样计算的，而是将最大项的指数除以分子和分母：\(\frac{e^{x_{i}}}{\sum_{j=1}^{n}
    e^{x_{j}}} = \frac{e^{x_{i} - M}}{\sum_{j=1}^{n} e^{x_{j} - M}}\)。
- en: with<math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math>M
    the maximum of all<math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation
    encoding="application/x-tex">x_{j}</annotation></semantics></math>xj​. So here
    on top of saving the numerator state (\(\hat{N}\)) and the denominator state (\(\hat{D}\))
    we also keep track of the maximum of all terms encountered in the exponentials.
    So we actually use <math display="block"><semantics><mrow><msub><mover accent="true"><mi>N</mi><mo>~</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><msub><mi>M</mi><mi>i</mi></msub></mrow></msup><msub><mover
    accent="true"><mi>N</mi><mo>^</mo></mover><mi>i</mi></msub><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><msub><mi>M</mi><mi>i</mi></msub></mrow></msup><msub><mover
    accent="true"><mi>D</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">\tilde{N}_{i} = e^{-M_{i}} \hat{N}_{i} \hbox{ and
    } \tilde{D}_{i} = e^{-M_{i}} \hat{D}_{i}</annotation></semantics></math>N~i​=e−Mi​N^i​ and D~i​=e−Mi​D^i​
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: M是所有xj的最大值。因此，在保存分子状态（\(\hat{N}\)）和分母状态（\(\hat{D}\)）的同时，我们还跟踪遇到的所有指数项的最大值。因此，我们实际上使用\(\tilde{N}_{i}
    = e^{-M_{i}} \hat{N}_{i}\)和\(\tilde{D}_{i} = e^{-M_{i}} \hat{D}_{i}\)。
- en: 'defined by the following recurrent formulas: <math display="block"><semantics><mrow><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>j</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{N}_{0}
    = 0 \hbox{ and } \tilde{N}_{j+1} = e^{K_{j} - q} V_{j} + e^{w + M_{j} - q} \tilde{N}_{j}
    \hbox{ where } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>N~0​=0 and N~j+1​=eKj​−qVj​+ew+Mj​−qN~j​ where q=max(Kj​,w+Mj​)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下递归公式定义：<math display="block"><semantics><mrow><msub><mover accent="true"><mi>N</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> 和 </mtext></mrow><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>j</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> 其中 </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{N}_{0}
    = 0 \hbox{ 和 } \tilde{N}_{j+1} = e^{K_{j} - q} V_{j} + e^{w + M_{j} - q} \tilde{N}_{j}
    \hbox{ 其中 } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>N~0​=0 和 N~j+1​=eKj​−qVj​+ew+Mj​−qN~j​ 其中 q=max(Kj​,w+Mj​)
- en: and <math display="block"><semantics><mrow><msub><mover accent="true"><mi>D</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> and </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{D}_{0}
    = 0 \hbox{ and } \tilde{D}_{j+1} = e^{K_{j} - q} + e^{w + M_{j} - q} \tilde{D}_{j}
    \hbox{ where } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>D~0​=0 and D~j+1​=eKj​−q+ew+Mj​−qD~j​ where q=max(Kj​,w+Mj​)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下递归公式定义：<math display="block"><semantics><mrow><msub><mover accent="true"><mi>D</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> 和 </mtext></mrow><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> 其中 </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{D}_{0}
    = 0 \hbox{ 和 } \tilde{D}_{j+1} = e^{K_{j} - q} + e^{w + M_{j} - q} \tilde{D}_{j}
    \hbox{ 其中 } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>D~0​=0 和 D~j+1​=eKj​−q+ew+Mj​−qD~j​ 其中 q=max(Kj​,w+Mj​)
- en: and<math><semantics><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>q</mi></mrow><annotation
    encoding="application/x-tex">M_{j+1} = q</annotation></semantics></math>Mj+1​=q.
    With those, we can then compute <math display="block"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>i</mi></msub><mo>+</mo><msup><mi>e</mi><msub><mi>M</mi><mi>i</mi></msub></msup><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo
    separator="true">,</mo><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">N_{i} = e^{u + K_{i} - q} V_{i} + e^{M_{i}} \tilde{N}_{i}
    \hbox{ where } q = \max(u + K_{i}, M_{i})</annotation></semantics></math>Ni​=eu+Ki​−qVi​+eMi​N~i​ where q=max(u+Ki​,Mi​)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 和<math><semantics><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>q</mi></mrow><annotation
    encoding="application/x-tex">M_{j+1} = q</annotation></semantics></math>Mj+1​=q。有了这些，我们可以计算<math
    display="block"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>i</mi></sub><mo>+</mo><msup><mi>e</mi><msub><mi>M</mi><mi>i</mi></msub></msup><msub><mover
    accent="true"><mi>N</mi><mo>~</mo></mover><mi>i</mi></msub><mrow><mtext> 其中 </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo
    separator="true">,</mo><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">N_{i} = e^{u + K_{i} - q} V_{i} + e^{M_{i}} \tilde{N}_{i}
    \hbox{ 其中 } q = \max(u + K_{i}, M_{i})</annotation></semantics></math>Ni​=eu+Ki​−qVi​+eMi​N~i​ 其中 q=max(u+Ki​,Mi​)
- en: and <math display="block"><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><msub><mi>M</mi><mi>i</mi></msub></msup><msub><mover
    accent="true"><mi>D</mi><mo>~</mo></mover><mi>i</mi></msub><mrow><mtext> where </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo
    separator="true">,</mo><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">D_{i} = e^{u + K_{i} - q} + e^{M_{i}} \tilde{D}_{i}
    \hbox{ where } q = \max(u + K_{i}, M_{i})</annotation></semantics></math>Di​=eu+Ki​−q+eMi​D~i​ where q=max(u+Ki​,Mi​)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 和Di = e^(u + Ki - q) + e^Mi D~i，其中q = max(u + Ki, Mi)
- en: which finally gives us <math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mi>σ</mi><mo
    stretchy="false">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mfrac><msub><mi>N</mi><mi>i</mi></msub><msub><mi>D</mi><mi>i</mi></msub></mfrac></mrow><annotation
    encoding="application/x-tex">O_{i} = \sigma(R_{i}) \frac{N_{i}}{D_{i}}</annotation></semantics></math>Oi​=σ(Ri​)Di​Ni​​
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最终给出了Oi = σ(Ri) Ni / Di
