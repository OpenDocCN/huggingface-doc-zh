- en: Neuron Model Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¥ç»å…ƒæ¨¡å‹æ¨ç†
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/guides/models](https://huggingface.co/docs/optimum-neuron/guides/models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/optimum-neuron/guides/models](https://huggingface.co/docs/optimum-neuron/guides/models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '*The APIs presented in the following documentation are relevant for the inference
    on [inf2](https://aws.amazon.com/ec2/instance-types/inf2/), [trn1](https://aws.amazon.com/ec2/instance-types/trn1/)
    and [inf1](https://aws.amazon.com/ec2/instance-types/inf1/).*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä»¥ä¸‹æ–‡æ¡£ä¸­ä»‹ç»çš„APIé€‚ç”¨äº[inf2](https://aws.amazon.com/ec2/instance-types/inf2/)ã€[trn1](https://aws.amazon.com/ec2/instance-types/trn1/)å’Œ[inf1](https://aws.amazon.com/ec2/instance-types/inf1/)ä¸Šçš„æ¨ç†ã€‚*'
- en: '`NeuronModelForXXX` classes help to load models from the [Hugging Face Hub](hf.co/models)
    and compile them to a serialized format optimized for neuron devices. You will
    then be able to load the model and run inference with the acceleration powered
    by AWS Neuron devices.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '`NeuronModelForXXX`ç±»æœ‰åŠ©äºä»[Hugging Face Hub](hf.co/models)åŠ è½½æ¨¡å‹å¹¶å°†å…¶ç¼–è¯‘ä¸ºé’ˆå¯¹ç¥ç»å…ƒè®¾å¤‡ä¼˜åŒ–çš„åºåˆ—åŒ–æ ¼å¼ã€‚ç„¶åï¼Œæ‚¨å°†èƒ½å¤ŸåŠ è½½æ¨¡å‹å¹¶é€šè¿‡AWS
    Neuronè®¾å¤‡æä¾›çš„åŠ é€Ÿè¿è¡Œæ¨ç†ã€‚'
- en: Switching from Transformers to Optimum
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»Transformersåˆ‡æ¢åˆ°Optimum
- en: The `optimum.neuron.NeuronModelForXXX` model classes are APIs compatible with
    Hugging Face Transformers models. This means seamless integration with Hugging
    Faceâ€™s ecosystem. You can just replace your `AutoModelForXXX` class with the corresponding
    `NeuronModelForXXX` class in `optimum.neuron`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimum.neuron.NeuronModelForXXX`æ¨¡å‹ç±»æ˜¯ä¸Hugging Face Transformersæ¨¡å‹å…¼å®¹çš„APIã€‚è¿™æ„å‘³ç€ä¸Hugging
    Faceçš„ç”Ÿæ€ç³»ç»Ÿæ— ç¼é›†æˆã€‚æ‚¨åªéœ€åœ¨`optimum.neuron`ä¸­ç”¨ç›¸åº”çš„`NeuronModelForXXX`ç±»æ›¿æ¢æ‚¨çš„`AutoModelForXXX`ç±»ã€‚'
- en: 'If you already use Transformers, you will be able to reuse your code just by
    replacing model classes:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å·²ç»ä½¿ç”¨Transformersï¼Œæ‚¨å°†èƒ½å¤Ÿé€šè¿‡æ›¿æ¢æ¨¡å‹ç±»æ¥é‡ç”¨æ‚¨çš„ä»£ç ï¼š
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As shown above, when you use `NeuronModelForXXX` for the first time, you will
    need to set `export=True` to compile your model from PyTorch to a neuron-compatible
    format.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€ç¤ºï¼Œå½“æ‚¨ç¬¬ä¸€æ¬¡ä½¿ç”¨`NeuronModelForXXX`æ—¶ï¼Œæ‚¨éœ€è¦è®¾ç½®`export=True`å°†æ‚¨çš„æ¨¡å‹ä»PyTorchç¼–è¯‘ä¸ºç¥ç»å…ƒå…¼å®¹æ ¼å¼ã€‚
- en: You will also need to pass Neuron specific parameters to configure the export.
    Each model architecture has its own set of parameters, as detailed in the next
    paragraphs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜éœ€è¦ä¼ é€’ç¥ç»å…ƒç‰¹å®šçš„å‚æ•°æ¥é…ç½®å¯¼å‡ºã€‚æ¯ä¸ªæ¨¡å‹æ¶æ„éƒ½æœ‰è‡ªå·±çš„ä¸€ç»„å‚æ•°ï¼Œå¦‚ä¸‹ä¸€æ®µè¯¦ç»†è¯´æ˜ã€‚
- en: 'Once your model has been exported, you can save it either on your local or
    in the [Hugging Face Model Hub](https://hf.co/models):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨çš„æ¨¡å‹è¢«å¯¼å‡ºï¼Œæ‚¨å¯ä»¥å°†å…¶ä¿å­˜åœ¨æœ¬åœ°æˆ–[Hugging Face Model Hub](https://hf.co/models)ä¸­ï¼š
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And the next time when you want to run inference, just load your compiled model
    which will save you the compilation time:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡å½“æ‚¨æƒ³è¦è¿è¡Œæ¨ç†æ—¶ï¼Œåªéœ€åŠ è½½æ‚¨ç¼–è¯‘çš„æ¨¡å‹ï¼Œè¿™å°†èŠ‚çœæ‚¨çš„ç¼–è¯‘æ—¶é—´ï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you see, there is no need to pass the neuron arguments used during the export
    as they are saved in a `config.json` file, and will be restored automatically
    by `NeuronModelForXXX` class.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€è§ï¼Œæ— éœ€ä¼ é€’å¯¼å‡ºæœŸé—´ä½¿ç”¨çš„ç¥ç»å…ƒå‚æ•°ï¼Œå› ä¸ºå®ƒä»¬ä¿å­˜åœ¨`config.json`æ–‡ä»¶ä¸­ï¼Œå¹¶å°†ç”±`NeuronModelForXXX`ç±»è‡ªåŠ¨æ¢å¤ã€‚
- en: When running inference for the first time, there is a warmup phase when you
    run the pipeline for the first time. This run would take 3x-4x higher latency
    than a regular run.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡è¿è¡Œæ¨ç†æ—¶ï¼Œå½“æ‚¨ç¬¬ä¸€æ¬¡è¿è¡Œç®¡é“æ—¶ä¼šæœ‰ä¸€ä¸ªé¢„çƒ­é˜¶æ®µã€‚è¿™æ¬¡è¿è¡Œçš„å»¶è¿Ÿæ¯”å¸¸è§„è¿è¡Œé«˜3å€è‡³4å€ã€‚
- en: Discriminative NLP models
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­§è§†æ€§NLPæ¨¡å‹
- en: 'As explained in the previous section, you will need only few modifications
    to your Transformers code to export and run NLP models:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæ‚¨åªéœ€è¦å¯¹Transformersä»£ç è¿›è¡Œå°‘é‡ä¿®æ”¹ï¼Œå³å¯å¯¼å‡ºå’Œè¿è¡ŒNLPæ¨¡å‹ï¼š
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`compiler_args` are optional arguments for the compiler, these arguments usually
    control how the compiler makes tradeoff between the inference performance (latency
    and throughput) and the accuracy. Here we cast FP32 operations to BF16 using the
    Neuron matrix-multiplication engine.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`compiler_args`æ˜¯ç¼–è¯‘å™¨çš„å¯é€‰å‚æ•°ï¼Œè¿™äº›å‚æ•°é€šå¸¸æ§åˆ¶ç¼–è¯‘å™¨åœ¨æ¨ç†æ€§èƒ½ï¼ˆå»¶è¿Ÿå’Œååé‡ï¼‰å’Œå‡†ç¡®æ€§ä¹‹é—´åšå‡ºæƒè¡¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ç¥ç»å…ƒçŸ©é˜µä¹˜æ³•å¼•æ“å°†FP32æ“ä½œè½¬æ¢ä¸ºBF16ã€‚'
- en: '`input_shapes` are mandatory static shape information that you need to send
    to the neuron compiler. Wondering what shapes are mandatory for your model? Check
    it out with the following code:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_shapes`æ˜¯æ‚¨éœ€è¦å‘é€ç»™ç¥ç»å…ƒç¼–è¯‘å™¨çš„å¼ºåˆ¶é™æ€å½¢çŠ¶ä¿¡æ¯ã€‚æƒ³çŸ¥é“æ‚¨çš„æ¨¡å‹éœ€è¦å“ªäº›å¼ºåˆ¶å½¢çŠ¶ï¼Ÿä½¿ç”¨ä»¥ä¸‹ä»£ç æŸ¥çœ‹ï¼š'
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Be careful, the input shapes used for compilation should be inferior than the
    size of inputs that you will feed into the model during the inference.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œç”¨äºç¼–è¯‘çš„è¾“å…¥å½¢çŠ¶åº”è¯¥å°äºæ‚¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†é¦ˆé€åˆ°æ¨¡å‹ä¸­çš„è¾“å…¥å¤§å°ã€‚
- en: What if input sizes are smaller than compilation input shapes?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœè¾“å…¥å¤§å°å°äºç¼–è¯‘è¾“å…¥å½¢çŠ¶æ€ä¹ˆåŠï¼Ÿ
- en: No worries, `NeuronModelForXXX` class will pad your inputs to an eligible shape.
    Besides you can set `dynamic_batch_size=True` in the `from_pretrained` method
    to enable dynamic batching, which means that your inputs can have variable batch
    size.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ«æ‹…å¿ƒï¼Œ`NeuronModelForXXX`ç±»å°†å¡«å……æ‚¨çš„è¾“å…¥åˆ°ä¸€ä¸ªåˆé€‚çš„å½¢çŠ¶ã€‚æ­¤å¤–ï¼Œæ‚¨å¯ä»¥åœ¨`from_pretrained`æ–¹æ³•ä¸­è®¾ç½®`dynamic_batch_size=True`æ¥å¯ç”¨åŠ¨æ€æ‰¹å¤„ç†ï¼Œè¿™æ„å‘³ç€æ‚¨çš„è¾“å…¥å¯ä»¥å…·æœ‰å¯å˜çš„æ‰¹å¤„ç†å¤§å°ã€‚
- en: '*(Just keep in mind: dynamicity and padding comes with not only flexibility
    but also performance drop. Fair enough!)*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆåªéœ€è®°ä½ï¼šåŠ¨æ€æ€§å’Œå¡«å……ä¸ä»…å¸¦æ¥äº†çµæ´»æ€§ï¼Œè¿˜å¸¦æ¥äº†æ€§èƒ½ä¸‹é™ã€‚å¤Ÿå…¬å¹³ï¼ï¼‰
- en: Generative NLP models
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ€§NLPæ¨¡å‹
- en: 'As explained before, you will need only a few modifications to your Transformers
    code to export and run NLP models:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæ‚¨åªéœ€è¦å¯¹Transformersä»£ç è¿›è¡Œå°‘é‡ä¿®æ”¹ï¼Œå³å¯å¯¼å‡ºå’Œè¿è¡ŒNLPæ¨¡å‹ï¼š
- en: Configuring the export of a generative model
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é…ç½®ç”Ÿæˆæ¨¡å‹çš„å¯¼å‡º
- en: 'As for non-generative models, two sets of parameters can be passed to the `from_pretrained()`
    method to configure how a transformers checkpoint is exported to a neuron optimized
    model:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºéç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ä¼ é€’ä¸¤ç»„å‚æ•°ç»™`from_pretrained()`æ–¹æ³•ï¼Œä»¥é…ç½®å¦‚ä½•å°†transformersæ£€æŸ¥ç‚¹å¯¼å‡ºä¸ºç¥ç»å…ƒä¼˜åŒ–æ¨¡å‹ï¼š
- en: '`compiler_args = { num_cores, auto_cast_type }` are optional arguments for
    the compiler, these arguments usually control how the compiler makes tradeoff
    between the inference latency and throughput and the accuracy.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compiler_args = { num_cores, auto_cast_type }`æ˜¯ç¼–è¯‘å™¨çš„å¯é€‰å‚æ•°ï¼Œè¿™äº›å‚æ•°é€šå¸¸æ§åˆ¶ç¼–è¯‘å™¨åœ¨æ¨ç†å»¶è¿Ÿã€ååé‡å’Œå‡†ç¡®æ€§ä¹‹é—´åšå‡ºæƒè¡¡ã€‚'
- en: '`input_shapes = { batch_size, sequence_length }` correspond to the static shape
    of the model input and the KV-cache (attention keys and values for past tokens).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shapes = { batch_size, sequence_length }`å¯¹åº”äºæ¨¡å‹è¾“å…¥å’ŒKV-cacheï¼ˆè¿‡å»æ ‡è®°çš„æ³¨æ„åŠ›é”®å’Œå€¼ï¼‰çš„é™æ€å½¢çŠ¶ã€‚'
- en: '`num_cores` is the number of neuron cores used when instantiating the model.
    Each neuron core has 16 Gb of memory, which means that bigger models need to be
    split on multiple cores. Defaults to 1,'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_cores`æ˜¯å®ä¾‹åŒ–æ¨¡å‹æ—¶ä½¿ç”¨çš„ç¥ç»å…ƒæ ¸å¿ƒæ•°é‡ã€‚æ¯ä¸ªç¥ç»å…ƒæ ¸å¿ƒæœ‰16Gbçš„å†…å­˜ï¼Œè¿™æ„å‘³ç€æ›´å¤§çš„æ¨¡å‹éœ€è¦åˆ†å‰²åˆ°å¤šä¸ªæ ¸å¿ƒä¸Šã€‚é»˜è®¤ä¸º1ï¼Œ'
- en: '`auto_cast_type` specifies the format to encode the weights. It can be one
    of `fp32` (`float32`), `fp16` (`float16`) or `bf16` (`bfloat16`). Defaults to
    `fp32`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_cast_type`æŒ‡å®šç¼–ç æƒé‡çš„æ ¼å¼ã€‚å¯ä»¥æ˜¯`fp32`ï¼ˆ`float32`ï¼‰ï¼Œ`fp16`ï¼ˆ`float16`ï¼‰æˆ–`bf16`ï¼ˆ`bfloat16`ï¼‰ã€‚é»˜è®¤ä¸º`fp32`ã€‚'
- en: '`batch_size` is the number of input sequences that the model will accept. Defaults
    to 1,'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`æ˜¯æ¨¡å‹å°†æ¥å—çš„è¾“å…¥åºåˆ—çš„æ•°é‡ã€‚é»˜è®¤ä¸º1ï¼Œ'
- en: '`sequence_length` is the maximum number of tokens in an input sequence. Defaults
    to `max_position_embeddings` (`n_positions` for older models).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_length`æ˜¯è¾“å…¥åºåˆ—ä¸­æ ‡è®°çš„æœ€å¤§æ•°é‡ã€‚é»˜è®¤ä¸º`max_position_embeddings`ï¼ˆæ—§æ¨¡å‹çš„`n_positions`ï¼‰ã€‚'
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As explained before, these parameters can only be configured during export.
    This means in particular that during inference:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¿™äº›å‚æ•°åªèƒ½åœ¨å¯¼å‡ºæœŸé—´é…ç½®ã€‚è¿™æ„å‘³ç€åœ¨æ¨ç†æœŸé—´ï¼š
- en: the `batch_size` of the inputs should be equal to the `batch_size` used during
    export,
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥çš„`batch_size`åº”ç­‰äºå¯¼å‡ºæ—¶ä½¿ç”¨çš„`batch_size`ï¼Œ
- en: the `length` of the input sequences should be lower than the `sequence_length`
    used during export,
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥åºåˆ—çš„`length`åº”ä½äºå¯¼å‡ºæ—¶ä½¿ç”¨çš„`sequence_length`ï¼Œ
- en: the maximum number of tokens (input + generated) cannot exceed the `sequence_length`
    used during export.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€å¤§æ ‡è®°æ•°ï¼ˆè¾“å…¥+ç”Ÿæˆï¼‰ä¸èƒ½è¶…è¿‡å¯¼å‡ºæ—¶ä½¿ç”¨çš„`sequence_length`ã€‚
- en: Text generation inference
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç”Ÿæˆæ¨ç†
- en: As with the original transformers models, use `generate()` instead of `forward()`
    to generate text sequences.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŸå§‹transformersæ¨¡å‹ä¸€æ ·ï¼Œä½¿ç”¨`generate()`è€Œä¸æ˜¯`forward()`æ¥ç”Ÿæˆæ–‡æœ¬åºåˆ—ã€‚
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The generation is highly configurable. Please refer to [https://huggingface.co/docs/transformers/generation_strategies](https://huggingface.co/docs/transformers/generation_strategies)
    for details.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ˜¯é«˜åº¦å¯é…ç½®çš„ã€‚è¯·å‚è€ƒ[https://huggingface.co/docs/transformers/generation_strategies](https://huggingface.co/docs/transformers/generation_strategies)è·å–è¯¦ç»†ä¿¡æ¯ã€‚
- en: 'Please be aware that:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼š
- en: for each model architecture, default values are provided for all parameters,
    but values passed to the `generate` method will take precedence,
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæ¨¡å‹æ¶æ„ï¼Œä¸ºæ‰€æœ‰å‚æ•°æä¾›äº†é»˜è®¤å€¼ï¼Œä½†ä¼ é€’ç»™`generate`æ–¹æ³•çš„å€¼å°†ä¼˜å…ˆè€ƒè™‘ï¼Œ
- en: the generation parameters can be stored in a `generation_config.json` file.
    When such a file is present in model directory, it will be parsed to set the default
    parameters (the values passed to the `generate` method still take precedence).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå‚æ•°å¯ä»¥å­˜å‚¨åœ¨`generation_config.json`æ–‡ä»¶ä¸­ã€‚å½“æ¨¡å‹ç›®å½•ä¸­å­˜åœ¨è¿™æ ·ä¸€ä¸ªæ–‡ä»¶æ—¶ï¼Œå°†è§£æå®ƒä»¥è®¾ç½®é»˜è®¤å‚æ•°ï¼ˆä¼ é€’ç»™`generate`æ–¹æ³•çš„å€¼ä»ç„¶ä¼˜å…ˆï¼‰ã€‚
- en: Happy inference with Neuron! ğŸš€
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥æ‚¨åœ¨Neuronä¸­è¿›è¡Œæ„‰å¿«çš„æ¨ç†ï¼ğŸš€
