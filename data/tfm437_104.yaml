- en: Model training anatomy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练解剖学
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy](https://huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy](https://huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: To understand performance optimization techniques that one can apply to improve
    efficiency of model training speed and memory utilization, it’s helpful to get
    familiar with how GPU is utilized during training, and how compute intensity varies
    depending on an operation performed.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解可以应用的性能优化技术，以提高模型训练速度和内存利用效率，有助于熟悉在训练期间GPU的使用情况，以及根据执行的操作而变化的计算强度。
- en: 'Let’s start by exploring a motivating example of GPU utilization and the training
    run of a model. For the demonstration, we’ll need to install a few libraries:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从探索GPU利用率和模型训练运行的一个激励示例开始。为了演示，我们需要安装一些库：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `nvidia-ml-py3` library allows us to monitor the memory usage of the models
    from within Python. You might be familiar with the `nvidia-smi` command in the
    terminal - this library allows to access the same information in Python directly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-ml-py3`库允许我们从Python内部监视模型的内存使用情况。您可能熟悉终端中的`nvidia-smi`命令-这个库允许直接在Python中访问相同的信息。'
- en: 'Then, we create some dummy data: random token IDs between 100 and 30000 and
    binary labels for a classifier. In total, we get 512 sequences each with length
    512 and store them in a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)
    with PyTorch format.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一些虚拟数据：100到30000之间的随机标记ID和用于分类器的二进制标签。总共，我们得到512个长度为512的序列，并将它们存储在PyTorch格式的[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)中。
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To print summary statistics for the GPU utilization and the training run with
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    we define two helper functions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了打印GPU利用率和使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)进行训练的摘要统计信息，我们定义了两个辅助函数：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s verify that we start with a free GPU memory:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证一下我们从空闲GPU内存开始：
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'That looks good: the GPU memory is not occupied as we would expect before we
    load any models. If that’s not the case on your machine make sure to stop all
    processes that are using GPU memory. However, not all free GPU memory can be used
    by the user. When a model is loaded to the GPU the kernels are also loaded, which
    can take up 1-2GB of memory. To see how much it is we load a tiny tensor into
    the GPU which triggers the kernels to be loaded as well.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错：GPU内存没有被占用，正如我们在加载任何模型之前所期望的那样。如果在您的计算机上不是这种情况，请确保停止使用GPU内存的所有进程。然而，并非所有空闲GPU内存都可以被用户使用。当模型加载到GPU时，内核也会被加载，这可能占用1-2GB的内存。为了查看有多少内存被占用，我们将一个微小的张量加载到GPU中，这将触发内核的加载。
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We see that the kernels alone take up 1.3GB of GPU memory. Now let’s see how
    much space the model uses.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到内核单独占用了1.3GB的GPU内存。现在让我们看看模型使用了多少空间。
- en: Load Model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型
- en: First, we load the `bert-large-uncased` model. We load the model weights directly
    to the GPU so that we can check how much space just the weights use.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载`bert-large-uncased`模型。我们直接将模型权重加载到GPU，以便我们可以检查仅权重使用了多少空间。
- en: '[PRE5]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can see that the model weights alone take up 1.3 GB of GPU memory. The exact
    number depends on the specific GPU you are using. Note that on newer GPUs a model
    can sometimes take up more space since the weights are loaded in an optimized
    fashion that speeds up the usage of the model. Now we can also quickly check if
    we get the same result as with `nvidia-smi` CLI:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型权重单独占用了1.3GB的GPU内存。确切的数字取决于您使用的具体GPU。请注意，在较新的GPU上，模型有时可能占用更多空间，因为权重以优化的方式加载，加快了模型的使用速度。现在我们还可以快速检查是否与`nvidia-smi`
    CLI得到相同的结果：
- en: '[PRE6]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get the same number as before and you can also see that we are using a V100
    GPU with 16GB of memory. So now we can start training the model and see how the
    GPU memory consumption changes. First, we set up a few standard training arguments:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了与之前相同的数字，您还可以看到我们正在使用一块具有16GB内存的V100 GPU。现在我们可以开始训练模型并查看GPU内存消耗的变化。首先，我们设置一些标准的训练参数：
- en: '[PRE8]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you plan to run multiple experiments, in order to properly clear the memory
    between experiments, restart the Python kernel between experiments.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划运行多个实验，为了在实验之间正确清除内存，请在实验之间重新启动Python内核。
- en: Memory utilization at vanilla training
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在普通训练中的内存利用率
- en: 'Let’s use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and train the model without using any GPU performance optimization techniques
    and a batch size of 4:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)并在不使用任何GPU性能优化技术的情况下训练模型，批量大小为4：
- en: '[PRE9]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We see that already a relatively small batch size almost fills up our GPU’s
    entire memory. However, a larger batch size can often result in faster model convergence
    or better end performance. So ideally we want to tune the batch size to our model’s
    needs and not to the GPU limitations. What’s interesting is that we use much more
    memory than the size of the model. To understand a bit better why this is the
    case let’s have a look at a model’s operations and memory needs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到即使是一个相对较小的批量大小几乎填满了我们GPU的整个内存。然而，较大的批量大小通常会导致模型更快地收敛或获得更好的最终性能。因此，理想情况下，我们希望根据模型的需求而不是GPU的限制来调整批量大小。有趣的是，我们使用的内存比模型的大小要多得多。为了更好地理解为什么会这样，让我们看一下模型的操作和内存需求。
- en: Anatomy of Model’s Operations
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型操作解剖学
- en: Transformers architecture includes 3 main groups of operations grouped below
    by compute-intensity.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers架构包括3个主要的操作组，根据计算强度分组如下。
- en: '**Tensor Contractions**'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**张量收缩**'
- en: Linear layers and components of Multi-Head Attention all do batched **matrix-matrix
    multiplications**. These operations are the most compute-intensive part of training
    a transformer.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性层和多头注意力组件都进行批量**矩阵-矩阵乘法**。这些操作是训练transformer最计算密集的部分。
- en: '**Statistical Normalizations**'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**统计归一化**'
- en: Softmax and layer normalization are less compute-intensive than tensor contractions,
    and involve one or more **reduction operations**, the result of which is then
    applied via a map.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Softmax和层归一化的计算密度低于张量收缩，并涉及一个或多个**减少操作**，其结果然后通过映射应用。
- en: '**Element-wise Operators**'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逐元素运算符**'
- en: 'These are the remaining operators: **biases, dropout, activations, and residual
    connections**. These are the least compute-intensive operations.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些是剩余的运算符：**偏置、丢弃、激活和残差连接**。这些是计算密集度最低的操作。
- en: This knowledge can be helpful to know when analyzing performance bottlenecks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些知识在分析性能瓶颈时可能会有所帮助。
- en: 'This summary is derived from [Data Movement Is All You Need: A Case Study on
    Optimizing Transformers 2020](https://arxiv.org/abs/2007.00072)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个总结来源于[数据移动就是你需要的一切：优化Transformer的案例研究2020](https://arxiv.org/abs/2007.00072)
- en: Anatomy of Model’s Memory
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型内存的解剖
- en: 'We’ve seen that training the model uses much more memory than just putting
    the model on the GPU. This is because there are many components during training
    that use GPU memory. The components on GPU memory are the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，训练模型使用的内存比仅将模型放在GPU上要多得多。这是因为在训练期间有许多组件使用GPU内存。在GPU内存中的组件包括以下内容：
- en: model weights
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型权重
- en: optimizer states
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器状态
- en: gradients
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度
- en: forward activations saved for gradient computation
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为梯度计算保存的前向激活
- en: temporary buffers
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 临时缓冲区
- en: functionality-specific memory
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 功能特定的内存
- en: A typical model trained in mixed precision with AdamW requires 18 bytes per
    model parameter plus activation memory. For inference there are no optimizer states
    and gradients, so we can subtract those. And thus we end up with 6 bytes per model
    parameter for mixed precision inference, plus activation memory.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AdamW进行混合精度训练的典型模型每个模型参数需要18字节加上激活内存。对于推断，没有优化器状态和梯度，因此我们可以减去这些。因此，对于混合精度推断，每个模型参数需要6字节，加上激活内存。
- en: Let’s look at the details.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看细节。
- en: '**Model Weights:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型权重：**'
- en: 4 bytes * number of parameters for fp32 training
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个参数的4字节，用于fp32训练
- en: 6 bytes * number of parameters for mixed precision training (maintains a model
    in fp32 and one in fp16 in memory)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个参数的6字节，用于混合精度训练（在内存中维护一个fp32模型和一个fp16模型）
- en: '**Optimizer States:**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器状态：**'
- en: 8 bytes * number of parameters for normal AdamW (maintains 2 states)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个参数的8字节，用于普通的AdamW（维护2个状态）
- en: 2 bytes * number of parameters for 8-bit AdamW optimizers like [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个参数的8位AdamW优化器需要2字节，例如[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
- en: 4 bytes * number of parameters for optimizers like SGD with momentum (maintains
    only 1 state)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个参数的4字节，用于像带有动量的SGD这样的优化器（仅维护1个状态）
- en: '**Gradients**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度**'
- en: 4 bytes * number of parameters for either fp32 or mixed precision training (gradients
    are always kept in fp32)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个参数的4字节，用于fp32或混合精度训练（梯度始终保持在fp32中）
- en: '**Forward Activations**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向激活**'
- en: size depends on many factors, the key ones being sequence length, hidden size
    and batch size.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小取决于许多因素，关键因素是序列长度、隐藏大小和批量大小。
- en: There are the input and output that are being passed and returned by the forward
    and the backward functions and the forward activations saved for gradient computation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有通过前向和后向函数传递和返回的输入和输出，以及为梯度计算保存的前向激活。
- en: '**Temporary Memory**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**临时内存**'
- en: Additionally, there are all kinds of temporary variables which get released
    once the calculation is done, but in the moment these could require additional
    memory and could push to OOM. Therefore, when coding it’s crucial to think strategically
    about such temporary variables and sometimes to explicitly free those as soon
    as they are no longer needed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有各种临时变量，一旦计算完成就会释放，但在某些时刻这些变量可能需要额外的内存并可能导致OOM。因此，在编码时，战略性地考虑这些临时变量并有时明确释放那些不再需要的变量是至关重要的。
- en: '**Functionality-specific memory**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**功能特定的内存**'
- en: Then, your software could have special memory needs. For example, when generating
    text using beam search, the software needs to maintain multiple copies of inputs
    and outputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您的软件可能有特殊的内存需求。例如，使用波束搜索生成文本时，软件需要维护多个输入和输出的副本。
- en: '**`forward` vs `backward` Execution Speed**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**`前向`与`后向`执行速度**'
- en: For convolutions and linear layers there are 2x flops in the backward compared
    to the forward, which generally translates into ~2x slower (sometimes more, because
    sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited,
    and it’s typical for an activation to have to read more data in the backward than
    in the forward (e.g. activation forward reads once, writes once, activation backward
    reads twice, gradOutput and output of the forward, and writes once, gradInput).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积和线性层，与前向相比，后向中的flops是前向的2倍，这通常会导致大约2倍的速度变慢（有时更多，因为后向中的大小往往更加尴尬）。激活通常受带宽限制，一个激活在后向中通常需要读取比前向更多的数据（例如，激活前向读取一次，写入一次，激活后向读取两次，gradOutput和前向的输出，然后写入一次，gradInput）。
- en: As you can see, there are potentially a few places where we could save GPU memory
    or speed up operations. Now that you understand what affects GPU utilization and
    computation speed, refer to the [Methods and tools for efficient training on a
    single GPU](perf_train_gpu_one) documentation page to learn about performance
    optimization techniques.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，有一些地方我们可以节省GPU内存或加快操作速度。现在您已经了解了影响GPU利用率和计算速度的因素，请参考[单个GPU上高效训练的方法和工具](perf_train_gpu_one)文档页面，了解性能优化技术。
