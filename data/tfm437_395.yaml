- en: Utilities for Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成工具
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This page lists all the utility functions used by [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search),
    [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search),
    [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample),
    [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search),
    [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample),
    [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search),
    and [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此页面列出了 [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)、[greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)、[contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)、[sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)、[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)、[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)、[group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)
    和 [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)
    使用的所有实用函数。
- en: Most of those are only useful if you are studying the code of the generate methods
    in the library.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，这些只有在研究库中生成方法的代码时才有用。
- en: Generate Outputs
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成输出
- en: The output of [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    is an instance of a subclass of [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput).
    This output is a data structure containing all the information returned by [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    but that can also be used as tuple or dictionary.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    的输出是 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    的子类实例。这个输出是一个数据结构，包含了 [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    返回的所有信息，但也可以用作元组或字典。'
- en: 'Here’s an example:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `generation_output` object is a [GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput),
    as we can see in the documentation of that class below, it means it has the following
    attributes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`generation_output` 对象是一个 [GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)，正如我们在下面该类的文档中所看到的，它具有以下属性：'
- en: '`sequences`: the generated sequences of tokens'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`：生成的 token 序列'
- en: '`scores` (optional): the prediction scores of the language modelling head,
    for each generation step'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（可选）：语言建模头的预测分数，每个生成步骤'
- en: '`hidden_states` (optional): the hidden states of the model, for each generation
    step'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（可选）：模型的隐藏状态，每个生成步骤'
- en: '`attentions` (optional): the attention weights of the model, for each generation
    step'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（可选）：模型的注意力权重，每个生成步骤'
- en: Here we have the `scores` since we passed along `output_scores=True`, but we
    don’t have `hidden_states` and `attentions` because we didn’t pass `output_hidden_states=True`
    or `output_attentions=True`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有 `scores`，因为我们传递了 `output_scores=True`，但我们没有 `hidden_states` 和 `attentions`，因为我们没有传递
    `output_hidden_states=True` 或 `output_attentions=True`。
- en: You can access each attribute as you would usually do, and if that attribute
    has not been returned by the model, you will get `None`. Here for instance `generation_output.scores`
    are all the generated prediction scores of the language modeling head, and `generation_output.attentions`
    is `None`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像通常那样访问每个属性，如果该属性未被模型返回，您将得到 `None`。在这里，例如 `generation_output.scores` 是语言建模头生成的所有预测分数，而
    `generation_output.attentions` 是 `None`。
- en: When using our `generation_output` object as a tuple, it only keeps the attributes
    that don’t have `None` values. Here, for instance, it has two elements, `loss`
    then `logits`, so
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当将我们的 `generation_output` 对象用作元组时，它只保留那些没有 `None` 值的属性。在这里，例如，它有两个元素，`loss`
    然后 `logits`，所以
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: will return the tuple `(generation_output.sequences, generation_output.scores)`
    for instance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将返回元组 `(generation_output.sequences, generation_output.scores)`。
- en: When using our `generation_output` object as a dictionary, it only keeps the
    attributes that don’t have `None` values. Here, for instance, it has two keys
    that are `sequences` and `scores`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当将我们的 `generation_output` 对象用作字典时，它只保留那些没有 `None` 值的属性。在这里，例如，它有两个键，分别是 `sequences`
    和 `scores`。
- en: We document here all output types.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里记录所有输出类型。
- en: PyTorch
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: '### `class transformers.generation.GenerateDecoderOnlyOutput`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.GenerateDecoderOnlyOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L96)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L96)'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`） — 生成的序列。第二维（sequence_length）要么等于
    `max_length`，要么如果所有批次由于 `eos_token_id` 提前结束，则要短一些。'
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（`tuple(torch.FloatTensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    在每个生成步骤中语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。元组`torch.FloatTensor`，最多包含`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size,
    config.vocab_size)`。'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的标记一个元素）的元组（解码器的每一层一个元素）的`torch.FloatTensor`，形状为`(batch_size, num_heads,
    generated_length, sequence_length)`。'
- en: '`hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    元组（每个生成的标记一个元素）的元组（解码器的每一层一个元素）的`torch.FloatTensor`，形状为`(batch_size, generated_length,
    hidden_size)`。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）—
    注意：一些模型具有不同的`past_key_values`格式，请查阅模型的文档进行确认。通常是一个元组（解码器的每一层一个元素）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size,
    num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Outputs of decoder-only generation models, when using non-beam methods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器生成模型的输出，在使用非beam方法时。
- en: '### `class transformers.generation.GenerateEncoderDecoderOutput`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.GenerateEncoderDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L131)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L131)'
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。'
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（`tuple(torch.FloatTensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    在每个生成步骤中语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。元组`torch.FloatTensor`，最多包含`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size,
    config.vocab_size)`。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组的`torch.FloatTensor`（解码器的每一层一个）的形状为`(batch_size, num_heads, sequence_length,
    sequence_length)`。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    元组的`torch.FloatTensor`（嵌入的输出一个加上每一层的输出一个）的形状为`(batch_size, sequence_length, hidden_size)`。'
- en: '`decoder_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的标记一个元素）的元组（解码器的每一层一个元素）的`torch.FloatTensor`，形状为`(batch_size, num_heads,
    generated_length, sequence_length)`。'
- en: '`cross_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回
    — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`decoder_hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回
    — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`的元组，形状为`(batch_size, generated_length,
    hidden_size)`。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回
    — 注意：一些模型具有不同的`past_key_values`格式，请查阅模型文档确认。通常是一个元组（解码器每一层一个元素）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size,
    num_heads, sequence_length, embed_size_per_head)`的张量，并且如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Outputs of encoder-decider generation models, when using non-beam methods.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用非beam方法时的编码器-解码器生成模型的输出。
- en: '### `class transformers.generation.GenerateBeamDecoderOnlyOutput`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.GenerateBeamDecoderOnlyOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L178)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L178)'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`) — The generated sequences. The second dimension (sequence_length)
    is either equal to `max_length` or shorter if all batches finished early due to
    the `eos_token_id`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`torch.LongTensor`，形状为`(batch_size*num_return_sequences, sequence_length)`)
    — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前结束，则较短。'
- en: '`sequences_scores` (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences_scores` (`torch.FloatTensor`，形状为`(batch_size*num_return_sequences)`)，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回
    — 生成`sequences`的最终beam分数。'
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam transition scores for each
    vocabulary token at each generation step. Beam transition scores consisting of
    log probabilities of tokens conditioned on log softmax of previously generated
    tokens in this beam. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回
    — 每一代步每个词汇令牌的beam转移分数。beam转移分数由tokens的对数概率条件于该beam中先前生成的tokens的对数softmax组成。具有最多`max_new_tokens`元素的`torch.FloatTensor`元组（每个生成的令牌一个元素），每个张量的形状为`(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`。'
- en: '`beam_indices` (`torch.LongTensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices` (`torch.LongTensor`)，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回
    — 每一代步生成的令牌id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`torch.LongTensor`。'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads,
    generated_length, sequence_length)`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回
    — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`的元组，形状为`(batch_size*num_beams, num_heads,
    generated_length, sequence_length)`。'
- en: '`hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回
    — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`，形状为`(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor)))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）—
    注意：一些模型具有不同的`past_key_values`格式，请查阅模型的文档。通常是一个元组（解码器每一层一个）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size,
    num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Outputs of decoder-only generation models, when using beam methods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用beam方法时，仅解码器生成模型的输出。
- en: '### `class transformers.generation.GenerateBeamEncoderDecoderOutput`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.GenerateBeamEncoderDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L221)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L221)'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`) — The generated sequences. The second dimension (sequence_length)
    is either equal to `max_length` or shorter if all batches finished early due to
    the `eos_token_id`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（`torch.LongTensor`，形状为`(batch_size*num_return_sequences, sequence_length)`）—
    生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前结束，则较短。'
- en: '`sequences_scores` (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences_scores`（`torch.FloatTensor`，形状为`(batch_size*num_return_sequences)`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    生成的`sequences`的最终beam分数。'
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam transition scores for each
    vocabulary token at each generation step. Beam transition scores consisting of
    log probabilities of tokens conditioned on log softmax of previously generated
    tokens in this beam. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size*num_beams,
    config.vocab_size)`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    每一代步骤中每个词汇标记的beam转移分数。Beam转移分数由tokens的log概率组成，条件是该beam中先前生成的tokens的log softmax。`torch.FloatTensor`的元组，最多有`max_new_tokens`个元素（每个生成的token一个元素），每个张量的形状为`(batch_size*num_beams,
    config.vocab_size)`。'
- en: '`beam_indices` (`torch.LongTensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices`（`torch.LongTensor`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    每一代步骤中生成的token id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`torch.LongTensor`。'
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（解码器每一层一个）。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size*num_beams*num_return_sequences,
    sequence_length, hidden_size)`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: '`decoder_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences,
    num_heads, generated_length, sequence_length)`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每一层一个）的`torch.FloatTensor`，形状为`(batch_size*num_beams*num_return_sequences,
    num_heads, generated_length, sequence_length)`。'
- en: '`cross_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每一层一个）的`torch.FloatTensor`，形状为`(batch_size, num_heads,
    generated_length, sequence_length)`。'
- en: '`decoder_hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每一层一个）的`torch.FloatTensor`，形状为`(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 注意：一些模型具有不同的`past_key_values`格式，请查阅模型文档。通常是一个元组（解码器每层一个元素）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size,
    num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Outputs of encoder-decoder generation models, when using beam methods.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用beam方法时，编码器-解码器生成模型的输出。
- en: TensorFlow
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow
- en: '### `class transformers.generation.TFGreedySearchEncoderDecoderOutput`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFGreedySearchEncoderDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L85)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L85)'
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短一些。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）在每个生成步骤。`tf.Tensor`的元组，最多有`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size,
    config.vocab_size)`。'
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, generated_length,
    hidden_size)`。'
- en: Base class for outputs of encoder-decoder generation models using greedy search.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贪婪搜索的编码器-解码器生成模型的输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（或`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。
- en: '### `class transformers.generation.TFGreedySearchDecoderOnlyOutput`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFGreedySearchDecoderOnlyOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L57)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L57)'
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短一些。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤的语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size,
    config.vocab_size)`。'
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size, generated_length,
    hidden_size)`。'
- en: Base class for outputs of decoder-only generation models using greedy search.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器生成模型使用贪婪搜索的输出的基类。
- en: '### `class transformers.generation.TFSampleEncoderDecoderOutput`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFSampleEncoderDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L155)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L155)'
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`)
    — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_return_sequences,
    config.vocab_size)`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤的语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_return_sequences,
    config.vocab_size)`。'
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size*num_return_sequences, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size*num_return_sequences, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个解码器层一个）。'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size*num_return_sequences, sequence_length, hidden_size)`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size*num_return_sequences, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size*num_return_sequences, num_heads,
    generated_length, sequence_length)`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size*num_return_sequences,
    num_heads, generated_length, sequence_length)`。'
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size*num_return_sequences,
    generated_length, hidden_size)`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size*num_return_sequences,
    generated_length, hidden_size)`。'
- en: Base class for outputs of encoder-decoder generation models using sampling.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用采样的编码器-解码器生成模型的输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（或`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。
- en: '### `class transformers.generation.TFSampleDecoderOnlyOutput`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFSampleDecoderOnlyOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L127)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L127)'
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`)
    — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_return_sequences,
    config.vocab_size)`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)`，*optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 语言建模头的预测分数（SoftMax之前的每个词汇标记的分数）在每个生成步骤。`tf.Tensor`的元组，最多有`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_return_sequences,
    config.vocab_size)`。'
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(num_return_sequences*batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(num_return_sequences*batch_size,
    num_heads, generated_length, sequence_length)`。'
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(num_return_sequences*batch_size, generated_length, hidden_size)`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(num_return_sequences*batch_size,
    generated_length, hidden_size)`。'
- en: Base class for outputs of decoder-only generation models using sampling.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 用于仅使用采样的解码器生成模型的输出的基类。
- en: '### `class transformers.generation.TFBeamSearchEncoderDecoderOutput`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFBeamSearchEncoderDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L232)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L232)'
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`)
    — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。'
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences_scores` (`tf.Tensor`，形状为`(batch_size*num_return_sequences)`，*optional*,
    当传递`output_scores=True`或`config.output_scores=True`时返回) — 生成的`sequences`的最终beam分数。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. `Tuple of` tf.Tensor`with up to`max_new_tokens`elements (one
    element for each generated token), with each tensor of shape`(batch_size*num_beams,
    config.vocab_size)`.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)`，*optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤中每个词汇标记的处理过的beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成标记的log softmax之和组成。`tf.Tensor`的元组，最多有`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_beams,
    config.vocab_size)`。'
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices` (`tf.Tensor`，*optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤中生成的标记id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。'
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（解码器每一层一个）。'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences,
    num_heads, generated_length, sequence_length)`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams*num_return_sequences,
    num_heads, generated_length, sequence_length)`。'
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`。'
- en: Base class for outputs of encoder-decoder generation models using beam search.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基类，用于使用beam搜索的编码器-解码器生成模型的输出。可以通过`encoder_attentions`和`encoder_hidden_states`属性（分别是`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。
- en: '### `class transformers.generation.TFBeamSearchDecoderOnlyOutput`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFBeamSearchDecoderOnlyOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L197)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L197)'
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`）—
    生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。'
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences_scores`（形状为`(batch_size*num_return_sequences)`的`tf.Tensor`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    生成的`sequences`的最终beam分数。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（`tuple(tf.Tensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    每个生成步骤中每个词汇标记的处理过的beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成的标记的log softmax之和组成。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的token一个元素），每个张量的形状为`(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`。'
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices`（`tf.Tensor`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    每个生成步骤中生成的token id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。'
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams, num_heads,
    generated_length, sequence_length)`。'
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length,
    hidden_size)`.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`。'
- en: Base class for outputs of decoder-only generation models using beam search.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用beam搜索的解码器生成模型的输出的基类。
- en: '### `class transformers.generation.TFBeamSampleEncoderDecoderOutput`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFBeamSampleEncoderDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L317)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L317)'
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_beams, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size*num_beams, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。'
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size * num_return_sequence)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences_scores` (`tf.Tensor`，形状为`(batch_size * num_return_sequence)`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 生成的`sequences`的最终beam分数。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_beams,
    config.vocab_size)`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤中每个词汇标记的处理beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成标记的log softmax之和组成。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量形状为`(batch_size*num_beams,
    config.vocab_size)`。'
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices` (`tf.Tensor`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤中生成的标记id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。'
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个解码器层一个）。'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size*num_beams, sequence_length, hidden_size)`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size*num_beams, sequence_length, hidden_size)`的`tf.Tensor`元组（嵌入输出和每个层输出各一个）。'
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size*num_beams, num_heads, generated_length,
    sequence_length)`.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（每个解码器层一个）的`tf.Tensor`，形状为`(batch_size*num_beams, num_heads,
    generated_length, sequence_length)`。'
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(tuple(tf.Tensor))`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（每个解码器层一个）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size*num_beams, generated_length,
    hidden_size)`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（每个解码器层一个）的`tf.Tensor`，形状为`(batch_size*num_beams, generated_length,
    hidden_size)`。'
- en: Base class for outputs of encoder-decoder generation models using beam sampling.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用beam采样的编码器-解码器生成模型输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（分别为`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。
- en: '### `class transformers.generation.TFBeamSampleDecoderOnlyOutput`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFBeamSampleDecoderOnlyOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L282)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L282)'
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`)
    — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。'
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size * num_return_sequence)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences_scores` (`tf.Tensor`，形状为`(batch_size * num_return_sequence)`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 生成的`sequences`的最终beam分数。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤中每个词汇标记的处理beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成的标记的log softmax之和组成。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`。'
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices` (`tf.Tensor`, *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 每个生成步骤中生成的标记id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。'
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams, num_heads,
    generated_length, sequence_length)`。'
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size*num_beams, generated_length, hidden_size)`.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams, generated_length,
    hidden_size)`。'
- en: Base class for outputs of decoder-only generation models using beam sample.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用beam sample的仅解码器生成模型的输出的基类。
- en: '### `class transformers.generation.TFContrastiveSearchEncoderDecoderOutput`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFContrastiveSearchEncoderDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L393)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L393)'
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。 '
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回)
    — 在每个生成步骤中语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size,
    config.vocab_size)`。'
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length,
    sequence_length)`。'
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, generated_length,
    hidden_size)`。'
- en: Base class for outputs of encoder-decoder generation models using contrastive
    search. Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用对比搜索的编码器-解码器生成模型的输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（分别是`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。
- en: '### `class transformers.generation.TFContrastiveSearchDecoderOnlyOutput`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.TFContrastiveSearchDecoderOnlyOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L366)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L366)'
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前结束，则要短。'
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（`tuple(tf.Tensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）—
    语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）在每个生成步骤。每个生成标记一个元素，每个张量形状为`(batch_size, config.vocab_size)`的`tf.Tensor`的元组，最多有`max_new_tokens`个元素。'
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的形状为`(batch_size, num_heads, generated_length, sequence_length)`的`tf.Tensor`。'
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的形状为`(batch_size, generated_length, hidden_size)`的`tf.Tensor`。'
- en: Base class for outputs of decoder-only generation models using contrastive search.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 用于仅使用对比搜索生成模型的输出的基类。
- en: FLAX
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FLAX
- en: '### `class transformers.generation.FlaxSampleOutput`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.FlaxSampleOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L68)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L68)'
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`jnp.ndarray` of shape `(batch_size, max_length)`) — The generated
    sequences.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（形状为`(batch_size, max_length)`的`jnp.ndarray`）— 生成的序列。'
- en: Flax Base class for outputs of decoder-only generation models using sampling.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Flax基类，用于仅使用抽样生成模型的输出。
- en: '#### `replace`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `替换`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: “Returns a new object replacing the specified fields with new values.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: “返回一个用新值替换指定字段的新对象。
- en: '### `class transformers.generation.FlaxGreedySearchOutput`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.FlaxGreedySearchOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L54)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L54)'
- en: '[PRE18]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`jnp.ndarray` of shape `(batch_size, max_length)`) — The generated
    sequences.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（形状为`(batch_size, max_length)`的`jnp.ndarray`）— 生成的序列。'
- en: Flax Base class for outputs of decoder-only generation models using greedy search.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Flax基类，用于仅使用贪婪搜索生成模型的输出。
- en: '#### `replace`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `替换`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: “Returns a new object replacing the specified fields with new values.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: “返回一个用新值替换指定字段的新对象。
- en: '### `class transformers.generation.FlaxBeamSearchOutput`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.generation.FlaxBeamSearchOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L82)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L82)'
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`jnp.ndarray` of shape `(batch_size, max_length)`) — The generated
    sequences.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（形状为`(batch_size, max_length)`的`jnp.ndarray`）— 生成的序列。'
- en: '`scores` (`jnp.ndarray` of shape `(batch_size,)`) — The scores (log probabilities)
    of the generated sequences.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（形状为`(batch_size,)`的`jnp.ndarray`）— 生成序列的分数（对数概率）。'
- en: Flax Base class for outputs of decoder-only generation models using greedy search.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Flax基类，用于仅使用贪婪搜索生成模型的输出。
- en: '#### `replace`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `替换`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
- en: '[PRE21]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: “Returns a new object replacing the specified fields with new values.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: “返回一个用新值替换指定字段的新对象。
- en: LogitsProcessor
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LogitsProcessor
- en: A [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    can be used to modify the prediction scores of a language model head for generation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)可以用于修改语言模型头的预测分数以进行生成。'
- en: PyTorch
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: '### `class transformers.AlternatingCodebooksLogitsProcessor`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AlternatingCodebooksLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2011)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2011)'
- en: '[PRE22]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_start_len` (`int`) — The length of the initial input sequence.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_start_len`（`int`）— 初始输入序列的长度。'
- en: '`semantic_vocab_size` (`int`) — Vocabulary size of the semantic part, i.e number
    of tokens associated to the semantic vocabulary.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_vocab_size`（`int`）— 语义部分的词汇表大小，即与语义词汇表相关联的标记数。'
- en: '`codebook_size` (`int`) — Number of tokens associated to the codebook.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codebook_size`（`int`）— 与代码簿相关联的标记数。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforcing alternated generation between the two codebooks of Bark.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    强制在Bark的两个代码簿之间交替生成。'
- en: This logits processor is exclusively compatible with [Bark](https://huggingface.co/docs/transformers/en/model_doc/bark)’s
    fine submodel. See the model documentation for examples.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 此对数处理器仅与[Bark](https://huggingface.co/docs/transformers/en/model_doc/bark)的精细子模型兼容。请参阅模型文档以获取示例。
- en: '#### `__call__`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2040)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2040)'
- en: '[PRE23]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '### `class transformers.ClassifierFreeGuidanceLogitsProcessor`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClassifierFreeGuidanceLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1947)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1947)'
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`guidance_scale` (float) — The guidance scale for classifier free guidance
    (CFG). CFG is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages
    the model to generate samples that are more closely linked to the input prompt,
    usually at the expense of poorer quality.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`（浮点数）— 用于分类器自由引导（CFG）的引导比例。通过设置`guidance_scale > 1`来启用CFG。更高的引导比例鼓励模型生成与输入提示更紧密相关的样本，通常以牺牲质量为代价。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    for classifier free guidance (CFG). The scores are split over the batch dimension,
    where the first half correspond to the conditional logits (predicted from the
    input prompt) and the second half correspond to the unconditional logits (predicted
    from an empty or ‘null’ prompt). The processor computes a weighted average across
    the conditional and unconditional logits, parameterised by the `guidance_scale`.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    用于分类器自由引导（CFG）。分数在批处理维度上分割，其中前半部分对应条件对数（从输入提示预测），后半部分对应无条件对数（从空或“null”提示预测）。处理器计算条件和无条件对数之间的加权平均值，由`guidance_scale`参数化。'
- en: See [the paper](https://arxiv.org/abs/2306.05284) for more information.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[论文](https://arxiv.org/abs/2306.05284)。
- en: This logits processor is exclusively compatible with [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 此对数处理器仅与[MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)兼容
- en: 'Examples:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#### `__call__`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1995)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1995)'
- en: '[PRE26]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax'
- en: Returns
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`的形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.EncoderNoRepeatNGramLogitsProcessor`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.EncoderNoRepeatNGramLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L874)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L874)'
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_ngram_size` (`int`) — All ngrams of size `ngram_size` can only occur
    within the encoder input ids.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ngram_size`（`int`）— 所有大小为`ngram_size`的ngram只能出现在编码器输入ID中。'
- en: '`encoder_input_ids` (`int`) — The encoder_input_ids that should not be repeated
    within the decoder ids.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_input_ids`（`int`）— 不应在解码器ID中重复的编码器输入ID。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that works similarly to [NoRepeatNGramLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoRepeatNGramLogitsProcessor),
    but applied exclusively to prevent the repetition of n-grams present in the prompt.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)，其工作方式类似于[NoRepeatNGramLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoRepeatNGramLogitsProcessor)，但专门用于防止在提示中重复出现的n-gram。'
- en: It was designed to promote chattiness in a language model, by preventing the
    generation of n-grams present in previous conversation rounds.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在通过阻止生成先前对话轮中存在的n-gram来促进语言模型中的喋喋不休。
- en: 'Examples:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE28]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#### `__call__`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L923)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L923)'
- en: '[PRE29]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入标识？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax'
- en: Returns
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.EncoderRepetitionPenaltyLogitsProcessor`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.EncoderRepetitionPenaltyLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L342)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L342)'
- en: '[PRE30]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`penalty` (`float`) — The parameter for repetition penalty. 1.0 means no penalty.
    Above 1.0 rewards prompt tokens. Between 0.0 and 1.0 penalizes prompt tokens.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`penalty` (`float`) — 重复惩罚的参数。1.0 表示没有惩罚。大于 1.0 奖励提示标记。在 0.0 和 1.0 之间惩罚提示标记。'
- en: '`encoder_input_ids` (`torch.LongTensor`) — The encoder_input_ids that should
    be repeated within the decoder ids.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_input_ids` (`torch.LongTensor`) — 应在解码器标识中重复的编码器输入标识。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that works similarly to [RepetitionPenaltyLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.RepetitionPenaltyLogitsProcessor),
    but with an *inverse* penalty that is applied to the tokens present in the prompt.
    In other words, a penalty above 1.0 increases the odds of selecting tokens that
    were present in the prompt.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    与 [RepetitionPenaltyLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.RepetitionPenaltyLogitsProcessor)
    类似，但应用于提示中存在的标记的*反向*惩罚。换句话说，大于 1.0 的惩罚增加了选择提示中存在的标记的几率。'
- en: It was designed to avoid hallucination in input-grounded tasks, like summarization.
    Although originally intended for encoder-decoder models, it can also be used with
    decoder-only models like LLMs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在避免输入驱动任务中的幻觉，如摘要。虽然最初是为编码器-解码器模型设计的，但也可以与仅解码器模型（如LLMs）一起使用。
- en: 'Examples:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE31]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#### `__call__`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L386)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L386)'
- en: '[PRE32]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入标识？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax'
- en: Returns
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.EpsilonLogitsWarper`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.EpsilonLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L603)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L603)'
- en: '[PRE33]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`epsilon` (`float`) — If set to > 0, only the most tokens with probabilities
    `epsilon` or higher are kept for generation.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon` (`float`) — 如果设置为 > 0，则只保留概率大于 `epsilon` 的最多的标记用于生成。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`，*可选*，默认为 -inf) — 所有过滤值将设置为此浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`，*可选*，默认为 1) — 不能被过滤的最小标记数。'
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs epsilon-sampling, i.e. restricting to tokens with `prob >= epsilon`.
    Takes the largest min_tokens_to_keep tokens if no tokens satisfy this constraint.
    See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191)
    for more information.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    执行 epsilon-sampling，即限制到概率 `prob >= epsilon` 的标记。如果没有标记满足此约束，则取最大的 min_tokens_to_keep
    个标记。有关更多信息，请参阅[截断抽样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。'
- en: 'Examples:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE34]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '#### `__call__`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L656)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L656)'
- en: '[PRE35]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入标识？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax'
- en: Returns
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.EtaLogitsWarper`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.EtaLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L670)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L670)'
- en: '[PRE36]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`epsilon` (`float`) — A float value in the range (0, 1). Hyperparameter used
    to calculate the dynamic cutoff value, `eta`. The suggested values from the paper
    ranges from 3e-4 to 4e-3 depending on the size of the model.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon` (`float`) — 在范围(0, 1)内的浮点值。用于计算动态截断值`eta`的超参数。根据论文，建议的值范围为3e-4到4e-3，具体取决于模型的大小。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All values that are
    found to be below the dynamic cutoff value, `eta`, are set to this float value.
    This parameter is useful when logits need to be modified for very low probability
    tokens that should be excluded from generation entirely.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`，*可选*，默认为-inf) — 所有低于动态截断值`eta`的值都设置为此浮点值。当需要修改logits以排除生成过程中应完全排除的概率非常低的标记时，此参数很有用。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Specifies the minimum
    number of tokens that must be kept for generation, regardless of their probabilities.
    For example, if `min_tokens_to_keep` is set to 1, at least one token will always
    be kept for generation, even if all tokens have probabilities below the cutoff
    `eta`.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`，*可选*，默认为1) — 指定必须保留的最小标记数，无论它们的概率如何。例如，如果将`min_tokens_to_keep`设置为1，则始终会保留至少一个标记用于生成，即使所有标记的概率都低于截断`eta`。'
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs eta-sampling, a technique to filter out tokens with probabilities
    below a dynamic cutoff value, `eta`, which is calculated based on a combination
    of the hyperparameter `epsilon` and the entropy of the token probabilities, i.e.
    `eta := min(epsilon, sqrt(epsilon * e^-entropy(probabilities)))`. Takes the largest
    min_tokens_to_keep tokens if no tokens satisfy this constraint. It addresses the
    issue of poor quality in long samples of text generated by neural language models
    leading to more coherent and fluent text. See [Truncation Sampling as Language
    Model Desmoothing](https://arxiv.org/abs/2210.15191) for more information. Note:
    `do_sample` must be set to `True` for this `LogitsWarper` to work.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)执行eta采样，一种过滤掉概率低于动态截断值`eta`的标记的技术，该值是基于超参数`epsilon`和标记概率的熵的组合计算得出的，即`eta
    := min(epsilon, sqrt(epsilon * e^-entropy(probabilities)))`。如果没有标记满足此约束，则保留最大的`min_tokens_to_keep`个标记。它解决了由神经语言模型生成的长文本样本中存在的质量差问题，从而生成更连贯和流畅的文本。有关更多信息，请参阅[截断采样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。注意：必须将`do_sample`设置为`True`，才能使此`LogitsWarper`正常工作。'
- en: 'Examples:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE37]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#### `__call__`'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L733)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L733)'
- en: '[PRE38]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax'
- en: Returns
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`
- en: The processed prediction scores.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.ExponentialDecayLengthPenalty`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ExponentialDecayLengthPenalty`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1494)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1494)'
- en: '[PRE39]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`exponential_decay_length_penalty` (`tuple(int, float)`) — This tuple shall
    consist of: `(start_index, decay_factor)` where `start_index` indicates where
    penalty starts and `decay_factor` represents the factor of exponential decay'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exponential_decay_length_penalty` (`tuple(int, float)`) — 此元组应包含：`(start_index,
    decay_factor)`，其中`start_index`表示惩罚开始的位置，`decay_factor`表示指数衰减的因子'
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`) — *序列结束*标记的ID。可选择使用列表设置多个*序列结束*标记。'
- en: '`input_ids_seq_length` (`int`) — The length of the input sequence.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids_seq_length` (`int`) — 输入序列的长度。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that exponentially increases the score of the `eos_token_id` after `start_index`
    has been reached. This allows generating shorter sequences without having a hard
    cutoff, allowing the `eos_token` to be predicted in a meaningful position.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)在达到`start_index`后指数增加`eos_token_id`的分数。这允许生成较短的序列而不会有硬性截断，从而使`eos_token`能够在有意义的位置被预测。'
- en: 'Examples:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE40]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '#### `__call__`'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1574)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1574)'
- en: '[PRE41]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.ForcedBOSTokenLogitsProcessor`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ForcedBOSTokenLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1378)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1378)'
- en: '[PRE42]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`bos_token_id` (`int`) — The id of the token to force as the first generated
    token.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`) — 强制作为第一个生成的标记的标记ID。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces the specified token as the first generated token. Used with encoder-decoder
    models.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    会强制指定的标记作为第一个生成的标记。与编码器-解码器模型一起使用。'
- en: 'Examples:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE43]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#### `__call__`'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1413)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1413)'
- en: '[PRE44]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.ForcedEOSTokenLogitsProcessor`'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ForcedEOSTokenLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1423)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1423)'
- en: '[PRE45]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`max_length` (`int`) — The maximum length of the sequence to be generated.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`) — 要生成的序列的最大长度。'
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the token to force as
    the last generated token when `max_length` is reached. Optionally, use a list
    to set multiple *end-of-sequence* tokens.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`) — 当达到`max_length`时，强制指定的标记作为最后生成的标记。可选择使用列表设置多个*序列结束*标记。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces the specified token as the last generated token when `max_length`
    is reached.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    会在达到`max_length`时强制指定的标记作为最后生成的标记。'
- en: 'Examples:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE46]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '#### `__call__`'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1462)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1462)'
- en: '[PRE47]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.ForceTokensLogitsProcessor`'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ForceTokensLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1710)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1710)'
- en: '[PRE48]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This processor takes a list of pairs of integers which indicates a mapping from
    generation indices to token indices that will be forced before generation. The
    processor will set their log probs to `inf` so that they are sampled at their
    corresponding index. Originally created for [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这个处理器接受一对整数的列表，指示从生成索引到强制生成之前的标记索引的映射。处理器将它们的log概率设置为`inf`，以便在相应的索引处对它们进行采样。最初为[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)创建。
- en: 'Examples:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE49]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#### `__call__`'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1751)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1751)'
- en: '[PRE50]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为 `(batch_size, config.vocab_size)` 的 `torch.FloatTensor`
- en: The processed prediction scores.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.HammingDiversityLogitsProcessor`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.HammingDiversityLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1245)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1245)'
- en: '[PRE51]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Parameters
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`diversity_penalty` (`float`) — This value is subtracted from a beam’s score
    if it generates a token same as any beam from other group at a particular time.
    A higher `diversity_penalty` will enforce greater diversity among the beams. Adjusting
    this value can help strike a balance between diversity and natural likelihood.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_penalty` (`float`) — 如果一个 beam 在特定时间生成与其他组中的任何 beam 相同的标记，则从该 beam
    的分数中减去此值。较高的 `diversity_penalty` 将强制在 beam 之间实现更大的多样性。调整此值可以帮助在多样性和自然可能性之间取得平衡。'
- en: '`num_beams` (`int`) — Number of beams for beam search. 1 means no beam search.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`) — beam 搜索的数量。1 表示没有 beam 搜索。'
- en: '`num_beam_groups` (`int`) — Number of groups to divide `num_beams` into in
    order to ensure diversity among different groups of beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf)
    for more details.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beam_groups` (`int`) — 将 `num_beams` 分成多少组，以确保不同组的 beam 之间的多样性。[此论文](https://arxiv.org/pdf/1610.02424.pdf)
    了解更多细节。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces diverse beam search.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    用于强制进行多样性 beam 搜索。'
- en: 'Note that this logits processor is only effective for [PreTrainedModel.group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search).
    See [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf)
    for more details.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，此 logits 处理器仅对 [PreTrainedModel.group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)
    有效。有关更多细节，请参阅 [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence
    Models](https://arxiv.org/pdf/1610.02424.pdf)。'
- en: Traditional beam search often generates very similar sequences across different
    beams. `HammingDiversityLogitsProcessor` addresses this by penalizing beams that
    generate tokens already chosen by other beams in the same time step.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 beam 搜索经常在不同 beam 之间生成非常相似的序列。`HammingDiversityLogitsProcessor` 通过惩罚在同一时间步生成已被其他
    beam 选择的标记的 beam 来解决这个问题。
- en: 'Examples:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE52]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '#### `__call__`'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1332)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1332)'
- en: '[PRE53]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Parameters
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入
    ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用
    beam 搜索时，这些可以是每个词汇表的 logits，或者在使用 beam 搜索时，可以是每个词汇表标记的 log softmax'
- en: '`current_tokens` (`torch.LongTensor` of shape `(batch_size)`) — Indices of
    input sequence tokens in the vocabulary, corresponding to the tokens selected
    by the other beam groups in the current generation step.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`current_tokens` (`torch.LongTensor`，形状为 `(batch_size)`) — 输入序列标记在词汇表中的索引，对应于当前生成步骤中其他
    beam 组选择的标记。'
- en: '`beam_group_idx` (`int`) — The index of the beam group currently being processed.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_group_idx` (`int`) — 当前正在处理的 beam 组的索引。'
- en: Returns
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为 `(batch_size, config.vocab_size)` 的 `torch.FloatTensor`
- en: The processed prediction scores.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.InfNanRemoveLogitsProcessor`'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InfNanRemoveLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1473)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1473)'
- en: '[PRE54]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that removes all `nan` and `inf` values to avoid the generation method to fail.
    Note that using the logits processor should only be used if necessary since it
    can slow down the generation method.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    用于移除所有的 `nan` 和 `inf` 值，以避免生成方法失败。请注意，只有在必要时才应该使用 logits 处理器，因为它可能会减慢生成方法的速度。'
- en: This logits processor has no `generate` example, as there shouldn’t be a correct
    combination of flags that warrants its use.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 logits 处理器没有 `generate` 示例，因为不应该有正确的标志组合来保证其使用。
- en: '#### `__call__`'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1482)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1482)'
- en: '[PRE55]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Parameters
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入
    ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用
    beam 搜索时，这些可以是每个词汇表的 logits，或者在使用 beam 搜索时，可以是每个词汇表标记的 log softmax'
- en: Returns
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为 `(batch_size, config.vocab_size)` 的 `torch.FloatTensor`
- en: The processed prediction scores.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.LogitNormalization`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LogitNormalization`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1585)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1585)'
- en: '[PRE56]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    and [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    for normalizing the scores using log-softmax. It’s important to normalize the
    scores during beam search, after applying the logits processors or warpers, since
    the search algorithm used in this library doesn’t do it (it only does it before,
    but they may need re-normalization) but it still supposes that the scores are
    normalized when comparing the hypotheses.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)和[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)用于使用log-softmax对分数进行归一化。在应用logits处理器或warper后，在波束搜索期间对分数进行归一化是很重要的，因为此库中使用的搜索算法不会这样做（它只在之前这样做，但它们可能需要重新归一化），但它仍然假设在比较假设时分数已经归一化。'
- en: 'Examples:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE57]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `__call__`'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1616)'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1616)'
- en: '[PRE58]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.LogitsProcessor`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L44)'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L44)'
- en: '[PRE59]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Abstract base class for all logit processors that can be applied during generation.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在生成过程中可以应用的logit处理器的抽象基类。
- en: '#### `__call__`'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L47)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L47)'
- en: '[PRE60]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.LogitsProcessorList`'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LogitsProcessorList`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L64)'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L64)'
- en: '[PRE61]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This class can be used to create a list of [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    or [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    to subsequently process a `scores` input tensor. This class inherits from list
    and adds a specific ***call*** method to apply each [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    or [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    to the inputs.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可以用来创建一个[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)或[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)列表，以后处理`scores`输入张量。这个类继承自列表，并添加了一个特定的***call***方法来应用每个[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)或[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)到输入中。
- en: '#### `__call__`'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L71)'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L71)'
- en: '[PRE62]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional kwargs that are specific
    to a logits processor.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`，*可选*) — 特定于logits处理器的其他kwargs。'
- en: Returns
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.LogitsWarper`'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L54)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L54)'
- en: '[PRE63]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Abstract base class for all logit warpers that can be applied during generation
    with multinomial sampling.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可以在使用多项式采样进行生成时应用的对数变换器的抽象基类。
- en: '#### `__call__`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L57)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L57)'
- en: '[PRE64]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Parameters
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax'
- en: Returns
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`
- en: The processed prediction scores.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.MinLengthLogitsProcessor`'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MinLengthLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L102)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L102)'
- en: '[PRE65]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`min_length` (`int`) — The minimum length below which the score of `eos_token_id`
    is set to `-float("Inf")`.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_length`（`int`）— 小于此长度时，`eos_token_id`的分数将设置为`-float("Inf")`。'
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id`（`Union[int, List[int]]`）— *end-of-sequence*标记的ID。可选地，使用列表设置多个*end-of-sequence*标记。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforcing a min-length by setting EOS probability to 0\. Note that, for decoder-only
    models like most LLMs, the length includes the prompt.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: LogitsProcessor通过将EOS概率设置为0来强制最小长度。请注意，对于像大多数LLMs这样的仅解码器模型，长度包括提示。
- en: 'Examples:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE66]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '#### `__call__`'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L151)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L151)'
- en: '[PRE67]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax'
- en: Returns
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`
- en: The processed prediction scores.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.MinNewTokensLengthLogitsProcessor`'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MinNewTokensLengthLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L160)'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L160)'
- en: '[PRE68]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Parameters
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt_length_to_skip` (`int`) — The input tokens length. Not a valid argument
    when used with `generate` as it will automatically assign the input length.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_length_to_skip`（`int`）— 输入标记的长度。当与`generate`一起使用时，这不是一个有效的参数，因为它会自动分配输入长度。'
- en: '`min_new_tokens` (`int`) — The minimum *new* tokens length below which the
    score of `eos_token_id` is set to `-float("Inf")`.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_new_tokens`（`int`）— 小于此长度时，`eos_token_id`的分数将设置为`-float("Inf")`。'
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id`（`Union[int, List[int]]`）— *end-of-sequence*标记的ID。可选地，使用列表设置多个*end-of-sequence*标记。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforcing a min-length of new tokens by setting EOS (End-Of-Sequence) token probability
    to 0. Contrarily to [MinLengthLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.MinLengthLogitsProcessor),
    this processor ignores the prompt.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: LogitsProcessor通过将EOS（序列结束）标记的概率设置为0来强制新标记的最小长度。与MinLengthLogitsProcessor相反，此处理器忽略提示。
- en: 'Examples:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE69]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '#### `__call__`'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L212)'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L212)'
- en: '[PRE70]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Parameters
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax'
- en: Returns
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`
- en: The processed prediction scores.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.NoBadWordsLogitsProcessor`'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NoBadWordsLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1090)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1090)'
- en: '[PRE71]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Parameters
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`bad_words_ids` (`List[List[int]]`) — List of list of token ids that are not
    allowed to be generated.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bad_words_ids` (`List[List[int]]`) — 不允许生成的标记ID列表。'
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`) — *结束序列*标记的ID。可选地，使用列表设置多个*结束序列*标记。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces that specified sequences will never be selected.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)，强制指定的序列永远不会被选中。'
- en: In order to get the token ids of the words that should not appear in the generated
    text, make sure to set `add_prefix_space=True` when initializing the tokenizer,
    and use `tokenizer(bad_words, add_special_tokens=False).input_ids`. The `add_prefix_space`
    argument is only supported for some slow tokenizers, as fast tokenizers’ prefixing
    behaviours come from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取不应出现在生成文本中的单词的标记ID，请确保在初始化分词器时设置`add_prefix_space=True`，并使用`tokenizer(bad_words,
    add_special_tokens=False).input_ids`。`add_prefix_space`参数仅支持一些慢速分词器，因为快速分词器的前缀行为来自`pre
    tokenizers`。在这里阅读更多信息(https://huggingface.co/docs/tokenizers/api/pre-tokenizers)。
- en: 'Examples:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE72]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '#### `__call__`'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)'
- en: '[PRE73]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Parameters
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax'
- en: Returns
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.NoRepeatNGramLogitsProcessor`'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NoRepeatNGramLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L816)'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L816)'
- en: '[PRE74]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Parameters
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`ngram_size` (`int`) — All ngrams of size `ngram_size` can only occur once.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_size` (`int`) — 所有大小为`ngram_size`的ngrams只能出现一次。'
- en: 'N-grams are groups of “n” consecutive words, characters, or tokens taken from
    a sequence of text. Given the sentence: “She runs fast”, the bi-grams (n=2) would
    be (“she”, “runs”) and (“runs”, “fast”). In text generation, avoiding repetitions
    of word sequences provides a more diverse output. This [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforces no repetition of n-grams by setting the scores of banned tokens to negative
    infinity which eliminates those tokens from consideration when further processing
    the scores. Note that, for decoder-only models like most LLMs, the prompt is also
    considered to obtain the n-grams. [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: N-grams是从文本序列中获取的“n”个连续单词、字符或标记的组合。给定句子：“她跑得快”，二元组（n=2）将是（“她”，“跑”）和（“跑”，“快”）。在文本生成中，避免单词序列的重复提供了更多样化的输出。这个[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)通过将被禁止的标记的分数设置为负无穷来强制不重复n-grams，从而消除了这些标记在进一步处理分数时的考虑。请注意，对于大多数仅解码器模型（如大多数LLMs），提示也被视为获取n-grams。[Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345)。
- en: Use n-gram penalties with care. For instance, penalizing 2-grams (bigrams) in
    an article about the city of New York might lead to undesirable outcomes where
    the city’s name appears only once in the entire text. [Reference](https://huggingface.co/blog/how-to-generate)
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 谨慎使用n-gram惩罚。例如，在关于纽约市的文章中惩罚2-gram（二元组）可能导致不良结果，其中城市的名称仅出现一次在整个文本中。[参考](https://huggingface.co/blog/how-to-generate)
- en: 'Examples:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE75]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '#### `__call__`'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L863)'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L863)'
- en: '[PRE76]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Parameters
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax'
- en: Returns
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.PrefixConstrainedLogitsProcessor`'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PrefixConstrainedLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1177)'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1177)'
- en: '[PRE77]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Parameters
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`) — This
    function constraints the beam search to allowed tokens only at each step. This
    function takes 2 arguments `inputs_ids` and the batch ID `batch_id`. It has to
    return a list with the allowed tokens for the next generation step conditioned
    on the previously generated tokens `inputs_ids` and the batch ID `batch_id`.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`) — 此函数将波束搜索限制为每个步骤仅允许的标记。此函数接受2个参数`inputs_ids`和批次ID`batch_id`。它必须返回一个列表，其中包含下一代步骤的允许标记，条件是先前生成的标记`inputs_ids`和批次ID`batch_id`。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces constrained generation and is useful for prefix-conditioned constrained
    generation. See [Autoregressive Entity Retrieval](https://arxiv.org/abs/2010.00904)
    for more information.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    强制执行受限制的生成，对于前缀条件的受限制生成很有用。有关更多信息，请参阅[自回归实体检索](https://arxiv.org/abs/2010.00904)。'
- en: 'Examples:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE78]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '#### `__call__`'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1228)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1228)'
- en: '[PRE79]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Parameters
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.RepetitionPenaltyLogitsProcessor`'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RepetitionPenaltyLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L288)'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L288)'
- en: '[PRE80]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Parameters
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`penalty` (`float`) — The parameter for repetition penalty. 1.0 means no penalty.
    Above 1.0 penalizes previously generated tokens. Between 0.0 and 1.0 rewards previously
    generated tokens.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`penalty` (`float`) — 重复惩罚的参数。1.0表示没有惩罚。大于1.0会惩罚先前生成的标记。在0.0和1.0之间会奖励先前生成的标记。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that prevents the repetition of previous tokens through a penalty. This penalty
    is applied at most once per token. Note that, for decoder-only models like most
    LLMs, the considered tokens include the prompt.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    通过惩罚防止先前标记的重复。此惩罚最多每个标记应用一次。请注意，对于大多数仅解码器模型（如大多数LLMs），考虑的标记包括提示。'
- en: In the original [paper](https://arxiv.org/pdf/1909.05858.pdf), the authors suggest
    the use of a penalty of around 1.2 to achieve a good balance between truthful
    generation and lack of repetition. To penalize and reduce repetition, use `penalty`
    values above 1.0, where a higher value penalizes more strongly. To reward and
    encourage repetition, use `penalty` values between 0.0 and 1.0, where a lower
    value rewards more strongly.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始[论文](https://arxiv.org/pdf/1909.05858.pdf)中，作者建议使用约1.2的惩罚来实现真实生成和减少重复之间的良好平衡。为了惩罚和减少重复，使用大于1.0的`penalty`值，其中较高的值会更强烈地惩罚。为了奖励和鼓励重复，使用0.0和1.0之间的`penalty`值，较低的值会更强烈地奖励。
- en: 'Examples:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE81]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '#### `__call__`'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L331)'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L331)'
- en: '[PRE82]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Parameters
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax。'
- en: Returns
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.SequenceBiasLogitsProcessor`'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SequenceBiasLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L942)'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L942)'
- en: '[PRE83]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Parameters
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequence_bias` (`Dict[Tuple[int], float]`) — Dictionary that maps a sequence
    of tokens to its bias term. Positive biases increase the odds of the sequence
    being selected, while negative biases do the opposite. If a sequence has a length
    of 1, its bias will always be applied. Otherwise, the bias will only be applied
    if the sequence in question is about to be completed (in the token selection step
    after this processor is applied).'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_bias` (`Dict[Tuple[int], float]`) — 将标记序列映射到其偏差项的字典。正偏差增加选择该序列的几率，而负偏差则相反。如果序列长度为1，则其偏差将始终应用。否则，仅当所讨论的序列即将完成时（在应用此处理器后的标记选择步骤中）才会应用偏差。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that applies an additive bias on sequences. The bias is applied to the last token
    of a sequence when the next generated token can complete it. Consequently, to
    take the most of biasing sequences with more than one token, consider using beam
    methods (to gracefully work around partially completed sequences that have a negative
    bias) and applying the bias to their prefixes (to ensure the bias is applied earlier).'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)应用于序列的附加偏置。当下一个生成的标记可以完成序列时，将偏置应用于序列的最后一个标记。因此，为了充分利用对具有多个标记的序列进行偏置，考虑使用波束方法（以优雅地解决部分完成的序列具有负偏差的问题）并将偏置应用于它们的前缀（以确保较早地应用偏置）。'
- en: In order to get the token ids of the sequences that you want to bias, make sure
    to set `add_prefix_space=True` when initializing the tokenizer, and use `tokenizer(bad_words,
    add_special_tokens=False).input_ids`. The `add_prefix_space` argument is only
    supported for some slow tokenizers, as fast tokenizers’ prefixing behaviours come
    from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取您想要偏置的序列的标记ID，请确保在初始化分词器时设置`add_prefix_space=True`，并使用`tokenizer(bad_words,
    add_special_tokens=False).input_ids`。`add_prefix_space`参数仅支持一些慢速分词器，因为快速分词器的前缀行为来自`pre
    tokenizers`。[在这里](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)阅读更多。
- en: 'Examples:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE84]'
  id: totrans-606
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '#### `__call__`'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)'
- en: '[PRE85]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Parameters
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log
    softmax'
- en: Returns
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.SuppressTokensAtBeginLogitsProcessor`'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SuppressTokensAtBeginLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1622)'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1622)'
- en: '[PRE86]'
  id: totrans-618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[SuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SuppressTokensAtBeginLogitsProcessor)
    supresses a list of tokens as soon as the `generate` function starts generating
    using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens`
    are not generated at the begining. Originally created for [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '[SuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SuppressTokensAtBeginLogitsProcessor)在`generate`函数开始生成时立即抑制一系列标记，使用`begin_index`标记。这应该确保由`begin_suppress_tokens`定义的标记在开始时不会被生成。最初为[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)创建。'
- en: 'Examples:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE87]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '#### `__call__`'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1664)'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1664)'
- en: '[PRE88]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Parameters
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log
    softmax'
- en: Returns
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.SuppressTokensLogitsProcessor`'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SuppressTokensLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1672)'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1672)'
- en: '[PRE89]'
  id: totrans-633
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: This processor can be used to suppress a list of tokens. The processor will
    set their log probs to `-inf` so that they are not generated. Originally created
    for [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 此处理器可用于抑制一系列标记。处理器将将它们的对数概率设置为`-inf`，以便它们不会被生成。最初为[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)创建。
- en: 'Examples:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE90]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '#### `__call__`'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1704)'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1704)'
- en: '[PRE91]'
  id: totrans-639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Parameters
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log
    softmax'
- en: Returns
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.TemperatureLogitsWarper`'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TemperatureLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L222)'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L222)'
- en: '[PRE92]'
  id: totrans-648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Parameters
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`temperature` (`float`) — Strictly positive float value used to modulate the
    logits distribution. A value smaller than `1` decreases randomness (and vice versa),
    with `0` being equivalent to shifting all probability mass to the most likely
    token.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` (`float`) — 用于调节logits分布的严格正值浮点值。小于 `1` 的值会减少随机性（反之亦然），`0` 相当于将所有概率质量转移到最可能的标记。'
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    for temperature (exponential scaling output probability distribution), which effectively
    means that it can control the randomness of the predicted tokens. Often used together
    with [TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper)
    and [TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper).'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    用于温度（指数缩放输出概率分布），这有效地意味着它可以控制预测标记的随机性。通常与[TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper)和[TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper)一起使用。'
- en: Make sure that `do_sample=True` is included in the `generate` arguments otherwise
    the temperature value won’t have any effect.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在 `generate` 参数中包含 `do_sample=True`，否则温度值将不会产生任何效果。
- en: 'Examples:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE93]'
  id: totrans-654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '#### `__call__`'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L282)'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L282)'
- en: '[PRE94]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Parameters
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax'
- en: Returns
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.TopKLogitsWarper`'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TopKLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L462)'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L462)'
- en: '[PRE95]'
  id: totrans-666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Parameters
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_k` (`int`) — The number of highest probability vocabulary tokens to keep
    for top-k-filtering.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`) — 要保留的最高概率词汇标记的数量。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`, *可选*, 默认为 1) — 不能被过滤的最小标记数量。'
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs top-k, i.e. restricting to the k highest probability elements. Often
    used together with [TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)
    and [TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper).'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    执行 top-k，即限制为最高概率元素 k。通常与[TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)和[TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper)一起使用。'
- en: 'Examples:'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE96]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '#### `__call__`'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L506)'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L506)'
- en: '[PRE97]'
  id: totrans-676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Parameters
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax'
- en: Returns
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.TopPLogitsWarper`'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TopPLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L397)'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L397)'
- en: '[PRE98]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Parameters
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_p` (`float`) — If set to < 1, only the smallest set of most probable tokens
    with probabilities that add up to `top_p` or higher are kept for generation.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p` (`float`) — 如果设置为 < 1，则仅保留概率相加达到 `top_p` 或更高的最可能标记集合用于生成。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`, *可选*, 默认为 1) — 不能被过滤的最小标记数量。'
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <=
    prob_cut_off. Often used together with [TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)
    and [TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper).'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)执行top-p，即限制总和小于等于prob_cut_off的前几个标记。通常与[TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)和[TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper)一起使用。'
- en: 'Examples:'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE99]'
  id: totrans-692
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '#### `__call__`'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L446)'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L446)'
- en: '[PRE100]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Parameters
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log
    softmax。'
- en: Returns
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.TypicalLogitsWarper`'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TypicalLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L515)'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L515)'
- en: '[PRE101]'
  id: totrans-704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Parameters
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`mass` (`float`, *optional*, defaults to 0.9) — Value of typical_p between
    0 and 1 inclusive, defaults to 0.9.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mass` (`float`, *可选*, 默认为0.9) — 典型p值在0到1之间，默认为0.9。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`，*可选*，默认为-inf) — 所有被过滤的值将被设置为此浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`，*可选*，默认为1) — 不能被过滤的最小标记数。'
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs typical decoding. Inspired on how humans use language, it prioritizes
    tokens whose log probability is close to the entropy of the token probability
    distribution. This means that the most likely tokens may be discarded in the process.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)执行典型解码。受到人类如何使用语言的启发，它优先考虑对数概率接近标记概率分布的熵的标记。这意味着在过程中可能会丢弃最有可能的标记。'
- en: See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666)
    for more information.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[自然语言生成的典型解码](https://arxiv.org/abs/2202.00666)获取更多信息。
- en: 'Examples:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE102]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '#### `__call__`'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L579)'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L579)'
- en: '[PRE103]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Parameters
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log
    softmax。'
- en: Returns
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.UnbatchedClassifierFreeGuidanceLogitsProcessor`'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UnbatchedClassifierFreeGuidanceLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2055)'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2055)'
- en: '[PRE104]'
  id: totrans-724
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Parameters
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`guidance_scale` (`float`) — The guidance scale for classifier free guidance
    (CFG). CFG is enabled by setting `guidance_scale != 1`. Higher guidance scale
    encourages the model to generate samples that are more closely linked to the input
    prompt, usually at the expense of poorer quality. A value smaller than 1 has the
    opposite effect, while making the negative prompt provided with negative_prompt_ids
    (if any) act as a positive prompt.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`) — 分类器自由引导（CFG）的引导比例。通过设置`guidance_scale != 1`来启用CFG。较高的引导比例鼓励模型生成与输入提示更紧密相关的样本，通常以牺牲质量为代价。小于1的值具有相反的效果，同时使得与负面提示提供的负面提示ID（如果有）作为正面提示。'
- en: '`model` (`PreTrainedModel`) — The model computing the unconditional scores.
    Supposedly the same as the one computing the conditional scores. Both models must
    use the same tokenizer.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`PreTrainedModel`) — 计算无条件分数的模型。假设与计算条件分数的模型相同。两个模型必须使用相同的分词器。'
- en: '`unconditional_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of input sequence tokens in the vocabulary for the unconditional
    branch. If unset, will default to the last token of the prompt.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unconditional_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 无条件分支中词汇表中输入序列标记的索引。如果未设置，将默认为提示的最后一个标记。'
- en: '`unconditional_attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Attention mask for unconditional_ids.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unconditional_attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于无条件ID的注意力掩码。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether to cache key/values
    during the negative prompt forward pass.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*，默认为`True`）— 是否在负prompt前向传递期间缓存键/值。'
- en: Logits processor for Classifier-Free Guidance (CFG). The processors computes
    a weighted average across scores from prompt conditional and prompt unconditional
    (or negative) logits, parameterized by the `guidance_scale`. The unconditional
    scores are computed internally by prompting `model` with the `unconditional_ids`
    branch.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 用于无分类器引导（CFG）的Logits处理器。处理器通过`guidance_scale`参数化的prompt条件和prompt无条件（或负）logits的分数进行加权平均。无条件分数是通过提示`model`使用`unconditional_ids`分支内部计算的。
- en: See [the paper](https://arxiv.org/abs/2306.17806) for more information.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[论文](https://arxiv.org/abs/2306.17806)。
- en: 'Examples:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE105]'
  id: totrans-734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '#### `__call__`'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2161)'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2161)'
- en: '[PRE106]'
  id: totrans-737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '### `class transformers.WhisperTimeStampLogitsProcessor`'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperTimeStampLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1761)'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1761)'
- en: '[PRE107]'
  id: totrans-740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Parameters
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`generate_config` (`GenerateConfig`) — The generate config used to generate
    the output. The following parameters are required: eos_token_id (`int`, *optional*,
    defaults to 50257): The id of the *end-of-sequence* token. no_timestamps_token_id
    (`int`, *optional*, defaults to 50363): The id of the `"<|notimestamps|>"` token.
    max_initial_timestamp_index (`int`, *optional*, defaults to 1): Used to set the
    maximum value of the initial timestamp. This is used to prevent the model from
    predicting timestamps that are too far in the future.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_config`（`GenerateConfig`）--用于生成输出的生成配置。需要以下参数：eos_token_id（`int`，*optional*，默认为50257）：*sequence*结束标记的id。`no_timestamps_token_id`（`int`，*optional*，默认50363）：`“<|notimestamps|>”`令牌。`max_initial_timestamp_index`（`int`，*optional*，默认1）的id：用于设置初始时间戳的最大值。这用于防止模型预测未来太远的时间戳。'
- en: '`begin_index` (`Optional`, *optional*) — Token index of the first token that
    is generated by the model.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`begin_index`（`Optional`，*可选*）— 模型生成的第一个标记的标记索引。'
- en: '`_detect_timestamp_from_logprob` (`bool`, *optional*) — Whether timestamps
    can be predicted from logprobs over all timestamps.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_detect_timestamp_from_logprob`（`bool`，*可选*）— 是否可以从所有时间戳的logprobs中预测时间戳。'
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that modifies the logits for the generation of timestamps in the transcription.
    When the input tokens are at a specific threshold, the processor sets the scores
    to negative infinity. The processor makes sure that timestamp tokens appear in
    pairs, by masking out the logits that would break this pairing pattern. This is
    done to maintain the consistency and structure of generated timestamps. It also
    ensures that when the predicted probability of sampling any of the timestamp token
    is greater than any individual non-timestamp token, those non-timestamp logits
    are set to negative infinity. This is done to ensure the generation of timestamps
    over other potential tokens.'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)用于修改时间戳生成中的logits。当输入标记达到特定阈值时，处理器将分数设置为负无穷大。处理器确保时间戳标记成对出现，通过屏蔽会破坏这种配对模式的logits。这样做是为了保持生成的时间戳的一致性和结构。它还确保当预测任何时间戳标记的采样概率大于任何单个非时间戳标记时，这些非时间戳logits被设置为负无穷大。这样做是为了确保生成时间戳而不是其他潜在标记。'
- en: See [the paper](https://arxiv.org/abs/2212.04356) for more information.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[论文](https://arxiv.org/abs/2212.04356)。
- en: 'Examples:'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE108]'
  id: totrans-748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '#### `__call__`'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1843)'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1843)'
- en: '[PRE109]'
  id: totrans-751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Parameters
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log
    softmax'
- en: Returns
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`的形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: TensorFlow
  id: totrans-758
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow
- en: '### `class transformers.TFForcedBOSTokenLogitsProcessor`'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFForcedBOSTokenLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L448)'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L448)'
- en: '[PRE110]'
  id: totrans-761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Parameters
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`bos_token_id` (`int`) — The id of the token to force as the first generated
    token.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id`（`int`）— 强制作为第一个生成的标记的标记ID。'
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces the specified token as the first generated token.'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)强制指定的标记作为第一个生成的标记。'
- en: '#### `__call__`'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L462)'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L462)'
- en: '[PRE111]'
  id: totrans-767
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '### `class transformers.TFForcedEOSTokenLogitsProcessor`'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFForcedEOSTokenLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L478)'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L478)'
- en: '[PRE112]'
  id: totrans-770
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Parameters
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`max_length` (`int`) — The maximum length of the sequence to be generated.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`）— 要生成的序列的最大长度。'
- en: '`eos_token_id` (`int`) — The id of the token to force as the last generated
    token when `max_length` is reached.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id`（`int`）— 在达到`max_length`时强制作为最后生成的标记的标记ID。'
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces the specified token as the last generated token when `max_length`
    is reached.'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)强制指定的标记作为达到`max_length`时的最后生成的标记。'
- en: '#### `__call__`'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L495)'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L495)'
- en: '[PRE113]'
  id: totrans-777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '### `class transformers.TFForceTokensLogitsProcessor`'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFForceTokensLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L551)'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L551)'
- en: '[PRE114]'
  id: totrans-780
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: This processor takes a list of pairs of integers which indicates a mapping from
    generation indices to token indices that will be forced before sampling. The processor
    will set their log probs to `0` and all other tokens to `-inf` so that they are
    sampled at their corresponding index.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 这个处理器接受一对整数的列表，指示从生成索引到标记索引的映射，这些将在采样之前被强制。处理器将它们的log概率设置为`0`，并将所有其他标记设置为`-inf`，以便在相应的索引处对它们进行采样。
- en: '#### `__call__`'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L567)'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L567)'
- en: '[PRE115]'
  id: totrans-784
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '### `class transformers.TFLogitsProcessor`'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L53)'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L53)'
- en: '[PRE116]'
  id: totrans-787
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Abstract base class for all logit processors that can be applied during generation.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在生成过程中应用的所有logit处理器的抽象基类。
- en: '#### `__call__`'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L56)'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L56)'
- en: '[PRE117]'
  id: totrans-791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Parameters
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-794
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。详情请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-795
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`tf.Tensor` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tf.Tensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam
    search时，这些可以是每个词汇表的logits，或者在使用beam search时，可以是每个词汇表标记的log softmax。'
- en: '`cur_len` (`int`) — The current length of valid input sequence tokens. In the
    TF implementation, the input_ids’ sequence length is the maximum length generate
    can produce, and we need to know which of its tokens are valid.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cur_len` (`int`) — 有效输入序列标记的当前长度。在TF实现中，input_ids的序列长度是生成器可以生成的最大长度，我们需要知道哪些标记是有效的。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`，*可选*) — 其他logits处理器特定的kwargs。'
- en: Returns
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tf.Tensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Tensor`，形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: TF method for processing logits.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 处理logits的TF方法。
- en: '### `class transformers.TFLogitsProcessorList`'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFLogitsProcessorList`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L75)'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L75)'
- en: '[PRE118]'
  id: totrans-805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: This class can be used to create a list of [TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    to subsequently process a `scores` input tensor. This class inherits from list
    and adds a specific ***call*** method to apply each [TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    to the inputs.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可用于创建一个[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)列表，以后处理`scores`输入张量。该类继承自列表，并添加了一个特定的***call***方法，以应用每个[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)到输入。
- en: '#### `__call__`'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L82)'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L82)'
- en: '[PRE119]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Parameters
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。详情请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`tf.Tensor` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tf.Tensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam
    search时，这些可以是每个词汇表的logits，或者在使用beam search时，可以是每个词汇表标记的log softmax。'
- en: '`cur_len` (`int`) — The current length of valid input sequence tokens. In the
    TF implementation, the input_ids’ sequence length is the maximum length generate
    can produce, and we need to know which of its tokens are valid.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cur_len` (`int`) — 有效输入序列标记的当前长度。在 TF 实现中，input_ids 的序列长度是生成的最大长度，我们需要知道哪些标记是有效的。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 额外的 logits 处理器特定 kwargs。'
- en: Returns
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tf.Tensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为 `(batch_size, config.vocab_size)` 的 `tf.Tensor`
- en: The processed prediction scores.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.TFLogitsWarper`'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L64)'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L64)'
- en: '[PRE120]'
  id: totrans-822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Abstract base class for all logit warpers that can be applied during generation
    with multinomial sampling.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在多项式采样生成期间应用的所有 logits 扭曲的抽象基类。
- en: '#### `__call__`'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L67)'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L67)'
- en: '[PRE121]'
  id: totrans-826
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Parameters
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`scores` (`tf.Tensor` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`tf.Tensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的
    logits，或者在使用波束搜索时，可以是每个词汇标记的 log softmax。'
- en: '`cur_len` (`int`) — The current length of valid input sequence tokens. In the
    TF implementation, the input_ids’ sequence length is the maximum length generate
    can produce, and we need to know which of its tokens are valid.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cur_len` (`int`) — 有效输入序列标记的当前长度。在 TF 实现中，input_ids 的序列长度是生成的最大长度，我们需要知道哪些标记是有效的。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 额外的 logits 处理器特定 kwargs。'
- en: Returns
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tf.Tensor` of shape `(batch_size, config.vocab_size)`'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为 `(batch_size, config.vocab_size)` 的 `tf.Tensor`
- en: The processed prediction scores.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: TF method for warping logits.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 用于扭曲 logits 的 TF 方法。
- en: '### `class transformers.TFMinLengthLogitsProcessor`'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFMinLengthLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L202)'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L202)'
- en: '[PRE122]'
  id: totrans-840
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Parameters
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`min_length` (`int`) — The minimum length below which the score of `eos_token_id`
    is set to `-float("Inf")`.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_length` (`int`) — 将 `eos_token_id` 的分数设置为 `-float("Inf")` 的最小长度。'
- en: '`eos_token_id` (`int`) — The id of the *end-of-sequence* token.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`) — *序列结束* 标记的 id。'
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    enforcing a min-length by setting EOS probability to 0.'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    通过将 EOS 概率设置为 0 来强制最小长度。'
- en: '#### `__call__`'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L228)'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L228)'
- en: '[PRE123]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '### `class transformers.TFNoBadWordsLogitsProcessor`'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFNoBadWordsLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L288)'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L288)'
- en: '[PRE124]'
  id: totrans-850
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Parameters
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`bad_words_ids` (`List[List[int]]`) — List of list of token ids that are not
    allowed to be generated. In order to get the tokens of the words that should not
    appear in the generated text, make sure to set `add_prefix_space=True` when initializing
    the tokenizer, and use `tokenizer(bad_words, add_special_tokens=False).input_ids`.
    The `add_prefix_space` argument is only supported for some slow tokenizers, as
    fast tokenizers’ prefixing behaviours come from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bad_words_ids` (`List[List[int]]`) — 不允许生成的标记 id 的列表。为了获取不应出现在生成文本中的单词的标记，请确保在初始化分词器时设置
    `add_prefix_space=True`，并使用 `tokenizer(bad_words, add_special_tokens=False).input_ids`。`add_prefix_space`
    参数仅支持某些慢速分词器，因为快速分词器的前缀行为来自 `pre tokenizers`。更多信息请阅读[这里](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)。'
- en: '`eos_token_id` (`int`) — The id of the *end-of-sequence* token.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`) — *序列结束* 标记的 id。'
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces that specified sequences will never be sampled.'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    强制指定序列永远不会被采样。'
- en: '#### `__call__`'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L367)'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L367)'
- en: '[PRE125]'
  id: totrans-857
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '### `class transformers.TFNoRepeatNGramLogitsProcessor`'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFNoRepeatNGramLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L388)'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L388)'
- en: '[PRE126]'
  id: totrans-860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Parameters
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`ngram_size` (`int`) — All ngrams of size `ngram_size` can only occur once.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_size` (`int`) — 所有大小为`ngram_size`的n-gram只能出现一次。'
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces no repetition of n-grams. See [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    强制不重复n-gram。参见[Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345)。'
- en: '#### `__call__`'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L427)'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L427)'
- en: '[PRE127]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '### `class transformers.TFRepetitionPenaltyLogitsProcessor`'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFRepetitionPenaltyLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L238)'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L238)'
- en: '[PRE128]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Parameters
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`repetition_penalty` (`float`) — The parameter for repetition penalty. 1.0
    means no penalty. See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more
    details.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repetition_penalty` (`float`) — 重复惩罚的参数。1.0表示没有惩罚。更多细节请参阅[这篇论文](https://arxiv.org/pdf/1909.05858.pdf)。'
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    enforcing an exponential penalty on repeated sequences.'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    对重复序列施加指数惩罚。'
- en: '#### `__call__`'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L280)'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L280)'
- en: '[PRE129]'
  id: totrans-875
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '### `class transformers.TFSuppressTokensAtBeginLogitsProcessor`'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFSuppressTokensAtBeginLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L511)'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L511)'
- en: '[PRE130]'
  id: totrans-878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[TFSuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFSuppressTokensAtBeginLogitsProcessor)
    suppresses a list of tokens as soon as the `generate` function starts generating
    using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens`
    at not sampled at the begining of the generation.'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFSuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFSuppressTokensAtBeginLogitsProcessor)
    在`generate`函数开始生成时立即抑制一组标记。这应该确保在生成开始时不会抽样到由`begin_suppress_tokens`定义的标记。'
- en: '#### `__call__`'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L522)'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L522)'
- en: '[PRE131]'
  id: totrans-882
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '### `class transformers.TFSuppressTokensLogitsProcessor`'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFSuppressTokensLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L535)'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L535)'
- en: '[PRE132]'
  id: totrans-885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: This processor can be used to suppress a list of tokens. The processor will
    set their log probs to `-inf` so that they are not sampled.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 这个处理器可以用来抑制一组标记。处理器将把它们的对数概率设置为`-inf`，以便它们不被抽样。
- en: '#### `__call__`'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L542)'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L542)'
- en: '[PRE133]'
  id: totrans-889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '### `class transformers.TFTemperatureLogitsWarper`'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFTemperatureLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L98)'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L98)'
- en: '[PRE134]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Parameters
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`temperature` (`float`) — The value used to module the logits distribution.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` (`float`) — 用于调节logits分布的值。'
- en: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    for temperature (exponential scaling output probability distribution).'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    用于温度（指数缩放输出概率分布）。'
- en: '#### `__call__`'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L113)'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L113)'
- en: '[PRE135]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '### `class transformers.TFTopKLogitsWarper`'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFTopKLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L118)'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L118)'
- en: '[PRE136]'
  id: totrans-901
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Parameters
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_k` (`int`) — The number of highest probability vocabulary tokens to keep
    for top-k-filtering.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`) — 保留的最高概率词汇标记数。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`, *可选*, 默认为-inf) — 所有被过滤的值将被设置为这个浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`, *可选*, 默认为1) — 不能被过滤的最小标记数。'
- en: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    that performs top-k, i.e. restricting to the k highest probability elements.'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    执行top-k，即限制为概率最高的k个元素。'
- en: '#### `__call__`'
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L138)'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L138)'
- en: '[PRE137]'
  id: totrans-909
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '### `class transformers.TFTopPLogitsWarper`'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFTopPLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L146)'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L146)'
- en: '[PRE138]'
  id: totrans-912
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Parameters
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_p` (`float`) — If set to < 1, only the smallest set of most probable tokens
    with probabilities that add up to `top_p` or higher are kept for generation.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p` (`float`) — 如果设置为<1，则仅保留概率相加大于`top_p`或更高的最可能标记集。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`, *可选*, 默认为-inf) — 所有被过滤的值将被设置为这个浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`, *可选*, 默认为1) — 不能被过滤的最小标记数。'
- en: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    that performs top-p, i.e. restricting to top tokens summing to <= prob_cut_off.'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    执行top-p，即限制总和小于等于prob_cut_off的顶级标记。'
- en: '#### `__call__`'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L170)'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L170)'
- en: '[PRE139]'
  id: totrans-920
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: FLAX
  id: totrans-921
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FLAX
- en: '### `class transformers.FlaxForcedBOSTokenLogitsProcessor`'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxForcedBOSTokenLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L194)'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L194)'
- en: '[PRE140]'
  id: totrans-924
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: Parameters
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`bos_token_id` (`int`) — The id of the token to force as the first generated
    token.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`) — 强制作为第一个生成的标记的标记id。'
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    that enforces the specified token as the first generated token.'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    将指定的标记作为第一个生成的标记。'
- en: '#### `__call__`'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L206)'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L206)'
- en: '[PRE141]'
  id: totrans-930
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '### `class transformers.FlaxForcedEOSTokenLogitsProcessor`'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxForcedEOSTokenLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L216)'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L216)'
- en: '[PRE142]'
  id: totrans-933
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Parameters
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`max_length` (`int`) — The maximum length of the sequence to be generated.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`) — 要生成的序列的最大长度。'
- en: '`eos_token_id` (`int`) — The id of the token to force as the last generated
    token when `max_length` is reached.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`) — 当达到`max_length`时，强制作为最后生成的标记的标记id。'
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    that enforces the specified token as the last generated token when `max_length`
    is reached.'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    在达到`max_length`时将指定的标记强制为最后生成的标记。'
- en: '#### `__call__`'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L231)'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L231)'
- en: '[PRE143]'
  id: totrans-940
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '### `class transformers.FlaxForceTokensLogitsProcessor`'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxForceTokensLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L315)'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L315)'
- en: '[PRE144]'
  id: totrans-943
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: Parameters
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`force_token_map` (`list`) — Map giving token ids and indices where they will
    be forced to be sampled.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_token_map` (`list`) — 给出标记id和它们将被强制采样的索引的映射。'
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    that takes a list of pairs of integers which indicates a mapping from generation
    indices to token indices that will be forced before sampling. The processor will
    set their log probs to 0 and all other tokens to `-inf` so that they are sampled
    at their corresponding index.'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    接受一对整数的列表，指示从生成索引到标记索引的映射，这些标记将在采样之前被强制。处理器将它们的对数概率设置为0，将所有其他标记设置为`-inf`，以便它们在相应的索引处被采样。'
- en: '#### `__call__`'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L337)'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L337)'
- en: '[PRE145]'
  id: totrans-949
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '### `class transformers.FlaxLogitsProcessor`'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L50)'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L50)'
- en: '[PRE146]'
  id: totrans-952
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: Abstract base class for all logit processors that can be applied during generation.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在生成期间可以应用的logit处理器的抽象基类。
- en: '#### `__call__`'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L53)'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L53)'
- en: '[PRE147]'
  id: totrans-956
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Parameters
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-959
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-960
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam
    search时，这些可以是每个词汇的logits，或者在使用beam search时，可以是每个词汇标记的log softmax'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *optional*) — 特定于logits处理器的额外kwargs。'
- en: Returns
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`jnp.ndarray` of shape `(batch_size, config.vocab_size)`'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: '`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: Flax method for processing logits.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: Flax处理logits的方法。
- en: '### `class transformers.FlaxLogitsProcessorList`'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxLogitsProcessorList`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L72)'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L72)'
- en: '[PRE148]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: This class can be used to create a list of [FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    or [FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    to subsequently process a `scores` input tensor. This class inherits from list
    and adds a specific ***call*** method to apply each [FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    or [FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    to the inputs.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 此类可用于创建[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)或[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)的列表，以随后处理`scores`输入张量。此类继承自列表，并添加了一个特定的***call***方法来应用每个[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)或[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)到输入中。
- en: '#### `__call__`'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L79)'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L79)'
- en: '[PRE149]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: Parameters
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`jnp.ndarray`的形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-976
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-977
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam
    search时，这些可以是每个词汇的logits，或者在使用beam search时，可以是每个词汇标记的log softmax'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *optional*) — 特定于logits处理器的额外kwargs。'
- en: Returns
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`jnp.ndarray` of shape `(batch_size, config.vocab_size)`'
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: '`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: '### `class transformers.FlaxLogitsWarper`'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L61)'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L61)'
- en: '[PRE150]'
  id: totrans-985
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: Abstract base class for all logit warpers that can be applied during generation
    with multinomial sampling.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在生成过程中应用多项式采样的所有logit扭曲器的抽象基类。
- en: '#### `__call__`'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L64)'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L64)'
- en: '[PRE151]'
  id: totrans-989
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: Parameters
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`jnp.ndarray`的形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-992
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-993
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam
    search时，这些可以是每个词汇的logits，或者在使用beam search时，可以是每个词汇标记的log softmax'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *optional*) — 特定于logits处理器的额外kwargs。'
- en: Returns
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`jnp.ndarray` of shape `(batch_size, config.vocab_size)`'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: '`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`'
- en: The processed prediction scores.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的预测分数。
- en: Flax method for warping logits.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: Flax扭曲logits的方法。
- en: '### `class transformers.FlaxMinLengthLogitsProcessor`'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxMinLengthLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L241)'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L241)'
- en: '[PRE152]'
  id: totrans-1002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: Parameters
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`min_length` (`int`) — The minimum length below which the score of `eos_token_id`
    is set to `-float("Inf")`.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_length` (`int`) — 当长度低于此值时，`eos_token_id` 的分数将被设置为 `-float("Inf")`。'
- en: '`eos_token_id` (`int`) — The id of the *end-of-sequence* token.'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`) — *序列结束* 标记的 id。'
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    enforcing a min-length by setting EOS probability to 0.'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    通过将 EOS 的概率设置为 0 来强制执行最小长度。'
- en: '#### `__call__`'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L262)'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L262)'
- en: '[PRE153]'
  id: totrans-1009
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '### `class transformers.FlaxSuppressTokensAtBeginLogitsProcessor`'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxSuppressTokensAtBeginLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L271)'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L271)'
- en: '[PRE154]'
  id: totrans-1012
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: Parameters
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`begin_suppress_tokens` (`List[int]`) — Tokens to not sample.'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`begin_suppress_tokens` (`List[int]`) — 不采样的标记。'
- en: '`begin_index` (`int`) — Index where the tokens are suppressed.'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`begin_index` (`int`) — 抑制标记的索引。'
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    supressing a list of tokens as soon as the `generate` function starts generating
    using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens`
    are not sampled at the begining of the generation.'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    在 `generate` 函数开始生成时立即抑制一组标记，使用 `begin_index` 标记。这应该确保由 `begin_suppress_tokens`
    定义的标记在生成开始时不被采样。'
- en: '#### `__call__`'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L288)'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L288)'
- en: '[PRE155]'
  id: totrans-1019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '### `class transformers.FlaxSuppressTokensLogitsProcessor`'
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxSuppressTokensLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L296)'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L296)'
- en: '[PRE156]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: Parameters
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`suppress_tokens` (`list`) — Tokens to not sample.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suppress_tokens` (`list`) — 不采样的标记。'
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    suppressing a list of tokens at each decoding step. The processor will set their
    log probs to be `-inf` so they are not sampled.'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    在每个解码步骤抑制一组标记。处理器将它们的对数概率设置为 `-inf`，以便它们不被采样。'
- en: '#### `__call__`'
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L309)'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L309)'
- en: '[PRE157]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '### `class transformers.FlaxTemperatureLogitsWarper`'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxTemperatureLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L95)'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L95)'
- en: '[PRE158]'
  id: totrans-1031
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: Parameters
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`temperature` (`float`) — The value used to module the logits distribution.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` (`float`) — 用于调节 logits 分布的值。'
- en: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    for temperature (exponential scaling output probability distribution).'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    用于温度（指数缩放输出概率分布）。'
- en: '#### `__call__`'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L110)'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L110)'
- en: '[PRE159]'
  id: totrans-1037
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '### `class transformers.FlaxTopKLogitsWarper`'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxTopKLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L159)'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L159)'
- en: '[PRE160]'
  id: totrans-1040
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: Parameters
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_k` (`int`) — The number of highest probability vocabulary tokens to keep
    for top-k-filtering.'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`) — 保留最高概率词汇标记的数量以进行 top-k 过滤。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`, *可选*, 默认为 1) — 不能被过滤的最小标记数。'
- en: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    that performs top-k, i.e. restricting to the k highest probability elements.'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    执行 top-k，即限制为概率最高的 k 个元素。'
- en: '#### `__call__`'
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L179)'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L179)'
- en: '[PRE161]'
  id: totrans-1048
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '### `class transformers.FlaxTopPLogitsWarper`'
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxTopPLogitsWarper`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L115)'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L115)'
- en: '[PRE162]'
  id: totrans-1051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: Parameters
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_p` (`float`) — If set to < 1, only the smallest set of most probable tokens
    with probabilities that add up to `top_p` or higher are kept for generation.'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p` (`float`) — 如果设置为 < 1，则只保留概率相加达到 `top_p` 或更高的最小一组最可能的标记用于生成。'
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`，*可选*，默认为1) — 不能被过滤的最小标记数。'
- en: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <=
    prob_cut_off.'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)执行
    top-p，即限制总概率小于等于 prob_cut_off 的前 p 个标记。'
- en: '#### `__call__`'
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L139)'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L139)'
- en: '[PRE163]'
  id: totrans-1059
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '### `class transformers.FlaxWhisperTimeStampLogitsProcessor`'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWhisperTimeStampLogitsProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L363)'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L363)'
- en: '[PRE164]'
  id: totrans-1062
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: Parameters
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`generate_config` (`GenerateConfig`) — The generate config used to generate
    the output. The following parameters are required: eos_token_id (`int`, *optional*,
    defaults to 50257): The id of the *end-of-sequence* token. no_timestamps_token_id
    (`int`, *optional*, defaults to 50363): The id of the `"<|notimestamps|>"` token.
    max_initial_timestamp_index (`int`, *optional*, defaults to 1): Used to set the
    maximum value of the initial timestamp. This is used to prevent the model from
    predicting timestamps that are too far in the future.'
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_config` (`GenerateConfig`) — 用于生成输出的生成配置。需要以下参数：eos_token_id (`int`，*可选*，默认为50257)：*序列结束*标记的id。no_timestamps_token_id
    (`int`，*可选*，默认为50363)：`"<|notimestamps|>"`标记的id。max_initial_timestamp_index (`int`，*可选*，默认为1)：用于设置初始时间戳的最大值。这用于防止模型预测太遥远的时间戳。'
- en: Whisper specific Processor. This processor can be used to force a list of tokens.
    The processor will set their log probs to `inf` so that they are sampled at their
    corresponding index.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper特定的处理器。此处理器可用于强制一个标记列表。处理器将将它们的对数概率设置为`inf`，以便在相应的索引处对它们进行采样。
- en: '#### `__call__`'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L397)'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L397)'
- en: '[PRE165]'
  id: totrans-1068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: StoppingCriteria
  id: totrans-1069
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StoppingCriteria
- en: A [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    can be used to change when to stop generation (other than EOS token). Please note
    that this is exclusively available to our PyTorch implementations.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: '[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)可用于更改生成过程何时停止（除了
    EOS 标记）。请注意，这仅适用于我们的 PyTorch 实现。'
- en: '### `class transformers.StoppingCriteria`'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.StoppingCriteria`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L37)'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L37)'
- en: '[PRE166]'
  id: totrans-1073
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: Abstract base class for all stopping criteria that can be applied during generation.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可以在生成过程中应用的停止标准的抽象基类。
- en: If your stopping criteria depends on the `scores` input, make sure you pass
    `return_dict_in_generate=True, output_scores=True` to `generate`.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的停止标准取决于`scores`输入，请确保将`return_dict_in_generate=True, output_scores=True`传递给`generate`。
- en: '#### `__call__`'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L44)'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L44)'
- en: '[PRE167]'
  id: totrans-1078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: Parameters
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1081
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1082
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。这些可以是
    SoftMax 之前每个词汇标记的分数，也可以是 SoftMax 之后每个词汇标记的分数。如果此停止标准取决于`scores`输入，请确保将`return_dict_in_generate=True,
    output_scores=True`传递给`generate`。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`，*可选*) — 其他特定停止标准的关键字参数。'
- en: '### `class transformers.StoppingCriteriaList`'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.StoppingCriteriaList`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L129)'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L129)'
- en: '[PRE168]'
  id: totrans-1087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '#### `__call__`'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L130)'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L130)'
- en: '[PRE169]'
  id: totrans-1090
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: Parameters
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1093
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1094
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    语言建模头的预测分数。这些可以是SoftMax之前每个词汇标记的分数，也可以是SoftMax之后每个词汇标记的分数。如果此停止标准取决于`scores`输入，请确保您传递`return_dict_in_generate=True,
    output_scores=True`给`generate`。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 其他特定停止标准的kwargs。'
- en: '### `class transformers.MaxLengthCriteria`'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MaxLengthCriteria`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L49)'
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L49)'
- en: '[PRE170]'
  id: totrans-1099
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: Parameters
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`max_length` (`int`) — The maximum length that the output sequence can have
    in number of tokens.'
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`) — 输出序列在标记数量上可以具有的最大长度。'
- en: '`max_position_embeddings` (`int`, *optional*) — The maximum model length, as
    defined by the model’s `config.max_position_embeddings` attribute.'
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *可选*) — 模型的最大长度，由模型的`config.max_position_embeddings`属性定义。'
- en: This class can be used to stop generation whenever the full generated number
    of tokens exceeds `max_length`. Keep in mind for decoder-only type of transformers,
    this will include the initial prompted tokens.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可以用来在生成的标记数超过`max_length`时停止生成。请注意，对于仅解码器类型的transformers，这将包括初始提示的标记。
- en: '#### `__call__`'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L65)'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L65)'
- en: '[PRE171]'
  id: totrans-1106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: Parameters
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  id: totrans-1111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    语言建模头的预测分数。这些可以是SoftMax之前每个词汇标记的分数，也可以是SoftMax之后每个词汇标记的分数。如果此停止标准取决于`scores`输入，请确保您传递`return_dict_in_generate=True,
    output_scores=True`给`generate`。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  id: totrans-1112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 其他特定停止标准的kwargs。'
- en: '### `class transformers.MaxTimeCriteria`'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MaxTimeCriteria`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L107)'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L107)'
- en: '[PRE172]'
  id: totrans-1115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Parameters
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`max_time` (`float`) — The maximum allowed time in seconds for the generation.'
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_time` (`float`) — 生成的最大允许时间（以秒为单位）。'
- en: '`initial_time` (`float`, *optional*, defaults to `time.time()`) — The start
    of the generation allowed time.'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial_time` (`float`, *可选*, 默认为`time.time()`) — 允许生成的开始时间。 '
- en: This class can be used to stop generation whenever the full generation exceeds
    some amount of time. By default, the time will start being counted when you initialize
    this function. You can override this by passing an `initial_time`.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可以用来在完整生成超过一定时间时停止生成。默认情况下，当初始化此函数时，时间将开始计算。您可以通过传递`initial_time`来覆盖这一点。
- en: '#### `__call__`'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L124)'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L124)'
- en: '[PRE173]'
  id: totrans-1122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: Parameters
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  id: totrans-1127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    语言建模头的预测分数。这些可以是 SoftMax 之前每个词汇标记的分数，也可以是 SoftMax 之后每个词汇标记的分数。如果这个停止标准依赖于 `scores`
    输入，确保你传递 `return_dict_in_generate=True, output_scores=True` 给 `generate`。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 其他特定的停止标准参数。'
- en: Constraints
  id: totrans-1129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 约束
- en: A [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    can be used to force the generation to include specific tokens or sequences in
    the output. Please note that this is exclusively available to our PyTorch implementations.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: '[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    可以用来强制生成结果中包含特定的标记或序列。请注意，这仅适用于我们的 PyTorch 实现。'
- en: '### `class transformers.Constraint`'
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Constraint`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L5)'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L5)'
- en: '[PRE174]'
  id: totrans-1133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: Abstract base class for all constraints that can be applied during generation.
    It must define how the constraint can be satisfied.
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可以在生成过程中应用的约束的抽象基类。它必须定义约束如何被满足。
- en: All classes that inherit Constraint must follow the requirement that
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 所有继承 Constraint 的类必须遵循的要求
- en: '[PRE175]'
  id: totrans-1136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: will always terminate (halt).
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 将始终终止（停止）。
- en: '#### `advance`'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `advance`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L48)'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L48)'
- en: '[PRE176]'
  id: totrans-1140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: Returns
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: token_ids(`torch.tensor`)
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: token_ids(`torch.tensor`)
- en: Must be a tensor of a list of indexable tokens, not some integer.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 必须是一个可索引的标记列表的张量，而不是某个整数。
- en: When called, returns the token that would take this constraint one step closer
    to being fulfilled.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 调用时，返回一个标记，这个标记会使这个约束更接近被满足一步。
- en: '#### `copy`'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `copy`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L113)'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L113)'
- en: '[PRE177]'
  id: totrans-1147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: Returns
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: constraint(`Constraint`)
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: constraint(`Constraint`)
- en: The same constraint as the one being called from.
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 与被调用的相同的约束。
- en: Creates a new instance of this constraint.
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这个约束的一个新实例。
- en: '#### `does_advance`'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `does_advance`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L60)'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L60)'
- en: '[PRE178]'
  id: totrans-1154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: Reads in a token and returns whether it creates progress.
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 读取一个标记并返回它是否推进了进度。
- en: '#### `remaining`'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `remaining`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L104)'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L104)'
- en: '[PRE179]'
  id: totrans-1158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: Returns the number of remaining steps of `advance()` in order to complete this
    constraint.
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 `advance()` 完成这个约束还需要多少步骤。
- en: '#### `reset`'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reset`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L94)'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L94)'
- en: '[PRE180]'
  id: totrans-1162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: Resets the state of this constraint to its initialization. We would call this
    in cases where the fulfillment of a constraint is abrupted by an unwanted token.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 重置这个约束的状态到初始化状态。我们会在约束的实现被不想要的标记中断时调用这个方法。
- en: '#### `test`'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `test`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L24)'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L24)'
- en: '[PRE181]'
  id: totrans-1166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: Tests whether this constraint has been properly defined.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 测试这个约束是否已经正确定义。
- en: '#### `update`'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L69)'
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L69)'
- en: '[PRE182]'
  id: totrans-1170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: Returns
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: stepped(`bool`)
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: stepped(`bool`)
- en: 'Whether this constraint has become one step closer to being fulfuilled. completed(`bool`):
    Whether this constraint has been completely fulfilled by this token being generated.
    reset (`bool`): Whether this constraint has reset its progress by this token being
    generated.'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: '这个约束是否变得更接近被满足一步。completed(`bool`): 这个约束是否已经被这个生成的标记完全满足。reset (`bool`): 这个约束是否已经被这个生成的标记重置了进度。'
- en: 'Reads in a token and returns booleans that indicate the progress made by it.
    This function will update the state of this object unlikes `does_advance(self,
    token_id: int)`.'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: '读取一个标记并返回指示其推进程度的布尔值。这个函数会更新这个对象的状态，不像 `does_advance(self, token_id: int)`。'
- en: This isn’t to test whether a certain token will advance the progress; it’s to
    update its state as if it has been generated. This becomes important if token_id
    != desired token (refer to else statement in PhrasalConstraint)
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是为了测试某个特定的标记是否会推进进度；而是为了更新它的状态，就好像它已经被生成了。如果 token_id != desired token（参考
    PhrasalConstraint 中的 else 语句），这变得很重要。
- en: '### `class transformers.PhrasalConstraint`'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PhrasalConstraint`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L129)'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L129)'
- en: '[PRE183]'
  id: totrans-1178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: Parameters
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids` (`List[int]`) — The id of the token that must be generated by the
    output.'
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`（`List[int]`）— 必须由输出生成的token的id。'
- en: '[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    enforcing that an ordered sequence of tokens is included in the output.'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: '[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)强制要求输出中包含一个有序的token序列。'
- en: '### `class transformers.DisjunctiveConstraint`'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DisjunctiveConstraint`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L261)'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L261)'
- en: '[PRE184]'
  id: totrans-1184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Parameters
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`nested_token_ids` (`List[List[int]]`) — A list of words, where each word is
    a list of ids. This constraint is fulfilled by generating just one from the list
    of words.'
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nested_token_ids`（`List[List[int]]`）— 一个单词列表，其中每个单词都是一个id列表。通过从单词列表中生成一个单词来满足此约束。'
- en: A special [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    that is fulfilled by fulfilling just one of several constraints.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特殊的[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)，通过满足几个约束中的一个来实现。
- en: '### `class transformers.ConstraintListState`'
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConstraintListState`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L351)'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L351)'
- en: '[PRE185]'
  id: totrans-1190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: Parameters
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`constraints` (`List[Constraint]`) — A list of [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    objects that must be fulfilled by the beam scorer.'
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`constraints`（`List[Constraint]`）— 必须由beam评分器满足的[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)对象列表。'
- en: A class for beam scorers to track its progress through a list of constraints.
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 用于跟踪beam评分器通过一系列约束的进度的类。
- en: '#### `advance`'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `advance`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L383)'
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L383)'
- en: '[PRE186]'
  id: totrans-1196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: The list of tokens to generate such that we can make progress. By “list” we
    don’t mean the list of token that will fully fulfill a constraint.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成的token列表，以便我们可以取得进展。这里的“列表”并不意味着将完全满足约束的token列表。
- en: 'Given constraints `c_i = {t_ij | j == # of tokens}`, If we’re not in the middle
    of progressing through a specific constraint `c_i`, we return:'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: '给定约束`c_i = {t_ij | j == # of tokens}`，如果我们不处于通过特定约束`c_i`进行进度的中间阶段，我们返回：'
- en: '`[t_k1 for k in indices of unfulfilled constraints]`'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: '`[t_k1 for k in indices of unfulfilled constraints]`'
- en: 'If we are in the middle of a constraint, then we return: `[t_ij]`, where `i`
    is the index of the inprogress constraint, `j` is the next step for the constraint.'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处于约束的中间阶段，那么我们返回：`[t_ij]`，其中`i`是正在进行的约束的索引，`j`是约束的下一步。
- en: Though we don’t care which constraint is fulfilled first, if we are in the progress
    of fulfilling a constraint, that’s the only one we’ll return.
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不关心哪个约束先被满足，但如果我们正在满足一个约束，那么这是我们唯一会返回的。
- en: '#### `reset`'
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reset`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L418)'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L418)'
- en: '[PRE187]'
  id: totrans-1204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: 'token_ids: the tokens generated thus far to reset the state of the progress
    through constraints.'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: token_ids：到目前为止生成的token，以重置通过约束的进度状态。
- en: BeamSearch
  id: totrans-1206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeamSearch
- en: '### `class transformers.BeamScorer`'
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeamScorer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L91)'
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L91)'
- en: '[PRE188]'
  id: totrans-1209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: Abstract base class for all beam scorers that are used for [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    and [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample).
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 所有用于[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)和[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)的beam评分器的抽象基类。
- en: '#### `process`'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `process`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L97)'
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L97)'
- en: '[PRE189]'
  id: totrans-1213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: Parameters
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-1215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size * num_beams, sequence_length)`的`torch.LongTensor`）—
    词汇表中输入序列token的索引。'
- en: Indices can be obtained using any class inheriting from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用任何继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的类来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`next_scores` (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`)
    — Current scores of the top `2 * num_beams` non-finished beam hypotheses.'
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_scores`（形状为`(batch_size, 2 * num_beams)`的`torch.FloatTensor`）— 前`2 *
    num_beams`个未完成的beam假设的当前分数。'
- en: '`next_tokens` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`) —
    `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished
    beam hypotheses.'
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_tokens`（形状为`(batch_size, 2 * num_beams)`的`torch.LongTensor`）— 与前`2 *
    num_beams`个未完成的beam假设对应的`input_ids`的tokens。'
- en: '`next_indices` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`)
    — Beam indices indicating to which beam hypothesis the `next_tokens` correspond.'
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_indices`（形状为`(batch_size, 2 * num_beams)`的`torch.LongTensor`）— 指示`next_tokens`对应于哪个beam假设的beam索引。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id`（`int`，*可选*）— *填充*标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。'
- en: '`beam_indices` (`torch.LongTensor`, *optional*) — Beam indices indicating to
    which beam hypothesis each token correspond.'
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices`（`torch.LongTensor`，*可选*）— 指示每个标记对应于哪个beam假设的beam索引。'
- en: '`group_index` (`int`, *optional*) — The index of the group of beams. Used with
    [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search).'
  id: totrans-1224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_index`（`int`，*可选*）— beam组的索引。与[group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)一起使用。'
- en: Returns
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`UserDict`'
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: '`UserDict`'
- en: 'A dictionary composed of the fields as defined above:'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: 由上述字段组成的字典：
- en: '`next_beam_scores` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Updated scores of all non-finished beams.'
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_beam_scores`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 所有未完成beam的更新分数。'
- en: '`next_beam_tokens` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Next tokens to be added to the non-finished beam_hypotheses.'
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_beam_tokens`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 要添加到未完成beam_hypotheses的下一个标记。'
- en: '`next_beam_indices` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Beam indices indicating to which beam the next tokens shall be added.'
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_beam_indices`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 指示下一个标记应添加到哪个beam的beam索引。'
- en: '#### `finalize`'
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `finalize`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L109)'
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L109)'
- en: '[PRE190]'
  id: totrans-1233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: Parameters
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-1235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size * num_beams, sequence_length)`的`torch.LongTensor`）—
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using any class inheriting from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用任何继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的类来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`final_beam_scores` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — The final scores of all non-finished beams.'
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_beam_scores`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 所有未完成beam的最终分数。'
- en: '`final_beam_tokens` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — The last tokens to be added to the non-finished beam_hypotheses.'
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_beam_tokens`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 要添加到未完成beam_hypotheses的最后一个标记。'
- en: '`final_beam_indices` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — The beam indices indicating to which beam the `final_beam_tokens` shall be added.'
  id: totrans-1240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_beam_indices`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 指示`final_beam_tokens`应添加到哪个beam的beam索引。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-1241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id`（`int`，*可选*）— *填充*标记的id。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-1242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。'
- en: Returns
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.LongTensor`的形状为`(batch_size * num_return_sequences, sequence_length)`'
- en: The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。
- en: '### `class transformers.BeamSearchScorer`'
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeamSearchScorer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L123)'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L123)'
- en: '[PRE191]'
  id: totrans-1248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: Parameters
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_size` (`int`) — Batch Size of `input_ids` for which standard beam search
    decoding is run in parallel.'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`）— `input_ids`的批量大小，用于并行运行标准beam搜索解码。'
- en: '`num_beams` (`int`) — Number of beams for beam search.'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams`（`int`）— beam搜索的beam数量。'
- en: '`device` (`torch.device`) — Defines the device type (*e.g.*, `"cpu"` or `"cuda"`)
    on which this instance of `BeamSearchScorer` will be allocated.'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`torch.device`）— 定义此`BeamSearchScorer`实例将分配到的设备类型（例如，`"cpu"`或`"cuda"`）。'
- en: '`length_penalty` (`float`, *optional*, defaults to 1.0) — Exponential penalty
    to the length that is used with beam-based generation. It is applied as an exponent
    to the sequence length, which in turn is used to divide the score of the sequence.
    Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty`
    > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter
    sequences.'
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_penalty`（`float`，*可选*，默认为1.0）— 用于基于beam的生成的长度的指数惩罚。它作为指数应用于序列长度，然后用于分割序列的分数。由于分数是序列的对数似然（即负数），`length_penalty`
    > 0.0 促进更长的序列，而`length_penalty` < 0.0 鼓励更短的序列。'
- en: '`do_early_stopping` (`bool` or `str`, *optional*, defaults to `False`) — Controls
    the stopping condition for beam-based methods, like beam-search. It accepts the
    following values: `True`, where the generation stops as soon as there are `num_beams`
    complete candidates; `False`, where an heuristic is applied and the generation
    stops when is it very unlikely to find better candidates; `"never"`, where the
    beam search procedure only stops when there cannot be better candidates (canonical
    beam search algorithm).'
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_early_stopping` (`bool`或`str`，*可选*，默认为`False`) — 控制基于beam的方法（如beam-search）的停止条件。接受以下值：`True`，生成在有`num_beams`个完整候选时停止；`False`，应用启发式方法，当很难找到更好的候选时停止生成；`"never"`，仅当不能有更好的候选时，beam搜索过程才会停止（经典beam搜索算法）。'
- en: '`num_beam_hyps_to_keep` (`int`, *optional*, defaults to 1) — The number of
    beam hypotheses that shall be returned upon calling [finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize).'
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beam_hyps_to_keep` (`int`，*可选*，默认为1) — 在调用[finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize)时应返回的beam假设数量。'
- en: '`num_beam_groups` (`int`, *optional*, defaults to 1) — Number of groups to
    divide `num_beams` into in order to ensure diversity among different groups of
    beams. See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.'
  id: totrans-1256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beam_groups` (`int`，*可选*，默认为1) — 将`num_beams`分成多个组以确保不同组的beam之间的多样性。有关更多详细信息，请参阅[此论文](https://arxiv.org/pdf/1610.02424.pdf)。'
- en: '`max_length` (`int`, *optional*) — The maximum length of the sequence to be
    generated.'
  id: totrans-1257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`，*可选*) — 要生成的序列的最大长度。'
- en: '[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    implementing standard beam search decoding.'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)实现标准beam搜索解码。'
- en: Adapted in part from [Facebook’s XLM beam search code](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 部分改编自[Facebook的XLM beam搜索代码](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529)。
- en: Reference for the diverse beam search algorithm and implementation [Ashwin Kalyan’s
    DBS implementation](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性beam搜索算法和实现的参考[Ashwin Kalyan的DBS实现](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)
- en: '#### `process`'
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `process`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L215)'
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L215)'
- en: '[PRE192]'
  id: totrans-1263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '#### `finalize`'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `finalize`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L318)'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L318)'
- en: '[PRE193]'
  id: totrans-1266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '### `class transformers.ConstrainedBeamSearchScorer`'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConstrainedBeamSearchScorer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L415)'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L415)'
- en: '[PRE194]'
  id: totrans-1269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: Parameters
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_size` (`int`) — Batch Size of `input_ids` for which standard beam search
    decoding is run in parallel.'
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`) — 并行运行标准beam搜索解码的`input_ids`的批处理大小。'
- en: '`num_beams` (`int`) — Number of beams for beam search.'
  id: totrans-1272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`) — beam搜索的beam数量。'
- en: '`constraints` (`List[Constraint]`) — A list of positive constraints represented
    as `Constraint` objects that must be fulfilled in the generation output. For more
    information, the documentation of [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    should be read.'
  id: totrans-1273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`constraints` (`List[Constraint]`) — 以`Constraint`对象表示的正约束列表，必须在生成输出中满足。有关更多信息，请阅读[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)的文档。'
- en: '`device` (`torch.device`) — Defines the device type (*e.g.*, `"cpu"` or `"cuda"`)
    on which this instance of `BeamSearchScorer` will be allocated.'
  id: totrans-1274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`torch.device`) — 定义此`BeamSearchScorer`实例将分配到的设备类型（例如，`"cpu"`或`"cuda"`）。'
- en: '`length_penalty` (`float`, *optional*, defaults to 1.0) — Exponential penalty
    to the length that is used with beam-based generation. It is applied as an exponent
    to the sequence length, which in turn is used to divide the score of the sequence.
    Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty`
    > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter
    sequences.'
  id: totrans-1275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length_penalty` (`float`，*可选*，默认为1.0) — 用于基于beam的生成的长度的指数惩罚。它作为序列长度的指数应用，然后用于分割序列的分数。由于分数是序列的对数似然（即负数），`length_penalty`
    > 0.0 促进更长的序列，而`length_penalty` < 0.0 鼓励更短的序列。'
- en: '`do_early_stopping` (`bool` or `str`, *optional*, defaults to `False`) — Controls
    the stopping condition for beam-based methods, like beam-search. It accepts the
    following values: `True`, where the generation stops as soon as there are `num_beams`
    complete candidates; `False`, where an heuristic is applied and the generation
    stops when is it very unlikely to find better candidates; `"never"`, where the
    beam search procedure only stops when there cannot be better candidates (canonical
    beam search algorithm).'
  id: totrans-1276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_early_stopping` (`bool`或`str`，*可选*，默认为`False`) — 控制基于beam的方法（如beam-search）的停止条件。接受以下值：`True`，生成在有`num_beams`个完整候选时停止；`False`，应用启发式方法，当很难找到更好的候选时停止生成；`"never"`，仅当不能有更好的候选时，beam搜索过程才会停止（经典beam搜索算法）。'
- en: '`num_beam_hyps_to_keep` (`int`, *optional*, defaults to 1) — The number of
    beam hypotheses that shall be returned upon calling [finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize).'
  id: totrans-1277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beam_hyps_to_keep` (`int`，*可选*，默认为1) — 在调用[finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize)时应返回的beam假设数量。'
- en: '`num_beam_groups` (`int`, *optional*, defaults to 1) — Number of groups to
    divide `num_beams` into in order to ensure diversity among different groups of
    beams. See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.'
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beam_groups` (`int`，*可选*，默认为1）— 将`num_beams`分成几组以确保不同组束之间的多样性。有关更多详细信息，请参阅[此论文](https://arxiv.org/pdf/1610.02424.pdf)。'
- en: '`max_length` (`int`, *optional*) — The maximum length of the sequence to be
    generated.'
  id: totrans-1279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`，*可选*）— 要生成的序列的最大长度。'
- en: '[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    implementing constrained beam search decoding.'
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    实现受限束搜索解码。'
- en: '#### `process`'
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `process`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L509)'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L509)'
- en: '[PRE195]'
  id: totrans-1283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: Parameters
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-1285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size * num_beams, sequence_length)`）—
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using any class inheriting from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-1286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用任何继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的类来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`next_scores` (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`)
    — Current scores of the top `2 * num_beams` non-finished beam hypotheses.'
  id: totrans-1288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_scores` (`torch.FloatTensor`，形状为`(batch_size, 2 * num_beams)`）— 前`2 *
    num_beams`个未完成束假设的当前分数。'
- en: '`next_tokens` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`) —
    `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished
    beam hypotheses.'
  id: totrans-1289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_tokens` (`torch.LongTensor`，形状为`(batch_size, 2 * num_beams)`）— 与前`2 *
    num_beams`个未完成束假设对应的标记的`input_ids`。'
- en: '`next_indices` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`)
    — Beam indices indicating to which beam hypothesis the `next_tokens` correspond.'
  id: totrans-1290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_indices` (`torch.LongTensor`，形状为`(batch_size, 2 * num_beams)`）— 指示`next_tokens`对应的束假设的束索引。'
- en: '`scores_for_all_vocab` (`torch.FloatTensor` of shape `(batch_size * num_beams,
    sequence_length)`) — The scores of all tokens in the vocabulary for each of the
    beam hypotheses.'
  id: totrans-1291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores_for_all_vocab` (`torch.FloatTensor`，形状为`(batch_size * num_beams, sequence_length)`）—
    每个束假设的词汇表中所有标记的分数。'
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`，*可选*）— *填充*标记的ID。'
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  id: totrans-1293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`Union[int, List[int]]`，*可选*）— *结束序列*标记的ID。可以选择使用列表设置多个*结束序列*标记。'
- en: '`beam_indices` (`torch.LongTensor`, *optional*) — Beam indices indicating to
    which beam hypothesis each token correspond.'
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_indices` (`torch.LongTensor`，*可选*）— 指示每个标记对应的束假设的束索引。'
- en: '`decoder_prompt_len` (`int`, *optional*) — The length of prompt that is included
    in the input to decoder.'
  id: totrans-1295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_prompt_len` (`int`，*可选*）— 包含在输入到解码器中的提示长度。'
- en: Returns
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`UserDict`'
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: '`UserDict`'
- en: 'A dictionary composed of the fields as defined above:'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 由上述字段组成的字典：
- en: '`next_beam_scores` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Updated scores of all non-finished beams.'
  id: totrans-1299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_beam_scores` (`torch.FloatTensor`，形状为`(batch_size * num_beams)`）— 所有未完成束的更新分数。'
- en: '`next_beam_tokens` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Next tokens to be added to the non-finished beam_hypotheses.'
  id: totrans-1300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_beam_tokens` (`torch.FloatTensor`，形状为`(batch_size * num_beams)`）— 要添加到未完成束假设的下一个标记。'
- en: '`next_beam_indices` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Beam indices indicating to which beam the next tokens shall be added.'
  id: totrans-1301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_beam_indices` (`torch.FloatTensor`，形状为`(batch_size * num_beams)`）— 指示下一个标记应添加到哪个束中的束索引。'
- en: '#### `finalize`'
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `finalize`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L807)'
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L807)'
- en: '[PRE196]'
  id: totrans-1304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: Utilities
  id: totrans-1305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用工具
- en: '#### `transformers.top_k_top_p_filtering`'
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.top_k_top_p_filtering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L4610)'
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L4610)'
- en: '[PRE197]'
  id: totrans-1308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: Parameters
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_k` (`int`, *optional*, defaults to 0) — If > 0, only keep the top k tokens
    with highest probability (top-k filtering)'
  id: totrans-1310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`，*可选*，默认为0）— 如果大于0，则仅保留具有最高概率的前k个标记（top-k过滤）'
- en: '`top_p` (`float`, *optional*, defaults to 1.0) — If < 1.0, only keep the top
    tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering
    is described in Holtzman et al. ([http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751))'
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p` (`float`，*可选*，默认为1.0）— 如果小于1.0，则仅保留累积概率大于等于top_p的前几个标记（nucleus过滤）。Nucleus过滤在Holtzman等人的论文中有描述（[http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751))。'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimumber of tokens
    we keep per batch example in the output.'
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep` (`int`，*可选*，默认为1）— 输出中每个批次示例保留的最小标记数。'
- en: Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用top-k和/或nucleus（top-p）过滤对数分布
- en: 'From: [https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 来自：[https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)
- en: '#### `transformers.tf_top_k_top_p_filtering`'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.tf_top_k_top_p_filtering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L3092)'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L3092)'
- en: '[PRE198]'
  id: totrans-1317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: Parameters
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`top_k` (`int`, *optional*, defaults to 0) — If > 0, only keep the top k tokens
    with highest probability (top-k filtering)'
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k`（`int`，*可选*，默认为0）— 如果> 0，则仅保留具有最高概率的前k个标记（top-k过滤）'
- en: '`top_p` (`float`, *optional*, defaults to 1.0) — If < 1.0, only keep the top
    tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering
    is described in Holtzman et al. ([http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751))'
  id: totrans-1320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p`（`float`，*可选*，默认为1.0）— 如果<1.0，则仅保留累积概率>= top_p的前几个标记（nucleus过滤）。Nucleus过滤在Holtzman等人的论文中有描述（[http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751)）'
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimumber of tokens
    we keep per batch example in the output.'
  id: totrans-1321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_tokens_to_keep`（`int`，*可选*，默认为1）— 输出中每个批示例要保留的最小标记数。'
- en: Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用top-k和/或nucleus（top-p）过滤对数分布
- en: 'From: [https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 来自：[https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)
- en: Streamers
  id: totrans-1324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流媒体器
- en: '### `class transformers.TextStreamer`'
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TextStreamer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L38)'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L38)'
- en: '[PRE199]'
  id: totrans-1327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: Parameters
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`AutoTokenizer`) — The tokenized used to decode the tokens.'
  id: totrans-1329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`AutoTokenizer`）— 用于解码标记的标记器。'
- en: '`skip_prompt` (`bool`, *optional*, defaults to `False`) — Whether to skip the
    prompt to `.generate()` or not. Useful e.g. for chatbots.'
  id: totrans-1330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_prompt`（`bool`，*可选*，默认为`False`）— 是否跳过提示以执行`.generate()`。例如，对于聊天机器人很有用。'
- en: '`decode_kwargs` (`dict`, *optional*) — Additional keyword arguments to pass
    to the tokenizer’s `decode` method.'
  id: totrans-1331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode_kwargs`（`dict`，*可选*）— 传递给标记器的`decode`方法的其他关键字参数。'
- en: Simple text streamer that prints the token(s) to stdout as soon as entire words
    are formed.
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的文本流媒体器，一旦形成完整单词，就会将标记打印到标准输出。
- en: The API for the streamer classes is still under development and may change in
    the future.
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: 流媒体类的API仍在开发中，可能会在未来发生变化。
- en: 'Examples:'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE200]'
  id: totrans-1335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '#### `end`'
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L116)'
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L116)'
- en: '[PRE201]'
  id: totrans-1338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: Flushes any remaining cache and prints a newline to stdout.
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新任何剩余的缓存并将换行符打印到标准输出。
- en: '#### `on_finalized_text`'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_finalized_text`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L130)'
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L130)'
- en: '[PRE202]'
  id: totrans-1342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: Prints the new text to stdout. If the stream is ending, also prints a newline.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 将新文本打印到标准输出。如果流结束，也会打印一个换行符。
- en: '#### `put`'
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `put`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L82)'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L82)'
- en: '[PRE203]'
  id: totrans-1346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: Receives tokens, decodes them, and prints them to stdout as soon as they form
    entire words.
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 接收标记，解码它们，并在形成完整单词时立即将它们打印到标准输出。
- en: '### `class transformers.TextIteratorStreamer`'
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TextIteratorStreamer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L159)'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L159)'
- en: '[PRE204]'
  id: totrans-1350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: Parameters
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`AutoTokenizer`) — The tokenized used to decode the tokens.'
  id: totrans-1352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`AutoTokenizer`）— 用于解码标记的标记器。'
- en: '`skip_prompt` (`bool`, *optional*, defaults to `False`) — Whether to skip the
    prompt to `.generate()` or not. Useful e.g. for chatbots.'
  id: totrans-1353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_prompt`（`bool`，*可选*，默认为`False`）— 是否跳过提示以执行`.generate()`。例如，对于聊天机器人很有用。'
- en: '`timeout` (`float`, *optional*) — The timeout for the text queue. If `None`,
    the queue will block indefinitely. Useful to handle exceptions in `.generate()`,
    when it is called in a separate thread.'
  id: totrans-1354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout`（`float`，*可选*）— 文本队列的超时时间。如果为`None`，队列将无限期阻塞。在单独的线程中调用`.generate()`时，有助于处理异常。'
- en: '`decode_kwargs` (`dict`, *optional*) — Additional keyword arguments to pass
    to the tokenizer’s `decode` method.'
  id: totrans-1355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode_kwargs`（`dict`，*可选*）— 传递给标记器的`decode`方法的其他关键字参数。'
- en: Streamer that stores print-ready text in a queue, to be used by a downstream
    application as an iterator. This is useful for applications that benefit from
    acessing the generated text in a non-blocking way (e.g. in an interactive Gradio
    demo).
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: 将可打印文本存储在队列中的流，供下游应用程序用作迭代器。这对于那些从以非阻塞方式访问生成的文本中受益的应用程序很有用（例如，在交互式Gradio演示中）。
- en: The API for the streamer classes is still under development and may change in
    the future.
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 流媒体类的API仍在开发中，可能会在未来发生变化。
- en: 'Examples:'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE205]'
  id: totrans-1359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '#### `on_finalized_text`'
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_finalized_text`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L213)'
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L213)'
- en: '[PRE206]'
  id: totrans-1362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: Put the new text in the queue. If the stream is ending, also put a stop signal
    in the queue.
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: 将新文本放入队列中。如果流结束，也将停止信号放入队列中。
- en: Caches
  id: totrans-1364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: '### `class transformers.Cache`'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Cache`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L6)'
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L6)'
- en: '[PRE207]'
  id: totrans-1367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: Base, abstract class for all caches. The actual data structure is specific to
    each subclass.
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: 所有缓存的基类。实际数据结构对于每个子类是特定的。
- en: '#### `update`'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L11)'
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L11)'
- en: '[PRE208]'
  id: totrans-1371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: Parameters
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`key_states` (`torch.Tensor`) — The new key states to cache.'
  id: totrans-1373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key_states`（`torch.Tensor`）— 要缓存的新键状态。'
- en: '`value_states` (`torch.Tensor`) — The new value states to cache.'
  id: totrans-1374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value_states`（`torch.Tensor`）— 要缓存的新值状态。'
- en: '`layer_idx` (`int`) — The index of the layer to cache the states for.'
  id: totrans-1375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_idx`（`int`）— 用于缓存状态的层的索引。'
- en: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — Additional arguments for the
    cache subclass. These are specific to each subclass and allow new types of cache
    to be created.'
  id: totrans-1376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — 缓存子类的额外参数。这些参数针对每个子类是特定的，并允许创建新类型的缓存。'
- en: Updates the cache with the new `key_states` and `value_states` for the layer
    `layer_idx`.
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新的`key_states`和`value_states`更新层`layer_idx`的缓存。
- en: '### `class transformers.DynamicCache`'
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DynamicCache`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L57)'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L57)'
- en: '[PRE209]'
  id: totrans-1380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: A cache that grows dynamically as more tokens are generated. This is the default
    for generative models.
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成更多令牌，缓存会动态增长。这是生成模型的默认设置。
- en: It stores the Key and Value states as a list of tensors, one for each layer.
    The expected shape for each tensor is `[batch_size, num_heads, seq_len, head_dim]`.
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: 它将键和值状态存储为张量列表，每个层一个张量。每个张量的预期形状为`[batch_size, num_heads, seq_len, head_dim]`。
- en: '#### `update`'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L95)'
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L95)'
- en: '[PRE210]'
  id: totrans-1385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: Parameters
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`key_states` (`torch.Tensor`) — The new key states to cache.'
  id: totrans-1387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key_states` (`torch.Tensor`) — 要缓存的新键状态。'
- en: '`value_states` (`torch.Tensor`) — The new value states to cache.'
  id: totrans-1388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value_states` (`torch.Tensor`) — 要缓存的新值状态。'
- en: '`layer_idx` (`int`) — The index of the layer to cache the states for.'
  id: totrans-1389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_idx` (`int`) — 要为其缓存状态的层的索引。'
- en: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — Additional arguments for the
    cache subclass. No additional arguments are used in `DynamicCache`.'
  id: totrans-1390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — 缓存子类的额外参数。在`DynamicCache`中不使用额外参数。'
- en: Updates the cache with the new `key_states` and `value_states` for the layer
    `layer_idx`.
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新的`key_states`和`value_states`更新层`layer_idx`的缓存。
- en: '#### `get_seq_length`'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_seq_length`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L132)'
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L132)'
- en: '[PRE211]'
  id: totrans-1394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: Returns the sequence length of the cached states. A layer index can be optionally
    passed.
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 返回缓存状态的序列长度。可以选择传递层索引。
- en: '#### `reorder_cache`'
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reorder_cache`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L142)'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L142)'
- en: '[PRE212]'
  id: totrans-1398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: Reorders the cache for beam search, given the selected beam indices.
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: 为束搜索重新排序缓存，给定所选的束索引。
- en: '#### `to_legacy_cache`'
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_legacy_cache`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L150)'
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L150)'
- en: '[PRE213]'
  id: totrans-1402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: Converts the `DynamicCache` instance into the its equivalent in the legacy cache
    format.
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 将`DynamicCache`实例转换为其在传统缓存格式中的等效形式。
- en: '#### `from_legacy_cache`'
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_legacy_cache`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L157)'
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L157)'
- en: '[PRE214]'
  id: totrans-1406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: Converts a cache in the legacy cache format into an equivalent `DynamicCache`.
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: 将传统缓存格式中的缓存转换为等效的`DynamicCache`。
- en: '### `class transformers.SinkCache`'
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SinkCache`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L168)'
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L168)'
- en: '[PRE215]'
  id: totrans-1410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: Parameters
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`window_length` (`int`) — The length of the context window.'
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window_length` (`int`) — 上下文窗口的长度。'
- en: '`num_sink_tokens` (`int`) — The number of sink tokens. See the original paper
    for more information.'
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_sink_tokens` (`int`) — 沉没令牌的数量。有关更多信息，请参阅原始论文。'
- en: A cache that as described in the [Attention Sinks paper](https://arxiv.org/abs/2309.17453).
    It allows the model to generate beyond the length of its context window, without
    losing fluency in the conversation. As it discards past tokens, the model will
    lose the ability to generate tokens that depend on the context that was discarded.
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: 如[Attention Sinks论文](https://arxiv.org/abs/2309.17453)中描述的缓存。它允许模型在超出其上下文窗口长度的情况下生成，而不会失去对话流畅性。随着丢弃过去的令牌，模型将失去生成依赖于被丢弃上下文的令牌的能力。
- en: It stores the Key and Value states as a list of tensors, one for each layer.
    The expected shape for each tensor is `[batch_size, num_heads, seq_len, head_dim]`.
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 它将键和值状态存储为张量列表，每个层一个张量。每个张量的预期形状为`[batch_size, num_heads, seq_len, head_dim]`。
- en: '#### `update`'
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L237)'
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L237)'
- en: '[PRE216]'
  id: totrans-1418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: Parameters
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`key_states` (`torch.Tensor`) — The new key states to cache.'
  id: totrans-1420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key_states` (`torch.Tensor`) — 要缓存的新键状态。'
- en: '`value_states` (`torch.Tensor`) — The new value states to cache.'
  id: totrans-1421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value_states` (`torch.Tensor`) — 要缓存的新值状态。'
- en: '`layer_idx` (`int`) — The index of the layer to cache the states for.'
  id: totrans-1422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_idx` (`int`) — 要为其缓存状态的层的索引。'
- en: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — Additional arguments for the
    cache subclass. The following arguments can be used in `SinkCache`: `sin`, `cos`
    and `partial_rotation_size`. These arguments are used with models using RoPE,
    to recompute the rotation as the tokens are shifted.'
  id: totrans-1423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — 缓存子类的额外参数。在`SinkCache`中可以使用以下参数：`sin`，`cos`和`partial_rotation_size`。这些参数用于使用RoPE的模型，在令牌移位时重新计算旋转。'
- en: Updates the cache with the new `key_states` and `value_states` for the layer
    `layer_idx`.
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新的`key_states`和`value_states`更新层`layer_idx`的缓存。
- en: '#### `get_seq_length`'
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_seq_length`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L226)'
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L226)'
- en: '[PRE217]'
  id: totrans-1427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: Returns the sequence length of the cached states. A layer index can be optionally
    passed.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: 返回缓存状态的序列长度。可以选择传递层索引。
- en: '#### `reorder_cache`'
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reorder_cache`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L316)'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L316)'
- en: '[PRE218]'
  id: totrans-1431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: Reorders the cache for beam search, given the selected beam indices.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列缓存以进行波束搜索，给定所选的波束索引。
