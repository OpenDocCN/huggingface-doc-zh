- en: Summarization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/summarization](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/summarization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/summarization](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/summarization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/yHnr5Dk2zCI](https://www.youtube-nocookie.com/embed/yHnr5Dk2zCI)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/yHnr5Dk2zCI](https://www.youtube-nocookie.com/embed/yHnr5Dk2zCI)'
- en: 'Summarization creates a shorter version of a document or an article that captures
    all the important information. Along with translation, it is another example of
    a task that can be formulated as a sequence-to-sequence task. Summarization can
    be:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‘˜è¦åˆ›å»ºæ–‡æ¡£æˆ–æ–‡ç« çš„ç®€çŸ­ç‰ˆæœ¬ï¼Œæ•æ‰æ‰€æœ‰é‡è¦ä¿¡æ¯ã€‚é™¤äº†ç¿»è¯‘ä¹‹å¤–ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªå¯ä»¥è¢«åˆ¶å®šä¸ºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡çš„ä»»åŠ¡çš„ä¾‹å­ã€‚æ‘˜è¦å¯ä»¥æ˜¯ï¼š
- en: 'Extractive: extract the most relevant information from a document.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ½å–å¼ï¼šä»æ–‡æ¡£ä¸­æå–æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
- en: 'Abstractive: generate new text that captures the most relevant information.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼ï¼šç”Ÿæˆæ•æ‰æœ€ç›¸å…³ä¿¡æ¯çš„æ–°æ–‡æœ¬ã€‚
- en: 'This guide will show you how to:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Finetune [T5](https://huggingface.co/t5-small) on the California state bill
    subset of the [BillSum](https://huggingface.co/datasets/billsum) dataset for abstractive
    summarization.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[BillSum](https://huggingface.co/datasets/billsum)æ•°æ®é›†çš„åŠ åˆ©ç¦å°¼äºšå·è®®æ¡ˆå­é›†ä¸Šå¯¹[T5](https://huggingface.co/t5-small)è¿›è¡Œå¾®è°ƒï¼Œç”¨äºç”Ÿæˆæ‘˜è¦ã€‚
- en: Use your finetuned model for inference.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­å±•ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
- en: '[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus),
    [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small),
    [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt),
    [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5),
    [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart),
    [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe),
    [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart),
    [ProphetNet](../model_doc/prophetnet), [SeamlessM4T](../model_doc/seamless_m4t),
    [SeamlessM4Tv2](../model_doc/seamless_m4t_v2), [SwitchTransformers](../model_doc/switch_transformers),
    [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus),
    [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small),
    [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt),
    [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5),
    [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart),
    [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe),
    [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart),
    [ProphetNet](../model_doc/prophetnet), [SeamlessM4T](../model_doc/seamless_m4t),
    [SeamlessM4Tv2](../model_doc/seamless_m4t_v2), [SwitchTransformers](../model_doc/switch_transformers),
    [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•åˆ°æ‚¨çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œåˆ†äº«æ‚¨çš„æ¨¡å‹ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load BillSum dataset
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½BillSumæ•°æ®é›†
- en: 'Start by loading the smaller California state bill subset of the BillSum dataset
    from the ğŸ¤— Datasets library:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆåŠ è½½ğŸ¤—æ•°æ®é›†åº“ä¸­è¾ƒå°çš„åŠ åˆ©ç¦å°¼äºšå·è®®æ¡ˆå­é›†ï¼š
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ•°æ®é›†åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä½¿ç”¨[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)æ–¹æ³•ï¼š
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then take a look at an example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åçœ‹ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are two fields that youâ€™ll want to use:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ä¸ªå­—æ®µæ‚¨å°†è¦ä½¿ç”¨ï¼š
- en: '`text`: the text of the bill whichâ€™ll be the input to the model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`ï¼šå°†æˆä¸ºæ¨¡å‹è¾“å…¥çš„è®®æ¡ˆæ–‡æœ¬ã€‚'
- en: '`summary`: a condensed version of `text` whichâ€™ll be the model target.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary`ï¼š`text`çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå°†æˆä¸ºæ¨¡å‹çš„ç›®æ ‡ã€‚'
- en: Preprocess
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: 'The next step is to load a T5 tokenizer to process `text` and `summary`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯åŠ è½½T5åˆ†è¯å™¨æ¥å¤„ç†`text`å’Œ`summary`ï¼š
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preprocessing function you want to create needs to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¦åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°éœ€è¦ï¼š
- en: Prefix the input with a prompt so T5 knows this is a summarization task. Some
    models capable of multiple NLP tasks require prompting for specific tasks.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è¾“å…¥å‰åŠ ä¸Šæç¤ºï¼Œä»¥ä¾¿T5çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ä»»åŠ¡ã€‚ä¸€äº›èƒ½å¤Ÿæ‰§è¡Œå¤šä¸ªNLPä»»åŠ¡çš„æ¨¡å‹éœ€è¦ä¸ºç‰¹å®šä»»åŠ¡æä¾›æç¤ºã€‚
- en: Use the keyword `text_target` argument when tokenizing labels.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ ‡è®°æ ‡ç­¾æ—¶ä½¿ç”¨å…³é”®å­—`text_target`å‚æ•°ã€‚
- en: Truncate sequences to be no longer than the maximum length set by the `max_length`
    parameter.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆªæ–­åºåˆ—ï¼Œä½¿å…¶ä¸è¶…è¿‡ç”±`max_length`å‚æ•°è®¾ç½®çš„æœ€å¤§é•¿åº¦ã€‚
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    method. You can speed up the `map` function by setting `batched=True` to process
    multiple elements of the dataset at once:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨ğŸ¤—æ•°æ®é›†çš„[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)æ–¹æ³•ã€‚é€šè¿‡è®¾ç½®`batched=True`æ¥åŠ é€Ÿ`map`å‡½æ•°ï¼Œä»¥ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼š
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now create a batch of examples using [DataCollatorForSeq2Seq](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq).
    Itâ€™s more efficient to *dynamically pad* the sentences to the longest length in
    a batch during collation, instead of padding the whole dataset to the maximum
    length.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨[DataCollatorForSeq2Seq](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚åœ¨æ•´ç†è¿‡ç¨‹ä¸­ï¼Œå°†å¥å­åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: PytorchHide Pytorch content
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—Pytorchå†…å®¹
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: TensorFlowHide TensorFlow content
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluate
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: 'Including a metric during training is often helpful for evaluating your modelâ€™s
    performance. You can quickly load a evaluation method with the ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge)
    metric (see the ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªæŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼ŒåŠ è½½[ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge)æŒ‡æ ‡ï¼ˆæŸ¥çœ‹ğŸ¤—
    Evaluate [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡ï¼‰ï¼š
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then create a function that passes your predictions and labels to `compute`
    to calculate the ROUGE metric:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™`compute`ä»¥è®¡ç®—ROUGEæŒ‡æ ‡ï¼š
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Your `compute_metrics` function is ready to go now, and youâ€™ll return to it
    when you setup your training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„`compute_metrics`å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶ä¼šè¿”å›åˆ°å®ƒã€‚
- en: Train
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: PytorchHide Pytorch content
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—Pytorchå†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[è¿™é‡Œ](../training#train-with-pytorch-trainer)ï¼
- en: 'Youâ€™re ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[AutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM)åŠ è½½T5ï¼š
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, only three steps remain:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: Define your training hyperparameters in [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. Youâ€™ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model). At the end of each epoch,
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the ROUGE metric and save the training checkpoint.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼Œå®ƒæŒ‡å®šäº†ä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆæ‚¨éœ€è¦ç™»å½•Hugging
    Faceæ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªepochç»“æŸæ—¶ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è¯„ä¼°ROUGEæŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
- en: Pass the training arguments to [Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)ï¼ŒåŒæ—¶è¿˜è¦ä¼ é€’æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: TensorFlowHide TensorFlow content
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with Keras, take a look at the
    basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨Keraså¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[è¿™é‡Œ](../training#train-a-tensorflow-model-with-keras)ï¼
- en: 'To finetune a model in TensorFlow, start by setting up an optimizer function,
    learning rate schedule, and some training hyperparameters:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¾®è°ƒTensorFlowæ¨¡å‹ï¼Œé¦–å…ˆè®¾ç½®ä¸€ä¸ªä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è°ƒåº¦å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then you can load T5 with [TFAutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨[TFAutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM)åŠ è½½T5ï¼š
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Convert your datasets to the `tf.data.Dataset` format with [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method).
    Note that Transformers models all have a default task-relevant loss function,
    so you donâ€™t need to specify one unless you want to:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ä¸ºè®­ç»ƒé…ç½®æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒTransformersæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦ï¼š
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The last two things to setup before you start training is to compute the ROUGE
    score from the predictions, and provide a way to push your model to the Hub. Both
    are done by using [Keras callbacks](../main_classes/keras_callbacks).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œè¿˜æœ‰æœ€åä¸¤ä»¶äº‹è¦è®¾ç½®ï¼šä»é¢„æµ‹ä¸­è®¡ç®—ROUGEåˆ†æ•°ï¼Œå¹¶æä¾›ä¸€ç§å°†æ¨¡å‹æ¨é€åˆ°Hubçš„æ–¹æ³•ã€‚è¿™ä¸¤ä¸ªéƒ½å¯ä»¥é€šè¿‡ä½¿ç”¨[Keraså›è°ƒ](../main_classes/keras_callbacks)æ¥å®Œæˆã€‚
- en: 'Pass your `compute_metrics` function to [KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„`compute_metrics`å‡½æ•°ä¼ é€’ç»™[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)ï¼š
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Specify where to push your model and tokenizer in the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)ä¸­æŒ‡å®šè¦æ¨é€æ¨¡å‹å’Œåˆ†è¯å™¨çš„ä½ç½®ï¼š
- en: '[PRE20]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then bundle your callbacks together:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†æ‚¨çš„å›è°ƒæ†ç»‘åœ¨ä¸€èµ·ï¼š
- en: '[PRE21]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, youâ€™re ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)
    with your training and validation datasets, the number of epochs, and your callbacks
    to finetune the model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€epochæ•°é‡å’Œå›è°ƒå‡½æ•°è°ƒç”¨[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)æ¥å¾®è°ƒæ¨¡å‹ï¼š
- en: '[PRE22]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once training is completed, your model is automatically uploaded to the Hub
    so everyone can use it!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæ‚¨çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼
- en: For a more in-depth example of how to finetune a model for summarization, take
    a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)
    or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å¦‚ä½•ä¸ºæ‘˜è¦å¾®è°ƒæ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„ [PyTorch ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)
    æˆ– [TensorFlow ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)ã€‚
- en: Inference
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Great, now that youâ€™ve finetuned a model, you can use it for inference!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥ç”¨äºæ¨ç†äº†ï¼
- en: 'Come up with some text youâ€™d like to summarize. For T5, you need to prefix
    your input depending on the task youâ€™re working on. For summarization you should
    prefix your input as shown below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³å‡ºä¸€äº›æ‚¨æƒ³è¦æ€»ç»“çš„æ–‡æœ¬ã€‚å¯¹äº T5ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‚¨æ­£åœ¨å¤„ç†çš„ä»»åŠ¡ä¸ºè¾“å…¥æ·»åŠ å‰ç¼€ã€‚å¯¹äºæ‘˜è¦ï¼Œæ‚¨åº”è¯¥åƒä¸‹é¢æ‰€ç¤ºä¸ºè¾“å…¥æ·»åŠ å‰ç¼€ï¼š
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for summarization with your model, and pass your text
    to it:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨ [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    ä¸­ä½¿ç”¨å®ƒã€‚ä¸ºæ‘˜è¦å®ä¾‹åŒ–ä¸€ä¸ªå¸¦æœ‰æ‚¨çš„æ¨¡å‹çš„ `pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š
- en: '[PRE24]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can also manually replicate the results of the `pipeline` if youâ€™d like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š
- en: PytorchHide Pytorch content
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè— Pytorch å†…å®¹
- en: 'Tokenize the text and return the `input_ids` as PyTorch tensors:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°† `input_ids` è¿”å›ä¸º PyTorch å¼ é‡ï¼š
- en: '[PRE25]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Use the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method to create the summarization. For more details about the different text
    generation strategies and parameters for controlling generation, check out the
    [Text Generation](../main_classes/text_generation) API.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    æ–¹æ³•æ¥åˆ›å»ºæ‘˜è¦ã€‚æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [Text Generation](../main_classes/text_generation)
    APIã€‚
- en: '[PRE26]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Decode the generated token ids back into text:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç”Ÿæˆçš„æ ‡è®° id è§£ç å›æ–‡æœ¬ï¼š
- en: '[PRE27]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: TensorFlowHide TensorFlow content
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè— TensorFlow å†…å®¹
- en: 'Tokenize the text and return the `input_ids` as TensorFlow tensors:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°† `input_ids` è¿”å›ä¸º TensorFlow å¼ é‡ï¼š
- en: '[PRE28]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Use the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)
    method to create the summarization. For more details about the different text
    generation strategies and parameters for controlling generation, check out the
    [Text Generation](../main_classes/text_generation) API.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)
    æ–¹æ³•æ¥åˆ›å»ºæ‘˜è¦ã€‚æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [Text Generation](../main_classes/text_generation)
    APIã€‚
- en: '[PRE29]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Decode the generated token ids back into text:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç”Ÿæˆçš„æ ‡è®° id è§£ç å›æ–‡æœ¬ï¼š
- en: '[PRE30]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
