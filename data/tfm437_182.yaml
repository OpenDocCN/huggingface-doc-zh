- en: GPT-Sw3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-Sw3
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The GPT-Sw3 model was first proposed in [Lessons Learned from GPT-SW3: Building
    the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf)
    by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine
    Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT-Sw3 模型首次提出于 [Lessons Learned from GPT-SW3: Building the First Large-Scale
    Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf)
    由 Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine
    Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren 撰写。'
- en: Since that first paper the authors have extended their work and trained new
    models on their new 1.2TB corpora named The Nordic Pile.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自第一篇论文以来，作者已经扩展了他们的工作，并在他们的新 1.2TB 语料库上训练了新模型，名为 The Nordic Pile。
- en: GPT-Sw3 is a collection of large decoder-only pretrained transformer language
    models that were developed by AI Sweden in collaboration with RISE and the WASP
    WARA for Media and Language. GPT-Sw3 has been trained on a dataset containing
    320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming
    code. The model was pretrained using a causal language modeling (CLM) objective
    utilizing the NeMo Megatron GPT implementation.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-Sw3 是由 AI Sweden 与 RISE 和 WASP WARA for Media and Language 合作开发的一组大型仅解码器预训练的变压器语言模型。GPT-Sw3
    在包含 320B 个瑞典语、挪威语、丹麦语、冰岛语、英语和编程代码的数据集上进行了训练。该模型使用了因果语言建模（CLM）目标进行预训练，利用了 NeMo
    Megatron GPT 实现。
- en: This model was contributed by [AI Sweden Models](https://huggingface.co/AI-Sweden-Models).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由 [AI Sweden Models](https://huggingface.co/AI-Sweden-Models) 贡献。
- en: Usage example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示例
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标记分类任务指南](../tasks/token_classification)'
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[因果语言建模任务指南](../tasks/language_modeling)'
- en: The implementation uses the `GPT2Model` coupled with our `GPTSw3Tokenizer`.
    Refer to [GPT2Model documentation](gpt2) for API reference and examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现使用 `GPT2Model` 与我们的 `GPTSw3Tokenizer` 结合。请参考 [GPT2Model 文档](gpt2) 获取 API
    参考和示例。
- en: Note that sentencepiece is required to use our tokenizer and can be installed
    with `pip install transformers[sentencepiece]` or `pip install sentencepiece`
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用我们的标记器需要 sentencepiece，并可通过 `pip install transformers[sentencepiece]`
    或 `pip install sentencepiece` 进行安装。
- en: GPTSw3Tokenizer
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTSw3Tokenizer
- en: '### `class transformers.GPTSw3Tokenizer`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GPTSw3Tokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L45)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L45)'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含实例化标记器所需词汇表的 [SentencePiece](https://github.com/google/sentencepiece)
    文件（通常具有 *.spm* 扩展名）。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `False`) — Whether or not
    to lowercase the input when tokenizing.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *optional*, 默认为 `False`) — 在标记化时是否将输入转换为小写。'
- en: '`remove_space` (`bool`, *optional*, defaults to `False`) — Whether or not to
    strip the text when tokenizing (removing excess spaces before and after the string).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_space` (`bool`, *optional*, 默认为 `False`) — 在标记化时是否去除文本（删除字符串前后的多余空格）。'
- en: '`keep_accents` (`bool`, *optional*, defaults to `False`) — Whether or not to
    keep accents when tokenizing.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_accents` (`bool`, *optional*, 默认为 `False`) — 在标记化时是否保留重音。'
- en: '`pad_token` (`str`, *optional*) — The token used for padding, for example when
    batching sequences of different lengths. If not provided, will default to ’<pad>’
    or ’<unk>’ depending on model size.</unk></pad>'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*) — 用于填充的标记，例如在批处理不同长度的序列时使用。如果未提供，默认为 ’<pad>’
    或 ’<unk>’，取决于模型大小。</unk></pad>'
- en: '`unk_token` (`str`, *optional*) — The unknown token. A token that is not in
    the vocabulary cannot be converted to an ID and is set to be this token instead.
    If not provided, will default to ’<unk>‘.</unk>'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。如果未提供，默认为
    ’<unk>‘。</unk>'
- en: '`eos_token` (`str`, *optional*) — The end of sequence token seen during pretraining.
    If not provided, will default to ’<|endoftext|>’'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*）--在预训练过程中看到的序列结束标记。如果未提供，则默认为`<|endoftext|>`'
- en: '`bos_token` (`str`, *optional*) — The beginning of sequence token that can
    be used for downstream task, was not seen during pretraining. If not provided,
    will default to ’~~’ or ’<|endoftext|>’, depending on model size.~~'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*optional*）--可用于下游任务的序列标记的开头，在预训练过程中没有看到。如果未提供，将默认为`~~`或`<|endoftext|>`，具体取决于模型大小'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs` (`dict`, *optional*) — 将传递给 `SentencePieceProcessor.__init__()`
    方法的参数。[SentencePiece 的 Python 封装](https://github.com/google/sentencepiece/tree/master/python)
    可以用来设置：'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`: 启用子词规范化。'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`: unigram 的抽样参数。对于 BPE-Dropout 无效。'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`: 不执行抽样。'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`: 从 nbest_size 结果中进行抽样。'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`: 假设 nbest_size 为无限，并使用前向过滤和后向抽样算法从所有假设（格）中进行抽样。'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`: unigram 抽样的平滑参数，以及 BPE-Dropout 合并操作的丢弃概率。'
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和 ID）的 *SentencePiece*
    处理器。'
- en: '`whitespaces` (`set`) — The whitespaces that are replaced in the whitespace
    normalization in preprocessing.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`whitespaces` (`set`) — 在预处理中进行空格规范化时替换的空格。'
- en: '`non_printing_characters_re` (`Pattern`) — The compiled regular expression
    to remove non-printing characters in preprocessing.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`non_printing_characters_re` (`Pattern`) — 编译后的正则表达式，用于在预处理中删除非打印字符。'
- en: Construct an GPTSw3 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 GPTSw3 分词器。基于 [SentencePiece](https://github.com/google/sentencepiece)。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考这个超类以获取有关这些方法的更多信息。
- en: 'Example usage:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 示例用法：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `save_vocabulary`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L258)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L258)'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
