- en: List Parquet files
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ—å‡ºParquetæ–‡ä»¶
- en: 'Original text: [https://huggingface.co/docs/datasets-server/parquet](https://huggingface.co/docs/datasets-server/parquet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets-server/parquet](https://huggingface.co/docs/datasets-server/parquet)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Datasets can be published in any format (CSV, JSONL, directories of images,
    etc.) to the Hub, and they are easily accessed with the ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/)
    library. For a more performant experience (especially when it comes to large datasets),
    Datasets Server automatically converts every dataset to the [Parquet](https://parquet.apache.org/)
    format.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å¯ä»¥ä»¥ä»»ä½•æ ¼å¼ï¼ˆCSVï¼ŒJSONLï¼Œå›¾åƒç›®å½•ç­‰ï¼‰å‘å¸ƒåˆ°Hubï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨ğŸ¤— [æ•°æ®é›†](https://huggingface.co/docs/datasets/)åº“è½»æ¾è®¿é—®ã€‚ä¸ºäº†è·å¾—æ›´é«˜æ€§èƒ½çš„ä½“éªŒï¼ˆç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹æ•°æ®é›†ï¼‰ï¼Œæ•°æ®é›†æœåŠ¡å™¨ä¼šè‡ªåŠ¨å°†æ¯ä¸ªæ•°æ®é›†è½¬æ¢ä¸º[Parquet](https://parquet.apache.org/)æ ¼å¼ã€‚
- en: What is Parquet?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯Parquetï¼Ÿ
- en: Parquet is a columnar storage format optimized for querying and processing large
    datasets. Parquet is a popular choice for big data processing and analytics and
    is widely used for data processing and machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Parquetæ˜¯ä¸€ç§é’ˆå¯¹æŸ¥è¯¢å’Œå¤„ç†å¤§å‹æ•°æ®é›†è¿›è¡Œä¼˜åŒ–çš„åˆ—å­˜å‚¨æ ¼å¼ã€‚Parquetæ˜¯å¤„ç†å¤§æ•°æ®å’Œåˆ†æçš„çƒ­é—¨é€‰æ‹©ï¼Œå¹¿æ³›ç”¨äºæ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ ã€‚
- en: In Parquet, data is divided into chunks called â€œrow groupsâ€, and within each
    row group, it is stored in columns rather than rows. Each row group column is
    compressed separately using the best compression algorithm depending on the data,
    and contains metadata and statistics (min/max value, number of NULL values) about
    the data it contains.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Parquetä¸­ï¼Œæ•°æ®è¢«åˆ†æˆç§°ä¸ºâ€œè¡Œç»„â€çš„å—ï¼Œæ¯ä¸ªè¡Œç»„ä¸­çš„æ•°æ®ä»¥åˆ—è€Œä¸æ˜¯è¡Œçš„å½¢å¼å­˜å‚¨ã€‚æ¯ä¸ªè¡Œç»„åˆ—ä½¿ç”¨æœ€ä½³çš„å‹ç¼©ç®—æ³•å•ç‹¬å‹ç¼©ï¼ŒåŒ…å«æœ‰å…³å…¶åŒ…å«çš„æ•°æ®çš„å…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯ï¼ˆæœ€å°/æœ€å¤§å€¼ï¼ŒNULLå€¼çš„æ•°é‡ï¼‰ã€‚
- en: 'This structure allows for efficient data reading and querying:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç»“æ„å…è®¸é«˜æ•ˆåœ°è¯»å–å’ŒæŸ¥è¯¢æ•°æ®ï¼š
- en: only the necessary columns are read from disk (projection pushdown); no need
    to read the entire file. This reduces the memory requirement for working with
    Parquet data.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªéœ€ä»ç£ç›˜è¯»å–å¿…è¦çš„åˆ—ï¼ˆæŠ•å½±ä¸‹æ¨ï¼‰ï¼›æ— éœ€è¯»å–æ•´ä¸ªæ–‡ä»¶ã€‚è¿™å‡å°‘äº†å¤„ç†Parquetæ•°æ®æ‰€éœ€çš„å†…å­˜ã€‚
- en: entire row groups are skipped if the statistics stored in its metadata do not
    match the data of interest (automatic filtering)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå…¶å…ƒæ•°æ®ä¸­å­˜å‚¨çš„ç»Ÿè®¡æ•°æ®ä¸æ„Ÿå…´è¶£çš„æ•°æ®ä¸åŒ¹é…ï¼Œåˆ™å°†è·³è¿‡æ•´ä¸ªè¡Œç»„ï¼ˆè‡ªåŠ¨è¿‡æ»¤ï¼‰
- en: the data is compressed, which reduces the amount of data that needs to be stored
    and transferred.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®è¢«å‹ç¼©ï¼Œä»è€Œå‡å°‘äº†éœ€è¦å­˜å‚¨å’Œä¼ è¾“çš„æ•°æ®é‡ã€‚
- en: A Parquet file contains a single table. If a dataset has multiple tables (e.g.
    multiple splits or configurations), each table is stored in a separate Parquet
    file.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Parquetæ–‡ä»¶åŒ…å«ä¸€ä¸ªå•ç‹¬çš„è¡¨ã€‚å¦‚æœæ•°æ®é›†æœ‰å¤šä¸ªè¡¨ï¼ˆä¾‹å¦‚å¤šä¸ªæ‹†åˆ†æˆ–é…ç½®ï¼‰ï¼Œåˆ™æ¯ä¸ªè¡¨å­˜å‚¨åœ¨å•ç‹¬çš„Parquetæ–‡ä»¶ä¸­ã€‚
- en: Conversion to Parquet
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è½¬æ¢ä¸ºParquet
- en: The Parquet files are published to the Hub on a specific `refs/convert/parquet`
    branch (like this `amazon_polarity` [branch](https://huggingface.co/datasets/amazon_polarity/tree/refs%2Fconvert%2Fparquet)
    for example) that parallels the `main` branch.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Parquetæ–‡ä»¶å‘å¸ƒåˆ°Hubçš„ç‰¹å®š`refs/convert/parquet`åˆ†æ”¯ï¼ˆä¾‹å¦‚`amazon_polarity` [åˆ†æ”¯](https://huggingface.co/datasets/amazon_polarity/tree/refs%2Fconvert%2Fparquet)ï¼‰ä¸`main`åˆ†æ”¯å¹³è¡Œã€‚
- en: In order for Datasets Server to generate a Parquet version of a dataset, the
    dataset must be *public*, or owned by a [PRO user](https://huggingface.co/pricing)
    or an [Enterprise Hub organization](https://huggingface.co/enterprise).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æ•°æ®é›†æœåŠ¡å™¨ç”Ÿæˆæ•°æ®é›†çš„Parquetç‰ˆæœ¬ï¼Œæ•°æ®é›†å¿…é¡»æ˜¯*å…¬å…±çš„*ï¼Œæˆ–è€…ç”±[PROç”¨æˆ·](https://huggingface.co/pricing)æˆ–[ä¼ä¸šHubç»„ç»‡](https://huggingface.co/enterprise)æ‹¥æœ‰ã€‚
- en: Using the Datasets Server API
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ•°æ®é›†æœåŠ¡å™¨API
- en: This guide shows you how to use Datasets Serverâ€™s `/parquet` endpoint to retrieve
    a list of a datasetâ€™s files converted to Parquet. Feel free to also try it out
    with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8),
    [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api),
    or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®é›†æœåŠ¡å™¨çš„`/parquet`ç«¯ç‚¹æ£€ç´¢è½¬æ¢ä¸ºParquetçš„æ•°æ®é›†æ–‡ä»¶åˆ—è¡¨ã€‚ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨[Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8)ï¼Œ[RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api)æˆ–[ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)ã€‚
- en: 'The `/parquet` endpoint accepts the dataset name as its query parameter:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`/parquet`ç«¯ç‚¹æ¥å—æ•°æ®é›†åç§°ä½œä¸ºå…¶æŸ¥è¯¢å‚æ•°ï¼š'
- en: PythonJavaScriptcURL
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The endpoint response is a JSON containing a list of the datasetâ€™s files in
    the Parquet format. For example, the [`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)
    dataset has six Parquet files, which corresponds to the `test`, `train` and `validation`
    splits of its two configurations, `ParaphraseRC` and `SelfRC` (see the [List splits
    and configurations](./splits) guide for more details about splits and configurations).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç«¯ç‚¹å“åº”æ˜¯ä¸€ä¸ªJSONï¼ŒåŒ…å«æ•°æ®é›†æ–‡ä»¶çš„Parquetæ ¼å¼åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œ[`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)æ•°æ®é›†æœ‰å…­ä¸ªParquetæ–‡ä»¶ï¼Œå¯¹åº”äºå…¶ä¸¤ä¸ªé…ç½®`ParaphraseRC`å’Œ`SelfRC`çš„`test`ï¼Œ`train`å’Œ`validation`æ‹†åˆ†ï¼ˆæœ‰å…³æ‹†åˆ†å’Œé…ç½®çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[åˆ—å‡ºæ‹†åˆ†å’Œé…ç½®](./splits)æŒ‡å—ï¼‰ã€‚
- en: 'The endpoint also gives the filename and size of each file:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç«¯ç‚¹è¿˜æä¾›æ¯ä¸ªæ–‡ä»¶çš„æ–‡ä»¶åå’Œå¤§å°ï¼š
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sharded Parquet files
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡çš„Parquetæ–‡ä»¶
- en: 'Big datasets are partitioned into Parquet files (shards) of about 500MB each.
    The filename contains the name of the dataset, the split, the shard index, and
    the total number of shards (`dataset-name-train-0000-of-0004.parquet`). For a
    given split, the elements in the list are sorted by their shard index, in ascending
    order. For example, the `train` split of the [`amazon_polarity`](https://datasets-server.huggingface.co/parquet?dataset=amazon_polarity)
    dataset is partitioned into 4 shards:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹æ•°æ®é›†è¢«åˆ†æˆå¤§çº¦500MBçš„Parquetæ–‡ä»¶ï¼ˆåˆ†ç‰‡ï¼‰ã€‚æ–‡ä»¶ååŒ…å«æ•°æ®é›†çš„åç§°ï¼Œæ‹†åˆ†ï¼Œåˆ†ç‰‡ç´¢å¼•å’Œæ€»åˆ†ç‰‡æ•°ï¼ˆ`dataset-name-train-0000-of-0004.parquet`ï¼‰ã€‚å¯¹äºç»™å®šçš„æ‹†åˆ†ï¼Œåˆ—è¡¨ä¸­çš„å…ƒç´ æŒ‰å…¶åˆ†ç‰‡ç´¢å¼•æŒ‰å‡åºæ’åºã€‚ä¾‹å¦‚ï¼Œ[`amazon_polarity`](https://datasets-server.huggingface.co/parquet?dataset=amazon_polarity)æ•°æ®é›†çš„`train`æ‹†åˆ†è¢«åˆ†æˆ4ä¸ªåˆ†ç‰‡ï¼š
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To read and query the Parquet files, take a look at the [Query datasets from
    Datasets Server](parquet_process) guide.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¯»å–å’ŒæŸ¥è¯¢Parquetæ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹[ä»æ•°æ®é›†æœåŠ¡å™¨æŸ¥è¯¢æ•°æ®é›†](parquet_process)æŒ‡å—ã€‚
- en: Partially converted datasets
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éƒ¨åˆ†è½¬æ¢çš„æ•°æ®é›†
- en: The Parquet version can be partial if the dataset is not already in Parquet
    format or if it is bigger than 5GB.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ•°æ®é›†å°šæœªä»¥Parquetæ ¼å¼å­˜åœ¨ï¼Œæˆ–è€…æ•°æ®é›†å¤§äº5GBï¼Œåˆ™Parquetç‰ˆæœ¬å¯èƒ½æ˜¯éƒ¨åˆ†çš„ã€‚
- en: In that case the Parquet files are generated up to 5GB and placed in a split
    directory prefixed with â€œpartialâ€, e.g. â€œpartial-trainâ€ instead of â€œtrainâ€.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒParquetæ–‡ä»¶ç”Ÿæˆçš„å¤§å°ä¸º5GBï¼Œå¹¶æ”¾ç½®åœ¨ä»¥â€œpartialâ€ä¸ºå‰ç¼€çš„åˆ†å‰²ç›®å½•ä¸­ï¼Œä¾‹å¦‚â€œpartial-trainâ€è€Œä¸æ˜¯â€œtrainâ€ã€‚
- en: Parquet-native datasets
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquetæœ¬åœ°æ•°æ®é›†
- en: 'When the dataset is already in Parquet format, the data are not converted and
    the files in `refs/convert/parquet` are links to the original files. This rule
    suffers an exception to ensure the Datasets Server API to stay fast: if the [row
    group](https://parquet.apache.org/docs/concepts/) size of the original Parquet
    files is too big, new Parquet files are generated.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ•°æ®é›†å·²ç»ä»¥Parquetæ ¼å¼å­˜åœ¨æ—¶ï¼Œæ•°æ®ä¸ä¼šè¢«è½¬æ¢ï¼Œ`refs/convert/parquet`ä¸­çš„æ–‡ä»¶æ˜¯æŒ‡å‘åŸå§‹æ–‡ä»¶çš„é“¾æ¥ã€‚è¿™ä¸ªè§„åˆ™æœ‰ä¸€ä¸ªä¾‹å¤–ï¼Œä»¥ç¡®ä¿æ•°æ®é›†æœåŠ¡å™¨APIä¿æŒå¿«é€Ÿï¼šå¦‚æœåŸå§‹Parquetæ–‡ä»¶çš„[row
    group](https://parquet.apache.org/docs/concepts/)å¤§å°å¤ªå¤§ï¼Œå°†ç”Ÿæˆæ–°çš„Parquetæ–‡ä»¶ã€‚
- en: Using the Hugging Face Hub API
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Hugging Face Hub API
- en: 'For convenience, you can directly use the Hugging Face Hub `/api/parquet` endpoint
    which returns the list of Parquet URLs:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨Hugging Face Hubçš„`/api/parquet`ç«¯ç‚¹ï¼Œè¯¥ç«¯ç‚¹è¿”å›Parquet URLåˆ—è¡¨ï¼š
- en: PythonJavaScriptcURL
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The endpoint response is a JSON containing a list of the datasetâ€™s files URLs
    in the Parquet format for each split and configuration. For example, the [`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)
    dataset has one Parquet file for the train split of the â€œParaphraseRCâ€ configuration
    (see the [List splits and configurations](./splits) guide for more details about
    splits and configurations).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç«¯ç‚¹å“åº”æ˜¯ä¸€ä¸ªJSONï¼ŒåŒ…å«æ•°æ®é›†æ–‡ä»¶URLåˆ—è¡¨ï¼Œä»¥Parquetæ ¼å¼æä¾›æ¯ä¸ªåˆ†å‰²å’Œé…ç½®ã€‚ä¾‹å¦‚ï¼Œ[`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)
    æ•°æ®é›†å¯¹äºâ€œParaphraseRCâ€é…ç½®çš„è®­ç»ƒåˆ†å‰²æœ‰ä¸€ä¸ªParquetæ–‡ä»¶ï¼ˆæœ‰å…³åˆ†å‰²å’Œé…ç½®çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[åˆ—å‡ºåˆ†å‰²å’Œé…ç½®](./splits)æŒ‡å—ï¼‰ã€‚
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Optionally you can specify which configuration name to return, as well as which
    split:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥æŒ‡å®šè¦è¿”å›çš„é…ç½®åç§°ï¼Œä»¥åŠè¦è¿”å›çš„åˆ†å‰²ï¼š
- en: PythonJavaScriptcURL
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Each parquet file can also be accessed using its shard index: `https://huggingface.co/api/datasets/ibm/duorc/parquet/ParaphraseRC/train/0.parquet`
    redirects to `https://huggingface.co/datasets/ibm/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet`
    for example.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªParquetæ–‡ä»¶ä¹Ÿå¯ä»¥é€šè¿‡å…¶åˆ†ç‰‡ç´¢å¼•è¿›è¡Œè®¿é—®ï¼š`https://huggingface.co/api/datasets/ibm/duorc/parquet/ParaphraseRC/train/0.parquet`
    é‡å®šå‘åˆ° `https://huggingface.co/datasets/ibm/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet`
    ä¾‹å¦‚ã€‚
