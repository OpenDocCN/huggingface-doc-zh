- en: List Parquet files
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列出Parquet文件
- en: 'Original text: [https://huggingface.co/docs/datasets-server/parquet](https://huggingface.co/docs/datasets-server/parquet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/datasets-server/parquet](https://huggingface.co/docs/datasets-server/parquet)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Datasets can be published in any format (CSV, JSONL, directories of images,
    etc.) to the Hub, and they are easily accessed with the 🤗 [Datasets](https://huggingface.co/docs/datasets/)
    library. For a more performant experience (especially when it comes to large datasets),
    Datasets Server automatically converts every dataset to the [Parquet](https://parquet.apache.org/)
    format.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以以任何格式（CSV，JSONL，图像目录等）发布到Hub，并且可以使用🤗 [数据集](https://huggingface.co/docs/datasets/)库轻松访问。为了获得更高性能的体验（特别是对于大型数据集），数据集服务器会自动将每个数据集转换为[Parquet](https://parquet.apache.org/)格式。
- en: What is Parquet?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Parquet？
- en: Parquet is a columnar storage format optimized for querying and processing large
    datasets. Parquet is a popular choice for big data processing and analytics and
    is widely used for data processing and machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet是一种针对查询和处理大型数据集进行优化的列存储格式。Parquet是处理大数据和分析的热门选择，广泛用于数据处理和机器学习。
- en: In Parquet, data is divided into chunks called “row groups”, and within each
    row group, it is stored in columns rather than rows. Each row group column is
    compressed separately using the best compression algorithm depending on the data,
    and contains metadata and statistics (min/max value, number of NULL values) about
    the data it contains.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在Parquet中，数据被分成称为“行组”的块，每个行组中的数据以列而不是行的形式存储。每个行组列使用最佳的压缩算法单独压缩，包含有关其包含的数据的元数据和统计信息（最小/最大值，NULL值的数量）。
- en: 'This structure allows for efficient data reading and querying:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构允许高效地读取和查询数据：
- en: only the necessary columns are read from disk (projection pushdown); no need
    to read the entire file. This reduces the memory requirement for working with
    Parquet data.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需从磁盘读取必要的列（投影下推）；无需读取整个文件。这减少了处理Parquet数据所需的内存。
- en: entire row groups are skipped if the statistics stored in its metadata do not
    match the data of interest (automatic filtering)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果其元数据中存储的统计数据与感兴趣的数据不匹配，则将跳过整个行组（自动过滤）
- en: the data is compressed, which reduces the amount of data that needs to be stored
    and transferred.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被压缩，从而减少了需要存储和传输的数据量。
- en: A Parquet file contains a single table. If a dataset has multiple tables (e.g.
    multiple splits or configurations), each table is stored in a separate Parquet
    file.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet文件包含一个单独的表。如果数据集有多个表（例如多个拆分或配置），则每个表存储在单独的Parquet文件中。
- en: Conversion to Parquet
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换为Parquet
- en: The Parquet files are published to the Hub on a specific `refs/convert/parquet`
    branch (like this `amazon_polarity` [branch](https://huggingface.co/datasets/amazon_polarity/tree/refs%2Fconvert%2Fparquet)
    for example) that parallels the `main` branch.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet文件发布到Hub的特定`refs/convert/parquet`分支（例如`amazon_polarity` [分支](https://huggingface.co/datasets/amazon_polarity/tree/refs%2Fconvert%2Fparquet)）与`main`分支平行。
- en: In order for Datasets Server to generate a Parquet version of a dataset, the
    dataset must be *public*, or owned by a [PRO user](https://huggingface.co/pricing)
    or an [Enterprise Hub organization](https://huggingface.co/enterprise).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据集服务器生成数据集的Parquet版本，数据集必须是*公共的*，或者由[PRO用户](https://huggingface.co/pricing)或[企业Hub组织](https://huggingface.co/enterprise)拥有。
- en: Using the Datasets Server API
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据集服务器API
- en: This guide shows you how to use Datasets Server’s `/parquet` endpoint to retrieve
    a list of a dataset’s files converted to Parquet. Feel free to also try it out
    with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8),
    [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api),
    or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南向您展示如何使用数据集服务器的`/parquet`端点检索转换为Parquet的数据集文件列表。也可以尝试使用[Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8)，[RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api)或[ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)。
- en: 'The `/parquet` endpoint accepts the dataset name as its query parameter:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`/parquet`端点接受数据集名称作为其查询参数：'
- en: PythonJavaScriptcURL
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The endpoint response is a JSON containing a list of the dataset’s files in
    the Parquet format. For example, the [`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)
    dataset has six Parquet files, which corresponds to the `test`, `train` and `validation`
    splits of its two configurations, `ParaphraseRC` and `SelfRC` (see the [List splits
    and configurations](./splits) guide for more details about splits and configurations).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 端点响应是一个JSON，包含数据集文件的Parquet格式列表。例如，[`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)数据集有六个Parquet文件，对应于其两个配置`ParaphraseRC`和`SelfRC`的`test`，`train`和`validation`拆分（有关拆分和配置的更多详细信息，请参阅[列出拆分和配置](./splits)指南）。
- en: 'The endpoint also gives the filename and size of each file:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 端点还提供每个文件的文件名和大小：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sharded Parquet files
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分片的Parquet文件
- en: 'Big datasets are partitioned into Parquet files (shards) of about 500MB each.
    The filename contains the name of the dataset, the split, the shard index, and
    the total number of shards (`dataset-name-train-0000-of-0004.parquet`). For a
    given split, the elements in the list are sorted by their shard index, in ascending
    order. For example, the `train` split of the [`amazon_polarity`](https://datasets-server.huggingface.co/parquet?dataset=amazon_polarity)
    dataset is partitioned into 4 shards:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大型数据集被分成大约500MB的Parquet文件（分片）。文件名包含数据集的名称，拆分，分片索引和总分片数（`dataset-name-train-0000-of-0004.parquet`）。对于给定的拆分，列表中的元素按其分片索引按升序排序。例如，[`amazon_polarity`](https://datasets-server.huggingface.co/parquet?dataset=amazon_polarity)数据集的`train`拆分被分成4个分片：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To read and query the Parquet files, take a look at the [Query datasets from
    Datasets Server](parquet_process) guide.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取和查询Parquet文件，请查看[从数据集服务器查询数据集](parquet_process)指南。
- en: Partially converted datasets
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分转换的数据集
- en: The Parquet version can be partial if the dataset is not already in Parquet
    format or if it is bigger than 5GB.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集尚未以Parquet格式存在，或者数据集大于5GB，则Parquet版本可能是部分的。
- en: In that case the Parquet files are generated up to 5GB and placed in a split
    directory prefixed with “partial”, e.g. “partial-train” instead of “train”.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Parquet文件生成的大小为5GB，并放置在以“partial”为前缀的分割目录中，例如“partial-train”而不是“train”。
- en: Parquet-native datasets
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet本地数据集
- en: 'When the dataset is already in Parquet format, the data are not converted and
    the files in `refs/convert/parquet` are links to the original files. This rule
    suffers an exception to ensure the Datasets Server API to stay fast: if the [row
    group](https://parquet.apache.org/docs/concepts/) size of the original Parquet
    files is too big, new Parquet files are generated.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集已经以Parquet格式存在时，数据不会被转换，`refs/convert/parquet`中的文件是指向原始文件的链接。这个规则有一个例外，以确保数据集服务器API保持快速：如果原始Parquet文件的[row
    group](https://parquet.apache.org/docs/concepts/)大小太大，将生成新的Parquet文件。
- en: Using the Hugging Face Hub API
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hugging Face Hub API
- en: 'For convenience, you can directly use the Hugging Face Hub `/api/parquet` endpoint
    which returns the list of Parquet URLs:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，您可以直接使用Hugging Face Hub的`/api/parquet`端点，该端点返回Parquet URL列表：
- en: PythonJavaScriptcURL
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The endpoint response is a JSON containing a list of the dataset’s files URLs
    in the Parquet format for each split and configuration. For example, the [`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)
    dataset has one Parquet file for the train split of the “ParaphraseRC” configuration
    (see the [List splits and configurations](./splits) guide for more details about
    splits and configurations).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 端点响应是一个JSON，包含数据集文件URL列表，以Parquet格式提供每个分割和配置。例如，[`ibm/duorc`](https://huggingface.co/datasets/ibm/duorc)
    数据集对于“ParaphraseRC”配置的训练分割有一个Parquet文件（有关分割和配置的更多详细信息，请参阅[列出分割和配置](./splits)指南）。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Optionally you can specify which configuration name to return, as well as which
    split:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，您可以指定要返回的配置名称，以及要返回的分割：
- en: PythonJavaScriptcURL
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PythonJavaScriptcURL
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Each parquet file can also be accessed using its shard index: `https://huggingface.co/api/datasets/ibm/duorc/parquet/ParaphraseRC/train/0.parquet`
    redirects to `https://huggingface.co/datasets/ibm/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet`
    for example.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Parquet文件也可以通过其分片索引进行访问：`https://huggingface.co/api/datasets/ibm/duorc/parquet/ParaphraseRC/train/0.parquet`
    重定向到 `https://huggingface.co/datasets/ibm/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet`
    例如。
