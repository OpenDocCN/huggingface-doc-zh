- en: Create a dataset loading script
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ•°æ®é›†åŠ è½½è„šæœ¬
- en: 'Original text: [https://huggingface.co/docs/datasets/dataset_script](https://huggingface.co/docs/datasets/dataset_script)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/dataset_script](https://huggingface.co/docs/datasets/dataset_script)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset loading script is likely not needed if your dataset is in one of
    the following formats: CSV, JSON, JSON lines, text, images, audio or Parquet.
    With those formats, you should be able to load your dataset automatically with
    [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset),
    as long as your dataset repository has a [required structure](./repository_structure).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ•°æ®é›†æ˜¯ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ï¼Œåˆ™å¯èƒ½ä¸éœ€è¦æ•°æ®é›†åŠ è½½è„šæœ¬ï¼šCSVã€JSONã€JSON linesã€æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘æˆ–Parquetã€‚å¯¹äºè¿™äº›æ ¼å¼ï¼Œåªè¦æ‚¨çš„æ•°æ®é›†å­˜å‚¨åº“å…·æœ‰[æ‰€éœ€ç»“æ„](./repository_structure)ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)è‡ªåŠ¨åŠ è½½æ•°æ®é›†ã€‚
- en: In the next major release, the new safety features of ğŸ¤— Datasets will disable
    running dataset loading scripts by default, and you will have to pass `trust_remote_code=True`
    to load datasets that require running a dataset script.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªä¸»è¦ç‰ˆæœ¬ä¸­ï¼ŒğŸ¤— Datasetsçš„æ–°å®‰å…¨åŠŸèƒ½å°†é»˜è®¤ç¦ç”¨è¿è¡Œæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œå¹¶ä¸”æ‚¨å°†éœ€è¦ä¼ é€’`trust_remote_code=True`æ¥åŠ è½½éœ€è¦è¿è¡Œæ•°æ®é›†è„šæœ¬çš„æ•°æ®é›†ã€‚
- en: Write a dataset script to load and share datasets that consist of data files
    in unsupported formats or require more complex data preparation. This is a more
    advanced way to define a dataset than using [YAML metadata in the dataset card](./repository_structure#define-your-splits-in-yaml).
    A dataset script is a Python file that defines the different configurations and
    splits of your dataset, as well as how to download and process the data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–å†™æ•°æ®é›†è„šæœ¬ä»¥åŠ è½½å’Œå…±äº«ç”±ä¸å—æ”¯æŒçš„æ ¼å¼ä¸­çš„æ•°æ®æ–‡ä»¶ç»„æˆæˆ–éœ€è¦æ›´å¤æ‚æ•°æ®å‡†å¤‡çš„æ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ç§æ¯”ä½¿ç”¨[æ•°æ®é›†å¡ä¸­çš„YAMLå…ƒæ•°æ®](./repository_structure#define-your-splits-in-yaml)æ›´é«˜çº§çš„å®šä¹‰æ•°æ®é›†çš„æ–¹å¼ã€‚æ•°æ®é›†è„šæœ¬æ˜¯ä¸€ä¸ªå®šä¹‰æ•°æ®é›†çš„ä¸åŒé…ç½®å’Œæ‹†åˆ†ä»¥åŠå¦‚ä½•ä¸‹è½½å’Œå¤„ç†æ•°æ®çš„Pythonæ–‡ä»¶ã€‚
- en: The script can download data files from any website, or from the same dataset
    repository.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬å¯ä»¥ä»ä»»ä½•ç½‘ç«™æˆ–ç›¸åŒçš„æ•°æ®é›†å­˜å‚¨åº“ä¸‹è½½æ•°æ®æ–‡ä»¶ã€‚
- en: 'A dataset loading script should have the same name as a dataset repository
    or directory. For example, a repository named `my_dataset` should contain `my_dataset.py`
    script. This way it can be loaded with:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†åŠ è½½è„šæœ¬åº”è¯¥ä¸æ•°æ®é›†å­˜å‚¨åº“æˆ–ç›®å½•åŒåã€‚ä¾‹å¦‚ï¼Œåä¸º`my_dataset`çš„å­˜å‚¨åº“åº”åŒ…å«`my_dataset.py`è„šæœ¬ã€‚è¿™æ ·å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½ï¼š
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following guide includes instructions for dataset scripts for how to:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æŒ‡å—åŒ…æ‹¬æœ‰å…³æ•°æ®é›†è„šæœ¬çš„è¯´æ˜ï¼š
- en: Add dataset metadata.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®ã€‚
- en: Download data files.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ•°æ®æ–‡ä»¶ã€‚
- en: Generate samples.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ ·æœ¬ã€‚
- en: Generate dataset metadata.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ã€‚
- en: Upload a dataset to the Hub.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hubã€‚
- en: Open the [SQuAD dataset loading script](https://huggingface.co/datasets/squad/blob/main/squad.py)
    template to follow along on how to share a dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰“å¼€[SQuADæ•°æ®é›†åŠ è½½è„šæœ¬](https://huggingface.co/datasets/squad/blob/main/squad.py)æ¨¡æ¿ï¼Œä»¥äº†è§£å¦‚ä½•å…±äº«æ•°æ®é›†ã€‚
- en: To help you get started, try beginning with the dataset loading script [template](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œè¯·å°è¯•ä»æ•°æ®é›†åŠ è½½è„šæœ¬[æ¨¡æ¿](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)å¼€å§‹ï¼
- en: Add dataset attributes
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·»åŠ æ•°æ®é›†å±æ€§
- en: 'The first step is to add some information, or attributes, about your dataset
    in `DatasetBuilder._info()`. The most important attributes you should specify
    are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯åœ¨`DatasetBuilder._info()`ä¸­æ·»åŠ å…³äºæ•°æ®é›†çš„ä¸€äº›ä¿¡æ¯æˆ–å±æ€§ã€‚æ‚¨åº”è¯¥æŒ‡å®šçš„æœ€é‡è¦çš„å±æ€§æ˜¯ï¼š
- en: '`DatasetInfo.description` provides a concise description of your dataset. The
    description informs the user whatâ€™s in the dataset, how it was collected, and
    how it can be used for a NLP task.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DatasetInfo.description`æä¾›äº†æ•°æ®é›†çš„ç®€æ´æè¿°ã€‚æè¿°å‘Šè¯‰ç”¨æˆ·æ•°æ®é›†ä¸­åŒ…å«ä»€ä¹ˆï¼Œå¦‚ä½•æ”¶é›†æ•°æ®ä»¥åŠå¦‚ä½•ç”¨äºNLPä»»åŠ¡ã€‚'
- en: '`DatasetInfo.features` defines the name and type of each column in your dataset.
    This will also provide the structure for each example, so it is possible to create
    nested subfields in a column if you want. Take a look at [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    for a full list of feature types you can use.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DatasetInfo.features`å®šä¹‰äº†æ•°æ®é›†ä¸­æ¯åˆ—çš„åç§°å’Œç±»å‹ã€‚è¿™ä¹Ÿä¸ºæ¯ä¸ªç¤ºä¾‹æä¾›äº†ç»“æ„ï¼Œå› æ­¤å¯ä»¥åœ¨åˆ—ä¸­åˆ›å»ºåµŒå¥—å­å­—æ®µã€‚æŸ¥çœ‹[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)ä»¥è·å–å¯ä»¥ä½¿ç”¨çš„ç‰¹å¾ç±»å‹çš„å®Œæ•´åˆ—è¡¨ã€‚'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`DatasetInfo.homepage` contains the URL to the dataset homepage so users can
    find more details about the dataset.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DatasetInfo.homepage`åŒ…å«æ•°æ®é›†ä¸»é¡µçš„URLï¼Œç”¨æˆ·å¯ä»¥åœ¨é‚£é‡Œæ‰¾åˆ°æœ‰å…³æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚'
- en: '`DatasetInfo.citation` contains a BibTeX citation for the dataset.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DatasetInfo.citation`åŒ…å«æ•°æ®é›†çš„BibTeXå¼•ç”¨ã€‚'
- en: 'After youâ€™ve filled out all these fields in the template, it should look like
    the following example from the SQuAD loading script:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡æ¿ä¸­å¡«å†™æ‰€æœ‰è¿™äº›å­—æ®µåï¼Œå®ƒåº”è¯¥çœ‹èµ·æ¥åƒæ¥è‡ªSQuADåŠ è½½è„šæœ¬çš„ä»¥ä¸‹ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Multiple configurations
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šä¸ªé…ç½®
- en: In some cases, your dataset may have multiple configurations. For example, the
    [SuperGLUE](https://huggingface.co/datasets/super_glue) dataset is a collection
    of 5 datasets designed to evaluate language understanding tasks. ğŸ¤— Datasets provides
    [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    which allows you to create different configurations for the user to select from.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨çš„æ•°æ®é›†å¯èƒ½å…·æœ‰å¤šä¸ªé…ç½®ã€‚ä¾‹å¦‚ï¼Œ[SuperGLUE](https://huggingface.co/datasets/super_glue)æ•°æ®é›†æ˜¯ä¸€ä¸ªåŒ…å«5ä¸ªæ•°æ®é›†çš„é›†åˆï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€ç†è§£ä»»åŠ¡ã€‚ğŸ¤—
    Datasetsæä¾›äº†[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)ï¼Œå…è®¸æ‚¨ä¸ºç”¨æˆ·åˆ›å»ºä¸åŒçš„é…ç½®é€‰æ‹©ã€‚
- en: Letâ€™s study the [SuperGLUE loading script](https://huggingface.co/datasets/super_glue/blob/main/super_glue.py)
    to see how you can define several configurations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç ”ç©¶[SuperGLUEåŠ è½½è„šæœ¬](https://huggingface.co/datasets/super_glue/blob/main/super_glue.py)ï¼Œçœ‹çœ‹æ‚¨å¦‚ä½•å®šä¹‰å¤šä¸ªé…ç½®ã€‚
- en: Create a [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    subclass with attributes about your dataset. These attributes can be the features
    of your dataset, label classes, and a URL to the data files.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªåŒ…å«æœ‰å…³æ•°æ®é›†å±æ€§çš„[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)å­ç±»ã€‚è¿™äº›å±æ€§å¯ä»¥æ˜¯æ•°æ®é›†çš„ç‰¹å¾ã€æ ‡ç­¾ç±»åˆ«å’Œæ•°æ®æ–‡ä»¶çš„URLã€‚
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create instances of your config to specify the values of the attributes of
    each configuration. This gives you the flexibility to specify all the name and
    description of each configuration. These sub-class instances should be listed
    under `DatasetBuilder.BUILDER_CONFIGS`:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºé…ç½®çš„å®ä¾‹ä»¥æŒ‡å®šæ¯ä¸ªé…ç½®çš„å±æ€§å€¼ã€‚è¿™ä½¿æ‚¨å¯ä»¥çµæ´»åœ°æŒ‡å®šæ¯ä¸ªé…ç½®çš„åç§°å’Œæè¿°ã€‚è¿™äº›å­ç±»å®ä¾‹åº”åˆ—åœ¨`DatasetBuilder.BUILDER_CONFIGS`ä¸‹ï¼š
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, users can load a specific configuration of the dataset with the configuration
    `name`:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨é…ç½®`name`åŠ è½½æ•°æ®é›†çš„ç‰¹å®šé…ç½®ï¼š
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Additionally, users can instantiate a custom builder configuration by passing
    the builder configuration arguments to [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å°†æ„å»ºå™¨é…ç½®å‚æ•°ä¼ é€’ç»™[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)æ¥å®ä¾‹åŒ–è‡ªå®šä¹‰æ„å»ºå™¨é…ç½®ï¼š
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Default configurations
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é»˜è®¤é…ç½®
- en: 'Users must specify a configuration name when they load a dataset with multiple
    configurations. Otherwise, ğŸ¤— Datasets will raise a `ValueError`, and prompt the
    user to select a configuration name. You can avoid this by setting a default dataset
    configuration with the `DEFAULT_CONFIG_NAME` attribute:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æˆ·åœ¨åŠ è½½å…·æœ‰å¤šä¸ªé…ç½®çš„æ•°æ®é›†æ—¶å¿…é¡»æŒ‡å®šé…ç½®åç§°ã€‚å¦åˆ™ï¼ŒğŸ¤—æ•°æ®é›†å°†å¼•å‘`ValueError`ï¼Œå¹¶æç¤ºç”¨æˆ·é€‰æ‹©é…ç½®åç§°ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®å…·æœ‰`DEFAULT_CONFIG_NAME`å±æ€§çš„é»˜è®¤æ•°æ®é›†é…ç½®æ¥é¿å…è¿™ç§æƒ…å†µï¼š
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Only use a default configuration when it makes sense. Donâ€™t set one because
    it may be more convenient for the user to not specify a configuration when they
    load your dataset. For example, multi-lingual datasets often have a separate configuration
    for each language. An appropriate default may be an aggregated configuration that
    loads all the languages of the dataset if the user doesnâ€™t request a particular
    one.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨æœ‰æ„ä¹‰çš„æƒ…å†µä¸‹æ‰ä½¿ç”¨é»˜è®¤é…ç½®ã€‚ä¸è¦è®¾ç½®é»˜è®¤é…ç½®ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½æ›´æ–¹ä¾¿åœ°åœ¨åŠ è½½æ•°æ®é›†æ—¶ä¸æŒ‡å®šé…ç½®ã€‚ä¾‹å¦‚ï¼Œå¤šè¯­è¨€æ•°æ®é›†é€šå¸¸ä¸ºæ¯ç§è¯­è¨€è®¾ç½®å•ç‹¬çš„é…ç½®ã€‚ä¸€ä¸ªåˆé€‚çš„é»˜è®¤é…ç½®å¯èƒ½æ˜¯ä¸€ä¸ªèšåˆé…ç½®ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰è¯·æ±‚ç‰¹å®šè¯­è¨€ï¼Œåˆ™åŠ è½½æ•°æ®é›†çš„æ‰€æœ‰è¯­è¨€ã€‚
- en: Download data files and organize splits
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ•°æ®æ–‡ä»¶å¹¶ç»„ç»‡æ‹†åˆ†
- en: After youâ€™ve defined the attributes of your dataset, the next step is to download
    the data files and organize them according to their splits.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®šä¹‰æ•°æ®é›†å±æ€§ä¹‹åï¼Œä¸‹ä¸€æ­¥æ˜¯ä¸‹è½½æ•°æ®æ–‡ä»¶å¹¶æ ¹æ®å®ƒä»¬çš„æ‹†åˆ†è¿›è¡Œç»„ç»‡ã€‚
- en: 'Create a dictionary of URLs in the loading script that point to the original
    SQuAD data files:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½è„šæœ¬ä¸­åˆ›å»ºä¸€ä¸ªæŒ‡å‘åŸå§‹SQuADæ•°æ®æ–‡ä»¶çš„URLå­—å…¸ï¼š
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If the data files live in the same folder or repository of the dataset script,
    you can just pass the relative paths to the files instead of URLs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ•°æ®æ–‡ä»¶å­˜å‚¨åœ¨ä¸æ•°æ®é›†è„šæœ¬ç›¸åŒçš„æ–‡ä»¶å¤¹æˆ–å­˜å‚¨åº“ä¸­ï¼Œæ‚¨å¯ä»¥åªä¼ é€’æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„è€Œä¸æ˜¯URLã€‚
- en: '[DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)
    takes this dictionary and downloads the data files. Once the files are downloaded,
    use [SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)
    to organize each split in the dataset. This is a simple class that contains:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)æ¥å—æ­¤å­—å…¸å¹¶ä¸‹è½½æ•°æ®æ–‡ä»¶ã€‚ä¸€æ—¦æ–‡ä»¶ä¸‹è½½å®Œæˆï¼Œä½¿ç”¨[SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)æ¥ç»„ç»‡æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‹†åˆ†ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç±»ï¼ŒåŒ…å«ï¼š'
- en: 'The `name` of each split. You should use the standard split names: `Split.TRAIN`,
    `Split.TEST`, and `Split.VALIDATION`.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ‹†åˆ†çš„`name`ã€‚æ‚¨åº”è¯¥ä½¿ç”¨æ ‡å‡†çš„æ‹†åˆ†åç§°ï¼š`Split.TRAIN`ã€`Split.TEST`å’Œ`Split.VALIDATION`ã€‚
- en: '`gen_kwargs` provides the file paths to the data files to load for each split.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gen_kwargs`ä¸ºæ¯ä¸ªæ‹†åˆ†æä¾›è¦åŠ è½½çš„æ•°æ®æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ã€‚'
- en: 'Your `DatasetBuilder._split_generator()` should look like this now:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„`DatasetBuilder._split_generator()`ç°åœ¨åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Generate samples
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ ·æœ¬
- en: 'At this point, you have:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œæ‚¨å·²ç»æœ‰ï¼š
- en: Added the dataset attributes.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ æ•°æ®é›†å±æ€§ã€‚
- en: Provided instructions for how to download the data files.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æä¾›äº†å¦‚ä½•ä¸‹è½½æ•°æ®æ–‡ä»¶çš„è¯´æ˜ã€‚
- en: Organized the splits.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»„ç»‡æ‹†åˆ†ã€‚
- en: The next step is to actually generate the samples in each split.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å®é™…åœ¨æ¯ä¸ªæ‹†åˆ†ä¸­ç”Ÿæˆæ ·æœ¬ã€‚
- en: '`DatasetBuilder._generate_examples` takes the file path provided by `gen_kwargs`
    to read and parse the data files. You need to write a function that loads the
    data files and extracts the columns.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DatasetBuilder._generate_examples`ä½¿ç”¨`gen_kwargs`æä¾›çš„æ–‡ä»¶è·¯å¾„æ¥è¯»å–å’Œè§£ææ•°æ®æ–‡ä»¶ã€‚æ‚¨éœ€è¦ç¼–å†™ä¸€ä¸ªåŠ è½½æ•°æ®æ–‡ä»¶å¹¶æå–åˆ—çš„å‡½æ•°ã€‚'
- en: Your function should yield a tuple of an `id_`, and an example from the dataset.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‚¨çš„å‡½æ•°åº”è¯¥ç”Ÿæˆä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª`id_`å’Œæ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ã€‚
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: (Optional) Generate dataset metadata
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ï¼ˆå¯é€‰ï¼‰ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®
- en: Adding dataset metadata is a great way to include information about your dataset.
    The metadata is stored in the dataset card `README.md` in YAML. It includes information
    like the number of examples required to confirm the dataset was correctly generated,
    and information about the dataset like its `features`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®æ˜¯åŒ…å«æœ‰å…³æ•°æ®é›†ä¿¡æ¯çš„é‡è¦æ–¹å¼ã€‚å…ƒæ•°æ®å­˜å‚¨åœ¨æ•°æ®é›†å¡ç‰‡`README.md`ä¸­çš„YAMLä¸­ã€‚å®ƒåŒ…æ‹¬è¯¸å¦‚ç¡®è®¤æ•°æ®é›†æ˜¯å¦æ­£ç¡®ç”Ÿæˆæ‰€éœ€çš„ç¤ºä¾‹æ•°é‡ç­‰ä¿¡æ¯ï¼Œä»¥åŠæœ‰å…³æ•°æ®é›†çš„ä¿¡æ¯ï¼Œå¦‚å…¶`features`ã€‚
- en: 'Run the following command to generate your dataset metadata in `README.md`
    and make sure your new dataset loading script works correctly:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆ`README.md`ä¸­çš„æ•°æ®é›†å…ƒæ•°æ®ï¼Œå¹¶ç¡®ä¿æ‚¨çš„æ–°æ•°æ®é›†åŠ è½½è„šæœ¬æ­£å¸¸å·¥ä½œï¼š
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If your dataset loading script passed the test, you should now have a `README.md`
    file in your dataset folder containing a `dataset_info` field with some metadata.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ•°æ®é›†åŠ è½½è„šæœ¬é€šè¿‡äº†æµ‹è¯•ï¼Œç°åœ¨æ‚¨çš„æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­åº”è¯¥æœ‰ä¸€ä¸ªåŒ…å«ä¸€äº›å…ƒæ•°æ®çš„`README.md`æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª`dataset_info`å­—æ®µã€‚
- en: Upload to the Hub
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸Šä¼ åˆ°Hub
- en: Once your script is ready, [create a dataset card](dataset_card) and [upload
    it to the Hub](share).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨çš„è„šæœ¬å‡†å¤‡å¥½äº†ï¼Œ[åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å¡ç‰‡](dataset_card)å¹¶[ä¸Šä¼ åˆ°Hub](share)ã€‚
- en: Congratulations, you can now load your dataset from the Hub! ğŸ¥³
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼Œæ‚¨ç°åœ¨å¯ä»¥ä»HubåŠ è½½æ‚¨çš„æ•°æ®é›†äº†ï¼ğŸ¥³
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Advanced features
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜çº§ç‰¹æ€§
- en: Sharding
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡
- en: If your dataset is made of many big files, ğŸ¤— Datasets automatically runs your
    script in parallel to make it super fast! It can help if you have hundreds or
    thousands of TAR archives, or JSONL files like [oscar](https://huggingface.co/datasets/oscar/blob/main/oscar.py)
    for example.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ•°æ®é›†ç”±è®¸å¤šå¤§æ–‡ä»¶ç»„æˆï¼ŒğŸ¤—æ•°æ®é›†ä¼šè‡ªåŠ¨å¹¶è¡Œè¿è¡Œæ‚¨çš„è„šæœ¬ï¼Œä½¿å…¶è¿è¡Œé€Ÿåº¦éå¸¸å¿«ï¼ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰æ•°ç™¾æˆ–æ•°åƒä¸ªTARå­˜æ¡£æ–‡ä»¶ï¼Œæˆ–è€…åƒ[oscar](https://huggingface.co/datasets/oscar/blob/main/oscar.py)è¿™æ ·çš„JSONLæ–‡ä»¶ã€‚
- en: To make it work, we consider lists of files in `gen_kwargs` to be shards. Therefore
    ğŸ¤— Datasets can automatically spawn several workers to run `_generate_examples`
    in parallel, and each worker is given a subset of shards to process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿å…¶å·¥ä½œï¼Œæˆ‘ä»¬è®¤ä¸º`gen_kwargs`ä¸­çš„æ–‡ä»¶åˆ—è¡¨æ˜¯åˆ†ç‰‡ã€‚å› æ­¤ï¼ŒğŸ¤—æ•°æ®é›†å¯ä»¥è‡ªåŠ¨å¯åŠ¨å¤šä¸ªå·¥ä½œè¿›ç¨‹å¹¶è¡Œè¿è¡Œ`_generate_examples`ï¼Œæ¯ä¸ªå·¥ä½œè¿›ç¨‹éƒ½ä¼šè¢«åˆ†é…ä¸€ä¸ªè¦å¤„ç†çš„åˆ†ç‰‡å­é›†ã€‚
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Users can also specify `num_proc=` in `load_dataset()` to specify the number
    of processes to use as workers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æˆ·è¿˜å¯ä»¥åœ¨`load_dataset()`ä¸­æŒ‡å®š`num_proc=`æ¥æŒ‡å®šè¦ç”¨ä½œå·¥ä½œè¿›ç¨‹çš„è¿›ç¨‹æ•°ã€‚
- en: ArrowBasedBuilder
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ArrowBasedBuilder
- en: 'For some datasets it can be much faster to yield batches of data rather than
    examples one by one. You can speed up the dataset generation by yielding Arrow
    tables directly, instead of examples. This is especially useful if your data comes
    from Pandas DataFrames for example, since the conversion from Pandas to Arrow
    is as simple as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€äº›æ•°æ®é›†ï¼Œæ‰¹é‡ç”Ÿæˆæ•°æ®å¯èƒ½æ¯”é€ä¸ªç”Ÿæˆç¤ºä¾‹è¦å¿«å¾—å¤šã€‚æ‚¨å¯ä»¥é€šè¿‡ç›´æ¥ç”ŸæˆArrowè¡¨è€Œä¸æ˜¯ç¤ºä¾‹æ¥åŠ å¿«æ•°æ®é›†ç”Ÿæˆé€Ÿåº¦ã€‚è¿™åœ¨æ‚¨çš„æ•°æ®æ¥è‡ªPandas DataFramesç­‰æƒ…å†µä¸‹å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºä»Pandasè½¬æ¢ä¸ºArrowéå¸¸ç®€å•ï¼š
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To yield Arrow tables instead of single examples, make your dataset builder
    inherit from [ArrowBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ArrowBasedBuilder)
    instead of [GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder),
    and use `_generate_tables` instead of `_generate_examples`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç”ŸæˆArrowè¡¨è€Œä¸æ˜¯å•ä¸ªç¤ºä¾‹ï¼Œè¯·ä½¿æ‚¨çš„æ•°æ®é›†æ„å»ºå™¨ç»§æ‰¿è‡ª[ArrowBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ArrowBasedBuilder)è€Œä¸æ˜¯[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)ï¼Œå¹¶ä½¿ç”¨`_generate_tables`è€Œä¸æ˜¯`_generate_examples`ï¼š
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Donâ€™t forget to keep your script memory efficient, in case users run them on
    machines with a low amount of RAM.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦å¿˜è®°ä¿æŒè„šæœ¬çš„å†…å­˜æ•ˆç‡ï¼Œä»¥é˜²ç”¨æˆ·åœ¨å†…å­˜è¾ƒå°‘çš„æœºå™¨ä¸Šè¿è¡Œå®ƒä»¬ã€‚
