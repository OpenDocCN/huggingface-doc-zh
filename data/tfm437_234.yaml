- en: Splinter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Splinter
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/splinter](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/splinter)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/splinter](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/splinter)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The Splinter model was proposed in [Few-Shot Question Answering by Pretraining
    Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain,
    Jonathan Berant, Amir Globerson, Omer Levy. Splinter is an encoder-only transformer
    (similar to BERT) pretrained using the recurring span selection task on a large
    corpus comprising Wikipedia and the Toronto Book Corpus.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Splinter 模型是由 Ori Ram、Yuval Kirstain、Jonathan Berant、Amir Globerson、Omer Levy
    在 [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438)
    中提出的。Splinter 是一个仅编码器的 transformer 模型（类似于 BERT），使用包含维基百科和多伦多图书语料库的大型语料库上的循环跨度选择任务进行预训练。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: 'In several question answering benchmarks, pretrained models have reached human
    parity through fine-tuning on an order of 100,000 annotated questions and answers.
    We explore the more realistic few-shot setting, where only a few hundred training
    examples are available, and observe that standard models perform poorly, highlighting
    the discrepancy between current pretraining objectives and question answering.
    We propose a new pretraining scheme tailored for question answering: recurring
    span selection. Given a passage with multiple sets of recurring spans, we mask
    in each set all recurring spans but one, and ask the model to select the correct
    span in the passage for each masked span. Masked spans are replaced with a special
    token, viewed as a question representation, that is later used during fine-tuning
    to select the answer span. The resulting model obtains surprisingly good results
    on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples),
    while maintaining competitive performance in the high-resource setting.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在几个问答基准测试中，预训练模型通过在约 100,000 个注释问题和答案上进行微调达到了人类水平。我们探索了更现实的少样本设置，在这种情况下只有几百个训练示例可用，并观察到标准模型表现不佳，突显了当前预训练目标和问答之间的差异。我们提出了一种针对问答定制的新的预训练方案：循环跨度选择。给定一个包含多组循环跨度的段落，我们在每组中屏蔽所有循环跨度，然后要求模型为每个屏蔽的跨度在段落中选择正确的跨度。屏蔽的跨度将被一个特殊
    token 替换，被视为问题表示，稍后在微调过程中用于选择答案跨度。结果模型在多个基准测试中获得了令人惊讶的好结果（例如，在只有 128 个训练示例的情况下，在
    SQuAD 上达到了 72.7 的 F1 分数），同时在高资源设置中保持了竞争性能。
- en: This model was contributed by [yuvalkirstain](https://huggingface.co/yuvalkirstain)
    and [oriram](https://huggingface.co/oriram). The original code can be found [here](https://github.com/oriram/splinter).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型由 [yuvalkirstain](https://huggingface.co/yuvalkirstain) 和 [oriram](https://huggingface.co/oriram)
    贡献。原始代码可以在 [这里](https://github.com/oriram/splinter) 找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: 'Splinter was trained to predict answers spans conditioned on a special [QUESTION]
    token. These tokens contextualize to question representations which are used to
    predict the answers. This layer is called QASS, and is the default behaviour in
    the [SplinterForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterForQuestionAnswering)
    class. Therefore:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Splinter 被训练来预测在特定 [QUESTION] token 条件下的答案跨度。这些 token 用于上下文化问题表示，用于预测答案。这一层被称为
    QASS，并且是 [SplinterForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterForQuestionAnswering)
    类中的默认行为。因此：
- en: Use [SplinterTokenizer](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterTokenizer)
    (rather than [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)),
    as it already contains this special token. Also, its default behavior is to use
    this token when two sequences are given (for example, in the *run_qa.py* script).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 [SplinterTokenizer](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterTokenizer)（而不是
    [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)），因为它已经包含了这个特殊
    token。此外，当给定两个序列时（例如在 *run_qa.py* 脚本中），其默认行为是使用这个 token。
- en: If you plan on using Splinter outside *run_qa.py*, please keep in mind the question
    token - it might be important for the success of your model, especially in a few-shot
    setting.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您计划在 *run_qa.py* 之外使用 Splinter，请记住问题 token - 这对于您的模型的成功可能很重要，特别是在少样本设置中。
- en: Please note there are two different checkpoints for each size of Splinter. Both
    are basically the same, except that one also has the pretrained weights of the
    QASS layer (*tau/splinter-base-qass* and *tau/splinter-large-qass*) and one doesn’t
    (*tau/splinter-base* and *tau/splinter-large*). This is done to support randomly
    initializing this layer at fine-tuning, as it is shown to yield better results
    for some cases in the paper.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，每个 Splinter 大小都有两个不同的检查点。它们基本上是相同的，只是一个还具有 QASS 层的预训练权重（*tau/splinter-base-qass*
    和 *tau/splinter-large-qass*），另一个没有（*tau/splinter-base* 和 *tau/splinter-large*）。这样做是为了支持在微调时随机初始化这一层，因为在某些情况下已经证明这样做可以获得更好的结果。
- en: Resources
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Question answering task guide](../tasks/question-answering)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question-answering)'
- en: SplinterConfig
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SplinterConfig
- en: '### `class transformers.SplinterConfig`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SplinterConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/configuration_splinter.py#L32)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/configuration_splinter.py#L32)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    Splinter model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, 默认为 30522) — Splinter 模型的词汇大小。定义了在调用 [SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel)
    时可以表示的不同 token 数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimension of the encoder
    layers and the pooler layer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为 768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`，*optional*，默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimension of the
    “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`，*optional*，默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`function`，*optional*，默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`，*optional*，默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`，*optional*，默认为0.1) — 注意力概率的丢弃比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`，*optional*，默认为512) — 该模型可能被使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling [SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`，*optional*，默认为2) — 调用[SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel)时传递的`token_type_ids`的词汇表大小。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`，*optional*，默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`，*optional*，默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`，*optional*，默认为`True`) — 模型是否应返回最后的键/值注意力（不是所有模型都使用）。仅在`config.is_decoder=True`时相关。'
- en: '`question_token_id` (`int`, *optional*, defaults to 104) — The id of the `[QUESTION]`
    token.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_token_id` (`int`，*optional*，默认为104) — `[QUESTION]`标记的ID。'
- en: This is the configuration class to store the configuration of a [SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel).
    It is used to instantiate an Splinter model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Splinter [tau/splinter-base](https://huggingface.co/tau/splinter-base)
    architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel)配置的配置类。它用于根据指定的参数实例化Splinter模型，定义模型架构。使用默认值实例化配置将产生类似于Splinter
    [tau/splinter-base](https://huggingface.co/tau/splinter-base)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: SplinterTokenizer
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SplinterTokenizer
- en: '### `class transformers.SplinterTokenizer`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SplinterTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L75)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L75)'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含词汇表的文件。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether or not to
    lowercase the input when tokenizing.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`，*optional*，默认为`True`) — 在分词时是否将输入转换为小写。'
- en: '`do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — Whether or not
    to do basic tokenization before WordPiece.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — 是否在WordPiece之前进行基本的分词。'
- en: '`never_split` (`Iterable`, *optional*) — Collection of tokens which will never
    be split during tokenization. Only has an effect when `do_basic_tokenize=True`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`never_split` (`Iterable`，*optional*) — 在分词时永远不会被拆分的标记集合。仅在`do_basic_tokenize=True`时有效。'
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`，*optional*，默认为`"[UNK]"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`，*optional*，默认为`"[SEP]"`) — 分隔符标记，在构建多个序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`，*optional*，默认为`"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — 在进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`question_token` (`str`, *optional*, defaults to `"[QUESTION]"`) — The token
    used for constructing question representations.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_token` (`str`, *optional*, defaults to `"[QUESTION]"`) — 用于构建问题表示的标记。'
- en: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — Whether
    or not to tokenize Chinese characters.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — 是否对中文字符进行分词。'
- en: This should likely be deactivated for Japanese (see this [issue](https://github.com/huggingface/transformers/issues/328)).
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于日语，这可能应该被停用（参见此[问题](https://github.com/huggingface/transformers/issues/328)）。
- en: '`strip_accents` (`bool`, *optional*) — Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original BERT).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strip_accents` (`bool`, *optional*) — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始BERT相同）。'
- en: Construct a Splinter tokenizer. Based on WordPiece.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Splinter分词器。基于WordPiece。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L216)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L216)'
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — The question token IDs if pad_on_right, else
    context tokens IDs'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 如果pad_on_right，则为问题标记ID，否则为上下文标记ID'
- en: '`token_ids_1` (`List[int]`, *optional*) — The context token IDs if pad_on_right,
    else question token IDs'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 如果pad_on_right，则为上下文标记ID，否则为问题标记ID'
- en: Returns
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a pair of sequence for question answering tasks by
    concatenating and adding special tokens. A Splinter sequence has the following
    format:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记来从一对序列构建用于问答任务的模型输入。Splinter序列具有以下格式：
- en: 'single sequence: `[CLS] X [SEP]`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`[CLS] X [SEP]`
- en: 'pair of sequences for question answering: `[CLS] question_tokens [QUESTION]
    . [SEP] context_tokens [SEP]`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于问答的序列对：`[CLS] 问题标记 [QUESTION] . [SEP] 上下文标记 [SEP]`
- en: '#### `get_special_tokens_mask`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L248)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L248)'
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — 是否已经为模型格式化了特殊标记的标记列表。'
- en: Returns
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：特殊标记为1，序列标记为0。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。当使用分词器的`prepare_for_model`方法添加特殊标记时，将调用此方法。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L276)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L276)'
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — The first tokenized sequence.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 第一个标记化序列。'
- en: '`token_ids_1` (`List[int]`, *optional*) — The second tokenized sequence.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 第二个标记化序列。'
- en: Returns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: The token type ids.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 标记类型ID。
- en: Create the token type IDs corresponding to the sequences passed. [What are token
    type IDs?](../glossary#token-type-ids)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 创建与传递的序列对应的标记类型ID。[什么是标记类型ID？](../glossary#token-type-ids)
- en: Should be overridden in a subclass if the model has a special way of building
    those.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型有特殊的构建方式，则应在子类中重写。
- en: '#### `save_vocabulary`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L305)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter.py#L305)'
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: SplinterTokenizerFast
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SplinterTokenizerFast
- en: '### `class transformers.SplinterTokenizerFast`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SplinterTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter_fast.py#L55)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter_fast.py#L55)'
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含词汇表的文件。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether or not to
    lowercase the input when tokenizing.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — 在分词时是否将输入转换为小写。'
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — 分隔符标记，在从多个序列构建序列时使用，例如，用于序列分类的两个序列或用于问题回答的文本和问题。它也被用作使用特殊标记构建的序列的最后一个标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — 分类器标记，在进行序列分类（对整个序列而不是每个标记进行分类）时使用。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`question_token` (`str`, *optional*, defaults to `"[QUESTION]"`) — The token
    used for constructing question representations.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_token` (`str`, *optional*, defaults to `"[QUESTION]"`) — 用于构建问题表示的标记。'
- en: '`clean_text` (`bool`, *optional*, defaults to `True`) — Whether or not to clean
    the text before tokenization by removing any control characters and replacing
    all whitespaces by the classic one.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_text` (`bool`, *optional*, defaults to `True`) — 是否在分词之前清理文本，通过删除任何控制字符并将所有空格替换为经典空格。'
- en: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — Whether
    or not to tokenize Chinese characters. This should likely be deactivated for Japanese
    (see [this issue](https://github.com/huggingface/transformers/issues/328)).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — 是否对中文字符进行分词。这可能应该在日语中停用（参见[此问题](https://github.com/huggingface/transformers/issues/328)）。'
- en: '`strip_accents` (`bool`, *optional*) — Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original BERT).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strip_accents` (`bool`, *optional*) — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始BERT相同）。'
- en: '`wordpieces_prefix` (`str`, *optional*, defaults to `"##"`) — The prefix for
    subwords.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wordpieces_prefix` (`str`, *optional*, defaults to `"##"`) — 子词的前缀。'
- en: Construct a “fast” Splinter tokenizer (backed by HuggingFace’s *tokenizers*
    library). Based on WordPiece.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速”Splinter分词器（由HuggingFace的*tokenizers*库支持）。基于WordPiece。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应该参考这个超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter_fast.py#L153)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/tokenization_splinter_fast.py#L153)'
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — The question token IDs if pad_on_right, else
    context tokens IDs'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 如果pad_on_right，则为问题标记ID，否则为上下文标记ID'
- en: '`token_ids_1` (`List[int]`, *optional*) — The context token IDs if pad_on_right,
    else question token IDs'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 如果pad_on_right，则为上下文标记ID，否则为问题标记ID'
- en: Returns
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a pair of sequence for question answering tasks by
    concatenating and adding special tokens. A Splinter sequence has the following
    format:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，从一对序列构建问题回答任务的模型输入。Splinter序列的格式如下：
- en: 'single sequence: `[CLS] X [SEP]`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`[CLS] X [SEP]`
- en: 'pair of sequences for question answering: `[CLS] question_tokens [QUESTION]
    . [SEP] context_tokens [SEP]`'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题回答的序列对：`[CLS] question_tokens [QUESTION] . [SEP] context_tokens [SEP]`
- en: SplinterModel
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SplinterModel
- en: '### `class transformers.SplinterModel`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SplinterModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L604)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L604)'
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Splinter Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的Splinter模型变压器输出原始隐藏状态，没有特定的头部在顶部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: The model is an encoder (with only self-attention) following the architecture
    described in [Attention is all you need](https://arxiv.org/abs/1706.03762) by
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser and Illia Polosukhin.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是一个编码器（仅具有自注意力），遵循[Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin](https://arxiv.org/abs/1706.03762)描述的架构。
- en: '#### `forward`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L639)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L639)'
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`batch_size, sequence_length`的`torch.FloatTensor`，*可选*）-
    避免对填充标记索引执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被“掩盖”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被“掩盖”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`batch_size, sequence_length`的`torch.LongTensor`，*可选*）-
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`batch_size, sequence_length`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。在范围`[0,
    config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块中选择的头部无效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。'
- en: '`encoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on the padding token indices
    of the encoder input. This mask is used in the cross-attention if the model is
    configured as a decoder. Mask values selected in `[0, 1]`:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在编码器输入的填充标记索引上执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用此掩码。掩码值选在 `[0, 1]`
    之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被 `masked` 的标记为 1。
- en: 0 for tokens that are `masked`.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被 `masked` 的标记为 0。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding. If `past_key_values`
    are used, the user can optionally input only the last `decoder_input_ids` (those
    that don’t have their past key value states given to this model) of shape `(batch_size,
    1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为 `config.n_layers`，每个元组包含
    4 个形状为 `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)` 的张量）
    — 包含注意力块的预计算的键和值隐藏状态。可用于加速解码。如果使用了 `past_key_values`，用户可以选择只输入最后的 `decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）的形状为
    `(batch_size, 1)`，而不是形状为 `(batch_size, sequence_length)` 的所有 `decoder_input_ids`。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`，*optional*) — 如果设置为 `True`，则返回 `past_key_values` 键值状态，可用于加速解码（参见
    `past_key_values`）。'
- en: Returns
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    或 `torch.FloatTensor` 元组'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig))
    and inputs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（[SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列的输出。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了 `past_key_values`，则输出形状为 `(batch_size, 1, hidden_size)` 的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，*optional*，当传递 `use_cache=True`
    或 `config.use_cache=True` 时返回） — 长度为 `config.n_layers` 的 `tuple(torch.FloatTensor)`
    元组，每个元组有 2 个形状为 `(batch_size, num_heads, sequence_length, embed_size_per_head)`
    的张量，如果 `config.is_encoder_decoder=True` 还有 2 个额外的形状为 `(batch_size, num_heads,
    encoder_sequence_length, embed_size_per_head)` 的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预计算的隐藏状态（自注意力块中的键和值以及在交叉注意力块中可选地如果 `config.is_encoder_decoder=True`）可以使用（参见
    `past_key_values` 输入）加速顺序解码。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层的输出，则为一个 + 每层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递 `output_attentions=True`
    和 `config.add_cross_attention=True` 或 `config.output_attentions=True` 时返回） — 形状为
    `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor`
    元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: The [SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel)
    forward method, overrides the `__call__` special method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[SplinterModel](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: SplinterForQuestionAnswering
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SplinterForQuestionAnswering
- en: '### `class transformers.SplinterForQuestionAnswering`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SplinterForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L823)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L823)'
- en: '[PRE12]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Splinter Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Splinter模型在顶部具有一个跨度分类头，用于提取式问答任务，如SQuAD（在隐藏状态输出的顶部进行线性层计算`span start logits`和`span
    end logits`）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L841)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L841)'
- en: '[PRE13]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)以获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[输入ID是什么？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`torch.FloatTensor`，形状为`batch_size, sequence_length`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被遮罩的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被遮罩的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[注意力掩码是什么？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（`torch.LongTensor`，形状为`batch_size, sequence_length`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引选在`[0, 1]`之间：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[令牌类型ID是什么？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，形状为`batch_size, sequence_length`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0,
    config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[位置ID是什么？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮罩，
- en: 0 indicates the head is `masked`.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮罩。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）—
    可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为关联向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — 用于计算标记范围的开始位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度(`sequence_length`)。超出序列范围的位置不会被考虑在内计算损失。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    用于计算标记分类损失的标签位置（索引）的标签。位置被夹紧到序列的长度(`sequence_length`)。超出序列范围的位置不会被考虑在内计算损失。'
- en: '`question_positions` (`torch.LongTensor` of shape `(batch_size, num_questions)`,
    *optional*) — The positions of all question tokens. If given, start_logits and
    end_logits will be of shape `(batch_size, num_questions, sequence_length)`. If
    None, the first question token in each sequence in the batch will be the only
    one for which start_logits and end_logits are calculated and they will be of shape
    `(batch_size, sequence_length)`.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_positions` (`torch.LongTensor` of shape `(batch_size, num_questions)`,
    *optional*) — 所有问题标记的位置。如果给定，`start_logits`和`end_logits`的形状将为`(batch_size, num_questions,
    sequence_length)`。如果为None，则批处理中每个序列中的第一个问题标记将是唯一一个计算`start_logits`和`end_logits`的标记，它们的形状将为`(batch_size,
    sequence_length)`。'
- en: Returns
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)或`tuple(torch.FloatTensor)`。'
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig))
    and inputs.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)或一个`torch.FloatTensor`的元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置([SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig))和输入。'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    总跨度提取损失是开始和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 跨度开始分数（SoftMax之前）。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 跨度结束分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当`output_hidden_states=True`被传递或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`的元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当`output_attentions=True`被传递或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`的元组（每个层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [SplinterForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[SplinterForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: SplinterForPreTraining
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SplinterForPreTraining
- en: '### `class transformers.SplinterForPreTraining`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SplinterForPreTraining`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L976)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L976)'
- en: '[PRE15]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SplinterConfig](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Splinter Model for the recurring span selection task as done during the pretraining.
    The difference to the QA task is that we do not have a question, but multiple
    question tokens that replace the occurrences of recurring spans instead.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Splinter模型用于在预训练期间执行重复跨度选择任务。与QA任务的不同之处在于我们没有问题，而是有多个问题标记，这些标记替换了重复跨度的出现。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L995)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/splinter/modeling_splinter.py#L995)'
- en: '[PRE16]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, num_questions, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, num_questions, sequence_length)`的`torch.LongTensor`）—
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入ID是什么？
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, num_questions,
    sequence_length`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`batch_size, num_questions, sequence_length`的`torch.FloatTensor`，*可选*）—
    避免对填充标记索引执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被屏蔽的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被屏蔽的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, num_questions, sequence_length`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`batch_size, num_questions, sequence_length`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, num_questions, sequence_length`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`batch_size, num_questions, sequence_length`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被屏蔽。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_questions,
    sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing `input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert *input_ids* indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, num_questions, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size, num_questions)`,
    *optional*) — Labels for position (index) of the start of the labelled span for
    computing the token classification loss. Positions are clamped to the length of
    the sequence (`sequence_length`). Position outside of the sequence are not taken
    into account for computing the loss.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions`（形状为`(batch_size, num_questions)`的`torch.LongTensor`，*可选*）—
    用于计算标记跨度的起始位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内以计算损失。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size, num_questions)`,
    *optional*) — Labels for position (index) of the end of the labelled span for
    computing the token classification loss. Positions are clamped to the length of
    the sequence (`sequence_length`). Position outside of the sequence are not taken
    into account for computing the loss.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions`（形状为`(batch_size, num_questions)`的`torch.LongTensor`，*可选*）—
    用于计算标记跨度的结束位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内以计算损失。'
- en: '`question_positions` (`torch.LongTensor` of shape `(batch_size, num_questions)`,
    *optional*) — The positions of all question tokens. If given, start_logits and
    end_logits will be of shape `(batch_size, num_questions, sequence_length)`. If
    None, the first question token in each sequence in the batch will be the only
    one for which start_logits and end_logits are calculated and they will be of shape
    `(batch_size, sequence_length)`.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_positions`（形状为`(batch_size, num_questions)`的`torch.LongTensor`，*可选*）—
    所有问题标记的位置。如果给定，start_logits和end_logits将具有形状`(batch_size, num_questions, sequence_length)`。如果为None，则批处理中每个序列中的第一个问题标记将是唯一一个计算start_logits和end_logits的标记，并且它们将具有形状`(batch_size,
    sequence_length)`。'
- en: The [SplinterForPreTraining](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[SplinterForPreTraining](/docs/transformers/v4.37.2/en/model_doc/splinter#transformers.SplinterForPreTraining)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是调用此函数，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
