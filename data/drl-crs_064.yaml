- en: Hands on
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®è·µ
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/hands-on](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/learn/deep-rl-course/unit4/hands-on](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![æé—®](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb)'
- en: Now that weâ€™ve studied the theory behind Reinforce, **youâ€™re ready to code your
    Reinforce agent with PyTorch**. And youâ€™ll test its robustness using CartPole-v1
    and PixelCopter,.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ç ”ç©¶äº†ReinforceèƒŒåçš„ç†è®ºï¼Œ**æ‚¨å·²ç»å‡†å¤‡å¥½ä½¿ç”¨PyTorchç¼–å†™æ‚¨çš„Reinforceä»£ç†**ã€‚å¹¶ä¸”æ‚¨å°†ä½¿ç”¨CartPole-v1å’ŒPixelCopteræµ‹è¯•å…¶ç¨³å¥æ€§ã€‚
- en: Youâ€™ll then be able to iterate and improve this implementation for more advanced
    environments.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å°†èƒ½å¤Ÿè¿­ä»£å’Œæ”¹è¿›æ­¤å®ç°ï¼Œä»¥é€‚ç”¨äºæ›´é«˜çº§çš„ç¯å¢ƒã€‚
- en: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
- en: 'To validate this hands-on for the certification process, you need to push your
    trained models to the Hub and:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†éªŒè¯è¿™ä¸ªå®è·µè¿‡ç¨‹ï¼Œæ‚¨éœ€è¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ°Hubå¹¶ï¼š
- en: Get a result of >= 350 for `Cartpole-v1`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è·å¾—`Cartpole-v1`çš„ç»“æœ>= 350
- en: Get a result of >= 5 for `PixelCopter`.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è·å¾—`PixelCopter`çš„ç»“æœ>= 5ã€‚
- en: To find your result, go to the leaderboard and find your model, **the result
    = mean_reward - std of reward**. **If you donâ€™t see your model on the leaderboard,
    go at the bottom of the leaderboard page and click on the refresh button**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰¾åˆ°æ‚¨çš„ç»“æœï¼Œè¯·è½¬åˆ°æ’è¡Œæ¦œå¹¶æ‰¾åˆ°æ‚¨çš„æ¨¡å‹ï¼Œ**ç»“æœ=å¹³å‡å¥–åŠ±-å¥–åŠ±çš„æ ‡å‡†å·®**ã€‚**å¦‚æœæ‚¨åœ¨æ’è¡Œæ¦œä¸Šæ‰¾ä¸åˆ°æ‚¨çš„æ¨¡å‹ï¼Œè¯·è½¬åˆ°æ’è¡Œæ¦œé¡µé¢åº•éƒ¨ï¼Œç„¶åå•å‡»åˆ·æ–°æŒ‰é’®**ã€‚
- en: '**If you donâ€™t find your model, go to the bottom of the page and click on the
    refresh button.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœæ‚¨æ‰¾ä¸åˆ°æ‚¨çš„æ¨¡å‹ï¼Œè¯·è½¬åˆ°é¡µé¢åº•éƒ¨ï¼Œç„¶åå•å‡»åˆ·æ–°æŒ‰é’®ã€‚**'
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: And you can check your progress here ğŸ‘‰ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ£€æŸ¥æ‚¨çš„è¿›åº¦ğŸ‘‰ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
- en: '**To start the hands-on click on Open In Colab button** ğŸ‘‡ :'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¦å¼€å§‹å®è·µï¼Œè¯·ç‚¹å‡»â€œåœ¨Colabä¸­æ‰“å¼€â€æŒ‰é’®**ğŸ‘‡ï¼š'
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)'
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼ºçƒˆå»ºè®®å­¦ç”Ÿä»¬åœ¨è¿›è¡Œå®è·µç»ƒä¹ æ—¶ä½¿ç”¨Google Colab**ï¼Œè€Œä¸æ˜¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šè¿è¡Œå®ƒä»¬**ã€‚
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨Google Colabï¼Œ**æ‚¨å¯ä»¥ä¸“æ³¨äºå­¦ä¹ å’Œå®éªŒï¼Œè€Œä¸å¿…æ‹…å¿ƒè®¾ç½®ç¯å¢ƒçš„æŠ€æœ¯ç»†èŠ‚**ã€‚
- en: 'Unit 4: Code your first Deep Reinforcement Learning Algorithm with PyTorch:
    Reinforce. And test its robustness ğŸ’ª'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬4å•å…ƒï¼šä½¿ç”¨PyTorchç¼–å†™æ‚¨çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šReinforceã€‚å¹¶æµ‹è¯•å…¶ç¨³å¥æ€§ğŸ’ª
- en: '![thumbnail](../Images/207886028f30a9a8c43010256f915e88.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![ç¼©ç•¥å›¾](../Images/207886028f30a9a8c43010256f915e88.png)'
- en: 'In this notebook, youâ€™ll code your first Deep Reinforcement Learning algorithm
    from scratch: Reinforce (also called Monte Carlo Policy Gradient).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æœ¬ç¬”è®°æœ¬ä¸­ï¼Œæ‚¨å°†ä»å¤´å¼€å§‹ç¼–å†™æ‚¨çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šReinforceï¼ˆä¹Ÿç§°ä¸ºè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ï¼‰ã€‚
- en: 'Reinforce is a *Policy-based method*: a Deep Reinforcement Learning algorithm
    that tries **to optimize the policy directly without using an action-value function**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Reinforceæ˜¯ä¸€ç§*åŸºäºç­–ç•¥çš„æ–¹æ³•*ï¼šä¸€ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯•å›¾**ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œè€Œä¸ä½¿ç”¨åŠ¨ä½œå€¼å‡½æ•°**ã€‚
- en: More precisely, Reinforce is a *Policy-gradient method*, a subclass of *Policy-based
    methods* that aims **to optimize the policy directly by estimating the weights
    of the optimal policy using gradient ascent**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å‡†ç¡®åœ°è¯´ï¼ŒReinforceæ˜¯ä¸€ç§*ç­–ç•¥æ¢¯åº¦æ–¹æ³•*ï¼Œæ˜¯*åŸºäºç­–ç•¥çš„æ–¹æ³•*çš„ä¸€ä¸ªå­ç±»ï¼Œæ—¨åœ¨**é€šè¿‡ä¼°è®¡æœ€ä¼˜ç­–ç•¥çš„æƒé‡æ¥ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿ç”¨æ¢¯åº¦ä¸Šå‡**ã€‚
- en: 'To test its robustness, weâ€™re going to train it in 2 different simple environments:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹è¯•å…¶ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å°†åœ¨2ä¸ªä¸åŒçš„ç®€å•ç¯å¢ƒä¸­å¯¹å…¶è¿›è¡Œè®­ç»ƒï¼š
- en: Cartpole-v1
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cartpole-v1
- en: PixelcopterEnv
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PixelcopterEnv
- en: â¬‡ï¸ Here is an example of what **you will achieve at the end of this notebook.**
    â¬‡ï¸
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: â¬‡ï¸ è¿™æ˜¯**æ‚¨å°†åœ¨æœ¬ç¬”è®°æœ¬æœ«å°¾å®ç°çš„ç¤ºä¾‹ã€‚** â¬‡ï¸
- en: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
- en: 'ğŸ® Environments:'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ğŸ® ç¯å¢ƒï¼š
- en: '[CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)'
- en: '[PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
- en: 'ğŸ“š RL-Library:'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'ğŸ“š RL-Library:'
- en: Python
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: PyTorch
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Weâ€™re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸æ–­åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœæ‚¨åœ¨æœ¬ç¬”è®°æœ¬ä¸­å‘ç°ä¸€äº›é—®é¢˜**ï¼Œè¯·[åœ¨GitHub Repoä¸Šæå‡ºé—®é¢˜](https://github.com/huggingface/deep-rl-class/issues)ã€‚
- en: Objectives of this notebook ğŸ†
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬ç¬”è®°æœ¬çš„ç›®æ ‡ğŸ†
- en: 'At the end of the notebook, you will:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬”è®°æœ¬çš„æœ«å°¾ï¼Œæ‚¨å°†ï¼š
- en: Be able to **code a Reinforce algorithm from scratch using PyTorch.**
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿ**ä½¿ç”¨PyTorchä»å¤´å¼€å§‹ç¼–å†™ä¸€ä¸ªReinforceç®—æ³•**ã€‚
- en: Be able to **test the robustness of your agent using simple environments.**
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿ**ä½¿ç”¨ç®€å•ç¯å¢ƒæµ‹è¯•æ‚¨çš„ä»£ç†çš„ç¨³å¥æ€§**ã€‚
- en: Be able to **push your trained agent to the Hub** with a nice video replay and
    an evaluation score ğŸ”¥.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿ**å°†è®­ç»ƒå¥½çš„ä»£ç†æ¨é€åˆ°Hub**ï¼Œå¹¶é™„å¸¦ä¸€ä¸ªæ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•°ğŸ”¥ã€‚
- en: Prerequisites ğŸ—ï¸
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ˆå†³æ¡ä»¶ğŸ—ï¸
- en: 'Before diving into the notebook, you need to:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥ç ”ç©¶ç¬”è®°æœ¬ä¹‹å‰ï¼Œæ‚¨éœ€è¦ï¼š
- en: ğŸ”² ğŸ“š [Study Policy Gradients by reading Unit 4](https://huggingface.co/deep-rl-course/unit4/introduction)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”² ğŸ“š [é€šè¿‡é˜…è¯»ç¬¬4å•å…ƒæ¥å­¦ä¹ ç­–ç•¥æ¢¯åº¦](https://huggingface.co/deep-rl-course/unit4/introduction)
- en: Letâ€™s code Reinforce algorithm from scratch ğŸ”¥
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ç¼–å†™Reinforceç®—æ³• ğŸ”¥
- en: Some advice ğŸ’¡
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€äº›å»ºè®® ğŸ’¡
- en: Itâ€™s better to run this colab in a copy on your Google Drive, so that **if it
    times out** you still have the saved notebook on your Google Drive and do not
    need to fill everything in from scratch.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¥½åœ¨æ‚¨çš„è°·æ­Œé©±åŠ¨å™¨ä¸Šè¿è¡Œæ­¤colabï¼Œè¿™æ ·**å¦‚æœè¶…æ—¶**ï¼Œæ‚¨ä»ç„¶å¯ä»¥åœ¨æ‚¨çš„è°·æ­Œé©±åŠ¨å™¨ä¸Šä¿å­˜ç¬”è®°æœ¬ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»å¤´å¼€å§‹å¡«å†™æ‰€æœ‰å†…å®¹ã€‚
- en: To do that you can either do `Ctrl + S` or `File > Save a copy in Google Drive.`
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `Ctrl + S` æˆ– `æ–‡ä»¶ > åœ¨è°·æ­Œé©±åŠ¨å™¨ä¸­ä¿å­˜å‰¯æœ¬ã€‚`
- en: Set the GPU ğŸ’ª
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®GPU ğŸ’ª
- en: To **accelerate the agentâ€™s training, weâ€™ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†**åŠ é€Ÿä»£ç†çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨GPU**ã€‚ä¸ºæ­¤ï¼Œè¯·è½¬åˆ° `è¿è¡Œæ—¶ > æ›´æ”¹è¿è¡Œæ—¶ç±»å‹`
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![GPU æ­¥éª¤ 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
- en: '`Hardware Accelerator > GPU`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ç¡¬ä»¶åŠ é€Ÿå™¨ > GPU`'
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![GPU æ­¥éª¤ 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
- en: Create a virtual display ğŸ–¥
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ˜¾ç¤º ğŸ–¥
- en: During the notebook, weâ€™ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªé‡æ’­è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œåœ¨colabä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚
- en: The following cell will install the librairies and create and run a virtual
    screen ğŸ–¥
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å•å…ƒæ ¼å°†å®‰è£…åº“å¹¶åˆ›å»ºå¹¶è¿è¡Œä¸€ä¸ªè™šæ‹Ÿå±å¹• ğŸ–¥
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install the dependencies ğŸ”½
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®‰è£…ä¾èµ– ğŸ”½
- en: 'The first step is to install the dependencies. Weâ€™ll install multiple ones:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯å®‰è£…ä¾èµ–é¡¹ã€‚æˆ‘ä»¬å°†å®‰è£…å¤šä¸ªï¼š
- en: '`gym`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gym`'
- en: '`gym-games`: Extra gym environments made with PyGame.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gym-games`ï¼šä½¿ç”¨PyGameåˆ¶ä½œçš„é¢å¤–gymç¯å¢ƒã€‚'
- en: '`huggingface_hub`: The Hub works as a central place where anyone can share
    and explore models and datasets. It has versioning, metrics, visualizations, and
    other features that will allow you to easily collaborate with others.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_hub`ï¼šHubä½œä¸ºä¸€ä¸ªä¸­å¿ƒåœ°æ–¹ï¼Œä»»ä½•äººéƒ½å¯ä»¥åˆ†äº«å’Œæ¢ç´¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒå…·æœ‰ç‰ˆæœ¬æ§åˆ¶ã€æŒ‡æ ‡ã€å¯è§†åŒ–å’Œå…¶ä»–åŠŸèƒ½ï¼Œå¯ä»¥è®©æ‚¨è½»æ¾ä¸ä»–äººåˆä½œã€‚'
- en: You may be wondering why we install gym and not gymnasium, a more recent version
    of gym? **Because the gym-games we are using are not updated yet with gymnasium**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬å®‰è£…gymè€Œä¸æ˜¯gymnasiumï¼Œä¸€ä¸ªæ›´æ–°ç‰ˆæœ¬çš„gymï¼Ÿ**å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„gym-gamesè¿˜æ²¡æœ‰ä¸gymnasiumæ›´æ–°ã€‚**
- en: 'The differences youâ€™ll encounter here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œä½ ä¼šé‡åˆ°çš„åŒºåˆ«ï¼š
- en: In `gym` we donâ€™t have `terminated` and `truncated` but only `done`.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`gym`ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰`terminated`å’Œ`truncated`ï¼Œåªæœ‰`done`ã€‚
- en: In `gym` using `env.step()` returns `state, reward, done, info`
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`gym`ä¸­ï¼Œä½¿ç”¨`env.step()`è¿”å›`state, reward, done, info`
- en: You can learn more about the differences between Gym and Gymnasium here ğŸ‘‰ [https://gymnasium.farama.org/content/migration-guide/](https://gymnasium.farama.org/content/migration-guide/)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨è¿™é‡Œäº†è§£Gymå’ŒGymnasiumä¹‹é—´çš„åŒºåˆ« ğŸ‘‰ [https://gymnasium.farama.org/content/migration-guide/](https://gymnasium.farama.org/content/migration-guide/)
- en: You can see here all the Reinforce models available ğŸ‘‰ [https://huggingface.co/models?other=reinforce](https://huggingface.co/models?other=reinforce)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ‰€æœ‰å¯ç”¨çš„Reinforceæ¨¡å‹ ğŸ‘‰ [https://huggingface.co/models?other=reinforce](https://huggingface.co/models?other=reinforce)
- en: And you can find all the Deep Reinforcement Learning models here ğŸ‘‰ [https://huggingface.co/models?pipeline_tag=reinforcement-learning](https://huggingface.co/models?pipeline_tag=reinforcement-learning)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æ‰€æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ ğŸ‘‰ [https://huggingface.co/models?pipeline_tag=reinforcement-learning](https://huggingface.co/models?pipeline_tag=reinforcement-learning)
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Import the packages ğŸ“¦
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥åŒ… ğŸ“¦
- en: 'In addition to importing the installed libraries, we also import:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å¯¼å…¥å·²å®‰è£…çš„åº“ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¼å…¥ï¼š
- en: '`imageio`: A library that will help us to generate a replay video'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imageio`ï¼šä¸€ä¸ªå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç”Ÿæˆé‡æ’­è§†é¢‘çš„åº“'
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Check if we have a GPU
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰GPU
- en: Letâ€™s check if we have a GPU
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰GPU
- en: If itâ€™s the case you should see `device:cuda0`
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ˜¯çš„è¯ï¼Œä½ åº”è¯¥çœ‹åˆ° `device:cuda0`
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Weâ€™re now ready to implement our Reinforce algorithm ğŸ”¥
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡å®ç°æˆ‘ä»¬çš„Reinforceç®—æ³• ğŸ”¥
- en: 'First agent: Playing CartPole-v1 ğŸ¤–'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªä»£ç†ï¼šç©CartPole-v1 ğŸ¤–
- en: Create the CartPole environment and understand how it works
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºCartPoleç¯å¢ƒå¹¶äº†è§£å…¶å·¥ä½œåŸç†
- en: The environment ğŸ®
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¯å¢ƒ ğŸ®
- en: Why do we use a simple environment like CartPole-v1?
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ä½¿ç”¨åƒCartPole-v1è¿™æ ·ç®€å•çš„ç¯å¢ƒï¼Ÿ
- en: As explained in [Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html),
    when you implement your agent from scratch, you need **to be sure that it works
    correctly and find bugs with easy environments before going deeper** as finding
    bugs will be much easier in simple environments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å¼ºåŒ–å­¦ä¹ æŠ€å·§å’ŒæŠ€å·§](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)ä¸­æ‰€è§£é‡Šçš„ï¼Œå½“æ‚¨ä»å¤´å¼€å§‹å®ç°ä»£ç†æ—¶ï¼Œæ‚¨éœ€è¦**ç¡®ä¿å®ƒèƒ½å¤Ÿæ­£ç¡®è¿è¡Œï¼Œå¹¶åœ¨æ›´ç®€å•çš„ç¯å¢ƒä¸­æ‰¾åˆ°é”™è¯¯**ï¼Œå› ä¸ºåœ¨ç®€å•ç¯å¢ƒä¸­æ‰¾åˆ°é”™è¯¯ä¼šæ›´å®¹æ˜“ã€‚
- en: Try to have some â€œsign of lifeâ€ on toy problems
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å°è¯•åœ¨ç©å…·é—®é¢˜ä¸Šæœ‰ä¸€äº›â€œç”Ÿå‘½è¿¹è±¡â€
- en: Validate the implementation by making it run on harder and harder envs (you
    can compare results against the RL zoo). You usually need to run hyperparameter
    optimization for that step.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨è¶Šæ¥è¶Šå›°éš¾çš„ç¯å¢ƒä¸­è¿è¡Œæ¥éªŒè¯å®ç°ï¼ˆæ‚¨å¯ä»¥å°†ç»“æœä¸RLåŠ¨ç‰©å›­è¿›è¡Œæ¯”è¾ƒï¼‰ã€‚é€šå¸¸éœ€è¦è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–æ¥è¿›è¡Œè¿™ä¸€æ­¥ã€‚
- en: The CartPole-v1 environment
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CartPole-v1ç¯å¢ƒ
- en: A pole is attached by an un-actuated joint to a cart, which moves along a frictionless
    track. The pendulum is placed upright on the cart and the goal is to balance the
    pole by applying forces in the left and right direction on the cart.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ†é€šè¿‡ä¸€ä¸ªæœªæ¿€æ´»çš„å…³èŠ‚è¿æ¥åˆ°ä¸€ä¸ªå°è½¦ä¸Šï¼Œè¯¥å°è½¦æ²¿ç€ä¸€ä¸ªæ— æ‘©æ“¦çš„è½¨é“ç§»åŠ¨ã€‚æ‘†æ”¾åœ¨å°è½¦ä¸Šæ–¹ï¼Œå¹¶ä¸”ç›®æ ‡æ˜¯é€šè¿‡åœ¨å°è½¦å·¦å³æ–¹å‘ä¸Šæ–½åŠ åŠ›æ¥å¹³è¡¡æ†ã€‚
- en: So, we start with CartPole-v1\. The goal is to push the cart left or right **so
    that the pole stays in the equilibrium.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä»¬ä»CartPole-v1å¼€å§‹ã€‚ç›®æ ‡æ˜¯å°†å°è½¦å‘å·¦æˆ–å‘å³æ¨åŠ¨ï¼Œ**ä½¿æ†ä¿æŒåœ¨å¹³è¡¡çŠ¶æ€ã€‚**
- en: 'The episode ends if:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœï¼š
- en: The pole Angle is greater than Â±12Â°
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ†è§’å¤§äºÂ±12Â°
- en: The Cart Position is greater than Â±2.4
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è½¦ä½ç½®å¤§äºÂ±2.4
- en: The episode length is greater than 500
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥é›†æ•°é•¿åº¦å¤§äº500
- en: We get a reward ğŸ’° of +1 every timestep that the Pole stays in the equilibrium.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥è·å¾— +1 çš„å¥–åŠ± ğŸ’°ï¼Œæ†ä¿æŒåœ¨å¹³è¡¡çŠ¶æ€ã€‚
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Letâ€™s build the Reinforce Architecture
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ„å»ºReinforceæ¶æ„
- en: 'This implementation is based on three implementations:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®ç°åŸºäºä¸‰ä¸ªå®ç°ï¼š
- en: '[PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorchå®˜æ–¹å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)'
- en: '[Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)'
- en: '[Improvement of the integration by Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç”±Chris1nexusæ”¹è¿›çš„é›†æˆ](https://github.com/huggingface/deep-rl-class/pull/95)'
- en: '![Reinforce](../Images/36cf0376b1e1c1f32df0bf4cc6199001.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Reinforce](../Images/36cf0376b1e1c1f32df0bf4cc6199001.png)'
- en: 'So we want:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›ï¼š
- en: Two fully connected layers (fc1 and fc2).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆfc1å’Œfc2ï¼‰ã€‚
- en: To use ReLU as activation function of fc1
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ReLUä½œä¸ºfc1çš„æ¿€æ´»å‡½æ•°
- en: To use Softmax to output a probability distribution over actions
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Softmaxè¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Solution
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: I made a mistake, can you guess where?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œä½ èƒ½çŒœåˆ°åœ¨å“ªé‡Œå—ï¼Ÿ
- en: 'To find out letâ€™s make a forward pass:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¾å‡ºæ¥ï¼Œè®©æˆ‘ä»¬è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼š
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here we see that the error says `ValueError: The value argument to log_prob
    must be a Tensor`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°é”™è¯¯è¯´`ValueError: The value argument to log_prob must be a Tensor`'
- en: It means that `action` in `m.log_prob(action)` must be a Tensor **but itâ€™s not.**
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€`m.log_prob(action)`ä¸­çš„`action`å¿…é¡»æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œä½†å®ƒä¸æ˜¯ã€‚
- en: Do you know why? Check the act function and try to see why it does not work.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ä¸ºä»€ä¹ˆå—ï¼Ÿæ£€æŸ¥actå‡½æ•°ï¼Œçœ‹çœ‹ä¸ºä»€ä¹ˆå®ƒä¸èµ·ä½œç”¨ã€‚
- en: 'Advice ğŸ’¡: Something is wrong in this implementation. Remember that for the
    act function **we want to sample an action from the probability distribution over
    actions**.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºè®®ğŸ’¡ï¼šè¿™ä¸ªå®ç°ä¸­æœ‰é—®é¢˜ã€‚è®°ä½å¯¹äºactå‡½æ•°ï¼Œæˆ‘ä»¬æƒ³è¦ä»åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œã€‚
- en: (Real) Solution
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ï¼ˆçœŸæ­£çš„ï¼‰è§£å†³æ–¹æ¡ˆ
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By using CartPole, it was easier to debug since **we know that the bug comes
    from our integration and not from our simple environment**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨CartPoleï¼Œæ›´å®¹æ˜“è°ƒè¯•ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“é”™è¯¯æ¥è‡ªäºæˆ‘ä»¬çš„é›†æˆï¼Œè€Œä¸æ˜¯æˆ‘ä»¬ç®€å•çš„ç¯å¢ƒã€‚
- en: Since **we want to sample an action from the probability distribution over actions**,
    we canâ€™t use `action = np.argmax(m)` since it will always output the action that
    has the highest probability.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬æƒ³è¦ä»åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨`action = np.argmax(m)`ï¼Œå› ä¸ºå®ƒæ€»æ˜¯è¾“å‡ºå…·æœ‰æœ€é«˜æ¦‚ç‡çš„åŠ¨ä½œã€‚
- en: We need to replace this with `action = m.sample()` which will sample an action
    from the probability distribution P(.|s)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ç”¨`action = m.sample()`æ›¿æ¢è¿™ä¸ªï¼Œå®ƒå°†ä»æ¦‚ç‡åˆ†å¸ƒP(.|s)ä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ
- en: Letâ€™s build the Reinforce Training Algorithm
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ„å»ºReinforceè®­ç»ƒç®—æ³•
- en: 'This is the Reinforce algorithm pseudocode:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯Reinforceç®—æ³•çš„ä¼ªä»£ç ï¼š
- en: '![Policy gradient pseudocode](../Images/98fc23971db3e1a9e35baeee827641dc.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![ç­–ç•¥æ¢¯åº¦ä¼ªä»£ç ](../Images/98fc23971db3e1a9e35baeee827641dc.png)'
- en: When we calculate the return Gt (line 6), we see that we calculate the sum of
    discounted rewards **starting at timestep t**.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è®¡ç®—å›æŠ¥Gtï¼ˆç¬¬6è¡Œï¼‰æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬è®¡ç®—ä»æ—¶é—´æ­¥tå¼€å§‹çš„æŠ˜æ‰£å¥–åŠ±çš„æ€»å’Œã€‚
- en: 'Why? Because our policy should only **reinforce actions on the basis of the
    consequences**: so rewards obtained before taking an action are useless (since
    they were not because of the action), **only the ones that come after the action
    matters**.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæˆ‘ä»¬çš„ç­–ç•¥åº”è¯¥åªæ ¹æ®åæœæ¥å¼ºåŒ–åŠ¨ä½œï¼šå› æ­¤åœ¨é‡‡å–è¡ŒåŠ¨ä¹‹å‰è·å¾—çš„å¥–åŠ±æ˜¯æ— ç”¨çš„ï¼ˆå› ä¸ºå®ƒä»¬ä¸æ˜¯å› ä¸ºè¡ŒåŠ¨ï¼‰ï¼Œåªæœ‰ä¹‹åçš„å¥–åŠ±æ‰é‡è¦ã€‚
- en: Before coding this you should read this section [donâ€™t let the past distract
    you](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)
    that explains why we use reward-to-go policy gradient.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç¼–å†™è¿™ä¹‹å‰ï¼Œä½ åº”è¯¥é˜…è¯»è¿™ä¸€éƒ¨åˆ†[ä¸è¦è®©è¿‡å»åˆ†æ•£ä½ çš„æ³¨æ„åŠ›](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨å¥–åŠ±é€æ­¥ç­–ç•¥æ¢¯åº¦ã€‚
- en: We use an interesting technique coded by [Chris1nexus](https://github.com/Chris1nexus)
    to **compute the return at each timestep efficiently**. The comments explained
    the procedure. Donâ€™t hesitate also [to check the PR explanation](https://github.com/huggingface/deep-rl-class/pull/95)
    But overall the idea is to **compute the return at each timestep efficiently**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç”±[Chris1nexus](https://github.com/Chris1nexus)ç¼–å†™çš„æœ‰è¶£çš„æŠ€æœ¯æ¥é«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥ã€‚è¯„è®ºä¸­è§£é‡Šäº†è¯¥è¿‡ç¨‹ã€‚ä¹Ÿä¸è¦çŠ¹è±«[æŸ¥çœ‹PRè§£é‡Š](https://github.com/huggingface/deep-rl-class/pull/95)ï¼Œä½†æ€»ä½“æ€è·¯æ˜¯é«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥ã€‚
- en: The second question you may ask is **why do we minimize the loss**? Didnâ€™t we
    talk about Gradient Ascent, not Gradient Descent earlier?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªé—®é¢˜ä½ å¯èƒ½ä¼šé—®çš„æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦æœ€å°åŒ–æŸå¤±ï¼Ÿä¹‹å‰ä¸æ˜¯è®¨è®ºè¿‡æ¢¯åº¦ä¸Šå‡ï¼Œè€Œä¸æ˜¯æ¢¯åº¦ä¸‹é™å—ï¼Ÿ
- en: We want to maximize our utility function $J(\theta)$, but in PyTorch and TensorFlow,
    itâ€™s better to **minimize an objective function.**
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–æˆ‘ä»¬çš„æ•ˆç”¨å‡½æ•°$J(\theta)$ï¼Œä½†åœ¨PyTorchå’ŒTensorFlowä¸­ï¼Œæ›´å¥½çš„åšæ³•æ˜¯æœ€å°åŒ–ä¸€ä¸ªç›®æ ‡å‡½æ•°ã€‚
- en: So letâ€™s say we want to reinforce action 3 at a certain timestep. Before training
    this action P is 0.25.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³è¦åœ¨æŸä¸ªæ—¶é—´æ­¥å¼ºåŒ–åŠ¨ä½œ3ã€‚åœ¨è®­ç»ƒè¿™ä¸ªåŠ¨ä½œä¹‹å‰ï¼ŒPä¸º0.25ã€‚
- en: So we want to modify<math><semantics><mrow><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi></mrow><annotation
    encoding="application/x-tex">theta</annotation></semantics></math> theta such
    that<math><semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi
    mathvariant="normal">âˆ£</mi><mi>s</mi><mo separator="true">;</mo><mi>Î¸</mi><mo
    stretchy="false">)</mo><mo>></mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s;
    \theta) > 0.25</annotation></semantics></math> Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)>0.25
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æƒ³è¦ä¿®æ”¹Î¸ thetaï¼Œä½¿å¾—Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)>0.25
- en: Because all P must sum to 1, max<math><semantics><mrow><mi>p</mi><msub><mi>i</mi><mi>Î¸</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo
    separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">pi_\theta(a_3|s; \theta)</annotation></semantics></math>piÎ¸â€‹(a3â€‹âˆ£s;Î¸)
    will **minimize other action probability.**
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› ä¸ºæ‰€æœ‰På¿…é¡»æ€»å’Œä¸º1ï¼Œmax<math><semantics><mrow><mi>p</mi><msub><mi>i</mi><mi>Î¸</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo
    separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">pi_\theta(a_3|s; \theta)</annotation></semantics></math>piÎ¸â€‹(a3â€‹âˆ£s;Î¸)å°†**æœ€å°åŒ–å…¶ä»–åŠ¨ä½œçš„æ¦‚ç‡ã€‚**
- en: So we should tell PyTorch **to min<math><semantics><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo
    separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">1 - \pi_\theta(a_3|s; \theta)</annotation></semantics></math>1âˆ’Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸).**
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åº”è¯¥å‘Šè¯‰PyTorch**æœ€å°åŒ–<math><semantics><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo
    separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">1 - \pi_\theta(a_3|s; \theta)</annotation></semantics></math>1âˆ’Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)ã€‚**
- en: This loss function approaches 0 as<math><semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo
    separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a_3|s; \theta)</annotation></semantics></math>Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)
    nears 1.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸå¤±å‡½æ•°åœ¨<math><semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi
    mathvariant="normal">âˆ£</mi><mi>s</mi><mo separator="true">;</mo><mi>Î¸</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s;
    \theta)</annotation></semantics></math>Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)æ¥è¿‘1æ—¶è¶‹è¿‘äº0ã€‚
- en: So we are encouraging the gradient to max<math><semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">âˆ£</mi><mi>s</mi><mo
    separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a_3|s; \theta)</annotation></semantics></math>Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±æ¢¯åº¦æœ€å¤§åŒ–<math><semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi
    mathvariant="normal">âˆ£</mi><mi>s</mi><mo separator="true">;</mo><mi>Î¸</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s;
    \theta)</annotation></semantics></math>Ï€Î¸â€‹(a3â€‹âˆ£s;Î¸)
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Solution
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Train it
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®ƒ
- en: Weâ€™re now ready to train our agent.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†ã€‚
- en: But first, we define a variable containing all the training hyperparameters.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è®­ç»ƒè¶…å‚æ•°çš„å˜é‡ã€‚
- en: You can change the training parameters (and should ğŸ˜‰)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æ›´æ”¹è®­ç»ƒå‚æ•°ï¼ˆå¹¶åº”è¯¥ğŸ˜‰ï¼‰
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Define evaluation method ğŸ“
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰è¯„ä¼°æ–¹æ³• ğŸ“
- en: Here we define the evaluation method that weâ€™re going to use to test our Reinforce
    agent.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†è¦ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•æ¥æµ‹è¯•æˆ‘ä»¬çš„Reinforceä»£ç†ã€‚
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Evaluate our agent ğŸ“ˆ
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æˆ‘ä»¬çš„ä»£ç† ğŸ“ˆ
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Publish our trained model on the Hub ğŸ”¥
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨Hubä¸Šå‘å¸ƒæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹ğŸ”¥
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub ğŸ¤— with one line of code.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒåå–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹å‘å¸ƒåˆ°hub ğŸ¤—ã€‚
- en: 'Hereâ€™s an example of a Model Card:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ¨¡å‹å¡çš„ç¤ºä¾‹ï¼š
- en: '![](../Images/afdb3be9b09aa528c9ee40968ca15774.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afdb3be9b09aa528c9ee40968ca15774.png)'
- en: Push to the Hub
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨é€åˆ°Hub
- en: Do not modify this code
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸è¦ä¿®æ”¹è¿™æ®µä»£ç 
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: By using `push_to_hub`, **you evaluate, record a replay, generate a model card
    of your agent, and push it to the Hub**.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨`push_to_hub`ï¼Œ**æ‚¨å¯ä»¥è¯„ä¼°ã€è®°å½•é‡æ’­ã€ç”Ÿæˆä»£ç†çš„æ¨¡å‹å¡ï¼Œå¹¶å°†å…¶æ¨é€åˆ°Hub**ã€‚
- en: 'This way:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼š
- en: You can **showcase our work** ğŸ”¥
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ**ğŸ”¥
- en: You can **visualize your agent playing** ğŸ‘€
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**å¯è§†åŒ–æ‚¨çš„ä»£ç†è¿›è¡Œæ¸¸æˆ**ğŸ‘€
- en: You can **share an agent with the community that others can use** ğŸ’¾
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**ä¸ç¤¾åŒºåˆ†äº«ä¸€ä¸ªå…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„ä»£ç†**ğŸ’¾
- en: You can **access a leaderboard ğŸ† to see how well your agent is performing compared
    to your classmates** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**è®¿é—®æ’è¡Œæ¦œğŸ†ï¼ŒæŸ¥çœ‹æ‚¨çš„ä»£ç†ä¸åŒå­¦ç›¸æ¯”è¡¨ç°å¦‚ä½•**ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œéœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤ï¼š
- en: 1ï¸âƒ£ (If itâ€™s not already done) create an account to HF â¡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰åˆ›å»ºä¸€ä¸ªHFå¸æˆ·â¡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2ï¸âƒ£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç™»å½•ï¼Œç„¶åï¼Œæ‚¨éœ€è¦å­˜å‚¨æ¥è‡ªHugging Faceç½‘ç«™çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªæ–°çš„ä»¤ç‰Œï¼ˆ[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)ï¼‰**å…·æœ‰å†™å…¥è§’è‰²**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ›å»ºHFä»¤ç‰Œ](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you donâ€™t want to use Google Colab or a Jupyter Notebook, you need to use
    this command instead: `huggingface-cli login` (or `login`)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨Google Colabæˆ–Jupyter Notebookï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`ï¼ˆæˆ–`login`ï¼‰
- en: 3ï¸âƒ£ Weâ€™re now ready to push our trained agent to the ğŸ¤— Hub ğŸ”¥ using `package_to_hub()`
    function
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡å°†æˆ‘ä»¬è®­ç»ƒçš„ä»£ç†æ¨é€åˆ°ğŸ¤— Hub ğŸ”¥ï¼Œä½¿ç”¨`package_to_hub()`å‡½æ•°
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now that we tested the robustness of our implementation, letâ€™s try a more complex
    environment: PixelCopter ğŸš'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æµ‹è¯•äº†æˆ‘ä»¬å®ç°çš„é²æ£’æ€§ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ›´å¤æ‚çš„ç¯å¢ƒï¼šPixelCopter ğŸš
- en: 'Second agent: PixelCopter ğŸš'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªä»£ç†ï¼šPixelCopter ğŸš
- en: Study the PixelCopter environment ğŸ‘€
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç ”ç©¶PixelCopterç¯å¢ƒğŸ‘€
- en: '[The Environment documentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç¯å¢ƒæ–‡æ¡£](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
- en: '[PRE25]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The observation space (7) ğŸ‘€:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿç©ºé—´ï¼ˆ7ï¼‰ğŸ‘€ï¼š
- en: player y position
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©å®¶yä½ç½®
- en: player velocity
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©å®¶é€Ÿåº¦
- en: player distance to floor
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©å®¶åˆ°åœ°æ¿çš„è·ç¦»
- en: player distance to ceiling
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©å®¶åˆ°å¤©èŠ±æ¿çš„è·ç¦»
- en: next block x distance to player
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªæ–¹å—xè·ç¦»ç©å®¶
- en: next blocks top y location
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªæ–¹å—çš„é¡¶éƒ¨yä½ç½®
- en: next blocks bottom y location
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªæ–¹å—çš„åº•éƒ¨yä½ç½®
- en: 'The action space(2) ğŸ®:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œç©ºé—´ï¼ˆ2ï¼‰ï¼š
- en: Up (press accelerator)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ é€Ÿï¼ˆæŒ‰ä¸‹åŠ é€Ÿå™¨ï¼‰
- en: Do nothing (donâ€™t press accelerator)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆéƒ½ä¸åšï¼ˆä¸æŒ‰ä¸‹åŠ é€Ÿå™¨ï¼‰
- en: 'The reward function ğŸ’°:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å‡½æ•°ï¼š
- en: For each vertical block it passes, it gains a positive reward of +1\. Each time
    a terminal state is reached it receives a negative reward of -1.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯é€šè¿‡ä¸€ä¸ªå‚ç›´å—ï¼Œå®ƒè·å¾—+1çš„æ­£å¥–åŠ±ã€‚æ¯æ¬¡è¾¾åˆ°ç»ˆæ­¢çŠ¶æ€æ—¶ï¼Œå®ƒä¼šè·å¾—-1çš„è´Ÿå¥–åŠ±ã€‚
- en: Define the new Policy ğŸ§ 
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®šä¹‰æ–°ç­–ç•¥
- en: We need to have a deeper neural network since the environment is more complex
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºç¯å¢ƒæ›´åŠ å¤æ‚ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´æ·±å±‚çš„ç¥ç»ç½‘ç»œ
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Solution
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE29]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Define the hyperparameters âš™ï¸
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®šä¹‰è¶…å‚æ•°
- en: Because this environment is more complex.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› ä¸ºè¿™ä¸ªç¯å¢ƒæ›´åŠ å¤æ‚ã€‚
- en: Especially for the hidden size, we need more neurons.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯å¯¹äºéšè—å¤§å°ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ç¥ç»å…ƒã€‚
- en: '[PRE30]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Train it
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®ƒ
- en: Weâ€™re now ready to train our agent ğŸ”¥.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†ã€‚
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Publish our trained model on the Hub ğŸ”¥
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨Hubä¸Šå‘å¸ƒæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Some additional challenges ğŸ†
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜
- en: The best way to learn **is to try things on your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    But also try to find better parameters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„æœ€ä½³æ–¹å¼æ˜¯è‡ªå·±å°è¯•ï¼æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰ä»£ç†è¡¨ç°ä¸ä½³ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œæ‚¨å¯ä»¥è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚ä½†ä¹Ÿå°è¯•æ‰¾åˆ°æ›´å¥½çš„å‚æ•°ã€‚
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ’è¡Œæ¦œä¸­ï¼Œæ‚¨å°†æ‰¾åˆ°æ‚¨çš„ä»£ç†ã€‚æ‚¨èƒ½å¤Ÿè¾¾åˆ°æ¦œé¦–å—ï¼Ÿ
- en: 'Here are some ideas to climb up the leaderboard:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€äº›æå‡æ’è¡Œæ¦œçš„æƒ³æ³•ï¼š
- en: Train more steps
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ›´å¤šæ­¥éª¤
- en: Try different hyperparameters by looking at what your classmates have done ğŸ‘‰
    [https://huggingface.co/models?other=reinforce](https://huggingface.co/models?other=reinforce)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•ä¸åŒçš„è¶…å‚æ•°ï¼ŒæŸ¥çœ‹åŒå­¦ä»¬çš„åšæ³•
- en: '**Push your new trained model** on the Hub ğŸ”¥'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„æ–°è®­ç»ƒæ¨¡å‹å‘å¸ƒåˆ°Hub
- en: '**Improving the implementation for more complex environments** (for instance,
    what about changing the network to a Convolutional Neural Network to handle frames
    as observation)?'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¹è¿›å®ç°ä»¥é€‚åº”æ›´å¤æ‚çš„ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œå°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œä»¥å¤„ç†å¸§ä½œä¸ºè§‚å¯Ÿï¼Ÿï¼‰
- en: '* * *'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Congrats on finishing this unit**!Â There was a lot of information. And congrats
    on finishing the tutorial. Youâ€™ve just coded your first Deep Reinforcement Learning
    agent from scratch using PyTorch and shared it on the Hub ğŸ¥³.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥è´ºæ‚¨å®Œæˆè¿™ä¸ªå•å…ƒï¼è¿™é‡Œæœ‰å¾ˆå¤šä¿¡æ¯ã€‚ç¥è´ºæ‚¨å®Œæˆæ•™ç¨‹ã€‚æ‚¨åˆšåˆšä½¿ç”¨PyTorchä»å¤´å¼€å§‹ç¼–å†™äº†æ‚¨çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œå¹¶åœ¨Hubä¸Šåˆ†äº«äº†å®ƒã€‚
- en: Donâ€™t hesitate to iterate on this unit **by improving the implementation for
    more complex environments** (for instance, what about changing the network to
    a Convolutional Neural Network to handle frames as observation)?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦çŠ¹è±«ï¼Œé€šè¿‡æ”¹è¿›å®ç°ä»¥é€‚åº”æ›´å¤æ‚çš„ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œå°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œä»¥å¤„ç†å¸§ä½œä¸ºè§‚å¯Ÿï¼‰æ¥è¿­ä»£è¿™ä¸ªå•å…ƒã€‚
- en: In the next unit, **weâ€™re going to learn more about Unity MLAgents**, by training
    agents in Unity environments. This way, you will be ready to participate in the
    **AI vs AI challenges where youâ€™ll train your agents to compete against other
    agents in a snowball fight and a soccer game.**
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªå•å…ƒä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ æ›´å¤šå…³äºUnity MLAgentsï¼Œé€šè¿‡åœ¨Unityç¯å¢ƒä¸­è®­ç»ƒä»£ç†ã€‚è¿™æ ·ï¼Œæ‚¨å°†å‡†å¤‡å¥½å‚åŠ AIå¯¹AIæŒ‘æˆ˜ï¼Œæ‚¨å°†è®­ç»ƒæ‚¨çš„ä»£ç†æ¥ä¸å…¶ä»–ä»£ç†åœ¨é›ªä»—å’Œè¶³çƒæ¯”èµ›ä¸­ç«äº‰ã€‚
- en: Sound fun? See you next time!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥å¾ˆæœ‰è¶£å—ï¼Ÿä¸‹æ¬¡è§ï¼
- en: Finally, we would love **to hear what you think of the course and how we can
    improve it**. If you have some feedback then please ğŸ‘‰ [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¾ˆæƒ³çŸ¥é“æ‚¨å¯¹è¯¾ç¨‹çš„çœ‹æ³•ä»¥åŠæˆ‘ä»¬å¦‚ä½•æ”¹è¿›å®ƒã€‚å¦‚æœæ‚¨æœ‰åé¦ˆæ„è§ï¼Œè¯·å¡«å†™æ­¤è¡¨æ ¼ã€‚
- en: See you in Unit 5! ğŸ”¥
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡è§åœ¨ç¬¬5å•å…ƒï¼
- en: Keep Learning, stay awesome ğŸ¤—
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»§ç»­å­¦ä¹ ï¼Œä¿æŒç²¾å½©
