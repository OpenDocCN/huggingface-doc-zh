- en: Hands on
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/hands-on](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/learn/deep-rl-course/unit4/hands-on](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![提问](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb)'
- en: Now that we’ve studied the theory behind Reinforce, **you’re ready to code your
    Reinforce agent with PyTorch**. And you’ll test its robustness using CartPole-v1
    and PixelCopter,.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了Reinforce背后的理论，**您已经准备好使用PyTorch编写您的Reinforce代理**。并且您将使用CartPole-v1和PixelCopter测试其稳健性。
- en: You’ll then be able to iterate and improve this implementation for more advanced
    environments.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将能够迭代和改进此实现，以适用于更高级的环境。
- en: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
- en: 'To validate this hands-on for the certification process, you need to push your
    trained models to the Hub and:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这个实践过程，您需要将训练好的模型推送到Hub并：
- en: Get a result of >= 350 for `Cartpole-v1`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得`Cartpole-v1`的结果>= 350
- en: Get a result of >= 5 for `PixelCopter`.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得`PixelCopter`的结果>= 5。
- en: To find your result, go to the leaderboard and find your model, **the result
    = mean_reward - std of reward**. **If you don’t see your model on the leaderboard,
    go at the bottom of the leaderboard page and click on the refresh button**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到您的结果，请转到排行榜并找到您的模型，**结果=平均奖励-奖励的标准差**。**如果您在排行榜上找不到您的模型，请转到排行榜页面底部，然后单击刷新按钮**。
- en: '**If you don’t find your model, go to the bottom of the page and click on the
    refresh button.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您找不到您的模型，请转到页面底部，然后单击刷新按钮。**'
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有关认证过程的更多信息，请查看此部分👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: And you can check your progress here 👉 [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里检查您的进度👉 [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
- en: '**To start the hands-on click on Open In Colab button** 👇 :'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**要开始实践，请点击“在Colab中打开”按钮**👇：'
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)'
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议学生们在进行实践练习时使用Google Colab**，而不是在个人计算机上运行它们**。
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Google Colab，**您可以专注于学习和实验，而不必担心设置环境的技术细节**。
- en: 'Unit 4: Code your first Deep Reinforcement Learning Algorithm with PyTorch:
    Reinforce. And test its robustness 💪'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4单元：使用PyTorch编写您的第一个深度强化学习算法：Reinforce。并测试其稳健性💪
- en: '![thumbnail](../Images/207886028f30a9a8c43010256f915e88.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![缩略图](../Images/207886028f30a9a8c43010256f915e88.png)'
- en: 'In this notebook, you’ll code your first Deep Reinforcement Learning algorithm
    from scratch: Reinforce (also called Monte Carlo Policy Gradient).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本笔记本中，您将从头开始编写您的第一个深度强化学习算法：Reinforce（也称为蒙特卡洛策略梯度）。
- en: 'Reinforce is a *Policy-based method*: a Deep Reinforcement Learning algorithm
    that tries **to optimize the policy directly without using an action-value function**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Reinforce是一种*基于策略的方法*：一种深度强化学习算法，试图**直接优化策略，而不使用动作值函数**。
- en: More precisely, Reinforce is a *Policy-gradient method*, a subclass of *Policy-based
    methods* that aims **to optimize the policy directly by estimating the weights
    of the optimal policy using gradient ascent**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，Reinforce是一种*策略梯度方法*，是*基于策略的方法*的一个子类，旨在**通过估计最优策略的权重来直接优化策略，使用梯度上升**。
- en: 'To test its robustness, we’re going to train it in 2 different simple environments:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试其稳健性，我们将在2个不同的简单环境中对其进行训练：
- en: Cartpole-v1
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cartpole-v1
- en: PixelcopterEnv
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PixelcopterEnv
- en: ⬇️ Here is an example of what **you will achieve at the end of this notebook.**
    ⬇️
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ⬇️ 这是**您将在本笔记本末尾实现的示例。** ⬇️
- en: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Environments](../Images/3b1f63eab47a364ef05dcdca4df7bf08.png)'
- en: '🎮 Environments:'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 🎮 环境：
- en: '[CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)'
- en: '[PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
- en: '📚 RL-Library:'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '📚 RL-Library:'
- en: Python
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: PyTorch
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断努力改进我们的教程，所以**如果您在本笔记本中发现一些问题**，请[在GitHub Repo上提出问题](https://github.com/huggingface/deep-rl-class/issues)。
- en: Objectives of this notebook 🏆
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本笔记本的目标🏆
- en: 'At the end of the notebook, you will:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的末尾，您将：
- en: Be able to **code a Reinforce algorithm from scratch using PyTorch.**
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**使用PyTorch从头开始编写一个Reinforce算法**。
- en: Be able to **test the robustness of your agent using simple environments.**
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**使用简单环境测试您的代理的稳健性**。
- en: Be able to **push your trained agent to the Hub** with a nice video replay and
    an evaluation score 🔥.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**将训练好的代理推送到Hub**，并附带一个漂亮的视频回放和评估分数🔥。
- en: Prerequisites 🏗️
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件🏗️
- en: 'Before diving into the notebook, you need to:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究笔记本之前，您需要：
- en: 🔲 📚 [Study Policy Gradients by reading Unit 4](https://huggingface.co/deep-rl-course/unit4/introduction)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 🔲 📚 [通过阅读第4单元来学习策略梯度](https://huggingface.co/deep-rl-course/unit4/introduction)
- en: Let’s code Reinforce algorithm from scratch 🔥
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们从头开始编写Reinforce算法 🔥
- en: Some advice 💡
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些建议 💡
- en: It’s better to run this colab in a copy on your Google Drive, so that **if it
    times out** you still have the saved notebook on your Google Drive and do not
    need to fill everything in from scratch.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最好在您的谷歌驱动器上运行此colab，这样**如果超时**，您仍然可以在您的谷歌驱动器上保存笔记本，并且不需要从头开始填写所有内容。
- en: To do that you can either do `Ctrl + S` or `File > Save a copy in Google Drive.`
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，您可以使用 `Ctrl + S` 或 `文件 > 在谷歌驱动器中保存副本。`
- en: Set the GPU 💪
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置GPU 💪
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了**加速代理的训练，我们将使用GPU**。为此，请转到 `运行时 > 更改运行时类型`
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![GPU 步骤 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
- en: '`Hardware Accelerator > GPU`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`硬件加速器 > GPU`'
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![GPU 步骤 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
- en: Create a virtual display 🖥
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个虚拟显示 🖥
- en: During the notebook, we’ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们需要生成一个重播视频。为此，在colab中，**我们需要有一个虚拟屏幕来渲染环境**（从而记录帧）。
- en: The following cell will install the librairies and create and run a virtual
    screen 🖥
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下单元格将安装库并创建并运行一个虚拟屏幕 🖥
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install the dependencies 🔽
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装依赖 🔽
- en: 'The first step is to install the dependencies. We’ll install multiple ones:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装依赖项。我们将安装多个：
- en: '`gym`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gym`'
- en: '`gym-games`: Extra gym environments made with PyGame.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gym-games`：使用PyGame制作的额外gym环境。'
- en: '`huggingface_hub`: The Hub works as a central place where anyone can share
    and explore models and datasets. It has versioning, metrics, visualizations, and
    other features that will allow you to easily collaborate with others.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_hub`：Hub作为一个中心地方，任何人都可以分享和探索模型和数据集。它具有版本控制、指标、可视化和其他功能，可以让您轻松与他人合作。'
- en: You may be wondering why we install gym and not gymnasium, a more recent version
    of gym? **Because the gym-games we are using are not updated yet with gymnasium**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们安装gym而不是gymnasium，一个更新版本的gym？**因为我们使用的gym-games还没有与gymnasium更新。**
- en: 'The differences you’ll encounter here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你会遇到的区别：
- en: In `gym` we don’t have `terminated` and `truncated` but only `done`.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`gym`中，我们没有`terminated`和`truncated`，只有`done`。
- en: In `gym` using `env.step()` returns `state, reward, done, info`
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`gym`中，使用`env.step()`返回`state, reward, done, info`
- en: You can learn more about the differences between Gym and Gymnasium here 👉 [https://gymnasium.farama.org/content/migration-guide/](https://gymnasium.farama.org/content/migration-guide/)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里了解Gym和Gymnasium之间的区别 👉 [https://gymnasium.farama.org/content/migration-guide/](https://gymnasium.farama.org/content/migration-guide/)
- en: You can see here all the Reinforce models available 👉 [https://huggingface.co/models?other=reinforce](https://huggingface.co/models?other=reinforce)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里看到所有可用的Reinforce模型 👉 [https://huggingface.co/models?other=reinforce](https://huggingface.co/models?other=reinforce)
- en: And you can find all the Deep Reinforcement Learning models here 👉 [https://huggingface.co/models?pipeline_tag=reinforcement-learning](https://huggingface.co/models?pipeline_tag=reinforcement-learning)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到所有深度强化学习模型 👉 [https://huggingface.co/models?pipeline_tag=reinforcement-learning](https://huggingface.co/models?pipeline_tag=reinforcement-learning)
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Import the packages 📦
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入包 📦
- en: 'In addition to importing the installed libraries, we also import:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了导入已安装的库之外，我们还导入：
- en: '`imageio`: A library that will help us to generate a replay video'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imageio`：一个可以帮助我们生成重播视频的库'
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Check if we have a GPU
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查一下是否有GPU
- en: Let’s check if we have a GPU
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们检查一下是否有GPU
- en: If it’s the case you should see `device:cuda0`
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是的话，你应该看到 `device:cuda0`
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’re now ready to implement our Reinforce algorithm 🔥
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备实现我们的Reinforce算法 🔥
- en: 'First agent: Playing CartPole-v1 🤖'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个代理：玩CartPole-v1 🤖
- en: Create the CartPole environment and understand how it works
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建CartPole环境并了解其工作原理
- en: The environment 🎮
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境 🎮
- en: Why do we use a simple environment like CartPole-v1?
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么我们要使用像CartPole-v1这样简单的环境？
- en: As explained in [Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html),
    when you implement your agent from scratch, you need **to be sure that it works
    correctly and find bugs with easy environments before going deeper** as finding
    bugs will be much easier in simple environments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如[强化学习技巧和技巧](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)中所解释的，当您从头开始实现代理时，您需要**确保它能够正确运行，并在更简单的环境中找到错误**，因为在简单环境中找到错误会更容易。
- en: Try to have some “sign of life” on toy problems
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尝试在玩具问题上有一些“生命迹象”
- en: Validate the implementation by making it run on harder and harder envs (you
    can compare results against the RL zoo). You usually need to run hyperparameter
    optimization for that step.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过在越来越困难的环境中运行来验证实现（您可以将结果与RL动物园进行比较）。通常需要运行超参数优化来进行这一步。
- en: The CartPole-v1 environment
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CartPole-v1环境
- en: A pole is attached by an un-actuated joint to a cart, which moves along a frictionless
    track. The pendulum is placed upright on the cart and the goal is to balance the
    pole by applying forces in the left and right direction on the cart.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个杆通过一个未激活的关节连接到一个小车上，该小车沿着一个无摩擦的轨道移动。摆放在小车上方，并且目标是通过在小车左右方向上施加力来平衡杆。
- en: So, we start with CartPole-v1\. The goal is to push the cart left or right **so
    that the pole stays in the equilibrium.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们从CartPole-v1开始。目标是将小车向左或向右推动，**使杆保持在平衡状态。**
- en: 'The episode ends if:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果：
- en: The pole Angle is greater than ±12°
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆角大于±12°
- en: The Cart Position is greater than ±2.4
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车位置大于±2.4
- en: The episode length is greater than 500
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该集数长度大于500
- en: We get a reward 💰 of +1 every timestep that the Pole stays in the equilibrium.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个时间步获得 +1 的奖励 💰，杆保持在平衡状态。
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s build the Reinforce Architecture
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们构建Reinforce架构
- en: 'This implementation is based on three implementations:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现基于三个实现：
- en: '[PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch官方强化学习示例](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)'
- en: '[Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)'
- en: '[Improvement of the integration by Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[由Chris1nexus改进的集成](https://github.com/huggingface/deep-rl-class/pull/95)'
- en: '![Reinforce](../Images/36cf0376b1e1c1f32df0bf4cc6199001.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Reinforce](../Images/36cf0376b1e1c1f32df0bf4cc6199001.png)'
- en: 'So we want:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们希望：
- en: Two fully connected layers (fc1 and fc2).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个全连接层（fc1和fc2）。
- en: To use ReLU as activation function of fc1
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ReLU作为fc1的激活函数
- en: To use Softmax to output a probability distribution over actions
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Softmax输出动作的概率分布
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Solution
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: I made a mistake, can you guess where?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我犯了一个错误，你能猜到在哪里吗？
- en: 'To find out let’s make a forward pass:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了找出来，让我们进行一次前向传播：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here we see that the error says `ValueError: The value argument to log_prob
    must be a Tensor`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在这里我们看到错误说`ValueError: The value argument to log_prob must be a Tensor`'
- en: It means that `action` in `m.log_prob(action)` must be a Tensor **but it’s not.**
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这意味着`m.log_prob(action)`中的`action`必须是一个张量，但它不是。
- en: Do you know why? Check the act function and try to see why it does not work.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道为什么吗？检查act函数，看看为什么它不起作用。
- en: 'Advice 💡: Something is wrong in this implementation. Remember that for the
    act function **we want to sample an action from the probability distribution over
    actions**.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 建议💡：这个实现中有问题。记住对于act函数，我们想要从动作的概率分布中采样一个动作。
- en: (Real) Solution
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （真正的）解决方案
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By using CartPole, it was easier to debug since **we know that the bug comes
    from our integration and not from our simple environment**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用CartPole，更容易调试，因为我们知道错误来自于我们的集成，而不是我们简单的环境。
- en: Since **we want to sample an action from the probability distribution over actions**,
    we can’t use `action = np.argmax(m)` since it will always output the action that
    has the highest probability.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为我们想要从动作概率分布中采样一个动作，所以不能使用`action = np.argmax(m)`，因为它总是输出具有最高概率的动作。
- en: We need to replace this with `action = m.sample()` which will sample an action
    from the probability distribution P(.|s)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要用`action = m.sample()`替换这个，它将从概率分布P(.|s)中采样一个动作
- en: Let’s build the Reinforce Training Algorithm
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们构建Reinforce训练算法
- en: 'This is the Reinforce algorithm pseudocode:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Reinforce算法的伪代码：
- en: '![Policy gradient pseudocode](../Images/98fc23971db3e1a9e35baeee827641dc.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度伪代码](../Images/98fc23971db3e1a9e35baeee827641dc.png)'
- en: When we calculate the return Gt (line 6), we see that we calculate the sum of
    discounted rewards **starting at timestep t**.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们计算回报Gt（第6行）时，我们看到我们计算从时间步t开始的折扣奖励的总和。
- en: 'Why? Because our policy should only **reinforce actions on the basis of the
    consequences**: so rewards obtained before taking an action are useless (since
    they were not because of the action), **only the ones that come after the action
    matters**.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么？因为我们的策略应该只根据后果来强化动作：因此在采取行动之前获得的奖励是无用的（因为它们不是因为行动），只有之后的奖励才重要。
- en: Before coding this you should read this section [don’t let the past distract
    you](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)
    that explains why we use reward-to-go policy gradient.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编写这之前，你应该阅读这一部分[不要让过去分散你的注意力](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)，解释了为什么我们使用奖励逐步策略梯度。
- en: We use an interesting technique coded by [Chris1nexus](https://github.com/Chris1nexus)
    to **compute the return at each timestep efficiently**. The comments explained
    the procedure. Don’t hesitate also [to check the PR explanation](https://github.com/huggingface/deep-rl-class/pull/95)
    But overall the idea is to **compute the return at each timestep efficiently**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个由[Chris1nexus](https://github.com/Chris1nexus)编写的有趣的技术来高效地计算每个时间步的回报。评论中解释了该过程。也不要犹豫[查看PR解释](https://github.com/huggingface/deep-rl-class/pull/95)，但总体思路是高效地计算每个时间步的回报。
- en: The second question you may ask is **why do we minimize the loss**? Didn’t we
    talk about Gradient Ascent, not Gradient Descent earlier?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题你可能会问的是为什么我们要最小化损失？之前不是讨论过梯度上升，而不是梯度下降吗？
- en: We want to maximize our utility function $J(\theta)$, but in PyTorch and TensorFlow,
    it’s better to **minimize an objective function.**
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要最大化我们的效用函数$J(\theta)$，但在PyTorch和TensorFlow中，更好的做法是最小化一个目标函数。
- en: So let’s say we want to reinforce action 3 at a certain timestep. Before training
    this action P is 0.25.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们想要在某个时间步强化动作3。在训练这个动作之前，P为0.25。
- en: So we want to modify<math><semantics><mrow><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi></mrow><annotation
    encoding="application/x-tex">theta</annotation></semantics></math> theta such
    that<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi
    mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>></mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s;
    \theta) > 0.25</annotation></semantics></math> πθ​(a3​∣s;θ)>0.25
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以我们想要修改θ theta，使得πθ​(a3​∣s;θ)>0.25
- en: Because all P must sum to 1, max<math><semantics><mrow><mi>p</mi><msub><mi>i</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">pi_\theta(a_3|s; \theta)</annotation></semantics></math>piθ​(a3​∣s;θ)
    will **minimize other action probability.**
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为所有P必须总和为1，max<math><semantics><mrow><mi>p</mi><msub><mi>i</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">pi_\theta(a_3|s; \theta)</annotation></semantics></math>piθ​(a3​∣s;θ)将**最小化其他动作的概率。**
- en: So we should tell PyTorch **to min<math><semantics><mrow><mn>1</mn><mo>−</mo><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">1 - \pi_\theta(a_3|s; \theta)</annotation></semantics></math>1−πθ​(a3​∣s;θ).**
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以我们应该告诉PyTorch**最小化<math><semantics><mrow><mn>1</mn><mo>−</mo><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">1 - \pi_\theta(a_3|s; \theta)</annotation></semantics></math>1−πθ​(a3​∣s;θ)。**
- en: This loss function approaches 0 as<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a_3|s; \theta)</annotation></semantics></math>πθ​(a3​∣s;θ)
    nears 1.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个损失函数在<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi
    mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s;
    \theta)</annotation></semantics></math>πθ​(a3​∣s;θ)接近1时趋近于0。
- en: So we are encouraging the gradient to max<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a_3|s; \theta)</annotation></semantics></math>πθ​(a3​∣s;θ)
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们鼓励梯度最大化<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mn>3</mn></msub><mi
    mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_3|s;
    \theta)</annotation></semantics></math>πθ​(a3​∣s;θ)
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Solution
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Train it
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练它
- en: We’re now ready to train our agent.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在准备训练我们的代理。
- en: But first, we define a variable containing all the training hyperparameters.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但首先，我们定义一个包含所有训练超参数的变量。
- en: You can change the training parameters (and should 😉)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以更改训练参数（并应该😉）
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Define evaluation method 📝
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义评估方法 📝
- en: Here we define the evaluation method that we’re going to use to test our Reinforce
    agent.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们定义了要使用的评估方法来测试我们的Reinforce代理。
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Evaluate our agent 📈
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估我们的代理 📈
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Publish our trained model on the Hub 🔥
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Hub上发布我们训练的模型🔥
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub 🤗 with one line of code.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到训练后取得了良好的结果，我们可以用一行代码将我们训练的模型发布到hub 🤗。
- en: 'Here’s an example of a Model Card:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个模型卡的示例：
- en: '![](../Images/afdb3be9b09aa528c9ee40968ca15774.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afdb3be9b09aa528c9ee40968ca15774.png)'
- en: Push to the Hub
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推送到Hub
- en: Do not modify this code
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不要修改这段代码
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: By using `push_to_hub`, **you evaluate, record a replay, generate a model card
    of your agent, and push it to the Hub**.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`push_to_hub`，**您可以评估、记录重播、生成代理的模型卡，并将其推送到Hub**。
- en: 'This way:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这样：
- en: You can **showcase our work** 🔥
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**展示我们的工作**🔥
- en: You can **visualize your agent playing** 👀
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**可视化您的代理进行游戏**👀
- en: You can **share an agent with the community that others can use** 💾
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**与社区分享一个其他人可以使用的代理**💾
- en: You can **access a leaderboard 🏆 to see how well your agent is performing compared
    to your classmates** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**访问排行榜🏆，查看您的代理与同学相比表现如何**👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要与社区分享您的模型，需要遵循三个步骤：
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣（如果尚未完成）创建一个HF帐户➡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2️⃣ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 登录，然后，您需要存储来自Hugging Face网站的身份验证令牌。
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的令牌（[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)）**具有写入角色**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![创建HF令牌](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you don’t want to use Google Colab or a Jupyter Notebook, you need to use
    this command instead: `huggingface-cli login` (or `login`)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用Google Colab或Jupyter Notebook，您需要使用这个命令：`huggingface-cli login`（或`login`）
- en: 3️⃣ We’re now ready to push our trained agent to the 🤗 Hub 🔥 using `package_to_hub()`
    function
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 现在我们准备将我们训练的代理推送到🤗 Hub 🔥，使用`package_to_hub()`函数
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now that we tested the robustness of our implementation, let’s try a more complex
    environment: PixelCopter 🚁'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们测试了我们实现的鲁棒性，让我们尝试一个更复杂的环境：PixelCopter 🚁
- en: 'Second agent: PixelCopter 🚁'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二个代理：PixelCopter 🚁
- en: Study the PixelCopter environment 👀
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 研究PixelCopter环境👀
- en: '[The Environment documentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[环境文档](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)'
- en: '[PRE25]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The observation space (7) 👀:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 观察空间（7）👀：
- en: player y position
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家y位置
- en: player velocity
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家速度
- en: player distance to floor
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家到地板的距离
- en: player distance to ceiling
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家到天花板的距离
- en: next block x distance to player
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个方块x距离玩家
- en: next blocks top y location
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个方块的顶部y位置
- en: next blocks bottom y location
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个方块的底部y位置
- en: 'The action space(2) 🎮:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 动作空间（2）：
- en: Up (press accelerator)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速（按下加速器）
- en: Do nothing (don’t press accelerator)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么都不做（不按下加速器）
- en: 'The reward function 💰:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数：
- en: For each vertical block it passes, it gains a positive reward of +1\. Each time
    a terminal state is reached it receives a negative reward of -1.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每通过一个垂直块，它获得+1的正奖励。每次达到终止状态时，它会获得-1的负奖励。
- en: Define the new Policy 🧠
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义新策略
- en: We need to have a deeper neural network since the environment is more complex
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于环境更加复杂，我们需要一个更深层的神经网络
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Solution
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE29]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Define the hyperparameters ⚙️
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义超参数
- en: Because this environment is more complex.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为这个环境更加复杂。
- en: Especially for the hidden size, we need more neurons.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特别是对于隐藏大小，我们需要更多的神经元。
- en: '[PRE30]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Train it
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练它
- en: We’re now ready to train our agent 🔥.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在准备训练我们的代理。
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Publish our trained model on the Hub 🔥
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Hub上发布我们训练的模型
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Some additional challenges 🏆
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些额外的挑战
- en: The best way to learn **is to try things on your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    But also try to find better parameters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的最佳方式是自己尝试！正如您所看到的，当前代理表现不佳。作为第一个建议，您可以训练更多步骤。但也尝试找到更好的参数。
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在排行榜中，您将找到您的代理。您能够达到榜首吗？
- en: 'Here are some ideas to climb up the leaderboard:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些提升排行榜的想法：
- en: Train more steps
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练更多步骤
- en: Try different hyperparameters by looking at what your classmates have done 👉
    [https://huggingface.co/models?other=reinforce](https://huggingface.co/models?other=reinforce)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的超参数，查看同学们的做法
- en: '**Push your new trained model** on the Hub 🔥'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的新训练模型发布到Hub
- en: '**Improving the implementation for more complex environments** (for instance,
    what about changing the network to a Convolutional Neural Network to handle frames
    as observation)?'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进实现以适应更复杂的环境（例如，将网络更改为卷积神经网络以处理帧作为观察？）
- en: '* * *'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Congrats on finishing this unit**! There was a lot of information. And congrats
    on finishing the tutorial. You’ve just coded your first Deep Reinforcement Learning
    agent from scratch using PyTorch and shared it on the Hub 🥳.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 祝贺您完成这个单元！这里有很多信息。祝贺您完成教程。您刚刚使用PyTorch从头开始编写了您的第一个深度强化学习代理，并在Hub上分享了它。
- en: Don’t hesitate to iterate on this unit **by improving the implementation for
    more complex environments** (for instance, what about changing the network to
    a Convolutional Neural Network to handle frames as observation)?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 不要犹豫，通过改进实现以适应更复杂的环境（例如，将网络更改为卷积神经网络以处理帧作为观察）来迭代这个单元。
- en: In the next unit, **we’re going to learn more about Unity MLAgents**, by training
    agents in Unity environments. This way, you will be ready to participate in the
    **AI vs AI challenges where you’ll train your agents to compete against other
    agents in a snowball fight and a soccer game.**
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个单元中，我们将学习更多关于Unity MLAgents，通过在Unity环境中训练代理。这样，您将准备好参加AI对AI挑战，您将训练您的代理来与其他代理在雪仗和足球比赛中竞争。
- en: Sound fun? See you next time!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很有趣吗？下次见！
- en: Finally, we would love **to hear what you think of the course and how we can
    improve it**. If you have some feedback then please 👉 [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们很想知道您对课程的看法以及我们如何改进它。如果您有反馈意见，请填写此表格。
- en: See you in Unit 5! 🔥
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 下次见在第5单元！
- en: Keep Learning, stay awesome 🤗
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 继续学习，保持精彩
