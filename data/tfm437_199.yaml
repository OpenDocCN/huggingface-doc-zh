- en: MegatronGPT2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MegatronGPT2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The MegatronGPT2 model was proposed in [Megatron-LM: Training Multi-Billion
    Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
    by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper
    and Bryan Catanzaro.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: MegatronGPT2模型是由Mohammad Shoeybi、Mostofa Patwary、Raul Puri、Patrick LeGresley、Jared
    Casper和Bryan Catanzaro在[使用模型并行训练多十亿参数语言模型的Megatron-LM](https://arxiv.org/abs/1909.08053)中提出的。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Recent work in language modeling demonstrates that training large transformer
    models advances the state of the art in Natural Language Processing applications.
    However, very large models can be quite difficult to train due to memory constraints.
    In this work, we present our techniques for training very large transformer models
    and implement a simple, efficient intra-layer model parallel approach that enables
    training transformer models with billions of parameters. Our approach does not
    require a new compiler or library changes, is orthogonal and complimentary to
    pipeline model parallelism, and can be fully implemented with the insertion of
    a few communication operations in native PyTorch. We illustrate this approach
    by converging transformer based models up to 8.3 billion parameters using 512
    GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling
    efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs,
    which is 30% of peak FLOPs. To demonstrate that large language models can further
    advance the state of the art (SOTA), we train an 8.3 billion parameter transformer
    language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT.
    We show that careful attention to the placement of layer normalization in BERT-like
    models is critical to achieving increased performance as the model size grows.
    Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared
    to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)
    datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared
    to SOTA accuracy of 89.4%).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近在语言建模方面的工作表明，训练大型Transformer模型可以推动自然语言处理应用的最新技术。然而，非常大的模型可能由于内存限制而难以训练。在这项工作中，我们提出了训练非常大的Transformer模型的技术，并实现了一种简单高效的层内模型并行方法，使得可以训练具有数十亿参数的Transformer模型。我们的方法不需要新的编译器或库更改，与管道模型并行性正交且互补，并且可以通过在原生PyTorch中插入几个通信操作来完全实现。我们通过使用512个GPU收敛基于Transformer的模型，达到了83亿参数。与维持39
    TeraFLOPs的强单GPU基线相比，我们在整个应用程序中保持了15.1 PetaFLOPs，其扩展效率为76%，这是峰值FLOPs的30%。为了证明大型语言模型可以进一步推动最新技术（SOTA），我们训练了一个83亿参数的Transformer语言模型，类似于GPT-2，以及一个39亿参数的类似BERT的模型。我们展示了在BERT类似模型中对层归一化的放置要特别注意，这对于随着模型规模增长而实现性能提升至关重要。使用GPT-2模型，我们在WikiText103（10.8，与15.8的SOTA困惑度相比）和LAMBADA（66.5%，与63.2%的SOTA准确率相比）数据集上取得了SOTA结果。我们的BERT模型在RACE数据集上取得了SOTA结果（90.9%，与89.4%的SOTA准确率相比）。*'
- en: This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The
    original code can be found [here](https://github.com/NVIDIA/Megatron-LM). That
    repository contains a multi-GPU and multi-node implementation of the Megatron
    Language models. In particular, it contains a hybrid model parallel approach using
    “tensor parallel” and “pipeline parallel” techniques.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[jdemouth](https://huggingface.co/jdemouth)贡献的。原始代码可以在[这里](https://github.com/NVIDIA/Megatron-LM)找到。该存储库包含了Megatron语言模型的多GPU和多节点实现。特别是，它包含了使用“张量并行”和“管道并行”技术的混合模型并行方法。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: We have provided pretrained [GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m)
    checkpoints for use to evaluate or finetuning downstream tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了预训练的[GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m)检查点，用于评估或微调下游任务。
- en: To access these checkpoints, first [sign up](https://ngc.nvidia.com/signup)
    for and setup the NVIDIA GPU Cloud (NGC) Registry CLI. Further documentation for
    downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这些检查点，首先[注册](https://ngc.nvidia.com/signup)并设置NVIDIA GPU云（NGC）注册表CLI。有关下载模型的更多文档，请参阅[NGC文档](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1)。
- en: 'Alternatively, you can directly download the checkpoints using:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以直接下载检查点：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have
    to convert it to a format that will easily be loaded by Hugging Face Transformers
    GPT2 implementation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您从NVIDIA GPU云（NGC）获得了检查点，您需要将其转换为Hugging Face Transformers GPT2实现可以轻松加载的格式。
- en: 'The following command allows you to do the conversion. We assume that the folder
    `models/megatron_gpt2` contains `megatron_gpt2_345m_v0_0.zip` and that the command
    is run from that folder:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令允许您进行转换。我们假设文件夹`models/megatron_gpt2`包含`megatron_gpt2_345m_v0_0.zip`，并且该命令是从该文件夹运行的：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: MegatronGPT2 architecture is the same as OpenAI GPT-2 . Refer to [GPT-2 documentation](gpt2)
    for information on configuration classes and their parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: MegatronGPT2架构与OpenAI GPT-2相同。有关配置类和其参数的信息，请参考[GPT-2文档](gpt2)。
