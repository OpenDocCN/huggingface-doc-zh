- en: Kwargs Handlers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键字参数处理程序
- en: 'Original text: [https://huggingface.co/docs/accelerate/package_reference/kwargs](https://huggingface.co/docs/accelerate/package_reference/kwargs)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/accelerate/package_reference/kwargs](https://huggingface.co/docs/accelerate/package_reference/kwargs)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The following objects can be passed to the main [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how some PyTorch objects related to distributed training or mixed
    precision are created.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下对象可以传递给主要的 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    来自定义与分布式训练或混合精度相关的一些 PyTorch 对象的创建方式。
- en: AutocastKwargs
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动转换关键字参数
- en: '### `class accelerate.AutocastKwargs`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.AutocastKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L60)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L60)'
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how `torch.autocast` behaves. Please refer to the documentation of
    this [context manager](https://pytorch.org/docs/stable/amp.html#torch.autocast)
    for more information on each argument.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    中使用此对象，以自定义 `torch.autocast` 的行为。请参阅此 [上下文管理器](https://pytorch.org/docs/stable/amp.html#torch.autocast)
    的文档，以获取有关每个参数的更多信息。
- en: 'Example:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DistributedDataParallelKwargs
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DistributedDataParallelKwargs
- en: '### `class accelerate.DistributedDataParallelKwargs`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.DistributedDataParallelKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L82)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L82)'
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how your model is wrapped in a `torch.nn.parallel.DistributedDataParallel`.
    Please refer to the documentation of this [wrapper](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    for more information on each argument.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    中使用此对象，以自定义如何将您的模型包装在 `torch.nn.parallel.DistributedDataParallel` 中。请参阅此 [包装器](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    的文档，以获取有关每个参数的更多信息。
- en: '`gradient_as_bucket_view` is only available in PyTorch 1.7.0 and later versions.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`gradient_as_bucket_view` 仅适用于 PyTorch 1.7.0 及更高版本。'
- en: '`static_graph` is only available in PyTorch 1.11.0 and later versions.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`static_graph` 仅适用于 PyTorch 1.11.0 及更高版本。'
- en: 'Example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: FP8RecipeKwargs
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FP8RecipeKwargs
- en: '### `class accelerate.utils.FP8RecipeKwargs`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.FP8RecipeKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L179)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L179)'
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`backend` (`str`, *optional*, defaults to “msamp”) — Which FP8 engine to use.
    Must be one of `"msamp"` (MS-AMP) or `"te"` (TransformerEngine).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend`（`str`，*可选*，默认为 “msamp”）— 要使用的 FP8 引擎。必须是 `"msamp"`（MS-AMP）或 `"te"`（TransformerEngine）之一。'
- en: '`margin` (`int`, *optional*, default to 0) — The margin to use for the gradient
    scaling.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`margin`（`int`，*可选*，默认为 0）— 用于梯度缩放的边距。'
- en: '`interval` (`int`, *optional*, default to 1) — The interval to use for how
    often the scaling factor is recomputed.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interval`（`int`，*可选*，默认为 1）— 用于多久重新计算一次缩放因子的间隔。'
- en: '`fp8_format` (`str`, *optional*, default to “E4M3”) — The format to use for
    the FP8 recipe. Must be one of `E4M3` or `HYBRID`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp8_format`（`str`，*可选*，默认为 “E4M3”）— 用于 FP8 配方的格式。必须是 `E4M3` 或 `HYBRID` 之一。'
- en: '`amax_history_len` (`int`, *optional*, default to 1024) — The length of the
    history to use for the scaling factor computation'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amax_history_len`（`int`，*可选*，默认为 1024）— 用于缩放因子计算的历史长度'
- en: '`amax_compute_algo` (`str`, *optional*, default to “most_recent”) — The algorithm
    to use for the scaling factor computation. Must be one of `max` or `most_recent`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amax_compute_algo`（`str`，*可选*，默认为 “most_recent”）— 用于缩放因子计算的算法。必须是 `max` 或
    `most_recent` 之一。'
- en: '`override_linear_precision` (`tuple` of three `bool`, *optional*, default to
    `(False, False, False)`) — Whether or not to execute `fprop`, `dgrad`, and `wgrad`
    GEMMS in higher precision.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`override_linear_precision`（三个 `bool` 的 `tuple`，*可选*，默认为 `(False, False, False)`）—
    是否在更高精度中执行 `fprop`、`dgrad` 和 `wgrad` GEMMS。'
- en: '`optimization_level` (`str`), one of `O1`, `O2`. (default is `O2`) — What level
    of 8-bit collective communication should be used with MS-AMP. In general:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimization_level`（`str`），其中之一为 `O1`、`O2`。 （默认为 `O2`）— 应使用哪个级别的 8 位集体通信与
    MS-AMP。一般来说：'
- en: 'O1: Weight gradients and `all_reduce` communications are done in fp8, reducing
    GPU memory usage and communication bandwidth'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: O1：权重梯度和 `all_reduce` 通信以 fp8 完成，减少 GPU 内存使用和通信带宽
- en: 'O2: First-order optimizer states are in 8-bit, and second order states are
    in FP16. Only available when using Adam or AdamW. This maintains accuracy and
    can potentially save the highest memory.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: O2：一阶优化器状态为 8 位，二阶状态为 FP16。仅在使用 Adam 或 AdamW 时可用。这可以保持准确性并潜在地节省最高的内存。
- en: '03: Specifically for DeepSpeed, implements capabilities so weights and master
    weights of models are stored in FP8\. If `fp8` is selected and deepspeed is enabled,
    will be used by default. (Not available currently).'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 03：专门为 DeepSpeed 实现功能，使模型的权重和主权重存储在 FP8 中。如果选择了 `fp8` 并启用了 deepspeed，则将默认使用。
    （目前不可用）。
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the recipe for FP8 mixed precision training
    with `transformer-engine` or `ms-amp`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    中使用此对象，以自定义使用 `transformer-engine` 或 `ms-amp` 进行 FP8 混合精度训练的配方的初始化。
- en: For more information on `transformer-engine` args, please refer to the API [documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 `transformer-engine` 参数的更多信息，请参阅 API [文档](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)。
- en: For more information on the `ms-amp` args, please refer to the Optimization
    Level [documentation](https://azure.github.io/MS-AMP/docs/user-tutorial/optimization-level).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 `ms-amp` 参数的更多信息，请参考优化级别 [文档](https://azure.github.io/MS-AMP/docs/user-tutorial/optimization-level)。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To use MS-AMP as an engine, pass `backend="msamp"` and the `optimization_level`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 MS-AMP 用作引擎，请传递 `backend="msamp"` 和 `optimization_level`：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: GradScalerKwargs
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GradScalerKwargs
- en: '### `class accelerate.GradScalerKwargs`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.GradScalerKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L118)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L118)'
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the behavior of mixed precision, specifically how the `torch.cuda.amp.GradScaler`
    used is created. Please refer to the documentation of this [scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler)
    for more information on each argument.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    中使用此对象来自定义混合精度的行为，特别是如何创建 `torch.cuda.amp.GradScaler` 的使用。有关每个参数的更多信息，请参考此 [scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler)
    的文档。
- en: '`GradScaler` is only available in PyTorch 1.5.0 and later versions.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradScaler` 仅在 PyTorch 1.5.0 及更高版本中可用。'
- en: 'Example:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: InitProcessGroupKwargs
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InitProcessGroupKwargs
- en: '### `class accelerate.InitProcessGroupKwargs`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.InitProcessGroupKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L149)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L149)'
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the distributed processes. Please refer to
    the documentation of this [method](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)
    for more information on each argument.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    中使用此对象来自定义分布式进程的初始化。有关每个参数的更多信息，请参考此 [方法](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)
    的文档。
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
