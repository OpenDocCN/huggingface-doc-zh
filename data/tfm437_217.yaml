- en: Persimmon
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Persimmon
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/persimmon](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/persimmon)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/persimmon](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/persimmon)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The Persimmon model was created by [ADEPT](https://www.adept.ai/blog/persimmon-8b),
    and authored by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri
    Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Persimmon模型是由[ADEPT](https://www.adept.ai/blog/persimmon-8b)创建的，作者是Erich Elsen，Augustus
    Odena，Maxwell Nye，Sağnak Taşırlar，Tri Dao，Curtis Hawthorne，Deepak Moparthi，Arushi
    Somani。
- en: The authors introduced Persimmon-8B, a decoder model based on the classic transformers
    architecture, with query and key normalization. Persimmon-8B is a fully permissively-licensed
    model with approximately 8 billion parameters, released under the Apache license.
    Some of the key attributes of Persimmon-8B are long context size (16K), performance,
    and capabilities for multimodal extensions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作者介绍了Persimmon-8B，这是一个基于经典transformers架构的解码器模型，具有查询和键规范化。Persimmon-8B是一个完全允许许可的模型，拥有约80亿个参数，发布在Apache许可下。Persimmon-8B的一些关键属性是长上下文大小（16K）、性能和多模态扩展的能力。
- en: The authors showcase their approach to model evaluation, focusing on practical
    text generation, mirroring how users interact with language models. The work also
    includes a comparative analysis, pitting Persimmon-8B against other prominent
    models (MPT 7B Instruct and Llama 2 Base 7B 1-Shot), across various evaluation
    tasks. The results demonstrate Persimmon-8B’s competitive performance, even with
    limited training data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了他们对模型评估的方法，重点放在实际文本生成上，反映了用户与语言模型的交互方式。该工作还包括了一项比较分析，将Persimmon-8B与其他知名模型（MPT
    7B Instruct和Llama 2 Base 7B 1-Shot）在各种评估任务中进行对比。结果表明，即使训练数据有限，Persimmon-8B的性能也很有竞争力。
- en: In terms of model details, the work outlines the architecture and training methodology
    of Persimmon-8B, providing insights into its design choices, sequence length,
    and dataset composition. The authors present a fast inference code that outperforms
    traditional implementations through operator fusion and CUDA graph utilization
    while maintaining code coherence. They express their anticipation of how the community
    will leverage this contribution to drive innovation, hinting at further upcoming
    releases as part of an ongoing series of developments.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型细节方面，该工作概述了Persimmon-8B的架构和训练方法，提供了关于其设计选择、序列长度和数据集组成的见解。作者提供了一个快速推理代码，通过操作融合和CUDA图利用来优于传统实现，同时保持代码的一致性。他们表达了他们对社区如何利用这一贡献推动创新的期待，并暗示将作为一系列持续发展的一部分发布更多即将推出的版本。
- en: This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ). The
    original code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[ArthurZ](https://huggingface.co/ArthurZ)贡献的。原始代码可以在[这里](https://github.com/persimmon-ai-labs/adept-inference)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: The `Persimmon` models were trained using `bfloat16`, but the original inference
    uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'`
    which will be used by the `AutoModel` API to cast the checkpoints from `torch.float32`
    to `torch.float16`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`Persimmon`模型使用`bfloat16`进行训练，但原始推理使用`float16`。上传到hub的检查点使用`torch_dtype = ''float16''`，`AutoModel`
    API将使用它将检查点从`torch.float32`转换为`torch.float16`。'
- en: The `dtype` of the online weights is mostly irrelevant, unless you are using
    `torch_dtype="auto"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`. The reason is that the model will first be downloaded
    ( using the `dtype` of the checkpoints online) then it will be cast to the default
    `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype`
    they want, and if they don’t it will be `torch.float32`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在线权重的`dtype`大多数情况下并不重要，除非您在使用`torch_dtype="auto"`初始化模型时使用`model = AutoModelForCausalLM.from_pretrained("path",
    torch_dtype = "auto")`。原因是模型将首先被下载（使用在线检查点的`dtype`），然后将被转换为`torch`的默认`dtype`（变为`torch.float32`）。用户应该指定他们想要的`torch_dtype`，如果他们不这样做，它将是`torch.float32`。
- en: Finetuning the model in `float16` is not recommended and known to produce `nan`,
    as such the model should be fine-tuned in `bfloat16`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议在`float16`中微调模型，已知会产生`nan`，因此模型应该在`bfloat16`中进行微调。
- en: 'Tips:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'To convert the model, you need to clone the original repository using `git
    clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要转换模型，您需要克隆原始存储库，使用`git clone https://github.com/persimmon-ai-labs/adept-inference`，然后获取检查点：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the chat model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天模型：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Thereafter, models can be loaded via:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，模型可以通过以下方式加载：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Perismmon uses a `sentencepiece` based tokenizer, with a `Unigram` model. It
    supports bytefallback, which is only available in `tokenizers==0.14.0` for the
    fast tokenizer. The `LlamaTokenizer` is used as it is a standard wrapper around
    sentencepiece. The `chat` template will be updated with the templating functions
    in a follow up PR!
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perismmon使用基于`sentencepiece`的分词器，具有一个`Unigram`模型。它支持bytefallback，仅在快速分词器的`tokenizers==0.14.0`中可用。`LlamaTokenizer`被用作它是一个围绕sentencepiece的标准包装器。`chat`模板将在后续PR中使用模板函数进行更新！
- en: 'The authors suggest to use the following prompt format for the chat mode: `f"human:
    {prompt}\n\nadept:"`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '作者建议为聊天模式使用以下提示格式：`f"human: {prompt}\n\nadept:"`'
- en: PersimmonConfig
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PersimmonConfig
- en: '### `class transformers.PersimmonConfig`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PersimmonConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/configuration_persimmon.py#L28)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/configuration_persimmon.py#L28)'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 262144) — Vocabulary size of the
    Persimmon model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [PersimmonModel](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonModel)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为262144）—Persimmon模型的词汇量。定义了在调用[PersimmonModel](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonModel)时可以表示的不同标记的数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) — Dimension of the hidden
    representations.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 4096) — 隐藏表示的维度。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 16384) — Dimension of the
    MLP representations.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 16384) — MLP表示的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 36) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 36) — Transformer编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 64) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 64) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"relu2"`) — The
    non-linear activation function (function or string) in the decoder.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"relu2"`) — 解码器中的非线性激活函数（函数或字符串）。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 16384) — The maximum
    sequence length that this model might ever be used with.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 16384) — 该模型可能使用的最大序列长度。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — The epsilon used
    by the rms normalization layers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — rms归一化层使用的epsilon。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（不是所有模型都使用）。仅在`config.is_decoder=True`时相关。'
- en: '`tie_word_embeddings(bool,` *optional*, defaults to `False`) — Whether to tie
    weight embeddings'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tie_word_embeddings(bool,` *optional*, defaults to `False`) — 是否绑定权重嵌入'
- en: '`rope_theta` (`float`, *optional*, defaults to 25000.0) — The base period of
    the RoPE embeddings.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rope_theta` (`float`, *optional*, defaults to 25000.0) — RoPE嵌入的基本周期。'
- en: '`rope_scaling` (`Dict`, *optional*) — Dictionary containing the scaling configuration
    for the RoPE embeddings. Currently supports two scaling strategies: linear and
    dynamic. Their scaling factor must be a float greater than 1\. The expected format
    is `{"type": strategy name, "factor": scaling factor}`. When using this flag,
    don’t update `max_position_embeddings` to the expected new maximum. See the following
    thread for more information on how these scaling strategies behave: [https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/).
    This is an experimental feature, subject to breaking API changes in future versions.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rope_scaling` (`Dict`, *optional*) — 包含RoPE嵌入缩放配置的字典。当前支持两种缩放策略：线性和动态。它们的缩放因子必须是大于1的浮点数。期望的格式是`{"type":
    策略名称, "factor": 缩放因子}`。使用此标志时，不要将`max_position_embeddings`更新为预期的新最大值。有关这些缩放策略行为的更多信息，请参阅以下主题：[https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/)。这是一个实验性功能，可能在未来版本中发生破坏性API更改。'
- en: '`qk_layernorm` (`bool`, *optional*, default to `True`) — Whether or not to
    normalize the Queries and Keys after projecting the hidden states'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qk_layernorm` (`bool`, *optional*, default to `True`) — 在投影隐藏状态后是否对查询和键进行归一化'
- en: '`hidden_dropout` (`float`, *optional*, default to 0.0) — The dropout ratio
    after applying the MLP to the hidden states.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, default to 0.0) — 在将MLP应用于隐藏状态后的dropout比率。'
- en: '`attention_dropout` (`float`, *optional*, default to 0.0) — The dropout ratio
    after computing the attention scores.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, default to 0.0) — 计算注意力分数后的dropout比率。'
- en: '`partial_rotary_factor` (`float`, *optional*, default to 0.5) — Percentage
    of the query and keys which will have rotary embedding.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`partial_rotary_factor` (`float`, *optional*, default to 0.5) — 查询和键中将具有旋转嵌入的百分比。'
- en: Example —
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例 —
- en: This is the configuration class to store the configuration of a [PersimmonModel](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonModel).
    It is used to instantiate an Persimmon model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the [adept/persimmon-8b-base](https://huggingface.co/adept/persimmon-8b-base).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[PersimmonModel](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonModel)配置的配置类。根据指定的参数实例化Persimmon模型，定义模型架构。使用默认值实例化配置将产生类似于[adept/persimmon-8b-base](https://huggingface.co/adept/persimmon-8b-base)的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PersimmonModel
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PersimmonModel
- en: '### `class transformers.PersimmonModel`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PersimmonModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L544)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L544)'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PersimmonConfig](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights. config — PersimmonConfig'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PersimmonConfig](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。config
    — PersimmonConfig'
- en: The bare Persimmon Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的Persimmon模型输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头部等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer
    is a `PersimmonDecoderLayer`
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由*config.num_hidden_layers*层组成的Transformer解码器。每一层都是一个`PersimmonDecoderLayer`
- en: '#### `forward`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L577)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L577)'
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选定为`[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被“掩盖”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被“掩盖”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，可选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改填充行为，应阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: 1 indicates the head is `not masked`,
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被“掩盖”。
- en: 0 indicates the head is `masked`.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被“掩盖”。
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`Cache` 或 `tuple(tuple(torch.FloatTensor))`, *可选*) — 预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。通常包括模型在先前解码阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。'
- en: 'Two formats are allowed:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 允许两种格式：
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量。这也被称为传统缓存格式。
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统的缓存格式。
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`input_ids`（即未将其过去的键值状态提供给此模型的那些）的形状为`(batch_size,
    1)`，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这很有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: The [PersimmonModel](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonModel)
    forward method, overrides the `__call__` special method.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[PersimmonModel](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonModel)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: PersimmonForCausalLM
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PersimmonForCausalLM
- en: '### `class transformers.PersimmonForCausalLM`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PersimmonForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L701)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L701)'
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `forward`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L738)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L738)'
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。默认情况下，如果提供填充，则将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0,
    1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“未掩码”的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“掩码”的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则可选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您想要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: 1 indicates the head is `not masked`,
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`Cache`或`tuple(tuple(torch.FloatTensor))`，*可选*）— 预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在先前解码阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。'
- en: 'Two formats are allowed:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 允许两种格式：
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度为`config.n_layers`的元组`tuple(torch.FloatTensor)`，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量。这也被称为传统缓存格式。
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统缓存格式。
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`input_ids`（这些`input_ids`没有将它们的过去键值状态提供给此模型）的形状为`(batch_size,
    1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）—
    可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: 'Args — labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数 — 标签（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）：用于计算掩盖语言建模损失的标签。索引应该在`[0,
    ..., config.vocab_size]`或-100（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩盖），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`中的标记。
- en: Returns
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PersimmonConfig](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonConfig))
    and inputs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括根据配置（[PersimmonConfig](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回）— 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（用于嵌入的输出，如果模型有嵌入层，则为一个
    + 每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [PersimmonForCausalLM](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[PersimmonForCausalLM](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonForCausalLM)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: PersimmonForSequenceClassification
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PersimmonForSequenceClassification
- en: '### `class transformers.PersimmonForSequenceClassification`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PersimmonForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L893)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L893)'
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PersimmonConfig](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PersimmonConfig](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Persimmon transformer with a sequence classification head on top (linear
    layer).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 带有顶部序列分类头（线性层）的Persimmon变换器。
- en: '[PersimmonForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[PersimmonForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification)使用最后一个标记来进行分类，就像其他因果模型（例如GPT-2）一样。'
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它对最后一个标记进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，则在每一行中找到不是填充标记的最后一个标记。如果未定义`pad_token_id`，则简单地取每一行批次中的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，因此它执行相同操作（取每一行批次中的最后一个值）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L925)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L925)'
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被掩码的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被掩码的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，可选地只需输入最后的`decoder_input_ids`（参见`past_key_values`）。
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: 1 indicates the head is `not masked`,
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — 预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在解码的先前阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。'
- en: 'Two formats are allowed:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 允许两种格式：
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量）。这也被称为传统的缓存格式。
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统的缓存格式。
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入最后的`input_ids`（即那些没有将其过去键值状态提供给此模型的输入）的形状为`(batch_size,
    1)`，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: The [PersimmonForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[PersimmonForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
