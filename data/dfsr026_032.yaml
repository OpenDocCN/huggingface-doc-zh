- en: Distributed inference with multiple GPUs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªGPUè¿›è¡Œåˆ†å¸ƒå¼æ¨ç†
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/distributed_inference](https://huggingface.co/docs/diffusers/training/distributed_inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/training/distributed_inference](https://huggingface.co/docs/diffusers/training/distributed_inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: On distributed setups, you can run inference across multiple GPUs with ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index)
    or [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html),
    which is useful for generating with multiple prompts in parallel.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index)æˆ–[PyTorch
    Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)åœ¨å¤šä¸ªGPUä¸Šè¿è¡Œæ¨ç†ï¼Œè¿™å¯¹äºå¹¶è¡Œç”Ÿæˆå¤šä¸ªæç¤ºå¾ˆæœ‰ç”¨ã€‚
- en: This guide will show you how to use ğŸ¤— Accelerate and PyTorch Distributed for
    distributed inference.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ğŸ¤— Accelerateå’ŒPyTorch Distributedè¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ã€‚
- en: ğŸ¤— Accelerate
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerate
- en: ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) is a library designed
    to make it easy to train or run inference across distributed setups. It simplifies
    the process of setting up the distributed environment, allowing you to focus on
    your PyTorch code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index)æ˜¯ä¸€ä¸ªæ—¨åœ¨ç®€åŒ–è·¨åˆ†å¸ƒå¼è®¾ç½®è®­ç»ƒæˆ–è¿è¡Œæ¨ç†çš„åº“ã€‚å®ƒç®€åŒ–äº†è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒçš„è¿‡ç¨‹ï¼Œè®©æ‚¨å¯ä»¥ä¸“æ³¨äºæ‚¨çš„PyTorchä»£ç ã€‚
- en: To begin, create a Python file and initialize an [accelerate.PartialState](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState)
    to create a distributed environment; your setup is automatically detected so you
    donâ€™t need to explicitly define the `rank` or `world_size`. Move the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    to `distributed_state.device` to assign a GPU to each process.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªPythonæ–‡ä»¶å¹¶åˆå§‹åŒ–ä¸€ä¸ª[accelerate.PartialState](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState)æ¥åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç¯å¢ƒï¼›æ‚¨çš„è®¾ç½®ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œå› æ­¤ä¸éœ€è¦æ˜¾å¼å®šä¹‰`rank`æˆ–`world_size`ã€‚å°†[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ç§»åŠ¨åˆ°`distributed_state.device`ä»¥ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ªGPUã€‚
- en: Now use the [split_between_processes](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState.split_between_processes)
    utility as a context manager to automatically distribute the prompts between the
    number of processes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨[split_between_processes](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState.split_between_processes)å®ç”¨ç¨‹åºä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨åœ¨è¿›ç¨‹æ•°é‡ä¹‹é—´åˆ†å‘æç¤ºã€‚
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Use the `--num_processes` argument to specify the number of GPUs to use, and
    call `accelerate launch` to run the script:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`--num_processes`å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„GPUæ•°é‡ï¼Œå¹¶è°ƒç”¨`accelerate launch`æ¥è¿è¡Œè„šæœ¬ï¼š
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To learn more, take a look at the [Distributed Inference with ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate)
    guide.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šï¼Œè¯·æŸ¥çœ‹[ä½¿ç”¨ğŸ¤— Accelerateè¿›è¡Œåˆ†å¸ƒå¼æ¨ç†](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate)æŒ‡å—ã€‚
- en: PyTorch Distributed
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch Distributed
- en: PyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    which enables data parallelism.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchæ”¯æŒ[`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)ï¼Œå®ƒå®ç°äº†æ•°æ®å¹¶è¡Œã€‚
- en: 'To start, create a Python file and import `torch.distributed` and `torch.multiprocessing`
    to set up the distributed process group and to spawn the processes for inference
    on each GPU. You should also initialize a [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªPythonæ–‡ä»¶ï¼Œå¯¼å…¥`torch.distributed`å’Œ`torch.multiprocessing`æ¥è®¾ç½®åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼Œå¹¶åœ¨æ¯ä¸ªGPUä¸Šç”Ÿæˆæ¨ç†è¿›ç¨‹ã€‚æ‚¨è¿˜åº”è¯¥åˆå§‹åŒ–ä¸€ä¸ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ï¼š
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Youâ€™ll want to create a function to run inference; [`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)
    handles creating a distributed environment with the type of backend to use, the
    `rank` of the current process, and the `world_size` or the number of processes
    participating. If youâ€™re running inference in parallel over 2 GPUs, then the `world_size`
    is 2.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªè¿è¡Œæ¨ç†çš„å‡½æ•°ï¼›[`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)ç”¨äºåˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç¯å¢ƒï¼ŒæŒ‡å®šè¦ä½¿ç”¨çš„åç«¯ç±»å‹ã€å½“å‰è¿›ç¨‹çš„`rank`å’Œå‚ä¸çš„è¿›ç¨‹æ•°é‡`world_size`ã€‚å¦‚æœæ‚¨è¦åœ¨2ä¸ªGPUä¸Šå¹¶è¡Œè¿è¡Œæ¨ç†ï¼Œåˆ™`world_size`ä¸º2ã€‚
- en: 'Move the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    to `rank` and use `get_rank` to assign a GPU to each process, where each process
    handles a different prompt:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å°†[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ç§»åŠ¨åˆ°`rank`å¹¶ä½¿ç”¨`get_rank`ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ªGPUï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æç¤ºï¼š
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn)
    to run the `run_inference` function on the number of GPUs defined in `world_size`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œè°ƒç”¨[`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn)åœ¨`world_size`ä¸­å®šä¹‰çš„GPUæ•°é‡ä¸Šè¿è¡Œ`run_inference`å‡½æ•°ï¼š
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once youâ€™ve completed the inference script, use the `--nproc_per_node` argument
    to specify the number of GPUs to use and call `torchrun` to run the script:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆæ¨ç†è„šæœ¬åï¼Œä½¿ç”¨`--nproc_per_node`å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„GPUæ•°é‡ï¼Œå¹¶è°ƒç”¨`torchrun`æ¥è¿è¡Œè„šæœ¬ï¼š
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
