- en: UniDiffuser
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UniDiffuser
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser](https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser](https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The UniDiffuser model was proposed in [One Transformer Fits All Distributions
    in Multi-Modal Diffusion at Scale](https://huggingface.co/papers/2303.06555) by
    Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue
    Cao, Hang Su, Jun Zhu.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: UniDiffuser模型是由Fan Bao、Shen Nie、Kaiwen Xue、Chongxuan Li、Shi Pu、Yaole Wang、Gang
    Yue、Yue Cao、Hang Su、Jun Zhu在[One Transformer Fits All Distributions in Multi-Modal
    Diffusion at Scale](https://huggingface.co/papers/2303.06555)中提出的。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*This paper proposes a unified diffusion framework (dubbed UniDiffuser) to
    fit all distributions relevant to a set of multi-modal data in one model. Our
    key insight is — learning diffusion models for marginal, conditional, and joint
    distributions can be unified as predicting the noise in the perturbed data, where
    the perturbation levels (i.e. timesteps) can be different for different modalities.
    Inspired by the unified view, UniDiffuser learns all distributions simultaneously
    with a minimal modification to the original diffusion model — perturbs data in
    all modalities instead of a single modality, inputs individual timesteps in different
    modalities, and predicts the noise of all modalities instead of a single modality.
    UniDiffuser is parameterized by a transformer for diffusion models to handle input
    types of different modalities. Implemented on large-scale paired image-text data,
    UniDiffuser is able to perform image, text, text-to-image, image-to-text, and
    image-text pair generation by setting proper timesteps without additional overhead.
    In particular, UniDiffuser is able to produce perceptually realistic samples in
    all tasks and its quantitative results (e.g., the FID and CLIP score) are not
    only superior to existing general-purpose models but also comparable to the bespoken
    models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image
    generation).*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文提出了一个统一的扩散框架（称为UniDiffuser），以适应一个模型中与一组多模态数据相关的所有分布。我们的关键见解是——学习边缘、条件和联合分布的扩散模型可以统一为预测扰动数据中的噪声，其中扰动水平（即时间步长）可以针对不同的模态而不同。受到统一视角的启发，UniDiffuser通过最小的修改原始扩散模型——扰动所有模态的数据而不是单个模态，输入不同模态中的单个时间步长，并预测所有模态的噪声而不是单个模态。UniDiffuser由一个用于处理不同模态输入类型的扩散模型的变压器参数化。在大规模配对的图像-文本数据上实现，UniDiffuser能够通过设置适当的时间步长执行图像、文本、文本到图像、图像到文本和图像-文本对生成，而无需额外的开销。特别是，UniDiffuser能够在所有任务中生成感知上逼真的样本，其定量结果（例如FID和CLIP分数）不仅优于现有的通用模型，而且在代表性任务（例如文本到图像生成）中也与定制模型（例如稳定扩散和DALL-E
    2）可比。*'
- en: You can find the original codebase at [thu-ml/unidiffuser](https://github.com/thu-ml/unidiffuser)
    and additional checkpoints at [thu-ml](https://huggingface.co/thu-ml).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[thu-ml/unidiffuser](https://github.com/thu-ml/unidiffuser)找到原始代码库，并在[thu-ml](https://huggingface.co/thu-ml)找到额外的检查点。
- en: There is currently an issue on PyTorch 1.X where the output images are all black
    or the pixel values become `NaNs`. This issue can be mitigated by switching to
    PyTorch 2.X.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 1.X目前存在一个问题，即输出图像全部为黑色或像素值变为`NaNs`。可以通过切换到PyTorch 2.X来缓解这个问题。
- en: This pipeline was contributed by [dg845](https://github.com/dg845). ❤️
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道由[dg845](https://github.com/dg845)贡献。❤️
- en: Usage Examples
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示例
- en: 'Because the UniDiffuser model is trained to model the joint distribution of
    (image, text) pairs, it is capable of performing a diverse range of generation
    tasks:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于UniDiffuser模型经过训练，可以对（图像、文本）对的联合分布进行建模，因此它能够执行各种生成任务：
- en: Unconditional Image and Text Generation
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无条件图像和文本生成
- en: 'Unconditional generation (where we start from only latents sampled from a standard
    Gaussian prior) from a [UniDiffuserPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline)
    will produce a (image, text) pair:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从[UniDiffuserPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline)生成的无条件生成（从标准高斯先验中采样的潜变量开始）将产生一个（图像、文本）对：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is also called “joint” generation in the UniDiffuser paper, since we are
    sampling from the joint image-text distribution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这在UniDiffuser论文中也被称为“联合”生成，因为我们是从联合图像-文本分布中进行采样。
- en: 'Note that the generation task is inferred from the inputs used when calling
    the pipeline. It is also possible to manually specify the unconditional generation
    task (“mode”) manually with [UniDiffuserPipeline.set_joint_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_joint_mode):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成任务是从调用管道时使用的输入中推断出来的。也可以使用[UniDiffuserPipeline.set_joint_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_joint_mode)手动指定无条件生成任务（“模式”）：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When the mode is set manually, subsequent calls to the pipeline will use the
    set mode without attempting to infer the mode. You can reset the mode with [UniDiffuserPipeline.reset_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.reset_mode),
    after which the pipeline will once again infer the mode.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当手动设置模式时，后续对管道的调用将使用设置的模式，而不会尝试推断模式。您可以使用[UniDiffuserPipeline.reset_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.reset_mode)重置模式，之后管道将再次推断模式。
- en: 'You can also generate only an image or only text (which the UniDiffuser paper
    calls “marginal” generation since we sample from the marginal distribution of
    images and text, respectively):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以仅生成图像或仅文本（UniDiffuser论文称为“边缘”生成，因为我们分别从图像和文本的边缘分布中进行采样）：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Text-to-Image Generation
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本到图像生成
- en: 'UniDiffuser is also capable of sampling from conditional distributions; that
    is, the distribution of images conditioned on a text prompt or the distribution
    of texts conditioned on an image. Here is an example of sampling from the conditional
    image distribution (text-to-image generation or text-conditioned image generation):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: UniDiffuser还能够从条件分布中进行采样；即，基于文本提示的图像分布或基于图像的文本分布。以下是从条件图像分布（文本到图像生成或文本条件图像生成）中进行采样的示例：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `text2img` mode requires that either an input `prompt` or `prompt_embeds`
    be supplied. You can set the `text2img` mode manually with [UniDiffuserPipeline.set_text_to_image_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_text_to_image_mode).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`text2img`模式要求提供输入的`prompt`或`prompt_embeds`。您可以使用[UniDiffuserPipeline.set_text_to_image_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_text_to_image_mode)手动设置`text2img`模式。'
- en: Image-to-Text Generation
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像到文本生成
- en: 'Similarly, UniDiffuser can also produce text samples given an image (image-to-text
    or image-conditioned text generation):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，UniDiffuser也可以在给定图像的情况下生成文本样本（图像到文本或图像条件文本生成）：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `img2text` mode requires that an input `image` be supplied. You can set
    the `img2text` mode manually with [UniDiffuserPipeline.set_image_to_text_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_image_to_text_mode).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`img2text`模式要求提供输入的`image`。您可以使用[UniDiffuserPipeline.set_image_to_text_mode()](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.UniDiffuserPipeline.set_image_to_text_mode)手动设置`img2text`模式。'
- en: Image Variation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像变化
- en: 'The UniDiffuser authors suggest performing image variation through a “round-trip”
    generation method, where given an input image, we first perform an image-to-text
    generation, and then perform a text-to-image generation on the outputs of the
    first generation. This produces a new image which is semantically similar to the
    input image:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: UniDiffuser的作者建议通过“往返”生成方法执行图像变化，即给定一个输入图像，首先执行图像到文本的生成，然后在第一代的输出上执行文本到图像的生成。这会产生一个与输入图像在语义上相似的新图像：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Text Variation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本变化
- en: 'Similarly, text variation can be performed on an input prompt with a text-to-image
    generation followed by a image-to-text generation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，可以通过文本到图像生成，然后通过图像到文本生成，在输入提示上执行文本变化：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必查看调度器指南，了解如何在调度器速度和质量之间进行权衡，并查看重复使用组件跨管道部分，以了解如何有效地将相同组件加载到多个管道中。
- en: UniDiffuserPipeline
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniDiffuserPipeline
- en: '### `class diffusers.UniDiffuserPipeline`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.UniDiffuserPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L51)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L51)'
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations. This is part of the UniDiffuser image representation along
    with the CLIP vision encoding.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）
    — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。这是UniDiffuser图像表示的一部分，与CLIP视觉编码一起。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`image_encoder` (`CLIPVisionModel`) — A [CLIPVisionModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)
    to encode images as part of its image representation along with the VAE latent
    representation.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_encoder` (`CLIPVisionModel`) — 一个[CLIPVisionModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)，用于将图像编码为其图像表示的一部分，以及VAE潜在表示。'
- en: '`image_processor` (`CLIPImageProcessor`) — [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    to preprocess an image before CLIP encoding it with `image_encoder`.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` (`CLIPImageProcessor`) — [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)，用于在使用`image_encoder`对图像进行编码之前预处理图像。'
- en: '`clip_tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize the prompt before encoding it with `text_encoder`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_tokenizer` (`CLIPTokenizer`) — 一个[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)，用于在使用`text_encoder`对提示进行编码之前对其进行标记化。'
- en: '`text_decoder` (`UniDiffuserTextDecoder`) — Frozen text decoder. This is a
    GPT-style model which is used to generate text from the UniDiffuser embedding.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_decoder` (`UniDiffuserTextDecoder`) — 冻结的文本解码器。这是一个类似GPT的模型，用于从UniDiffuser嵌入生成文本。'
- en: '`text_tokenizer` (`GPT2Tokenizer`) — A [GPT2Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Tokenizer)
    to decode text for text generation; used along with the `text_decoder`.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_tokenizer` (`GPT2Tokenizer`) — 一个[GPT2Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Tokenizer)，用于解码文本以进行文本生成；与`text_decoder`一起使用。'
- en: '`unet` (`UniDiffuserModel`) — A [U-ViT](https://github.com/baofff/U-ViT) model
    with UNNet-style skip connections between transformer layers to denoise the encoded
    image latents.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` (`UniDiffuserModel`) — 一个[U-ViT](https://github.com/baofff/U-ViT)模型，具有在变压器层之间的UNNet风格跳过连接，用于去噪编码图像潜在表示。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    and/or text latents. The original UniDiffuser paper uses the [DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)
    scheduler.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 与 `unet` 结合使用以去噪编码图像和/或文本潜在空间的调度器。原始 UniDiffuser 论文使用 [DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)
    调度器。'
- en: Pipeline for a bimodal image-text model which supports unconditional text and
    image generation, text-conditioned image generation, image-conditioned text generation,
    and joint image-text generation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用于支持无条件文本和图像生成、文本条件图像生成、图像条件文本生成以及联合图像文本生成的双模态图像文本模型的流水线。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L1079)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L1079)'
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`. Required for
    text-conditioned image generation (`text2img`) mode.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。文本条件图像生成（`text2img`）模式需要。'
- en: '`image` (`torch.FloatTensor` or `PIL.Image.Image`, *optional*) — `Image` or
    tensor representing an image batch. Required for image-conditioned text generation
    (`img2text`) mode.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.FloatTensor` or `PIL.Image.Image`, *optional*) — 代表图像批次的 `Image`
    或张量。图像条件文本生成（`img2text`）模式需要。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated image.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素宽度。'
- en: '`data_type` (`int`, *optional*, defaults to 1) — The data type (either 0 or
    1). Only used if you are loading a checkpoint which supports a data type embedding;
    this is added for compatibility with the [UniDiffuser-v1](https://huggingface.co/thu-ml/unidiffuser-v1)
    checkpoint.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_type` (`int`, *optional*, defaults to 1) — 数据类型（0 或 1）。仅在加载支持数据类型嵌入的检查点时使用；这是为了与
    [UniDiffuser-v1](https://huggingface.co/thu-ml/unidiffuser-v1) 检查点兼容而添加的。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, defaults to 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 8.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, defaults to 8.0) — 更高的引导比例值鼓励模型生成与文本
    `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`). Used in text-conditioned image generation (`text2img`) mode.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成中不包括的提示或提示。如果未定义，则需要传递
    `negative_prompt_embeds`。在不使用引导时被忽略（`guidance_scale < 1`）。用于文本条件图像生成（`text2img`）模式。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt. Used in `text2img` (text-conditioned image generation)
    and `img` mode. If the mode is joint and both `num_images_per_prompt` and `num_prompts_per_image`
    are supplied, `min(num_images_per_prompt, num_prompts_per_image)` samples are
    generated.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。用于
    `text2img`（文本条件图像生成）和 `img` 模式。如果模式是联合的，并且同时提供了 `num_images_per_prompt` 和 `num_prompts_per_image`，则生成
    `min(num_images_per_prompt, num_prompts_per_image)` 个样本。'
- en: '`num_prompts_per_image` (`int`, *optional*, defaults to 1) — The number of
    prompts to generate per image. Used in `img2text` (image-conditioned text generation)
    and `text` mode. If the mode is joint and both `num_images_per_prompt` and `num_prompts_per_image`
    are supplied, `min(num_images_per_prompt, num_prompts_per_image)` samples are
    generated.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_prompts_per_image` (`int`, *optional*, defaults to 1) — 每个图像生成的提示数量。用于
    `img2text`（图像条件文本生成）和 `text` 模式。如果模式是联合的，并且同时提供了 `num_images_per_prompt` 和 `num_prompts_per_image`，则生成
    `min(num_images_per_prompt, num_prompts_per_image)` 个样本。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数
    eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for joint image-text generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.
    This assumes a full set of VAE, CLIP, and text latents, if supplied, overrides
    the value of `prompt_latents`, `vae_latents`, and `clip_latents`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作联合图像文本生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。假设提供了完整的VAE、CLIP和文本潜变量，如果提供，则覆盖`prompt_latents`、`vae_latents`和`clip_latents`的值。'
- en: '`prompt_latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for text generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作文本生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`vae_latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`clip_latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument. Used in text-conditioned
    image generation (`text2img`) mode.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。用于文本条件图像生成（`text2img`）模式。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are be generated from the `negative_prompt`
    input argument. Used in text-conditioned image generation (`text2img`) mode.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。用于文本条件图像生成（`text2img`）模式。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    instead of a plain tuple.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)而不是普通的tuple。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *可选*) — 在推断过程中每`callback_steps`步调用一次的函数。该函数使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *可选*, 默认为1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调函数。'
- en: Returns
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    or `tuple`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)或`tuple`'
- en: If `return_dict` is `True`, [ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of generated texts.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[ImageTextPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/unidiffuser#diffusers.ImageTextPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是生成的文本列表。
- en: The call function to the pipeline for generation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: '#### `disable_vae_slicing`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L147)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L147)'
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片式VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `disable_vae_tiling`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L164)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L164)'
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用平铺式VAE解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。
- en: '#### `enable_vae_slicing`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L139)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L139)'
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。当启用此选项时，VAE将在几个步骤中将输入张量分割成片段以计算解码。这对于节省一些内存并允许更大的批量大小很有用。
- en: '#### `enable_vae_tiling`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L155)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L155)'
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 启用平铺VAE解码。当启用此选项时，VAE将将输入张量分成瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像很有用。
- en: '#### `encode_prompt`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L382)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L382)'
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str`或`List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 每个提示应生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用分类器自由引导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str`或`List[str]`, *可选*) — 不用于引导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用引导（即，如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *可选*) — 将应用于文本编码器的所有LoRA层的LoRA比例，如果加载了LoRA层。 '
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的层数。值为1意味着将使用预最终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: '#### `reset_mode`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reset_mode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L268)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L268)'
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Removes a manually set mode; after calling this, the pipeline will infer the
    mode from inputs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 移除手动设置的模式；调用此方法后，管道将从输入中推断模式。
- en: '#### `set_image_mode`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_image_mode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L252)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L252)'
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Manually set the generation mode to unconditional (“marginal”) image generation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 手动设置生成模式为无条件（“边际”）图像生成。
- en: '#### `set_image_to_text_mode`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_image_to_text_mode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L260)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L260)'
- en: '[PRE16]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Manually set the generation mode to image-conditioned text generation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 手动设置生成模式为图像条件的文本生成。
- en: '#### `set_joint_mode`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_joint_mode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L264)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L264)'
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Manually set the generation mode to unconditional joint image-text generation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 手动设置生成模式为无条件联合图像文本生成。
- en: '#### `set_text_mode`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_text_mode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L248)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L248)'
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Manually set the generation mode to unconditional (“marginal”) text generation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 手动设置生成模式为无条件（“边际”）文本生成。
- en: '#### `set_text_to_image_mode`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_text_to_image_mode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L256)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L256)'
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Manually set the generation mode to text-conditioned image generation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 手动设置生成模式为文本条件的图像生成。
- en: ImageTextPipelineOutput
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageTextPipelineOutput
- en: '### `class diffusers.ImageTextPipelineOutput`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.ImageTextPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L33)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py#L33)'
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL
    图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。'
- en: '`text` (`List[str]` or `List[List[str]]`) — List of generated text strings
    of length `batch_size` or a list of list of strings whose outer list has length
    `batch_size`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`List[str]` or `List[List[str]]`) — 长度为 `batch_size` 的生成文本字符串列表或外部列表长度为
    `batch_size` 的字符串列表的列表。'
- en: Output class for joint image-text pipelines.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 联合图像文本管道的输出类。
