- en: Bark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bark
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bark](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bark)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bark](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bark)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: Bark is a transformer-based text-to-speech model proposed by Suno AI in [suno-ai/bark](https://github.com/suno-ai/bark).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Barkæ˜¯ç”±Suno AIæå‡ºçš„åŸºäºTransformerçš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œä½äº[suno-ai/bark](https://github.com/suno-ai/bark)ã€‚
- en: 'Bark is made of 4 main models:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Barkç”±4ä¸ªä¸»è¦æ¨¡å‹ç»„æˆï¼š
- en: '[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)
    (also referred to as the â€˜textâ€™ model): a causal auto-regressive transformer model
    that takes as input tokenized text, and predicts semantic text tokens that capture
    the meaning of the text.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)ï¼ˆä¹Ÿç§°ä¸ºâ€œæ–‡æœ¬â€æ¨¡å‹ï¼‰ï¼šä¸€ä¸ªå› æœè‡ªå›å½’Transformeræ¨¡å‹ï¼Œå…¶è¾“å…¥ä¸ºæ ‡è®°åŒ–æ–‡æœ¬ï¼Œå¹¶é¢„æµ‹æ•æ‰æ–‡æœ¬å«ä¹‰çš„è¯­ä¹‰æ–‡æœ¬æ ‡è®°ã€‚'
- en: '[BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel)
    (also referred to as the â€˜coarse acousticsâ€™ model): a causal autoregressive transformer,
    that takes as input the results of the [BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)
    model. It aims at predicting the first two audio codebooks necessary for EnCodec.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel)ï¼ˆä¹Ÿç§°ä¸ºâ€œç²—å£°å­¦â€æ¨¡å‹ï¼‰ï¼šä¸€ä¸ªå› æœè‡ªå›å½’Transformerï¼Œå…¶è¾“å…¥ä¸º[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)æ¨¡å‹çš„ç»“æœã€‚å®ƒæ—¨åœ¨é¢„æµ‹EnCodecæ‰€éœ€çš„å‰ä¸¤ä¸ªéŸ³é¢‘ç æœ¬ã€‚'
- en: '[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)
    (the â€˜fine acousticsâ€™ model), this time a non-causal autoencoder transformer,
    which iteratively predicts the last codebooks based on the sum of the previous
    codebooks embeddings.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)ï¼ˆâ€œç²¾ç»†å£°å­¦â€æ¨¡å‹ï¼‰ï¼Œè¿™æ¬¡æ˜¯ä¸€ä¸ªéå› æœè‡ªç¼–ç å™¨Transformerï¼Œå®ƒæ ¹æ®å…ˆå‰ç æœ¬åµŒå…¥çš„æ€»å’Œè¿­ä»£é¢„æµ‹æœ€åçš„ç æœ¬ã€‚'
- en: having predicted all the codebook channels from the [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel),
    Bark uses it to decode the output audio array.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ä¸­é¢„æµ‹äº†æ‰€æœ‰ç æœ¬é€šé“åï¼ŒBarkä½¿ç”¨å®ƒæ¥è§£ç è¾“å‡ºéŸ³é¢‘æ•°ç»„ã€‚
- en: It should be noted that each of the first three modules can support conditional
    speaker embeddings to condition the output sound according to specific predefined
    voice.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‰ä¸‰ä¸ªæ¨¡å—ä¸­çš„æ¯ä¸€ä¸ªéƒ½å¯ä»¥æ”¯æŒæ¡ä»¶è¯´è¯è€…åµŒå…¥ï¼Œä»¥æ ¹æ®ç‰¹å®šé¢„å®šä¹‰çš„å£°éŸ³æ¥è°ƒæ•´è¾“å‡ºå£°éŸ³ã€‚
- en: This model was contributed by [Yoach Lacombe (ylacombe)](https://huggingface.co/ylacombe)
    and [Sanchit Gandhi (sanchit-gandhi)](https://github.com/sanchit-gandhi). The
    original code can be found [here](https://github.com/suno-ai/bark).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[Yoach Lacombe (ylacombe)](https://huggingface.co/ylacombe)å’Œ[Sanchit Gandhi
    (sanchit-gandhi)](https://github.com/sanchit-gandhi)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/suno-ai/bark)æ‰¾åˆ°ã€‚
- en: Optimizing Bark
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–Bark
- en: Bark can be optimized with just a few extra lines of code, which **significantly
    reduces its memory footprint** and **accelerates inference**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Barkå¯ä»¥é€šè¿‡æ·»åŠ å‡ è¡Œé¢å¤–çš„ä»£ç è¿›è¡Œä¼˜åŒ–ï¼Œ**æ˜¾è‘—å‡å°‘å…¶å†…å­˜å ç”¨**å¹¶**åŠ é€Ÿæ¨ç†**ã€‚
- en: Using half-precision
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŠç²¾åº¦
- en: You can speed up inference and reduce memory footprint by 50% simply by loading
    the model in half-precision.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†æ¨¡å‹åŠ è½½ä¸ºåŠç²¾åº¦ï¼Œå¯ä»¥å°†æ¨ç†åŠ é€Ÿå¹¶å‡å°‘å†…å­˜å ç”¨é‡50%ã€‚
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using CPU offload
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä½¿ç”¨CPUå¸è½½
- en: As mentioned above, Bark is made up of 4 sub-models, which are called up sequentially
    during audio generation. In other words, while one sub-model is in use, the other
    sub-models are idle.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€è¿°ï¼ŒBarkç”±4ä¸ªå­æ¨¡å‹ç»„æˆï¼Œåœ¨éŸ³é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­æŒ‰é¡ºåºè°ƒç”¨ã€‚æ¢å¥è¯è¯´ï¼Œå½“ä¸€ä¸ªå­æ¨¡å‹åœ¨ä½¿ç”¨æ—¶ï¼Œå…¶ä»–å­æ¨¡å‹å¤„äºç©ºé—²çŠ¶æ€ã€‚
- en: 'If youâ€™re using a CUDA device, a simple solution to benefit from an 80% reduction
    in memory footprint is to offload the submodels from GPU to CPU when theyâ€™re idle.
    This operation is called *CPU offloading*. You can use it with one line of code
    as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨CUDAè®¾å¤‡ï¼Œè¦è·å¾—80%çš„å†…å­˜å ç”¨å‡å°‘ï¼Œä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨å­æ¨¡å‹ç©ºé—²æ—¶å°†å…¶ä»GPUå¸è½½åˆ°CPUã€‚è¿™ä¸ªæ“ä½œç§°ä¸º*CPUå¸è½½*ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä¸€è¡Œä»£ç æ¥å®ç°ï¼š
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that ğŸ¤— Accelerate must be installed before using this feature. [Hereâ€™s
    how to install it.](https://huggingface.co/docs/accelerate/basic_tutorials/install)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨ä½¿ç”¨æ­¤åŠŸèƒ½ä¹‹å‰ï¼Œå¿…é¡»å®‰è£…ğŸ¤— Accelerateã€‚[è¿™é‡Œæ˜¯å¦‚ä½•å®‰è£…å®ƒçš„æ–¹æ³•ã€‚](https://huggingface.co/docs/accelerate/basic_tutorials/install)
- en: Using Better Transformer
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Better Transformer
- en: 'Better Transformer is an ğŸ¤— Optimum feature that performs kernel fusion under
    the hood. You can gain 20% to 30% in speed with zero performance degradation.
    It only requires one line of code to export the model to ğŸ¤— Better Transformer:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Better Transformeræ˜¯ä¸€ä¸ªğŸ¤— OptimumåŠŸèƒ½ï¼Œå¯ä»¥åœ¨åå°æ‰§è¡Œå†…æ ¸èåˆã€‚æ‚¨å¯ä»¥è·å¾—20%è‡³30%çš„é€Ÿåº¦æå‡ï¼Œè€Œæ€§èƒ½ä¸ä¼šé™ä½ã€‚åªéœ€ä¸€è¡Œä»£ç å³å¯å°†æ¨¡å‹å¯¼å‡ºåˆ°ğŸ¤—
    Better Transformerï¼š
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that ğŸ¤— Optimum must be installed before using this feature. [Hereâ€™s how
    to install it.](https://huggingface.co/docs/optimum/installation)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨ä½¿ç”¨æ­¤åŠŸèƒ½ä¹‹å‰ï¼Œå¿…é¡»å®‰è£…ğŸ¤— Optimumã€‚[è¿™é‡Œæ˜¯å¦‚ä½•å®‰è£…å®ƒçš„æ–¹æ³•ã€‚](https://huggingface.co/docs/optimum/installation)
- en: Using Flash Attention 2
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Flash Attention 2
- en: Flash Attention 2 is an even faster, optimized version of the previous optimization.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Flash Attention 2æ˜¯å‰ä¸€ä¸ªä¼˜åŒ–çš„æ›´å¿«ã€ä¼˜åŒ–ç‰ˆæœ¬ã€‚
- en: Installation
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å®‰è£…
- en: First, check whether your hardware is compatible with Flash Attention 2\. The
    latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).
    If your hardware is not compatible with Flash Attention 2, you can still benefit
    from attention kernel optimisations through Better Transformer support covered
    [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ£€æŸ¥æ‚¨çš„ç¡¬ä»¶æ˜¯å¦ä¸Flash Attention 2å…¼å®¹ã€‚æœ€æ–°çš„å…¼å®¹ç¡¬ä»¶åˆ—è¡¨å¯ä»¥åœ¨[å®˜æ–¹æ–‡æ¡£](https://github.com/Dao-AILab/flash-attention#installation-and-features)ä¸­æ‰¾åˆ°ã€‚å¦‚æœæ‚¨çš„ç¡¬ä»¶ä¸Flash
    Attention 2ä¸å…¼å®¹ï¼Œæ‚¨ä»ç„¶å¯ä»¥é€šè¿‡ä¸Šé¢æåˆ°çš„Better Transformeræ”¯æŒä»æ³¨æ„åŠ›å†…æ ¸ä¼˜åŒ–ä¸­å—ç›Šã€‚
- en: 'Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features)
    the latest version of Flash Attention 2:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œ[å®‰è£…](https://github.com/Dao-AILab/flash-attention#installation-and-features)æœ€æ–°ç‰ˆæœ¬çš„Flash
    Attention 2ï¼š
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Usage
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç”¨æ³•
- en: 'To load a model using Flash Attention 2, we can pass the `attn_implementation="flash_attention_2"`
    flag to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained).
    Weâ€™ll also load the model in half-precision (e.g. `torch.float16`), since it results
    in almost no degradation to audio quality but significantly lower memory usage
    and faster inference:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨Flash Attention 2åŠ è½½æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨[`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)ä¸­ä¼ é€’`attn_implementation="flash_attention_2"`æ ‡å¿—æ¥å®ç°ã€‚æˆ‘ä»¬è¿˜å°†ä»¥åŠç²¾åº¦ï¼ˆä¾‹å¦‚`torch.float16`ï¼‰åŠ è½½æ¨¡å‹ï¼Œå› ä¸ºè¿™å‡ ä¹ä¸ä¼šå¯¹éŸ³é¢‘è´¨é‡é€ æˆé™çº§ï¼Œä½†å†…å­˜ä½¿ç”¨é‡æ˜æ˜¾é™ä½ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ï¼š
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Performance comparison
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ€§èƒ½æ¯”è¾ƒ
- en: 'The following diagram shows the latency for the native attention implementation
    (no optimisation) against Better Transformer and Flash Attention 2\. In all cases,
    we generate 400 semantic tokens on a 40GB A100 GPU with PyTorch 2.1\. Flash Attention
    2 is also consistently faster than Better Transformer, and its performance improves
    even more as batch sizes increase:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾è¡¨æ˜¾ç¤ºäº†åŸç”Ÿæ³¨æ„åŠ›å®ç°ï¼ˆæ— ä¼˜åŒ–ï¼‰ä¸Better Transformerå’ŒFlash Attention 2ä¹‹é—´çš„å»¶è¿Ÿã€‚åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨40GB
    A100 GPUä¸Šä½¿ç”¨PyTorch 2.1ç”Ÿæˆ400ä¸ªè¯­ä¹‰æ ‡è®°ã€‚Flash Attention 2ä¹Ÿæ¯”Better Transformeræ›´å¿«ï¼Œå¹¶ä¸”éšç€æ‰¹é‡å¤§å°çš„å¢åŠ ï¼Œå…¶æ€§èƒ½ç”šè‡³æ›´å¥½ï¼š
- en: '![](../Images/62ff1093eff6a3b02e14aad370aaa0c2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62ff1093eff6a3b02e14aad370aaa0c2.png)'
- en: To put this into perspective, on an NVIDIA A100 and when generating 400 semantic
    tokens with a batch size of 16, you can get 17 times the [throughput](https://huggingface.co/blog/optimizing-bark#throughput)
    and still be 2 seconds faster than generating sentences one by one with the native
    model implementation. In other words, all the samples will be generated 17 times
    faster.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¸ªä¾‹å­ï¼Œåœ¨NVIDIA A100ä¸Šï¼Œå½“ä½¿ç”¨æ‰¹é‡å¤§å°ä¸º16ç”Ÿæˆ400ä¸ªè¯­ä¹‰æ ‡è®°æ—¶ï¼Œæ‚¨å¯ä»¥è·å¾—17å€çš„[ååé‡](https://huggingface.co/blog/optimizing-bark#throughput)ï¼Œå¹¶ä¸”ä»ç„¶æ¯”ä½¿ç”¨åŸç”Ÿæ¨¡å‹å®ç°é€å¥ç”Ÿæˆå¥å­å¿«2ç§’ã€‚æ¢å¥è¯è¯´ï¼Œæ‰€æœ‰æ ·æœ¬å°†ç”Ÿæˆé€Ÿåº¦æé«˜17å€ã€‚
- en: At batch size 8, on an NVIDIA A100, Flash Attention 2 is also 10% faster than
    Better Transformer, and at batch size 16, 25%.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰¹é‡å¤§å°ä¸º8æ—¶ï¼Œåœ¨NVIDIA A100ä¸Šï¼ŒFlash Attention 2ä¹Ÿæ¯”Better Transformerå¿«10%ï¼Œåœ¨æ‰¹é‡å¤§å°ä¸º16æ—¶ï¼Œå¿«25%ã€‚
- en: Combining optimization techniques
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç»“åˆä¼˜åŒ–æŠ€æœ¯
- en: You can combine optimization techniques, and use CPU offload, half-precision
    and Flash Attention 2 (or ğŸ¤— Better Transformer) all at once.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ç»“åˆä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒæ—¶ä½¿ç”¨CPUå¸è½½ã€åŠç²¾åº¦å’ŒFlash Attention 2ï¼ˆæˆ–ğŸ¤— Better Transformerï¼‰ã€‚
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Find out more on inference optimization techniques [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨ç†ä¼˜åŒ–æŠ€æœ¯ä¸Šäº†è§£æ›´å¤šä¿¡æ¯[è¿™é‡Œ](https://huggingface.co/docs/transformers/perf_infer_gpu_one)ã€‚
- en: Usage tips
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: Suno offers a library of voice presets in a number of languages [here](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c).
    These presets are also uploaded in the hub [here](https://huggingface.co/suno/bark-small/tree/main/speaker_embeddings)
    or [here](https://huggingface.co/suno/bark/tree/main/speaker_embeddings).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Sunoæä¾›äº†å¤šç§è¯­è¨€çš„å£°éŸ³é¢„è®¾åº“[è¿™é‡Œ](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c)ã€‚è¿™äº›é¢„è®¾ä¹Ÿä¸Šä¼ åˆ°äº†hub
    [è¿™é‡Œ](https://huggingface.co/suno/bark-small/tree/main/speaker_embeddings) æˆ– [è¿™é‡Œ](https://huggingface.co/suno/bark/tree/main/speaker_embeddings)ã€‚
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Bark can generate highly realistic, **multilingual** speech as well as other
    audio - including music, background noise and simple sound effects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Barkå¯ä»¥ç”Ÿæˆé«˜åº¦é€¼çœŸçš„**å¤šè¯­è¨€**è¯­éŸ³ä»¥åŠå…¶ä»–éŸ³é¢‘ - åŒ…æ‹¬éŸ³ä¹ã€èƒŒæ™¯å™ªéŸ³å’Œç®€å•çš„éŸ³æ•ˆã€‚
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The model can also produce **nonverbal communications** like laughing, sighing
    and crying.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹è¿˜å¯ä»¥äº§ç”Ÿåƒç¬‘ã€å¹æ¯å’Œå“­æ³£ç­‰**éè¯­è¨€äº¤æµ**ã€‚
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To save the audio, simply take the sample rate from the model config and some
    scipy utility:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¿å­˜éŸ³é¢‘ï¼Œåªéœ€ä»æ¨¡å‹é…ç½®ä¸­è·å–é‡‡æ ·ç‡å’Œä¸€äº›scipyå®ç”¨ç¨‹åºï¼š
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: BarkConfig
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkConfig
- en: '### `class transformers.BarkConfig`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L219)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L219)'
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`semantic_config` ([BarkSemanticConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticConfig),
    *optional*) â€” Configuration of the underlying semantic sub-model.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_config` ([BarkSemanticConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticConfig),
    *optional*) â€” åº•å±‚è¯­ä¹‰å­æ¨¡å‹çš„é…ç½®ã€‚'
- en: '`coarse_acoustics_config` ([BarkCoarseConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseConfig),
    *optional*) â€” Configuration of the underlying coarse acoustics sub-model.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coarse_acoustics_config` ([BarkCoarseConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseConfig),
    *optional*) â€” åº•å±‚ç²—ç³™å£°å­¦å­æ¨¡å‹çš„é…ç½®ã€‚'
- en: '`fine_acoustics_config` ([BarkFineConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineConfig),
    *optional*) â€” Configuration of the underlying fine acoustics sub-model.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fine_acoustics_config` ([BarkFineConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineConfig),
    *optional*) â€” åº•å±‚ç²¾ç»†å£°å­¦å­æ¨¡å‹çš„é…ç½®ã€‚'
- en: '`codec_config` ([AutoConfig](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoConfig),
    *optional*) â€” Configuration of the underlying codec sub-model.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codec_config` ([AutoConfig](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoConfig),
    *optional*) â€” åº•å±‚ç¼–è§£ç å™¨å­æ¨¡å‹çš„é…ç½®ã€‚'
- en: Example â€”
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ â€”
- en: This is the configuration class to store the configuration of a [BarkModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkModel).
    It is used to instantiate a Bark model according to the specified sub-models configurations,
    defining the model architecture.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[BarkModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkModel)çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å­æ¨¡å‹é…ç½®å®ä¾‹åŒ–Barkæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚
- en: Instantiating a configuration with the defaults will yield a similar configuration
    to that of the Bark [suno/bark](https://huggingface.co/suno/bark) architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿä¸Bark [suno/bark](https://huggingface.co/suno/bark)æ¶æ„ç±»ä¼¼çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `from_sub_model_configs`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_sub_model_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L309)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L309)'
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Returns
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BarkConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkConfig)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[BarkConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkConfig)'
- en: An instance of a configuration object
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡çš„å®ä¾‹
- en: Instantiate a [BarkConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkConfig)
    (or a derived class) from bark sub-models configuration.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä»barkå­æ¨¡å‹é…ç½®å®ä¾‹åŒ–ä¸€ä¸ª[BarkConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkConfig)ï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚
- en: BarkProcessor
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkProcessor
- en: '### `class transformers.BarkProcessor`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L34)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L34)'
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” An instance of [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)çš„å®ä¾‹ã€‚'
- en: '`speaker_embeddings` (`Dict[Dict[str]]`, *optional*) â€” Optional nested speaker
    embeddings dictionary. The first level contains voice preset names (e.g `"en_speaker_4"`).
    The second level contains `"semantic_prompt"`, `"coarse_prompt"` and `"fine_prompt"`
    embeddings. The values correspond to the path of the corresponding `np.ndarray`.
    See [here](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c)
    for a list of `voice_preset_names`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speaker_embeddings` (`Dict[Dict[str]]`, *å¯é€‰*) â€” å¯é€‰çš„åµŒå¥—è¯´è¯è€…åµŒå…¥å­—å…¸ã€‚ç¬¬ä¸€çº§åŒ…å«å£°éŸ³é¢„è®¾åç§°ï¼ˆä¾‹å¦‚`"en_speaker_4"`ï¼‰ã€‚ç¬¬äºŒçº§åŒ…å«`"semantic_prompt"`ã€`"coarse_prompt"`å’Œ`"fine_prompt"`åµŒå…¥ã€‚å€¼å¯¹åº”äºç›¸åº”`np.ndarray`çš„è·¯å¾„ã€‚è¯·å‚é˜…[æ­¤å¤„](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c)è·å–`voice_preset_names`åˆ—è¡¨ã€‚'
- en: Constructs a Bark processor which wraps a text tokenizer and optional Bark voice
    presets into a single processor.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªBarkå¤„ç†å™¨ï¼Œå°†æ–‡æœ¬æ ‡è®°å™¨å’Œå¯é€‰çš„Barkå£°éŸ³é¢„è®¾åŒ…è£…æˆä¸€ä¸ªå¤„ç†å™¨ã€‚
- en: '#### `__call__`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L219)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L219)'
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence can be a string or a list of strings (pretokenized
    string). If the sequences are provided as list of strings (pretokenized), you
    must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸åºåˆ—æ‰¹æ¬¡çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`voice_preset` (`str`, `Dict[np.ndarray]`) â€” The voice preset, i.e the speaker
    embeddings. It can either be a valid voice_preset name, e.g `"en_speaker_1"`,
    or directly a dictionnary of `np.ndarray` embeddings for each submodel of `Bark`.
    Or it can be a valid file name of a local `.npz` single voice preset.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`voice_preset` (`str`, `Dict[np.ndarray]`) â€” å£°éŸ³é¢„è®¾ï¼Œå³è¯´è¯è€…åµŒå…¥ã€‚å®ƒå¯ä»¥æ˜¯æœ‰æ•ˆçš„voice_presetåç§°ï¼Œä¾‹å¦‚`"en_speaker_1"`ï¼Œæˆ–ç›´æ¥æ˜¯`Bark`çš„æ¯ä¸ªå­æ¨¡å‹çš„`np.ndarray`åµŒå…¥çš„å­—å…¸ã€‚æˆ–è€…å®ƒå¯ä»¥æ˜¯æœ¬åœ°`.npz`å•ä¸ªå£°éŸ³é¢„è®¾çš„æœ‰æ•ˆæ–‡ä»¶åã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors of a particular framework. Acceptable
    values are:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›ç‰¹å®šæ¡†æ¶çš„å¼ é‡ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return NumPy `np.ndarray` objects.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å›NumPy `np.ndarray`å¯¹è±¡ã€‚'
- en: Returns
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: Tuple([BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding),
    [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature))
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å…ƒç»„([BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding),
    [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature))
- en: A tuple composed of a [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding),
    i.e the output of the `tokenizer` and a [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature),
    i.e the voice preset with the right tensors type.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…ƒç»„ï¼Œç”±ä¸€ä¸ª[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ç»„æˆï¼Œå³`tokenizer`çš„è¾“å‡ºï¼Œä»¥åŠä¸€ä¸ª[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)ï¼Œå³å…·æœ‰æ­£ç¡®å¼ é‡ç±»å‹çš„å£°éŸ³é¢„è®¾ã€‚
- en: Main method to prepare for the model one or several sequences(s). This method
    forwards the `text` and `kwargs` arguments to the AutoTokenizerâ€™s `__call__()`
    to encode the text. The method also proposes a voice preset which is a dictionary
    of arrays that conditions `Bark`â€™s output. `kwargs` arguments are forwarded to
    the tokenizer and to `cached_file` method if `voice_preset` is a valid filename.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡æ¨¡å‹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—çš„ä¸»è¦æ–¹æ³•ã€‚æ­¤æ–¹æ³•å°†`text`å’Œ`kwargs`å‚æ•°è½¬å‘ç»™AutoTokenizerçš„`__call__()`ä»¥å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸€ä¸ªå£°éŸ³é¢„è®¾ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ•°ç»„å­—å…¸ï¼Œç”¨äºæ¡ä»¶åŒ–`Bark`çš„è¾“å‡ºã€‚å¦‚æœ`voice_preset`æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶åï¼Œåˆ™`kwargs`å‚æ•°å°†è¢«è½¬å‘ç»™tokenizerå’Œ`cached_file`æ–¹æ³•ã€‚
- en: '#### `from_pretrained`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L64)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L64)'
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) â€” This can be either:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path` (`str` æˆ– `os.PathLike`) â€” è¿™å¯ä»¥æ˜¯ï¼š'
- en: a string, the *model id* of a pretrained [BarkProcessor](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkProcessor)
    hosted inside a model repo on huggingface.co. Valid model ids can be located at
    the root-level, like `bert-base-uncased`, or namespaced under a user or organization
    name, like `dbmdz/bert-base-german-cased`.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒçš„ [BarkProcessor](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkProcessor)
    çš„*æ¨¡å‹ID*ï¼Œæ‰˜ç®¡åœ¨huggingface.coä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ä¸­ã€‚æœ‰æ•ˆçš„æ¨¡å‹IDå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚ `bert-base-uncased`ï¼Œæˆ–è€…åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹å‘½åç©ºé—´åŒ–ï¼Œå¦‚
    `dbmdz/bert-base-german-cased`ã€‚
- en: a path to a *directory* containing a processor saved using the [save_pretrained()](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkProcessor.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡å‘åŒ…å«ä½¿ç”¨ [save_pretrained()](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkProcessor.save_pretrained)
    æ–¹æ³•ä¿å­˜çš„å¤„ç†å™¨çš„*ç›®å½•*è·¯å¾„ï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/`ã€‚
- en: '`speaker_embeddings_dict_path` (`str`, *optional*, defaults to `"speaker_embeddings_path.json"`)
    â€” The name of the `.json` file containing the speaker_embeddings dictionnary located
    in `pretrained_model_name_or_path`. If `None`, no speaker_embeddings is loaded.
    **kwargs â€” Additional keyword arguments passed along to both `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speaker_embeddings_dict_path` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"speaker_embeddings_path.json"`)
    â€” åŒ…å«ä½äº `pretrained_model_name_or_path` ä¸­çš„è¯´è¯è€…åµŒå…¥å­—å…¸çš„ `.json` æ–‡ä»¶çš„åç§°ã€‚å¦‚æœä¸º `None`ï¼Œåˆ™ä¸åŠ è½½è¯´è¯è€…åµŒå…¥ã€‚**kwargs
    â€” ä¼ é€’ç»™ `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained` çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Instantiate a Bark processor associated with a pretrained model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸å…³è”çš„Barkå¤„ç†å™¨ã€‚
- en: '#### `save_pretrained`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L118)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/processing_bark.py#L118)'
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory where the tokenizer files
    and the speaker embeddings will be saved (directory will be created if it does
    not exist).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory` (`str` æˆ– `os.PathLike`) â€” å°†åˆ†è¯å™¨æ–‡ä»¶å’Œè¯´è¯è€…åµŒå…¥ä¿å­˜åœ¨å…¶ä¸­çš„ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºç›®å½•ï¼‰ã€‚'
- en: '`speaker_embeddings_dict_path` (`str`, *optional*, defaults to `"speaker_embeddings_path.json"`)
    â€” The name of the `.json` file that will contains the speaker_embeddings nested
    path dictionnary, if it exists, and that will be located in `pretrained_model_name_or_path/speaker_embeddings_directory`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speaker_embeddings_dict_path` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"speaker_embeddings_path.json"`)
    â€” åŒ…å«è¯´è¯è€…åµŒå…¥åµŒå¥—è·¯å¾„å­—å…¸çš„ `.json` æ–‡ä»¶çš„åç§°ï¼Œå¦‚æœå­˜åœ¨ï¼Œå°†ä½äº `pretrained_model_name_or_path/speaker_embeddings_directory`
    ä¸­ã€‚'
- en: '`speaker_embeddings_directory` (`str`, *optional*, defaults to `"speaker_embeddings/"`)
    â€” The name of the folder in which the speaker_embeddings arrays will be saved.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speaker_embeddings_directory` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"speaker_embeddings/"`) â€”
    è¯´è¯è€…åµŒå…¥æ•°ç»„å°†ä¿å­˜åœ¨å…¶ä¸­çš„æ–‡ä»¶å¤¹çš„åç§°ã€‚'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace). kwargs â€” Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨ä¿å­˜åå°†æ¨¡å‹æ¨é€åˆ°Hugging Faceæ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨
    `repo_id` æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„ `save_directory` åç§°ï¼‰ã€‚kwargs â€” ä¼ é€’ç»™ [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Saves the attributes of this processor (tokenizerâ€¦) in the specified directory
    so that it can be reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkProcessor.from_pretrained)
    method.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å¤„ç†å™¨çš„å±æ€§ï¼ˆåˆ†è¯å™¨ç­‰ï¼‰ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkProcessor.from_pretrained)
    æ–¹æ³•é‡æ–°åŠ è½½ã€‚
- en: BarkModel
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkModel
- en: '### `class transformers.BarkModel`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1629)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1629)'
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([BarkConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([BarkConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkConfig))
    â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: 'The full Bark model, a text-to-speech model composed of 4 sub-models:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„Barkæ¨¡å‹ï¼Œä¸€ä¸ªç”±4ä¸ªå­æ¨¡å‹ç»„æˆçš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼š
- en: '[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)
    (also referred to as the â€˜textâ€™ model): a causal auto-regressive transformer model
    that takes as input tokenized text, and predicts semantic text tokens that capture
    the meaning of the text.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)ï¼ˆä¹Ÿç§°ä¸ºâ€˜æ–‡æœ¬â€™æ¨¡å‹ï¼‰ï¼šä¸€ä¸ªå› æœè‡ªå›å½’å˜æ¢å™¨æ¨¡å‹ï¼Œä»¥æ ‡è®°åŒ–æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹æ•æ‰æ–‡æœ¬å«ä¹‰çš„è¯­ä¹‰æ–‡æœ¬æ ‡è®°ã€‚'
- en: '[BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel)
    (also refered to as the â€˜coarse acousticsâ€™ model), also a causal autoregressive
    transformer, that takes into input the results of the last model. It aims at regressing
    the first two audio codebooks necessary to `encodec`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel)ï¼ˆä¹Ÿç§°ä¸ºâ€˜ç²—å£°å­¦â€™æ¨¡å‹ï¼‰ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå› æœè‡ªå›å½’å˜æ¢å™¨ï¼Œå®ƒæ¥å—ä¸Šä¸€ä¸ªæ¨¡å‹çš„ç»“æœä½œä¸ºè¾“å…¥ã€‚å®ƒæ—¨åœ¨å›å½’å‡ºç¼–ç æ‰€éœ€çš„å‰ä¸¤ä¸ªéŸ³é¢‘ç ä¹¦ã€‚'
- en: '[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)
    (the â€˜fine acousticsâ€™ model), this time a non-causal autoencoder transformer,
    which iteratively predicts the last codebooks based on the sum of the previous
    codebooks embeddings.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)ï¼ˆ''fine
    acoustics''æ¨¡å‹ï¼‰ï¼Œè¿™æ¬¡æ˜¯ä¸€ä¸ªéå› æœè‡ªåŠ¨ç¼–ç å™¨å˜å‹å™¨ï¼Œå®ƒåŸºäºå‰ä¸€ä¸ªç æœ¬åµŒå…¥çš„æ€»å’Œæ¥è¿­ä»£é¢„æµ‹æœ€åçš„ç æœ¬ã€‚'
- en: having predicted all the codebook channels from the [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel),
    Bark uses it to decode the output audio array.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ä¸­é¢„æµ‹å‡ºæ‰€æœ‰ç æœ¬é€šé“åï¼ŒBarkä½¿ç”¨å®ƒæ¥è§£ç è¾“å‡ºéŸ³é¢‘æ•°ç»„ã€‚
- en: It should be noted that each of the first three modules can support conditional
    speaker embeddings to condition the output sound according to specific predefined
    voice.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‰ä¸‰ä¸ªæ¨¡å—ä¸­çš„æ¯ä¸€ä¸ªéƒ½å¯ä»¥æ”¯æŒæ¡ä»¶è¯´è¯è€…åµŒå…¥ï¼Œæ ¹æ®ç‰¹å®šé¢„å®šä¹‰çš„å£°éŸ³æ¥è°ƒæ•´è¾“å‡ºå£°éŸ³ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚
- en: '#### `generate`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1737)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1737)'
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`Optional[torch.Tensor]` of shape (batch_size, seq_len), *optional*)
    â€” Input ids. Will be truncated up to 256 tokens. Note that the output audios will
    be as long as the longest generation among the batch.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸ºï¼ˆbatch_sizeï¼Œseq_lenï¼‰çš„`Optional[torch.Tensor]`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥idã€‚å°†è¢«æˆªæ–­è‡³256ä¸ªæ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œè¾“å‡ºéŸ³é¢‘çš„é•¿åº¦å°†ä¸æ‰¹æ¬¡ä¸­æœ€é•¿çš„ç”Ÿæˆé•¿åº¦ä¸€æ ·ã€‚'
- en: '`history_prompt` (`Optional[Dict[str,torch.Tensor]]`, *optional*) â€” Optional
    `Bark` speaker prompt. Note that for now, this model takes only one speaker prompt
    per batch.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`history_prompt`ï¼ˆ`Optional[Dict[str,torch.Tensor]]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰çš„`Bark`è¯´è¯è€…æç¤ºã€‚è¯·æ³¨æ„ï¼Œç›®å‰ï¼Œè¯¥æ¨¡å‹æ¯æ‰¹æ¬¡åªæ¥å—ä¸€ä¸ªè¯´è¯è€…æç¤ºã€‚'
- en: '`kwargs` (*optional*) â€” Remaining dictionary of keyword arguments. Keyword
    arguments are of two types:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ*å¯é€‰*ï¼‰â€” å‰©ä½™çš„å…³é”®å­—å‚æ•°å­—å…¸ã€‚å…³é”®å­—å‚æ•°æœ‰ä¸¤ç§ç±»å‹ï¼š'
- en: Without a prefix, they will be entered as `**kwargs` for the `generate` method
    of each sub-model.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ²¡æœ‰å‰ç¼€ï¼Œå®ƒä»¬å°†ä½œä¸ºæ¯ä¸ªå­æ¨¡å‹çš„`generate`æ–¹æ³•çš„`**kwargs`è¾“å…¥ã€‚
- en: With a *semantic_*, *coarse_*, *fine_* prefix, they will be input for the `generate`
    method of the semantic, coarse and fine respectively. It has the priority over
    the keywords without a prefix.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨*semantic_*ã€*coarse_*ã€*fine_*å‰ç¼€ï¼Œå®ƒä»¬å°†ä½œä¸ºè¯­ä¹‰ã€ç²—ç³™å’Œç»†è‡´çš„`generate`æ–¹æ³•çš„è¾“å…¥ã€‚å®ƒä¼˜å…ˆäºæ²¡æœ‰å‰ç¼€çš„å…³é”®å­—ã€‚
- en: This means you can, for example, specify a generation strategy for all sub-models
    except one.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä¸ºæ‰€æœ‰å­æ¨¡å‹æŒ‡å®šä¸€ä¸ªç”Ÿæˆç­–ç•¥ï¼Œé™¤äº†ä¸€ä¸ªã€‚
- en: '`return_output_lengths` (`bool`, *optional*) â€” Whether or not to return the
    waveform lengths. Useful when batching.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_output_lengths`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¢å½¢é•¿åº¦ã€‚åœ¨æ‰¹å¤„ç†æ—¶å¾ˆæœ‰ç”¨ã€‚'
- en: Returns
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: By default
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹
- en: '`audio_waveform` (`torch.Tensor` of shape (batch_size, seq_len)): Generated
    audio waveform. When `return_output_lengths=True`: Returns a tuple made of:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio_waveform`ï¼ˆå½¢çŠ¶ä¸ºï¼ˆbatch_sizeï¼Œseq_lenï¼‰çš„`torch.Tensor`ï¼‰ï¼šç”Ÿæˆçš„éŸ³é¢‘æ³¢å½¢ã€‚å½“`return_output_lengths=True`æ—¶ï¼šè¿”å›ä¸€ä¸ªç”±ä»¥ä¸‹å…ƒç»„ç»„æˆï¼š'
- en: '`audio_waveform` (`torch.Tensor` of shape (batch_size, seq_len)): Generated
    audio waveform.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio_waveform`ï¼ˆå½¢çŠ¶ä¸ºï¼ˆbatch_sizeï¼Œseq_lenï¼‰çš„`torch.Tensor`ï¼‰ï¼šç”Ÿæˆçš„éŸ³é¢‘æ³¢å½¢ã€‚'
- en: '`output_lengths` (`torch.Tensor` of shape (batch_size)): The length of each
    waveform in the batch'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_lengths`ï¼ˆå½¢çŠ¶ä¸ºï¼ˆbatch_sizeï¼‰çš„`torch.Tensor`ï¼‰ï¼šæ‰¹æ¬¡ä¸­æ¯ä¸ªæ³¢å½¢çš„é•¿åº¦'
- en: Generates audio from an input prompt and an additional optional `Bark` speaker
    prompt.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å…¥æç¤ºå’Œä¸€ä¸ªé¢å¤–çš„å¯é€‰`Bark`è¯´è¯è€…æç¤ºç”ŸæˆéŸ³é¢‘ã€‚
- en: 'Example:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#### `enable_cpu_offload`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1680)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1680)'
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`gpu_id` (`int`, *optional*, defaults to 0) â€” GPU id on which the sub-models
    will be loaded and offloaded.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpu_id`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” å­æ¨¡å‹å°†åŠ è½½å’Œå¸è½½çš„GPU idã€‚'
- en: Offloads all sub-models to CPU using accelerate, reducing memory usage with
    a low impact on performance. This method moves one whole sub-model at a time to
    the GPU when it is used, and the sub-model remains in GPU until the next sub-model
    runs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŠ é€Ÿå™¨å°†æ‰€æœ‰å­æ¨¡å‹å¸è½½åˆ°CPUï¼Œå‡å°‘å†…å­˜ä½¿ç”¨é‡ï¼Œå¯¹æ€§èƒ½å½±å“è¾ƒå°ã€‚è¯¥æ–¹æ³•åœ¨ä½¿ç”¨æ—¶ä¸€æ¬¡å°†ä¸€ä¸ªå®Œæ•´çš„å­æ¨¡å‹ç§»åŠ¨åˆ°GPUï¼Œå¹¶ä¸”å­æ¨¡å‹åœ¨GPUä¸­ä¿æŒï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªå­æ¨¡å‹è¿è¡Œã€‚
- en: BarkSemanticModel
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkSemanticModel
- en: '### `class transformers.BarkSemanticModel`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkSemanticModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L913)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L913)'
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([BarkSemanticConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[BarkSemanticConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Bark semantic (or text) model. It shares the same architecture as the coarse
    model. It is a GPT-2 like autoregressive model with a language modeling head on
    top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Barkè¯­ä¹‰ï¼ˆæˆ–æ–‡æœ¬ï¼‰æ¨¡å‹ã€‚å®ƒä¸ç²—æ¨¡å‹å…±äº«ç›¸åŒçš„æ¶æ„ã€‚è¿™æ˜¯ä¸€ä¸ªç±»ä¼¼äºGPT-2çš„è‡ªå›å½’æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L749)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L749)'
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†è¢«å¿½ç•¥ã€‚å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’äº†`use_cache`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰-
    é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„æ ‡è®°ï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0,
    1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºâ€œæœªå±è”½â€çš„æ ‡è®°ï¼Œä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºâ€œå±è”½â€çš„æ ‡è®°ï¼Œä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºå°†ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªâ€œå±è”½â€ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«â€œå±è”½â€ã€‚
- en: '`input_embeds` (`torch.FloatTensor` of shape `(batch_size, input_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. Here, due to `Bark` particularities,
    if `past_key_values` is used, `input_embeds` will be ignored and you have to use
    `input_ids`. If `past_key_values` is not used and `use_cache` is set to `True`,
    `input_embeds` is used in priority instead of `input_ids`.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, input_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰-
    å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚åœ¨è¿™é‡Œï¼Œç”±äº`Bark`çš„ç‰¹æ®Šæ€§ï¼Œå¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå°†å¿½ç•¥`input_embeds`ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨`input_ids`ã€‚å¦‚æœæœªä½¿ç”¨`past_key_values`ä¸”`use_cache`è®¾ç½®ä¸º`True`ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨`input_embeds`è€Œä¸æ˜¯`input_ids`ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: The [BarkCausalModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCausalModel)
    forward method, overrides the `__call__` special method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[BarkCausalModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCausalModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: BarkCoarseModel
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkCoarseModel
- en: '### `class transformers.BarkCoarseModel`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkCoarseModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1022)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1022)'
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([BarkCoarseConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([BarkCoarseConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Bark coarse acoustics model. It shares the same architecture as the semantic
    (or text) model. It is a GPT-2 like autoregressive model with a language modeling
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Barkç²—ç³™å£°å­¦æ¨¡å‹ã€‚å®ƒä¸è¯­ä¹‰ï¼ˆæˆ–æ–‡æœ¬ï¼‰æ¨¡å‹å…±äº«ç›¸åŒçš„æ¶æ„ã€‚è¿™æ˜¯ä¸€ä¸ªç±»ä¼¼äºGPT-2çš„è‡ªå›å½’æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L749)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L749)'
- en: '[PRE23]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache`å‚æ•°æˆ–è€…`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆæŸ¥çœ‹`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`çš„`input_ids`ã€‚
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æ˜¯`æœªå±è”½`ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨æ˜¯`masked`ã€‚
- en: '`input_embeds` (`torch.FloatTensor` of shape `(batch_size, input_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. Here, due to `Bark` particularities,
    if `past_key_values` is used, `input_embeds` will be ignored and you have to use
    `input_ids`. If `past_key_values` is not used and `use_cache` is set to `True`,
    `input_embeds` is used in priority instead of `input_ids`.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, input_sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚åœ¨è¿™é‡Œï¼Œç”±äº`Bark`çš„ç‰¹æ®Šæ€§ï¼Œå¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å°†å¿½ç•¥`input_embeds`ï¼Œå¿…é¡»ä½¿ç”¨`input_ids`ã€‚å¦‚æœæœªä½¿ç”¨`past_key_values`å¹¶ä¸”`use_cache`è®¾ç½®ä¸º`True`ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨`input_embeds`è€Œä¸æ˜¯`input_ids`ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`ï¼Œ*å¯é€‰*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: The [BarkCausalModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCausalModel)
    forward method, overrides the `__call__` special method.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[BarkCausalModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCausalModel)çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰é…æ–¹ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: BarkFineModel
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkFineModel
- en: '### `class transformers.BarkFineModel`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkFineModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1243)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1243)'
- en: '[PRE24]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([BarkFineConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([BarkFineConfig](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Bark fine acoustics model. It is a non-causal GPT-like model with `config.n_codes_total`
    embedding layers and language modeling heads, one for each codebook. This model
    inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Bark fine acoustics model. It is a non-causal GPT-like model with `config.n_codes_total`
    embedding layers and language modeling heads, one for each codebook. This model
    inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1381)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L1381)'
- en: '[PRE25]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`codebook_idx` (`int`) â€” Index of the codebook that will be predicted.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codebook_idx` (`int`) â€” å°†è¢«é¢„æµ‹çš„ç ä¹¦çš„ç´¢å¼•ã€‚'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length, number_of_codebooks)`)
    â€” Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it. Initially, indices of the first two codebooks
    are obtained from the `coarse` sub-model. The rest is predicted recursively by
    attending the previously predicted channels. The model predicts on windows of
    length 1024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, number_of_codebooks)`ï¼‰
    â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚æœ€åˆï¼Œå‰ä¸¤ä¸ªç ä¹¦çš„ç´¢å¼•æ˜¯ä»`coarse`å­æ¨¡å‹ä¸­è·å–çš„ã€‚å…¶ä½™çš„é€šè¿‡é€’å½’é¢„æµ‹å‰é¢é¢„æµ‹çš„é€šé“æ¥é¢„æµ‹ã€‚æ¨¡å‹å¯¹é•¿åº¦ä¸º1024çš„çª—å£è¿›è¡Œé¢„æµ‹ã€‚'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€”
    é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤º`æœªè¢«mask`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤º`masked`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚ '
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`ï¼Œ*å¯é€‰*)
    â€” åœ¨ç¼–ç å™¨ä¸­å°†æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«mask`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” NOT IMPLEMENTED YET.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” å°šæœªå®ç°ã€‚'
- en: '`input_embeds` (`torch.FloatTensor` of shape `(batch_size, input_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. If `past_key_values` is used,
    optionally only the last `input_embeds` have to be input (see `past_key_values`).
    This is useful if you want more control over how to convert `input_ids` indices
    into associated vectors than the modelâ€™s internal embedding lookup matrix.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, input_sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªéœ€è¾“å…¥æœ€åçš„`input_embeds`ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: The [BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)
    forward method, overrides the `__call__` special method.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°ä¸­å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: BarkCausalModel
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkCausalModel
- en: '### `class transformers.BarkCausalModel`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkCausalModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L658)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L658)'
- en: '[PRE26]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#### `forward`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L749)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/modeling_bark.py#L749)'
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’äº†`use_cache`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ä¸¤ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€ä¼ é€’ç»™è¯¥æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€”
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæ ‡è®°æœªè¢«â€œmaskedâ€ã€‚
- en: 0 for tokens that are `masked`.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«â€œmaskedâ€çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä½ç½®IDæ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œmaskedâ€ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«â€œmaskedâ€ã€‚
- en: '`input_embeds` (`torch.FloatTensor` of shape `(batch_size, input_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. Here, due to `Bark` particularities,
    if `past_key_values` is used, `input_embeds` will be ignored and you have to use
    `input_ids`. If `past_key_values` is not used and `use_cache` is set to `True`,
    `input_embeds` is used in priority instead of `input_ids`.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, input_sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚åœ¨è¿™é‡Œï¼Œç”±äº`Bark`çš„ç‰¹æ®Šæ€§ï¼Œå¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå°†å¿½ç•¥`input_embeds`ï¼Œå¿…é¡»ä½¿ç”¨`input_ids`ã€‚å¦‚æœæœªä½¿ç”¨`past_key_values`ä¸”`use_cache`è®¾ç½®ä¸º`True`ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨`input_embeds`è€Œä¸æ˜¯`input_ids`ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: The [BarkCausalModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCausalModel)
    forward method, overrides the `__call__` special method.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[BarkCausalModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCausalModel)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: BarkCoarseConfig
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkCoarseConfig
- en: '### `class transformers.BarkCoarseConfig`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkCoarseConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L164)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L164)'
- en: '[PRE28]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`block_size` (`int`, *optional*, defaults to 1024) â€” The maximum sequence length
    that this model might ever be used with. Typically set this to something large
    just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1024) â€” è¯¥æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512ã€1024æˆ–2048ï¼‰ã€‚'
- en: '`input_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Vocabulary size
    of a Bark sub-model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel).
    Defaults to 10_048 but should be carefully thought with regards to the chosen
    sub-model.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 10_048) â€” Bark å­æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ [BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel)
    æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚é»˜è®¤ä¸º 10_048ï¼Œä½†åº”è°¨æ…è€ƒè™‘æ‰€é€‰å­æ¨¡å‹ã€‚'
- en: '`output_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Output vocabulary
    size of a Bark sub-model. Defines the number of different tokens that can be represented
    by the: `output_ids` when passing forward a [BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel).
    Defaults to 10_048 but should be carefully thought with regards to the chosen
    sub-model.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_vocab_size` (`int`, *optional*, é»˜è®¤ä¸º10_048) â€” Barkå­æ¨¡å‹çš„è¾“å‡ºè¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨å‘å‰ä¼ é€’[BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ï¼š`output_ids`ã€‚é»˜è®¤ä¸º10_048ï¼Œä½†åº”æ ¹æ®æ‰€é€‰å­æ¨¡å‹æ…é‡è€ƒè™‘ã€‚'
- en: '`num_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden layers
    in the given sub-model.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, é»˜è®¤ä¸º12) â€” ç»™å®šå­æ¨¡å‹ä¸­éšè—å±‚çš„æ•°é‡ã€‚'
- en: '`num_heads` (`int`, *optional*, defaults to 12) â€” Number of attention heads
    for each attention layer in the Transformer architecture.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`int`, *optional*, é»˜è®¤ä¸º12) â€” Transformeræ¶æ„ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    â€œintermediateâ€ (often named feed-forward) layer in the architecture.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, é»˜è®¤ä¸º768) â€” æ¶æ„ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`bias` (`bool`, *optional*, defaults to `True`) â€” Whether or not to use bias
    in the linear layers and layer norm layers.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨çº¿æ€§å±‚å’Œå±‚å½’ä¸€åŒ–å±‚ä¸­ä½¿ç”¨åç½®ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚'
- en: This is the configuration class to store the configuration of a [BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel).
    It is used to instantiate the model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the Bark [suno/bark](https://huggingface.co/suno/bark)
    architecture.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[BarkCoarseModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkCoarseModel)çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿä¸Bark
    [suno/bark](https://huggingface.co/suno/bark)æ¶æ„ç±»ä¼¼çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE29]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: BarkFineConfig
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkFineConfig
- en: '### `class transformers.BarkFineConfig`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkFineConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L186)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L186)'
- en: '[PRE30]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`block_size` (`int`, *optional*, defaults to 1024) â€” The maximum sequence length
    that this model might ever be used with. Typically set this to something large
    just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_size` (`int`, *optional*, é»˜è®¤ä¸º1024) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512æˆ–1024æˆ–2048ï¼‰ã€‚'
- en: '`input_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Vocabulary size
    of a Bark sub-model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel).
    Defaults to 10_048 but should be carefully thought with regards to the chosen
    sub-model.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_vocab_size` (`int`, *optional*, é»˜è®¤ä¸º10_048) â€” Barkå­æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ï¼š`inputs_ids`ã€‚é»˜è®¤ä¸º10_048ï¼Œä½†åº”æ ¹æ®æ‰€é€‰å­æ¨¡å‹æ…é‡è€ƒè™‘ã€‚'
- en: '`output_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Output vocabulary
    size of a Bark sub-model. Defines the number of different tokens that can be represented
    by the: `output_ids` when passing forward a [BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel).
    Defaults to 10_048 but should be carefully thought with regards to the chosen
    sub-model.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_vocab_size` (`int`, *optional*, é»˜è®¤ä¸º10_048) â€” Barkå­æ¨¡å‹çš„è¾“å‡ºè¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨å‘å‰ä¼ é€’[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ï¼š`output_ids`ã€‚é»˜è®¤ä¸º10_048ï¼Œä½†åº”æ ¹æ®æ‰€é€‰å­æ¨¡å‹æ…é‡è€ƒè™‘ã€‚'
- en: '`num_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden layers
    in the given sub-model.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, é»˜è®¤ä¸º12) â€” ç»™å®šå­æ¨¡å‹ä¸­éšè—å±‚çš„æ•°é‡ã€‚'
- en: '`num_heads` (`int`, *optional*, defaults to 12) â€” Number of attention heads
    for each attention layer in the Transformer architecture.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`int`, *optional*, é»˜è®¤ä¸º12) â€” Transformeræ¶æ„ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    â€œintermediateâ€ (often named feed-forward) layer in the architecture.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, é»˜è®¤ä¸º768) â€” æ¶æ„ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`bias` (`bool`, *optional*, defaults to `True`) â€” Whether or not to use bias
    in the linear layers and layer norm layers.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨çº¿æ€§å±‚å’Œå±‚å½’ä¸€åŒ–å±‚ä¸­ä½¿ç”¨åç½®ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚'
- en: '`n_codes_total` (`int`, *optional*, defaults to 8) â€” The total number of audio
    codebooks predicted. Used in the fine acoustics sub-model.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_codes_total` (`int`, *optional*, defaults to 8) â€” é¢„æµ‹çš„éŸ³é¢‘ç ä¹¦æ€»æ•°ã€‚ç”¨äºç»†å£°å­¦å­æ¨¡å‹ã€‚'
- en: '`n_codes_given` (`int`, *optional*, defaults to 1) â€” The number of audio codebooks
    predicted in the coarse acoustics sub-model. Used in the acoustics sub-models.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_codes_given` (`int`, *optional*, defaults to 1) â€” ç²—å£°å­¦å­æ¨¡å‹ä¸­é¢„æµ‹çš„éŸ³é¢‘ç ä¹¦æ•°é‡ã€‚ç”¨äºå£°å­¦å­æ¨¡å‹ã€‚'
- en: This is the configuration class to store the configuration of a [BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel).
    It is used to instantiate the model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the Bark [suno/bark](https://huggingface.co/suno/bark)
    architecture.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[BarkFineModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkFineModel)çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºBark
    [suno/bark](https://huggingface.co/suno/bark)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE31]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: BarkSemanticConfig
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BarkSemanticConfig
- en: '### `class transformers.BarkSemanticConfig`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BarkSemanticConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L142)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bark/configuration_bark.py#L142)'
- en: '[PRE32]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`block_size` (`int`, *optional*, defaults to 1024) â€” The maximum sequence length
    that this model might ever be used with. Typically set this to something large
    just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_size` (`int`, *optional*, defaults to 1024) â€” è¯¥æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512æˆ–1024æˆ–2048ï¼‰ã€‚'
- en: '`input_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Vocabulary size
    of a Bark sub-model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel).
    Defaults to 10_048 but should be carefully thought with regards to the chosen
    sub-model.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Barkå­æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)æ—¶ä¼ é€’çš„`inputs_ids`å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚é»˜è®¤ä¸º10_048ï¼Œä½†åº”æ ¹æ®æ‰€é€‰å­æ¨¡å‹æ…é‡è€ƒè™‘ã€‚'
- en: '`output_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Output vocabulary
    size of a Bark sub-model. Defines the number of different tokens that can be represented
    by the: `output_ids` when passing forward a [BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel).
    Defaults to 10_048 but should be carefully thought with regards to the chosen
    sub-model.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_vocab_size` (`int`, *optional*, defaults to 10_048) â€” Barkå­æ¨¡å‹çš„è¾“å‡ºè¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨ä¼ é€’[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)æ—¶`output_ids`å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚é»˜è®¤ä¸º10_048ï¼Œä½†åº”æ ¹æ®æ‰€é€‰å­æ¨¡å‹æ…é‡è€ƒè™‘ã€‚'
- en: '`num_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden layers
    in the given sub-model.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, defaults to 12) â€” ç»™å®šå­æ¨¡å‹ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_heads` (`int`, *optional*, defaults to 12) â€” Number of attention heads
    for each attention layer in the Transformer architecture.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`int`, *optional*, defaults to 12) â€” Transformeræ¶æ„ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    â€œintermediateâ€ (often named feed-forward) layer in the architecture.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) â€” æ¶æ„ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚'
- en: '`bias` (`bool`, *optional*, defaults to `True`) â€” Whether or not to use bias
    in the linear layers and layer norm layers.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨çº¿æ€§å±‚å’Œå±‚å½’ä¸€åŒ–å±‚ä¸­ä½¿ç”¨åç½®ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚'
- en: This is the configuration class to store the configuration of a [BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel).
    It is used to instantiate the model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the Bark [suno/bark](https://huggingface.co/suno/bark)
    architecture.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[BarkSemanticModel](/docs/transformers/v4.37.2/en/model_doc/bark#transformers.BarkSemanticModel)çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºBark
    [suno/bark](https://huggingface.co/suno/bark)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE33]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
