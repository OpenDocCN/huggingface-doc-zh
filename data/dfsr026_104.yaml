- en: LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/loaders/lora](https://huggingface.co/docs/diffusers/api/loaders/lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/diffusers/api/loaders/lora](https://huggingface.co/docs/diffusers/api/loaders/lora)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'LoRA is a fast and lightweight training method that inserts and trains a significantly
    smaller number of parameters instead of all the model parameters. This produces
    a smaller file (~100 MBs) and makes it easier to quickly train a model to learn
    a new concept. LoRA weights are typically loaded into the UNet, text encoder or
    both. There are two classes for loading LoRA weights:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA是一种快速轻量级的训练方法，它插入并训练了数量明显较少的参数，而不是所有模型参数。这会产生一个较小的文件（约100 MB），使得可以更快地训练模型来学习一个新概念。LoRA权重通常加载到UNet、文本编码器或两者中。有两个用于加载LoRA权重的类：
- en: '`LoraLoaderMixin` provides functions for loading and unloading, fusing and
    unfusing, enabling and disabling, and more functions for managing LoRA weights.
    This class can be used with any model.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LoraLoaderMixin`提供了用于加载和卸载、融合和解除融合、启用和禁用以及管理LoRA权重的更多函数的函数。此类可与任何模型一起使用。'
- en: '`StableDiffusionXLLoraLoaderMixin` is a [Stable Diffusion (SDXL)](../../api/pipelines/stable_diffusion/stable_diffusion_xl)
    version of the `LoraLoaderMixin` class for loading and saving LoRA weights. It
    can only be used with the SDXL model.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StableDiffusionXLLoraLoaderMixin`是`LoraLoaderMixin`类的[稳定扩散（SDXL）](../../api/pipelines/stable_diffusion/stable_diffusion_xl)版本，用于加载和保存LoRA权重。它只能与SDXL模型一起使用。'
- en: To learn more about how to load LoRA weights, see the [LoRA](../../using-diffusers/loading_adapters#lora)
    loading guide.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何加载LoRA权重的更多信息，请参阅[LoRA](../../using-diffusers/loading_adapters#lora)加载指南。
- en: LoraLoaderMixin
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoraLoaderMixin
- en: '### `class diffusers.loaders.LoraLoaderMixin`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.loaders.LoraLoaderMixin`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L72)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L72)'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load LoRA layers into [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    and [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 将LoRA层加载到[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)和[`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)中。
- en: '#### `delete_adapters`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `delete_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1284)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1284)'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`Deletes` the LoRA layers of `adapter_name` for the unet and text-encoder(s).
    — adapter_names (`Union[List[str], str]`): The names of the adapter to delete.
    Can be a single string or a list of strings'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`删除`适配器名称的LoRA层，用于UNet和文本编码器。 — adapter_names (`Union[List[str], str]`): 要删除的适配器的名称。可以是单个字符串或字符串列表'
- en: '#### `disable_lora_for_text_encoder`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_lora_for_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1208)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1208)'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_encoder` (`torch.nn.Module`, *optional*) — The text encoder module to
    disable the LoRA layers for. If `None`, it will try to get the `text_encoder`
    attribute.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`torch.nn.Module`, *可选*) — 用于禁用LoRA层的文本编码器模块。如果为`None`，则会尝试获取`text_encoder`属性。'
- en: Disables the LoRA layers for the text encoder.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用文本编码器的LoRA层。
- en: '#### `enable_lora_for_text_encoder`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_lora_for_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1225)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1225)'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_encoder` (`torch.nn.Module`, *optional*) — The text encoder module to
    enable the LoRA layers for. If `None`, it will try to get the `text_encoder` attribute.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`torch.nn.Module`, *可选*) — 用于启用LoRA层的文本编码器模块。如果为`None`，则会尝试获取`text_encoder`属性。'
- en: Enables the LoRA layers for the text encoder.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 启用文本编码器的LoRA层。
- en: '#### `fuse_lora`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `fuse_lora`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1000)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1000)'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`fuse_unet` (`bool`, defaults to `True`) — Whether to fuse the UNet LoRA parameters.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fuse_unet` (`bool`, 默认为 `True`) — 是否融合UNet的LoRA参数。'
- en: '`fuse_text_encoder` (`bool`, defaults to `True`) — Whether to fuse the text
    encoder LoRA parameters. If the text encoder wasn’t monkey-patched with the LoRA
    parameters then it won’t have any effect.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fuse_text_encoder` (`bool`, 默认为 `True`) — 是否融合文本编码器LoRA参数。如果文本编码器未使用LoRA参数进行monkey-patch，则不会产生任何效果。'
- en: '`lora_scale` (`float`, defaults to 1.0) — Controls how much to influence the
    outputs with the LoRA parameters.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, 默认为 1.0) — 控制LoRA参数对输出的影响程度。'
- en: '`safe_fusing` (`bool`, defaults to `False`) — Whether to check fused weights
    for NaN values before fusing and if values are NaN not fusing them.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_fusing` (`bool`, 默认为 `False`) — 在融合之前检查融合权重是否为NaN值，如果值为NaN，则不进行融合。'
- en: '`adapter_names` (`List[str]`, *optional*) — Adapter names to be used for fusing.
    If nothing is passed, all active adapters will be fused.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names` (`List[str]`, *可选*) — 用于融合的适配器名称。如果未传递任何内容，则将融合所有活动适配器。'
- en: Fuses the LoRA parameters into the original parameters of the corresponding
    blocks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将LoRA参数融合到相应块的原始参数中。
- en: This is an experimental API.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实验性API。
- en: 'Example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `get_active_adapters`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_active_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1308)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1308)'
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Gets the list of the current active adapters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 获取当前活动适配器的列表。
- en: 'Example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `get_list_adapters`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_list_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1340)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1340)'
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Gets the current list of all available adapters in the pipeline.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 获取管道中所有可用适配器的当前列表。
- en: '#### `load_lora_into_text_encoder`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_into_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L484)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L484)'
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`state_dict` (`dict`) — A standard state dict containing the lora layer parameters.
    The key should be prefixed with an additional `text_encoder` to distinguish between
    unet lora layers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`dict`) — 包含lora层参数的标准状态字典。键应该以额外的`text_encoder`为前缀，以区分unet lora层。'
- en: '`network_alphas` (`Dict[str, float]`) — See `LoRALinearLayer` for more details.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alphas` (`Dict[str, float]`) — 有关更多详细信息，请参阅`LoRALinearLayer`。'
- en: '`text_encoder` (`CLIPTextModel`) — The text encoder model to load the LoRA
    layers into.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — 要加载LoRA层的文本编码器模型。'
- en: '`prefix` (`str`) — Expected prefix of the `text_encoder` in the `state_dict`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix` (`str`) — `state_dict`中`text_encoder`的预期前缀。'
- en: '`lora_scale` (`float`) — How much to scale the output of the lora linear layer
    before it is added with the output of the regular lora layer.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`) — 在将lora线性层的输出与常规lora层的输出相加之前，需要对其进行缩放的比例。'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) — Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`, *可选*, 如果torch版本 >= 1.9.0则默认为`True`，否则为`False`)
    — 加快模型加载，仅加载预训练权重而不初始化权重。在加载模型时，还尝试不使用超过CPU内存中的1倍模型大小（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。'
- en: '`adapter_name` (`str`, *optional*) — Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器的总数。'
- en: This will load the LoRA layers specified in `state_dict` into `text_encoder`
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把`state_dict`中指定的LoRA层加载到`text_encoder`中
- en: '#### `load_lora_into_transformer`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_into_transformer`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L667)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L667)'
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`state_dict` (`dict`) — A standard state dict containing the lora layer parameters.
    The keys can either be indexed directly into the unet or prefixed with an additional
    `unet` which can be used to distinguish between text encoder lora layers.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`dict`) — 包含lora层参数的标准状态字典。键可以直接索引到unet，也可以以额外的`unet`为前缀，用于区分文本编码器lora层。'
- en: '`network_alphas` (`Dict[str, float]`) — See `LoRALinearLayer` for more details.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alphas` (`Dict[str, float]`) — 有关更多详细信息，请参阅`LoRALinearLayer`。'
- en: '`unet` (`UNet2DConditionModel`) — The UNet model to load the LoRA layers into.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` (`UNet2DConditionModel`) — 要加载LoRA层的UNet模型。'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) — Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`, *可选*, 如果torch版本 >= 1.9.0则默认为`True`，否则为`False`)
    — 加快模型加载，仅加载预训练权重而不初始化权重。在加载模型时，还尝试不使用超过CPU内存中的1倍模型大小（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。'
- en: '`adapter_name` (`str`, *optional*) — Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器的总数。'
- en: This will load the LoRA layers specified in `state_dict` into `transformer`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把`state_dict`中指定的LoRA层加载到`transformer`中。
- en: '#### `load_lora_into_unet`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_into_unet`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L375)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L375)'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`state_dict` (`dict`) — A standard state dict containing the lora layer parameters.
    The keys can either be indexed directly into the unet or prefixed with an additional
    `unet` which can be used to distinguish between text encoder lora layers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`dict`) — 包含lora层参数的标准状态字典。键可以直接索引到unet，也可以以额外的`unet`为前缀，用于区分文本编码器lora层。'
- en: '`network_alphas` (`Dict[str, float]`) — See `LoRALinearLayer` for more details.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alphas` (`Dict[str, float]`) — 有关更多详细信息，请参阅`LoRALinearLayer`。'
- en: '`unet` (`UNet2DConditionModel`) — The UNet model to load the LoRA layers into.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` (`UNet2DConditionModel`) — 要加载LoRA层的UNet模型。'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) — Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`, *可选*, 如果torch版本 >= 1.9.0则默认为`True`，否则为`False`)
    — 加快模型加载，仅加载预训练权重而不初始化权重。在加载模型时，还尝试不使用超过CPU内存中的1倍模型大小（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。'
- en: '`adapter_name` (`str`, *optional*) — Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器的总数。'
- en: This will load the LoRA layers specified in `state_dict` into `unet`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把`state_dict`中指定的LoRA层加载到`unet`中。
- en: '#### `load_lora_weights`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)'
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    — See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path_or_dict` (`str`或`os.PathLike`或`dict`) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。'
- en: '`kwargs` (`dict`, *optional*) — See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`dict`，*可选*) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。'
- en: '`adapter_name` (`str`, *optional*) — Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`，*可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器总数。'
- en: Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into
    `self.unet` and `self.text_encoder`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将`pretrained_model_name_or_path_or_dict`中指定的LoRA权重加载到`self.unet`和`self.text_encoder`中。
- en: All kwargs are forwarded to `self.lora_state_dict`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所有kwargs都将转发到`self.lora_state_dict`。
- en: See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    for more details on how the state dict is loaded.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)以获取有关如何加载状态字典的更多详细信息。
- en: See [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    for more details on how the state dict is loaded into `self.unet`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)以获取有关如何将状态字典加载到`self.unet`中的更多详细信息。
- en: See [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    for more details on how the state dict is loaded into `self.text_encoder`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)以获取有关如何将状态字典加载到`self.text_encoder`中的更多详细信息。
- en: '#### `lora_state_dict`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `lora_state_dict`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L138)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L138)'
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    — Can be either:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path_or_dict` (`str`或`os.PathLike`或`dict`) — 可以是：'
- en: A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained
    model hosted on the Hub.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，预训练模型在Hub上托管的*模型ID*（例如`google/ddpm-celebahq-256`）。
- en: A path to a *directory* (for example `./my_model_directory`) containing the
    model weights saved with [ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained).
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*目录*路径（例如`./my_model_directory`），其中包含使用[ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained)保存的模型权重。
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) — Path to a directory where
    a downloaded pretrained model configuration is cached if the standard cache is
    not used.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir` (`Union[str, os.PathLike]`，*可选*) — 下载预训练模型配置的目录路径，如果未使用标准缓存。'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download` (`bool`，*可选*，默认为`False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to resume downloading the model weights and configuration files. If set to `False`,
    any incompletely downloaded files are deleted.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download` (`bool`，*可选*，默认为`False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。'
- en: '`proxies` (`Dict[str, str]`, *optional*) — A dictionary of proxy servers to
    use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies` (`Dict[str, str]`，*可选*) — 要使用的代理服务器字典，按协议或端点，例如，`{''http'': ''foo.bar:3128'',
    ''http://hostname'': ''foo.bar:4012''}`。代理在每个请求上使用。'
- en: '`local_files_only` (`bool`, *optional*, defaults to `False`) — Whether to only
    load local model weights and configuration files or not. If set to `True`, the
    model won’t be downloaded from the Hub.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only` (`bool`，*可选*，默认为`False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，则不会从Hub下载模型。'
- en: '`token` (`str` or *bool*, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str`或*bool*，*可选*) — 用作远程文件的HTTP令牌的授权。如果为`True`，则使用从`diffusers-cli
    login`生成的令牌（存储在`~/.huggingface`）。'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, a commit id, or any identifier allowed
    by Git.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`，*可选*，默认为`"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。'
- en: '`subfolder` (`str`, *optional*, defaults to `""`) — The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subfolder` (`str`，*可选*，默认为`""`) — 模型文件在Hub或本地较大模型存储库中的子文件夹位置。'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) — Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`，*可选*，如果torch版本>= 1.9.0，默认为`True`，否则为`False`) —
    加速模型加载，仅加载预训练权重而不初始化权重。这还尝试在加载模型时不使用超过1倍模型大小的CPU内存（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。'
- en: '`mirror` (`str`, *optional*) — Mirror source to resolve accessibility issues
    if you’re downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mirror`（`str`，*可选*）— 如果您在中国下载模型时遇到可访问性问题，可以将源镜像到解决问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。'
- en: Return state dict for lora weights and the network alphas.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 返回LoRA权重和网络α的状态字典。
- en: We support loading A1111 formatted LoRA checkpoints in a limited capacity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们支持以有限的能力加载A1111格式的LoRA检查点。
- en: This function is experimental and might change in the future.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数是实验性的，可能会在将来更改。
- en: '#### `save_lora_weights`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)'
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str` or `os.PathLike`) — Directory to save LoRA parameters
    to. Will be created if it doesn’t exist.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`（`str`或`os.PathLike`）— 要保存LoRA参数的目录。如果目录不存在，将会创建。'
- en: '`unet_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    — State dict of the LoRA layers corresponding to the `unet`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet_lora_layers`（`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`）—
    与`unet`对应的LoRA层的状态字典。'
- en: '`text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    — State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly
    pass the text encoder LoRA state dict because it comes from 🤗 Transformers.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_lora_layers`（`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`）—
    与`text_encoder`对应的LoRA层的状态字典。必须显式传递文本编码器LoRA状态字典，因为它来自🤗 Transformers。'
- en: '`is_main_process` (`bool`, *optional*, defaults to `True`) — Whether the process
    calling this is the main process or not. Useful during distributed training and
    you need to call this function on all processes. In this case, set `is_main_process=True`
    only on the main process to avoid race conditions.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_main_process`（`bool`，*可选*，默认为`True`）— 调用此函数的进程是否为主进程。在分布式训练期间非常有用，当您需要在所有进程上调用此函数时。在这种情况下，仅在主进程上设置`is_main_process=True`，以避免竞争条件。'
- en: '`save_function` (`Callable`) — The function to use to save the state dictionary.
    Useful during distributed training when you need to replace `torch.save` with
    another method. Can be configured with the environment variable `DIFFUSERS_SAVE_MODE`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_function`（`Callable`）— 用于保存状态字典的函数。在分布式训练期间需要用另一种方法替换`torch.save`时很有用。可以使用环境变量`DIFFUSERS_SAVE_MODE`进行配置。'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether to
    save the model using `safetensors` or the traditional PyTorch way with `pickle`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`（`bool`，*可选*，默认为`True`）— 是否使用`safetensors`保存模型，还是使用传统的PyTorch方式与`pickle`保存。'
- en: Save the LoRA parameters corresponding to the UNet and text encoder.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 保存与UNet和文本编码器对应的LoRA参数。
- en: '#### `set_adapters_for_text_encoder`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_adapters_for_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1166)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1166)'
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_names` (`List[str]` or `str`) — The names of the adapters to use.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names`（`List[str]`或`str`）— 要使用的适配器的名称。'
- en: '`text_encoder` (`torch.nn.Module`, *optional*) — The text encoder module to
    set the adapter layers for. If `None`, it will try to get the `text_encoder` attribute.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`torch.nn.Module`，*可选*）— 要设置适配器层的文本编码器模块。如果为`None`，将尝试获取`text_encoder`属性。'
- en: '`text_encoder_weights` (`List[float]`, *optional*) — The weights to use for
    the text encoder. If `None`, the weights are set to `1.0` for all the adapters.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_weights`（`List[float]`，*可选*）— 用于文本编码器的权重。如果为`None`，则所有适配器的权重都设置为`1.0`。'
- en: Sets the adapter layers for the text encoder.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为文本编码器设置适配器层。
- en: '#### `set_lora_device`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_lora_device`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1363)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1363)'
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_names` (`List[str]`) — List of adapters to send device to.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names`（`List[str]`）— 要发送到设备的适配器列表。'
- en: '`device` (`Union[torch.device, str, int]`) — Device to send the adapters to.
    Can be either a torch device, a str or an integer.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`Union[torch.device, str, int]`）— 要发送适配器的设备。可以是torch设备、字符串或整数。'
- en: Moves the LoRAs listed in `adapter_names` to a target device. Useful for offloading
    the LoRA to the CPU in case you want to load multiple adapters and free some GPU
    memory.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将`adapter_names`中列出的LoRA移动到目标设备。如果要加载多个适配器并释放一些GPU内存，可以将LoRA卸载到CPU上。
- en: '#### `unfuse_lora`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unfuse_lora`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1106)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1106)'
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`unfuse_unet` (`bool`, defaults to `True`) — Whether to unfuse the UNet LoRA
    parameters.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unfuse_unet`（`bool`，默认为`True`）— 是否解除UNet的LoRA参数。'
- en: '`unfuse_text_encoder` (`bool`, defaults to `True`) — Whether to unfuse the
    text encoder LoRA parameters. If the text encoder wasn’t monkey-patched with the
    LoRA parameters then it won’t have any effect.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unfuse_text_encoder`（`bool`，默认为`True`）— 是否解除文本编码器LoRA参数的融合。如果文本编码器未使用LoRA参数进行monkey-patch，则不会产生任何效果。'
- en: Reverses the effect of [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 撤销[`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora)的效果。
- en: This is an experimental API.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实验性API。
- en: '#### `unload_lora_weights`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unload_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L968)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L968)'
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Unloads the LoRA parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 卸载LoRA参数。
- en: 'Examples:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: StableDiffusionXLLoraLoaderMixin
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StableDiffusionXLLoraLoaderMixin
- en: '### `class diffusers.loaders.StableDiffusionXLLoraLoaderMixin`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.loaders.StableDiffusionXLLoraLoaderMixin`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1404)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1404)'
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This class overrides `LoraLoaderMixin` with LoRA loading/saving code that’s
    specific to SDXL
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用 LoRA 加载/保存代码覆盖了 `LoraLoaderMixin`，该代码特定于 SDXL。
- en: '#### `load_lora_weights`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1408)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1408)'
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    — See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path_or_dict` (`str` 或 `os.PathLike` 或 `dict`) —
    查看 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。'
- en: '`adapter_name` (`str`, *optional*) — Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用 `default_{i}`，其中
    i 是要加载的适配器总数。'
- en: '`kwargs` (`dict`, *optional*) — See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`dict`, *可选*) — 查看 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。'
- en: Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into
    `self.unet` and `self.text_encoder`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 将在 `pretrained_model_name_or_path_or_dict` 中指定的 LoRA 权重加载到 `self.unet` 和 `self.text_encoder`
    中。
- en: All kwargs are forwarded to `self.lora_state_dict`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 kwargs 都会传递给 `self.lora_state_dict`。
- en: See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    for more details on how the state dict is loaded.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    以了解如何加载状态字典的更多细节。
- en: See [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    for more details on how the state dict is loaded into `self.unet`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    以了解如何将状态字典加载到 `self.unet` 中的更多细节。
- en: See [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    for more details on how the state dict is loaded into `self.text_encoder`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    以了解如何将状态字典加载到 `self.text_encoder` 中的更多细节。
