- en: LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/loaders/lora](https://huggingface.co/docs/diffusers/api/loaders/lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/api/loaders/lora](https://huggingface.co/docs/diffusers/api/loaders/lora)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'LoRA is a fast and lightweight training method that inserts and trains a significantly
    smaller number of parameters instead of all the model parameters. This produces
    a smaller file (~100 MBs) and makes it easier to quickly train a model to learn
    a new concept. LoRA weights are typically loaded into the UNet, text encoder or
    both. There are two classes for loading LoRA weights:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LoRAæ˜¯ä¸€ç§å¿«é€Ÿè½»é‡çº§çš„è®­ç»ƒæ–¹æ³•ï¼Œå®ƒæ’å…¥å¹¶è®­ç»ƒäº†æ•°é‡æ˜æ˜¾è¾ƒå°‘çš„å‚æ•°ï¼Œè€Œä¸æ˜¯æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚è¿™ä¼šäº§ç”Ÿä¸€ä¸ªè¾ƒå°çš„æ–‡ä»¶ï¼ˆçº¦100 MBï¼‰ï¼Œä½¿å¾—å¯ä»¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹æ¥å­¦ä¹ ä¸€ä¸ªæ–°æ¦‚å¿µã€‚LoRAæƒé‡é€šå¸¸åŠ è½½åˆ°UNetã€æ–‡æœ¬ç¼–ç å™¨æˆ–ä¸¤è€…ä¸­ã€‚æœ‰ä¸¤ä¸ªç”¨äºåŠ è½½LoRAæƒé‡çš„ç±»ï¼š
- en: '`LoraLoaderMixin` provides functions for loading and unloading, fusing and
    unfusing, enabling and disabling, and more functions for managing LoRA weights.
    This class can be used with any model.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LoraLoaderMixin`æä¾›äº†ç”¨äºåŠ è½½å’Œå¸è½½ã€èåˆå’Œè§£é™¤èåˆã€å¯ç”¨å’Œç¦ç”¨ä»¥åŠç®¡ç†LoRAæƒé‡çš„æ›´å¤šå‡½æ•°çš„å‡½æ•°ã€‚æ­¤ç±»å¯ä¸ä»»ä½•æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚'
- en: '`StableDiffusionXLLoraLoaderMixin` is a [Stable Diffusion (SDXL)](../../api/pipelines/stable_diffusion/stable_diffusion_xl)
    version of the `LoraLoaderMixin` class for loading and saving LoRA weights. It
    can only be used with the SDXL model.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StableDiffusionXLLoraLoaderMixin`æ˜¯`LoraLoaderMixin`ç±»çš„[ç¨³å®šæ‰©æ•£ï¼ˆSDXLï¼‰](../../api/pipelines/stable_diffusion/stable_diffusion_xl)ç‰ˆæœ¬ï¼Œç”¨äºåŠ è½½å’Œä¿å­˜LoRAæƒé‡ã€‚å®ƒåªèƒ½ä¸SDXLæ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚'
- en: To learn more about how to load LoRA weights, see the [LoRA](../../using-diffusers/loading_adapters#lora)
    loading guide.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£å¦‚ä½•åŠ è½½LoRAæƒé‡çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[LoRA](../../using-diffusers/loading_adapters#lora)åŠ è½½æŒ‡å—ã€‚
- en: LoraLoaderMixin
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoraLoaderMixin
- en: '### `class diffusers.loaders.LoraLoaderMixin`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.loaders.LoraLoaderMixin`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L72)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L72)'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load LoRA layers into [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    and [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å°†LoRAå±‚åŠ è½½åˆ°[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)å’Œ[`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)ä¸­ã€‚
- en: '#### `delete_adapters`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `delete_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1284)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1284)'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`Deletes` the LoRA layers of `adapter_name` for the unet and text-encoder(s).
    â€” adapter_names (`Union[List[str], str]`): The names of the adapter to delete.
    Can be a single string or a list of strings'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`åˆ é™¤`é€‚é…å™¨åç§°çš„LoRAå±‚ï¼Œç”¨äºUNetå’Œæ–‡æœ¬ç¼–ç å™¨ã€‚ â€” adapter_names (`Union[List[str], str]`): è¦åˆ é™¤çš„é€‚é…å™¨çš„åç§°ã€‚å¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨'
- en: '#### `disable_lora_for_text_encoder`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_lora_for_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1208)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1208)'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`torch.nn.Module`, *optional*) â€” The text encoder module to
    disable the LoRA layers for. If `None`, it will try to get the `text_encoder`
    attribute.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`torch.nn.Module`, *å¯é€‰*) â€” ç”¨äºç¦ç”¨LoRAå±‚çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å—ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™ä¼šå°è¯•è·å–`text_encoder`å±æ€§ã€‚'
- en: Disables the LoRA layers for the text encoder.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦ç”¨æ–‡æœ¬ç¼–ç å™¨çš„LoRAå±‚ã€‚
- en: '#### `enable_lora_for_text_encoder`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_lora_for_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1225)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1225)'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`torch.nn.Module`, *optional*) â€” The text encoder module to
    enable the LoRA layers for. If `None`, it will try to get the `text_encoder` attribute.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`torch.nn.Module`, *å¯é€‰*) â€” ç”¨äºå¯ç”¨LoRAå±‚çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å—ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™ä¼šå°è¯•è·å–`text_encoder`å±æ€§ã€‚'
- en: Enables the LoRA layers for the text encoder.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨æ–‡æœ¬ç¼–ç å™¨çš„LoRAå±‚ã€‚
- en: '#### `fuse_lora`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `fuse_lora`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1000)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1000)'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`fuse_unet` (`bool`, defaults to `True`) â€” Whether to fuse the UNet LoRA parameters.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fuse_unet` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦èåˆUNetçš„LoRAå‚æ•°ã€‚'
- en: '`fuse_text_encoder` (`bool`, defaults to `True`) â€” Whether to fuse the text
    encoder LoRA parameters. If the text encoder wasnâ€™t monkey-patched with the LoRA
    parameters then it wonâ€™t have any effect.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fuse_text_encoder` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦èåˆæ–‡æœ¬ç¼–ç å™¨LoRAå‚æ•°ã€‚å¦‚æœæ–‡æœ¬ç¼–ç å™¨æœªä½¿ç”¨LoRAå‚æ•°è¿›è¡Œmonkey-patchï¼Œåˆ™ä¸ä¼šäº§ç”Ÿä»»ä½•æ•ˆæœã€‚'
- en: '`lora_scale` (`float`, defaults to 1.0) â€” Controls how much to influence the
    outputs with the LoRA parameters.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, é»˜è®¤ä¸º 1.0) â€” æ§åˆ¶LoRAå‚æ•°å¯¹è¾“å‡ºçš„å½±å“ç¨‹åº¦ã€‚'
- en: '`safe_fusing` (`bool`, defaults to `False`) â€” Whether to check fused weights
    for NaN values before fusing and if values are NaN not fusing them.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_fusing` (`bool`, é»˜è®¤ä¸º `False`) â€” åœ¨èåˆä¹‹å‰æ£€æŸ¥èåˆæƒé‡æ˜¯å¦ä¸ºNaNå€¼ï¼Œå¦‚æœå€¼ä¸ºNaNï¼Œåˆ™ä¸è¿›è¡Œèåˆã€‚'
- en: '`adapter_names` (`List[str]`, *optional*) â€” Adapter names to be used for fusing.
    If nothing is passed, all active adapters will be fused.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names` (`List[str]`, *å¯é€‰*) â€” ç”¨äºèåˆçš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªä¼ é€’ä»»ä½•å†…å®¹ï¼Œåˆ™å°†èåˆæ‰€æœ‰æ´»åŠ¨é€‚é…å™¨ã€‚'
- en: Fuses the LoRA parameters into the original parameters of the corresponding
    blocks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å°†LoRAå‚æ•°èåˆåˆ°ç›¸åº”å—çš„åŸå§‹å‚æ•°ä¸­ã€‚
- en: This is an experimental API.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§APIã€‚
- en: 'Example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `get_active_adapters`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_active_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1308)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1308)'
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Gets the list of the current active adapters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å½“å‰æ´»åŠ¨é€‚é…å™¨çš„åˆ—è¡¨ã€‚
- en: 'Example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `get_list_adapters`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_list_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1340)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1340)'
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Gets the current list of all available adapters in the pipeline.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–ç®¡é“ä¸­æ‰€æœ‰å¯ç”¨é€‚é…å™¨çš„å½“å‰åˆ—è¡¨ã€‚
- en: '#### `load_lora_into_text_encoder`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_into_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L484)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L484)'
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`state_dict` (`dict`) â€” A standard state dict containing the lora layer parameters.
    The key should be prefixed with an additional `text_encoder` to distinguish between
    unet lora layers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`dict`) â€” åŒ…å«loraå±‚å‚æ•°çš„æ ‡å‡†çŠ¶æ€å­—å…¸ã€‚é”®åº”è¯¥ä»¥é¢å¤–çš„`text_encoder`ä¸ºå‰ç¼€ï¼Œä»¥åŒºåˆ†unet loraå±‚ã€‚'
- en: '`network_alphas` (`Dict[str, float]`) â€” See `LoRALinearLayer` for more details.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alphas` (`Dict[str, float]`) â€” æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`LoRALinearLayer`ã€‚'
- en: '`text_encoder` (`CLIPTextModel`) â€” The text encoder model to load the LoRA
    layers into.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) â€” è¦åŠ è½½LoRAå±‚çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ã€‚'
- en: '`prefix` (`str`) â€” Expected prefix of the `text_encoder` in the `state_dict`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix` (`str`) â€” `state_dict`ä¸­`text_encoder`çš„é¢„æœŸå‰ç¼€ã€‚'
- en: '`lora_scale` (`float`) â€” How much to scale the output of the lora linear layer
    before it is added with the output of the regular lora layer.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`) â€” åœ¨å°†loraçº¿æ€§å±‚çš„è¾“å‡ºä¸å¸¸è§„loraå±‚çš„è¾“å‡ºç›¸åŠ ä¹‹å‰ï¼Œéœ€è¦å¯¹å…¶è¿›è¡Œç¼©æ”¾çš„æ¯”ä¾‹ã€‚'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`, *å¯é€‰*, å¦‚æœtorchç‰ˆæœ¬ >= 1.9.0åˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`)
    â€” åŠ å¿«æ¨¡å‹åŠ è½½ï¼Œä»…åŠ è½½é¢„è®­ç»ƒæƒé‡è€Œä¸åˆå§‹åŒ–æƒé‡ã€‚åœ¨åŠ è½½æ¨¡å‹æ—¶ï¼Œè¿˜å°è¯•ä¸ä½¿ç”¨è¶…è¿‡CPUå†…å­˜ä¸­çš„1å€æ¨¡å‹å¤§å°ï¼ˆåŒ…æ‹¬å³°å€¼å†…å­˜ï¼‰ã€‚ä»…æ”¯æŒPyTorch >= 1.9.0ã€‚å¦‚æœæ‚¨ä½¿ç”¨è¾ƒæ—§ç‰ˆæœ¬çš„PyTorchï¼Œå°†æ­¤å‚æ•°è®¾ç½®ä¸º`True`å°†å¼•å‘é”™è¯¯ã€‚'
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *å¯é€‰*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`default_{i}`ï¼Œå…¶ä¸­iæ˜¯æ­£åœ¨åŠ è½½çš„é€‚é…å™¨çš„æ€»æ•°ã€‚'
- en: This will load the LoRA layers specified in `state_dict` into `text_encoder`
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æŠŠ`state_dict`ä¸­æŒ‡å®šçš„LoRAå±‚åŠ è½½åˆ°`text_encoder`ä¸­
- en: '#### `load_lora_into_transformer`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_into_transformer`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L667)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L667)'
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`state_dict` (`dict`) â€” A standard state dict containing the lora layer parameters.
    The keys can either be indexed directly into the unet or prefixed with an additional
    `unet` which can be used to distinguish between text encoder lora layers.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`dict`) â€” åŒ…å«loraå±‚å‚æ•°çš„æ ‡å‡†çŠ¶æ€å­—å…¸ã€‚é”®å¯ä»¥ç›´æ¥ç´¢å¼•åˆ°unetï¼Œä¹Ÿå¯ä»¥ä»¥é¢å¤–çš„`unet`ä¸ºå‰ç¼€ï¼Œç”¨äºåŒºåˆ†æ–‡æœ¬ç¼–ç å™¨loraå±‚ã€‚'
- en: '`network_alphas` (`Dict[str, float]`) â€” See `LoRALinearLayer` for more details.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alphas` (`Dict[str, float]`) â€” æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`LoRALinearLayer`ã€‚'
- en: '`unet` (`UNet2DConditionModel`) â€” The UNet model to load the LoRA layers into.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` (`UNet2DConditionModel`) â€” è¦åŠ è½½LoRAå±‚çš„UNetæ¨¡å‹ã€‚'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`, *å¯é€‰*, å¦‚æœtorchç‰ˆæœ¬ >= 1.9.0åˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`)
    â€” åŠ å¿«æ¨¡å‹åŠ è½½ï¼Œä»…åŠ è½½é¢„è®­ç»ƒæƒé‡è€Œä¸åˆå§‹åŒ–æƒé‡ã€‚åœ¨åŠ è½½æ¨¡å‹æ—¶ï¼Œè¿˜å°è¯•ä¸ä½¿ç”¨è¶…è¿‡CPUå†…å­˜ä¸­çš„1å€æ¨¡å‹å¤§å°ï¼ˆåŒ…æ‹¬å³°å€¼å†…å­˜ï¼‰ã€‚ä»…æ”¯æŒPyTorch >= 1.9.0ã€‚å¦‚æœæ‚¨ä½¿ç”¨è¾ƒæ—§ç‰ˆæœ¬çš„PyTorchï¼Œå°†æ­¤å‚æ•°è®¾ç½®ä¸º`True`å°†å¼•å‘é”™è¯¯ã€‚'
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *å¯é€‰*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`default_{i}`ï¼Œå…¶ä¸­iæ˜¯æ­£åœ¨åŠ è½½çš„é€‚é…å™¨çš„æ€»æ•°ã€‚'
- en: This will load the LoRA layers specified in `state_dict` into `transformer`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æŠŠ`state_dict`ä¸­æŒ‡å®šçš„LoRAå±‚åŠ è½½åˆ°`transformer`ä¸­ã€‚
- en: '#### `load_lora_into_unet`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_into_unet`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L375)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L375)'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`state_dict` (`dict`) â€” A standard state dict containing the lora layer parameters.
    The keys can either be indexed directly into the unet or prefixed with an additional
    `unet` which can be used to distinguish between text encoder lora layers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`dict`) â€” åŒ…å«loraå±‚å‚æ•°çš„æ ‡å‡†çŠ¶æ€å­—å…¸ã€‚é”®å¯ä»¥ç›´æ¥ç´¢å¼•åˆ°unetï¼Œä¹Ÿå¯ä»¥ä»¥é¢å¤–çš„`unet`ä¸ºå‰ç¼€ï¼Œç”¨äºåŒºåˆ†æ–‡æœ¬ç¼–ç å™¨loraå±‚ã€‚'
- en: '`network_alphas` (`Dict[str, float]`) â€” See `LoRALinearLayer` for more details.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alphas` (`Dict[str, float]`) â€” æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`LoRALinearLayer`ã€‚'
- en: '`unet` (`UNet2DConditionModel`) â€” The UNet model to load the LoRA layers into.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` (`UNet2DConditionModel`) â€” è¦åŠ è½½LoRAå±‚çš„UNetæ¨¡å‹ã€‚'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`, *å¯é€‰*, å¦‚æœtorchç‰ˆæœ¬ >= 1.9.0åˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`)
    â€” åŠ å¿«æ¨¡å‹åŠ è½½ï¼Œä»…åŠ è½½é¢„è®­ç»ƒæƒé‡è€Œä¸åˆå§‹åŒ–æƒé‡ã€‚åœ¨åŠ è½½æ¨¡å‹æ—¶ï¼Œè¿˜å°è¯•ä¸ä½¿ç”¨è¶…è¿‡CPUå†…å­˜ä¸­çš„1å€æ¨¡å‹å¤§å°ï¼ˆåŒ…æ‹¬å³°å€¼å†…å­˜ï¼‰ã€‚ä»…æ”¯æŒPyTorch >= 1.9.0ã€‚å¦‚æœæ‚¨ä½¿ç”¨è¾ƒæ—§ç‰ˆæœ¬çš„PyTorchï¼Œå°†æ­¤å‚æ•°è®¾ç½®ä¸º`True`å°†å¼•å‘é”™è¯¯ã€‚'
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *å¯é€‰*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`default_{i}`ï¼Œå…¶ä¸­iæ˜¯æ­£åœ¨åŠ è½½çš„é€‚é…å™¨çš„æ€»æ•°ã€‚'
- en: This will load the LoRA layers specified in `state_dict` into `unet`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æŠŠ`state_dict`ä¸­æŒ‡å®šçš„LoRAå±‚åŠ è½½åˆ°`unet`ä¸­ã€‚
- en: '#### `load_lora_weights`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)'
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path_or_dict` (`str`æˆ–`os.PathLike`æˆ–`dict`) â€” æŸ¥çœ‹[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚'
- en: '`kwargs` (`dict`, *optional*) â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`dict`ï¼Œ*å¯é€‰*) â€” æŸ¥çœ‹[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚'
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`ï¼Œ*å¯é€‰*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`default_{i}`ï¼Œå…¶ä¸­iæ˜¯æ­£åœ¨åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚'
- en: Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into
    `self.unet` and `self.text_encoder`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`pretrained_model_name_or_path_or_dict`ä¸­æŒ‡å®šçš„LoRAæƒé‡åŠ è½½åˆ°`self.unet`å’Œ`self.text_encoder`ä¸­ã€‚
- en: All kwargs are forwarded to `self.lora_state_dict`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰kwargséƒ½å°†è½¬å‘åˆ°`self.lora_state_dict`ã€‚
- en: See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    for more details on how the state dict is loaded.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ä»¥è·å–æœ‰å…³å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: See [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    for more details on how the state dict is loaded into `self.unet`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)ä»¥è·å–æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°`self.unet`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: See [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    for more details on how the state dict is loaded into `self.text_encoder`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)ä»¥è·å–æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°`self.text_encoder`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: '#### `lora_state_dict`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `lora_state_dict`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L138)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L138)'
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    â€” Can be either:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path_or_dict` (`str`æˆ–`os.PathLike`æˆ–`dict`) â€” å¯ä»¥æ˜¯ï¼š'
- en: A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained
    model hosted on the Hub.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨Hubä¸Šæ‰˜ç®¡çš„*æ¨¡å‹ID*ï¼ˆä¾‹å¦‚`google/ddpm-celebahq-256`ï¼‰ã€‚
- en: A path to a *directory* (for example `./my_model_directory`) containing the
    model weights saved with [ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained).
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª*ç›®å½•*è·¯å¾„ï¼ˆä¾‹å¦‚`./my_model_directory`ï¼‰ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨[ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained)ä¿å­˜çš„æ¨¡å‹æƒé‡ã€‚
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[torchçŠ¶æ€å­—å…¸](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)ã€‚
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” Path to a directory where
    a downloaded pretrained model configuration is cached if the standard cache is
    not used.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir` (`Union[str, os.PathLike]`ï¼Œ*å¯é€‰*) â€” ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹é…ç½®çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to resume downloading the model weights and configuration files. If set to `False`,
    any incompletely downloaded files are deleted.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ¢å¤ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œåˆ™åˆ é™¤ä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶ã€‚'
- en: '`proxies` (`Dict[str, str]`, *optional*) â€” A dictionary of proxy servers to
    use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies` (`Dict[str, str]`ï¼Œ*å¯é€‰*) â€” è¦ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼ŒæŒ‰åè®®æˆ–ç«¯ç‚¹ï¼Œä¾‹å¦‚ï¼Œ`{''http'': ''foo.bar:3128'',
    ''http://hostname'': ''foo.bar:4012''}`ã€‚ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸Šä½¿ç”¨ã€‚'
- en: '`local_files_only` (`bool`, *optional*, defaults to `False`) â€” Whether to only
    load local model weights and configuration files or not. If set to `True`, the
    model wonâ€™t be downloaded from the Hub.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™ä¸ä¼šä»Hubä¸‹è½½æ¨¡å‹ã€‚'
- en: '`token` (`str` or *bool*, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str`æˆ–*bool*ï¼Œ*å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTPä»¤ç‰Œçš„æˆæƒã€‚å¦‚æœä¸º`True`ï¼Œåˆ™ä½¿ç”¨ä»`diffusers-cli
    login`ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ï¼‰ã€‚'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) â€” The specific model version
    to use. It can be a branch name, a tag name, a commit id, or any identifier allowed
    by Git.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤IDæˆ–Gitå…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚'
- en: '`subfolder` (`str`, *optional*, defaults to `""`) â€” The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subfolder` (`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`""`) â€” æ¨¡å‹æ–‡ä»¶åœ¨Hubæˆ–æœ¬åœ°è¾ƒå¤§æ¨¡å‹å­˜å‚¨åº“ä¸­çš„å­æ–‡ä»¶å¤¹ä½ç½®ã€‚'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`ï¼Œ*å¯é€‰*ï¼Œå¦‚æœtorchç‰ˆæœ¬>= 1.9.0ï¼Œé»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`) â€”
    åŠ é€Ÿæ¨¡å‹åŠ è½½ï¼Œä»…åŠ è½½é¢„è®­ç»ƒæƒé‡è€Œä¸åˆå§‹åŒ–æƒé‡ã€‚è¿™è¿˜å°è¯•åœ¨åŠ è½½æ¨¡å‹æ—¶ä¸ä½¿ç”¨è¶…è¿‡1å€æ¨¡å‹å¤§å°çš„CPUå†…å­˜ï¼ˆåŒ…æ‹¬å³°å€¼å†…å­˜ï¼‰ã€‚ä»…æ”¯æŒPyTorch >= 1.9.0ã€‚å¦‚æœæ‚¨ä½¿ç”¨è¾ƒæ—§ç‰ˆæœ¬çš„PyTorchï¼Œå°†æ­¤å‚æ•°è®¾ç½®ä¸º`True`å°†å¼•å‘é”™è¯¯ã€‚'
- en: '`mirror` (`str`, *optional*) â€” Mirror source to resolve accessibility issues
    if youâ€™re downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mirror`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœæ‚¨åœ¨ä¸­å›½ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°å¯è®¿é—®æ€§é—®é¢˜ï¼Œå¯ä»¥å°†æºé•œåƒåˆ°è§£å†³é—®é¢˜ã€‚æˆ‘ä»¬ä¸ä¿è¯æºçš„åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ï¼Œæ‚¨åº”å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: Return state dict for lora weights and the network alphas.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›LoRAæƒé‡å’Œç½‘ç»œÎ±çš„çŠ¶æ€å­—å…¸ã€‚
- en: We support loading A1111 formatted LoRA checkpoints in a limited capacity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ”¯æŒä»¥æœ‰é™çš„èƒ½åŠ›åŠ è½½A1111æ ¼å¼çš„LoRAæ£€æŸ¥ç‚¹ã€‚
- en: This function is experimental and might change in the future.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‡½æ•°æ˜¯å®éªŒæ€§çš„ï¼Œå¯èƒ½ä¼šåœ¨å°†æ¥æ›´æ”¹ã€‚
- en: '#### `save_lora_weights`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)'
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory to save LoRA parameters
    to. Will be created if it doesnâ€™t exist.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” è¦ä¿å­˜LoRAå‚æ•°çš„ç›®å½•ã€‚å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå°†ä¼šåˆ›å»ºã€‚'
- en: '`unet_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    â€” State dict of the LoRA layers corresponding to the `unet`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet_lora_layers`ï¼ˆ`Dict[str, torch.nn.Module]`æˆ–`Dict[str, torch.Tensor]`ï¼‰â€”
    ä¸`unet`å¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸ã€‚'
- en: '`text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    â€” State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly
    pass the text encoder LoRA state dict because it comes from ğŸ¤— Transformers.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_lora_layers`ï¼ˆ`Dict[str, torch.nn.Module]`æˆ–`Dict[str, torch.Tensor]`ï¼‰â€”
    ä¸`text_encoder`å¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸ã€‚å¿…é¡»æ˜¾å¼ä¼ é€’æ–‡æœ¬ç¼–ç å™¨LoRAçŠ¶æ€å­—å…¸ï¼Œå› ä¸ºå®ƒæ¥è‡ªğŸ¤— Transformersã€‚'
- en: '`is_main_process` (`bool`, *optional*, defaults to `True`) â€” Whether the process
    calling this is the main process or not. Useful during distributed training and
    you need to call this function on all processes. In this case, set `is_main_process=True`
    only on the main process to avoid race conditions.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_main_process`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´éå¸¸æœ‰ç”¨ï¼Œå½“æ‚¨éœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨æ­¤å‡½æ•°æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè®¾ç½®`is_main_process=True`ï¼Œä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚'
- en: '`save_function` (`Callable`) â€” The function to use to save the state dictionary.
    Useful during distributed training when you need to replace `torch.save` with
    another method. Can be configured with the environment variable `DIFFUSERS_SAVE_MODE`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_function`ï¼ˆ`Callable`ï¼‰â€” ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´éœ€è¦ç”¨å¦ä¸€ç§æ–¹æ³•æ›¿æ¢`torch.save`æ—¶å¾ˆæœ‰ç”¨ã€‚å¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡`DIFFUSERS_SAVE_MODE`è¿›è¡Œé…ç½®ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether to
    save the model using `safetensors` or the traditional PyTorch way with `pickle`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦ä½¿ç”¨`safetensors`ä¿å­˜æ¨¡å‹ï¼Œè¿˜æ˜¯ä½¿ç”¨ä¼ ç»Ÿçš„PyTorchæ–¹å¼ä¸`pickle`ä¿å­˜ã€‚'
- en: Save the LoRA parameters corresponding to the UNet and text encoder.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜ä¸UNetå’Œæ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„LoRAå‚æ•°ã€‚
- en: '#### `set_adapters_for_text_encoder`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_adapters_for_text_encoder`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1166)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1166)'
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`adapter_names` (`List[str]` or `str`) â€” The names of the adapters to use.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names`ï¼ˆ`List[str]`æˆ–`str`ï¼‰â€” è¦ä½¿ç”¨çš„é€‚é…å™¨çš„åç§°ã€‚'
- en: '`text_encoder` (`torch.nn.Module`, *optional*) â€” The text encoder module to
    set the adapter layers for. If `None`, it will try to get the `text_encoder` attribute.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`ï¼ˆ`torch.nn.Module`ï¼Œ*å¯é€‰*ï¼‰â€” è¦è®¾ç½®é€‚é…å™¨å±‚çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å—ã€‚å¦‚æœä¸º`None`ï¼Œå°†å°è¯•è·å–`text_encoder`å±æ€§ã€‚'
- en: '`text_encoder_weights` (`List[float]`, *optional*) â€” The weights to use for
    the text encoder. If `None`, the weights are set to `1.0` for all the adapters.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_weights`ï¼ˆ`List[float]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æƒé‡ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™æ‰€æœ‰é€‚é…å™¨çš„æƒé‡éƒ½è®¾ç½®ä¸º`1.0`ã€‚'
- en: Sets the adapter layers for the text encoder.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ–‡æœ¬ç¼–ç å™¨è®¾ç½®é€‚é…å™¨å±‚ã€‚
- en: '#### `set_lora_device`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_lora_device`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1363)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1363)'
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`adapter_names` (`List[str]`) â€” List of adapters to send device to.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names`ï¼ˆ`List[str]`ï¼‰â€” è¦å‘é€åˆ°è®¾å¤‡çš„é€‚é…å™¨åˆ—è¡¨ã€‚'
- en: '`device` (`Union[torch.device, str, int]`) â€” Device to send the adapters to.
    Can be either a torch device, a str or an integer.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`ï¼ˆ`Union[torch.device, str, int]`ï¼‰â€” è¦å‘é€é€‚é…å™¨çš„è®¾å¤‡ã€‚å¯ä»¥æ˜¯torchè®¾å¤‡ã€å­—ç¬¦ä¸²æˆ–æ•´æ•°ã€‚'
- en: Moves the LoRAs listed in `adapter_names` to a target device. Useful for offloading
    the LoRA to the CPU in case you want to load multiple adapters and free some GPU
    memory.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`adapter_names`ä¸­åˆ—å‡ºçš„LoRAç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ã€‚å¦‚æœè¦åŠ è½½å¤šä¸ªé€‚é…å™¨å¹¶é‡Šæ”¾ä¸€äº›GPUå†…å­˜ï¼Œå¯ä»¥å°†LoRAå¸è½½åˆ°CPUä¸Šã€‚
- en: '#### `unfuse_lora`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unfuse_lora`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1106)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1106)'
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`unfuse_unet` (`bool`, defaults to `True`) â€” Whether to unfuse the UNet LoRA
    parameters.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unfuse_unet`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è§£é™¤UNetçš„LoRAå‚æ•°ã€‚'
- en: '`unfuse_text_encoder` (`bool`, defaults to `True`) â€” Whether to unfuse the
    text encoder LoRA parameters. If the text encoder wasnâ€™t monkey-patched with the
    LoRA parameters then it wonâ€™t have any effect.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unfuse_text_encoder`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è§£é™¤æ–‡æœ¬ç¼–ç å™¨LoRAå‚æ•°çš„èåˆã€‚å¦‚æœæ–‡æœ¬ç¼–ç å™¨æœªä½¿ç”¨LoRAå‚æ•°è¿›è¡Œmonkey-patchï¼Œåˆ™ä¸ä¼šäº§ç”Ÿä»»ä½•æ•ˆæœã€‚'
- en: Reverses the effect of [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æ’¤é”€[`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora)çš„æ•ˆæœã€‚
- en: This is an experimental API.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§APIã€‚
- en: '#### `unload_lora_weights`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unload_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L968)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L968)'
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Unloads the LoRA parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å¸è½½LoRAå‚æ•°ã€‚
- en: 'Examples:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: StableDiffusionXLLoraLoaderMixin
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StableDiffusionXLLoraLoaderMixin
- en: '### `class diffusers.loaders.StableDiffusionXLLoraLoaderMixin`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.loaders.StableDiffusionXLLoraLoaderMixin`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1404)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1404)'
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This class overrides `LoraLoaderMixin` with LoRA loading/saving code thatâ€™s
    specific to SDXL
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç±»ä½¿ç”¨ LoRA åŠ è½½/ä¿å­˜ä»£ç è¦†ç›–äº† `LoraLoaderMixin`ï¼Œè¯¥ä»£ç ç‰¹å®šäº SDXLã€‚
- en: '#### `load_lora_weights`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_lora_weights`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1408)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1408)'
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path_or_dict` (`str` æˆ– `os.PathLike` æˆ– `dict`) â€”
    æŸ¥çœ‹ [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚'
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`, *å¯é€‰*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨ `default_{i}`ï¼Œå…¶ä¸­
    i æ˜¯è¦åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚'
- en: '`kwargs` (`dict`, *optional*) â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`dict`, *å¯é€‰*) â€” æŸ¥çœ‹ [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚'
- en: Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into
    `self.unet` and `self.text_encoder`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åœ¨ `pretrained_model_name_or_path_or_dict` ä¸­æŒ‡å®šçš„ LoRA æƒé‡åŠ è½½åˆ° `self.unet` å’Œ `self.text_encoder`
    ä¸­ã€‚
- en: All kwargs are forwarded to `self.lora_state_dict`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ kwargs éƒ½ä¼šä¼ é€’ç»™ `self.lora_state_dict`ã€‚
- en: See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    for more details on how the state dict is loaded.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    ä»¥äº†è§£å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸çš„æ›´å¤šç»†èŠ‚ã€‚
- en: See [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    for more details on how the state dict is loaded into `self.unet`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    ä»¥äº†è§£å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ° `self.unet` ä¸­çš„æ›´å¤šç»†èŠ‚ã€‚
- en: See [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    for more details on how the state dict is loaded into `self.text_encoder`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    ä»¥äº†è§£å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ° `self.text_encoder` ä¸­çš„æ›´å¤šç»†èŠ‚ã€‚
