- en: Load adapters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ è½½é€‚é…å™¨
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters](https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters](https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are several [training](../training/overview) techniques for personalizing
    diffusion models to generate images of a specific subject or images in certain
    styles. Each of these training methods produces a different type of adapter. Some
    of the adapters generate an entirely new model, while other adapters only modify
    a smaller set of embeddings or weights. This means the loading process for each
    adapter is also different.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§[è®­ç»ƒ](../training/overview)æŠ€æœ¯å¯ç”¨äºä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆç‰¹å®šä¸»é¢˜çš„å›¾åƒæˆ–ç‰¹å®šé£æ ¼çš„å›¾åƒã€‚æ¯ç§è®­ç»ƒæ–¹æ³•éƒ½ä¼šäº§ç”Ÿä¸åŒç±»å‹çš„é€‚é…å™¨ã€‚ä¸€äº›é€‚é…å™¨ä¼šç”Ÿæˆå…¨æ–°çš„æ¨¡å‹ï¼Œè€Œå…¶ä»–é€‚é…å™¨åªä¼šä¿®æ”¹ä¸€å°éƒ¨åˆ†åµŒå…¥æˆ–æƒé‡ã€‚è¿™æ„å‘³ç€æ¯ä¸ªé€‚é…å™¨çš„åŠ è½½è¿‡ç¨‹ä¹Ÿæ˜¯ä¸åŒçš„ã€‚
- en: This guide will show you how to load DreamBooth, textual inversion, and LoRA
    weights.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è½½DreamBoothã€æ–‡æœ¬åè½¬å’ŒLoRAæƒé‡ã€‚
- en: Feel free to browse the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer),
    [LoRA the Explorer](https://huggingface.co/spaces/multimodalart/LoraTheExplorer),
    and the [Diffusers Models Gallery](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery)
    for checkpoints and embeddings to use.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ„æµè§ˆ[ç¨³å®šæ‰©æ•£æ¦‚å¿µåŒ–å™¨](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer)ã€[LoRAæ¢é™©å®¶](https://huggingface.co/spaces/multimodalart/LoraTheExplorer)å’Œ[Diffusersæ¨¡å‹åº“](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery)ä»¥è·å–æ£€æŸ¥ç‚¹å’ŒåµŒå…¥ä»¥ä¾›ä½¿ç”¨ã€‚
- en: DreamBooth
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DreamBooth
- en: '[DreamBooth](https://dreambooth.github.io/) finetunes an *entire diffusion
    model* on just several images of a subject to generate images of that subject
    in new styles and settings. This method works by using a special word in the prompt
    that the model learns to associate with the subject image. Of all the training
    methods, DreamBooth produces the largest file size (usually a few GBs) because
    it is a full checkpoint model.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[DreamBooth](https://dreambooth.github.io/)åœ¨ä»…ä½¿ç”¨å‡ å¹…ä¸»é¢˜å›¾åƒå¯¹æ•´ä¸ªæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆè¯¥ä¸»é¢˜çš„æ–°é£æ ¼å’Œè®¾ç½®çš„å›¾åƒã€‚è¿™ç§æ–¹æ³•é€šè¿‡åœ¨æç¤ºä¸­ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šè¯ï¼Œæ¨¡å‹å­¦ä¼šå°†å…¶ä¸ä¸»é¢˜å›¾åƒå…³è”èµ·æ¥ã€‚åœ¨æ‰€æœ‰è®­ç»ƒæ–¹æ³•ä¸­ï¼ŒDreamBoothäº§ç”Ÿçš„æ–‡ä»¶å¤§å°æœ€å¤§ï¼ˆé€šå¸¸ä¸ºå‡ GBï¼‰ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„æ£€æŸ¥ç‚¹æ¨¡å‹ã€‚'
- en: 'Letâ€™s load the [herge_style](https://huggingface.co/sd-dreambooth-library/herge-style)
    checkpoint, which is trained on just 10 images drawn by HergÃ©, to generate images
    in that style. For it to work, you need to include the special word `herge_style`
    in your prompt to trigger the checkpoint:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åŠ è½½[herge_style](https://huggingface.co/sd-dreambooth-library/herge-style)æ£€æŸ¥ç‚¹ï¼Œè¯¥æ£€æŸ¥ç‚¹ä»…è®­ç»ƒäº†ç”±HergÃ©ç»˜åˆ¶çš„10å¹…å›¾åƒï¼Œä»¥åœ¨è¯¥é£æ ¼ä¸­ç”Ÿæˆå›¾åƒã€‚ä¸ºäº†ä½¿å…¶å·¥ä½œï¼Œæ‚¨éœ€è¦åœ¨æç¤ºä¸­åŒ…å«ç‰¹æ®Šè¯`herge_style`æ¥è§¦å‘æ£€æŸ¥ç‚¹ï¼š
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/97b7dec0aa8cde8c3b74690416b5a5ca.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97b7dec0aa8cde8c3b74690416b5a5ca.png)'
- en: Textual inversion
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åè½¬
- en: '[Textual inversion](https://textual-inversion.github.io/) is very similar to
    DreamBooth and it can also personalize a diffusion model to generate certain concepts
    (styles, objects) from just a few images. This method works by training and finding
    new embeddings that represent the images you provide with a special word in the
    prompt. As a result, the diffusion model weights stay the same and the training
    process produces a relatively tiny (a few KBs) file.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬åè½¬](https://textual-inversion.github.io/)ä¸DreamBoothéå¸¸ç›¸ä¼¼ï¼Œå®ƒä¹Ÿå¯ä»¥ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä»å‡ å¹…å›¾åƒä¸­ç”Ÿæˆç‰¹å®šæ¦‚å¿µï¼ˆé£æ ¼ã€å¯¹è±¡ï¼‰ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è®­ç»ƒå¹¶æ‰¾åˆ°ä»£è¡¨æ‚¨åœ¨æç¤ºä¸­æä¾›çš„å›¾åƒçš„æ–°åµŒå…¥æ¥å·¥ä½œã€‚å› æ­¤ï¼Œæ‰©æ•£æ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ï¼Œè®­ç»ƒè¿‡ç¨‹äº§ç”Ÿä¸€ä¸ªç›¸å¯¹è¾ƒå°ï¼ˆå‡ KBï¼‰çš„æ–‡ä»¶ã€‚'
- en: Because textual inversion creates embeddings, it cannot be used on its own like
    DreamBooth and requires another model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæ–‡æœ¬åè½¬ä¼šåˆ›å»ºåµŒå…¥ï¼Œæ‰€ä»¥ä¸èƒ½åƒDreamBoothé‚£æ ·å•ç‹¬ä½¿ç”¨ï¼Œéœ€è¦å¦ä¸€ä¸ªæ¨¡å‹ã€‚
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now you can load the textual inversion embeddings with the [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    method and generate some images. Letâ€™s load the [sd-concepts-library/gta5-artwork](https://huggingface.co/sd-concepts-library/gta5-artwork)
    embeddings and youâ€™ll need to include the special word `<gta5-artwork>` in your
    prompt to trigger it:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)æ–¹æ³•åŠ è½½æ–‡æœ¬åè½¬åµŒå…¥ï¼Œå¹¶ç”Ÿæˆä¸€äº›å›¾åƒã€‚è®©æˆ‘ä»¬åŠ è½½[sd-concepts-library/gta5-artwork](https://huggingface.co/sd-concepts-library/gta5-artwork)åµŒå…¥ï¼Œæ‚¨éœ€è¦åœ¨æç¤ºä¸­åŒ…å«ç‰¹æ®Šè¯`<gta5-artwork>`æ¥è§¦å‘å®ƒï¼š
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/c039eaa48a71bfa7f36cbc3d5bc64464.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c039eaa48a71bfa7f36cbc3d5bc64464.png)'
- en: 'Textual inversion can also be trained on undesirable things to create *negative
    embeddings* to discourage a model from generating images with those undesirable
    things like blurry images or extra fingers on a hand. This can be an easy way
    to quickly improve your prompt. Youâ€™ll also load the embeddings with [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion),
    but this time, youâ€™ll need two more parameters:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åè½¬ä¹Ÿå¯ä»¥è®­ç»ƒä¸è‰¯äº‹ç‰©ï¼Œåˆ›å»º*è´ŸåµŒå…¥*ï¼Œä»¥é˜»æ­¢æ¨¡å‹ç”Ÿæˆå¸¦æœ‰è¿™äº›ä¸è‰¯äº‹ç‰©çš„å›¾åƒï¼Œå¦‚æ¨¡ç³Šå›¾åƒæˆ–æ‰‹ä¸Šé¢å¤–çš„æ‰‹æŒ‡ã€‚è¿™å¯ä»¥æ˜¯å¿«é€Ÿæ”¹è¿›æç¤ºçš„ç®€å•æ–¹æ³•ã€‚æ‚¨è¿˜å°†ä½¿ç”¨[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)åŠ è½½åµŒå…¥ï¼Œä½†è¿™æ¬¡ï¼Œæ‚¨éœ€è¦ä¸¤ä¸ªé¢å¤–çš„å‚æ•°ï¼š
- en: '`weight_name`: specifies the weight file to load if the file was saved in the
    ğŸ¤— Diffusers format with a specific name or if the file is stored in the A1111
    format'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_name`ï¼šæŒ‡å®šè¦åŠ è½½çš„æƒé‡æ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶ä»¥ç‰¹å®šåç§°ä¿å­˜åœ¨ğŸ¤— Diffusersæ ¼å¼ä¸­ï¼Œæˆ–è€…æ–‡ä»¶å­˜å‚¨åœ¨A1111æ ¼å¼ä¸­'
- en: '`token`: specifies the special word to use in the prompt to trigger the embeddings'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`ï¼šæŒ‡å®šåœ¨æç¤ºä¸­è§¦å‘åµŒå…¥çš„ç‰¹æ®Šè¯'
- en: 'Letâ€™s load the [sayakpaul/EasyNegative-test](https://huggingface.co/sayakpaul/EasyNegative-test)
    embeddings:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åŠ è½½[sayakpaul/EasyNegative-test](https://huggingface.co/sayakpaul/EasyNegative-test)åµŒå…¥ï¼š
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now you can use the `token` to generate an image with the negative embeddings:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨`token`ç”Ÿæˆå¸¦æœ‰è´ŸåµŒå…¥çš„å›¾åƒï¼š
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/e8cd1cf0f650f65d2b00c9db51b0e506.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8cd1cf0f650f65d2b00c9db51b0e506.png)'
- en: LoRA
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRA
- en: '[Low-Rank Adaptation (LoRA)](https://huggingface.co/papers/2106.09685) is a
    popular training technique because it is fast and generates smaller file sizes
    (a couple hundred MBs). Like the other methods in this guide, LoRA can train a
    model to learn new styles from just a few images. It works by inserting new weights
    into the diffusion model and then only the new weights are trained instead of
    the entire model. This makes LoRAs faster to train and easier to store.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰](https://huggingface.co/papers/2106.09685)æ˜¯ä¸€ç§æµè¡Œçš„è®­ç»ƒæŠ€æœ¯ï¼Œå› ä¸ºå®ƒå¿«é€Ÿä¸”ç”Ÿæˆè¾ƒå°çš„æ–‡ä»¶å¤§å°ï¼ˆå‡ ç™¾MBï¼‰ã€‚ä¸æœ¬æŒ‡å—ä¸­çš„å…¶ä»–æ–¹æ³•ä¸€æ ·ï¼ŒLoRAå¯ä»¥è®­ç»ƒæ¨¡å‹ä»…ä»å°‘é‡å›¾åƒä¸­å­¦ä¹ æ–°æ ·å¼ã€‚å®ƒé€šè¿‡å°†æ–°æƒé‡æ’å…¥æ‰©æ•£æ¨¡å‹ï¼Œç„¶åä»…è®­ç»ƒæ–°æƒé‡è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚è¿™ä½¿å¾—LoRAè®­ç»ƒæ›´å¿«ï¼Œå­˜å‚¨æ›´å®¹æ˜“ã€‚'
- en: LoRA is a very general training technique that can be used with other training
    methods. For example, it is common to train a model with DreamBooth and LoRA.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LoRAæ˜¯ä¸€ç§éå¸¸é€šç”¨çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯ä»¥ä¸å…¶ä»–è®­ç»ƒæ–¹æ³•ä¸€èµ·ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸ä¼šä½¿ç”¨DreamBoothå’ŒLoRAæ¥è®­ç»ƒæ¨¡å‹ã€‚
- en: 'LoRAs also need to be used with another model:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LoRAè¿˜éœ€è¦ä¸å¦ä¸€ä¸ªæ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼š
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then use the [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    method to load the [ostris/super-cereal-sdxl-lora](https://huggingface.co/ostris/super-cereal-sdxl-lora)
    weights and specify the weights filename from the repository:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½¿ç”¨[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)æ–¹æ³•åŠ è½½[ostris/super-cereal-sdxl-lora](https://huggingface.co/ostris/super-cereal-sdxl-lora)æƒé‡ï¼Œå¹¶ä»å­˜å‚¨åº“ä¸­æŒ‡å®šæƒé‡æ–‡ä»¶åï¼š
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/e378109cda4606520af62966b167614f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e378109cda4606520af62966b167614f.png)'
- en: 'The [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    method loads LoRA weights into both the UNet and text encoder. It is the preferred
    way for loading LoRAs because it can handle cases where:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)æ–¹æ³•å°†LoRAæƒé‡åŠ è½½åˆ°UNetå’Œæ–‡æœ¬ç¼–ç å™¨ä¸­ã€‚è¿™æ˜¯åŠ è½½LoRAçš„é¦–é€‰æ–¹å¼ï¼Œå› ä¸ºå®ƒå¯ä»¥å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š'
- en: the LoRA weights donâ€™t have separate identifiers for the UNet and text encoder
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRAæƒé‡æ²¡æœ‰UNetå’Œæ–‡æœ¬ç¼–ç å™¨çš„å•ç‹¬æ ‡è¯†ç¬¦
- en: the LoRA weights have separate identifiers for the UNet and text encoder
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRAæƒé‡å¯¹UNetå’Œæ–‡æœ¬ç¼–ç å™¨æœ‰å•ç‹¬çš„æ ‡è¯†ç¬¦
- en: 'But if you only need to load LoRA weights into the UNet, then you can use the
    [load_attn_procs()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs)
    method. Letâ€™s load the [jbilcke-hf/sdxl-cinematic-1](https://huggingface.co/jbilcke-hf/sdxl-cinematic-1)
    LoRA:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå¦‚æœåªéœ€è¦å°†LoRAæƒé‡åŠ è½½åˆ°UNetä¸­ï¼Œåˆ™å¯ä»¥ä½¿ç”¨[load_attn_procs()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs)æ–¹æ³•ã€‚è®©æˆ‘ä»¬åŠ è½½[jbilcke-hf/sdxl-cinematic-1](https://huggingface.co/jbilcke-hf/sdxl-cinematic-1)
    LoRAï¼š
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/b7bd2f5305aeb45b260a0d963df68046.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7bd2f5305aeb45b260a0d963df68046.png)'
- en: 'For both [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    and [load_attn_procs()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs),
    you can pass the `cross_attention_kwargs={"scale": 0.5}` parameter to adjust how
    much of the LoRA weights to use. A value of `0` is the same as only using the
    base model weights, and a value of `1` is equivalent to using the fully finetuned
    LoRA.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äº[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)å’Œ[load_attn_procs()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs)ï¼Œæ‚¨å¯ä»¥ä¼ é€’`cross_attention_kwargs={"scale":
    0.5}`å‚æ•°æ¥è°ƒæ•´ä½¿ç”¨LoRAæƒé‡çš„æ¯”ä¾‹ã€‚å€¼ä¸º`0`ç›¸å½“äºä»…ä½¿ç”¨åŸºæœ¬æ¨¡å‹æƒé‡ï¼Œå€¼ä¸º`1`ç›¸å½“äºä½¿ç”¨å®Œå…¨å¾®è°ƒçš„LoRAã€‚'
- en: 'To unload the LoRA weights, use the [unload_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.unload_lora_weights)
    method to discard the LoRA weights and restore the model to its original weights:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¸è½½LoRAæƒé‡ï¼Œè¯·ä½¿ç”¨[unload_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.unload_lora_weights)æ–¹æ³•ä¸¢å¼ƒLoRAæƒé‡å¹¶å°†æ¨¡å‹æ¢å¤ä¸ºå…¶åŸå§‹æƒé‡ï¼š
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Load multiple LoRAs
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ è½½å¤šä¸ªLoRA
- en: It can be fun to use multiple LoRAs together to create something entirely new
    and unique. The [fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)
    method allows you to fuse the LoRA weights with the original weights of the underlying
    model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å¤šä¸ªLoRAä¸€èµ·ä½¿ç”¨å¯ä»¥åˆ›å»ºå…¨æ–°ä¸”ç‹¬ç‰¹çš„ä¸œè¥¿ï¼Œè¿™å¾ˆæœ‰è¶£ã€‚[fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)æ–¹æ³•å…è®¸æ‚¨å°†LoRAæƒé‡ä¸åŸºç¡€æ¨¡å‹çš„åŸå§‹æƒé‡èåˆã€‚
- en: Fusing the weights can lead to a speedup in inference latency because you donâ€™t
    need to separately load the base model and LoRA! You can save your fused pipeline
    with [save_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.save_pretrained)
    to avoid loading and fusing the weights every time you want to use the model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: èåˆæƒé‡å¯ä»¥åŠ å¿«æ¨ç†å»¶è¿Ÿï¼Œå› ä¸ºæ‚¨æ— éœ€å•ç‹¬åŠ è½½åŸºæœ¬æ¨¡å‹å’ŒLoRAï¼æ‚¨å¯ä»¥ä½¿ç”¨[save_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.save_pretrained)ä¿å­˜èåˆçš„ç®¡é“ï¼Œä»¥é¿å…æ¯æ¬¡ä½¿ç”¨æ¨¡å‹æ—¶åŠ è½½å’Œèåˆæƒé‡ã€‚
- en: 'Load an initial model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½åˆå§‹æ¨¡å‹ï¼š
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, load the LoRA checkpoint and fuse it with the original weights. The `lora_scale`
    parameter controls how much to scale the output by with the LoRA weights. It is
    important to make the `lora_scale` adjustments in the [fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)
    method because it wonâ€™t work if you try to pass `scale` to the `cross_attention_kwargs`
    in the pipeline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼ŒåŠ è½½LoRAæ£€æŸ¥ç‚¹å¹¶å°†å…¶ä¸åŸå§‹æƒé‡èåˆã€‚`lora_scale`å‚æ•°æ§åˆ¶ä½¿ç”¨LoRAæƒé‡æ—¶è¾“å‡ºçš„ç¼©æ”¾æ¯”ä¾‹ã€‚åœ¨[fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)æ–¹æ³•ä¸­è¿›è¡Œ`lora_scale`è°ƒæ•´å¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æœå°è¯•åœ¨ç®¡é“ä¸­å°†`scale`ä¼ é€’ç»™`cross_attention_kwargs`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚
- en: If you need to reset the original model weights for any reason (use a different
    `lora_scale`), you should use the [unfuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.unfuse_lora)
    method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå‡ºäºä»»ä½•åŸå› éœ€è¦é‡ç½®åŸå§‹æ¨¡å‹æƒé‡ï¼ˆä½¿ç”¨ä¸åŒçš„`lora_scale`ï¼‰ï¼Œåº”ä½¿ç”¨[unfuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.unfuse_lora)æ–¹æ³•ã€‚
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then fuse this pipeline with the next set of LoRA weights:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†æ­¤ç®¡é“ä¸ä¸‹ä¸€ç»„LoRAæƒé‡èåˆï¼š
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You canâ€™t unfuse multiple LoRA checkpoints, so if you need to reset the model
    to its original weights, youâ€™ll need to reload it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨æ— æ³•è§£é™¤å¤šä¸ªLoRAæ£€æŸ¥ç‚¹çš„èåˆï¼Œå› æ­¤å¦‚æœéœ€è¦å°†æ¨¡å‹é‡ç½®ä¸ºå…¶åŸå§‹æƒé‡ï¼Œæ‚¨éœ€è¦é‡æ–°åŠ è½½å®ƒã€‚
- en: 'Now you can generate an image that uses the weights from both LoRAs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥ç”Ÿæˆä¸€å¼ ä½¿ç”¨ä¸¤ä¸ªLoRAæƒé‡çš„å›¾ç‰‡ï¼š
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ğŸ¤— PEFT
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ğŸ¤— PEFT
- en: Read the [Inference with ğŸ¤— PEFT](../tutorials/using_peft_for_inference) tutorial
    to learn more about its integration with ğŸ¤— Diffusers and how you can easily work
    with and juggle multiple adapters. Youâ€™ll need to install ğŸ¤— Diffusers and PEFT
    from source to run the example in this section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯»[ä½¿ç”¨ğŸ¤— PEFTè¿›è¡Œæ¨æ–­](../tutorials/using_peft_for_inference)æ•™ç¨‹ï¼Œäº†è§£æ›´å¤šå…³äºå®ƒä¸ğŸ¤— Diffusersé›†æˆä»¥åŠå¦‚ä½•è½»æ¾ä½¿ç”¨å’Œåˆ‡æ¢å¤šä¸ªé€‚é…å™¨çš„ä¿¡æ¯ã€‚æ‚¨éœ€è¦ä»æºä»£ç å®‰è£…ğŸ¤—
    Diffuserså’ŒPEFTæ‰èƒ½è¿è¡Œæœ¬èŠ‚ä¸­çš„ç¤ºä¾‹ã€‚
- en: 'Another way you can load and use multiple LoRAs is to specify the `adapter_name`
    parameter in [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights).
    This method takes advantage of the ğŸ¤— PEFT integration. For example, load and name
    both LoRA weights:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§åŠ è½½å’Œä½¿ç”¨å¤šä¸ªLoRAçš„æ–¹æ³•æ˜¯åœ¨[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)ä¸­æŒ‡å®š`adapter_name`å‚æ•°ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†ğŸ¤—
    PEFTé›†æˆã€‚ä¾‹å¦‚ï¼ŒåŠ è½½å¹¶å‘½åä¸¤ä¸ªLoRAæƒé‡ï¼š
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now use the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    to activate both LoRAs, and you can configure how much weight each LoRA should
    have on the output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨[set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)æ¥æ¿€æ´»ä¸¤ä¸ªLoRAï¼Œå¹¶å¯ä»¥é…ç½®æ¯ä¸ªLoRAåœ¨è¾“å‡ºä¸Šçš„æƒé‡ï¼š
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, generate an image:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œç”Ÿæˆä¸€å¼ å›¾ç‰‡ï¼š
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Kohya and TheLastBen
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kohyaå’ŒTheLastBen
- en: Other popular LoRA trainers from the community include those by [Kohya](https://github.com/kohya-ss/sd-scripts/)
    and [TheLastBen](https://github.com/TheLastBen/fast-stable-diffusion). These trainers
    create different LoRA checkpoints than those trained by ğŸ¤— Diffusers, but they
    can still be loaded in the same way.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤¾åŒºä¸­å…¶ä»–æµè¡Œçš„LoRAè®­ç»ƒå™¨åŒ…æ‹¬[Kohya](https://github.com/kohya-ss/sd-scripts/)å’Œ[TheLastBen](https://github.com/TheLastBen/fast-stable-diffusion)çš„è®­ç»ƒå™¨ã€‚è¿™äº›è®­ç»ƒå™¨åˆ›å»ºçš„LoRAæ£€æŸ¥ç‚¹ä¸ğŸ¤—
    Diffusersè®­ç»ƒçš„ä¸åŒï¼Œä½†ä»ç„¶å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼åŠ è½½ã€‚
- en: 'Letâ€™s download the [Blueprintify SD XL 1.0](https://civitai.com/models/150986/blueprintify-sd-xl-10)
    checkpoint from [Civitai](https://civitai.com/):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»[Civitai](https://civitai.com/)ä¸‹è½½[Blueprintify SD XL 1.0](https://civitai.com/models/150986/blueprintify-sd-xl-10)çš„æ£€æŸ¥ç‚¹ï¼š
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Load the LoRA checkpoint with the [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    method, and specify the filename in the `weight_name` parameter:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)æ–¹æ³•åŠ è½½LoRAæ£€æŸ¥ç‚¹ï¼Œå¹¶åœ¨`weight_name`å‚æ•°ä¸­æŒ‡å®šæ–‡ä»¶åï¼š
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Generate an image:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆä¸€å¼ å›¾ç‰‡ï¼š
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Some limitations of using Kohya LoRAs with ğŸ¤— Diffusers include:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Kohya LoRAä¸ğŸ¤— Diffusersçš„ä¸€äº›é™åˆ¶åŒ…æ‹¬ï¼š
- en: Images may not look like those generated by UIs - like ComfyUI - for multiple
    reasons, which are explained [here](https://github.com/huggingface/diffusers/pull/4287/#issuecomment-1655110736).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºå¤šç§åŸå› ï¼Œç”Ÿæˆçš„å›¾ç‰‡å¯èƒ½ä¸åƒUIsï¼ˆå¦‚ComfyUIï¼‰ç”Ÿæˆçš„å›¾ç‰‡ï¼Œè¿™äº›åŸå› åœ¨[è¿™é‡Œ](https://github.com/huggingface/diffusers/pull/4287/#issuecomment-1655110736)æœ‰è§£é‡Šã€‚
- en: '[LyCORIS checkpoints](https://github.com/KohakuBlueleaf/LyCORIS) arenâ€™t fully
    supported. The [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR
    are not supported.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LyCORISæ£€æŸ¥ç‚¹](https://github.com/KohakuBlueleaf/LyCORIS)ä¸å—å®Œå…¨æ”¯æŒã€‚[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)æ–¹æ³•ä½¿ç”¨LoRAå’ŒLoConæ¨¡å—åŠ è½½LyCORISæ£€æŸ¥ç‚¹ï¼Œä½†ä¸æ”¯æŒHadaå’ŒLoKRã€‚'
- en: 'Loading a checkpoint from TheLastBen is very similar. For example, to load
    the [TheLastBen/William_Eggleston_Style_SDXL](https://huggingface.co/TheLastBen/William_Eggleston_Style_SDXL)
    checkpoint:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½TheLastBençš„æ£€æŸ¥ç‚¹éå¸¸ç±»ä¼¼ã€‚ä¾‹å¦‚ï¼Œè¦åŠ è½½[TheLastBen/William_Eggleston_Style_SDXL](https://huggingface.co/TheLastBen/William_Eggleston_Style_SDXL)çš„æ£€æŸ¥ç‚¹ï¼š
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: IP-Adapter
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IP-Adapter
- en: '[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight
    adapter that adds image prompting capabilities to a diffusion model. This adapter
    works by decoupling the cross-attention layers of the image and text features.
    All the other model components are frozen and only the embedded image features
    in the UNet are trained. As a result, IP-Adapter files are typically only ~100MBs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[IP-Adapter](https://ip-adapter.github.io/)æ˜¯ä¸€ç§æœ‰æ•ˆä¸”è½»é‡çº§çš„é€‚é…å™¨ï¼Œå¯ä¸ºæ‰©æ•£æ¨¡å‹æ·»åŠ å›¾åƒæç¤ºåŠŸèƒ½ã€‚è¯¥é€‚é…å™¨é€šè¿‡è§£è€¦å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›å±‚æ¥å·¥ä½œã€‚æ‰€æœ‰å…¶ä»–æ¨¡å‹ç»„ä»¶éƒ½è¢«å†»ç»“ï¼Œåªæœ‰UNetä¸­çš„åµŒå…¥å›¾åƒç‰¹å¾è¢«è®­ç»ƒã€‚å› æ­¤ï¼ŒIP-Adapteræ–‡ä»¶é€šå¸¸åªæœ‰çº¦100MBã€‚'
- en: IP-Adapter works with most of our pipelines, including Stable Diffusion, Stable
    Diffusion XL (SDXL), ControlNet, T2I-Adapter, AnimateDiff. And you can use any
    custom models finetuned from the same base models. It also works with LCM-Lora
    out of box.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: IP-Adapteré€‚ç”¨äºæˆ‘ä»¬çš„å¤§å¤šæ•°ç®¡é“ï¼ŒåŒ…æ‹¬ç¨³å®šæ‰©æ•£ã€ç¨³å®šæ‰©æ•£XLï¼ˆSDXLï¼‰ã€ControlNetã€T2I-Adapterã€AnimateDiffã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä»ç›¸åŒåŸºç¡€æ¨¡å‹å¾®è°ƒçš„ä»»ä½•è‡ªå®šä¹‰æ¨¡å‹ã€‚å®ƒè¿˜å¯ä»¥ä¸LCM-Loraç›´æ¥é…åˆä½¿ç”¨ã€‚
- en: You can find official IP-Adapter checkpoints in [h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter)ä¸­æ‰¾åˆ°å®˜æ–¹IP-Adapteræ£€æŸ¥ç‚¹ã€‚
- en: IP-Adapter was contributed by [okotaku](https://github.com/okotaku).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: IP-Adapterç”±[okotaku](https://github.com/okotaku)è´¡çŒ®ã€‚
- en: Letâ€™s first create a Stable Diffusion Pipeline.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªç¨³å®šæ‰©æ•£ç®¡é“ã€‚
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now load the [h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter) weights
    with the [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    method.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)æ–¹æ³•åŠ è½½[h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter)çš„æƒé‡ã€‚
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: IP-Adapter relies on an image encoder to generate the image features, if your
    IP-Adapter weights folder contains a "image_encoder" subfolder, the image encoder
    will be automatically loaded and registered to the pipeline. Otherwise you can
    so load a [CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)
    model and pass it to a Stable Diffusion pipeline when you create it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: IP-é€‚é…å™¨ä¾èµ–äºå›¾åƒç¼–ç å™¨æ¥ç”Ÿæˆå›¾åƒç‰¹å¾ï¼Œå¦‚æœæ‚¨çš„IP-é€‚é…å™¨æƒé‡æ–‡ä»¶å¤¹åŒ…å«ä¸€ä¸ªåä¸ºâ€œimage_encoderâ€çš„å­æ–‡ä»¶å¤¹ï¼Œåˆ™å›¾åƒç¼–ç å™¨å°†è‡ªåŠ¨åŠ è½½å¹¶æ³¨å†Œåˆ°ç®¡é“ä¸­ã€‚å¦åˆ™ï¼Œæ‚¨å¯ä»¥åŠ è½½ä¸€ä¸ª[CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)æ¨¡å‹ï¼Œå¹¶åœ¨åˆ›å»ºæ—¶å°†å…¶ä¼ é€’ç»™ç¨³å®šæ‰©æ•£ç®¡é“ã€‚
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: IP-Adapter allows you to use both image and text to condition the image generation
    process. For example, letâ€™s use the bear image from the [Textual Inversion](#textual-inversion)
    section as the image prompt (`ip_adapter_image`) along with a text prompt to add
    â€œsunglassesâ€.Â ğŸ˜
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: IP-é€‚é…å™¨å…è®¸æ‚¨åŒæ—¶ä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬æ¥è°ƒèŠ‚å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª[æ–‡æœ¬åè½¬](#textual-inversion)éƒ¨åˆ†çš„ç†Šå›¾åƒä½œä¸ºå›¾åƒæç¤ºï¼ˆ`ip_adapter_image`ï¼‰ï¼Œå¹¶é™„ä¸Šä¸€ä¸ªæ–‡æœ¬æç¤ºæ·»åŠ â€œå¤ªé˜³é•œâ€ã€‚
    ğŸ˜
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/344a4a031aa6d71b9b57dc0b5139e9be.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/344a4a031aa6d71b9b57dc0b5139e9be.png)'
- en: You can use the `set_ip_adapter_scale()` method to adjust the text prompt and
    image prompt condition ratio. Â If youâ€™re only using the image prompt, you should
    set the scale to `1.0`. You can lower the scale to get more generation diversity,
    but itâ€™ll be less aligned with the prompt. `scale=0.5` can achieve good results
    in most cases when you use both text and image prompts.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`set_ip_adapter_scale()`æ–¹æ³•æ¥è°ƒæ•´æ–‡æœ¬æç¤ºå’Œå›¾åƒæç¤ºçš„æ¡ä»¶æ¯”ä¾‹ã€‚å¦‚æœæ‚¨åªä½¿ç”¨å›¾åƒæç¤ºï¼Œåº”å°†æ¯”ä¾‹è®¾ç½®ä¸º`1.0`ã€‚æ‚¨å¯ä»¥é™ä½æ¯”ä¾‹ä»¥è·å¾—æ›´å¤šçš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œä½†å®ƒå°†ä¸æç¤ºä¸å¤ªä¸€è‡´ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå½“æ‚¨åŒæ—¶ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒæç¤ºæ—¶ï¼Œ`scale=0.5`å¯ä»¥è·å¾—è‰¯å¥½çš„ç»“æœã€‚
- en: IP-Adapter also works great with Image-to-Image and Inpainting pipelines. See
    below examples of how you can use it with Image-to-Image and Inpaint.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: IP-é€‚é…å™¨ä¹Ÿä¸å›¾åƒåˆ°å›¾åƒå’Œä¿®è¡¥ç®¡é“éå¸¸é…åˆã€‚è¯·çœ‹ä¸‹é¢å¦‚ä½•å°†å…¶ä¸å›¾åƒåˆ°å›¾åƒå’Œä¿®è¡¥ä¸€èµ·ä½¿ç”¨çš„ç¤ºä¾‹ã€‚
- en: image-to-imageinpaint
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒä¿®è¡¥
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: IP-Adapters can also be used with [SDXL](../api/pipelines/stable_diffusion/stable_diffusion_xl.md)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: IP-é€‚é…å™¨ä¹Ÿå¯ä»¥ä¸[SDXL](../api/pipelines/stable_diffusion/stable_diffusion_xl.md)ä¸€èµ·ä½¿ç”¨
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/7714d8061d7ab0a16f36c9923a9a1d91.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7714d8061d7ab0a16f36c9923a9a1d91.png)'
- en: input image
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å›¾åƒ
- en: '![](../Images/6d3dc0e54f1d38caddd7daf1bd93c3fb.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d3dc0e54f1d38caddd7daf1bd93c3fb.png)'
- en: adapted image
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´åçš„å›¾åƒ
- en: You can use the IP-Adapter face model to apply specific faces to your images.
    It is an effective way to maintain consistent characters in your image generations.
    Weights are loaded with the same method used for the other IP-Adapters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨IP-é€‚é…å™¨é¢éƒ¨æ¨¡å‹å°†ç‰¹å®šé¢éƒ¨åº”ç”¨äºæ‚¨çš„å›¾åƒã€‚è¿™æ˜¯åœ¨å›¾åƒç”Ÿæˆä¸­ä¿æŒä¸€è‡´è§’è‰²çš„æœ‰æ•ˆæ–¹æ³•ã€‚æƒé‡çš„åŠ è½½æ–¹å¼ä¸å…¶ä»–IP-é€‚é…å™¨ç›¸åŒã€‚
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It is recommended to use `DDIMScheduler` and `EulerDiscreteScheduler` for face
    model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºè®®ä½¿ç”¨`DDIMScheduler`å’Œ`EulerDiscreteScheduler`æ¥è¿›è¡Œé¢éƒ¨æ¨¡å‹ã€‚
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/d45f8afb266397140047ea513967ec17.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d45f8afb266397140047ea513967ec17.png)'
- en: input image
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å›¾åƒ
- en: '![](../Images/1f753c6cacd994b80a20d9f822857c7e.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f753c6cacd994b80a20d9f822857c7e.png)'
- en: output image
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå›¾åƒ
- en: You can load multiple IP-Adapter models and use multiple reference images at
    the same time. In this example we use IP-Adapter-Plus face model to create a consistent
    character and also use IP-Adapter-Plus model along with 10 images to create a
    coherent style in the image we generate.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åŒæ—¶åŠ è½½å¤šä¸ªIP-é€‚é…å™¨æ¨¡å‹å¹¶ä½¿ç”¨å¤šä¸ªå‚è€ƒå›¾åƒã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨IP-é€‚é…å™¨-Plusé¢éƒ¨æ¨¡å‹åˆ›å»ºä¸€è‡´çš„è§’è‰²ï¼Œå¹¶åŒæ—¶ä½¿ç”¨IP-é€‚é…å™¨-Plusæ¨¡å‹ä»¥åŠ10ä¸ªå›¾åƒåˆ›å»ºæˆ‘ä»¬ç”Ÿæˆçš„å›¾åƒä¸­çš„è¿è´¯æ ·å¼ã€‚
- en: '[PRE28]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../Images/708cd0f9abdbb4eb3ad70ec47c4651d9.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/708cd0f9abdbb4eb3ad70ec47c4651d9.png)'
- en: style input image
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·å¼è¾“å…¥å›¾åƒ
- en: '![](../Images/b4c08b99e9974c3a3e5bb5f7f2d1054f.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4c08b99e9974c3a3e5bb5f7f2d1054f.png)'
- en: face input image
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: é¢éƒ¨è¾“å…¥å›¾åƒ
- en: '![](../Images/0f5d4fe5f2525dd88bd660385a4b080e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f5d4fe5f2525dd88bd660385a4b080e.png)'
- en: output image
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå›¾åƒ
- en: LCM-Lora
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LCM-Lora
- en: You can use IP-Adapter with LCM-Lora to achieve â€œinstant fine-tuneâ€ with custom
    images. Note that you need to load IP-Adapter weights before loading the LCM-Lora
    weights.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨IP-é€‚é…å™¨ä¸LCM-Loraä¸€èµ·å®ç°ä½¿ç”¨è‡ªå®šä¹‰å›¾åƒçš„â€œå³æ—¶å¾®è°ƒâ€ã€‚è¯·æ³¨æ„ï¼Œåœ¨åŠ è½½LCM-Loraæƒé‡ä¹‹å‰ï¼Œæ‚¨éœ€è¦åŠ è½½IP-é€‚é…å™¨æƒé‡ã€‚
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Other pipelines
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…¶ä»–ç®¡é“
- en: IP-Adapter is compatible with any pipeline that (1) uses a text prompt and (2)
    uses Stable Diffusion or Stable Diffusion XL checkpoint. To use IP-Adapter with
    a different pipeline, all you need to do is to run `load_ip_adapter()` method
    after you create the pipeline, and then pass your image to the pipeline as `ip_adapter_image`
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: IP-é€‚é…å™¨ä¸ä»»ä½•ä½¿ç”¨æ–‡æœ¬æç¤ºå’Œä½¿ç”¨ç¨³å®šæ‰©æ•£æˆ–ç¨³å®šæ‰©æ•£XLæ£€æŸ¥ç‚¹çš„ç®¡é“å…¼å®¹ã€‚è¦åœ¨ä¸åŒç®¡é“ä¸­ä½¿ç”¨IP-é€‚é…å™¨ï¼Œæ‚¨åªéœ€è¦åœ¨åˆ›å»ºç®¡é“åè¿è¡Œ`load_ip_adapter()`æ–¹æ³•ï¼Œç„¶åå°†æ‚¨çš„å›¾åƒä½œä¸º`ip_adapter_image`ä¼ é€’ç»™ç®¡é“
- en: ğŸ¤— Diffusers currently only supports using IP-Adapter with some of the most popular
    pipelines, feel free to open a [feature request](https://github.com/huggingface/diffusers/issues/new/choose)
    if you have a cool use-case and require integrating IP-adapters with a pipeline
    that does not support it yet!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—æ‰©æ•£å™¨ç›®å‰ä»…æ”¯æŒä¸ä¸€äº›æœ€å—æ¬¢è¿çš„ç®¡é“ä¸€èµ·ä½¿ç”¨IP-é€‚é…å™¨ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªå¾ˆé…·çš„ç”¨ä¾‹å¹¶éœ€è¦å°†IPé€‚é…å™¨é›†æˆåˆ°å°šä¸æ”¯æŒçš„ç®¡é“ä¸­ï¼Œè¯·éšæ—¶æå‡º[åŠŸèƒ½è¯·æ±‚](https://github.com/huggingface/diffusers/issues/new/choose)ï¼
- en: You can find below examples on how to use IP-Adapter with ControlNet and AnimateDiff.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ä¸‹é¢æ‰¾åˆ°å¦‚ä½•ä½¿ç”¨IP-é€‚é…å™¨ä¸ControlNetå’ŒAnimateDiffçš„ç¤ºä¾‹ã€‚
- en: ControlNetAnimateDiff
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNetAnimateDiff
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/16fb69a794e63d70afc3ca739b1eee43.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16fb69a794e63d70afc3ca739b1eee43.png)'
- en: input image
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å›¾åƒ
- en: '![](../Images/e6383b3af214075c7c04400114af5b74.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6383b3af214075c7c04400114af5b74.png)'
- en: adapted image
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´åçš„å›¾åƒ
