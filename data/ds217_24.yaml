- en: Cloud storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äº‘å­˜å‚¨
- en: 'Original text: [https://huggingface.co/docs/datasets/filesystems](https://huggingface.co/docs/datasets/filesystems)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/datasets/filesystems](https://huggingface.co/docs/datasets/filesystems)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'ğŸ¤— Datasets supports access to cloud storage providers through a `fsspec` FileSystem
    implementations. You can save and load datasets from any cloud storage in a Pythonic
    way. Take a look at the following table for some example of supported cloud storage
    providers:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— æ•°æ®é›†é€šè¿‡`fsspec`æ–‡ä»¶ç³»ç»Ÿå®ç°æ”¯æŒè®¿é—®äº‘å­˜å‚¨æä¾›å•†ã€‚æ‚¨å¯ä»¥ä»¥Pythonicæ–¹å¼ä»ä»»ä½•äº‘å­˜å‚¨ä¸­ä¿å­˜å’ŒåŠ è½½æ•°æ®é›†ã€‚æŸ¥çœ‹ä»¥ä¸‹è¡¨æ ¼ï¼Œäº†è§£ä¸€äº›å—æ”¯æŒçš„äº‘å­˜å‚¨æä¾›å•†çš„ç¤ºä¾‹ï¼š
- en: '| Storage provider | Filesystem implementation |'
  id: totrans-4
  prefs: []
  type: TYPE_TB
  zh: '| å­˜å‚¨æä¾›å•† | æ–‡ä»¶ç³»ç»Ÿå®ç° |'
- en: '| --- | --- |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Amazon S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| äºšé©¬é€ŠS3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |'
- en: '| Google Cloud Storage | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| Googleäº‘å­˜å‚¨ | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |'
- en: '| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |'
- en: '| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs)
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs)
    |'
- en: '| Google Drive | [gdrivefs](https://github.com/intake/gdrivefs) |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| Googleäº‘ç›˜ | [gdrivefs](https://github.com/intake/gdrivefs) |'
- en: '| Oracle Cloud Storage | [ocifs](https://ocifs.readthedocs.io/en/latest/) |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| Oracleäº‘å­˜å‚¨ | [ocifs](https://ocifs.readthedocs.io/en/latest/) |'
- en: This guide will show you how to save and load datasets with any cloud storage.
    Here are examples for S3, Google Cloud Storage, Azure Blob Storage, and Oracle
    Cloud Object Storage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä»»ä½•äº‘å­˜å‚¨ä¿å­˜å’ŒåŠ è½½æ•°æ®é›†ã€‚ä»¥ä¸‹æ˜¯S3ã€Googleäº‘å­˜å‚¨ã€Azure Blobå­˜å‚¨å’ŒOracleäº‘å¯¹è±¡å­˜å‚¨çš„ç¤ºä¾‹ã€‚
- en: Set up your cloud storage FileSystem
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®æ‚¨çš„äº‘å­˜å‚¨æ–‡ä»¶ç³»ç»Ÿ
- en: Amazon S3
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: äºšé©¬é€ŠS3
- en: 'Install the S3 FileSystem implementation:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£…S3æ–‡ä»¶ç³»ç»Ÿå®ç°ï¼š
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Define your credentials
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰æ‚¨çš„å‡­æ®
- en: To use an anonymous connection, use `anon=True`. Otherwise, include your `aws_access_key_id`
    and `aws_secret_access_key` whenever you are interacting with a private S3 bucket.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨åŒ¿åè¿æ¥ï¼Œè¯·ä½¿ç”¨`anon=True`ã€‚å¦åˆ™ï¼Œåœ¨ä¸ç§æœ‰S3å­˜å‚¨æ¡¶äº¤äº’æ—¶ï¼Œè¯·åŒ…æ‹¬æ‚¨çš„`aws_access_key_id`å’Œ`aws_secret_access_key`ã€‚
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Create your FileSystem instance
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Google Cloud Storage
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Googleäº‘å­˜å‚¨
- en: 'Install the Google Cloud Storage implementation:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£…Googleäº‘å­˜å‚¨å®ç°ï¼š
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Define your credentials
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰æ‚¨çš„å‡­æ®
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create your FileSystem instance
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Azure Blob Storage
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure Blobå­˜å‚¨
- en: 'Install the Azure Blob Storage implementation:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£…Azure Blobå­˜å‚¨å®ç°ï¼š
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Define your credentials
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰æ‚¨çš„å‡­æ®
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create your FileSystem instance
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Oracle Cloud Object Storage
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Oracleäº‘å¯¹è±¡å­˜å‚¨
- en: 'Install the OCI FileSystem implementation:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£…OCIæ–‡ä»¶ç³»ç»Ÿå®ç°ï¼š
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Define your credentials
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰æ‚¨çš„å‡­æ®
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Create your FileSystem instance
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ‚¨çš„æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Load and Save your datasets using your cloud storage FileSystem
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äº‘å­˜å‚¨æ–‡ä»¶ç³»ç»ŸåŠ è½½å’Œä¿å­˜æ•°æ®é›†
- en: Download and prepare a dataset into a cloud storage
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†åˆ°äº‘å­˜å‚¨
- en: You can download and prepare a dataset into your cloud storage by specifying
    a remote `output_dir` in `download_and_prepare`. Donâ€™t forget to use the previously
    defined `storage_options` containing your credentials to write into a private
    cloud storage.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡åœ¨`download_and_prepare`ä¸­æŒ‡å®šè¿œç¨‹`output_dir`æ¥ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†åˆ°æ‚¨çš„äº‘å­˜å‚¨ã€‚ä¸è¦å¿˜è®°ä½¿ç”¨å…ˆå‰å®šä¹‰çš„åŒ…å«æ‚¨çš„å‡­æ®çš„`storage_options`æ¥å†™å…¥ç§æœ‰äº‘å­˜å‚¨ã€‚
- en: 'The `download_and_prepare` method works in two steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`download_and_prepare`æ–¹æ³•åˆ†ä¸¤æ­¥è¿›è¡Œï¼š'
- en: it first downloads the raw data files (if any) in your local cache. You can
    set your cache directory by passing `cache_dir` to [load_dataset_builder()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset_builder)
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå®ƒä¼šä¸‹è½½åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰åˆ°æ‚¨çš„æœ¬åœ°ç¼“å­˜ä¸­ã€‚æ‚¨å¯ä»¥é€šè¿‡å°†`cache_dir`ä¼ é€’ç»™[load_dataset_builder()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset_builder)æ¥è®¾ç½®ç¼“å­˜ç›®å½•
- en: then it generates the dataset in Arrow or Parquet format in your cloud storage
    by iterating over the raw data files.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œé€šè¿‡è¿­ä»£åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œåœ¨æ‚¨çš„äº‘å­˜å‚¨ä¸­ç”ŸæˆArrowæˆ–Parquetæ ¼å¼çš„æ•°æ®é›†ã€‚
- en: 'Load a dataset builder from the Hugging Face Hub (see [how to load from the
    Hugging Face Hub](./loading#hugging-face-hub)):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Hugging Face HubåŠ è½½æ•°æ®é›†æ„å»ºå™¨ï¼ˆå‚è§[å¦‚ä½•ä»Hugging Face HubåŠ è½½](./loading#hugging-face-hub)ï¼‰ï¼š
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Load a dataset builder using a loading script (see [how to load a local loading
    script](./loading#local-loading-script)):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä½¿ç”¨åŠ è½½è„šæœ¬åŠ è½½æ•°æ®é›†æ„å»ºå™¨ï¼ˆå‚è§[å¦‚ä½•åŠ è½½æœ¬åœ°åŠ è½½è„šæœ¬](./loading#local-loading-script)ï¼‰:'
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Use your own data files (see [how to load local and remote files](./loading#local-and-remote-files)):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®æ–‡ä»¶ï¼ˆå‚è§[å¦‚ä½•åŠ è½½æœ¬åœ°å’Œè¿œç¨‹æ–‡ä»¶](./loading#local-and-remote-files)ï¼‰ï¼š
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is highly recommended to save the files as compressed Parquet files to optimize
    I/O by specifying `file_format="parquet"`. Otherwise the dataset is saved as an
    uncompressed Arrow file.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºçƒˆå»ºè®®å°†æ–‡ä»¶ä¿å­˜ä¸ºå‹ç¼©çš„Parquetæ–‡ä»¶ï¼Œä»¥é€šè¿‡æŒ‡å®š`file_format="parquet"`æ¥ä¼˜åŒ–I/Oã€‚å¦åˆ™ï¼Œæ•°æ®é›†å°†ä¿å­˜ä¸ºæœªå‹ç¼©çš„Arrowæ–‡ä»¶ã€‚
- en: 'You can also specify the size of the shards using `max_shard_size` (default
    is 500MB):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`max_shard_size`æŒ‡å®šç¢ç‰‡çš„å¤§å°ï¼ˆé»˜è®¤ä¸º500MBï¼‰ï¼š
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Dask
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dask
- en: Dask is a parallel computing library and it has a pandas-like API for working
    with larger than memory Parquet datasets in parallel. Dask can use multiple threads
    or processes on a single machine, or a cluster of machines to process data in
    parallel. Dask supports local data but also data from a cloud storage.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Daskæ˜¯ä¸€ä¸ªå¹¶è¡Œè®¡ç®—åº“ï¼Œå®ƒå…·æœ‰ç±»ä¼¼äºpandasçš„APIï¼Œç”¨äºå¹¶è¡Œå¤„ç†å¤§äºå†…å­˜çš„Parquetæ•°æ®é›†ã€‚Daskå¯ä»¥åœ¨å•å°æœºå™¨ä¸Šä½¿ç”¨å¤šä¸ªçº¿ç¨‹æˆ–è¿›ç¨‹ï¼Œæˆ–è€…åœ¨é›†ç¾¤ä¸­ä½¿ç”¨å¤šå°æœºå™¨å¹¶è¡Œå¤„ç†æ•°æ®ã€‚Daskæ”¯æŒæœ¬åœ°æ•°æ®ï¼Œä¹Ÿæ”¯æŒæ¥è‡ªäº‘å­˜å‚¨çš„æ•°æ®ã€‚
- en: Therefore you can load a dataset saved as sharded Parquet files in Dask with
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ‚¨å¯ä»¥åœ¨Daskä¸­åŠ è½½ä¿å­˜ä¸ºåˆ†ç‰‡Parquetæ–‡ä»¶çš„æ•°æ®é›†
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can find more about dask dataframes in their [documentation](https://docs.dask.org/en/stable/dataframe.html).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨å®ƒä»¬çš„[æ–‡æ¡£](https://docs.dask.org/en/stable/dataframe.html)ä¸­æ‰¾åˆ°æ›´å¤šå…³äºdaskæ•°æ®æ¡†çš„ä¿¡æ¯ã€‚
- en: Saving serialized datasets
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿å­˜åºåˆ—åŒ–æ•°æ®é›†
- en: 'After you have processed your dataset, you can save it to your cloud storage
    with [Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†å®Œæ•°æ®é›†åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)å°†å…¶ä¿å­˜åˆ°äº‘å­˜å‚¨ä¸­ï¼š
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Remember to define your credentials in your [FileSystem instance](#set-up-your-cloud-storage-filesystem)
    `fs` whenever you are interacting with a private cloud storage.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°å¾—åœ¨ä¸ç§æœ‰äº‘å­˜å‚¨äº¤äº’æ—¶ï¼Œåœ¨æ‚¨çš„[FileSystemå®ä¾‹](#set-up-your-cloud-storage-filesystem) `fs`ä¸­å®šä¹‰æ‚¨çš„å‡­æ®ã€‚
- en: Listing serialized datasets
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ—å‡ºåºåˆ—åŒ–æ•°æ®é›†
- en: 'List files from a cloud storage with your FileSystem instance `fs`, using `fs.ls`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨çš„FileSystemå®ä¾‹`fs`ä»äº‘å­˜å‚¨ä¸­åˆ—å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨`fs.ls`ï¼š
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Load serialized datasets
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ è½½åºåˆ—åŒ–æ•°æ®é›†
- en: 'When you are ready to use your dataset again, reload it with [Dataset.load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.load_from_disk):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨å‡†å¤‡å†æ¬¡ä½¿ç”¨æ•°æ®é›†æ—¶ï¼Œè¯·ä½¿ç”¨[Dataset.load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.load_from_disk)é‡æ–°åŠ è½½ï¼š
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
