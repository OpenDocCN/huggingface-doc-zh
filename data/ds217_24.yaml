- en: Cloud storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云存储
- en: 'Original text: [https://huggingface.co/docs/datasets/filesystems](https://huggingface.co/docs/datasets/filesystems)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/datasets/filesystems](https://huggingface.co/docs/datasets/filesystems)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '🤗 Datasets supports access to cloud storage providers through a `fsspec` FileSystem
    implementations. You can save and load datasets from any cloud storage in a Pythonic
    way. Take a look at the following table for some example of supported cloud storage
    providers:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 数据集通过`fsspec`文件系统实现支持访问云存储提供商。您可以以Pythonic方式从任何云存储中保存和加载数据集。查看以下表格，了解一些受支持的云存储提供商的示例：
- en: '| Storage provider | Filesystem implementation |'
  id: totrans-4
  prefs: []
  type: TYPE_TB
  zh: '| 存储提供商 | 文件系统实现 |'
- en: '| --- | --- |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Amazon S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |'
- en: '| Google Cloud Storage | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| Google云存储 | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |'
- en: '| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |'
- en: '| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs)
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs)
    |'
- en: '| Google Drive | [gdrivefs](https://github.com/intake/gdrivefs) |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| Google云盘 | [gdrivefs](https://github.com/intake/gdrivefs) |'
- en: '| Oracle Cloud Storage | [ocifs](https://ocifs.readthedocs.io/en/latest/) |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| Oracle云存储 | [ocifs](https://ocifs.readthedocs.io/en/latest/) |'
- en: This guide will show you how to save and load datasets with any cloud storage.
    Here are examples for S3, Google Cloud Storage, Azure Blob Storage, and Oracle
    Cloud Object Storage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将向您展示如何使用任何云存储保存和加载数据集。以下是S3、Google云存储、Azure Blob存储和Oracle云对象存储的示例。
- en: Set up your cloud storage FileSystem
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置您的云存储文件系统
- en: Amazon S3
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亚马逊S3
- en: 'Install the S3 FileSystem implementation:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装S3文件系统实现：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Define your credentials
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您的凭据
- en: To use an anonymous connection, use `anon=True`. Otherwise, include your `aws_access_key_id`
    and `aws_secret_access_key` whenever you are interacting with a private S3 bucket.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用匿名连接，请使用`anon=True`。否则，在与私有S3存储桶交互时，请包括您的`aws_access_key_id`和`aws_secret_access_key`。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Create your FileSystem instance
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建您的文件系统实例
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Google Cloud Storage
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google云存储
- en: 'Install the Google Cloud Storage implementation:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Google云存储实现：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Define your credentials
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您的凭据
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create your FileSystem instance
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建您的文件系统实例
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Azure Blob Storage
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure Blob存储
- en: 'Install the Azure Blob Storage implementation:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Azure Blob存储实现：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Define your credentials
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您的凭据
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create your FileSystem instance
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建您的文件系统实例
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Oracle Cloud Object Storage
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Oracle云对象存储
- en: 'Install the OCI FileSystem implementation:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装OCI文件系统实现：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Define your credentials
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您的凭据
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Create your FileSystem instance
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建您的文件系统实例
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Load and Save your datasets using your cloud storage FileSystem
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用云存储文件系统加载和保存数据集
- en: Download and prepare a dataset into a cloud storage
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载并准备数据集到云存储
- en: You can download and prepare a dataset into your cloud storage by specifying
    a remote `output_dir` in `download_and_prepare`. Don’t forget to use the previously
    defined `storage_options` containing your credentials to write into a private
    cloud storage.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在`download_and_prepare`中指定远程`output_dir`来下载并准备数据集到您的云存储。不要忘记使用先前定义的包含您的凭据的`storage_options`来写入私有云存储。
- en: 'The `download_and_prepare` method works in two steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`download_and_prepare`方法分两步进行：'
- en: it first downloads the raw data files (if any) in your local cache. You can
    set your cache directory by passing `cache_dir` to [load_dataset_builder()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset_builder)
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它会下载原始数据文件（如果有）到您的本地缓存中。您可以通过将`cache_dir`传递给[load_dataset_builder()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset_builder)来设置缓存目录
- en: then it generates the dataset in Arrow or Parquet format in your cloud storage
    by iterating over the raw data files.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过迭代原始数据文件，在您的云存储中生成Arrow或Parquet格式的数据集。
- en: 'Load a dataset builder from the Hugging Face Hub (see [how to load from the
    Hugging Face Hub](./loading#hugging-face-hub)):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hugging Face Hub加载数据集构建器（参见[如何从Hugging Face Hub加载](./loading#hugging-face-hub)）：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Load a dataset builder using a loading script (see [how to load a local loading
    script](./loading#local-loading-script)):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '使用加载脚本加载数据集构建器（参见[如何加载本地加载脚本](./loading#local-loading-script)）:'
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Use your own data files (see [how to load local and remote files](./loading#local-and-remote-files)):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您自己的数据文件（参见[如何加载本地和远程文件](./loading#local-and-remote-files)）：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is highly recommended to save the files as compressed Parquet files to optimize
    I/O by specifying `file_format="parquet"`. Otherwise the dataset is saved as an
    uncompressed Arrow file.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议将文件保存为压缩的Parquet文件，以通过指定`file_format="parquet"`来优化I/O。否则，数据集将保存为未压缩的Arrow文件。
- en: 'You can also specify the size of the shards using `max_shard_size` (default
    is 500MB):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`max_shard_size`指定碎片的大小（默认为500MB）：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Dask
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dask
- en: Dask is a parallel computing library and it has a pandas-like API for working
    with larger than memory Parquet datasets in parallel. Dask can use multiple threads
    or processes on a single machine, or a cluster of machines to process data in
    parallel. Dask supports local data but also data from a cloud storage.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Dask是一个并行计算库，它具有类似于pandas的API，用于并行处理大于内存的Parquet数据集。Dask可以在单台机器上使用多个线程或进程，或者在集群中使用多台机器并行处理数据。Dask支持本地数据，也支持来自云存储的数据。
- en: Therefore you can load a dataset saved as sharded Parquet files in Dask with
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以在Dask中加载保存为分片Parquet文件的数据集
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can find more about dask dataframes in their [documentation](https://docs.dask.org/en/stable/dataframe.html).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在它们的[文档](https://docs.dask.org/en/stable/dataframe.html)中找到更多关于dask数据框的信息。
- en: Saving serialized datasets
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存序列化数据集
- en: 'After you have processed your dataset, you can save it to your cloud storage
    with [Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完数据集后，您可以使用[Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)将其保存到云存储中：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Remember to define your credentials in your [FileSystem instance](#set-up-your-cloud-storage-filesystem)
    `fs` whenever you are interacting with a private cloud storage.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请记得在与私有云存储交互时，在您的[FileSystem实例](#set-up-your-cloud-storage-filesystem) `fs`中定义您的凭据。
- en: Listing serialized datasets
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列出序列化数据集
- en: 'List files from a cloud storage with your FileSystem instance `fs`, using `fs.ls`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您的FileSystem实例`fs`从云存储中列出文件，使用`fs.ls`：
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Load serialized datasets
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载序列化数据集
- en: 'When you are ready to use your dataset again, reload it with [Dataset.load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.load_from_disk):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当您准备再次使用数据集时，请使用[Dataset.load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.load_from_disk)重新加载：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
