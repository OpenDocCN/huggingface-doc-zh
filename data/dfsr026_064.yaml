- en: InstructPix2Pix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InstructPix2Pix
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/instructpix2pix](https://huggingface.co/docs/diffusers/training/instructpix2pix)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/training/instructpix2pix](https://huggingface.co/docs/diffusers/training/instructpix2pix)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[InstructPix2Pix](https://hf.co/papers/2211.09800) is a Stable Diffusion model
    trained to edit images from human-provided instructions. For example, your prompt
    can be â€œturn the clouds rainyâ€ and the model will edit the input image accordingly.
    This model is conditioned on the text prompt (or editing instruction) and the
    input image.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[InstructPix2Pix](https://hf.co/papers/2211.09800)æ˜¯ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ ¹æ®äººç±»æä¾›çš„æŒ‡ä»¤ç¼–è¾‘å›¾åƒã€‚ä¾‹å¦‚ï¼Œæ‚¨çš„æç¤ºå¯ä»¥æ˜¯â€œå°†äº‘å˜æˆé›¨â€ï¼Œæ¨¡å‹å°†ç›¸åº”åœ°ç¼–è¾‘è¾“å…¥å›¾åƒã€‚è¯¥æ¨¡å‹æ˜¯åŸºäºæ–‡æœ¬æç¤ºï¼ˆæˆ–ç¼–è¾‘æŒ‡ä»¤ï¼‰å’Œè¾“å…¥å›¾åƒçš„ã€‚'
- en: This guide will explore the [train_instruct_pix2pix.py](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)
    training script to help you become familiar with it, and how you can adapt it
    for your own use-case.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†æ¢ç´¢[train_instruct_pix2pix.py](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)è®­ç»ƒè„šæœ¬ï¼Œå¸®åŠ©æ‚¨ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•ä¸ºè‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œé€‚åº”ã€‚
- en: 'Before running the script, make sure you install the library from source:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä»æºä»£ç å®‰è£…åº“ï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then navigate to the example folder containing the training script and install
    the required dependencies for the script youâ€™re using:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè½¬åˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…æ‚¨æ­£åœ¨ä½¿ç”¨çš„è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ğŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. Itâ€™ll automatically configure your training setup based on your
    hardware and environment. Take a look at the ğŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateæ˜¯ä¸€ä¸ªå¸®åŠ©æ‚¨åœ¨å¤šä¸ªGPU/TPUä¸Šè¿›è¡Œè®­ç»ƒæˆ–ä½¿ç”¨æ··åˆç²¾åº¦çš„åº“ã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®æ‚¨çš„è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ğŸ¤— Accelerate
    [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour)ä»¥äº†è§£æ›´å¤šã€‚
- en: 'Initialize an ğŸ¤— Accelerate environment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ä¸€ä¸ªğŸ¤— Accelerateç¯å¢ƒï¼š
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To setup a default ğŸ¤— Accelerate environment without choosing any configurations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¾ç½®é»˜è®¤çš„ğŸ¤— Accelerateç¯å¢ƒè€Œä¸é€‰æ‹©ä»»ä½•é…ç½®ï¼š
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or if your environment doesnâ€™t support an interactive shell, like a notebook,
    you can use:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼shellï¼Œæ¯”å¦‚ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºé€‚ç”¨äºè®­ç»ƒè„šæœ¬çš„æ•°æ®é›†ã€‚
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesnâ€™t cover every aspect of the script
    in detail. If youâ€™re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)
    and let us know if you have any questions or concerns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹éƒ¨åˆ†çªå‡ºäº†è®­ç»ƒè„šæœ¬çš„é‡è¦éƒ¨åˆ†ï¼Œä»¥å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¿®æ”¹å®ƒï¼Œä½†å¹¶æœªè¯¦ç»†æ¶µç›–è„šæœ¬çš„æ¯ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·éšæ—¶é˜…è¯»[è„šæœ¬](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬æ‚¨æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ã€‚
- en: Script parameters
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è„šæœ¬å‚æ•°
- en: The training script has many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L65)
    function. Default values are provided for most parameters that work pretty well,
    but you can also set your own values in the training command if youâ€™d like.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬æœ‰è®¸å¤šå‚æ•°å¯å¸®åŠ©æ‚¨è‡ªå®šä¹‰è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰å‚æ•°åŠå…¶æè¿°éƒ½å¯ä»¥åœ¨[`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L65)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚å¤§å¤šæ•°å‚æ•°éƒ½æä¾›äº†é»˜è®¤å€¼ï¼Œæ•ˆæœç›¸å½“ä¸é”™ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚
- en: 'For example, to increase the resolution of the input image:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦å¢åŠ è¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡ï¼š
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Many of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters)
    training guide, so this guide just focuses on the relevant parameters for InstructPix2Pix:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šåŸºæœ¬å’Œé‡è¦çš„å‚æ•°åœ¨[æ–‡æœ¬åˆ°å›¾åƒ](text2image#script-parameters)è®­ç»ƒæŒ‡å—ä¸­æœ‰æè¿°ï¼Œå› æ­¤æœ¬æŒ‡å—åªå…³æ³¨InstructPix2Pixç›¸å…³å‚æ•°ï¼š
- en: '`--original_image_column`: the original image before the edits are made'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--original_image_column`ï¼šç¼–è¾‘å‰çš„åŸå§‹å›¾åƒ'
- en: '`--edited_image_column`: the image after the edits are made'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--edited_image_column`ï¼šç¼–è¾‘å®Œæˆåçš„å›¾åƒ'
- en: '`--edit_prompt_column`: the instructions to edit the image'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--edit_prompt_column`ï¼šç¼–è¾‘å›¾åƒçš„æŒ‡ä»¤'
- en: '`--conditioning_dropout_prob`: the dropout probability for the edited image
    and edit prompts during training which enables classifier-free guidance (CFG)
    for one or both conditioning inputs'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--conditioning_dropout_prob`ï¼šåœ¨è®­ç»ƒæœŸé—´ç”¨äºç¼–è¾‘å›¾åƒå’Œç¼–è¾‘æç¤ºçš„ä¸¢å¤±æ¦‚ç‡ï¼Œä»è€Œå®ç°æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆCFGï¼‰ç”¨äºä¸€ä¸ªæˆ–ä¸¤ä¸ªè°ƒèŠ‚è¾“å…¥'
- en: Training script
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬
- en: The dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L374)
    function. This is where youâ€™ll make your changes to the training script to adapt
    it for your own use-case.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯å¯ä»¥åœ¨[`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L374)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚è¿™æ˜¯æ‚¨å°†å¯¹è®­ç»ƒè„šæœ¬è¿›è¡Œæ›´æ”¹ä»¥é€‚åº”è‡ªå·±ç”¨ä¾‹çš„åœ°æ–¹ã€‚
- en: As with the script parameters, a walkthrough of the training script is provided
    in the [Text-to-image](text2image#training-script) training guide. Instead, this
    guide takes a look at the InstructPix2Pix relevant parts of the script.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è„šæœ¬å‚æ•°ä¸€æ ·ï¼Œåœ¨[æ–‡æœ¬åˆ°å›¾åƒ](text2image#training-script)è®­ç»ƒæŒ‡å—ä¸­æä¾›äº†è®­ç»ƒè„šæœ¬çš„è¯¦ç»†è¯´æ˜ã€‚ç›¸åï¼Œæœ¬æŒ‡å—å°†é‡ç‚¹ä»‹ç»è„šæœ¬ä¸­ä¸InstructPix2Pixç›¸å…³çš„éƒ¨åˆ†ã€‚
- en: 'The script begins by modifing the [number of input channels](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L445)
    in the first convolutional layer of the UNet to account for InstructPix2Pixâ€™s
    additional conditioning image:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬é¦–å…ˆé€šè¿‡ä¿®æ”¹UNetçš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å…¥é€šé“æ•°é‡æ¥è€ƒè™‘InstructPix2Pixçš„é¢å¤–è°ƒèŠ‚å›¾åƒï¼š
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'These UNet parameters are [updated](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L545C1-L551C6)
    by the optimizer:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›UNetå‚æ•°ç”±ä¼˜åŒ–å™¨è¿›è¡Œäº†æ›´æ–°ï¼š
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, the edited images and and edit instructions are [preprocessed](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L624)
    and [tokenized](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L610C24-L610C24).
    It is important the same image transformations are applied to the original and
    edited images.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œç¼–è¾‘åçš„å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤è¢«é¢„å¤„ç†å¹¶è¿›è¡Œäº†æ ‡è®°åŒ–ã€‚é‡è¦çš„æ˜¯å¯¹åŸå§‹å›¾åƒå’Œç¼–è¾‘åçš„å›¾åƒåº”ç”¨ç›¸åŒçš„å›¾åƒè½¬æ¢ã€‚
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, in the [training loop](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L730),
    it starts by encoding the edited images into latent space:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L730)ä¸­ï¼Œå®ƒé¦–å…ˆå°†ç¼–è¾‘åçš„å›¾åƒç¼–ç ä¸ºæ½œåœ¨ç©ºé—´ï¼š
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, the script applies dropout to the original image and edit instruction
    embeddings to support CFG. This is what enables the model to modulate the influence
    of the edit instruction and original image on the edited image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè„šæœ¬å¯¹åŸå§‹å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤åµŒå…¥åº”ç”¨äº†è¾å­¦ä»¥æ”¯æŒCFGã€‚è¿™å°±æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿè°ƒèŠ‚ç¼–è¾‘æŒ‡ä»¤å’ŒåŸå§‹å›¾åƒå¯¹ç¼–è¾‘åå›¾åƒçš„å½±å“çš„åŸå› ã€‚
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Thatâ€™s pretty much it! Aside from the differences described here, the rest of
    the script is very similar to the [Text-to-image](text2image#training-script)
    training script, so feel free to check it out for more details. If you want to
    learn more about how the training loop works, check out the [Understanding pipelines,
    models and schedulers](../using-diffusers/write_own_pipeline) tutorial which breaks
    down the basic pattern of the denoising process.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼é™¤äº†è¿™é‡Œæè¿°çš„å·®å¼‚ä¹‹å¤–ï¼Œå…¶ä½™éƒ¨åˆ†çš„è„šæœ¬ä¸[Text-to-image](text2image#training-script)è®­ç»ƒè„šæœ¬éå¸¸ç›¸ä¼¼ï¼Œæ‰€ä»¥è¯·éšæ—¶æŸ¥çœ‹æ›´å¤šç»†èŠ‚ã€‚å¦‚æœæ‚¨æƒ³äº†è§£è®­ç»ƒå¾ªç¯çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹[ç†è§£ç®¡é“ã€æ¨¡å‹å’Œè°ƒåº¦å™¨](../using-diffusers/write_own_pipeline)æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹è¯¦ç»†ä»‹ç»äº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚
- en: Launch the script
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯åŠ¨è„šæœ¬
- en: Once youâ€™re happy with the changes to your script or if youâ€™re okay with the
    default configuration, youâ€™re ready to launch the training script! ğŸš€
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å¯¹è„šæœ¬çš„æ›´æ”¹æ»¡æ„ï¼Œæˆ–è€…å¦‚æœæ‚¨å¯¹é»˜è®¤é…ç½®æ»¡æ„ï¼Œæ‚¨å°±å¯ä»¥å¯åŠ¨è®­ç»ƒè„šæœ¬äº†ï¼ğŸš€
- en: This guide uses the [fusing/instructpix2pix-1000-samples](https://huggingface.co/datasets/fusing/instructpix2pix-1000-samples)
    dataset, which is a smaller version of the [original dataset](https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered).
    You can also create and use your own dataset if youâ€™d like (see the [Create a
    dataset for training](create_dataset) guide).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—ä½¿ç”¨[fusing/instructpix2pix-1000-samples](https://huggingface.co/datasets/fusing/instructpix2pix-1000-samples)æ•°æ®é›†ï¼Œè¿™æ˜¯[åŸå§‹æ•°æ®é›†](https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered)çš„ä¸€ä¸ªè¾ƒå°ç‰ˆæœ¬ã€‚å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºå’Œä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ï¼ˆè¯·å‚é˜…[åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset)æŒ‡å—ï¼‰ã€‚
- en: Set the `MODEL_NAME` environment variable to the name of the model (can be a
    model id on the Hub or a path to a local model), and the `DATASET_ID` to the name
    of the dataset on the Hub. The script creates and saves all the components (feature
    extractor, scheduler, text encoder, UNet, etc.) to a subfolder in your repository.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`MODEL_NAME`ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºæ¨¡å‹çš„åç§°ï¼ˆå¯ä»¥æ˜¯Hubä¸Šçš„æ¨¡å‹IDæˆ–æœ¬åœ°æ¨¡å‹çš„è·¯å¾„ï¼‰ï¼Œå°†`DATASET_ID`è®¾ç½®ä¸ºHubä¸Šæ•°æ®é›†çš„åç§°ã€‚è„šæœ¬å°†åˆ›å»ºå¹¶ä¿å­˜æ‰€æœ‰ç»„ä»¶ï¼ˆç‰¹å¾æå–å™¨ã€è°ƒåº¦å™¨ã€æ–‡æœ¬ç¼–ç å™¨ã€UNetç­‰ï¼‰åˆ°æ‚¨çš„å­˜å‚¨åº“çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚
- en: For better results, try longer training runs with a larger dataset. Weâ€™ve only
    tested this training script on a smaller-scale dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œè¯·å°è¯•ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†è¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒã€‚æˆ‘ä»¬åªåœ¨è¾ƒå°è§„æ¨¡çš„æ•°æ®é›†ä¸Šæµ‹è¯•äº†è¿™ä¸ªè®­ç»ƒè„šæœ¬ã€‚
- en: To monitor training progress with Weights and Biases, add the `--report_to=wandb`
    parameter to the training command and specify a validation image with `--val_image_url`
    and a validation prompt with `--validation_prompt`. This can be really useful
    for debugging the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨Weights and Biasesç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œè¯·åœ¨è®­ç»ƒå‘½ä»¤ä¸­æ·»åŠ `--report_to=wandb`å‚æ•°ï¼Œå¹¶ä½¿ç”¨`--val_image_url`æŒ‡å®šä¸€ä¸ªéªŒè¯å›¾åƒå’Œ`--validation_prompt`æŒ‡å®šä¸€ä¸ªéªŒè¯æç¤ºã€‚è¿™å¯¹äºè°ƒè¯•æ¨¡å‹éå¸¸æœ‰ç”¨ã€‚
- en: If youâ€™re training on more than one GPU, add the `--multi_gpu` parameter to
    the `accelerate launch` command.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨å¤šä¸ªGPUä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯·åœ¨`accelerate launch`å‘½ä»¤ä¸­æ·»åŠ `--multi_gpu`å‚æ•°ã€‚
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After training is finished, you can use your new InstructPix2Pix for inference:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°çš„InstructPix2Pixè¿›è¡Œæ¨ç†ï¼š
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You should experiment with different `num_inference_steps`, `image_guidance_scale`,
    and `guidance_scale` values to see how they affect inference speed and quality.
    The guidance scale parameters are especially impactful because they control how
    much the original image and edit instructions affect the edited image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åº”è¯¥å°è¯•ä¸åŒçš„`num_inference_steps`ã€`image_guidance_scale`å’Œ`guidance_scale`å€¼ï¼Œä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ¨ç†é€Ÿåº¦å’Œè´¨é‡ã€‚æŒ‡å¯¼æ¯”ä¾‹å‚æ•°å°¤å…¶é‡è¦ï¼Œå› ä¸ºå®ƒä»¬æ§åˆ¶åŸå§‹å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤å¯¹ç¼–è¾‘åå›¾åƒçš„å½±å“ç¨‹åº¦ã€‚
- en: Stable Diffusion XL
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¨³å®šçš„Diffusion XL
- en: Stable Diffusion XL (SDXL) is a powerful text-to-image model that generates
    high-resolution images, and it adds a second text-encoder to its architecture.
    Use the [`train_instruct_pix2pix_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix_sdxl.py)
    script to train a SDXL model to follow image editing instructions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£XLï¼ˆSDXLï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶åœ¨å…¶æ¶æ„ä¸­æ·»åŠ äº†ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨ã€‚ä½¿ç”¨[`train_instruct_pix2pix_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix_sdxl.py)è„šæœ¬æ¥è®­ç»ƒä¸€ä¸ªSDXLæ¨¡å‹ï¼Œä»¥éµå¾ªå›¾åƒç¼–è¾‘æŒ‡ä»¤ã€‚
- en: The SDXL training script is discussed in more detail in the [SDXL training](sdxl)
    guide.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SDXLè®­ç»ƒè„šæœ¬åœ¨[SDXLè®­ç»ƒ](sdxl)æŒ‡å—ä¸­æœ‰æ›´è¯¦ç»†çš„è®¨è®ºã€‚
- en: Next steps
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'Congratulations on training your own InstructPix2Pix model! ğŸ¥³ To learn more
    about the model, it may be helpful to:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œæ‚¨è®­ç»ƒè‡ªå·±çš„InstructPix2Pixæ¨¡å‹ï¼ğŸ¥³ è¦äº†è§£æ›´å¤šå…³äºè¯¥æ¨¡å‹çš„ä¿¡æ¯ï¼Œå¯èƒ½æœ‰åŠ©äºï¼š
- en: Read the [Instruction-tuning Stable Diffusion with InstructPix2Pix](https://huggingface.co/blog/instruction-tuning-sd)
    blog post to learn more about some experiments weâ€™ve done with InstructPix2Pix,
    dataset preparation, and results for different instructions.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜…è¯»[Instruction-tuning Stable Diffusion with InstructPix2Pix](https://huggingface.co/blog/instruction-tuning-sd)åšå®¢æ–‡ç« ï¼Œäº†è§£æˆ‘ä»¬å¯¹InstructPix2Pixã€æ•°æ®é›†å‡†å¤‡ä»¥åŠä¸åŒæŒ‡ä»¤çš„å®éªŒç»“æœçš„æ›´å¤šä¿¡æ¯ã€‚
