- en: InstructPix2Pix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InstructPix2Pix
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/instructpix2pix](https://huggingface.co/docs/diffusers/training/instructpix2pix)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/training/instructpix2pix](https://huggingface.co/docs/diffusers/training/instructpix2pix)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[InstructPix2Pix](https://hf.co/papers/2211.09800) is a Stable Diffusion model
    trained to edit images from human-provided instructions. For example, your prompt
    can be “turn the clouds rainy” and the model will edit the input image accordingly.
    This model is conditioned on the text prompt (or editing instruction) and the
    input image.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[InstructPix2Pix](https://hf.co/papers/2211.09800)是一个经过训练的稳定扩散模型，用于根据人类提供的指令编辑图像。例如，您的提示可以是“将云变成雨”，模型将相应地编辑输入图像。该模型是基于文本提示（或编辑指令）和输入图像的。'
- en: This guide will explore the [train_instruct_pix2pix.py](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)
    training script to help you become familiar with it, and how you can adapt it
    for your own use-case.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将探索[train_instruct_pix2pix.py](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)训练脚本，帮助您熟悉它，以及如何为自己的用例进行适应。
- en: 'Before running the script, make sure you install the library from source:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行脚本之前，请确保从源代码安装库：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then navigate to the example folder containing the training script and install
    the required dependencies for the script you’re using:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后转到包含训练脚本的示例文件夹，并安装您正在使用的脚本所需的依赖项：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. It’ll automatically configure your training setup based on your
    hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Accelerate是一个帮助您在多个GPU/TPU上进行训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate
    [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多。
- en: 'Initialize an 🤗 Accelerate environment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个🤗 Accelerate环境：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To setup a default 🤗 Accelerate environment without choosing any configurations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置默认的🤗 Accelerate环境而不选择任何配置：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or if your environment doesn’t support an interactive shell, like a notebook,
    you can use:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您的环境不支持交互式shell，比如笔记本，您可以使用：
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您想在自己的数据集上训练模型，请查看[创建用于训练的数据集](create_dataset)指南，了解如何创建适用于训练脚本的数据集。
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesn’t cover every aspect of the script
    in detail. If you’re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)
    and let us know if you have any questions or concerns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分突出了训练脚本的重要部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)，并告诉我们您是否有任何问题或疑虑。
- en: Script parameters
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚本参数
- en: The training script has many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L65)
    function. Default values are provided for most parameters that work pretty well,
    but you can also set your own values in the training command if you’d like.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本有许多参数可帮助您自定义训练运行。所有参数及其描述都可以在[`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L65)函数中找到。大多数参数都提供了默认值，效果相当不错，但如果您愿意，也可以在训练命令中设置自己的值。
- en: 'For example, to increase the resolution of the input image:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要增加输入图像的分辨率：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Many of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters)
    training guide, so this guide just focuses on the relevant parameters for InstructPix2Pix:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基本和重要的参数在[文本到图像](text2image#script-parameters)训练指南中有描述，因此本指南只关注InstructPix2Pix相关参数：
- en: '`--original_image_column`: the original image before the edits are made'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--original_image_column`：编辑前的原始图像'
- en: '`--edited_image_column`: the image after the edits are made'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--edited_image_column`：编辑完成后的图像'
- en: '`--edit_prompt_column`: the instructions to edit the image'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--edit_prompt_column`：编辑图像的指令'
- en: '`--conditioning_dropout_prob`: the dropout probability for the edited image
    and edit prompts during training which enables classifier-free guidance (CFG)
    for one or both conditioning inputs'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--conditioning_dropout_prob`：在训练期间用于编辑图像和编辑提示的丢失概率，从而实现无分类器指导（CFG）用于一个或两个调节输入'
- en: Training script
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练脚本
- en: The dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L374)
    function. This is where you’ll make your changes to the training script to adapt
    it for your own use-case.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集预处理代码和训练循环可以在[`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L374)函数中找到。这是您将对训练脚本进行更改以适应自己用例的地方。
- en: As with the script parameters, a walkthrough of the training script is provided
    in the [Text-to-image](text2image#training-script) training guide. Instead, this
    guide takes a look at the InstructPix2Pix relevant parts of the script.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与脚本参数一样，在[文本到图像](text2image#training-script)训练指南中提供了训练脚本的详细说明。相反，本指南将重点介绍脚本中与InstructPix2Pix相关的部分。
- en: 'The script begins by modifing the [number of input channels](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L445)
    in the first convolutional layer of the UNet to account for InstructPix2Pix’s
    additional conditioning image:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本首先通过修改UNet的第一个卷积层的输入通道数量来考虑InstructPix2Pix的额外调节图像：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'These UNet parameters are [updated](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L545C1-L551C6)
    by the optimizer:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些UNet参数由优化器进行了更新：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, the edited images and and edit instructions are [preprocessed](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L624)
    and [tokenized](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L610C24-L610C24).
    It is important the same image transformations are applied to the original and
    edited images.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，编辑后的图像和编辑指令被预处理并进行了标记化。重要的是对原始图像和编辑后的图像应用相同的图像转换。
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, in the [training loop](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L730),
    it starts by encoding the edited images into latent space:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[训练循环](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L730)中，它首先将编辑后的图像编码为潜在空间：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, the script applies dropout to the original image and edit instruction
    embeddings to support CFG. This is what enables the model to modulate the influence
    of the edit instruction and original image on the edited image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，脚本对原始图像和编辑指令嵌入应用了辍学以支持CFG。这就是使模型能够调节编辑指令和原始图像对编辑后图像的影响的原因。
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That’s pretty much it! Aside from the differences described here, the rest of
    the script is very similar to the [Text-to-image](text2image#training-script)
    training script, so feel free to check it out for more details. If you want to
    learn more about how the training loop works, check out the [Understanding pipelines,
    models and schedulers](../using-diffusers/write_own_pipeline) tutorial which breaks
    down the basic pattern of the denoising process.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！除了这里描述的差异之外，其余部分的脚本与[Text-to-image](text2image#training-script)训练脚本非常相似，所以请随时查看更多细节。如果您想了解训练循环的工作原理，请查看[理解管道、模型和调度器](../using-diffusers/write_own_pipeline)教程，该教程详细介绍了去噪过程的基本模式。
- en: Launch the script
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动脚本
- en: Once you’re happy with the changes to your script or if you’re okay with the
    default configuration, you’re ready to launch the training script! 🚀
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您对脚本的更改满意，或者如果您对默认配置满意，您就可以启动训练脚本了！🚀
- en: This guide uses the [fusing/instructpix2pix-1000-samples](https://huggingface.co/datasets/fusing/instructpix2pix-1000-samples)
    dataset, which is a smaller version of the [original dataset](https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered).
    You can also create and use your own dataset if you’d like (see the [Create a
    dataset for training](create_dataset) guide).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南使用[fusing/instructpix2pix-1000-samples](https://huggingface.co/datasets/fusing/instructpix2pix-1000-samples)数据集，这是[原始数据集](https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered)的一个较小版本。如果您愿意，您也可以创建和使用自己的数据集（请参阅[创建用于训练的数据集](create_dataset)指南）。
- en: Set the `MODEL_NAME` environment variable to the name of the model (can be a
    model id on the Hub or a path to a local model), and the `DATASET_ID` to the name
    of the dataset on the Hub. The script creates and saves all the components (feature
    extractor, scheduler, text encoder, UNet, etc.) to a subfolder in your repository.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将`MODEL_NAME`环境变量设置为模型的名称（可以是Hub上的模型ID或本地模型的路径），将`DATASET_ID`设置为Hub上数据集的名称。脚本将创建并保存所有组件（特征提取器、调度器、文本编码器、UNet等）到您的存储库的子文件夹中。
- en: For better results, try longer training runs with a larger dataset. We’ve only
    tested this training script on a smaller-scale dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的结果，请尝试使用更大的数据集进行更长时间的训练。我们只在较小规模的数据集上测试了这个训练脚本。
- en: To monitor training progress with Weights and Biases, add the `--report_to=wandb`
    parameter to the training command and specify a validation image with `--val_image_url`
    and a validation prompt with `--validation_prompt`. This can be really useful
    for debugging the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Weights and Biases监控训练进度，请在训练命令中添加`--report_to=wandb`参数，并使用`--val_image_url`指定一个验证图像和`--validation_prompt`指定一个验证提示。这对于调试模型非常有用。
- en: If you’re training on more than one GPU, add the `--multi_gpu` parameter to
    the `accelerate launch` command.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在多个GPU上进行训练，请在`accelerate launch`命令中添加`--multi_gpu`参数。
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After training is finished, you can use your new InstructPix2Pix for inference:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，您可以使用新的InstructPix2Pix进行推理：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You should experiment with different `num_inference_steps`, `image_guidance_scale`,
    and `guidance_scale` values to see how they affect inference speed and quality.
    The guidance scale parameters are especially impactful because they control how
    much the original image and edit instructions affect the edited image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该尝试不同的`num_inference_steps`、`image_guidance_scale`和`guidance_scale`值，以查看它们如何影响推理速度和质量。指导比例参数尤其重要，因为它们控制原始图像和编辑指令对编辑后图像的影响程度。
- en: Stable Diffusion XL
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定的Diffusion XL
- en: Stable Diffusion XL (SDXL) is a powerful text-to-image model that generates
    high-resolution images, and it adds a second text-encoder to its architecture.
    Use the [`train_instruct_pix2pix_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix_sdxl.py)
    script to train a SDXL model to follow image editing instructions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散XL（SDXL）是一个强大的文本到图像模型，可以生成高分辨率图像，并在其架构中添加了第二个文本编码器。使用[`train_instruct_pix2pix_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix_sdxl.py)脚本来训练一个SDXL模型，以遵循图像编辑指令。
- en: The SDXL training script is discussed in more detail in the [SDXL training](sdxl)
    guide.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL训练脚本在[SDXL训练](sdxl)指南中有更详细的讨论。
- en: Next steps
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'Congratulations on training your own InstructPix2Pix model! 🥳 To learn more
    about the model, it may be helpful to:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您训练自己的InstructPix2Pix模型！🥳 要了解更多关于该模型的信息，可能有助于：
- en: Read the [Instruction-tuning Stable Diffusion with InstructPix2Pix](https://huggingface.co/blog/instruction-tuning-sd)
    blog post to learn more about some experiments we’ve done with InstructPix2Pix,
    dataset preparation, and results for different instructions.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读[Instruction-tuning Stable Diffusion with InstructPix2Pix](https://huggingface.co/blog/instruction-tuning-sd)博客文章，了解我们对InstructPix2Pix、数据集准备以及不同指令的实验结果的更多信息。
