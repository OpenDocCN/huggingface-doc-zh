- en: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym 🤖
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Panda-Gym 进行机器人模拟的 Advantage Actor Critic (A2C) 🤖
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/hands-on](https://huggingface.co/learn/deep-rl-course/unit6/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/learn/deep-rl-course/unit6/hands-on](https://huggingface.co/learn/deep-rl-course/unit6/hands-on)
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![提问](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![在 Colab 中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb)'
- en: 'Now that you’ve studied the theory behind Advantage Actor Critic (A2C), **you’re
    ready to train your A2C agent** using Stable-Baselines3 in a robotic environment.
    And train a:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学习了 Advantage Actor Critic (A2C) 的理论，**您已经准备好训练您的 A2C 代理**，使用 Stable-Baselines3
    在一个机器人环境中进行训练。并训练一个：
- en: A robotic arm 🦾 to move to the correct position.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个机器人手臂🦾移动到正确的位置。
- en: We’re going to use
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用
- en: '[panda-gym](https://github.com/qgallouedec/panda-gym)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[panda-gym](https://github.com/qgallouedec/panda-gym)'
- en: 'To validate this hands-on for the certification process, you need to push your
    two trained models to the Hub and get the following results:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这个实践过程的认证，您需要将您的两个训练模型推送到Hub并获得以下结果：
- en: '`PandaReachDense-v3` get a result of >= -3.5.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PandaReachDense-v3` 获得结果 >= -3.5。'
- en: To find your result, [go to the leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到您的结果，请[转到排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)并找到您的模型，**结果
    = 平均奖励 - 奖励的标准差**
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有关认证过程的更多信息，请查看此部分👉[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: '**To start the hands-on click on Open In Colab button** 👇 :'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**点击“在 Colab 中打开”按钮开始实践**👇：'
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![在 Colab 中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)'
- en: 'Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym
    🤖'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 单元：使用 Panda-Gym 进行机器人模拟的 Advantage Actor Critic (A2C) 🤖
- en: '🎮 Environments:'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 🎮 环境：
- en: '[Panda-Gym](https://github.com/qgallouedec/panda-gym)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Panda-Gym](https://github.com/qgallouedec/panda-gym)'
- en: '📚 RL-Library:'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 📚 强化学习库：
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)'
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断努力改进我们的教程，所以**如果您在本笔记本中发现问题**，请[在 GitHub 仓库上提出问题](https://github.com/huggingface/deep-rl-class/issues)。
- en: Objectives of this notebook 🏆
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本笔记本的目标🏆
- en: 'At the end of the notebook, you will:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的末尾，您将：
- en: Be able to use **Panda-Gym**, the environment library.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用**Panda-Gym**，这个环境库。
- en: Be able to **train robots using A2C**.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**使用 A2C 训练机器人**。
- en: Understand why **we need to normalize the input**.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解为什么**我们需要对输入进行归一化**。
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score 🔥.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**将您训练好的代理和代码推送到Hub**，并附带一个漂亮的视频回放和评估分数🔥。
- en: Prerequisites 🏗️
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件🏗️
- en: 'Before diving into the notebook, you need to:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究笔记本之前，您需要：
- en: 🔲 📚 Study [Actor-Critic methods by reading Unit 6](https://huggingface.co/deep-rl-course/unit6/introduction)
    🤗
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 🔲 📚 学习 [通过阅读第 6 单元的 Actor-Critic 方法](https://huggingface.co/deep-rl-course/unit6/introduction)
    🤗
- en: Let’s train our first robots 🤖
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们训练我们的第一个机器人🤖
- en: Set the GPU 💪
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 GPU 💪
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了**加速代理的训练，我们将使用 GPU**。为此，请转到 `运行时 > 更改运行时类型`
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![GPU 步骤 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
- en: '`Hardware Accelerator > GPU`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`硬件加速器 > GPU`'
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![GPU 步骤 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
- en: Create a virtual display 🔽
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个虚拟显示🔽
- en: During the notebook, we’ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们需要生成一个回放视频。为此，在 colab 中，**我们需要有一个虚拟屏幕来渲染环境**（从而记录帧）。
- en: The following cell will install the librairies and create and run a virtual
    screen 🖥
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下单元格将安装库并创建并运行一个虚拟屏幕🖥
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install dependencies 🔽
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装依赖🔽
- en: 'We’ll install multiple ones:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装多个：
- en: '`gymnasium`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gymnasium`'
- en: '`panda-gym`: Contains the robotics arm environments.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`panda-gym`：包含机器人手臂环境。'
- en: '`stable-baselines3`: The SB3 deep reinforcement learning library.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stable-baselines3`：SB3 深度强化学习库。'
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face 🤗 Hub.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_sb3`：用于 Stable-baselines3 的额外代码，用于从 Hugging Face 🤗 Hub 加载和上传模型。'
- en: '`huggingface_hub`: Library allowing anyone to work with the Hub repositories.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_hub`：允许任何人使用Hub存储库。'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Import the packages 📦
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入包📦
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PandaReachDense-v3 🦾
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PandaReachDense-v3 🦾
- en: The agent we’re going to train is a robotic arm that needs to do controls (moving
    the arm and using the end-effector).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练的代理是一个需要进行控制的机器人手臂（移动手臂并使用末端执行器）。
- en: In robotics, the *end-effector* is the device at the end of a robotic arm designed
    to interact with the environment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人技术中，*末端执行器*是设计用于与环境交互的机器人手臂末端的设备。
- en: In `PandaReach`, the robot must place its end-effector at a target position
    (green ball).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `PandaReach` 中，机器人必须将其末端执行器放置在目标位置（绿色球）。
- en: We’re going to use the dense version of this environment. It means we’ll get
    a *dense reward function* that **will provide a reward at each timestep** (the
    closer the agent is to completing the task, the higher the reward). Contrary to
    a *sparse reward function* where the environment **return a reward if and only
    if the task is completed**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用此环境的密集版本。这意味着我们将获得一个*密集奖励函数*，**将在每个时间步提供奖励** (代理越接近完成任务，奖励越高)。与*稀疏奖励函数*相反，环境**仅在任务完成时返回奖励**。
- en: Also, we’re going to use the *End-effector displacement control*, it means the
    **action corresponds to the displacement of the end-effector**. We don’t control
    the individual motion of each joint (joint control).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用 *末端执行器位移控制*，这意味着**动作对应于末端执行器的位移**。我们不控制每个关节的单独运动 (关节控制)。
- en: '![Robotics](../Images/d79d62b53f91999defb0b4eae0db003d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![机器人技术](../Images/d79d62b53f91999defb0b4eae0db003d.png)'
- en: This way **the training will be easier**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这样**训练将更容易**。
- en: Create the environment
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建环境
- en: The environment 🎮
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境 🎮
- en: In `PandaReachDense-v3` the robotic arm must place its end-effector at a target
    position (green ball).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `PandaReachDense-v3` 中，机械臂必须将其末端执行器放置在目标位置 (绿色球)。
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The observation space **is a dictionary with 3 different elements**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 观察空间**是一个具有 3 个不同元素的字典**：
- en: '`achieved_goal`: (x,y,z) position of the goal.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`achieved_goal`: 目标的位置 (x,y,z)。'
- en: '`desired_goal`: (x,y,z) distance between the goal position and the current
    object position.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`desired_goal`: 目标位置与当前物体位置之间的距离 (x,y,z)。'
- en: '`observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation`: 末端执行器的位置 (x,y,z) 和速度 (vx, vy, vz)。'
- en: Given it’s a dictionary as observation, **we will need to use a MultiInputPolicy
    policy instead of MlpPolicy**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于观察是一个字典，**我们需要使用 MultiInputPolicy 策略而不是 MlpPolicy**。
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The action space is a vector with 3 values:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 动作空间是一个具有 3 个值的向量：
- en: Control x, y, z movement
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制 x、y、z 运动
- en: Normalize observation and rewards
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 归一化观察和奖励
- en: A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中的一个好的实践是[归一化输入特征](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)。
- en: For that purpose, there is a wrapper that will compute a running average and
    standard deviation of input features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，有一个包装器将计算输入特征的运行平均值和标准差。
- en: We also normalize rewards with this same wrapper by adding `norm_reward = True`
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过添加 `norm_reward = True` 来使用相同的包装器对奖励进行归一化
- en: '[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[您应该查看文档以填写此单元格](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)'
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Solution
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Create the A2C Model 🤖
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 A2C 模型 🤖
- en: 'For more information about A2C implementation with StableBaselines3 check:
    [https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用 StableBaselines3 实现 A2C 的更多信息，请查看：[https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes)
- en: To find the best parameters I checked the [official trained agents by Stable-Baselines3
    team](https://huggingface.co/sb3).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳参数，我检查了[由 Stable-Baselines3 团队官方训练的代理](https://huggingface.co/sb3)。
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Solution
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Train the A2C agent 🏃
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 A2C 代理 🏃
- en: Let’s train our agent for 1,000,000 timesteps, don’t forget to use GPU on Colab.
    It will take approximately ~25-40min
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们对代理进行 1,000,000 个时间步的训练，不要忘记在 Colab 上使用 GPU。大约需要 25-40 分钟
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Evaluate the agent 📈
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估代理 📈
- en: Now that’s our agent is trained, we need to **check its performance**.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们的代理已经训练好了，我们需要**检查其性能**。
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stable-Baselines3 提供了一个方法来做到这一点：`evaluate_policy`
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Publish your trained model on the Hub 🔥
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Hub 上发布您训练好的模型 🔥
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the Hub with one line of code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到训练后取得了良好的结果，我们可以用一行代码将我们训练好的模型发布到 Hub 上。
- en: 📚 The libraries documentation 👉 [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face—x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 📚 图书馆文档 👉 [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face—x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
- en: By using `package_to_hub`, as we already mentionned in the former units, **you
    evaluate, record a replay, generate a model card of your agent and push it to
    the hub**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `package_to_hub`，正如我们在前面的单元中已经提到的，**您可以评估、记录重播、生成代理的模型卡并将其推送到 Hub**。
- en: 'This way:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这样：
- en: You can **showcase our work** 🔥
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以 **展示我们的工作** 🔥
- en: You can **visualize your agent playing** 👀
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以 **查看您的代理进行游戏** 👀
- en: You can **share with the community an agent that others can use** 💾
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以 **与社区分享其他人可以使用的代理** 💾
- en: You can **access a leaderboard 🏆 to see how well your agent is performing compared
    to your classmates** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以 **访问排行榜 🏆 查看您的代理相对于同学表现如何** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够与社区分享您的模型，还需要遵循三个步骤：
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ (如果尚未完成) 创建一个 HF 帐户 ➡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2️⃣ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 登录，然后，您需要从 Hugging Face 网站存储您的身份验证令牌。
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的令牌 ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **具有写入权限**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![创建 HF 令牌](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: Copy the token
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制令牌
- en: Run the cell below and paste the token
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行下面的单元格并粘贴令牌
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you don’t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用 Google Colab 或 Jupyter Notebook，您需要使用这个命令：`huggingface-cli login`
- en: 3️⃣ We’re now ready to push our trained agent to the 🤗 Hub 🔥 using `package_to_hub()`
    function. For this environment, **running this cell can take approximately 10min**
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 现在我们准备使用 `package_to_hub()` 函数将我们训练好的 agent 推送到 🤗 Hub 🔥。对于这个环境，**运行这个单元格可能需要大约
    10 分钟**。
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Some additional challenges 🏆
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些额外的挑战 🏆
- en: The best way to learn **is to try things by your own**! Why not trying `PandaPickAndPlace-v3`?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的最好方法 **是自己尝试**！为什么不尝试 `PandaPickAndPlace-v3`？
- en: 'If you want to try more advanced tasks for panda-gym, you need to check what
    was done using **TQC or SAC** (a more sample-efficient algorithm suited for robotics
    tasks). In real robotics, you’ll use a more sample-efficient algorithm for a simple
    reason: contrary to a simulation **if you move your robotic arm too much, you
    have a risk of breaking it**.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想尝试更高级的任务，您需要检查使用 **TQC 或 SAC**（适用于机器人任务的更高效的算法）完成了什么。在真实的机器人技术中，您将使用更高效的算法，一个简单的原因是：与模拟相反，**如果您移动机器人手臂太多，您有破坏的风险**。
- en: 'PandaPickAndPlace-v1 (this model uses the v1 version of the environment): [https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1](https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PandaPickAndPlace-v1（此模型使用环境的 v1 版本）：[https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1](https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1)
- en: 'And don’t hesitate to check panda-gym documentation here: [https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html](https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 并且不要犹豫查看 panda-gym 文档：[https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html](https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html)
- en: 'We provide you the steps to train another agent (optional):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为您提供了训练另一个 agent 的步骤（可选）：
- en: Define the environment called “PandaPickAndPlace-v3”
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义名为 “PandaPickAndPlace-v3” 的环境
- en: Make a vectorized environment
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个矢量化环境
- en: Add a wrapper to normalize the observations and rewards. [Check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个包装器来规范观察和奖励。[查看文档](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
- en: Create the A2C Model (don’t forget verbose=1 to print the training logs).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 A2C 模型（不要忘记 verbose=1 以打印训练日志）。
- en: Train it for 1M Timesteps
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 100 万个时间步
- en: Save the model and VecNormalize statistics when saving the agent
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型和 VecNormalize 统计数据时保存 agent
- en: Evaluate your agent
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估您的 agent
- en: Publish your trained model on the Hub 🔥 with `package_to_hub`
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `package_to_hub` 在 Hub 🔥 上发布您训练好的模型
- en: Solution (optional)
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案（可选）
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: See you on Unit 7! 🔥
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 7 单元见！🔥
- en: Keep learning, stay awesome 🤗
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续学习，保持出色 🤗
