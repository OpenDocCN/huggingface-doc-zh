- en: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym ğŸ¤–
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„ Advantage Actor Critic (A2C) ğŸ¤–
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/hands-on](https://huggingface.co/learn/deep-rl-course/unit6/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/learn/deep-rl-course/unit6/hands-on](https://huggingface.co/learn/deep-rl-course/unit6/hands-on)
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![æé—®](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![åœ¨ Colab ä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb)'
- en: 'Now that youâ€™ve studied the theory behind Advantage Actor Critic (A2C), **youâ€™re
    ready to train your A2C agent** using Stable-Baselines3 in a robotic environment.
    And train a:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å­¦ä¹ äº† Advantage Actor Critic (A2C) çš„ç†è®ºï¼Œ**æ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‚¨çš„ A2C ä»£ç†**ï¼Œä½¿ç”¨ Stable-Baselines3
    åœ¨ä¸€ä¸ªæœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚å¹¶è®­ç»ƒä¸€ä¸ªï¼š
- en: A robotic arm ğŸ¦¾ to move to the correct position.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚ğŸ¦¾ç§»åŠ¨åˆ°æ­£ç¡®çš„ä½ç½®ã€‚
- en: Weâ€™re going to use
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨
- en: '[panda-gym](https://github.com/qgallouedec/panda-gym)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[panda-gym](https://github.com/qgallouedec/panda-gym)'
- en: 'To validate this hands-on for the certification process, you need to push your
    two trained models to the Hub and get the following results:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†éªŒè¯è¿™ä¸ªå®è·µè¿‡ç¨‹çš„è®¤è¯ï¼Œæ‚¨éœ€è¦å°†æ‚¨çš„ä¸¤ä¸ªè®­ç»ƒæ¨¡å‹æ¨é€åˆ°Hubå¹¶è·å¾—ä»¥ä¸‹ç»“æœï¼š
- en: '`PandaReachDense-v3` get a result of >= -3.5.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PandaReachDense-v3` è·å¾—ç»“æœ >= -3.5ã€‚'
- en: To find your result, [go to the leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰¾åˆ°æ‚¨çš„ç»“æœï¼Œè¯·[è½¬åˆ°æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)å¹¶æ‰¾åˆ°æ‚¨çš„æ¨¡å‹ï¼Œ**ç»“æœ
    = å¹³å‡å¥–åŠ± - å¥–åŠ±çš„æ ‡å‡†å·®**
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: '**To start the hands-on click on Open In Colab button** ğŸ‘‡ :'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç‚¹å‡»â€œåœ¨ Colab ä¸­æ‰“å¼€â€æŒ‰é’®å¼€å§‹å®è·µ**ğŸ‘‡ï¼š'
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![åœ¨ Colab ä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)'
- en: 'Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym
    ğŸ¤–'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 6 å•å…ƒï¼šä½¿ç”¨ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„ Advantage Actor Critic (A2C) ğŸ¤–
- en: 'ğŸ® Environments:'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ğŸ® ç¯å¢ƒï¼š
- en: '[Panda-Gym](https://github.com/qgallouedec/panda-gym)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Panda-Gym](https://github.com/qgallouedec/panda-gym)'
- en: 'ğŸ“š RL-Library:'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ğŸ“š å¼ºåŒ–å­¦ä¹ åº“ï¼š
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)'
- en: Weâ€™re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸æ–­åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœæ‚¨åœ¨æœ¬ç¬”è®°æœ¬ä¸­å‘ç°é—®é¢˜**ï¼Œè¯·[åœ¨ GitHub ä»“åº“ä¸Šæå‡ºé—®é¢˜](https://github.com/huggingface/deep-rl-class/issues)ã€‚
- en: Objectives of this notebook ğŸ†
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬ç¬”è®°æœ¬çš„ç›®æ ‡ğŸ†
- en: 'At the end of the notebook, you will:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬”è®°æœ¬çš„æœ«å°¾ï¼Œæ‚¨å°†ï¼š
- en: Be able to use **Panda-Gym**, the environment library.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿä½¿ç”¨**Panda-Gym**ï¼Œè¿™ä¸ªç¯å¢ƒåº“ã€‚
- en: Be able to **train robots using A2C**.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿ**ä½¿ç”¨ A2C è®­ç»ƒæœºå™¨äºº**ã€‚
- en: Understand why **we need to normalize the input**.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº†è§£ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–**ã€‚
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score ğŸ”¥.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿ**å°†æ‚¨è®­ç»ƒå¥½çš„ä»£ç†å’Œä»£ç æ¨é€åˆ°Hub**ï¼Œå¹¶é™„å¸¦ä¸€ä¸ªæ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•°ğŸ”¥ã€‚
- en: Prerequisites ğŸ—ï¸
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ˆå†³æ¡ä»¶ğŸ—ï¸
- en: 'Before diving into the notebook, you need to:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥ç ”ç©¶ç¬”è®°æœ¬ä¹‹å‰ï¼Œæ‚¨éœ€è¦ï¼š
- en: ğŸ”² ğŸ“š Study [Actor-Critic methods by reading Unit 6](https://huggingface.co/deep-rl-course/unit6/introduction)
    ğŸ¤—
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”² ğŸ“š å­¦ä¹  [é€šè¿‡é˜…è¯»ç¬¬ 6 å•å…ƒçš„ Actor-Critic æ–¹æ³•](https://huggingface.co/deep-rl-course/unit6/introduction)
    ğŸ¤—
- en: Letâ€™s train our first robots ğŸ¤–
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæœºå™¨äººğŸ¤–
- en: Set the GPU ğŸ’ª
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½® GPU ğŸ’ª
- en: To **accelerate the agentâ€™s training, weâ€™ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†**åŠ é€Ÿä»£ç†çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ GPU**ã€‚ä¸ºæ­¤ï¼Œè¯·è½¬åˆ° `è¿è¡Œæ—¶ > æ›´æ”¹è¿è¡Œæ—¶ç±»å‹`
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![GPU æ­¥éª¤ 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
- en: '`Hardware Accelerator > GPU`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ç¡¬ä»¶åŠ é€Ÿå™¨ > GPU`'
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![GPU æ­¥éª¤ 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
- en: Create a virtual display ğŸ”½
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ˜¾ç¤ºğŸ”½
- en: During the notebook, weâ€™ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªå›æ”¾è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œåœ¨ colab ä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚
- en: The following cell will install the librairies and create and run a virtual
    screen ğŸ–¥
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å•å…ƒæ ¼å°†å®‰è£…åº“å¹¶åˆ›å»ºå¹¶è¿è¡Œä¸€ä¸ªè™šæ‹Ÿå±å¹•ğŸ–¥
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install dependencies ğŸ”½
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®‰è£…ä¾èµ–ğŸ”½
- en: 'Weâ€™ll install multiple ones:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®‰è£…å¤šä¸ªï¼š
- en: '`gymnasium`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gymnasium`'
- en: '`panda-gym`: Contains the robotics arm environments.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`panda-gym`ï¼šåŒ…å«æœºå™¨äººæ‰‹è‡‚ç¯å¢ƒã€‚'
- en: '`stable-baselines3`: The SB3 deep reinforcement learning library.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stable-baselines3`ï¼šSB3 æ·±åº¦å¼ºåŒ–å­¦ä¹ åº“ã€‚'
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face ğŸ¤— Hub.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_sb3`ï¼šç”¨äº Stable-baselines3 çš„é¢å¤–ä»£ç ï¼Œç”¨äºä» Hugging Face ğŸ¤— Hub åŠ è½½å’Œä¸Šä¼ æ¨¡å‹ã€‚'
- en: '`huggingface_hub`: Library allowing anyone to work with the Hub repositories.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_hub`ï¼šå…è®¸ä»»ä½•äººä½¿ç”¨Hubå­˜å‚¨åº“ã€‚'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Import the packages ğŸ“¦
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥åŒ…ğŸ“¦
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PandaReachDense-v3 ğŸ¦¾
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PandaReachDense-v3 ğŸ¦¾
- en: The agent weâ€™re going to train is a robotic arm that needs to do controls (moving
    the arm and using the end-effector).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®­ç»ƒçš„ä»£ç†æ˜¯ä¸€ä¸ªéœ€è¦è¿›è¡Œæ§åˆ¶çš„æœºå™¨äººæ‰‹è‡‚ï¼ˆç§»åŠ¨æ‰‹è‡‚å¹¶ä½¿ç”¨æœ«ç«¯æ‰§è¡Œå™¨ï¼‰ã€‚
- en: In robotics, the *end-effector* is the device at the end of a robotic arm designed
    to interact with the environment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œ*æœ«ç«¯æ‰§è¡Œå™¨*æ˜¯è®¾è®¡ç”¨äºä¸ç¯å¢ƒäº¤äº’çš„æœºå™¨äººæ‰‹è‡‚æœ«ç«¯çš„è®¾å¤‡ã€‚
- en: In `PandaReach`, the robot must place its end-effector at a target position
    (green ball).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ `PandaReach` ä¸­ï¼Œæœºå™¨äººå¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½®ï¼ˆç»¿è‰²çƒï¼‰ã€‚
- en: Weâ€™re going to use the dense version of this environment. It means weâ€™ll get
    a *dense reward function* that **will provide a reward at each timestep** (the
    closer the agent is to completing the task, the higher the reward). Contrary to
    a *sparse reward function* where the environment **return a reward if and only
    if the task is completed**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ­¤ç¯å¢ƒçš„å¯†é›†ç‰ˆæœ¬ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†è·å¾—ä¸€ä¸ª*å¯†é›†å¥–åŠ±å‡½æ•°*ï¼Œ**å°†åœ¨æ¯ä¸ªæ—¶é—´æ­¥æä¾›å¥–åŠ±** (ä»£ç†è¶Šæ¥è¿‘å®Œæˆä»»åŠ¡ï¼Œå¥–åŠ±è¶Šé«˜)ã€‚ä¸*ç¨€ç–å¥–åŠ±å‡½æ•°*ç›¸åï¼Œç¯å¢ƒ**ä»…åœ¨ä»»åŠ¡å®Œæˆæ—¶è¿”å›å¥–åŠ±**ã€‚
- en: Also, weâ€™re going to use the *End-effector displacement control*, it means the
    **action corresponds to the displacement of the end-effector**. We donâ€™t control
    the individual motion of each joint (joint control).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ *æœ«ç«¯æ‰§è¡Œå™¨ä½ç§»æ§åˆ¶*ï¼Œè¿™æ„å‘³ç€**åŠ¨ä½œå¯¹åº”äºæœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç§»**ã€‚æˆ‘ä»¬ä¸æ§åˆ¶æ¯ä¸ªå…³èŠ‚çš„å•ç‹¬è¿åŠ¨ (å…³èŠ‚æ§åˆ¶)ã€‚
- en: '![Robotics](../Images/d79d62b53f91999defb0b4eae0db003d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![æœºå™¨äººæŠ€æœ¯](../Images/d79d62b53f91999defb0b4eae0db003d.png)'
- en: This way **the training will be easier**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·**è®­ç»ƒå°†æ›´å®¹æ˜“**ã€‚
- en: Create the environment
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ›å»ºç¯å¢ƒ
- en: The environment ğŸ®
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¯å¢ƒ ğŸ®
- en: In `PandaReachDense-v3` the robotic arm must place its end-effector at a target
    position (green ball).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ `PandaReachDense-v3` ä¸­ï¼Œæœºæ¢°è‡‚å¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½® (ç»¿è‰²çƒ)ã€‚
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The observation space **is a dictionary with 3 different elements**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿç©ºé—´**æ˜¯ä¸€ä¸ªå…·æœ‰ 3 ä¸ªä¸åŒå…ƒç´ çš„å­—å…¸**ï¼š
- en: '`achieved_goal`: (x,y,z) position of the goal.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`achieved_goal`: ç›®æ ‡çš„ä½ç½® (x,y,z)ã€‚'
- en: '`desired_goal`: (x,y,z) distance between the goal position and the current
    object position.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`desired_goal`: ç›®æ ‡ä½ç½®ä¸å½“å‰ç‰©ä½“ä½ç½®ä¹‹é—´çš„è·ç¦» (x,y,z)ã€‚'
- en: '`observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation`: æœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç½® (x,y,z) å’Œé€Ÿåº¦ (vx, vy, vz)ã€‚'
- en: Given itâ€™s a dictionary as observation, **we will need to use a MultiInputPolicy
    policy instead of MlpPolicy**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºè§‚å¯Ÿæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œ**æˆ‘ä»¬éœ€è¦ä½¿ç”¨ MultiInputPolicy ç­–ç•¥è€Œä¸æ˜¯ MlpPolicy**ã€‚
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The action space is a vector with 3 values:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œç©ºé—´æ˜¯ä¸€ä¸ªå…·æœ‰ 3 ä¸ªå€¼çš„å‘é‡ï¼š
- en: Control x, y, z movement
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ§åˆ¶ xã€yã€z è¿åŠ¨
- en: Normalize observation and rewards
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å½’ä¸€åŒ–è§‚å¯Ÿå’Œå¥–åŠ±
- en: A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¥½çš„å®è·µæ˜¯[å½’ä¸€åŒ–è¾“å…¥ç‰¹å¾](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)ã€‚
- en: For that purpose, there is a wrapper that will compute a running average and
    standard deviation of input features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæœ‰ä¸€ä¸ªåŒ…è£…å™¨å°†è®¡ç®—è¾“å…¥ç‰¹å¾çš„è¿è¡Œå¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- en: We also normalize rewards with this same wrapper by adding `norm_reward = True`
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜é€šè¿‡æ·»åŠ  `norm_reward = True` æ¥ä½¿ç”¨ç›¸åŒçš„åŒ…è£…å™¨å¯¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–
- en: '[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ‚¨åº”è¯¥æŸ¥çœ‹æ–‡æ¡£ä»¥å¡«å†™æ­¤å•å…ƒæ ¼](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)'
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Solution
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Create the A2C Model ğŸ¤–
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ›å»º A2C æ¨¡å‹ ğŸ¤–
- en: 'For more information about A2C implementation with StableBaselines3 check:
    [https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³ä½¿ç”¨ StableBaselines3 å®ç° A2C çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š[https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes)
- en: To find the best parameters I checked the [official trained agents by Stable-Baselines3
    team](https://huggingface.co/sb3).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼Œæˆ‘æ£€æŸ¥äº†[ç”± Stable-Baselines3 å›¢é˜Ÿå®˜æ–¹è®­ç»ƒçš„ä»£ç†](https://huggingface.co/sb3)ã€‚
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Solution
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Train the A2C agent ğŸƒ
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒ A2C ä»£ç† ğŸƒ
- en: Letâ€™s train our agent for 1,000,000 timesteps, donâ€™t forget to use GPU on Colab.
    It will take approximately ~25-40min
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯¹ä»£ç†è¿›è¡Œ 1,000,000 ä¸ªæ—¶é—´æ­¥çš„è®­ç»ƒï¼Œä¸è¦å¿˜è®°åœ¨ Colab ä¸Šä½¿ç”¨ GPUã€‚å¤§çº¦éœ€è¦ 25-40 åˆ†é’Ÿ
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Evaluate the agent ğŸ“ˆ
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¯„ä¼°ä»£ç† ğŸ“ˆ
- en: Now thatâ€™s our agent is trained, we need to **check its performance**.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„ä»£ç†å·²ç»è®­ç»ƒå¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦**æ£€æŸ¥å…¶æ€§èƒ½**ã€‚
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stable-Baselines3 æä¾›äº†ä¸€ä¸ªæ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼š`evaluate_policy`
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Publish your trained model on the Hub ğŸ”¥
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨ Hub ä¸Šå‘å¸ƒæ‚¨è®­ç»ƒå¥½çš„æ¨¡å‹ ğŸ”¥
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the Hub with one line of code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒåå–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ° Hub ä¸Šã€‚
- en: ğŸ“š The libraries documentation ğŸ‘‰ [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-faceâ€”x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“š å›¾ä¹¦é¦†æ–‡æ¡£ ğŸ‘‰ [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-faceâ€”x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
- en: By using `package_to_hub`, as we already mentionned in the former units, **you
    evaluate, record a replay, generate a model card of your agent and push it to
    the hub**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨ `package_to_hub`ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢çš„å•å…ƒä¸­å·²ç»æåˆ°çš„ï¼Œ**æ‚¨å¯ä»¥è¯„ä¼°ã€è®°å½•é‡æ’­ã€ç”Ÿæˆä»£ç†çš„æ¨¡å‹å¡å¹¶å°†å…¶æ¨é€åˆ° Hub**ã€‚
- en: 'This way:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼š
- en: You can **showcase our work** ğŸ”¥
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ **å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ** ğŸ”¥
- en: You can **visualize your agent playing** ğŸ‘€
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ **æŸ¥çœ‹æ‚¨çš„ä»£ç†è¿›è¡Œæ¸¸æˆ** ğŸ‘€
- en: You can **share with the community an agent that others can use** ğŸ’¾
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ **ä¸ç¤¾åŒºåˆ†äº«å…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„ä»£ç†** ğŸ’¾
- en: You can **access a leaderboard ğŸ† to see how well your agent is performing compared
    to your classmates** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ **è®¿é—®æ’è¡Œæ¦œ ğŸ† æŸ¥çœ‹æ‚¨çš„ä»£ç†ç›¸å¯¹äºåŒå­¦è¡¨ç°å¦‚ä½•** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¦èƒ½å¤Ÿä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œè¿˜éœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤ï¼š
- en: 1ï¸âƒ£ (If itâ€™s not already done) create an account to HF â¡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ (å¦‚æœå°šæœªå®Œæˆ) åˆ›å»ºä¸€ä¸ª HF å¸æˆ· â¡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2ï¸âƒ£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç™»å½•ï¼Œç„¶åï¼Œæ‚¨éœ€è¦ä» Hugging Face ç½‘ç«™å­˜å‚¨æ‚¨çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªæ–°çš„ä»¤ç‰Œ ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **å…·æœ‰å†™å…¥æƒé™**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ›å»º HF ä»¤ç‰Œ](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: Copy the token
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤åˆ¶ä»¤ç‰Œ
- en: Run the cell below and paste the token
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼å¹¶ç²˜è´´ä»¤ç‰Œ
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you donâ€™t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨ Google Colab æˆ– Jupyter Notebookï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`
- en: 3ï¸âƒ£ Weâ€™re now ready to push our trained agent to the ğŸ¤— Hub ğŸ”¥ using `package_to_hub()`
    function. For this environment, **running this cell can take approximately 10min**
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨ `package_to_hub()` å‡½æ•°å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„ agent æ¨é€åˆ° ğŸ¤— Hub ğŸ”¥ã€‚å¯¹äºè¿™ä¸ªç¯å¢ƒï¼Œ**è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼å¯èƒ½éœ€è¦å¤§çº¦
    10 åˆ†é’Ÿ**ã€‚
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Some additional challenges ğŸ†
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ ğŸ†
- en: The best way to learn **is to try things by your own**! Why not trying `PandaPickAndPlace-v3`?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„æœ€å¥½æ–¹æ³• **æ˜¯è‡ªå·±å°è¯•**ï¼ä¸ºä»€ä¹ˆä¸å°è¯• `PandaPickAndPlace-v3`ï¼Ÿ
- en: 'If you want to try more advanced tasks for panda-gym, you need to check what
    was done using **TQC or SAC** (a more sample-efficient algorithm suited for robotics
    tasks). In real robotics, youâ€™ll use a more sample-efficient algorithm for a simple
    reason: contrary to a simulation **if you move your robotic arm too much, you
    have a risk of breaking it**.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³å°è¯•æ›´é«˜çº§çš„ä»»åŠ¡ï¼Œæ‚¨éœ€è¦æ£€æŸ¥ä½¿ç”¨ **TQC æˆ– SAC**ï¼ˆé€‚ç”¨äºæœºå™¨äººä»»åŠ¡çš„æ›´é«˜æ•ˆçš„ç®—æ³•ï¼‰å®Œæˆäº†ä»€ä¹ˆã€‚åœ¨çœŸå®çš„æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•ï¼Œä¸€ä¸ªç®€å•çš„åŸå› æ˜¯ï¼šä¸æ¨¡æ‹Ÿç›¸åï¼Œ**å¦‚æœæ‚¨ç§»åŠ¨æœºå™¨äººæ‰‹è‡‚å¤ªå¤šï¼Œæ‚¨æœ‰ç ´åçš„é£é™©**ã€‚
- en: 'PandaPickAndPlace-v1 (this model uses the v1 version of the environment): [https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1](https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PandaPickAndPlace-v1ï¼ˆæ­¤æ¨¡å‹ä½¿ç”¨ç¯å¢ƒçš„ v1 ç‰ˆæœ¬ï¼‰ï¼š[https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1](https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1)
- en: 'And donâ€™t hesitate to check panda-gym documentation here: [https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html](https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”ä¸è¦çŠ¹è±«æŸ¥çœ‹ panda-gym æ–‡æ¡£ï¼š[https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html](https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html)
- en: 'We provide you the steps to train another agent (optional):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†è®­ç»ƒå¦ä¸€ä¸ª agent çš„æ­¥éª¤ï¼ˆå¯é€‰ï¼‰ï¼š
- en: Define the environment called â€œPandaPickAndPlace-v3â€
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰åä¸º â€œPandaPickAndPlace-v3â€ çš„ç¯å¢ƒ
- en: Make a vectorized environment
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªçŸ¢é‡åŒ–ç¯å¢ƒ
- en: Add a wrapper to normalize the observations and rewards. [Check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸€ä¸ªåŒ…è£…å™¨æ¥è§„èŒƒè§‚å¯Ÿå’Œå¥–åŠ±ã€‚[æŸ¥çœ‹æ–‡æ¡£](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)
- en: Create the A2C Model (donâ€™t forget verbose=1 to print the training logs).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»º A2C æ¨¡å‹ï¼ˆä¸è¦å¿˜è®° verbose=1 ä»¥æ‰“å°è®­ç»ƒæ—¥å¿—ï¼‰ã€‚
- en: Train it for 1M Timesteps
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒ 100 ä¸‡ä¸ªæ—¶é—´æ­¥
- en: Save the model and VecNormalize statistics when saving the agent
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¿å­˜æ¨¡å‹å’Œ VecNormalize ç»Ÿè®¡æ•°æ®æ—¶ä¿å­˜ agent
- en: Evaluate your agent
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ‚¨çš„ agent
- en: Publish your trained model on the Hub ğŸ”¥ with `package_to_hub`
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `package_to_hub` åœ¨ Hub ğŸ”¥ ä¸Šå‘å¸ƒæ‚¨è®­ç»ƒå¥½çš„æ¨¡å‹
- en: Solution (optional)
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆï¼ˆå¯é€‰ï¼‰
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: See you on Unit 7! ğŸ”¥
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ 7 å•å…ƒè§ï¼ğŸ”¥
- en: Keep learning, stay awesome ğŸ¤—
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»§ç»­å­¦ä¹ ï¼Œä¿æŒå‡ºè‰² ğŸ¤—
