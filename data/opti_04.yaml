- en: Quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå¯¼è§ˆ
- en: 'Original text: [https://huggingface.co/docs/optimum/quicktour](https://huggingface.co/docs/optimum/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/optimum/quicktour](https://huggingface.co/docs/optimum/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This quick tour is intended for developers who are ready to dive into the code
    and see examples of how to integrate ğŸ¤— Optimum into their model training and inference
    workflows.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå¿«é€Ÿå¯¼è§ˆé€‚ç”¨äºå‡†å¤‡æ·±å…¥ä»£ç å¹¶æŸ¥çœ‹å¦‚ä½•å°†ğŸ¤— Optimumé›†æˆåˆ°ä»–ä»¬çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†å·¥ä½œæµç¨‹ä¸­çš„å¼€å‘äººå‘˜ã€‚
- en: Accelerated inference
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ é€Ÿæ¨ç†
- en: OpenVINO
  id: totrans-5
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenVINO
- en: To load a model and run inference with OpenVINO Runtime, you can just replace
    your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. If
    you want to load a PyTorch checkpoint, set `export=True` to convert your model
    to the OpenVINO IR (Intermediate Representation).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åŠ è½½æ¨¡å‹å¹¶ä½¿ç”¨OpenVINO Runtimeè¿›è¡Œæ¨ç†ï¼Œæ‚¨åªéœ€å°†æ‚¨çš„`AutoModelForXxx`ç±»æ›¿æ¢ä¸ºç›¸åº”çš„`OVModelForXxx`ç±»ã€‚å¦‚æœè¦åŠ è½½PyTorchæ£€æŸ¥ç‚¹ï¼Œè¯·è®¾ç½®`export=True`ä»¥å°†æ¨¡å‹è½¬æ¢ä¸ºOpenVINO
    IRï¼ˆä¸­é—´è¡¨ç¤ºï¼‰ã€‚
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference)
    and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/intel/inference)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚
- en: ONNX Runtime
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ONNX Runtime
- en: To accelerate inference with ONNX Runtime, ğŸ¤— Optimum uses *configuration objects*
    to define parameters for graph optimization and quantization. These objects are
    then used to instantiate dedicated *optimizers* and *quantizers*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åŠ é€Ÿä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†ï¼ŒğŸ¤— Optimumä½¿ç”¨*é…ç½®å¯¹è±¡*æ¥å®šä¹‰å›¾ä¼˜åŒ–å’Œé‡åŒ–çš„å‚æ•°ã€‚ç„¶åä½¿ç”¨è¿™äº›å¯¹è±¡æ¥å®ä¾‹åŒ–ä¸“ç”¨çš„*ä¼˜åŒ–å™¨*å’Œ*é‡åŒ–å™¨*ã€‚
- en: Before applying quantization or optimization, first we need to load our model.
    To load a model and run inference with ONNX Runtime, you can just replace the
    canonical Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)
    class with the corresponding [`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)
    class. If you want to load from a PyTorch checkpoint, set `export=True` to export
    your model to the ONNX format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åº”ç”¨é‡åŒ–æˆ–ä¼˜åŒ–ä¹‹å‰ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹ã€‚è¦åŠ è½½æ¨¡å‹å¹¶ä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†ï¼Œæ‚¨åªéœ€å°†ç»å…¸çš„Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)ç±»æ›¿æ¢ä¸ºç›¸åº”çš„[`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)ç±»ã€‚å¦‚æœè¦ä»PyTorchæ£€æŸ¥ç‚¹åŠ è½½ï¼Œè¯·è®¾ç½®`export=True`ä»¥å°†æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ã€‚
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Letâ€™s see now how we can apply dynamic quantization with ONNX Runtime:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ONNX Runtimeåº”ç”¨åŠ¨æ€é‡åŒ–ï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, weâ€™ve quantized a model from the Hugging Face Hub, in the
    same manner we can quantize a model hosted locally by providing the path to the
    directory containing the model weights. The result from applying the `quantize()`
    method is a `model_quantized.onnx` file that can be used to run inference. Hereâ€™s
    an example of how to load an ONNX Runtime model and generate predictions with
    it:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¥è‡ªHugging Face Hubçš„æ¨¡å‹è¿›è¡Œäº†é‡åŒ–ï¼Œä»¥ç›¸åŒçš„æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æä¾›åŒ…å«æ¨¡å‹æƒé‡çš„ç›®å½•è·¯å¾„æ¥å¯¹æœ¬åœ°æ‰˜ç®¡çš„æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚åº”ç”¨`quantize()`æ–¹æ³•çš„ç»“æœæ˜¯ä¸€ä¸ª`model_quantized.onnx`æ–‡ä»¶ï¼Œå¯ç”¨äºè¿è¡Œæ¨ç†ã€‚è¿™é‡Œæ˜¯å¦‚ä½•åŠ è½½ä¸€ä¸ªONNX
    Runtimeæ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹çš„ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/quickstart)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/onnxruntime/quickstart)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚
- en: Accelerated training
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ é€Ÿè®­ç»ƒ
- en: Habana
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Habana
- en: 'To train transformers on Habanaâ€™s Gaudi processors, ğŸ¤— Optimum provides a `GaudiTrainer`
    that is very similar to the ğŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨Habanaçš„Gaudiå¤„ç†å™¨ä¸Šè®­ç»ƒtransformersï¼ŒğŸ¤— Optimumæä¾›äº†ä¸€ä¸ª`GaudiTrainer`ï¼Œå®ƒä¸ğŸ¤— Transformers
    [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)éå¸¸ç›¸ä¼¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart)
    and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/habana/quickstart)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum-habana/tree/main/examples)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚
- en: ONNX Runtime
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ONNX Runtime
- en: 'To train transformers with ONNX Runtimeâ€™s acceleration features, ğŸ¤— Optimum
    provides a `ORTTrainer` that is very similar to the ğŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ONNX Runtimeçš„åŠ é€ŸåŠŸèƒ½æ¥è®­ç»ƒtransformersï¼ŒğŸ¤— Optimumæä¾›äº†ä¸€ä¸ª`ORTTrainer`ï¼Œå®ƒä¸ğŸ¤— Transformers
    [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)éå¸¸ç›¸ä¼¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚
- en: Out of the box ONNX export
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼€ç®±å³ç”¨çš„ONNXå¯¼å‡º
- en: The Optimum library handles out of the box the ONNX export of Transformers and
    Diffusers models!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Optimumåº“å¯ä»¥ç›´æ¥å¤„ç†Transformerså’ŒDiffusersæ¨¡å‹çš„ONNXå¯¼å‡ºï¼
- en: Exporting a model to ONNX is as simple as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹å¯¼å‡ºåˆ°ONNXå°±åƒè¿™æ ·ç®€å•
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Check out the help for more options:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹å¸®åŠ©ä»¥è·å–æ›´å¤šé€‰é¡¹ï¼š
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Check out the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)
    for more.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: PyTorchâ€™s BetterTransformer support
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorchçš„BetterTransformeræ”¯æŒ
- en: '[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    is a free-lunch PyTorch-native optimization to gain x1.25 - x4 speedup on the
    inference of Transformer-based models. It has been marked as stable in [PyTorch
    1.13](https://pytorch.org/blog/PyTorch-1.13-release/). We integrated BetterTransformer
    with the most-used models from the ğŸ¤— Transformers libary, and using the integration
    is as simple as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    æ˜¯ä¸€ä¸ªå…è´¹çš„ PyTorch æœ¬åœ°ä¼˜åŒ–ï¼Œå¯åœ¨åŸºäº Transformer çš„æ¨¡å‹æ¨ç†ä¸­è·å¾— x1.25 - x4 çš„åŠ é€Ÿã€‚å®ƒå·²åœ¨[PyTorch 1.13](https://pytorch.org/blog/PyTorch-1.13-release/)ä¸­æ ‡è®°ä¸ºç¨³å®šã€‚æˆ‘ä»¬å°†
    BetterTransformer ä¸ ğŸ¤— Transformers åº“ä¸­æœ€å¸¸ç”¨çš„æ¨¡å‹é›†æˆï¼Œä½¿ç”¨é›†æˆå°±åƒè¿™æ ·ç®€å•ï¼š'
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Check out the [documentation](https://huggingface.co/docs/optimum/bettertransformer/overview)
    for more details, and the [blog post on PyTorchâ€™s Medium](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)
    to find out more about the integration!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/optimum/bettertransformer/overview)ï¼Œå¹¶æŸ¥çœ‹[PyTorch
    Medium ä¸Šçš„åšæ–‡](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ä»¥äº†è§£æ›´å¤šå…³äºé›†æˆçš„ä¿¡æ¯ï¼
- en: torch.fx integration
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torch.fx é›†æˆ
- en: Optimum integrates with `torch.fx`, providing as a one-liner several graph transformations.
    We aim at supporting a better management of [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
    through `torch.fx`, both for quantization-aware training (QAT) and post-training
    quantization (PTQ).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Optimum ä¸ `torch.fx` é›†æˆï¼Œæä¾›äº†å‡ ä¸ªå›¾å½¢è½¬æ¢çš„ä¸€è¡Œä»£ç ã€‚æˆ‘ä»¬æ—¨åœ¨é€šè¿‡ `torch.fx` æ”¯æŒæ›´å¥½çš„[é‡åŒ–](https://huggingface.co/docs/optimum/concept_guides/quantization)ç®¡ç†ï¼ŒåŒ…æ‹¬é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œè®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰ã€‚
- en: Check out the [documentation](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)
    and [reference](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)
    for more!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æ›´å¤š[æ–‡æ¡£](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)å’Œ[å‚è€ƒèµ„æ–™](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)ï¼
