- en: ResNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/resnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/resnet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/resnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/resnet)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: The ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
    by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows
    the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch),
    we apply the `stride=2` for downsampling in bottleneckâ€™s `3x3` conv and not in
    the first `1x1`. This is generally known as â€œResNet v1.5â€.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ResNetæ¨¡å‹æ˜¯ç”±Kaiming Heã€Xiangyu Zhangã€Shaoqing Renå’ŒJian Sunåœ¨[å›¾åƒè¯†åˆ«çš„æ·±åº¦æ®‹å·®å­¦ä¹ ](https://arxiv.org/abs/1512.03385)ä¸­æå‡ºçš„ã€‚æˆ‘ä»¬çš„å®ç°éµå¾ªäº†[Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch)æ‰€åšçš„å°æ”¹åŠ¨ï¼Œæˆ‘ä»¬åœ¨ç“¶é¢ˆçš„`3x3`å·ç§¯ä¸­åº”ç”¨`stride=2`è¿›è¡Œä¸‹é‡‡æ ·ï¼Œè€Œä¸æ˜¯åœ¨ç¬¬ä¸€ä¸ª`1x1`ä¸­ã€‚è¿™é€šå¸¸è¢«ç§°ä¸ºâ€œResNet
    v1.5â€ã€‚
- en: ResNet introduced residual connections, they allow to train networks with an
    unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition,
    one important milestone in deep computer vision.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ResNetå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œå®ƒä»¬å…è®¸è®­ç»ƒå…·æœ‰æœªçŸ¥å±‚æ•°ï¼ˆé«˜è¾¾1000å±‚ï¼‰çš„ç½‘ç»œã€‚ResNetèµ¢å¾—äº†2015å¹´ILSVRCå’ŒCOCOç«èµ›ï¼Œè¿™æ˜¯æ·±åº¦è®¡ç®—æœºè§†è§‰çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Deeper neural networks are more difficult to train. We present a residual
    learning framework to ease the training of networks that are substantially deeper
    than those used previously. We explicitly reformulate the layers as learning residual
    functions with reference to the layer inputs, instead of learning unreferenced
    functions. We provide comprehensive empirical evidence showing that these residual
    networks are easier to optimize, and can gain accuracy from considerably increased
    depth. On the ImageNet dataset we evaluate residual nets with a depth of up to
    152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble
    of these residual nets achieves 3.57% error on the ImageNet test set. This result
    won the 1st place on the ILSVRC 2015 classification task. We also present analysis
    on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central
    importance for many visual recognition tasks. Solely due to our extremely deep
    representations, we obtain a 28% relative improvement on the COCO object detection
    dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO
    2015 competitions, where we also won the 1st places on the tasks of ImageNet detection,
    ImageNet localization, COCO detection, and COCO segmentation.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ›´æ·±çš„ç¥ç»ç½‘ç»œæ›´éš¾è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ®‹å·®å­¦ä¹ æ¡†æ¶ï¼Œä»¥ä¾¿è®­ç»ƒæ¯”ä»¥å‰ä½¿ç”¨çš„ç½‘ç»œæ›´æ·±çš„ç½‘ç»œå˜å¾—æ›´å®¹æ˜“ã€‚æˆ‘ä»¬æ˜ç¡®åœ°å°†å±‚é‡æ–°æ„å»ºä¸ºå­¦ä¹ æ®‹å·®å‡½æ•°ï¼Œå‚è€ƒå±‚è¾“å…¥ï¼Œè€Œä¸æ˜¯å­¦ä¹ æ— å‚è€ƒçš„å‡½æ•°ã€‚æˆ‘ä»¬æä¾›äº†å…¨é¢çš„å®è¯è¯æ®ï¼Œè¡¨æ˜è¿™äº›æ®‹å·®ç½‘ç»œæ›´å®¹æ˜“ä¼˜åŒ–ï¼Œå¹¶ä¸”å¯ä»¥ä»æ˜æ˜¾å¢åŠ çš„æ·±åº¦ä¸­è·å¾—å‡†ç¡®æ€§ã€‚åœ¨ImageNetæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ·±åº¦é«˜è¾¾152å±‚çš„æ®‹å·®ç½‘ç»œ---æ¯”VGGç½‘ç»œæ·±8å€ï¼Œä½†ä»å…·æœ‰è¾ƒä½çš„å¤æ‚æ€§ã€‚è¿™äº›æ®‹å·®ç½‘ç»œçš„é›†åˆåœ¨ImageNetæµ‹è¯•é›†ä¸Šå®ç°äº†3.57%çš„é”™è¯¯ç‡ã€‚è¿™ä¸€ç»“æœèµ¢å¾—äº†ILSVRC
    2015åˆ†ç±»ä»»åŠ¡çš„ç¬¬ä¸€åã€‚æˆ‘ä»¬è¿˜å¯¹å…·æœ‰100å’Œ1000å±‚çš„CIFAR-10è¿›è¡Œäº†åˆ†æã€‚è¡¨ç¤ºçš„æ·±åº¦å¯¹è®¸å¤šè§†è§‰è¯†åˆ«ä»»åŠ¡è‡³å…³é‡è¦ã€‚ä»…ä»…ç”±äºæˆ‘ä»¬æå…¶æ·±çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬åœ¨COCOç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè·å¾—äº†28%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ·±åº¦æ®‹å·®ç½‘ç»œæ˜¯æˆ‘ä»¬æäº¤ç»™ILSVRCå’ŒCOCO
    2015ç«èµ›çš„åŸºç¡€ï¼Œæˆ‘ä»¬è¿˜åœ¨ImageNetæ£€æµ‹ã€ImageNetå®šä½ã€COCOæ£€æµ‹å’ŒCOCOåˆ†å‰²ä»»åŠ¡ä¸Šè·å¾—äº†ç¬¬ä¸€åã€‚*'
- en: The figure below illustrates the architecture of ResNet. Taken from the [original
    paper](https://arxiv.org/abs/1512.03385).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„å›¾ç¤ºäº†ResNetçš„æ¶æ„ã€‚å–è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/1512.03385)ã€‚
- en: '![](../Images/9ccad54651391b48289acb1dad8b77f1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ccad54651391b48289acb1dad8b77f1.png)'
- en: This model was contributed by [Francesco](https://huggingface.co/Francesco).
    The TensorFlow version of this model was added by [amyeroberts](https://huggingface.co/amyeroberts).
    The original code can be found [here](https://github.com/KaimingHe/deep-residual-networks).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç”±[Francesco](https://huggingface.co/Francesco)è´¡çŒ®ã€‚è¿™ä¸ªæ¨¡å‹çš„TensorFlowç‰ˆæœ¬æ˜¯ç”±[amyeroberts](https://huggingface.co/amyeroberts)æ·»åŠ çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/KaimingHe/deep-residual-networks)æ‰¾åˆ°ã€‚
- en: Resources
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with ResNet.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ResNetã€‚
- en: Image Classification
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: '[ResNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ResNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚è§ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨è¿™é‡Œï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªæ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: ResNetConfig
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNetConfig
- en: '### `class transformers.ResNetConfig`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ResNetConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/configuration_resnet.py#L35)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/configuration_resnet.py#L35)'
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *å¯é€‰*, é»˜è®¤ä¸º3) â€” è¾“å…¥é€šé“æ•°ã€‚'
- en: '`embedding_size` (`int`, *optional*, defaults to 64) â€” Dimensionality (hidden
    size) for the embedding layer.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º64) â€” åµŒå…¥å±‚çš„ç»´åº¦ï¼ˆéšè—å¤§å°ï¼‰ã€‚'
- en: '`hidden_sizes` (`List[int]`, *optional*, defaults to `[256, 512, 1024, 2048]`)
    â€” Dimensionality (hidden size) at each stage.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_sizes` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[256, 512, 1024, 2048]`) â€” æ¯ä¸ªé˜¶æ®µçš„ç»´åº¦ï¼ˆéšè—å¤§å°ï¼‰ã€‚'
- en: '`depths` (`List[int]`, *optional*, defaults to `[3, 4, 6, 3]`) â€” Depth (number
    of layers) for each stage.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depths` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[3, 4, 6, 3]`) â€” æ¯ä¸ªé˜¶æ®µçš„æ·±åº¦ï¼ˆå±‚æ•°ï¼‰ã€‚'
- en: '`layer_type` (`str`, *optional*, defaults to `"bottleneck"`) â€” The layer to
    use, it can be either `"basic"` (used for smaller models, like resnet-18 or resnet-34)
    or `"bottleneck"` (used for larger models like resnet-50 and above).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"bottleneck"`) â€” è¦ä½¿ç”¨çš„å±‚ï¼Œå¯ä»¥æ˜¯ `"basic"`ï¼ˆç”¨äºè¾ƒå°çš„æ¨¡å‹ï¼Œå¦‚
    resnet-18 æˆ– resnet-34ï¼‰æˆ– `"bottleneck"`ï¼ˆç”¨äºè¾ƒå¤§çš„æ¨¡å‹ï¼Œå¦‚ resnet-50 åŠä»¥ä¸Šï¼‰ã€‚'
- en: '`hidden_act` (`str`, *optional*, defaults to `"relu"`) â€” The non-linear activation
    function in each block. If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"`
    are supported.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"relu"`) â€” æ¯ä¸ªå—ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`,
    `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚'
- en: '`downsample_in_first_stage` (`bool`, *optional*, defaults to `False`) â€” If
    `True`, the first stage will downsample the inputs using a `stride` of 2.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`downsample_in_first_stage` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼Œç¬¬ä¸€ä¸ªé˜¶æ®µå°†ä½¿ç”¨
    `stride` ä¸º 2 å¯¹è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·ã€‚'
- en: '`downsample_in_bottleneck` (`bool`, *optional*, defaults to `False`) â€” If `True`,
    the first conv 1x1 in ResNetBottleNeckLayer will downsample the inputs using a
    `stride` of 2.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`downsample_in_bottleneck` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼ŒResNetBottleNeckLayer
    ä¸­çš„ç¬¬ä¸€ä¸ª conv 1x1 å°†ä½¿ç”¨ `stride` ä¸º 2 å¯¹è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·ã€‚'
- en: '`out_features` (`List[str]`, *optional*) â€” If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_features` (`List[str]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œä¸»å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾åˆ—è¡¨ã€‚å¯ä»¥æ˜¯ `"stem"`, `"stage1"`,
    `"stage2"` ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº† `out_indices`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”æœªè®¾ç½® `out_indices`ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§
    `stage_names` å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚'
- en: '`out_indices` (`List[int]`, *optional*) â€” If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œä¸»å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾çš„ç´¢å¼•åˆ—è¡¨ã€‚å¯ä»¥æ˜¯ 0ã€1ã€2 ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº†
    `out_features`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”æœªè®¾ç½® `out_features`ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§ `stage_names`
    å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚'
- en: This is the configuration class to store the configuration of a [ResNetModel](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetModel).
    It is used to instantiate an ResNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ResNet [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50)
    architecture.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ [ResNetModel](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetModel)
    çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– ResNet æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº ResNet [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PytorchHide Pytorch content
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch å†…å®¹
- en: ResNetModel
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNetModel
- en: '### `class transformers.ResNetModel`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ResNetModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L311)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L311)'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare ResNet model outputting raw features without any specific head on top.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„ ResNet æ¨¡å‹è¾“å‡ºåŸå§‹ç‰¹å¾ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L325)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L325)'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    `tuple(torch.FloatTensor)`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    and inputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))å’Œè¾“å…¥ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” Last layer hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`) â€” åœ¨ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œæ± åŒ–æ“ä½œåçš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºçš„ä¸€ä¸ª
    + æ¯å±‚çš„è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: The [ResNetModel](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetModel)
    forward method, overrides the `__call__` special method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[ResNetModel](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ResNetForImageClassification
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNetForImageClassification
- en: '### `class transformers.ResNetForImageClassification`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ResNetForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L361)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L361)'
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: ResNet Model with an image classification head on top (a linear layer on top
    of the pooled features), e.g. for ImageNet.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´éƒ¨çš„ResNetæ¨¡å‹ï¼ˆåœ¨æ± åŒ–ç‰¹å¾ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L381)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_resnet.py#L381)'
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    and inputs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½®ï¼ˆ[ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: The [ResNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[ResNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlowå†…å®¹
- en: TFResNetModel
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFResNetModel
- en: '### `class transformers.TFResNetModel`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFResNetModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L472)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L472)'
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig)ï¼‰
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare ResNet model outputting raw features without any specific head on top.
    This model is a TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)
    sub-class. Use it as a regular TensorFlow Module and refer to the TensorFlow documentation
    for all matter related to general usage and behavior.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„ResNetæ¨¡å‹è¾“å‡ºåŸå§‹ç‰¹å¾ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TensorFlowæ¨¡å—ï¼Œå¹¶å‚è€ƒTensorFlowæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `call`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L481)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L481)'
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`)
    â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`
    or `tuple(tf.Tensor)`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`æˆ–`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    and inputs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`tf.Tensor`ï¼‰â€”
    æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) â€” Last layer
    hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`tf.Tensor`ï¼‰â€” åœ¨ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œæ± åŒ–æ“ä½œåçš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each layer) of shape `(batch_size, num_channels, height, width)`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: The [TFResNetModel](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.TFResNetModel)
    forward method, overrides the `__call__` special method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFResNetModel](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.TFResNetModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TFResNetForImageClassification
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFResNetForImageClassification
- en: '### `class transformers.TFResNetForImageClassification`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFResNetForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L519)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L519)'
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: ResNet Model with an image classification head on top (a linear layer on top
    of the pooled features), e.g. for ImageNet.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´éƒ¨çš„ResNetæ¨¡å‹ï¼ˆåœ¨æ± åŒ–ç‰¹å¾çš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚
- en: This model is a TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)
    sub-class. Use it as a regular TensorFlow Module and refer to the TensorFlow documentation
    for all matter related to general usage and behavior.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æ˜¯TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TensorFlowæ¨¡å—ï¼Œå¹¶å‚è€ƒTensorFlowæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `call`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L544)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_tf_resnet.py#L544)'
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`tf.Tensor`ï¼‰â€”
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) â€” Labels for computing
    the image classification/regression loss. Indices should be in `[0, ..., config.num_labels
    - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ...,
    config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`æˆ–`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    and inputs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`tf.Tensor`ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: The [TFResNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.TFResNetForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFResNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.TFResNetForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: JAXHide JAX content
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: JAXHide JAX content
- en: FlaxResNetModel
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxResNetModel
- en: '### `class transformers.FlaxResNetModel`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxResNetModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L576)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L576)'
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig)ï¼‰
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚
- en: The bare ResNet model outputting raw features without any specific head on top.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„ResNetæ¨¡å‹è¾“å‡ºåŸå§‹ç‰¹å¾ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä»PyTorchæ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„Flax
    linenæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„JAXç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L488)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L488)'
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Returns
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPoolingAndNoAttention`
    or `tuple(torch.FloatTensor)`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPoolingAndNoAttention`
    æˆ– `tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPoolingAndNoAttention`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.resnet.configuration_resnet.ResNetConfig'>`)
    and inputs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPoolingAndNoAttention`
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½® (`<class 'transformers.models.resnet.configuration_resnet.ResNetConfig'>`)
    å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, num_channels, height,
    width)`) â€” Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, num_channels, height,
    width)`) â€” æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) â€” Last
    layer hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) â€” ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œæ± åŒ–æ“ä½œåçš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states of the model at the output of each layer plus the optional
    initial embedding outputs.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€çš„å…ƒç»„ï¼ŒåŒ…æ‹¬å¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚'
- en: The `FlaxResNetPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxResNetPreTrainedModel` çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: FlaxResNetForImageClassification
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxResNetForImageClassification
- en: '### `class transformers.FlaxResNetForImageClassification`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxResNetForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L660)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L660)'
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ResNetConfig](/docs/transformers/v4.37.2/en/model_doc/resnet#transformers.ResNetConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`, *optional*, é»˜è®¤ä¸º `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯
    `jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨ GPU ä¸Šï¼‰å’Œ `jax.numpy.bfloat16`ï¼ˆåœ¨ TPU ä¸Šï¼‰ä¹‹ä¸€ã€‚'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº† `dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„ `dtype` æ‰§è¡Œã€‚
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`è¯·æ³¨æ„ï¼Œè¿™åªæŒ‡å®šäº†è®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœå¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜… [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    å’Œ [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚
- en: ResNet Model with an image classification head on top (a linear layer on top
    of the pooled features), e.g. for ImageNet.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´çš„ ResNet æ¨¡å‹ï¼ˆåœ¨æ± åŒ–ç‰¹å¾ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº ImageNetã€‚
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä»
    PyTorch æ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯ [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax linen Moduleï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè¿™ä¸ªæ¨¡å‹æ”¯æŒå†…åœ¨çš„ JAX ç‰¹æ€§ï¼Œæ¯”å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‘é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L488)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/resnet/modeling_flax_resnet.py#L488)'
- en: '[PRE18]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Returns
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_flax_outputs.FlaxImageClassifierOutputWithNoAttention`
    or `tuple(torch.FloatTensor)`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_flax_outputs.FlaxImageClassifierOutputWithNoAttention`
    æˆ– `tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_flax_outputs.FlaxImageClassifierOutputWithNoAttention`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.resnet.configuration_resnet.ResNetConfig'>`)
    and inputs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `transformers.modeling_flax_outputs.FlaxImageClassifierOutputWithNoAttention`
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.resnet.configuration_resnet.ResNetConfig'>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, config.num_labels)` çš„ `jnp.ndarray`ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1`
    åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’äº† `output_hidden_states=True`
    æˆ–å½“'
- en: '`config.output_hidden_states=True):` Tuple of `jnp.ndarray` (one for the output
    of the embeddings, if the model has an embedding layer, + one for the output of
    each stage) of shape `(batch_size, num_channels, height, width)`. Hidden-states
    (also called feature maps) of the model at the output of each stage.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config.output_hidden_states=True):` å½¢çŠ¶ä¸º `(batch_size, num_channels, height,
    width)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: The `FlaxResNetPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxResNetPreTrainedModel` çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
