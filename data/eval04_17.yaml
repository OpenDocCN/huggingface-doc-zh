- en: Types of Evaluations in 🤗 Evaluate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗评估中的评估类型
- en: 'Original text: [https://huggingface.co/docs/evaluate/types_of_evaluations](https://huggingface.co/docs/evaluate/types_of_evaluations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/evaluate/types_of_evaluations](https://huggingface.co/docs/evaluate/types_of_evaluations)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the 🤗 Evaluate library is to support different types of evaluation,
    depending on different goals, datasets and models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗评估库的目标是支持不同类型的评估，取决于不同的目标、数据集和模型。
- en: 'Here are the types of evaluations that are currently supported with a few examples
    for each:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是当前支持的评估类型及其各自的几个示例：
- en: Metrics
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 度量
- en: A metric measures the performance of a model on a given dataset. This is often
    based on an existing ground truth (i.e. a set of references), but there are also
    *referenceless metrics* which allow evaluating generated text by leveraging a
    pretrained model such as [GPT-2](https://huggingface.co/gpt2).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 度量标准衡量模型在给定数据集上的性能。这通常基于现有的地面真相（即一组参考），但也有*无参考度量标准*，允许通过利用预训练模型（如[GPT-2](https://huggingface.co/gpt2)）来评估生成的文本。
- en: 'Examples of metrics include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 度量的示例包括：
- en: '[Accuracy](https://huggingface.co/metrics/accuracy) : the proportion of correct
    predictions among the total number of cases processed.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[准确率](https://huggingface.co/metrics/accuracy)：在处理的总案例中，正确预测的比例。'
- en: '[Exact Match](https://huggingface.co/metrics/exact_match): the rate at which
    the input predicted strings exactly match their references.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[精确匹配](https://huggingface.co/metrics/exact_match)：输入预测字符串与其参考完全匹配的速率。'
- en: '[Mean Intersection over union (IoUO)](https://huggingface.co/metrics/mean_iou):
    the area of overlap between the predicted segmentation of an image and the ground
    truth divided by the area of union between the predicted segmentation and the
    ground truth.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[平均交集联合（IoUO）](https://huggingface.co/metrics/mean_iou)：图像预测分割与地面真相之间的重叠区域与图像预测分割与地面真相之间的联合区域之比。'
- en: Metrics are often used to track model performance on benchmark datasets, and
    to report progress on tasks such as [machine translation](https://huggingface.co/tasks/translation)
    and [image classification](https://huggingface.co/tasks/image-classification).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 度量标准通常用于跟踪模型在基准数据集上的性能，并报告诸如[机器翻译](https://huggingface.co/tasks/translation)和[图像分类](https://huggingface.co/tasks/image-classification)等任务的进展。
- en: Comparisons
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较
- en: Comparisons can be useful to compare the performance of two or more models on
    a single test dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 比较可以用于比较单个测试数据集上两个或多个模型的性能。
- en: For instance, the [McNemar Test](https://github.com/huggingface/evaluate/tree/main/comparisons/mcnemar)
    is a paired nonparametric statistical hypothesis test that takes the predictions
    of two models and compares them, aiming to measure whether the models’s predictions
    diverge or not. The p value it outputs, which ranges from `0.0` to `1.0`, indicates
    the difference between the two models’ predictions, with a lower p value indicating
    a more significant difference.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[麦克马尔检验](https://github.com/huggingface/evaluate/tree/main/comparisons/mcnemar)是一种成对的非参数统计假设检验，它接受两个模型的预测并比较它们，旨在衡量模型的预测是否发散。它输出的p值范围从`0.0`到`1.0`，指示两个模型预测之间的差异，较低的p值表示更显著的差异。
- en: Comparisons have yet to be systematically used when comparing and reporting
    model performance, however they are useful tools to go beyond simply comparing
    leaderboard scores and for getting more information on the way model prediction
    differ.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在比较和报告模型性能时，比较尚未系统地使用，但它们是超越简单比较排行榜分数并获取有关模型预测差异方式的有用工具。
- en: Measurements
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量
- en: In the 🤗 Evaluate library, measurements are tools for gaining more insights
    on datasets and model predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在🤗评估库中，测量是获取有关数据集和模型预测更多见解的工具。
- en: For instance, in the case of datasets, it can be useful to calculate the [average
    word length](https://github.com/huggingface/evaluate/tree/main/measurements/word_length)
    of a dataset’s entries, and how it is distributed — this can help when choosing
    the maximum input length for [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在数据集的情况下，可以计算数据集条目的[平均词长](https://github.com/huggingface/evaluate/tree/main/measurements/word_length)，以及其分布情况——这在选择[Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)的最大输入长度时会有所帮助。
- en: In the case of model predictions, it can help to calculate the average [perplexity](https://huggingface.co/metrics/perplexity)
    of model predictions using different models such as [GPT-2](https://huggingface.co/gpt2)
    and [BERT](https://huggingface.co/bert-base-uncased), which can indicate the quality
    of generated text when no reference is available.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型预测的情况下，可以帮助使用不同模型（如[GPT-2](https://huggingface.co/gpt2)和[BERT](https://huggingface.co/bert-base-uncased)）计算模型预测的平均[困惑度](https://huggingface.co/metrics/perplexity)，这可以指示在没有参考文本时生成文本的质量。
- en: All three types of evaluation supported by the 🤗 Evaluate library are meant
    to be mutually complementary, and help our community carry out more mindful and
    responsible evaluation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗评估库支持的三种评估类型旨在相互补充，并帮助我们的社区进行更加审慎和负责任的评估。
- en: We will continue adding more types of metrics, measurements and comparisons
    in coming months, and are counting on community involvement (via [PRs](https://github.com/huggingface/evaluate/compare)
    and [issues](https://github.com/huggingface/evaluate/issues/new/choose)) to make
    the library as extensive and inclusive as possible!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来几个月中，我们将继续添加更多类型的度量、测量和比较，并依靠社区参与（通过[PRs](https://github.com/huggingface/evaluate/compare)和[issues](https://github.com/huggingface/evaluate/issues/new/choose)）使该库尽可能广泛和包容！
