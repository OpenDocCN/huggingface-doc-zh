- en: Activation functions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/activations](https://huggingface.co/docs/diffusers/api/activations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/diffusers/api/activations](https://huggingface.co/docs/diffusers/api/activations)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Customized activation functions for supporting various models in 🤗 Diffusers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为🤗 Diffusers中支持各种模型的自定义激活函数。
- en: GELU
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GELU
- en: '### `class diffusers.models.activations.GELU`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.activations.GELU`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L50)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L50)'
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`dim_in` (`int`) — The number of channels in the input.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_in` (`int`) — 输入通道的数量。'
- en: '`dim_out` (`int`) — The number of channels in the output.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_out` (`int`) — 输出通道的数量。'
- en: '`approximate` (`str`, *optional*, defaults to `"none"`) — If `"tanh"`, use
    tanh approximation.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`approximate` (`str`, *可选*, 默认为`"none"`) — 如果为`"tanh"`，则使用tanh近似。'
- en: '`bias` (`bool`, defaults to True) — Whether to use a bias in the linear layer.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, 默认为True) — 是否在线性层中使用偏置。'
- en: GELU activation function with tanh approximation support with `approximate="tanh"`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 具有tanh近似支持的GELU激活函数，使用`approximate="tanh"`。
- en: GEGLU
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GEGLU
- en: '### `class diffusers.models.activations.GEGLU`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.activations.GEGLU`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L78)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L78)'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`dim_in` (`int`) — The number of channels in the input.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_in` (`int`) — 输入通道的数量。'
- en: '`dim_out` (`int`) — The number of channels in the output.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_out` (`int`) — 输出通道的数量。'
- en: '`bias` (`bool`, defaults to True) — Whether to use a bias in the linear layer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, 默认为True) — 是否在线性层中使用偏置。'
- en: A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation
    function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 门控线性单元激活函数的一种变体。
- en: ApproximateGELU
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ApproximateGELU
- en: '### `class diffusers.models.activations.ApproximateGELU`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.activations.ApproximateGELU`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L106)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L106)'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`dim_in` (`int`) — The number of channels in the input.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_in` (`int`) — 输入通道的数量。'
- en: '`dim_out` (`int`) — The number of channels in the output.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_out` (`int`) — 输出通道的数量。'
- en: '`bias` (`bool`, defaults to True) — Whether to use a bias in the linear layer.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, 默认为True) — 是否在线性层中使用偏置。'
- en: The approximate form of the Gaussian Error Linear Unit (GELU). For more details,
    see section 2 of this [paper](https://arxiv.org/abs/1606.08415).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯误差线性单元（GELU）的近似形式。更多细节请参见这篇[论文](https://arxiv.org/abs/1606.08415)的第2节。
