- en: Choosing a metric for your task
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºæ‚¨çš„ä»»åŠ¡é€‰æ‹©æŒ‡æ ‡
- en: 'Original text: [https://huggingface.co/docs/evaluate/choosing_a_metric](https://huggingface.co/docs/evaluate/choosing_a_metric)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/evaluate/choosing_a_metric](https://huggingface.co/docs/evaluate/choosing_a_metric)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**So youâ€™ve trained your model and want to see how well itâ€™s doing on a dataset
    of your choice. Where do you start?**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰€ä»¥æ‚¨å·²ç»è®­ç»ƒå¥½äº†æ¨¡å‹ï¼Œæƒ³è¦çœ‹çœ‹å®ƒåœ¨æ‚¨é€‰æ‹©çš„æ•°æ®é›†ä¸Šè¡¨ç°å¦‚ä½•ã€‚ä»å“ªé‡Œå¼€å§‹ï¼Ÿ**'
- en: 'There is no â€œone size fits allâ€ approach to choosing an evaluation metric,
    but some good guidelines to keep in mind are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©è¯„ä¼°æŒ‡æ ‡æ²¡æœ‰â€œä¸€åˆ€åˆ‡â€çš„æ–¹æ³•ï¼Œä½†ä¸€äº›å¥½çš„æŒ‡å¯¼åŸåˆ™è¦è®°ä½çš„æ˜¯ï¼š
- en: Categories of metrics
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒ‡æ ‡ç±»åˆ«
- en: 'There are 3 high-level categories of metrics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰3ä¸ªé«˜çº§åˆ«çš„æŒ‡æ ‡ç±»åˆ«ï¼š
- en: '*Generic metrics*, which can be applied to a variety of situations and datasets,
    such as precision and accuracy.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*é€šç”¨æŒ‡æ ‡*ï¼Œå¯åº”ç”¨äºå„ç§æƒ…å†µå’Œæ•°æ®é›†ï¼Œå¦‚precisionå’Œaccuracyã€‚'
- en: '*Task-specific metrics*, which are limited to a given task, such as Machine
    Translation (often evaluated using metrics [BLEU](https://huggingface.co/metrics/bleu)
    or [ROUGE](https://huggingface.co/metrics/rouge)) or Named Entity Recognition
    (often evaluated with [seqeval](https://huggingface.co/metrics/seqeval)).'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*ä»»åŠ¡ç‰¹å®šçš„æŒ‡æ ‡*ï¼Œé™äºç‰¹å®šä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ï¼ˆé€šå¸¸ä½¿ç”¨[BLEU](https://huggingface.co/metrics/bleu)æˆ–[ROUGE](https://huggingface.co/metrics/rouge)ç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼‰æˆ–å‘½åå®ä½“è¯†åˆ«ï¼ˆé€šå¸¸ä½¿ç”¨[seqeval](https://huggingface.co/metrics/seqeval)è¿›è¡Œè¯„ä¼°ï¼‰ã€‚'
- en: '*Dataset-specific metrics*, which aim to measure model performance on specific
    benchmarks: for instance, the [GLUE benchmark](https://huggingface.co/datasets/glue)
    has a dedicated [evaluation metric](https://huggingface.co/metrics/glue).'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ•°æ®é›†ç‰¹å®šçš„æŒ‡æ ‡*ï¼Œæ—¨åœ¨è¡¡é‡æ¨¡å‹åœ¨ç‰¹å®šåŸºå‡†ä¸Šçš„æ€§èƒ½ï¼šä¾‹å¦‚ï¼Œ[GLUEåŸºå‡†](https://huggingface.co/datasets/glue)æœ‰ä¸“é—¨çš„[è¯„ä¼°æŒ‡æ ‡](https://huggingface.co/metrics/glue)ã€‚'
- en: 'Letâ€™s look at each of these three cases:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸‰ç§æƒ…å†µï¼š
- en: Generic metrics
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šç”¨æŒ‡æ ‡
- en: Many of the metrics used in the Machine Learning community are quite generic
    and can be applied in a variety of tasks and datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ç¤¾åŒºä¸­ä½¿ç”¨çš„è®¸å¤šæŒ‡æ ‡éƒ½ç›¸å½“é€šç”¨ï¼Œå¯åº”ç”¨äºå„ç§ä»»åŠ¡å’Œæ•°æ®é›†ã€‚
- en: This is the case for metrics like [accuracy](https://huggingface.co/metrics/accuracy)
    and [precision](https://huggingface.co/metrics/precision), which can be used for
    evaluating labeled (supervised) datasets, as well as [perplexity](https://huggingface.co/metrics/perplexity),
    which can be used for evaluating different kinds of (unsupervised) generative
    tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é€‚ç”¨äºåƒ[accuracy](https://huggingface.co/metrics/accuracy)å’Œ[precision](https://huggingface.co/metrics/precision)è¿™æ ·çš„æŒ‡æ ‡ï¼Œå¯ç”¨äºè¯„ä¼°æ ‡è®°ï¼ˆç›‘ç£ï¼‰æ•°æ®é›†ï¼Œä»¥åŠ[perplexity](https://huggingface.co/metrics/perplexity)è¿™æ ·çš„æŒ‡æ ‡ï¼Œå¯ç”¨äºè¯„ä¼°ä¸åŒç±»å‹çš„ï¼ˆæ— ç›‘ç£ï¼‰ç”Ÿæˆä»»åŠ¡ã€‚
- en: 'To see the input structure of a given metric, you can look at its metric card.
    For example, in the case of [precision](https://huggingface.co/metrics/precision),
    the format is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ç»™å®šæŒ‡æ ‡çš„è¾“å…¥ç»“æ„ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹å…¶æŒ‡æ ‡å¡ã€‚ä¾‹å¦‚ï¼Œåœ¨[precision](https://huggingface.co/metrics/precision)çš„æƒ…å†µä¸‹ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Task-specific metrics
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»»åŠ¡ç‰¹å®šçš„æŒ‡æ ‡
- en: Popular ML tasks like Machine Translation and Named Entity Recognition have
    specific metrics that can be used to compare models. For example, a series of
    different metrics have been proposed for text generation, ranging from [BLEU](https://huggingface.co/metrics/bleu)
    and its derivatives such as [GoogleBLEU](https://huggingface.co/metrics/google_bleu)
    and [GLEU](https://huggingface.co/metrics/gleu), but also [ROUGE](https://huggingface.co/metrics/rouge),
    [MAUVE](https://huggingface.co/metrics/mauve), etc.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åƒæœºå™¨ç¿»è¯‘å’Œå‘½åå®ä½“è¯†åˆ«è¿™æ ·çš„æµè¡ŒMLä»»åŠ¡æœ‰ç‰¹å®šçš„æŒ‡æ ‡ï¼Œå¯ç”¨äºæ¯”è¾ƒæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œé’ˆå¯¹æ–‡æœ¬ç”Ÿæˆæå‡ºäº†ä¸€ç³»åˆ—ä¸åŒçš„æŒ‡æ ‡ï¼Œä»[BLEU](https://huggingface.co/metrics/bleu)åŠå…¶è¡ç”ŸæŒ‡æ ‡å¦‚[GoogleBLEU](https://huggingface.co/metrics/google_bleu)å’Œ[GLEU](https://huggingface.co/metrics/gleu)ï¼Œè¿˜æœ‰[ROUGE](https://huggingface.co/metrics/rouge)ã€[MAUVE](https://huggingface.co/metrics/mauve)ç­‰ã€‚
- en: 'You can find the right metric for your task by:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰¾åˆ°é€‚åˆæ‚¨ä»»åŠ¡çš„æ­£ç¡®æŒ‡æ ‡ï¼š
- en: '**Looking at the [Task pages](https://huggingface.co/tasks)** to see what metrics
    can be used for evaluating models for a given task.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥çœ‹[ä»»åŠ¡é¡µé¢](https://huggingface.co/tasks)**ï¼Œçœ‹çœ‹å¯ä»¥ç”¨äºè¯„ä¼°ç»™å®šä»»åŠ¡æ¨¡å‹çš„æŒ‡æ ‡ã€‚'
- en: '**Checking out leaderboards** on sites like [Papers With Code](https://paperswithcode.com/)
    (you can search by task and by dataset).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥çœ‹åƒ[Papers With Code](https://paperswithcode.com/)è¿™æ ·çš„ç½‘ç«™ä¸Šçš„æ’è¡Œæ¦œ**ï¼ˆæ‚¨å¯ä»¥æŒ‰ä»»åŠ¡å’Œæ•°æ®é›†æœç´¢ï¼‰ã€‚'
- en: '**Reading the metric cards** for the relevant metrics and see which ones are
    a good fit for your use case. For example, see the [BLEU metric card](https://github.com/huggingface/evaluate/tree/main/metrics/bleu)
    or [SQuaD metric card](https://github.com/huggingface/evaluate/tree/main/metrics/squad).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é˜…è¯»ç›¸å…³æŒ‡æ ‡çš„æŒ‡æ ‡å¡**ï¼Œçœ‹çœ‹å“ªäº›é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼ŒæŸ¥çœ‹[BLEUæŒ‡æ ‡å¡](https://github.com/huggingface/evaluate/tree/main/metrics/bleu)æˆ–[SQuaDæŒ‡æ ‡å¡](https://github.com/huggingface/evaluate/tree/main/metrics/squad)ã€‚'
- en: '**Looking at papers and blog posts** published on the topic and see what metrics
    they report. This can change over time, so try to pick papers from the last couple
    of years!'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥çœ‹å…³äºè¯¥ä¸»é¢˜å‘è¡¨çš„è®ºæ–‡å’Œåšå®¢æ–‡ç« **ï¼Œçœ‹çœ‹å®ƒä»¬æŠ¥å‘Šäº†å“ªäº›æŒ‡æ ‡ã€‚è¿™å¯èƒ½ä¼šéšæ—¶é—´å˜åŒ–ï¼Œå› æ­¤å°½é‡é€‰æ‹©æœ€è¿‘å‡ å¹´çš„è®ºæ–‡ï¼'
- en: Dataset-specific metrics
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ç‰¹å®šçš„æŒ‡æ ‡
- en: Some datasets have specific metrics associated with them â€” this is especially
    in the case of popular benchmarks like [GLUE](https://huggingface.co/metrics/glue)
    and [SQuAD](https://huggingface.co/metrics/squad).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æ•°æ®é›†æœ‰ä¸ä¹‹ç›¸å…³çš„ç‰¹å®šæŒ‡æ ‡ - å°¤å…¶æ˜¯åœ¨æµè¡ŒåŸºå‡†å¦‚[GLUE](https://huggingface.co/metrics/glue)å’Œ[SQuAD](https://huggingface.co/metrics/squad)çš„æƒ…å†µä¸‹ã€‚
- en: ğŸ’¡ GLUE is actually a collection of different subsets on different tasks, so
    first you need to choose the one that corresponds to the NLI task, such as mnli,
    which is described as â€œcrowdsourced collection of sentence pairs with textual
    entailment annotationsâ€
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ GLUEå®é™…ä¸Šæ˜¯ä¸åŒä»»åŠ¡ä¸Šä¸åŒå­é›†çš„é›†åˆï¼Œå› æ­¤é¦–å…ˆæ‚¨éœ€è¦é€‰æ‹©ä¸NLIä»»åŠ¡å¯¹åº”çš„å­é›†ï¼Œæ¯”å¦‚mnliï¼Œå®ƒè¢«æè¿°ä¸ºâ€œå¸¦æœ‰æ–‡æœ¬è•´æ¶µæ³¨é‡Šçš„ä¼—åŒ…å¥å¯¹é›†åˆâ€
- en: 'If you are evaluating your model on a benchmark dataset like the ones mentioned
    above, you can use its dedicated evaluation metric. Make sure you respect the
    format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad)
    dataset, you need to feed the `question` and `context` into your model and return
    the `prediction_text`, which should be compared with the `references` (based on
    matching the `id` of the question) :'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨è¯„ä¼°æ‚¨çš„æ¨¡å‹åœ¨ç±»ä¼¼ä¸Šé¢æåˆ°çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å…¶ä¸“ç”¨çš„è¯„ä¼°æŒ‡æ ‡ã€‚ç¡®ä¿æ‚¨éµå®ˆå®ƒä»¬è¦æ±‚çš„æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œè¦åœ¨[SQuAD](https://huggingface.co/datasets/squad)æ•°æ®é›†ä¸Šè¯„ä¼°æ‚¨çš„æ¨¡å‹ï¼Œæ‚¨éœ€è¦å°†`question`å’Œ`context`è¾“å…¥åˆ°æ‚¨çš„æ¨¡å‹ä¸­ï¼Œå¹¶è¿”å›`prediction_text`ï¼Œç„¶åå°†å…¶ä¸`references`ï¼ˆåŸºäºåŒ¹é…é—®é¢˜çš„`id`ï¼‰è¿›è¡Œæ¯”è¾ƒï¼š
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can find examples of dataset structures by consulting the â€œDataset Previewâ€
    function or the dataset card for a given dataset, and you can see how to use its
    dedicated evaluation function based on the metric card.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡æŸ¥çœ‹â€œæ•°æ®é›†é¢„è§ˆâ€åŠŸèƒ½æˆ–ç»™å®šæ•°æ®é›†çš„æ•°æ®é›†å¡æ¥æ‰¾åˆ°æ•°æ®é›†ç»“æ„çš„ç¤ºä¾‹ï¼Œå¹¶å¯ä»¥æ ¹æ®åº¦é‡å¡æ¥çœ‹å¦‚ä½•ä½¿ç”¨å…¶ä¸“ç”¨çš„è¯„ä¼°å‡½æ•°ã€‚
