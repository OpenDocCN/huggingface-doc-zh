- en: Choosing a metric for your task
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为您的任务选择指标
- en: 'Original text: [https://huggingface.co/docs/evaluate/choosing_a_metric](https://huggingface.co/docs/evaluate/choosing_a_metric)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/evaluate/choosing_a_metric](https://huggingface.co/docs/evaluate/choosing_a_metric)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**So you’ve trained your model and want to see how well it’s doing on a dataset
    of your choice. Where do you start?**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**所以您已经训练好了模型，想要看看它在您选择的数据集上表现如何。从哪里开始？**'
- en: 'There is no “one size fits all” approach to choosing an evaluation metric,
    but some good guidelines to keep in mind are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 选择评估指标没有“一刀切”的方法，但一些好的指导原则要记住的是：
- en: Categories of metrics
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标类别
- en: 'There are 3 high-level categories of metrics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有3个高级别的指标类别：
- en: '*Generic metrics*, which can be applied to a variety of situations and datasets,
    such as precision and accuracy.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*通用指标*，可应用于各种情况和数据集，如precision和accuracy。'
- en: '*Task-specific metrics*, which are limited to a given task, such as Machine
    Translation (often evaluated using metrics [BLEU](https://huggingface.co/metrics/bleu)
    or [ROUGE](https://huggingface.co/metrics/rouge)) or Named Entity Recognition
    (often evaluated with [seqeval](https://huggingface.co/metrics/seqeval)).'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*任务特定的指标*，限于特定任务，如机器翻译（通常使用[BLEU](https://huggingface.co/metrics/bleu)或[ROUGE](https://huggingface.co/metrics/rouge)等指标进行评估）或命名实体识别（通常使用[seqeval](https://huggingface.co/metrics/seqeval)进行评估）。'
- en: '*Dataset-specific metrics*, which aim to measure model performance on specific
    benchmarks: for instance, the [GLUE benchmark](https://huggingface.co/datasets/glue)
    has a dedicated [evaluation metric](https://huggingface.co/metrics/glue).'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据集特定的指标*，旨在衡量模型在特定基准上的性能：例如，[GLUE基准](https://huggingface.co/datasets/glue)有专门的[评估指标](https://huggingface.co/metrics/glue)。'
- en: 'Let’s look at each of these three cases:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这三种情况：
- en: Generic metrics
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通用指标
- en: Many of the metrics used in the Machine Learning community are quite generic
    and can be applied in a variety of tasks and datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习社区中使用的许多指标都相当通用，可应用于各种任务和数据集。
- en: This is the case for metrics like [accuracy](https://huggingface.co/metrics/accuracy)
    and [precision](https://huggingface.co/metrics/precision), which can be used for
    evaluating labeled (supervised) datasets, as well as [perplexity](https://huggingface.co/metrics/perplexity),
    which can be used for evaluating different kinds of (unsupervised) generative
    tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于像[accuracy](https://huggingface.co/metrics/accuracy)和[precision](https://huggingface.co/metrics/precision)这样的指标，可用于评估标记（监督）数据集，以及[perplexity](https://huggingface.co/metrics/perplexity)这样的指标，可用于评估不同类型的（无监督）生成任务。
- en: 'To see the input structure of a given metric, you can look at its metric card.
    For example, in the case of [precision](https://huggingface.co/metrics/precision),
    the format is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看给定指标的输入结构，您可以查看其指标卡。例如，在[precision](https://huggingface.co/metrics/precision)的情况下，格式如下：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Task-specific metrics
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务特定的指标
- en: Popular ML tasks like Machine Translation and Named Entity Recognition have
    specific metrics that can be used to compare models. For example, a series of
    different metrics have been proposed for text generation, ranging from [BLEU](https://huggingface.co/metrics/bleu)
    and its derivatives such as [GoogleBLEU](https://huggingface.co/metrics/google_bleu)
    and [GLEU](https://huggingface.co/metrics/gleu), but also [ROUGE](https://huggingface.co/metrics/rouge),
    [MAUVE](https://huggingface.co/metrics/mauve), etc.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 像机器翻译和命名实体识别这样的流行ML任务有特定的指标，可用于比较模型。例如，针对文本生成提出了一系列不同的指标，从[BLEU](https://huggingface.co/metrics/bleu)及其衍生指标如[GoogleBLEU](https://huggingface.co/metrics/google_bleu)和[GLEU](https://huggingface.co/metrics/gleu)，还有[ROUGE](https://huggingface.co/metrics/rouge)、[MAUVE](https://huggingface.co/metrics/mauve)等。
- en: 'You can find the right metric for your task by:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下方式找到适合您任务的正确指标：
- en: '**Looking at the [Task pages](https://huggingface.co/tasks)** to see what metrics
    can be used for evaluating models for a given task.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查看[任务页面](https://huggingface.co/tasks)**，看看可以用于评估给定任务模型的指标。'
- en: '**Checking out leaderboards** on sites like [Papers With Code](https://paperswithcode.com/)
    (you can search by task and by dataset).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查看像[Papers With Code](https://paperswithcode.com/)这样的网站上的排行榜**（您可以按任务和数据集搜索）。'
- en: '**Reading the metric cards** for the relevant metrics and see which ones are
    a good fit for your use case. For example, see the [BLEU metric card](https://github.com/huggingface/evaluate/tree/main/metrics/bleu)
    or [SQuaD metric card](https://github.com/huggingface/evaluate/tree/main/metrics/squad).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阅读相关指标的指标卡**，看看哪些适合您的用例。例如，查看[BLEU指标卡](https://github.com/huggingface/evaluate/tree/main/metrics/bleu)或[SQuaD指标卡](https://github.com/huggingface/evaluate/tree/main/metrics/squad)。'
- en: '**Looking at papers and blog posts** published on the topic and see what metrics
    they report. This can change over time, so try to pick papers from the last couple
    of years!'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查看关于该主题发表的论文和博客文章**，看看它们报告了哪些指标。这可能会随时间变化，因此尽量选择最近几年的论文！'
- en: Dataset-specific metrics
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集特定的指标
- en: Some datasets have specific metrics associated with them — this is especially
    in the case of popular benchmarks like [GLUE](https://huggingface.co/metrics/glue)
    and [SQuAD](https://huggingface.co/metrics/squad).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集有与之相关的特定指标 - 尤其是在流行基准如[GLUE](https://huggingface.co/metrics/glue)和[SQuAD](https://huggingface.co/metrics/squad)的情况下。
- en: 💡 GLUE is actually a collection of different subsets on different tasks, so
    first you need to choose the one that corresponds to the NLI task, such as mnli,
    which is described as “crowdsourced collection of sentence pairs with textual
    entailment annotations”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 GLUE实际上是不同任务上不同子集的集合，因此首先您需要选择与NLI任务对应的子集，比如mnli，它被描述为“带有文本蕴涵注释的众包句对集合”
- en: 'If you are evaluating your model on a benchmark dataset like the ones mentioned
    above, you can use its dedicated evaluation metric. Make sure you respect the
    format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad)
    dataset, you need to feed the `question` and `context` into your model and return
    the `prediction_text`, which should be compared with the `references` (based on
    matching the `id` of the question) :'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在评估您的模型在类似上面提到的基准数据集上的表现，您可以使用其专用的评估指标。确保您遵守它们要求的格式。例如，要在[SQuAD](https://huggingface.co/datasets/squad)数据集上评估您的模型，您需要将`question`和`context`输入到您的模型中，并返回`prediction_text`，然后将其与`references`（基于匹配问题的`id`）进行比较：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can find examples of dataset structures by consulting the “Dataset Preview”
    function or the dataset card for a given dataset, and you can see how to use its
    dedicated evaluation function based on the metric card.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看“数据集预览”功能或给定数据集的数据集卡来找到数据集结构的示例，并可以根据度量卡来看如何使用其专用的评估函数。
