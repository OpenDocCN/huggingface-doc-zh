- en: Troubleshoot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/troubleshooting](https://huggingface.co/docs/transformers/v4.37.2/en/troubleshooting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/troubleshooting](https://huggingface.co/docs/transformers/v4.37.2/en/troubleshooting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes errors occur, but we are here to help! This guide covers some of
    the most common issues we’ve seen and how you can resolve them. However, this
    guide isn’t meant to be a comprehensive collection of every 🤗 Transformers issue.
    For more help with troubleshooting your issue, try:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会出现错误，但我们在这里帮助您！本指南涵盖了我们见过的一些最常见问题以及您可以如何解决它们。但是，本指南并不旨在成为每个🤗 Transformers问题的全面集合。如需更多有关故障排除的帮助，请尝试：
- en: '[https://www.youtube-nocookie.com/embed/S2EEG3JIt2A](https://www.youtube-nocookie.com/embed/S2EEG3JIt2A)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/S2EEG3JIt2A](https://www.youtube-nocookie.com/embed/S2EEG3JIt2A)'
- en: Asking for help on the [forums](https://discuss.huggingface.co/). There are
    specific categories you can post your question to, like [Beginners](https://discuss.huggingface.co/c/beginners/5)
    or [🤗 Transformers](https://discuss.huggingface.co/c/transformers/9). Make sure
    you write a good descriptive forum post with some reproducible code to maximize
    the likelihood that your problem is solved!
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[论坛](https://discuss.huggingface.co/)上寻求帮助。您可以将问题发布到特定类别，如[初学者](https://discuss.huggingface.co/c/beginners/5)或[🤗
    Transformers](https://discuss.huggingface.co/c/transformers/9)。请确保编写一个具有一些可重现代码的良好描述性论坛帖子，以最大程度地提高解决问题的可能性！
- en: '[https://www.youtube-nocookie.com/embed/_PAli-V4wj0](https://www.youtube-nocookie.com/embed/_PAli-V4wj0)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/_PAli-V4wj0](https://www.youtube-nocookie.com/embed/_PAli-V4wj0)'
- en: Create an [Issue](https://github.com/huggingface/transformers/issues/new/choose)
    on the 🤗 Transformers repository if it is a bug related to the library. Try to
    include as much information describing the bug as possible to help us better figure
    out what’s wrong and how we can fix it.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是与库相关的错误，请在🤗 Transformers存储库上创建一个[Issue](https://github.com/huggingface/transformers/issues/new/choose)。尽量包含尽可能多描述错误的信息，以帮助我们更好地找出问题所在以及如何修复它。
- en: Check the [Migration](migration) guide if you use an older version of 🤗 Transformers
    since some important changes have been introduced between versions.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用较旧版本的🤗 Transformers，请查看[迁移](migration)指南，因为在版本之间引入了一些重要更改。
- en: For more details about troubleshooting and getting help, take a look at [Chapter
    8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有关故障排除和获取帮助的更多详细信息，请查看Hugging Face课程的[第8章](https://huggingface.co/course/chapter8/1?fw=pt)。
- en: Firewalled environments
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防火墙环境
- en: 'Some GPU instances on cloud and intranet setups are firewalled to external
    connections, resulting in a connection error. When your script attempts to download
    model weights or datasets, the download will hang and then timeout with the following
    message:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一些云端和内部设置的GPU实例被防火墙阻止对外部连接，导致连接错误。当您的脚本尝试下载模型权重或数据集时，下载将挂起，然后超时并显示以下消息：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this case, you should try to run 🤗 Transformers on [offline mode](installation#offline-mode)
    to avoid the connection error.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您应该尝试在[离线模式](installation#offline-mode)下运行🤗 Transformers以避免连接错误。
- en: CUDA out of memory
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA内存不足
- en: 'Training large models with millions of parameters can be challenging without
    the appropriate hardware. A common error you may encounter when the GPU runs out
    of memory is:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有适当硬件的情况下训练拥有数百万参数的大型模型可能会很具挑战性。当GPU内存不足时，您可能会遇到的常见错误是：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here are some potential solutions you can try to lessen memory use:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些潜在的解决方案，您可以尝试减少内存使用：
- en: Reduce the [`per_device_train_batch_size`](main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size)
    value in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中的[`per_device_train_batch_size`](main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size)值。
- en: Try using [`gradient_accumulation_steps`](main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps)
    in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    to effectively increase overall batch size.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中使用[`gradient_accumulation_steps`](main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps)来有效增加总体批量大小。
- en: Refer to the Performance [guide](performance) for more details about memory-saving
    techniques.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有关节省内存技术的更多详细信息，请参考性能[指南](performance)。
- en: Unable to load a saved TensorFlow model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无法加载保存的TensorFlow模型
- en: 'TensorFlow’s [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)
    method will save the entire model - architecture, weights, training configuration
    - in a single file. However, when you load the model file again, you may run into
    an error because 🤗 Transformers may not load all the TensorFlow-related objects
    in the model file. To avoid issues with saving and loading TensorFlow models,
    we recommend you:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的[model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)方法将整个模型（架构、权重、训练配置）保存在单个文件中。然而，当您再次加载模型文件时，可能会遇到错误，因为🤗
    Transformers可能不会加载模型文件中的所有与TensorFlow相关的对象。为避免保存和加载TensorFlow模型时出现问题，我们建议您：
- en: 'Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)
    and then reload the model with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)将模型权重保存为`h5`文件扩展名，然后使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)重新加载模型：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Save the model with `~TFPretrainedModel.save_pretrained` and load it again
    with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`~TFPretrainedModel.save_pretrained`保存模型，然后使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)再次加载它：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ImportError
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImportError
- en: 'Another common error you may encounter, especially if it is a newly released
    model, is `ImportError`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到另一种常见错误，特别是对于新发布的模型，即`ImportError`：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For these error types, check to make sure you have the latest version of 🤗
    Transformers installed to access the most recent models:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些错误类型，请确保您已安装了最新版本的🤗 Transformers 以访问最新的模型：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'CUDA error: device-side assert triggered'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'CUDA error: device-side assert triggered'
- en: Sometimes you may run into a generic CUDA error about an error in the device
    code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您可能会遇到有关设备代码错误的通用CUDA错误。
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should try to run the code on a CPU first to get a more descriptive error
    message. Add the following environment variable to the beginning of your code
    to switch to a CPU:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该首先尝试在CPU上运行代码，以获得更详细的错误消息。将以下环境变量添加到您的代码开头以切换到CPU：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Another option is to get a better traceback from the GPU. Add the following
    environment variable to the beginning of your code to get the traceback to point
    to the source of the error:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是从GPU获取更好的回溯。将以下环境变量添加到您的代码开头，以使回溯指向错误源：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Incorrect output when padding tokens aren’t masked
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当填充标记未被屏蔽时输出不正确
- en: In some cases, the output `hidden_state` may be incorrect if the `input_ids`
    include padding tokens. To demonstrate, load a model and tokenizer. You can access
    a model’s `pad_token_id` to see its value. The `pad_token_id` may be `None` for
    some models, but you can always manually set it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，如果`input_ids`包含填充标记，则输出的`hidden_state`可能是不正确的。为了演示，加载一个模型和分词器。您可以访问模型的`pad_token_id`以查看其值。对于一些模型，`pad_token_id`可能为`None`，但您总是可以手动设置它。
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following example shows the output without masking the padding tokens:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了不屏蔽填充标记时的输出：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the actual output of the second sequence:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是第二个序列的实际输出：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Most of the time, you should provide an `attention_mask` to your model to ignore
    the padding tokens to avoid this silent error. Now the output of the second sequence
    matches its actual output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，您应该为您的模型提供一个`attention_mask`来忽略填充标记，以避免这种潜在错误。现在第二个序列的输出与其实际输出匹配：
- en: By default, the tokenizer creates an `attention_mask` for you based on your
    specific tokenizer’s defaults.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，分词器根据特定分词器的默认值为您创建`attention_mask`。
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '🤗 Transformers doesn’t automatically create an `attention_mask` to mask a padding
    token if it is provided because:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers不会自动创建`attention_mask`来屏蔽填充标记，如果提供了填充标记，因为：
- en: Some models don’t have a padding token.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些模型没有填充标记。
- en: For some use-cases, users want a model to attend to a padding token.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些用例，用户希望模型关注填充标记。
- en: 'ValueError: Unrecognized configuration class XYZ for this kind of AutoModel'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ValueError: Unrecognized configuration class XYZ for this kind of AutoModel'
- en: 'Generally, we recommend using the [AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)
    class to load pretrained instances of models. This class can automatically infer
    and load the correct architecture from a given checkpoint based on the configuration.
    If you see this `ValueError` when loading a model from a checkpoint, this means
    the Auto class couldn’t find a mapping from the configuration in the given checkpoint
    to the kind of model you are trying to load. Most commonly, this happens when
    a checkpoint doesn’t support a given task. For instance, you’ll see this error
    in the following example because there is no GPT2 for question answering:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们建议使用[AutoModel](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModel)类来加载预训练模型的实例。这个类可以根据配置自动推断和加载给定检查点中的正确架构。如果在从检查点加载模型时看到`ValueError`，这意味着Auto类无法从给定检查点中的配置找到到您尝试加载的模型类型的映射。最常见的情况是，当检查点不支持给定任务时会发生这种情况。例如，在以下示例中，您将看到此错误，因为没有用于问答的GPT2：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
