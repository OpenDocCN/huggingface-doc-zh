- en: Two main approaches for solving RL problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决强化学习问题的两种主要方法
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/two-methods](https://huggingface.co/learn/deep-rl-course/unit1/two-methods)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit1/two-methods](https://huggingface.co/learn/deep-rl-course/unit1/two-methods)
- en: Now that we learned the RL framework, how do we solve the RL problem?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了RL框架，我们如何解决RL问题？
- en: In other words, how do we build an RL agent that can **select the actions that maximize
    its expected cumulative reward?**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们如何构建一个能够**选择最大化预期累积奖励的动作**的RL代理程序？
- en: 'The Policy π: the agent’s brain'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略π：代理程序的大脑
- en: The Policy **π** is the **brain of our Agent**, it’s the function that tells
    us what **action to take given the state we are in.** So it **defines the agent’s
    behavior** at a given time.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 策略**π**是我们代理程序的**大脑**，它是告诉我们在给定状态下应该采取什么**动作的函数**。因此，它**定义了代理程序在特定时间的行为**。
- en: '![Policy](../Images/83518e23a957f171ab1fe3fa7a6bbe35.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![策略](../Images/83518e23a957f171ab1fe3fa7a6bbe35.png)'
- en: Think of policy as the brain of our agent, the function that will tell us the
    action to take given a state
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 将策略视为我们代理程序的大脑，这个函数将告诉我们在给定状态下应该采取的动作
- en: This Policy **is the function we want to learn**, our goal is to find the optimal
    policy π*, the policy that **maximizes expected return** when the agent acts according
    to it. We find this π* **through training.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略**是我们想要学习的函数**，我们的目标是找到最优策略π*，当代理程序根据它行动时**最大化预期回报**的策略。我们通过训练找到这个π*。
- en: 'There are two approaches to train our agent to find this optimal policy π*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法来训练我们的代理程序找到这个最优策略π*：
- en: '**Directly,** by teaching the agent to learn which **action to take,** given
    the current state: **Policy-Based Methods.**'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的方法**，通过教导代理程序学习在当前状态下应该采取哪个动作。'
- en: 'Indirectly, **teach the agent to learn which state is more valuable** and then
    take the action that **leads to the more valuable states**: Value-Based Methods.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 间接地，**教导代理程序学习哪个状态更有价值**，然后采取**导致更有价值状态的动作**：基于值的方法。
- en: Policy-Based Methods
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于策略的方法
- en: In Policy-Based methods, **we learn a policy function directly.**
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方法中，**我们直接学习一个策略函数**。
- en: This function will define a mapping from each state to the best corresponding
    action. Alternatively, it could define **a probability distribution over the set
    of possible actions at that state.**
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将为每个状态定义一个映射到最佳对应动作的映射。或者，它可以定义**在该状态时可能动作集合的概率分布**。
- en: '![Policy](../Images/6758c67029516191953f67721d370c3e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![策略](../Images/6758c67029516191953f67721d370c3e.png)'
- en: As we can see here, the policy (deterministic) **directly indicates the action
    to take for each step.**
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，策略（确定性）**直接指示每一步应该采取的动作**。
- en: 'We have two types of policies:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种类型的策略：
- en: '*Deterministic*: a policy at a given state **will always return the same action.**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确定性*：在给定状态下，策略**总是返回相同的动作**。'
- en: '![Policy](../Images/ee7b2c571f3d32864167d58a4063e9d3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![策略](../Images/ee7b2c571f3d32864167d58a4063e9d3.png)'
- en: action = policy(state)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 动作 = 策略(状态)
- en: '![Policy](../Images/eab138ef553910f3be1dc2fc9a4c1d74.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![策略](../Images/eab138ef553910f3be1dc2fc9a4c1d74.png)'
- en: '*Stochastic*: outputs **a probability distribution over actions.**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机*：输出**动作的概率分布**。'
- en: '![Policy](../Images/651cfd4580146ecc7e4526d3c73cf0a3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![策略](../Images/651cfd4580146ecc7e4526d3c73cf0a3.png)'
- en: policy(actions | state) = probability distribution over the set of actions given
    the current state
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 策略(动作 | 状态) = 给定当前状态时动作集合的概率分布
- en: '![Policy Based](../Images/24f29ab03a77c489ff2e67423152c2f5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![基于策略的方法](../Images/24f29ab03a77c489ff2e67423152c2f5.png)'
- en: Given an initial state, our stochastic policy will output probability distributions
    over the possible actions at that state.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个初始状态，我们的随机策略将输出该状态可能动作的概率分布。
- en: 'If we recap:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下：
- en: '![Pbm recap](../Images/ccacb85446b779ce48e2b9c14852ff2b.png) ![Pbm recap](../Images/b65b570d8d5bdf35333f56e519f8ebbe.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Pbm recap](../Images/ccacb85446b779ce48e2b9c14852ff2b.png) ![Pbm recap](../Images/b65b570d8d5bdf35333f56e519f8ebbe.png)'
- en: Value-based methods
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于值的方法
- en: In value-based methods, instead of learning a policy function, we **learn a
    value function** that maps a state to the expected value **of being at that state.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于值的方法中，**我们不是学习一个策略函数，而是学习一个值函数**，它将一个状态映射到**在该状态时的预期值**。
- en: The value of a state is the **expected discounted return** the agent can get
    if it **starts in that state, and then acts according to our policy.**
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个状态的值是**代理程序在该状态开始，然后根据我们的策略行动时可以获得的预期折扣回报**。
- en: “Act according to our policy” just means that our policy is **“going to the
    state with the highest value”.**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: “根据我们的策略行动”只是意味着我们的策略是**“去到具有最高价值的状态”。**
- en: '![Value based RL](../Images/9f6cecad619e80db36cfe941f6772544.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![基于值的RL](../Images/9f6cecad619e80db36cfe941f6772544.png)'
- en: Here we see that our value function **defined values for each possible state.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们看到我们的值函数**为每个可能的状态定义了值**。
- en: '![Value based RL](../Images/a083a986fc5a289be300383a0bbc883f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![基于值的RL](../Images/a083a986fc5a289be300383a0bbc883f.png)'
- en: 'Thanks to our value function, at each step our policy will select the state
    with the biggest value defined by the value function: -7, then -6, then -5 (and
    so on) to attain the goal.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的值函数，在每一步，我们的策略将选择值函数定义的最大值的状态：-7，然后-6，然后-5（以此类推）以达到目标。
- en: 'Thanks to our value function, at each step our policy will select the state
    with the biggest value defined by the value function: -7, then -6, then -5 (and
    so on) to attain the goal.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的值函数，在每一步，我们的策略将选择值函数定义的最大值的状态：-7，然后-6，然后-5（以此类推）以达到目标。
- en: 'If we recap:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下：
- en: '![Vbm recap](../Images/ea2aeb10b56230f8695a306775324a19.png) ![Vbm recap](../Images/645f2468d4ce43d60dde1ff7d573fe7c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![Vbm recap](../Images/ea2aeb10b56230f8695a306775324a19.png) ![Vbm recap](../Images/645f2468d4ce43d60dde1ff7d573fe7c.png)'
