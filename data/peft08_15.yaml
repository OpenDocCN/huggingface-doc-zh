- en: Custom models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/custom_models](https://huggingface.co/docs/peft/developer_guides/custom_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/peft/developer_guides/custom_models](https://huggingface.co/docs/peft/developer_guides/custom_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Some fine-tuning techniques, such as prompt tuning, are specific to language
    models. That means in ğŸ¤— PEFT, it is assumed a ğŸ¤— Transformers model is being used.
    However, other fine-tuning techniques - like [LoRA](../conceptual_guides/lora)
    - are not restricted to specific model types.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›å¾®è°ƒæŠ€æœ¯ï¼Œå¦‚æç¤ºå¾®è°ƒï¼Œæ˜¯ç‰¹å®šäºè¯­è¨€æ¨¡å‹çš„ã€‚è¿™æ„å‘³ç€åœ¨ğŸ¤— PEFTä¸­ï¼Œå‡å®šæ­£åœ¨ä½¿ç”¨ğŸ¤— Transformersæ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶ä»–å¾®è°ƒæŠ€æœ¯ - å¦‚[LoRA](../conceptual_guides/lora)
    - ä¸é™äºç‰¹å®šçš„æ¨¡å‹ç±»å‹ã€‚
- en: In this guide, we will see how LoRA can be applied to a multilayer perceptron,
    a computer vision model from the [timm](https://huggingface.co/docs/timm/index)
    library, or a new ğŸ¤— Transformers architecture.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°LoRAå¦‚ä½•åº”ç”¨äºå¤šå±‚æ„ŸçŸ¥å™¨ï¼Œæ¥è‡ª[timm](https://huggingface.co/docs/timm/index)åº“çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼Œæˆ–è€…ä¸€ä¸ªæ–°çš„ğŸ¤—
    Transformersæ¶æ„ã€‚
- en: Multilayer perceptron
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨
- en: 'Letâ€™s assume that we want to fine-tune a multilayer perceptron with LoRA. Here
    is the definition:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³è¦ç”¨LoRAå¾®è°ƒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ã€‚è¿™æ˜¯å®šä¹‰ï¼š
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a straightforward multilayer perceptron with an input layer, a hidden
    layer, and an output layer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œæœ‰ä¸€ä¸ªè¾“å…¥å±‚ï¼Œä¸€ä¸ªéšè—å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ã€‚
- en: For this toy example, we choose an exceedingly large number of hidden units
    to highlight the efficiency gains from PEFT, but those gains are in line with
    more realistic examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç©å…·ç¤ºä¾‹ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ä¸ªéå¸¸å¤§çš„éšè—å•å…ƒæ•°ï¼Œä»¥çªå‡ºPEFTçš„æ•ˆç‡æå‡ï¼Œä½†è¿™äº›æå‡ä¸æ›´ç°å®çš„ç¤ºä¾‹ä¸€è‡´ã€‚
- en: 'There are a few linear layers in this model that could be tuned with LoRA.
    When working with common ğŸ¤— Transformers models, PEFT will know which layers to
    apply LoRA to, but in this case, it is up to us as a user to choose the layers.
    To determine the names of the layers to tune:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¸­æœ‰ä¸€äº›çº¿æ€§å±‚å¯ä»¥ç”¨LoRAè°ƒæ•´ã€‚å½“ä½¿ç”¨å¸¸è§çš„ğŸ¤— Transformersæ¨¡å‹æ—¶ï¼ŒPEFTä¼šçŸ¥é“è¦å°†LoRAåº”ç”¨äºå“ªäº›å±‚ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½œä¸ºç”¨æˆ·éœ€è¦é€‰æ‹©å±‚ã€‚è¦ç¡®å®šè¦è°ƒæ•´çš„å±‚çš„åç§°ï¼š
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This should print:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥æ‰“å°ï¼š
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Letâ€™s say we want to apply LoRA to the input layer and to the hidden layer,
    those are `''seq.0''` and `''seq.2''`. Moreover, letâ€™s assume we want to update
    the output layer without LoRA, that would be `''seq.4''`. The corresponding config
    would be:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³å°†LoRAåº”ç”¨äºè¾“å…¥å±‚å’Œéšè—å±‚ï¼Œå®ƒä»¬åˆ†åˆ«æ˜¯`'seq.0'`å’Œ`'seq.2'`ã€‚æ­¤å¤–ï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦æ›´æ–°è¾“å‡ºå±‚è€Œä¸ä½¿ç”¨LoRAï¼Œé‚£å°†æ˜¯`'seq.4'`ã€‚ç›¸åº”çš„é…ç½®å°†æ˜¯ï¼š
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With that, we can create our PEFT model and check the fraction of parameters
    trained:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„PEFTæ¨¡å‹å¹¶æ£€æŸ¥è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we can use any training framework we like, or write our own fit loop,
    to train the `peft_model`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•æˆ‘ä»¬å–œæ¬¢çš„è®­ç»ƒæ¡†æ¶ï¼Œæˆ–è€…ç¼–å†™æˆ‘ä»¬è‡ªå·±çš„æ‹Ÿåˆå¾ªç¯ï¼Œæ¥è®­ç»ƒ`peft_model`ã€‚
- en: For a complete example, check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb)ã€‚
- en: timm models
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: timmæ¨¡å‹
- en: The [timm](https://huggingface.co/docs/timm/index) library contains a large
    number of pretrained computer vision models. Those can also be fine-tuned with
    PEFT. Letâ€™s check out how this works in practice.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[timm](https://huggingface.co/docs/timm/index)åº“åŒ…å«å¤§é‡é¢„è®­ç»ƒçš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ä¹Ÿå¯ä»¥ç”¨PEFTè¿›è¡Œå¾®è°ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚'
- en: 'To start, ensure that timm is installed in the Python environment:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆç¡®ä¿timmå·²å®‰è£…åœ¨Pythonç¯å¢ƒä¸­ï¼š
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next we load a timm model for an image classification task:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºå›¾åƒåˆ†ç±»ä»»åŠ¡åŠ è½½ä¸€ä¸ªtimmæ¨¡å‹ï¼š
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Again, we need to make a decision about what layers to apply LoRA to. Since
    LoRA supports 2D conv layers, and since those are a major building block of this
    model, we should apply LoRA to the 2D conv layers. To identify the names of those
    layers, letâ€™s look at all the layer names:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦å†³å®šè¦å°†LoRAåº”ç”¨äºå“ªäº›å±‚ã€‚ç”±äºLoRAæ”¯æŒ2Då·ç§¯å±‚ï¼Œå¹¶ä¸”è¿™äº›å±‚æ˜¯è¯¥æ¨¡å‹çš„ä¸»è¦æ„å»ºæ¨¡å—ï¼Œæˆ‘ä»¬åº”è¯¥å°†LoRAåº”ç”¨äº2Då·ç§¯å±‚ã€‚ä¸ºäº†ç¡®å®šè¿™äº›å±‚çš„åç§°ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ‰€æœ‰çš„å±‚åç§°ï¼š
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will print a very long list, weâ€™ll only show the first few:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ‰“å°ä¸€ä¸ªéå¸¸é•¿çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬åªæ˜¾ç¤ºå‰å‡ ä¸ªï¼š
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Upon closer inspection, we see that the 2D conv layers have names such as `"stages.0.blocks.0.mlp.fc1"`
    and `"stages.0.blocks.0.mlp.fc2"`. How can we match those layer names specifically?
    You can write a [regular expressions](https://docs.python.org/3/library/re.html)
    to match the layer names. For our case, the regex `r".*\.mlp\.fc\d"` should do
    the job.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä»”ç»†æ£€æŸ¥åï¼Œæˆ‘ä»¬çœ‹åˆ°2Då·ç§¯å±‚çš„åç§°å¦‚`"stages.0.blocks.0.mlp.fc1"`å’Œ`"stages.0.blocks.0.mlp.fc2"`ã€‚æˆ‘ä»¬å¦‚ä½•åŒ¹é…è¿™äº›å±‚çš„åç§°ï¼Ÿæ‚¨å¯ä»¥ç¼–å†™ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…å±‚çš„åç§°ã€‚å¯¹äºæˆ‘ä»¬çš„æƒ…å†µï¼Œæ­£åˆ™è¡¨è¾¾å¼`r".*\.mlp\.fc\d"`åº”è¯¥å¯ä»¥èƒœä»»ã€‚
- en: 'Furthermore, as in the first example, we should ensure that the output layer,
    in this case the classification head, is also updated. Looking at the end of the
    list printed above, we can see that itâ€™s named `''head.fc''`. With that in mind,
    here is our LoRA config:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸€æ ·ï¼Œæˆ‘ä»¬åº”è¯¥ç¡®ä¿è¾“å‡ºå±‚ï¼Œå³åˆ†ç±»å¤´ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¹Ÿè¢«æ›´æ–°ã€‚æŸ¥çœ‹ä¸Šé¢æ‰“å°åˆ—è¡¨çš„æœ«å°¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒçš„åç§°æ˜¯`'head.fc'`ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„LoRAé…ç½®ï¼š
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we only need to create the PEFT model by passing our base model and the
    config to `get_peft_model`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åªéœ€è¦é€šè¿‡å°†åŸºæœ¬æ¨¡å‹å’Œé…ç½®ä¼ é€’ç»™`get_peft_model`æ¥åˆ›å»ºPEFTæ¨¡å‹ï¼š
- en: '[PRE10]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This shows us that we only need to train less than 2% of all parameters, which
    is a huge efficiency gain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å‘æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬åªéœ€è¦è®­ç»ƒä¸åˆ°æ‰€æœ‰å‚æ•°çš„2ï¼…ï¼Œè¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ•ˆç‡æå‡ã€‚
- en: For a complete example, check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb)ã€‚
- en: New transformers architectures
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–°çš„transformersæ¶æ„
- en: When new popular transformers architectures are released, we do our best to
    quickly add them to PEFT. If you come across a transformers model that is not
    supported out of the box, donâ€™t worry, it will most likely still work if the config
    is set correctly. Specifically, you have to identify the layers that should be
    adapted and set them correctly when initializing the corresponding config class,
    e.g. `LoraConfig`. Here are some tips to help with this.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‘å¸ƒæ–°çš„çƒ­é—¨transformersæ¶æ„æ—¶ï¼Œæˆ‘ä»¬ä¼šå°½å¿«å°†å®ƒä»¬æ·»åŠ åˆ°PEFTä¸­ã€‚å¦‚æœæ‚¨é‡åˆ°ä¸€ä¸ªä¸æ”¯æŒçš„transformersæ¨¡å‹ï¼Œä¸ç”¨æ‹…å¿ƒï¼Œåªè¦é…ç½®æ­£ç¡®ï¼Œå®ƒå¾ˆå¯èƒ½ä»ç„¶å¯ä»¥å·¥ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å¿…é¡»è¯†åˆ«åº”è¯¥é€‚åº”çš„å±‚ï¼Œå¹¶åœ¨åˆå§‹åŒ–ç›¸åº”çš„é…ç½®ç±»æ—¶æ­£ç¡®è®¾ç½®å®ƒä»¬ï¼Œä¾‹å¦‚`LoraConfig`ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸®åŠ©çš„æç¤ºã€‚
- en: 'As a first step, it is a good idea is to check the existing models for inspiration.
    You can find them inside of [constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)
    in the PEFT repository. Often, youâ€™ll find a similar architecture that uses the
    same names. For example, if the new model architecture is a variation of the â€œmistralâ€
    model and you want to apply LoRA, you can see that the entry for â€œmistralâ€ in
    `TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING` contains `["q_proj", "v_proj"]`.
    This tells you that for â€œmistralâ€ models, the `target_modules` for LoRA should
    be `["q_proj", "v_proj"]`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ£€æŸ¥ç°æœ‰æ¨¡å‹ä»¥è·å–çµæ„Ÿæ˜¯ä¸ªå¥½ä¸»æ„ã€‚æ‚¨å¯ä»¥åœ¨PEFTå­˜å‚¨åº“çš„[constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)ä¸­æ‰¾åˆ°å®ƒä»¬ã€‚é€šå¸¸ï¼Œæ‚¨ä¼šå‘ç°ä¸€ä¸ªç±»ä¼¼çš„æ¶æ„ä½¿ç”¨ç›¸åŒçš„åç§°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ–°æ¨¡å‹æ¶æ„æ˜¯â€œmistralâ€æ¨¡å‹çš„å˜ä½“ï¼Œå¹¶ä¸”æ‚¨æƒ³åº”ç”¨LoRAï¼Œæ‚¨å¯ä»¥çœ‹åˆ°`TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING`ä¸­â€œmistralâ€æ¡ç›®åŒ…å«`["q_proj",
    "v_proj"]`ã€‚è¿™å‘Šè¯‰æ‚¨å¯¹äºâ€œmistralâ€æ¨¡å‹ï¼ŒLoRAçš„`target_modules`åº”è¯¥æ˜¯`["q_proj", "v_proj"]`ï¼š
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If that doesnâ€™t help, check the existing modules in your model architecture
    with the `named_modules` method and try to identify the attention layers, especially
    the key, query, and value layers. Those will often have names such as `c_attn`,
    `query`, `q_proj`, etc. The key layer is not always adapted, and ideally, you
    should check whether including it results in better performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™æ²¡æœ‰å¸®åŠ©ï¼Œè¯·ä½¿ç”¨`named_modules`æ–¹æ³•æ£€æŸ¥æ¨¡å‹æ¶æ„ä¸­çš„ç°æœ‰æ¨¡å—ï¼Œå¹¶å°è¯•è¯†åˆ«æ³¨æ„åŠ›å±‚ï¼Œç‰¹åˆ«æ˜¯å…³é”®ã€æŸ¥è¯¢å’Œå€¼å±‚ã€‚è¿™äº›é€šå¸¸ä¼šæœ‰åç§°ï¼Œå¦‚`c_attn`ã€`query`ã€`q_proj`ç­‰ã€‚å…³é”®å±‚å¹¶ä¸æ€»æ˜¯é€‚åº”çš„ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥æ£€æŸ¥åŒ…æ‹¬å®ƒæ˜¯å¦ä¼šå¯¼è‡´æ›´å¥½çš„æ€§èƒ½ã€‚
- en: Additionally, linear layers are common targets to be adapted (e.g. in [QLoRA
    paper](https://arxiv.org/abs/2305.14314), authors suggest to adapt them as well).
    Their names will often contain the strings `fc` or `dense`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œçº¿æ€§å±‚æ˜¯å¸¸è§çš„é€‚åº”ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œåœ¨[QLoRAè®ºæ–‡](https://arxiv.org/abs/2305.14314)ä¸­ï¼Œä½œè€…å»ºè®®ä¹Ÿå¯¹å…¶è¿›è¡Œé€‚åº”ï¼‰ã€‚å®ƒä»¬çš„åç§°é€šå¸¸ä¼šåŒ…å«å­—ç¬¦ä¸²`fc`æˆ–`dense`ã€‚
- en: If you want to add a new model to PEFT, please create an entry in [constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)
    and open a pull request on the [repository](https://github.com/huggingface/peft/pulls).
    Donâ€™t forget to update the [README](https://github.com/huggingface/peft#models-support-matrix)
    as well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³å°†æ–°æ¨¡å‹æ·»åŠ åˆ°PEFTï¼Œè¯·åœ¨[constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)ä¸­åˆ›å»ºä¸€ä¸ªæ¡ç›®ï¼Œå¹¶åœ¨[å­˜å‚¨åº“](https://github.com/huggingface/peft/pulls)ä¸Šæ‰“å¼€ä¸€ä¸ªæ‹‰å–è¯·æ±‚ã€‚ä¸è¦å¿˜è®°æ›´æ–°[README](https://github.com/huggingface/peft#models-support-matrix)ã€‚
- en: Verify parameters and layers
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éªŒè¯å‚æ•°å’Œå±‚
- en: You can verify whether youâ€™ve correctly applied a PEFT method to your model
    in a few ways.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼éªŒè¯æ˜¯å¦å·²æ­£ç¡®å°†PEFTæ–¹æ³•åº”ç”¨äºæ‚¨çš„æ¨¡å‹ã€‚
- en: Check the fraction of parameters that are trainable with the [print_trainable_parameters()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters)
    method. If this number is lower or higher than expected, check the model `repr`
    by printing the model. This shows the names of all the layer types in the model.
    Ensure that only the intended target layers are replaced by the adapter layers.
    For example, if LoRA is applied to `nn.Linear` layers, then you should only see
    `lora.Linear` layers being used.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[print_trainable_parameters()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters)æ–¹æ³•æ£€æŸ¥å¯è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹ã€‚å¦‚æœæ­¤æ•°å­—ä½äºæˆ–é«˜äºé¢„æœŸï¼Œè¯·é€šè¿‡æ‰“å°æ¨¡å‹æ¥æ£€æŸ¥æ¨¡å‹çš„`repr`ã€‚è¿™å°†æ˜¾ç¤ºæ¨¡å‹ä¸­æ‰€æœ‰å±‚ç±»å‹çš„åç§°ã€‚ç¡®ä¿åªæœ‰é¢„æœŸçš„ç›®æ ‡å±‚è¢«é€‚é…å±‚æ›¿æ¢ã€‚ä¾‹å¦‚ï¼Œå¦‚æœLoRAåº”ç”¨äº`nn.Linear`å±‚ï¼Œåˆ™åº”åªçœ‹åˆ°ä½¿ç”¨`lora.Linear`å±‚ã€‚
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Another way you can view the adapted layers is to use the `targeted_module_names`
    attribute to list the name of each module that was adapted.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æŸ¥çœ‹é€‚åº”çš„å±‚çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`targeted_module_names`å±æ€§åˆ—å‡ºæ¯ä¸ªå·²é€‚åº”æ¨¡å—çš„åç§°ã€‚
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
