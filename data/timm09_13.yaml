- en: Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: 'Original text: [https://huggingface.co/docs/timm/reference/optimizers](https://huggingface.co/docs/timm/reference/optimizers)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/timm/reference/optimizers](https://huggingface.co/docs/timm/reference/optimizers)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This page contains the API reference documentation for learning rate optimizers
    included in `timm`.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此页面包含了`timm`中包含的学习率优化器的API参考文档。
- en: Optimizers
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: Factory functions
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工厂函数
- en: '#### `timm.optim.create_optimizer`'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `timm.optim.create_optimizer`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L182)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L182)'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Legacy optimizer factory for backwards compatibility. NOTE: Use create_optimizer_v2
    for new code.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 用于向后兼容的旧优化器工厂。注意：新代码请使用create_optimizer_v2。
- en: '#### `timm.optim.create_optimizer_v2`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `timm.optim.create_optimizer_v2`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L193)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L193)'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model_or_params` (nn.Module) — model containing parameters to optimize opt
    — name of optimizer to create lr — initial learning rate weight_decay — weight
    decay to apply in optimizer momentum — momentum for momentum based optimizers
    (others may use betas via kwargs) foreach — Enable / disable foreach (multi-tensor)
    operation if True / False. Choose safe default if None filter_bias_and_bn — filter
    out bias, bn and other 1d params from weight decay **kwargs — extra optimizer
    specific kwargs to pass through'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_or_params`（nn.Module）- 包含要优化的参数的模型 opt - 要创建的优化器的名称 lr - 初始学习率 weight_decay
    - 优化器应用的权重衰减 momentum - 动量用于基于动量的优化器（其他可能使用kwargs中的betas）foreach - 如果为True，则启用/禁用foreach（多张量）操作。如果为None，则选择安全默认值
    filter_bias_and_bn - 从权重衰减中过滤出偏置、bn和其他1d参数 **kwargs - 传递的额外优化器特定kwargs'
- en: Create an optimizer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个优化器。
- en: 'TODO currently the model is passed in and all parameters are selected for optimization.
    For more general use an interface that allows selection of parameters to optimize
    and lr groups, one of:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: TODO：目前传入模型并选择所有参数进行优化。为了更通用的用途，需要一个允许选择要优化的参数和lr组的接口之一：
- en: a filter fn interface that further breaks params into groups in a weight_decay
    compatible fashion
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个过滤器fn接口，以一种与weight_decay兼容的方式进一步将参数分组
- en: expose the parameters interface and leave it up to caller
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴露参数接口，并由调用者决定
- en: Optimizer Classes
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器类
- en: '### `class timm.optim.AdaBelief`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.AdaBelief`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L6)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L6)'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代）- 要优化的参数的可迭代对象或定义参数组的字典'
- en: '`lr` (float, optional) — learning rate (default: 1e-3)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（浮点数，可选）- 学习率（默认值：1e-3）'
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square (default: (0.9, 0.999))'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas`（Tuple[float, float]，可选）- 用于计算梯度及其平方的运行平均值的系数（默认值：（0.9，0.999））'
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-16)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（浮点数，可选）- 添加到分母以提高数值稳定性的项（默认值：1e-16）'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（浮点数，可选）- 权重衰减（L2惩罚）（默认值：0）'
- en: '`amsgrad` (boolean, optional) — whether to use the AMSGrad variant of this
    algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amsgrad`（布尔值，可选）- 是否使用来自论文`On the Convergence of Adam and Beyond`_的AMSGrad变体（默认值：False）'
- en: '`decoupled_decay` (boolean, optional) — (default: True) If set as True, then
    the optimizer uses decoupled weight decay as in AdamW'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoupled_decay`（布尔值，可选）- （默认值：True）如果设置为True，则优化器使用类似于AdamW的解耦权重衰减'
- en: '`fixed_decay` (boolean, optional) — (default: False) This is used when weight*decouple
    is set as True. When fixed_decay == True, the weight decay is performed as $W*{new}
    = W*{old} - W*{old} \times decay$. When fixed*decay == False, the weight decay
    is performed as $W*{new} = W*{old} - W*{old} \times decay \times lr$. Note that
    in this case, the weight decay ratio decreases with learning rate (lr).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fixed_decay`（布尔值，可选）- （默认值：False）当设置weight*decouple为True时使用。当fixed_decay ==
    True时，权重衰减为$W*{new} = W*{old} - W*{old} \times decay$。当fixed*decay == False时，权重衰减为$W*{new}
    = W*{old} - W*{old} \times decay \times lr$。请注意，在这种情况下，权重衰减比率随学习率（lr）减少。'
- en: '`rectify` (boolean, optional) — (default: True) If set as True, then perform
    the rectified update similar to RAdam'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rectify`（布尔值，可选）- （默认值：True）如果设置为True，则执行类似于RAdam的矫正更新'
- en: '`degenerated_to_sgd` (boolean, optional) (default —True) If set as True, then
    perform SGD update when variance of gradient is high'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`degenerated_to_sgd`（布尔值，可选）（默认值为True）如果设置为True，则在梯度方差较高时执行SGD更新'
- en: Implements AdaBelief algorithm. Modified from Adam in PyTorch
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 实现AdaBelief算法。修改自PyTorch中的Adam
- en: 'reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed
    gradients, NeurIPS 2020'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：AdaBelief Optimizer，通过对观察到的梯度的信念来调整步长，NeurIPS 2020
- en: For a complete table of recommended hyperparameters, see [https://github.com/juntang-zhuang/Adabelief-Optimizer’](https://github.com/juntang-zhuang/Adabelief-Optimizer')
    For example train/args for EfficientNet see these gists
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有关推荐超参数的完整表格，请参见[https://github.com/juntang-zhuang/Adabelief-Optimizer’](https://github.com/juntang-zhuang/Adabelief-Optimizer')
    例如EfficientNet的train/args，请参见这些gists
- en: 'link to train_scipt: [https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037](https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练脚本链接：[https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037](https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037)
- en: 'link to args.yaml: [https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3](https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: args.yaml链接：[https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3](https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3)
- en: '#### `step`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L89)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L89)'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用，可选）— 重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.Adafactor`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.Adafactor`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L16)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L16)'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）— 要优化的参数或定义参数组的字典的可迭代对象'
- en: '`lr` (float, optional) — external learning rate (default: None)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（浮点数，可选）— 外部学习率（默认值：None）'
- en: '`eps` (tuple[float, float]) — regularization constants for square gradient
    and parameter scale respectively (default: (1e-30, 1e-3))'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（元组[浮点数，浮点数]）— 正则化常数，用于平方梯度和参数比例（默认值：（1e-30，1e-3））'
- en: '`clip_threshold` (float) — threshold of root mean square of final gradient
    update (default: 1.0)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_threshold`（浮点数）— 最终梯度更新的均方根阈值（默认值：1.0）'
- en: '`decay_rate` (float) — coefficient used to compute running averages of square
    gradient (default: -0.8)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decay_rate`（浮点数）— 用于计算平方梯度的运行平均值的系数（默认值：-0.8）'
- en: '`beta1` (float) — coefficient used for computing running averages of gradient
    (default: None)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta1`（浮点数）— 用于计算梯度的运行平均值的系数（默认值：None）'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（浮点数，可选）— 权重衰减（L2惩罚）（默认值：0）'
- en: '`scale_parameter` (bool) — if True, learning rate is scaled by root mean square
    of parameter (default: True)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_parameter`（布尔值）— 如果为True，则学习率将按参数的均方根缩放（默认值：True）'
- en: '`warmup_init` (bool) — time-dependent learning rate computation depends on
    whether warm-up initialization is being used (default: False)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_init`（布尔值）— 时间相关的学习率计算取决于是否正在使用热身初始化（默认值：False）'
- en: 'Implements Adafactor algorithm. This implementation is based on: `Adafactor:
    Adaptive Learning Rates with Sublinear Memory Cost` (see [https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了Adafactor算法。此实现基于：`Adafactor：具有次线性内存成本的自适应学习率`（参见[https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235)）
- en: Note that this optimizer internally adjusts the learning rate depending on the
    *scale_parameter*, *relative_step* and *warmup_init* options.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此优化器根据*scale_parameter*、*relative_step*和*warmup_init*选项内部调整学习率。
- en: To use a manual (external) learning rate schedule you should set `scale_parameter=False`
    and `relative_step=False`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用手动（外部）学习率调度，您应该设置`scale_parameter=False`和`relative_step=False`。
- en: '#### `step`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L79)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L79)'
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用，可选）— 重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.Adahessian`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.Adahessian`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L9)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L9)'
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）— 要优化的参数或定义参数组的字典的可迭代对象'
- en: '`lr` (float, optional) — learning rate (default: 0.1)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（浮点数，可选）— 学习率（默认值：0.1）'
- en: '`betas` ((float, float), optional) — coefficients used for computing running
    averages of gradient and the squared hessian trace (default: (0.9, 0.999))'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas`（（浮点数，浮点数），可选）— 用于计算梯度和平方海森迹的运行平均值的系数（默认值：（0.9，0.999））'
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-8）'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0.0)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（浮点数，可选）— 权重衰减（L2惩罚）（默认值：0.0）'
- en: '`hessian_power` (float, optional) — exponent of the hessian trace (default:
    1.0)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hessian_power`（浮点数，可选）— 海森迹的指数（默认值：1.0）'
- en: '`update_each` (int, optional) — compute the hessian trace approximation only
    after *this* number of steps (to save time) (default: 1)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update_each`（整数，可选）— 仅在*此*步骤数后计算海森迹近似（以节省时间）（默认值：1）'
- en: '`n_samples` (int, optional) — how many times to sample `z` for the approximation
    of the hessian trace (default: 1)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_samples`（整数，可选）— 用于近似海森迹的`z`采样次数（默认值：1）'
- en: 'Implements the AdaHessian algorithm from “ADAHESSIAN: An Adaptive Second OrderOptimizer
    for Machine Learning”'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了“ADAHESSIAN：一种自适应的二阶优化器”中的AdaHessian算法
- en: '#### `get_params`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_params`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L58)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L58)'
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Gets all parameters in all param_groups with gradients
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 获取所有具有梯度的param_groups中的所有参数
- en: '#### `set_hessian`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_hessian`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L74)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L74)'
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Computes the Hutchinson approximation of the hessian trace and accumulates it
    for each trainable parameter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Hutchinson近似的海森迹并为每个可训练参数累积它。
- en: '#### `step`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L102)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L102)'
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — a closure that reevaluates the model and returns
    the loss (default — None)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用，可选）— 重新评估模型并返回损失的闭包（默认值为None）'
- en: Performs a single optimization step.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '#### `zero_hessian`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `zero_hessian`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L65)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L65)'
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Zeros out the accumalated hessian traces.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将累积的海森迹清零。
- en: '### `class timm.optim.AdamP`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.AdamP`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamp.py#L43)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamp.py#L43)'
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '### `class timm.optim.AdamW`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.AdamW`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L12)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L12)'
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）—要优化的参数或定义参数组的字典'
- en: '`lr` (float, optional) — learning rate (default: 1e-3)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（float，可选）—学习率（默认值：1e-3）'
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square (default: (0.9, 0.999))'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas`（Tuple[float, float]，可选）—用于计算梯度及其平方的运行平均值的系数（默认值：（0.9，0.999））'
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（float，可选）—添加到分母以提高数值稳定性的项（默认值：1e-8）'
- en: '`weight_decay` (float, optional) — weight decay coefficient (default: 1e-2)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（float，可选）—权重衰减系数（默认值：1e-2）'
- en: '`amsgrad` (boolean, optional) — whether to use the AMSGrad variant of this
    algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amsgrad`（布尔值，可选）—是否使用来自论文`Adam及其收敛性之外`的AMSGrad变体的算法（默认值：False）'
- en: Implements AdamW algorithm.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实现AdamW算法。
- en: 'The original Adam algorithm was proposed in `Adam: A Method for Stochastic
    Optimization`*. The AdamW variant was proposed in `Decoupled Weight Decay Regularization`*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的Adam算法是在`Adam：随机优化方法`中提出的。AdamW变体是在`解耦权重衰减正则化`中提出的。
- en: '.. _Adam\: A Method for Stochastic Optimization: [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
    .. _Decoupled Weight Decay Regularization: [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)
    .. _On the Convergence of Adam and Beyond: [https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: .. _Adam：随机优化方法：[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
    .. _解耦权重衰减正则化：[https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)
    .. _Adam及其收敛性之外：[https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)
- en: '#### `step`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L58)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L58)'
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用对象，可选）—重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.Lamb`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.Lamb`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L60)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L60)'
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）—要优化的参数或定义参数组的字典。'
- en: '`lr` (float, optional) — learning rate. (default: 1e-3)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（float，可选）—学习率。（默认值：1e-3）'
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its norm. (default: (0.9, 0.999))'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas`（Tuple[float, float]，可选）—用于计算梯度及其范数的运行平均值的系数。（默认值：（0.9，0.999））'
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability. (default: 1e-8)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（float，可选）—添加到分母以提高数值稳定性的项。（默认值：1e-8）'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（float，可选）—权重衰减（L2惩罚）（默认值：0）'
- en: '`grad_averaging` (bool, optional) — whether apply (1-beta2) to grad when calculating
    running averages of gradient. (default: True)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad_averaging`（bool，可选）—在计算梯度的运行平均值时是否应用（1-beta2）（默认值：True）'
- en: '`max_grad_norm` (float, optional) — value used to clip global grad norm (default:
    1.0)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_grad_norm`（float，可选）—用于裁剪全局梯度范数的值（默认值：1.0）'
- en: '`trust_clip` (bool) — enable LAMBC trust ratio clipping (default: False)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trust_clip`（bool）—启用LAMBC信任比例裁剪（默认值：False）'
- en: '`always_adapt` (boolean, optional) — Apply adaptive learning rate to 0.0 weight
    decay parameter (default: False)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`always_adapt`（布尔值，可选）—将自适应学习率应用于0.0权重衰减参数（默认值：False）'
- en: 'Implements a pure pytorch variant of FuseLAMB (NvLamb variant) optimizer from
    apex.optimizers.FusedLAMB reference: [https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了FuseLAMB（NvLamb变体）优化器的纯pytorch变体，来自apex.optimizers.FusedLAMB参考：[https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py)
- en: 'LAMB was proposed in `Large Batch Optimization for Deep Learning: Training
    BERT in 76 minutes`_.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LAMB是在`大批量优化深度学习：在76分钟内训练BERT`中提出的。
- en: '.. _Large Batch Optimization for Deep Learning - Training BERT in 76 minutes:
    [https://arxiv.org/abs/1904.00962](https://arxiv.org/abs/1904.00962) .. _On the
    Convergence of Adam and Beyond: [https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: .. _大批量优化深度学习 - 在76分钟内训练BERT：[https://arxiv.org/abs/1904.00962](https://arxiv.org/abs/1904.00962)
    .. _Adam及其收敛性之外：[https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)
- en: '#### `step`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L96)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L96)'
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用对象，可选）—重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.Lars`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.Lars`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L17)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L17)'
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）—要优化的参数或定义参数组的字典。'
- en: '`lr` (float, optional) — learning rate (default: 1.0).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（float，可选）—学习率（默认值：1.0）。'
- en: '`momentum` (float, optional) — momentum factor (default: 0)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`momentum`（float，可选）—动量因子（默认值：0）'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（float，可选）—权重衰减（L2惩罚）（默认值：0）'
- en: '`dampening` (float, optional) — dampening for momentum (default: 0)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dampening`（浮点数，可选）— 动量的阻尼（默认值：0）'
- en: '`nesterov` (bool, optional) — enables Nesterov momentum (default: False)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nesterov`（布尔值，可选）— 启用Nesterov动量（默认值：False）'
- en: '`trust_coeff` (float) — trust coefficient for computing adaptive lr / trust_ratio
    (default: 0.001)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trust_coeff`（浮点数）— 用于计算自适应lr / trust_ratio的信任系数（默认值：0.001）'
- en: '`eps` (float) — eps for division denominator (default: 1e-8)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（浮点数）— 除法分母的eps（默认值：1e-8）'
- en: '`trust_clip` (bool) — enable LARC trust ratio clipping (default: False)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trust_clip`（布尔值）— 启用LARC信任比例剪裁（默认值：False）'
- en: '`always_adapt` (bool) — always apply LARS LR adapt, otherwise only when group
    weight_decay != 0 (default: False)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`always_adapt`（布尔值）— 总是应用LARS LR适应，否则仅当组权重衰减！= 0时（默认值：False）'
- en: LARS for PyTorch
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的LARS
- en: 'Paper: `Large batch training of Convolutional Networks` - [https://arxiv.org/pdf/1708.03888.pdf](https://arxiv.org/pdf/1708.03888.pdf)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 论文：`卷积网络的大批量训练` - [https://arxiv.org/pdf/1708.03888.pdf](https://arxiv.org/pdf/1708.03888.pdf)
- en: '#### `step`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L75)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L75)'
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用，可选）— 重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.Lookahead`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.Lookahead`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lookahead.py#L15)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lookahead.py#L15)'
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '### `class timm.optim.MADGRAD`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.MADGRAD`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L24)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L24)'
- en: '[PRE19]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — Iterable of parameters to optimize or dicts defining
    parameter groups.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）— 要优化的参数或定义参数组的字典。'
- en: '`lr` (float) — Learning rate (default: 1e-2).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（浮点数）— 学习率（默认值：1e-2）。'
- en: '`momentum` (float) — Momentum value in the range [0,1) (default: 0.9).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`momentum`（浮点数）— 动量值范围为[0,1)（默认值：0.9）。'
- en: '`weight_decay` (float) — Weight decay, i.e. a L2 penalty (default: 0).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（浮点数）— 权重衰减，即L2惩罚（默认值：0）。'
- en: '`eps` (float) — Term added to the denominator outside of the root operation
    to improve numerical stability. (default: 1e-6).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（浮点数）— 添加到根操作之外的分母以提高数值稳定性的项（默认值：1e-6）。'
- en: 'MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic
    Optimization.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'MADGRAD_: 一种用于随机优化的动量化，自适应，双平均梯度方法。'
- en: '.. _MADGRAD: [https://arxiv.org/abs/2101.11075](https://arxiv.org/abs/2101.11075)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '.. _MADGRAD: [https://arxiv.org/abs/2101.11075](https://arxiv.org/abs/2101.11075)'
- en: MADGRAD is a general purpose optimizer that can be used in place of SGD or Adam
    may converge faster and generalize better. Currently GPU-only. Typically, the
    same learning rate schedule that is used for SGD or Adam may be used. The overall
    learning rate is not comparable to either method and should be determined by a
    hyper-parameter sweep.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: MADGRAD是一种通用优化器，可以用来替代SGD或Adam，可能会更快地收敛并更好地泛化。目前仅支持GPU。通常，可以使用与SGD或Adam相同的学习率调度。总体学习率不能与任一方法相比，并且应由超参数扫描确定。
- en: MADGRAD requires less weight decay than other methods, often as little as zero.
    Momentum values used for SGD or Adam’s beta1 should work here also.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: MADGRAD需要比其他方法更少的权重衰减，通常甚至可以为零。在SGD或Adam的beta1中使用的动量值也可以在这里使用。
- en: On sparse problems both weight_decay and momentum should be set to 0.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏问题上，weight_decay和momentum都应该设置为0。
- en: '#### `step`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L85)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L85)'
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用，可选）— 重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.Nadam`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.Nadam`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L7)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L7)'
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）— 要优化的参数或定义参数组的字典'
- en: '`lr` (float, optional) — learning rate (default: 2e-3)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（浮点数，可选）— 学习率（默认值：2e-3）'
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas`（Tuple[float, float]，可选）— 用于计算梯度及其平方的运行平均值的系数'
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-8）'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（浮点数，可选）— 权重衰减（L2惩罚）（默认值：0）'
- en: '`schedule_decay` (float, optional) — momentum schedule decay (default: 4e-3)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schedule_decay`（浮点数，可选）— 动量调度衰减（默认值：4e-3）'
- en: Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 实现Nadam算法（基于Nesterov动量的Adam变体）。
- en: It has been proposed in `Incorporating Nesterov Momentum into Adam`__.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了`将Nesterov动量纳入Adam`__。
- en: '**[http://cs229.stanford.edu/proj2015/054_report.pdf](http://cs229.stanford.edu/proj2015/054_report.pdf)**
    [http://www.cs.toronto.edu/~fritz/absps/momentum.pdf](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**[http://cs229.stanford.edu/proj2015/054_report.pdf](http://cs229.stanford.edu/proj2015/054_report.pdf)**
    [http://www.cs.toronto.edu/~fritz/absps/momentum.pdf](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)'
- en: 'Originally taken from: [https://github.com/pytorch/pytorch/pull/1408](https://github.com/pytorch/pytorch/pull/1408)
    NOTE: Has potential issues but does work well on some problems.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最初取自：[https://github.com/pytorch/pytorch/pull/1408](https://github.com/pytorch/pytorch/pull/1408)
    注意：可能存在潜在问题，但在某些问题上效果很好。
- en: '#### `step`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L43)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L43)'
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用，可选）— 重新评估模型并返回损失的闭包。'
- en: Performs a single optimization step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.NvNovoGrad`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.NvNovoGrad`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L13)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L13)'
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）— 要优化的参数的可迭代对象或定义参数组的字典'
- en: '`lr` (float, optional) — learning rate (default: 1e-3)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（浮点数，可选）— 学习率（默认值：1e-3）'
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square (default: (0.95, 0.98))'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas`（Tuple[float, float]，可选）— 用于计算梯度及其平方的运行平均值的系数（默认值：（0.95，0.98））'
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-8）'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0) grad_averaging
    — gradient averaging'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（浮点数，可选）— 权重衰减（L2惩罚）（默认值：0）grad_averaging — 梯度平均'
- en: '`amsgrad` (boolean, optional) — whether to use the AMSGrad variant of this
    algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amsgrad`（布尔值，可选）— 是否使用来自论文`On the Convergence of Adam and Beyond`_的AMSGrad变体的算法（默认值：False）'
- en: Implements Novograd algorithm.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 实现Novograd算法。
- en: '#### `step`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L54)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L54)'
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用对象，可选）— 重新评估模型的闭包'
- en: '`and` returns the loss. —'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`and` 返回损失。—'
- en: Performs a single optimization step.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.RAdam`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.RAdam`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/radam.py#L10)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/radam.py#L10)'
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '### `class timm.optim.RMSpropTF`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.RMSpropTF`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L14)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L14)'
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`（可迭代对象）— 要优化的参数的可迭代对象或定义参数组的字典'
- en: '`lr` (float, optional) — learning rate (default: 1e-2)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`（浮点数，可选）— 学习率（默认值：1e-2）'
- en: '`momentum` (float, optional) — momentum factor (default: 0)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`momentum`（浮点数，可选）— 动量因子（默认值：0）'
- en: '`alpha` (float, optional) — smoothing (decay) constant (default: 0.9)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`（浮点数，可选）— 平滑（衰减）常数（默认值：0.9）'
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-10)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-10）'
- en: '`centered` (bool, optional) — if `True`, compute the centered RMSProp, the
    gradient is normalized by an estimation of its variance'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`centered`（布尔值，可选）— 如果为`True`，计算中心化的RMSProp，梯度通过其方差的估计进行归一化'
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`（浮点数，可选）— 权重衰减（L2惩罚）（默认值：0）'
- en: '`decoupled_decay` (bool, optional) — decoupled weight decay as per [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoupled_decay`（布尔值，可选）— 根据[https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)进行解耦的权重衰减'
- en: '`lr_in_momentum` (bool, optional) — learning rate scaling is included in the
    momentum buffer update as per defaults in Tensorflow'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_in_momentum`（布尔值，可选）— 学习率缩放包含在动量缓冲区更新中，与Tensorflow中的默认值相同'
- en: Implements RMSprop algorithm (TensorFlow style epsilon)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 实现RMSprop算法（TensorFlow风格epsilon）
- en: 'NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before
    sqrt and a few other modifications to closer match Tensorflow for matching hyper-params.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这是PyTorch RMSprop的直接剪切和粘贴，eps应用在平方根之前，并对Tensorflow进行了一些其他修改以更接近匹配超参数。
- en: 'Noteworthy changes include:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的变化包括：
- en: Epsilon applied inside square-root
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Epsilon应用在平方根内
- en: square_avg initialized to ones
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: square_avg初始化为1
- en: LR scaling of update accumulated in momentum buffer
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在动量缓冲区中累积的更新的LR缩放
- en: Proposed by G. Hinton in his [course](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 由G. Hinton在他的[课程](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)中提出。
- en: The centered version first appears in [Generating Sequences With Recurrent Neural
    Networks](https://arxiv.org/pdf/1308.0850v5.pdf).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化版本首次出现在[使用递归神经网络生成序列](https://arxiv.org/pdf/1308.0850v5.pdf)中。
- en: '#### `step`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `step`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L72)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L72)'
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closure`（可调用对象，可选）— 重新评估模型并返回损失。'
- en: Performs a single optimization step.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤。
- en: '### `class timm.optim.SGDP`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class timm.optim.SGDP`'
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/sgdp.py#L19)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/sgdp.py#L19)'
- en: '[PRE28]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
