- en: Training customization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®šåˆ¶
- en: 'Original text: [https://huggingface.co/docs/trl/customization](https://huggingface.co/docs/trl/customization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/trl/customization](https://huggingface.co/docs/trl/customization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TRL is designed with modularity in mind so that users to be able to efficiently
    customize the training loop for their needs. Below are some examples on how you
    can apply and test different techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TRLæ—¨åœ¨å…·æœ‰æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸ºå…¶éœ€æ±‚å®šåˆ¶è®­ç»ƒå¾ªç¯ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºå¦‚ä½•åº”ç”¨å’Œæµ‹è¯•ä¸åŒæŠ€æœ¯çš„ç¤ºä¾‹ã€‚
- en: Train on multiple GPUs / nodes
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨å¤šä¸ªGPU/èŠ‚ç‚¹ä¸Šè®­ç»ƒ
- en: The trainers in TRL use ğŸ¤— Accelerate to enable distributed training across multiple
    GPUs or nodes. To do so, first create an ğŸ¤— Accelerate config file by running
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TRLä¸­çš„è®­ç»ƒå™¨ä½¿ç”¨ğŸ¤— Accelerateæ¥å®ç°è·¨å¤šä¸ªGPUæˆ–èŠ‚ç‚¹çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œé¦–å…ˆé€šè¿‡è¿è¡Œåˆ›å»ºä¸€ä¸ªğŸ¤— Accelerateé…ç½®æ–‡ä»¶
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'and answering the questions according to your multi-gpu / multi-node setup.
    You can then launch distributed training by running:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶æ ¹æ®æ‚¨çš„å¤šGPU/å¤šèŠ‚ç‚¹è®¾ç½®å›ç­”é—®é¢˜ã€‚ç„¶åé€šè¿‡è¿è¡Œå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼š
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We also provide config files in the [examples folder](https://github.com/huggingface/trl/tree/main/examples/accelerate_configs)
    that can be used as templates. To use these templates, simply pass the path to
    the config file when launching a job, e.g.:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜åœ¨[ç¤ºä¾‹æ–‡ä»¶å¤¹](https://github.com/huggingface/trl/tree/main/examples/accelerate_configs)ä¸­æä¾›é…ç½®æ–‡ä»¶ï¼Œå¯ç”¨ä½œæ¨¡æ¿ã€‚è¦ä½¿ç”¨è¿™äº›æ¨¡æ¿ï¼Œåªéœ€åœ¨å¯åŠ¨ä½œä¸šæ—¶ä¼ é€’é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œä¾‹å¦‚ï¼š
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Refer to the [examples page](https://github.com/huggingface/trl/tree/main/examples)
    for more details.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ[ç¤ºä¾‹é¡µé¢](https://github.com/huggingface/trl/tree/main/examples)ã€‚
- en: Distributed training with DeepSpeed
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DeepSpeedè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
- en: 'All of the trainers in TRL can be run on multiple GPUs together with DeepSpeed
    ZeRO-{1,2,3} for efficient sharding of the optimizer states, gradients, and model
    weights. To do so, run:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TRLä¸­çš„æ‰€æœ‰è®­ç»ƒå™¨éƒ½å¯ä»¥ä¸DeepSpeed ZeRO-{1,2,3}ä¸€èµ·åœ¨å¤šä¸ªGPUä¸Šè¿è¡Œï¼Œä»¥å®ç°ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹æƒé‡çš„æœ‰æ•ˆåˆ†ç‰‡ã€‚è¦è¿™æ ·åšï¼Œè¯·è¿è¡Œï¼š
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that for ZeRO-3, a small tweak is needed to initialize your reward model
    on the correct device via the `zero3_init_context_manager()` context manager.
    In particular, this is needed to avoid DeepSpeed hanging after a fixed number
    of training steps. Here is a snippet of what is involved from the [`sentiment_tuning`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py)
    example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¯¹äºZeRO-3ï¼Œéœ€è¦é€šè¿‡`zero3_init_context_manager()`ä¸Šä¸‹æ–‡ç®¡ç†å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šåˆå§‹åŒ–æ‚¨çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œä¸€äº›å°è°ƒæ•´ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™æ˜¯ä¸ºäº†é¿å…DeepSpeedåœ¨å›ºå®šæ•°é‡çš„è®­ç»ƒæ­¥éª¤åæŒ‚èµ·ã€‚ä»¥ä¸‹æ˜¯ä»[`sentiment_tuning`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py)ç¤ºä¾‹ä¸­æ¶‰åŠçš„éƒ¨åˆ†å†…å®¹ï¼š
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Consult the ğŸ¤— Accelerate [documentation](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)
    for more information about the DeepSpeed plugin.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³DeepSpeedæ’ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒğŸ¤— Accelerate[æ–‡æ¡£](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)ã€‚
- en: Use different optimizers
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨
- en: 'By default, the `PPOTrainer` creates a `torch.optim.Adam` optimizer. You can
    create and define a different optimizer and pass it to `PPOTrainer`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œ`PPOTrainer`ä¼šåˆ›å»ºä¸€ä¸ª`torch.optim.Adam`ä¼˜åŒ–å™¨ã€‚æ‚¨å¯ä»¥åˆ›å»ºå¹¶å®šä¹‰ä¸€ä¸ªä¸åŒçš„ä¼˜åŒ–å™¨ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™`PPOTrainer`ï¼š
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For memory efficient fine-tuning, you can also pass `Adam8bit` optimizer from
    `bitsandbytes`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´èŠ‚çœå†…å­˜çš„å¾®è°ƒï¼Œæ‚¨è¿˜å¯ä»¥ä»`bitsandbytes`ä¼ é€’`Adam8bit`ä¼˜åŒ–å™¨ï¼š
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Use LION optimizer
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LIONä¼˜åŒ–å™¨
- en: 'You can use the new [LION optimizer from Google](https://arxiv.org/abs/2302.06675)
    as well, first take the source code of the optimizer definition [here](https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/lion_pytorch.py),
    and copy it so that you can import the optimizer. Make sure to initialize the
    optimizer by considering the trainable parameters only for a more memory efficient
    training:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨æ¥è‡ªGoogleçš„æ–°[LIONä¼˜åŒ–å™¨](https://arxiv.org/abs/2302.06675)ï¼Œé¦–å…ˆè·å–ä¼˜åŒ–å™¨å®šä¹‰çš„æºä»£ç [æ­¤å¤„](https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/lion_pytorch.py)ï¼Œå¹¶å¤åˆ¶å®ƒä»¥ä¾¿å¯ä»¥å¯¼å…¥ä¼˜åŒ–å™¨ã€‚ç¡®ä¿é€šè¿‡ä»…è€ƒè™‘å¯è®­ç»ƒå‚æ•°æ¥åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼Œä»¥å®ç°æ›´èŠ‚çœå†…å­˜çš„è®­ç»ƒï¼š
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We advise you to use the learning rate that you would use for `Adam` divided
    by 3 as pointed out [here](https://github.com/lucidrains/lion-pytorch#lion---pytorch).
    We observed an improvement when using this optimizer compared to classic Adam
    (check the full logs [here](https://wandb.ai/distill-bloom/trl/runs/lj4bheke?workspace=user-younesbelkada)):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨`Adam`çš„å­¦ä¹ ç‡é™¤ä»¥3ï¼Œå¦‚[æ­¤å¤„](https://github.com/lucidrains/lion-pytorch#lion---pytorch)æ‰€æŒ‡å‡ºã€‚ä¸ç»å…¸çš„Adamç›¸æ¯”ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä½¿ç”¨æ­¤ä¼˜åŒ–å™¨æ—¶æœ‰æ‰€æ”¹å–„ï¼ˆæŸ¥çœ‹å®Œæ•´æ—¥å¿—[æ­¤å¤„](https://wandb.ai/distill-bloom/trl/runs/lj4bheke?workspace=user-younesbelkada)ï¼‰ï¼š
- en: '![](../Images/df158f76e736e28cb11b4474cc879ebe.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df158f76e736e28cb11b4474cc879ebe.png)'
- en: Add a learning rate scheduler
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
- en: You can also play with your training by adding learning rate schedulers!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€šè¿‡æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨æ¥è°ƒæ•´è®­ç»ƒï¼
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Memory efficient fine-tuning by sharing layers
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡å…±äº«å±‚è¿›è¡Œå†…å­˜é«˜æ•ˆå¾®è°ƒ
- en: Another tool you can use for more memory efficient fine-tuning is to share layers
    between the reference model and the model you want to train.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯ä»¥ç”¨äºæ›´èŠ‚çœå†…å­˜çš„å¾®è°ƒçš„å·¥å…·æ˜¯åœ¨å‚è€ƒæ¨¡å‹å’Œæ‚¨æƒ³è¦è®­ç»ƒçš„æ¨¡å‹ä¹‹é—´å…±äº«å±‚ã€‚
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Pass 8-bit reference models
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼ é€’8ä½å‚è€ƒæ¨¡å‹
- en: Since `trl` supports all key word arguments when loading a model from `transformers`
    using `from_pretrained`, you can also leverage `load_in_8bit` from `transformers`
    for more memory efficient fine-tuning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº`trl`åœ¨ä½¿ç”¨`from_pretrained`ä»`transformers`åŠ è½½æ¨¡å‹æ—¶æ”¯æŒæ‰€æœ‰å…³é”®å­—å‚æ•°ï¼Œæ‚¨è¿˜å¯ä»¥åˆ©ç”¨`transformers`ä¸­çš„`load_in_8bit`è¿›è¡Œæ›´èŠ‚çœå†…å­˜çš„å¾®è°ƒã€‚
- en: Read more about 8-bit model loading in `transformers` [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one#bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`transformers`ä¸­é˜…è¯»æœ‰å…³8ä½æ¨¡å‹åŠ è½½çš„æ›´å¤šä¿¡æ¯[æ­¤å¤„](https://huggingface.co/docs/transformers/perf_infer_gpu_one#bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition)ã€‚
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Use the CUDA cache optimizer
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨CUDAç¼“å­˜ä¼˜åŒ–å™¨
- en: 'When training large models, you should better handle the CUDA cache by iteratively
    clearing it. Do do so, simply pass `optimize_cuda_cache=True` to `PPOConfig`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œæœ€å¥½é€šè¿‡è¿­ä»£æ¸…é™¤CUDAç¼“å­˜æ¥å¤„ç†CUDAç¼“å­˜ã€‚è¦è¿™æ ·åšï¼Œåªéœ€å°†`optimize_cuda_cache=True`ä¼ é€’ç»™`PPOConfig`ï¼š
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Use score scaling/normalization/clipping
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†æ•°ç¼©æ”¾/å½’ä¸€åŒ–/è£å‰ª
- en: 'As suggested by [Secrets of RLHF in Large Language Models Part I: PPO](https://arxiv.org/abs/2307.04964),
    we support score (aka reward) scaling/normalization/clipping to improve training
    stability via `PPOConfig`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚[å¤§å‹è¯­è¨€æ¨¡å‹RLHFçš„ç§˜å¯†ç¬¬ä¸€éƒ¨åˆ†ï¼šPPO](https://arxiv.org/abs/2307.04964)æ‰€å»ºè®®çš„ï¼Œæˆ‘ä»¬æ”¯æŒé€šè¿‡`PPOConfig`è¿›è¡Œåˆ†æ•°ï¼ˆåˆåå¥–åŠ±ï¼‰ç¼©æ”¾/å½’ä¸€åŒ–/å‰ªåˆ‡ï¼Œä»¥æ”¹å–„è®­ç»ƒç¨³å®šæ€§ã€‚
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To run `ppo.py`, you can use the following command:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿è¡Œ`ppo.py`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
