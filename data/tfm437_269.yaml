- en: FocalNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FocalNet
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/focalnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/focalnet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/focalnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/focalnet)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926)
    by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. FocalNets completely
    replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a
    focal modulation mechanism for modeling token interactions in vision. The authors
    claim that FocalNets outperform self-attention based models with similar computational
    costs on the tasks of image classification, object detection, and segmentation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: FocalNet模型是由Jianwei Yang、Chunyuan Li、Xiyang Dai、Lu Yuan、Jianfeng Gao在[焦点调制网络](https://arxiv.org/abs/2203.11926)中提出的。FocalNets完全用焦点调制机制取代了自注意力（在模型中使用，如[ViT](vit)和[Swin](swin)），用于建模视觉中的令牌交互。作者声称，FocalNets在图像分类、目标检测和分割任务上优于基于自注意力的模型，且具有类似的计算成本。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*We propose focal modulation networks (FocalNets in short), where self-attention
    (SA) is completely replaced by a focal modulation mechanism for modeling token
    interactions in vision. Focal modulation comprises three components: (i) hierarchical
    contextualization, implemented using a stack of depth-wise convolutional layers,
    to encode visual contexts from short to long ranges, (ii) gated aggregation to
    selectively gather contexts for each query token based on its content, and (iii)
    element-wise modulation or affine transformation to inject the aggregated context
    into the query. Extensive experiments show FocalNets outperform the state-of-the-art
    SA counterparts (e.g., Swin and Focal Transformers) with similar computational
    costs on the tasks of image classification, object detection, and segmentation.
    Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1
    accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it
    attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and
    384, respectively. When transferred to downstream tasks, FocalNets exhibit clear
    superiority. For object detection with Mask R-CNN, FocalNet base trained with
    1\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin
    trained with 3\times schedule (49.0 v.s. 48.5). For semantic segmentation with
    UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin
    at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve
    58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation.
    Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and
    test-dev, respectively, establishing new SoTA on top of much larger attention-based
    models like Swinv2-G and BEIT-3.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们提出了焦点调制网络（简称FocalNets），其中自注意力（SA）完全被焦点调制机制取代，用于建模视觉中的令牌交互。焦点调制包括三个组件：（i）分层上下文化，使用一堆深度卷积层实现，以编码从短到长范围的视觉上下文，（ii）门控聚合，根据其内容选择性地收集每个查询令牌的上下文，以及（iii）逐元调制或仿射变换，将聚合的上下文注入查询中。大量实验证明，FocalNets在图像分类、目标检测和分割任务上优于最先进的自注意力对应物（例如Swin和Focal
    Transformers），并且具有类似的计算成本。具体而言，尺寸微小和基础尺寸的FocalNets在ImageNet-1K上分别实现了82.3%和83.9%的top-1准确率。在224分辨率上在ImageNet-22K上预训练后，当分别使用224和384分辨率进行微调时，它分别达到了86.5%和87.3%的top-1准确率。转移到下游任务时，FocalNets表现出明显的优势。对于使用Mask
    R-CNN进行目标检测，基础训练的FocalNet优于Swin对应物2.1个点，并且已经超过了使用3倍计划训练的Swin（49.0对48.5）。对于使用UPerNet进行语义分割，单尺度的FocalNet优于Swin
    2.4个点，并且在多尺度上击败了Swin（50.5对49.7）。使用大型FocalNet和Mask2former，我们在ADE20K语义分割上实现了58.5的mIoU，以及在COCO
    Panoptic分割上实现了57.9的PQ。使用巨大的FocalNet和DINO，我们在COCO minival和test-dev上分别实现了64.3和64.4的mAP，建立了新的SoTA，超越了像Swinv2-G和BEIT-3这样的更大的基于注意力的模型。*'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/microsoft/FocalNet).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/microsoft/FocalNet)找到。
- en: FocalNetConfig
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FocalNetConfig
- en: '### `class transformers.FocalNetConfig`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FocalNetConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/configuration_focalnet.py#L29)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/configuration_focalnet.py#L29)'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像大小（`int`，*可选*，默认为224）— 每个图像的大小（分辨率）。
- en: '`patch_size` (`int`, *optional*, defaults to 4) — The size (resolution) of
    each patch in the embeddings layer.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size`（`int`，*可选*，默认为4）— 嵌入层中每个补丁的大小（分辨率）。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels`（`int`，*可选*，默认为3）— 输入通道数。'
- en: '`embed_dim` (`int`, *optional*, defaults to 96) — Dimensionality of patch embedding.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_dim`（`int`，*可选*，默认为96）— 补丁嵌入的维度。'
- en: '`use_conv_embed` (`bool`, *optional*, defaults to `False`) — Whether to use
    convolutional embedding. The authors noted that using convolutional embedding
    usually improve the performance, but it’s not used by default.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_conv_embed`（`bool`，*可选*，默认为`False`）— 是否使用卷积嵌入。作者指出，使用卷积嵌入通常会提高性能，但默认情况下不使用。'
- en: '`hidden_sizes` (`List[int]`, *optional*, defaults to `[192, 384, 768, 768]`)
    — Dimensionality (hidden size) at each stage.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_sizes`（`List[int]`，*可选*，默认为`[192, 384, 768, 768]`）— 每个阶段的维度（隐藏大小）。'
- en: '`depths` (`list(int)`, *optional*, defaults to `[2, 2, 6, 2]`) — Depth (number
    of layers) of each stage in the encoder.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depths`（`list(int)`，*可选*，默认为`[2, 2, 6, 2]`）— 编码器中每个阶段的深度（层数）。'
- en: '`focal_levels` (`list(int)`, *optional*, defaults to `[2, 2, 2, 2]`) — Number
    of focal levels in each layer of the respective stages in the encoder.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`focal_levels`（`list(int)`，*可选*，默认为`[2, 2, 2, 2]`）— 编码器中各阶段的每层中的焦点级别数。'
- en: '`focal_windows` (`list(int)`, *optional*, defaults to `[3, 3, 3, 3]`) — Focal
    window size in each layer of the respective stages in the encoder.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`focal_windows` (`list(int)`, *可选*, 默认为`[3, 3, 3, 3]`) — 编码器中各阶段的各层中的焦点窗口大小。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder. If string,
    `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`function`, *可选*, 默认为`"gelu"`) — 编码器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`mlp_ratio` (`float`, *optional*, defaults to 4.0) — Ratio of MLP hidden dimensionality
    to embedding dimensionality.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_ratio` (`float`, *可选*, 默认为4.0) — MLP隐藏维度与嵌入维度的比率。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all fully connected layers in the embeddings and encoder.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *可选*, 默认为0.0) — 嵌入和编码器中所有全连接层的丢失概率。'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) — Stochastic depth
    rate.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *可选*, 默认为0.1) — 随机深度率。'
- en: '`use_layerscale` (`bool`, *optional*, defaults to `False`) — Whether to use
    layer scale in the encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_layerscale` (`bool`, *可选*, 默认为`False`) — 是否在编码器中使用层比例。'
- en: '`layerscale_value` (`float`, *optional*, defaults to 0.0001) — The initial
    value of the layer scale.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerscale_value` (`float`, *可选*, 默认为0.0001) — 层比例的初始值。'
- en: '`use_post_layernorm` (`bool`, *optional*, defaults to `False`) — Whether to
    use post layer normalization in the encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_post_layernorm` (`bool`, *可选*, 默认为`False`) — 是否在编码器中使用后层归一化。'
- en: '`use_post_layernorm_in_modulation` (`bool`, *optional*, defaults to `False`)
    — Whether to use post layer normalization in the modulation layer.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_post_layernorm_in_modulation` (`bool`, *可选*, 默认为`False`) — 是否在调制层中使用后层归一化。'
- en: '`normalize_modulator` (`bool`, *optional*, defaults to `False`) — Whether to
    normalize the modulator.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_modulator` (`bool`, *可选*, 默认为`False`) — 是否对调制器进行归一化。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *可选*, 默认为1e-05) — 层归一化层使用的epsilon。'
- en: '`encoder_stride` (`int`, *optional*, defaults to 32) — Factor to increase the
    spatial resolution by in the decoder head for masked image modeling.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_stride` (`int`, *可选*, 默认为32) — 用于掩蔽图像建模中解码器头部增加空间分辨率的因子。'
- en: '`out_features` (`List[str]`, *optional*) — If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_features` (`List[str]`, *可选*) — 如果用作骨干，要输出的特征列表。可以是任何一个`"stem"`、`"stage1"`、`"stage2"`等（取决于模型有多少阶段）。如果未设置且设置了`out_indices`，将默认为相应的阶段。如果未设置且`out_indices`未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。'
- en: '`out_indices` (`List[int]`, *optional*) — If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *可选*) — 如果用作骨干，要输出的特征的索引列表。可以是0、1、2等（取决于模型有多少阶段）。如果未设置且设置了`out_features`，将默认为相应的阶段。如果未设置且`out_features`未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。'
- en: This is the configuration class to store the configuration of a [FocalNetModel](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetModel).
    It is used to instantiate a FocalNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the FocalNet [microsoft/focalnet-tiny](https://huggingface.co/microsoft/focalnet-tiny)
    architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[FocalNetModel](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetModel)配置的配置类。它用于根据指定的参数实例化一个FocalNet模型，定义模型架构。使用默认值实例化配置将产生类似于[FocalNet](https://huggingface.co/microsoft/focalnet-tiny)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: FocalNetModel
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FocalNetModel
- en: '### `class transformers.FocalNetModel`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FocalNetModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L681)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L681)'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare FocalNet Model outputting raw hidden-states without any specific head
    on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的FocalNet模型输出原始隐藏状态，没有特定的头部。这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L704)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L704)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `AutoImageProcessor.__call__()` for details.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。查看`AutoImageProcessor.__call__()`获取详细信息。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。查看返回张量中的`hidden_states`获取更多细节。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bool_masked_pos` (`torch.BoolTensor`，形状为`(batch_size, num_patches)`) — 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。'
- en: Returns
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig))
    and inputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput`或一个`torch.FloatTensor`元组（如果传入`return_dict=False`或者`config.return_dict=False`时）包含根据配置（[FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`,
    *optional*, returned when `add_pooling_layer=True` is passed) — Average pooling
    of the last layer hidden-state.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`，*可选*，当传入`add_pooling_layer=True`时返回)
    — 最后一层隐藏状态的平均池化。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传入`output_hidden_states=True`或者`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每个阶段的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传入`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, hidden_size, height, width)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每个阶段的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs reshaped to include the spatial dimensions.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出重塑以包含空间维度。
- en: The [FocalNetModel](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetModel)
    forward method, overrides the `__call__` special method.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[FocalNetModel](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: FocalNetForMaskedImageModeling
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FocalNetForMaskedImageModeling
- en: '### `class transformers.FocalNetForMaskedImageModeling`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FocalNetForMaskedImageModeling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L761)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L761)'
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: FocalNet Model with a decoder on top for masked image modeling.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: FocalNet模型，顶部带有解码器，用于对遮蔽图像进行建模。
- en: This follows the same implementation as in [SimMIM](https://arxiv.org/abs/2111.09886).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这遵循与[SimMIM](https://arxiv.org/abs/2111.09886)中相同的实现。
- en: Note that we provide a script to pre-train this model on custom data in our
    [examples directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在我们的[示例目录](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining)中提供了一个脚本，用于在自定义数据上预训练此模型。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L793)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L793)'
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `AutoImageProcessor.__call__()` for details.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅`AutoImageProcessor.__call__()`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`)
    — 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。'
- en: Returns
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig))
    and inputs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含各种元素，具体取决于配置（[FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos`
    is provided) — Masked image modeling (MLM) loss.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos`
    is provided) — 遮蔽图像建模（MLM）损失。'
- en: '`reconstruction` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Reconstructed pixel values.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reconstruction` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — 重建的像素值。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出处的隐藏状态以及初始嵌入输出。
- en: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs reshaped to include the spatial dimensions.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出处的隐藏状态以及重塑以包含空间维度的初始嵌入输出。
- en: The [FocalNetForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling)
    forward method, overrides the `__call__` special method.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[FocalNetForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: FocalNetForImageClassification
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FocalNetForImageClassification
- en: '### `class transformers.FocalNetForImageClassification`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FocalNetForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L876)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L876)'
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: FocalNet Model with an image classification head on top (a linear layer on top
    of the pooled output) e.g. for ImageNet.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有图像分类头部的 FocalNet 模型（在池化输出之上的线性层），例如用于 ImageNet。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L899)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L899)'
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `AutoImageProcessor.__call__()` for details.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height,
    width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取像素值。有关详细信息，请参阅 `AutoImageProcessor.__call__()`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在
    `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果
    `config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput`
    或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig))
    and inputs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput`
    或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或当 `config.return_dict=False`
    时）包含各种元素，取决于配置（[FocalNetConfig](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 分类（或回归，如果 `config.num_labels==1`）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为 `(batch_size, config.num_labels)`) — 分类（或回归，如果
    `config.num_labels==1`）分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出以及初始嵌入输出的隐藏状态。
- en: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, hidden_size, height,
    width)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs reshaped to include the spatial dimensions.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出，重塑以包括空间维度。
- en: The [FocalNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[FocalNetForImageClassification](/docs/transformers/v4.37.2/en/model_doc/focalnet#transformers.FocalNetForImageClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
