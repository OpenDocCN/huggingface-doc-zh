- en: I2VGen-XL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I2VGen-XL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl](https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl](https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models](https://hf.co/papers/2311.04145.pdf)
    by Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin,
    Xiang Wang, Deli Zhao, and Jingren Zhou.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[I2VGen-XL：通过级联扩散模型实现高质量图像到视频合成](https://hf.co/papers/2311.04145.pdf) 作者：张世伟，王佳宇，张颖雅，赵康，袁航杰，秦志武，王翔，赵德力，周静仁。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Video synthesis has recently made remarkable strides benefiting from the rapid
    development of diffusion models. However, it still encounters challenges in terms
    of semantic accuracy, clarity and spatio-temporal continuity. They primarily arise
    from the scarcity of well-aligned text-video data and the complex inherent structure
    of videos, making it difficult for the model to simultaneously ensure semantic
    and qualitative excellence. In this report, we propose a cascaded I2VGen-XL approach
    that enhances model performance by decoupling these two factors and ensures the
    alignment of the input data by utilizing static images as a form of crucial guidance.
    I2VGen-XL consists of two stages: i) the base stage guarantees coherent semantics
    and preserves content from input images by using two hierarchical encoders, and
    ii) the refinement stage enhances the video’s details by incorporating an additional
    brief text and improves the resolution to 1280×720\. To improve the diversity,
    we collect around 35 million single-shot text-video pairs and 6 billion text-image
    pairs to optimize the model. By this means, I2VGen-XL can simultaneously enhance
    the semantic accuracy, continuity of details and clarity of generated videos.
    Through extensive experiments, we have investigated the underlying principles
    of I2VGen-XL and compared it with current top methods, which can demonstrate its
    effectiveness on diverse data. The source code and models will be publicly available
    at [this https URL](https://i2vgen-xl.github.io/).*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*视频合成最近取得了显著进展，这要归功于扩散模型的快速发展。然而，它仍然在语义准确性、清晰度和时空连续性方面遇到挑战。这主要源于文本视频数据的匮乏以及视频的复杂内在结构，使得模型难以同时确保语义和质量上的卓越性。在本报告中，我们提出了一种级联I2VGen-XL方法，通过解耦这两个因素来增强模型性能，并利用静态图像作为重要指导形式来确保输入数据的对齐。I2VGen-XL包括两个阶段：i）基础阶段通过使用两个分层编码器保证连贯的语义，并保留来自输入图像的内容，ii）细化阶段通过整合额外简短的文本来增强视频的细节，并将分辨率提高到1280×720。为了提高多样性，我们收集了约3500万个单镜头文本视频对和60亿个文本图像对来优化模型。通过这种方式，I2VGen-XL可以同时提高语义准确性、细节连续性和生成视频的清晰度。通过大量实验，我们研究了I2VGen-XL的基本原理，并将其与当前顶级方法进行了比较，从而展示了其在多样数据上的有效性。源代码和模型将在[此https
    URL](https://i2vgen-xl.github.io/)上公开。*'
- en: The original codebase can be found [here](https://github.com/ali-vilab/i2vgen-xl/).
    The model checkpoints can be found [here](https://huggingface.co/ali-vilab/).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码库可以在[此处](https://github.com/ali-vilab/i2vgen-xl/)找到。模型检查点可以在[此处](https://huggingface.co/ali-vilab/)找到。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
    Also, to know more about reducing the memory usage of this pipeline, refer to
    the [“Reduce memory usage”] section [here](../../using-diffusers/svd#reduce-memory-usage).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。此外，要了解如何减少此管道的内存使用量，请参考[“减少内存使用量”]部分[此处](../../using-diffusers/svd#reduce-memory-usage)。
- en: 'Sample output with I2VGenXL:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用I2VGenXL的示例输出：
- en: '| masterpiece, bestquality, sunset. ![library](../Images/a2129243953299cb71d17cb43a7d368b.png)
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 杰作，最佳质量，日落。![library](../Images/a2129243953299cb71d17cb43a7d368b.png) |'
- en: Notes
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注释
- en: I2VGenXL always uses a `clip_skip` value of 1\. This means it leverages the
    penultimate layer representations from the text encoder of CLIP.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I2VGenXL总是使用`clip_skip`值为1。这意味着它利用了CLIP文本编码器的倒数第二层表示。
- en: It can generate videos of quality that is often on par with [Stable Video Diffusion](../../using-diffusers/svd)
    (SVD).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以生成与[稳定视频扩散](../../using-diffusers/svd)（SVD）质量相当的视频。
- en: Unlike SVD, it additionally accepts text prompts as inputs.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与SVD不同，它还接受文本提示作为输入。
- en: It can generate higher resolution videos.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以生成更高分辨率的视频。
- en: When using the [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)
    (which is default for this pipeline), less than 50 steps for inference leads to
    bad results.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)（这是此管道的默认设置）时，推断步骤少于50步会导致糟糕的结果。
- en: I2VGenXLPipeline
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I2VGenXLPipeline
- en: '### `class diffusers.I2VGenXLPipeline`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.I2VGenXLPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L109)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L109)'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）
    — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并从潜在表示中生成图像。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — 冻结文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — 用于对文本进行标记化的[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` (`I2VGenXLUNet`) — A `I2VGenXLUNet` to denoise the encoded video latents.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` (`I2VGenXLUNet`) — 用于去噪编码视频潜在特征的`I2VGenXLUNet`。'
- en: '`scheduler` ([DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler))
    — 用于与`unet`结合使用以去噪编码图像潜在特征的调度器。'
- en: Pipeline for image-to-video generation as proposed in [I2VGenXL](https://i2vgen-xl.github.io/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的图像到视频生成管道，如[I2VGenXL](https://i2vgen-xl.github.io/)中提出的。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L594)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L594)'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`image` (`PIL.Image.Image` or `List[PIL.Image.Image]` or `torch.FloatTensor`)
    — Image or images to guide image generation. If you provide a tensor, it needs
    to be compatible with [`CLIPImageProcessor`](https://huggingface.co/lambdalabs/sd-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`PIL.Image.Image` 或 `List[PIL.Image.Image]` 或 `torch.FloatTensor`)
    — 用于指导图像生成的图像。如果提供张量，则需要与[`CLIPImageProcessor`](https://huggingface.co/lambdalabs/sd-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json)兼容。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated image.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素宽度。'
- en: '`target_fps` (`int`, *optional*) — Frames per second. The rate at which the
    generated images shall be exported to a video after generation. This is also used
    as a “micro-condition” while generation.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_fps` (`int`, *可选*) — 每秒帧数。在生成后将生成的图像导出到视频的速率。这也被用作生成时的“微条件”。'
- en: '`num_frames` (`int`, *optional*) — The number of video frames to generate.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_frames` (`int`, *可选*) — 要生成的视频帧数。'
- en: '`num_inference_steps` (`int`, *optional*) — The number of denoising steps.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*) — 去噪步骤的数量。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成中不包括的提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale
    < 1`）时被忽略。'
- en: '`eta` (`float`, *optional*) — Corresponds to parameter eta (η) from the [DDIM](https://arxiv.org/abs/2010.02502)
    paper. Only applies to the [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *可选*) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta
    (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。'
- en: '`num_videos_per_prompt` (`int`, *optional*) — The number of images to generate
    per prompt.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_videos_per_prompt` (`int`, *可选*) — 每个提示生成的图像数量。'
- en: '`decode_chunk_size` (`int`, *optional*) — The number of frames to decode at
    a time. The higher the chunk size, the higher the temporal consistency between
    frames, but also the higher the memory consumption. By default, the decoder will
    decode all frames at once for maximal quality. Reduce `decode_chunk_size` to reduce
    memory usage.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode_chunk_size` (`int`, *可选*) — 一次解码的帧数。块大小越大，帧之间的时间一致性越高，但内存消耗也越高。默认情况下，解码器将一次解码所有帧以获得最大质量。减少`decode_chunk_size`以减少内存使用。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中预先生成的嘈杂潜在特征，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在特征张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`
    将从 `negative_prompt` 输入参数生成。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或
    `np.array` 之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    而不是普通的元组。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) — 如果指定了，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中定义的 `AttentionProcessor` 的 kwargs 字典。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput)
    or `tuple`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated frames.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，将返回 [pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput)，否则将返回一个元组，其中第一个元素是生成帧的列表。
- en: The call function to the pipeline for image-to-video generation with [I2VGenXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.I2VGenXLPipeline).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[I2VGenXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.I2VGenXLPipeline)进行图像到视频生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `disable_freeu`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L590)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L590)'
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，将禁用 FreeU 机制。
- en: '#### `disable_vae_slicing`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L176)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L176)'
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片 VAE 解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `disable_vae_tiling`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L193)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L193)'
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用平铺 VAE 解码。如果之前启用了 `enable_vae_tiling`，则此方法将返回到一步计算解码。
- en: '#### `enable_freeu`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L567)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L567)'
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1` (`float`) — 第1阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2` (`float`) — 第2阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1` (`float`) — 第1阶段的缩放因子，用于放大骨干特征的贡献。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2` (`float`) — 第2阶段的缩放因子，用于放大骨干特征的贡献。'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 FreeU 机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后缀表示它们被应用的阶段。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同流水线（如 Stable Diffusion
    v1、v2 和 Stable Diffusion XL）的值组合。
- en: '#### `enable_vae_slicing`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L168)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L168)'
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。启用此选项时，VAE将将输入张量分割成片来计算解码。这对于节省一些内存并允许更大的批量大小非常有用。
- en: '#### `enable_vae_tiling`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L184)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L184)'
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 启用平铺VAE解码。启用此选项时，VAE将将输入张量分割成瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。
- en: '#### `encode_prompt`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L200)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L200)'
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`): torch设备'
- en: '`num_videos_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_videos_per_prompt` (`int`) — 每个提示应生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *optional*) — 不用于指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *optional*) — 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *optional*) — 从CLIP中跳过的层数，用于计算提示嵌入。值为1表示将使用预终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: I2VGenXLPipelineOutput
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I2VGenXLPipelineOutput
- en: '### `class diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L95)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L95)'
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`frames` (`List[np.ndarray]` or `torch.FloatTensor`) — List of denoised frames
    (essentially images) as NumPy arrays of shape `(height, width, num_channels)`
    or as a `torch` tensor. The length of the list denotes the video length (the number
    of frames).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frames` (`List[np.ndarray]` 或 `torch.FloatTensor`) — 去噪帧（基本上是图像）的列表，作为形状为`(height,
    width, num_channels)`的NumPy数组或`torch`张量。列表的长度表示视频长度（帧数）。'
- en: Output class for image-to-video pipeline.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到视频管道的输出类。
