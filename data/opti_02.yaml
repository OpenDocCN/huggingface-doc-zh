- en: 🤗 Optimum
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗 Optimum
- en: 'Original text: [https://huggingface.co/docs/optimum/index](https://huggingface.co/docs/optimum/index)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/optimum/index](https://huggingface.co/docs/optimum/index)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🤗 Optimum is an extension of [Transformers](https://huggingface.co/docs/transformers)
    that provides a set of performance optimization tools to train and run models
    on targeted hardware with maximum efficiency.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Optimum是[Transformers](https://huggingface.co/docs/transformers)的扩展，提供一组性能优化工具，以最大效率在目标硬件上训练和运行模型。
- en: The AI ecosystem evolves quickly, and more and more specialized hardware along
    with their own optimizations are emerging every day. As such, Optimum enables
    developers to efficiently use any of these platforms with the same ease inherent
    to Transformers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能生态系统发展迅速，每天都有越来越多的专用硬件及其自身的优化出现。因此，Optimum使开发人员能够高效地使用任何这些平台，与Transformers固有的便利性一样。
- en: 🤗 Optimum is distributed as a collection of packages - check out the links below
    for an in-depth look at each one.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Optimum被分发为一系列软件包 - 请查看下面的链接，深入了解每个软件包。
- en: '[Habana'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Habana'
- en: Maximize training throughput and efficiency with Habana's Gaudi processor](./habana/index)
    [Intel
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Habana的Gaudi处理器最大化训练吞吐量和效率](./habana/index) [Intel
- en: Optimize your model to speedup inference with OpenVINO and Neural Compressor](./intel/index)
    [AWS Trainium/Inferentia
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过OpenVINO和神经压缩器优化您的模型以加速推理](./intel/index) [AWS Trainium/Inferentia
- en: Accelerate your training and inference workflows with AWS Trainium and AWS Inferentia](https://huggingface.co/docs/optimum-neuron/index)
    [NVIDIA
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AWS Trainium和AWS Inferentia加速您的训练和推理工作流程](https://huggingface.co/docs/optimum-neuron/index)
    [NVIDIA
- en: Accelerate inference with NVIDIA TensorRT-LLM on the NVIDIA platform](https://github.com/huggingface/optimum-nvidia)
    [AMD
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在NVIDIA平台上使用NVIDIA TensorRT-LLM加速推理](https://github.com/huggingface/optimum-nvidia)
    [AMD
- en: Enable performance optimizations for AMD Instinct GPUs and AMD Ryzen AI NPUs](./amd/index)
    [FuriosaAI
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为AMD Instinct GPU和AMD Ryzen AI NPU应用性能优化](./amd/index) [FuriosaAI
- en: Fast and efficient inference on FuriosaAI WARBOY](./furiosa/index) [ONNX Runtime
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在FuriosaAI WARBOY上进行快速高效的推理](./furiosa/index) [ONNX Runtime
- en: Apply quantization and graph optimization to accelerate Transformers models
    training and inference with ONNX Runtime](./onnxruntime/overview) [BetterTransformer
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 应用量化和图优化来加速Transformers模型的训练和推理，使用ONNX Runtime](./onnxruntime/overview) [BetterTransformer
- en: A one-liner integration to use PyTorch's BetterTransformer with Transformers
    models](./bettertransformer/overview)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch的BetterTransformer与Transformers模型的一行集成](./bettertransformer/overview)
