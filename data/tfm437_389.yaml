- en: Graphormer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Graphormer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The Graphormer model was proposed in [Do Transformers Really Perform Bad for
    Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle
    Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu.
    It is a Graph Transformer model, modified to allow computations on graphs instead
    of text sequences by generating embeddings and features of interest during preprocessing
    and collation, then using a modified attention.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Graphormer模型是由Chengxuan Ying、Tianle Cai、Shengjie Luo、Shuxin Zheng、Guolin Ke、Di
    He、Yanming Shen和Tie-Yan Liu在[Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234)中提出的。这是一个图形变换器模型，经过修改以允许在图形上进行计算，而不是文本序列，通过在预处理和整理过程中生成感兴趣的嵌入和特征，然后使用修改后的注意力。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*The Transformer architecture has become a dominant choice in many domains,
    such as natural language processing and computer vision. Yet, it has not achieved
    competitive performance on popular leaderboards of graph-level prediction compared
    to mainstream GNN variants. Therefore, it remains a mystery how Transformers could
    perform well for graph representation learning. In this paper, we solve this mystery
    by presenting Graphormer, which is built upon the standard Transformer architecture,
    and could attain excellent results on a broad range of graph representation learning
    tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to
    utilizing Transformer in the graph is the necessity of effectively encoding the
    structural information of a graph into the model. To this end, we propose several
    simple yet effective structural encoding methods to help Graphormer better model
    graph-structured data. Besides, we mathematically characterize the expressive
    power of Graphormer and exhibit that with our ways of encoding the structural
    information of graphs, many popular GNN variants could be covered as the special
    cases of Graphormer.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*变压器架构已经成为许多领域的主要选择，如自然语言处理和计算机视觉。然而，与主流GNN变体相比，它在流行的图级预测排行榜上并没有取得竞争性表现。因此，变压器如何在图形表示学习中表现良好仍然是一个谜。在本文中，我们通过提出Graphormer来解决这个谜团，它建立在标准Transformer架构之上，并且在广泛的图形表示学习任务中取得了出色的结果，特别是在最近的OGB大规模挑战赛上。我们利用Transformer在图中的关键见解是有效地将图的结构信息编码到模型中。为此，我们提出了几种简单而有效的结构编码方法，以帮助Graphormer更好地建模图结构化数据。此外，我们数学地刻画了Graphormer的表达能力，并展示了通过我们的方式对图的结构信息进行编码，许多流行的GNN变体可以被覆盖为Graphormer的特殊情况。*'
- en: This model was contributed by [clefourrier](https://huggingface.co/clefourrier).
    The original code can be found [here](https://github.com/microsoft/Graphormer).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[clefourrier](https://huggingface.co/clefourrier)贡献。原始代码可以在[这里](https://github.com/microsoft/Graphormer)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: This model will not work well on large graphs (more than 100 nodes/edges), as
    it will make the memory explode. You can reduce the batch size, increase your
    RAM, or decrease the `UNREACHABLE_NODE_DISTANCE` parameter in algos_graphormer.pyx,
    but it will be hard to go above 700 nodes/edges.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在大型图上（超过100个节点/边）效果不佳，因为会导致内存爆炸。您可以减小批量大小，增加RAM，或者减小algos_graphormer.pyx中的`UNREACHABLE_NODE_DISTANCE`参数，但很难超过700个节点/边。
- en: This model does not use a tokenizer, but instead a special collator during training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型不使用分词器，而是在训练过程中使用特殊的整理器。
- en: GraphormerConfig
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphormerConfig
- en: '### `class transformers.GraphormerConfig`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GraphormerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/configuration_graphormer.py#L30)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/configuration_graphormer.py#L30)'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_classes` (`int`, *optional*, defaults to 1) — Number of target classes
    or labels, set to n for binary classification of n tasks.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_classes` (`int`, *optional*, defaults to 1) — 目标类别或标签的数量，设置为n用于n个任务的二元分类。'
- en: '`num_atoms` (`int`, *optional*, defaults to 512*9) — Number of node types in
    the graphs.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_atoms` (`int`, *optional*, defaults to 512*9) — 图中节点类型的数量。'
- en: '`num_edges` (`int`, *optional*, defaults to 512*3) — Number of edges types
    in the graph.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_edges` (`int`, *optional*, defaults to 512*3) — 图中边的类型数量。'
- en: '`num_in_degree` (`int`, *optional*, defaults to 512) — Number of in degrees
    types in the input graphs.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_in_degree` (`int`, *optional*, defaults to 512) — 输入图中的入度类型数量。'
- en: '`num_out_degree` (`int`, *optional*, defaults to 512) — Number of out degrees
    types in the input graphs.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_out_degree` (`int`, *optional*, defaults to 512) — 输入图中的出度类型数量。'
- en: '`num_edge_dis` (`int`, *optional*, defaults to 128) — Number of edge dis in
    the input graphs.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_edge_dis` (`int`, *optional*, defaults to 128) — 输入图中的边缘dis数量。'
- en: '`multi_hop_max_dist` (`int`, *optional*, defaults to 20) — Maximum distance
    of multi hop edges between two nodes.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multi_hop_max_dist` (`int`, *optional*, defaults to 20) — 两个节点之间多跳边的最大距离。'
- en: '`spatial_pos_max` (`int`, *optional*, defaults to 1024) — Maximum distance
    between nodes in the graph attention bias matrices, used during preprocessing
    and collation.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spatial_pos_max` (`int`, *optional*, defaults to 1024) — 图注意力偏置矩阵中节点之间的最大距离，在预处理和整理过程中使用。'
- en: '`edge_type` (`str`, *optional*, defaults to multihop) — Type of edge relation
    chosen.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edge_type` (`str`, *optional*, defaults to multihop) — 选择的边关系类型。'
- en: '`max_nodes` (`int`, *optional*, defaults to 512) — Maximum number of nodes
    which can be parsed for the input graphs.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_nodes` (`int`, *optional*, defaults to 512) — 可以解析的输入图中的最大节点数。'
- en: '`share_input_output_embed` (`bool`, *optional*, defaults to `False`) — Shares
    the embedding layer between encoder and decoder - careful, True is not implemented.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`share_input_output_embed` (`bool`, *optional*, defaults to `False`) — 在编码器和解码器之间共享嵌入层
    - 注意，True未实现。'
- en: '`num_layers` (`int`, *optional*, defaults to 12) — Number of layers.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, defaults to 12) — 层数。'
- en: '`embedding_dim` (`int`, *optional*, defaults to 768) — Dimension of the embedding
    layer in encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dim` (`int`, *optional*, defaults to 768) — 编码器中嵌入层的维度。'
- en: '`ffn_embedding_dim` (`int`, *optional*, defaults to 768) — Dimension of the
    “intermediate” (often named feed-forward) layer in encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ffn_embedding_dim` (`int`, *optional*, defaults to 768) — 编码器中“中间”（通常称为前馈）层的维度。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — Number of attention
    heads in the encoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 32) — 编码器中的注意力头数。'
- en: '`self_attention` (`bool`, *optional*, defaults to `True`) — Model is self attentive
    (False not implemented).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self_attention` (`bool`, *optional*, defaults to `True`) — 模型是自注意的（False未实现）。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention weights.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力权重的dropout概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the activation of the linear transformer layer.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 线性变换器层激活的dropout概率。'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop probability
    for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop
    paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。'
- en: '`bias` (`bool`, *optional*, defaults to `True`) — Uses bias in the attention
    module - unsupported at the moment.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, *optional*, defaults to `True`) — 在注意力模块中使用偏置 - 目前不支持。'
- en: '`embed_scale(float,` *optional*, defaults to None) — Scaling factor for the
    node embeddings.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_scale(float,` *optional*, defaults to None) — 节点嵌入的缩放因子。'
- en: '`num_trans_layers_to_freeze` (`int`, *optional*, defaults to 0) — Number of
    transformer layers to freeze.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_trans_layers_to_freeze` (`int`, *optional*, defaults to 0) — 要冻结的Transformer层数。'
- en: '`encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — Normalize
    features before encoding the graph.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — 在对图进行编码之前对特征进行归一化。'
- en: '`pre_layernorm` (`bool`, *optional*, defaults to `False`) — Apply layernorm
    before self attention and the feed forward network. Without this, post layernorm
    will be used.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pre_layernorm` (`bool`, *optional*, defaults to `False`) — 在自注意力和前馈网络之前应用层归一化。如果没有这个，将使用后层归一化。'
- en: '`apply_graphormer_init` (`bool`, *optional*, defaults to `False`) — Apply a
    custom graphormer initialisation to the model before training.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_graphormer_init` (`bool`, *optional*, defaults to `False`) — 在训练之前对模型应用自定义的graphormer初始化。'
- en: '`freeze_embeddings` (`bool`, *optional*, defaults to `False`) — Freeze the
    embedding layer, or train it along the model.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`freeze_embeddings` (`bool`, *optional*, defaults to `False`) — 冻结嵌入层，或者与模型一起训练。'
- en: '`encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — Apply
    the layer norm before each encoder block.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — 在每个编码器块之前应用层归一化。'
- en: '`q_noise` (`float`, *optional*, defaults to 0.0) — Amount of quantization noise
    (see “Training with Quantization Noise for Extreme Model Compression”). (For more
    detail, see fairseq’s documentation on quant_noise).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q_noise` (`float`, *optional*, defaults to 0.0) — 量化噪声的量（参见“使用量化噪声进行极端模型压缩”）。
    （更多细节，请参阅fairseq关于quant_noise的文档）。'
- en: '`qn_block_size` (`int`, *optional*, defaults to 8) — Size of the blocks for
    subsequent quantization with iPQ (see q_noise).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qn_block_size` (`int`, *optional*, defaults to 8) — 用于后续iPQ量化的块的大小（参见q_noise）。'
- en: '`kdim` (`int`, *optional*, defaults to None) — Dimension of the key in the
    attention, if different from the other values.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kdim` (`int`, *optional*, defaults to None) — 注意力中键的维度，如果与其他值不同。'
- en: '`vdim` (`int`, *optional*, defaults to None) — Dimension of the value in the
    attention, if different from the other values.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vdim` (`int`, *optional*, defaults to None) — 注意力中值的维度，如果与其他值不同。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: '`traceable` (`bool`, *optional*, defaults to `False`) — Changes return value
    of the encoder’s inner_state to stacked tensors.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`traceable` (`bool`, *optional*, defaults to `False`) — 将编码器的inner_state的返回值更改为堆叠的张量。'
- en: Example —
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例 —
- en: This is the configuration class to store the configuration of a [~GraphormerModel](/docs/transformers/v4.37.2/en/model_doc/graphormer#transformers.GraphormerModel).
    It is used to instantiate an Graphormer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Graphormer [graphormer-base-pcqm4mv1](https://huggingface.co/graphormer-base-pcqm4mv1)
    architecture.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[~GraphormerModel](/docs/transformers/v4.37.2/en/model_doc/graphormer#transformers.GraphormerModel)的配置。它用于根据指定的参数实例化一个Graphormer模型，定义模型架构。使用默认值实例化配置将产生类似于Graphormer
    [graphormer-base-pcqm4mv1](https://huggingface.co/graphormer-base-pcqm4mv1)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: GraphormerModel
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphormerModel
- en: '### `class transformers.GraphormerModel`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GraphormerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L775)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L775)'
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Graphormer model is a graph-encoder model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Graphormer模型是一个图编码器模型。
- en: It goes from a graph to its representation. If you want to use the model for
    a downstream classification task, use GraphormerForGraphClassification instead.
    For any other downstream task, feel free to add a new class, or combine this model
    with a downstream model of your choice, following the example in GraphormerForGraphClassification.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它将一个图转换为其表示。如果您想将模型用于下游分类任务，请改用GraphormerForGraphClassification。对于任何其他下游任务，请随意添加一个新类，或将此模型与您选择的下游模型结合，按照GraphormerForGraphClassification中的示例进行操作。
- en: '#### `forward`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L804)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L804)'
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: GraphormerForGraphClassification
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphormerForGraphClassification
- en: '### `class transformers.GraphormerForGraphClassification`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GraphormerForGraphClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L846)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L846)'
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This model can be used for graph-level classification or regression tasks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以用于图级分类或回归任务。
- en: It can be trained on
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下链接上进行训练
- en: regression (by setting config.num_classes to 1); there should be one float-type
    label per graph
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归（通过将config.num_classes设置为1）；每个图应有一个浮点类型标签
- en: one task classification (by setting config.num_classes to the number of classes);
    there should be one integer label per graph
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单任务分类（通过将config.num_classes设置为类别数）；每个图应有一个整数标签
- en: binary multi-task classification (by setting config.num_classes to the number
    of labels); there should be a list of integer labels for each graph.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元多任务分类（通过将config.num_classes设置为标签数）；每个图应有一个整数标签列表。
- en: '#### `forward`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L869)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L869)'
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
