- en: LayoutLMV2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LayoutLMV2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training
    for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by
    Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei
    Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. LayoutLMV2 improves
    [LayoutLM](layoutlm) to obtain state-of-the-art results across several document
    image understanding benchmarks:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'LayoutLMV2æ¨¡å‹æ˜¯ç”±Yang Xuã€Yiheng Xuã€Tengchao Lvã€Lei Cuiã€Furu Weiã€Guoxin Wangã€Yijuan
    Luã€Dinei Florencioã€Cha Zhangã€Wanxiang Cheã€Min Zhangã€Lidong Zhouæå‡ºçš„[LayoutLMv2:
    Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)ã€‚LayoutLMV2æ”¹è¿›äº†[LayoutLM](layoutlm)ä»¥è·å¾—è·¨å¤šä¸ªæ–‡æ¡£å›¾åƒç†è§£åŸºå‡†çš„æœ€æ–°ç»“æœï¼š'
- en: 'information extraction from scanned documents: the [FUNSD](https://guillaumejaume.github.io/FUNSD/)
    dataset (a collection of 199 annotated forms comprising more than 30,000 words),
    the [CORD](https://github.com/clovaai/cord) dataset (a collection of 800 receipts
    for training, 100 for validation and 100 for testing), the [SROIE](https://rrc.cvc.uab.es/?ch=13)
    dataset (a collection of 626 receipts for training and 347 receipts for testing)
    and the [Kleister-NDA](https://github.com/applicaai/kleister-nda) dataset (a collection
    of non-disclosure agreements from the EDGAR database, including 254 documents
    for training, 83 documents for validation, and 203 documents for testing).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»æ‰«ææ–‡æ¡£ä¸­æå–ä¿¡æ¯ï¼š[FUNSD](https://guillaumejaume.github.io/FUNSD/)æ•°æ®é›†ï¼ˆåŒ…å«è¶…è¿‡30,000ä¸ªå•è¯çš„199ä¸ªå¸¦æ³¨é‡Šè¡¨æ ¼ï¼‰ã€[CORD](https://github.com/clovaai/cord)æ•°æ®é›†ï¼ˆåŒ…å«800å¼ ç”¨äºè®­ç»ƒçš„æ”¶æ®ã€100å¼ ç”¨äºéªŒè¯å’Œ100å¼ ç”¨äºæµ‹è¯•ï¼‰ã€[SROIE](https://rrc.cvc.uab.es/?ch=13)æ•°æ®é›†ï¼ˆåŒ…å«626å¼ ç”¨äºè®­ç»ƒå’Œ347å¼ ç”¨äºæµ‹è¯•çš„æ”¶æ®ï¼‰ä»¥åŠ[Kleister-NDA](https://github.com/applicaai/kleister-nda)æ•°æ®é›†ï¼ˆåŒ…å«æ¥è‡ªEDGARæ•°æ®åº“çš„éæŠ«éœ²åè®®ï¼ŒåŒ…æ‹¬254ä»½ç”¨äºè®­ç»ƒã€83ä»½ç”¨äºéªŒè¯å’Œ203ä»½ç”¨äºæµ‹è¯•çš„æ–‡ä»¶ï¼‰ã€‚
- en: 'document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)
    dataset (a collection of 400,000 images belonging to one of 16 classes).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æ¡£å›¾åƒåˆ†ç±»ï¼š[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)æ•°æ®é›†ï¼ˆåŒ…å«40ä¸‡å¼ å±äº16ä¸ªç±»åˆ«çš„å›¾åƒï¼‰ã€‚
- en: 'document visual question answering: the [DocVQA](https://arxiv.org/abs/2007.00398)
    dataset (a collection of 50,000 questions defined on 12,000+ document images).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æ¡£è§†è§‰é—®ç­”ï¼š[DocVQA](https://arxiv.org/abs/2007.00398)æ•°æ®é›†ï¼ˆåŒ…å«åœ¨12,000å¤šä¸ªæ–‡æ¡£å›¾åƒä¸Šå®šä¹‰çš„5ä¸‡ä¸ªé—®é¢˜ï¼‰ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Pre-training of text and layout has proved effective in a variety of visually-rich
    document understanding tasks due to its effective model architecture and the advantage
    of large-scale unlabeled scanned/digital-born documents. In this paper, we present
    LayoutLMv2 by pre-training text, layout and image in a multi-modal framework,
    where new model architectures and pre-training tasks are leveraged. Specifically,
    LayoutLMv2 not only uses the existing masked visual-language modeling task but
    also the new text-image alignment and text-image matching tasks in the pre-training
    stage, where cross-modality interaction is better learned. Meanwhile, it also
    integrates a spatial-aware self-attention mechanism into the Transformer architecture,
    so that the model can fully understand the relative positional relationship among
    different text blocks. Experiment results show that LayoutLMv2 outperforms strong
    baselines and achieves new state-of-the-art results on a wide variety of downstream
    visually-rich document understanding tasks, including FUNSD (0.7895 -> 0.8420),
    CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852),
    RVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672). The pre-trained LayoutLMv2
    model is publicly available at this https URL.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ–‡æœ¬å’Œå¸ƒå±€çš„é¢„è®­ç»ƒåœ¨å„ç§è§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œè¿™æ˜¯ç”±äºå…¶æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„å’Œå¤§è§„æ¨¡æœªæ ‡è®°çš„æ‰«æ/æ•°å­—åŒ–æ–‡æ¡£çš„ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LayoutLMv2ï¼Œé€šè¿‡åœ¨å¤šæ¨¡æ€æ¡†æ¶ä¸­é¢„è®­ç»ƒæ–‡æœ¬ã€å¸ƒå±€å’Œå›¾åƒï¼Œåˆ©ç”¨äº†æ–°çš„æ¨¡å‹æ¶æ„å’Œé¢„è®­ç»ƒä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒLayoutLMv2ä¸ä»…ä½¿ç”¨ç°æœ‰çš„é®è”½è§†è§‰è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œè¿˜ä½¿ç”¨æ–°çš„æ–‡æœ¬-å›¾åƒå¯¹é½å’Œæ–‡æœ¬-å›¾åƒåŒ¹é…ä»»åŠ¡åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä»è€Œæ›´å¥½åœ°å­¦ä¹ è·¨æ¨¡æ€äº¤äº’ã€‚åŒæ—¶ï¼Œå®ƒè¿˜å°†ç©ºé—´æ„ŸçŸ¥è‡ªæ³¨æ„æœºåˆ¶é›†æˆåˆ°Transformeræ¶æ„ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå……åˆ†ç†è§£ä¸åŒæ–‡æœ¬å—ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayoutLMv2ä¼˜äºå¼ºåŸºçº¿ï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸è§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼ŒåŒ…æ‹¬FUNSDï¼ˆ0.7895
    -> 0.8420ï¼‰ã€CORDï¼ˆ0.9493 -> 0.9601ï¼‰ã€SROIEï¼ˆ0.9524 -> 0.9781ï¼‰ã€Kleister-NDAï¼ˆ0.834 ->
    0.852ï¼‰ã€RVL-CDIPï¼ˆ0.9443 -> 0.9564ï¼‰å’ŒDocVQAï¼ˆ0.7295 -> 0.8672ï¼‰ã€‚é¢„è®­ç»ƒçš„LayoutLMv2æ¨¡å‹å¯ä»¥åœ¨æ­¤https
    URLä¸Šå…¬å¼€è·å–ã€‚*'
- en: 'LayoutLMv2 depends on `detectron2`, `torchvision` and `tesseract`. Run the
    following to install them:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv2ä¾èµ–äº`detectron2`ã€`torchvision`å’Œ`tesseract`ã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: (If you are developing for LayoutLMv2, note that passing the doctests also requires
    the installation of these packages.)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆå¦‚æœæ‚¨æ­£åœ¨å¼€å‘LayoutLMv2ï¼Œè¯·æ³¨æ„é€šè¿‡doctestsè¿˜éœ€è¦å®‰è£…è¿™äº›åŒ…ã€‚ï¼‰
- en: Usage tips
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates
    visual embeddings during pre-training (while LayoutLMv1 only adds visual embeddings
    during fine-tuning).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LayoutLMv1å’ŒLayoutLMv2ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºåè€…åœ¨é¢„è®­ç»ƒæœŸé—´åŒ…å«äº†è§†è§‰åµŒå…¥ï¼ˆè€ŒLayoutLMv1ä»…åœ¨å¾®è°ƒæœŸé—´æ·»åŠ äº†è§†è§‰åµŒå…¥ï¼‰ã€‚
- en: LayoutLMv2 adds both a relative 1D attention bias as well as a spatial 2D attention
    bias to the attention scores in the self-attention layers. Details can be found
    on page 5 of the [paper](https://arxiv.org/abs/2012.14740).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LayoutLMv2åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­æ·»åŠ äº†ç›¸å¯¹1Dæ³¨æ„åŠ›åç½®å’Œç©ºé—´2Dæ³¨æ„åŠ›åç½®åˆ°æ³¨æ„åŠ›åˆ†æ•°ä¸­ã€‚è¯¦ç»†ä¿¡æ¯å¯åœ¨[è®ºæ–‡](https://arxiv.org/abs/2012.14740)çš„ç¬¬5é¡µæ‰¾åˆ°ã€‚
- en: Demo notebooks on how to use the LayoutLMv2 model on RVL-CDIP, FUNSD, DocVQA,
    CORD can be found [here](https://github.com/NielsRogge/Transformers-Tutorials).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials)æ‰¾åˆ°å¦‚ä½•åœ¨RVL-CDIPã€FUNSDã€DocVQAã€CORDä¸Šä½¿ç”¨LayoutLMv2æ¨¡å‹çš„æ¼”ç¤ºç¬”è®°æœ¬ã€‚
- en: LayoutLMv2 uses Facebook AIâ€™s [Detectron2](https://github.com/facebookresearch/detectron2/)
    package for its visual backbone. See [this link](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)
    for installation instructions.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LayoutLMv2ä½¿ç”¨Facebook AIçš„[Detectron2](https://github.com/facebookresearch/detectron2/)åŒ…ä½œä¸ºå…¶è§†è§‰éª¨å¹²ã€‚æŸ¥çœ‹[æ­¤é“¾æ¥](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)è·å–å®‰è£…è¯´æ˜ã€‚
- en: 'In addition to `input_ids`, [forward()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model.forward)
    expects 2 additional inputs, namely `image` and `bbox`. The `image` input corresponds
    to the original document image in which the text tokens occur. The model expects
    each document image to be of size 224x224\. This means that if you have a batch
    of document images, `image` should be a tensor of shape (batch_size, 3, 224, 224).
    This can be either a `torch.Tensor` or a `Detectron2.structures.ImageList`. You
    donâ€™t need to normalize the channels, as this is done by the model. Important
    to note is that the visual backbone expects BGR channels instead of RGB, as all
    models in Detectron2 are pre-trained using the BGR format. The `bbox` input are
    the bounding boxes (i.e. 2D-positions) of the input text tokens. This is identical
    to [LayoutLMModel](/docs/transformers/v4.37.2/en/model_doc/layoutlm#transformers.LayoutLMModel).
    These can be obtained using an external OCR engine such as Googleâ€™s [Tesseract](https://github.com/tesseract-ocr/tesseract)
    (thereâ€™s a [Python wrapper](https://pypi.org/project/pytesseract/) available).
    Each bounding box should be in (x0, y0, x1, y1) format, where (x0, y0) corresponds
    to the position of the upper left corner in the bounding box, and (x1, y1) represents
    the position of the lower right corner. Note that one first needs to normalize
    the bounding boxes to be on a 0-1000 scale. To normalize, you can use the following
    function:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™¤äº†`input_ids`ï¼Œ[forward()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model.forward)è¿˜éœ€è¦2ä¸ªé¢å¤–çš„è¾“å…¥ï¼Œå³`image`å’Œ`bbox`ã€‚`image`è¾“å…¥å¯¹åº”äºæ–‡æœ¬æ ‡è®°å‡ºç°çš„åŸå§‹æ–‡æ¡£å›¾åƒã€‚æ¨¡å‹æœŸæœ›æ¯ä¸ªæ–‡æ¡£å›¾åƒçš„å¤§å°ä¸º224x224ã€‚è¿™æ„å‘³ç€å¦‚æœæ‚¨æœ‰ä¸€æ‰¹æ–‡æ¡£å›¾åƒï¼Œ`image`åº”è¯¥æ˜¯å½¢çŠ¶ä¸º(batch_size,
    3, 224, 224)çš„å¼ é‡ã€‚è¿™å¯ä»¥æ˜¯`torch.Tensor`æˆ–`Detectron2.structures.ImageList`ã€‚æ‚¨ä¸éœ€è¦å¯¹é€šé“è¿›è¡Œå½’ä¸€åŒ–ï¼Œå› ä¸ºæ¨¡å‹ä¼šè‡ªè¡Œå¤„ç†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè§†è§‰ä¸»å¹²æœŸæœ›BGRé€šé“è€Œä¸æ˜¯RGBï¼Œå› ä¸ºDetectron2ä¸­çš„æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨BGRæ ¼å¼è¿›è¡Œé¢„è®­ç»ƒçš„ã€‚`bbox`è¾“å…¥æ˜¯è¾“å…¥æ–‡æœ¬æ ‡è®°çš„è¾¹ç•Œæ¡†ï¼ˆå³2Dä½ç½®ï¼‰ã€‚è¿™ä¸[LayoutLMModel](/docs/transformers/v4.37.2/en/model_doc/layoutlm#transformers.LayoutLMModel)ç›¸åŒã€‚å¯ä»¥ä½¿ç”¨å¤–éƒ¨OCRå¼•æ“ï¼ˆä¾‹å¦‚Googleçš„[Tesseract](https://github.com/tesseract-ocr/tesseract)ï¼‰ï¼ˆæœ‰ä¸€ä¸ª[PythonåŒ…è£…å™¨](https://pypi.org/project/pytesseract/)å¯ç”¨ï¼‰æ¥è·å–è¿™äº›ä¿¡æ¯ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”é‡‡ç”¨(x0,
    y0, x1, y1)æ ¼å¼ï¼Œå…¶ä¸­(x0, y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1)è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚è¯·æ³¨æ„ï¼Œé¦–å…ˆéœ€è¦å°†è¾¹ç•Œæ¡†å½’ä¸€åŒ–ä¸º0-1000çš„æ¯”ä¾‹ã€‚è¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, `width` and `height` correspond to the width and height of the original
    document in which the token occurs (before resizing the image). Those can be obtained
    using the Python Image Library (PIL) library for example, as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ`width`å’Œ`height`å¯¹åº”äºæ ‡è®°å‡ºç°çš„åŸå§‹æ–‡æ¡£çš„å®½åº¦å’Œé«˜åº¦ï¼ˆåœ¨è°ƒæ•´å›¾åƒå¤§å°ä¹‹å‰ï¼‰ã€‚å¯ä»¥ä½¿ç”¨Python Image Libraryï¼ˆPILï¼‰åº“æ¥è·å–è¿™äº›ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, this model includes a brand new [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    which can be used to directly prepare data for the model (including applying OCR
    under the hood). More information can be found in the â€œUsageâ€ section below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¯¥æ¨¡å‹åŒ…å«ä¸€ä¸ªå…¨æ–°çš„[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)ï¼Œå¯ç”¨äºç›´æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®ï¼ˆåŒ…æ‹¬åœ¨åº•å±‚åº”ç”¨OCRï¼‰ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨ä¸‹é¢çš„â€œä½¿ç”¨â€éƒ¨åˆ†æ‰¾åˆ°ã€‚
- en: Internally, [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    will send the `image` input through its visual backbone to obtain a lower-resolution
    feature map, whose shape is equal to the `image_feature_pool_shape` attribute
    of [LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config).
    This feature map is then flattened to obtain a sequence of image tokens. As the
    size of the feature map is 7x7 by default, one obtains 49 image tokens. These
    are then concatenated with the text tokens, and send through the Transformer encoder.
    This means that the last hidden states of the model will have a length of 512
    + 49 = 561, if you pad the text tokens up to the max length. More generally, the
    last hidden states will have a shape of `seq_length` + `image_feature_pool_shape[0]`
    * `config.image_feature_pool_shape[1]`.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å†…éƒ¨ï¼Œ[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)å°†é€šè¿‡å…¶è§†è§‰ä¸»å¹²å‘é€`image`è¾“å…¥ï¼Œä»¥è·å¾—ä¸€ä¸ªåˆ†è¾¨ç‡è¾ƒä½çš„ç‰¹å¾å›¾ï¼Œå…¶å½¢çŠ¶ç­‰äº[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)çš„`image_feature_pool_shape`å±æ€§ã€‚ç„¶åå°†è¯¥ç‰¹å¾å›¾å±•å¹³ä»¥è·å¾—ä¸€ç³»åˆ—å›¾åƒæ ‡è®°ã€‚ç”±äºç‰¹å¾å›¾çš„å¤§å°é»˜è®¤ä¸º7x7ï¼Œå› æ­¤è·å¾—49ä¸ªå›¾åƒæ ‡è®°ã€‚ç„¶åå°†è¿™äº›æ ‡è®°ä¸æ–‡æœ¬æ ‡è®°è¿æ¥ï¼Œå¹¶é€šè¿‡Transformerç¼–ç å™¨å‘é€ã€‚è¿™æ„å‘³ç€æ¨¡å‹çš„æœ€åéšè—çŠ¶æ€å°†å…·æœ‰é•¿åº¦ä¸º512
    + 49 = 561ï¼Œå¦‚æœæ‚¨å°†æ–‡æœ¬æ ‡è®°å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œæœ€åçš„éšè—çŠ¶æ€å°†å…·æœ‰å½¢çŠ¶`seq_length` + `image_feature_pool_shape[0]`
    * `config.image_feature_pool_shape[1]`ã€‚
- en: When calling [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained),
    a warning will be printed with a long list of parameter names that are not initialized.
    This is not a problem, as these parameters are batch normalization statistics,
    which are going to have values when fine-tuning on a custom dataset.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ—¶ï¼Œå°†æ‰“å°ä¸€ä¸ªè­¦å‘Šï¼Œå…¶ä¸­åŒ…å«ä¸€é•¿ä¸²æœªåˆå§‹åŒ–çš„å‚æ•°åç§°ã€‚è¿™ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºè¿™äº›å‚æ•°æ˜¯æ‰¹é‡å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®ï¼Œåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒæ—¶å°†å…·æœ‰å€¼ã€‚
- en: If you want to train the model in a distributed environment, make sure to call
    `synchronize_batch_norm` on the model in order to properly synchronize the batch
    normalization layers of the visual backbone.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœè¦åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®­ç»ƒæ¨¡å‹ï¼Œè¯·ç¡®ä¿åœ¨æ¨¡å‹ä¸Šè°ƒç”¨`synchronize_batch_norm`ï¼Œä»¥ä¾¿æ­£ç¡®åŒæ­¥è§†è§‰ä¸»å¹²çš„æ‰¹é‡å½’ä¸€åŒ–å±‚ã€‚
- en: In addition, thereâ€™s LayoutXLM, which is a multilingual version of LayoutLMv2\.
    More information can be found on [LayoutXLMâ€™s documentation page](layoutxlm).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¿˜æœ‰LayoutXLMï¼Œè¿™æ˜¯LayoutLMv2çš„å¤šè¯­è¨€ç‰ˆæœ¬ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨[LayoutXLMçš„æ–‡æ¡£é¡µé¢](layoutxlm)æ‰¾åˆ°ã€‚
- en: Resources
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with LayoutLMv2\. If youâ€™re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and weâ€™ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨LayoutLMv2ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æå‡ºæ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: Text Classification
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ†ç±»
- en: A notebook on how to [finetune LayoutLMv2 for text-classification on RVL-CDIP
    dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•åœ¨RVL-CDIPæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„ç¬”è®°ã€‚
- en: 'See also: [Text classification task guide](../tasks/sequence_classification)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼šæ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—
- en: Question Answering
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”
- en: A notebook on how to [finetune LayoutLMv2 for question-answering on DocVQA dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•åœ¨DocVQAæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œé—®ç­”å¾®è°ƒçš„ç¬”è®°ã€‚
- en: 'See also: [Question answering task guide](../tasks/question_answering)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼šé—®ç­”ä»»åŠ¡æŒ‡å—
- en: 'See also: [Document question answering task guide](../tasks/document_question_answering)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼šæ–‡æ¡£é—®ç­”ä»»åŠ¡æŒ‡å—
- en: Token Classification
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åˆ†ç±»
- en: A notebook on how to [finetune LayoutLMv2 for token-classification on CORD dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•åœ¨CORDæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ ‡è®°åˆ†ç±»çš„ç¬”è®°ã€‚
- en: A notebook on how to [finetune LayoutLMv2 for token-classification on FUNSD
    dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•åœ¨FUNSDæ•°æ®é›†ä¸Šå¯¹LayoutLMv2è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ ‡è®°åˆ†ç±»çš„ç¬”è®°ã€‚
- en: 'See also: [Token classification task guide](../tasks/token_classification)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼šæ ‡è®°åˆ†ç±»ä»»åŠ¡æŒ‡å—
- en: 'Usage: LayoutLMv2Processor'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨æ³•ï¼šLayoutLMv2Processor
- en: The easiest way to prepare data for the model is to use [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor),
    which internally combines a image processor ([LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor))
    and a tokenizer ([LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)).
    The image processor handles the image modality, while the tokenizer handles the
    text modality. A processor combines both, which is ideal for a multi-modal model
    like LayoutLMv2\. Note that you can still use both separately, if you only want
    to handle one modality.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨LayoutLMv2Processorï¼Œå®ƒåœ¨å†…éƒ¨ç»“åˆäº†å›¾åƒå¤„ç†å™¨ï¼ˆLayoutLMv2ImageProcessorï¼‰å’Œæ ‡è®°å™¨ï¼ˆLayoutLMv2Tokenizeræˆ–LayoutLMv2TokenizerFastï¼‰ã€‚å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒæ¨¡æ€ï¼Œè€Œæ ‡è®°å™¨å¤„ç†æ–‡æœ¬æ¨¡æ€ã€‚å¤„ç†å™¨ç»“åˆäº†ä¸¤è€…ï¼Œè¿™å¯¹äºåƒLayoutLMv2è¿™æ ·çš„å¤šæ¨¡æ€æ¨¡å‹æ˜¯ç†æƒ³çš„ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨åªæƒ³å¤„ç†ä¸€ä¸ªæ¨¡æ€ï¼Œä»ç„¶å¯ä»¥åˆ†åˆ«ä½¿ç”¨ä¸¤è€…ã€‚
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In short, one can provide a document image (and possibly additional data) to
    [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor),
    and it will create the inputs expected by the model. Internally, the processor
    first uses [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    to apply OCR on the image to get a list of words and normalized bounding boxes,
    as well to resize the image to a given size in order to get the `image` input.
    The words and normalized bounding boxes are then provided to [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast),
    which converts them to token-level `input_ids`, `attention_mask`, `token_type_ids`,
    `bbox`. Optionally, one can provide word labels to the processor, which are turned
    into token-level `labels`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œå¯ä»¥å°†æ–‡æ¡£å›¾åƒï¼ˆä»¥åŠå¯èƒ½çš„å…¶ä»–æ•°æ®ï¼‰æä¾›ç»™LayoutLMv2Processorï¼Œå®ƒå°†åˆ›å»ºæ¨¡å‹æœŸæœ›çš„è¾“å…¥ã€‚åœ¨å†…éƒ¨ï¼Œå¤„ç†å™¨é¦–å…ˆä½¿ç”¨LayoutLMv2ImageProcessoråœ¨å›¾åƒä¸Šåº”ç”¨OCRï¼Œä»¥è·å–å•è¯åˆ—è¡¨å’Œæ ‡å‡†åŒ–è¾¹ç•Œæ¡†ï¼Œå¹¶å°†å›¾åƒè°ƒæ•´å¤§å°ä»¥è·å¾—`image`è¾“å…¥ã€‚ç„¶åï¼Œå•è¯å’Œæ ‡å‡†åŒ–è¾¹ç•Œæ¡†æä¾›ç»™LayoutLMv2Tokenizeræˆ–LayoutLMv2TokenizerFastï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox`ã€‚å¯é€‰åœ°ï¼Œå¯ä»¥å‘å¤„ç†å™¨æä¾›å•è¯æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å°†è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`labels`ã€‚
- en: '[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    uses [PyTesseract](https://pypi.org/project/pytesseract/), a Python wrapper around
    Googleâ€™s Tesseract OCR engine, under the hood. Note that you can still use your
    own OCR engine of choice, and provide the words and normalized boxes yourself.
    This requires initializing [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    with `apply_ocr` set to `False`.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)ä½¿ç”¨[PyTesseract](https://pypi.org/project/pytesseract/)ï¼Œè¿™æ˜¯Googleçš„Tesseract
    OCRå¼•æ“çš„Pythonå°è£…ã€‚è¯·æ³¨æ„ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨è‡ªå·±é€‰æ‹©çš„OCRå¼•æ“ï¼Œå¹¶è‡ªå·±æä¾›å•è¯å’Œæ ‡å‡†åŒ–æ¡†ã€‚è¿™éœ€è¦ä½¿ç”¨`apply_ocr`è®¾ç½®ä¸º`False`æ¥åˆå§‹åŒ–[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)ã€‚'
- en: In total, there are 5 use cases that are supported by the processor. Below,
    we list them all. Note that each of these use cases work for both batched and
    non-batched inputs (we illustrate them for non-batched inputs).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»å…±æœ‰5ä¸ªå¤„ç†å™¨æ”¯æŒçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºå®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›ä½¿ç”¨æ¡ˆä¾‹å¯¹æ‰¹å¤„ç†å’Œéæ‰¹å¤„ç†è¾“å…¥éƒ½é€‚ç”¨ï¼ˆæˆ‘ä»¬ä¸ºéæ‰¹å¤„ç†è¾“å…¥è¿›è¡Œè¯´æ˜ï¼‰ã€‚
- en: '**Use case 1: document image classification (training, inference) + token classification
    (inference), apply_ocr = True**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ˆä¾‹1ï¼šæ–‡æ¡£å›¾åƒåˆ†ç±»ï¼ˆè®­ç»ƒã€æ¨ç†ï¼‰+æ ‡è®°åˆ†ç±»ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=True
- en: This is the simplest case, in which the processor (actually the image processor)
    will perform OCR on the image to get the words and normalized bounding boxes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ€ç®€å•çš„æƒ…å†µï¼Œå¤„ç†å™¨ï¼ˆå®é™…ä¸Šæ˜¯å›¾åƒå¤„ç†å™¨ï¼‰å°†å¯¹å›¾åƒæ‰§è¡ŒOCRï¼Œä»¥è·å–å•è¯å’Œæ ‡å‡†åŒ–è¾¹ç•Œæ¡†ã€‚
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Use case 2: document image classification (training, inference) + token classification
    (inference), apply_ocr=False**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ˆä¾‹2ï¼šæ–‡æ¡£å›¾åƒåˆ†ç±»ï¼ˆè®­ç»ƒã€æ¨ç†ï¼‰+æ ‡è®°åˆ†ç±»ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=False
- en: In case one wants to do OCR themselves, one can initialize the image processor
    with `apply_ocr` set to `False`. In that case, one should provide the words and
    corresponding (normalized) bounding boxes themselves to the processor.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæƒ³è¦è‡ªå·±æ‰§è¡ŒOCRï¼Œå¯ä»¥å°†å›¾åƒå¤„ç†å™¨åˆå§‹åŒ–ä¸º`apply_ocr`è®¾ç½®ä¸º`False`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåº”è¯¥è‡ªå·±å‘å¤„ç†å™¨æä¾›å•è¯å’Œç›¸åº”çš„ï¼ˆæ ‡å‡†åŒ–çš„ï¼‰è¾¹ç•Œæ¡†ã€‚
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Use case 3: token classification (training), apply_ocr=False**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ˆä¾‹3ï¼šæ ‡è®°åˆ†ç±»ï¼ˆè®­ç»ƒï¼‰ï¼Œapply_ocr=False
- en: For token classification tasks (such as FUNSD, CORD, SROIE, Kleister-NDA), one
    can also provide the corresponding word labels in order to train a model. The
    processor will then convert these into token-level `labels`. By default, it will
    only label the first wordpiece of a word, and label the remaining wordpieces with
    -100, which is the `ignore_index` of PyTorchâ€™s CrossEntropyLoss. In case you want
    all wordpieces of a word to be labeled, you can initialize the tokenizer with
    `only_label_first_subword` set to `False`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚FUNSDã€CORDã€SROIEã€Kleister-NDAï¼‰ï¼Œè¿˜å¯ä»¥æä¾›ç›¸åº”çš„å•è¯æ ‡ç­¾ä»¥è®­ç»ƒæ¨¡å‹ã€‚å¤„ç†å™¨å°†æŠŠè¿™äº›è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`labels`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒåªä¼šæ ‡è®°å•è¯çš„ç¬¬ä¸€ä¸ªè¯ç‰‡ï¼Œå¹¶ç”¨-100æ ‡è®°å‰©ä½™çš„è¯ç‰‡ï¼Œè¿™æ˜¯PyTorchçš„CrossEntropyLossçš„`ignore_index`ã€‚å¦‚æœå¸Œæœ›æ ‡è®°å•è¯çš„æ‰€æœ‰è¯ç‰‡ï¼Œå¯ä»¥å°†åˆ†è¯å™¨åˆå§‹åŒ–ä¸º`only_label_first_subword`è®¾ç½®ä¸º`False`ã€‚
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Use case 4: visual question answering (inference), apply_ocr=True**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ˆä¾‹4ï¼šè§†è§‰é—®ç­”ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=True
- en: For visual question answering tasks (such as DocVQA), you can provide a question
    to the processor. By default, the processor will apply OCR on the image, and create
    [CLS] question tokens [SEP] word tokens [SEP].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆå¦‚DocVQAï¼‰ï¼Œæ‚¨å¯ä»¥å‘å¤„ç†å™¨æä¾›é—®é¢˜ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¤„ç†å™¨å°†åœ¨å›¾åƒä¸Šåº”ç”¨OCRï¼Œå¹¶åˆ›å»º[CLS]é—®é¢˜æ ‡è®°[SEP]å•è¯æ ‡è®°[SEP]ã€‚
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Use case 5: visual question answering (inference), apply_ocr=False**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ˆä¾‹5ï¼šè§†è§‰é—®ç­”ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=False
- en: For visual question answering tasks (such as DocVQA), you can provide a question
    to the processor. If you want to perform OCR yourself, you can provide your own
    words and (normalized) bounding boxes to the processor.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆå¦‚DocVQAï¼‰ï¼Œæ‚¨å¯ä»¥å‘å¤„ç†å™¨æä¾›é—®é¢˜ã€‚å¦‚æœæ‚¨æƒ³è‡ªå·±æ‰§è¡ŒOCRï¼Œå¯ä»¥å‘å¤„ç†å™¨æä¾›è‡ªå·±çš„å•è¯å’Œï¼ˆæ ‡å‡†åŒ–çš„ï¼‰è¾¹ç•Œæ¡†ã€‚
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: LayoutLMv2Config
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2Config
- en: '### `class transformers.LayoutLMv2Config`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/configuration_layoutlmv2.py#L34)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/configuration_layoutlmv2.py#L34)'
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) â€” Vocabulary size of the
    LayoutLMv2 model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    or `TFLayoutLMv2Model`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º30522ï¼‰â€”LayoutLMv2æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)æˆ–`TFLayoutLMv2Model`æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimension of the encoder
    layers and the pooler layer.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º768ï¼‰â€”ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º12ï¼‰â€”å˜æ¢å™¨ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º12ï¼‰â€”å˜æ¢å™¨ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimension of the
    â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3072ï¼‰â€”å˜æ¢å™¨ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act`ï¼ˆ`str`æˆ–`function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"gelu"`ï¼‰â€”ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1ï¼‰â€”åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) â€” The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º512) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚512ã€1024æˆ–2048ï¼‰ã€‚'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) â€” The vocabulary size
    of the `token_type_ids` passed when calling [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    or `TFLayoutLMv2Model`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *optional*, é»˜è®¤ä¸º2) â€” åœ¨è°ƒç”¨[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)æˆ–`TFLayoutLMv2Model`æ—¶ä¼ é€’çš„`token_type_ids`çš„è¯æ±‡è¡¨å¤§å°ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, é»˜è®¤ä¸º1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`max_2d_position_embeddings` (`int`, *optional*, defaults to 1024) â€” The maximum
    value that the 2D position embedding might ever be used with. Typically set this
    to something large just in case (e.g., 1024).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_2d_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º1024) â€” 2Dä½ç½®åµŒå…¥å¯èƒ½ä½¿ç”¨çš„æœ€å¤§å€¼ã€‚é€šå¸¸è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚1024ï¼‰ã€‚'
- en: '`max_rel_pos` (`int`, *optional*, defaults to 128) â€” The maximum number of
    relative positions to be used in the self-attention mechanism.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_rel_pos` (`int`, *optional*, é»˜è®¤ä¸º128) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­è¦ä½¿ç”¨çš„ç›¸å¯¹ä½ç½®çš„æœ€å¤§æ•°é‡ã€‚'
- en: '`rel_pos_bins` (`int`, *optional*, defaults to 32) â€” The number of relative
    position bins to be used in the self-attention mechanism.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rel_pos_bins` (`int`, *optional*, é»˜è®¤ä¸º32) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­è¦ä½¿ç”¨çš„ç›¸å¯¹ä½ç½®æ¡¶çš„æ•°é‡ã€‚'
- en: '`fast_qkv` (`bool`, *optional*, defaults to `True`) â€” Whether or not to use
    a single matrix for the queries, keys, values in the self-attention layers.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fast_qkv` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨å•ä¸ªçŸ©é˜µä½œä¸ºæŸ¥è¯¢ã€é”®ã€å€¼ã€‚'
- en: '`max_rel_2d_pos` (`int`, *optional*, defaults to 256) â€” The maximum number
    of relative 2D positions in the self-attention mechanism.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_rel_2d_pos` (`int`, *optional*, é»˜è®¤ä¸º256) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨çš„ç›¸å¯¹2Dä½ç½®çš„æœ€å¤§æ•°é‡ã€‚'
- en: '`rel_2d_pos_bins` (`int`, *optional*, defaults to 64) â€” The number of 2D relative
    position bins in the self-attention mechanism.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rel_2d_pos_bins` (`int`, *optional*, é»˜è®¤ä¸º64) â€” è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„2Dç›¸å¯¹ä½ç½®æ¡¶çš„æ•°é‡ã€‚'
- en: '`image_feature_pool_shape` (`List[int]`, *optional*, defaults to [7, 7, 256])
    â€” The shape of the average-pooled feature map.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_feature_pool_shape` (`List[int]`, *optional*, é»˜è®¤ä¸º[7, 7, 256]) â€” å¹³å‡æ± åŒ–ç‰¹å¾å›¾çš„å½¢çŠ¶ã€‚'
- en: '`coordinate_size` (`int`, *optional*, defaults to 128) â€” Dimension of the coordinate
    embeddings.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coordinate_size` (`int`, *optional*, é»˜è®¤ä¸º128) â€” åæ ‡åµŒå…¥çš„ç»´åº¦ã€‚'
- en: '`shape_size` (`int`, *optional*, defaults to 128) â€” Dimension of the width
    and height embeddings.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shape_size` (`int`, *optional*, é»˜è®¤ä¸º128) â€” å®½åº¦å’Œé«˜åº¦åµŒå…¥çš„ç»´åº¦ã€‚'
- en: '`has_relative_attention_bias` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to use a relative attention bias in the self-attention mechanism.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`has_relative_attention_bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨ç›¸å¯¹æ³¨æ„åŠ›åç½®ã€‚'
- en: '`has_spatial_attention_bias` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to use a spatial attention bias in the self-attention mechanism.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`has_spatial_attention_bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›åç½®ã€‚'
- en: '`has_visual_segment_embedding` (`bool`, *optional*, defaults to `False`) â€”
    Whether or not to add visual segment embeddings.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`has_visual_segment_embedding` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ·»åŠ è§†è§‰æ®µåµŒå…¥ã€‚'
- en: '`detectron2_config_args` (`dict`, *optional*) â€” Dictionary containing the configuration
    arguments of the Detectron2 visual backbone. Refer to [this file](https://github.com/microsoft/unilm/blob/master/layoutlmft/layoutlmft/models/layoutlmv2/detectron2_config.py)
    for details regarding default values.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detectron2_config_args` (`dict`, *optional*) â€” åŒ…å«Detectron2è§†è§‰éª¨å¹²é…ç½®å‚æ•°çš„å­—å…¸ã€‚æœ‰å…³é»˜è®¤å€¼çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤æ–‡ä»¶](https://github.com/microsoft/unilm/blob/master/layoutlmft/layoutlmft/models/layoutlmv2/detectron2_config.py)ã€‚'
- en: This is the configuration class to store the configuration of a [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model).
    It is used to instantiate an LayoutLMv2 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the LayoutLMv2 [microsoft/layoutlmv2-base-uncased](https://huggingface.co/microsoft/layoutlmv2-base-uncased)
    architecture.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªLayoutLMv2æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºLayoutLMv2
    [microsoft/layoutlmv2-base-uncased](https://huggingface.co/microsoft/layoutlmv2-base-uncased)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: LayoutLMv2FeatureExtractor
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2FeatureExtractor
- en: '### `class transformers.LayoutLMv2FeatureExtractor`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2FeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py#L28)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py#L28)'
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `__call__`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Preprocess an image or a batch of images.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚
- en: LayoutLMv2ImageProcessor
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2ImageProcessor
- en: '### `class transformers.LayoutLMv2ImageProcessor`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2ImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L93)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L93)'
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Whether to resize the
    imageâ€™s (height, width) dimensions to `(size["height"], size["width"])`. Can be
    overridden by `do_resize` in `preprocess`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ (height, width) å°ºå¯¸è°ƒæ•´ä¸º `(size["height"],
    size["width"])`ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `do_resize` è¦†ç›–ã€‚'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 224, "width":
    224}`): Size of the image after resizing. Can be overridden by `size` in `preprocess`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *å¯é€‰*, é»˜è®¤ä¸º `{"height" -- 224, "width": 224}`): è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¯ä»¥è¢«
    `preprocess` ä¸­çš„ `size` è¦†ç›–ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    â€” Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º `Resampling.BILINEAR`) â€” ç”¨äºè°ƒæ•´å›¾åƒå¤§å°æ—¶ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢«
    `preprocess` æ–¹æ³•ä¸­çš„ `resample` å‚æ•°è¦†ç›–ã€‚'
- en: '`apply_ocr` (`bool`, *optional*, defaults to `True`) â€” Whether to apply the
    Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden
    by `apply_ocr` in `preprocess`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_ocr` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åº”ç”¨ Tesseract OCR å¼•æ“ä»¥è·å–å•è¯ + è§„èŒƒåŒ–è¾¹ç•Œæ¡†ã€‚å¯ä»¥è¢«
    `preprocess` ä¸­çš„ `apply_ocr` è¦†ç›–ã€‚'
- en: '`ocr_lang` (`str`, *optional*) â€” The language, specified by its ISO code, to
    be used by the Tesseract OCR engine. By default, English is used. Can be overridden
    by `ocr_lang` in `preprocess`.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ocr_lang` (`str`, *å¯é€‰*) â€” ç”±å…¶ ISO ä»£ç æŒ‡å®šçš„è¯­è¨€ï¼Œç”¨äº Tesseract OCR å¼•æ“ã€‚é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨è‹±è¯­ã€‚å¯ä»¥è¢«
    `preprocess` ä¸­çš„ `ocr_lang` è¦†ç›–ã€‚'
- en: '`tesseract_config` (`str`, *optional*, defaults to `""`) â€” Any additional custom
    configuration flags that are forwarded to the `config` parameter when calling
    Tesseract. For example: â€˜â€”psm 6â€™. Can be overridden by `tesseract_config` in `preprocess`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tesseract_config` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `""`) â€” è½¬å‘åˆ°è°ƒç”¨ Tesseract æ—¶ `config` å‚æ•°çš„ä»»ä½•é¢å¤–è‡ªå®šä¹‰é…ç½®æ ‡å¿—ã€‚ä¾‹å¦‚:
    â€˜â€”psm 6â€™ã€‚å¯ä»¥è¢« `preprocess` ä¸­çš„ `tesseract_config` è¦†ç›–ã€‚'
- en: Constructs a LayoutLMv2 image processor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª LayoutLMv2 å›¾åƒå¤„ç†å™¨ã€‚
- en: '#### `preprocess`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L189)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L189)'
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`ImageInput`) â€” Image to preprocess.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) â€” Whether to
    resize the image.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” Desired size
    of the output image after resizing.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º `self.size`) â€” è°ƒæ•´å¤§å°åè¾“å‡ºå›¾åƒçš„æœŸæœ›å°ºå¯¸ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    â€” Resampling filter to use if resizing the image. This can be one of the enum
    `PIL.Image` resampling filter. Only has an effect if `do_resize` is set to `True`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º `self.resample`) â€” ç”¨äºè°ƒæ•´å›¾åƒå¤§å°æ—¶ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥æ˜¯æšä¸¾
    `PIL.Image` é‡é‡‡æ ·æ»¤æ³¢å™¨ä¹‹ä¸€ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`apply_ocr` (`bool`, *optional*, defaults to `self.apply_ocr`) â€” Whether to
    apply the Tesseract OCR engine to get words + normalized bounding boxes.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_ocr` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `self.apply_ocr`) â€” æ˜¯å¦åº”ç”¨ Tesseract OCR å¼•æ“ä»¥è·å–å•è¯
    + è§„èŒƒåŒ–è¾¹ç•Œæ¡†ã€‚'
- en: '`ocr_lang` (`str`, *optional*, defaults to `self.ocr_lang`) â€” The language,
    specified by its ISO code, to be used by the Tesseract OCR engine. By default,
    English is used.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ocr_lang` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `self.ocr_lang`) â€” ç”±å…¶ ISO ä»£ç æŒ‡å®šçš„è¯­è¨€ï¼Œç”¨äº Tesseract
    OCR å¼•æ“ã€‚é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨è‹±è¯­ã€‚'
- en: '`tesseract_config` (`str`, *optional*, defaults to `self.tesseract_config`)
    â€” Any additional custom configuration flags that are forwarded to the `config`
    parameter when calling Tesseract.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tesseract_config` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `self.tesseract_config`) â€” è½¬å‘åˆ°è°ƒç”¨ Tesseract
    æ—¶ `config` å‚æ•°çš„ä»»ä½•é¢å¤–è‡ªå®šä¹‰é…ç½®æ ‡å¿—ã€‚'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) â€” The type of tensors
    to return. Can be one of:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– `TensorType`, *å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'æœªè®¾ç½®: è¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚'
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` æˆ– `''tf''`: è¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` æˆ– `''pt''`: è¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` æˆ– `''np''`: è¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹å¤„ç†ã€‚'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` æˆ– `''jax''`: è¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹å¤„ç†ã€‚'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” The channel dimension format for the output image. Can be one of:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º `ChannelDimension.FIRST`)
    â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (num_channels, height, width)ã€‚'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (height, width, num_channels)ã€‚'
- en: Preprocess an image or batch of images.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚
- en: LayoutLMv2Tokenizer
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2Tokenizer
- en: '### `class transformers.LayoutLMv2Tokenizer`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2Tokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L206)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L206)'
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Construct a LayoutLMv2 tokenizer. Based on WordPiece. [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    can be used to turn words, word-level bounding boxes and optional word labels
    to token-level `input_ids`, `attention_mask`, `token_type_ids`, `bbox`, and optional
    `labels` (for token classification).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª LayoutLMv2 åˆ†è¯å™¨ã€‚åŸºäº WordPieceã€‚[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    å¯ç”¨äºå°†å•è¯ã€å•è¯çº§è¾¹ç•Œæ¡†å’Œå¯é€‰å•è¯æ ‡ç­¾è½¬æ¢ä¸ºæ ‡è®°çº§çš„ `input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox`
    å’Œå¯é€‰çš„ `labels`ï¼ˆç”¨äºæ ‡è®°åˆ†ç±»ï¼‰ã€‚
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¿™ä¸ªè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    runs end-to-end tokenization: punctuation splitting and wordpiece. It also turns
    the word-level bounding boxes into token-level bounding boxes.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    è¿è¡Œç«¯åˆ°ç«¯çš„åˆ†è¯ï¼šæ ‡ç‚¹ç¬¦å·æ‹†åˆ†å’Œ wordpieceã€‚å®ƒè¿˜å°†å•è¯çº§è¾¹ç•Œæ¡†è½¬æ¢ä¸ºæ ‡è®°çº§è¾¹ç•Œæ¡†ã€‚'
- en: '#### `__call__`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L430)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L430)'
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`ï¼ˆ`str`ã€`List[str]`ã€`List[List[str]]`ï¼‰â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå•ä¸ªç¤ºä¾‹çš„å•è¯æˆ–ä¸€æ‰¹ç¤ºä¾‹çš„é—®é¢˜ï¼‰æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼ˆå•è¯æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`text_pair` (`List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`ï¼ˆ`List[str]`ã€`List[List[str]]`ï¼‰â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes`ï¼ˆ`List[List[int]]`ã€`List[List[List[int]]]`ï¼‰â€” å•è¯çº§è¾¹ç•Œæ¡†ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥è¢«å½’ä¸€åŒ–ä¸º 0-1000
    çš„æ¯”ä¾‹ã€‚'
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) â€” Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_labels`ï¼ˆ`List[int]`ã€`List[List[int]]`ï¼Œ*å¯é€‰*ï¼‰â€” å•è¯çº§æ•´æ•°æ ‡ç­¾ï¼ˆç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼Œå¦‚ FUNSDã€CORDï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰â€” æ˜¯å¦ä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ã€`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`ï¼šå¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œå¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ã€`str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚è¿™å°†é€æ ‡è®°æˆªæ–­ï¼Œå¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä¼šä»è¾ƒé•¿åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼ˆä½¿ç”¨å‚æ•° `max_length`ï¼‰æˆ–æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œæˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—é•¿åº¦çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œå¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€éœ€è¦æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦å°†è¢«åœç”¨ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” å¦‚æœä¸`max_length`ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºä»¤ç‰Œå°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›ä»¤ç‰Œï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰é‡å ä»¤ç‰Œçš„æ•°é‡ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨å¼ é‡æ ¸å¿ƒç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: '`return_token_type_ids` (`bool`, *optional*) â€” Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä»¤ç‰Œç±»å‹IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å›ä»¤ç‰Œç±»å‹IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ç‰Œç±»å‹IDæ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`return_attention_mask` (`bool`, *optional*) â€” Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›è’™ç‰ˆã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›è’™ç‰ˆï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›è’™ç‰ˆæ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„ä»¤ç‰Œåºåˆ—ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹è¾“å…¥IDåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy
    = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„ä»¤ç‰Œã€‚'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return special tokens mask information.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç‰¹æ®Šä»¤ç‰Œè’™ç‰ˆä¿¡æ¯ã€‚'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æ¯ä¸ªä»¤ç‰Œçš„`(char_start, char_end)`ã€‚'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Pythonâ€™s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚
- en: '`return_length` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) â€” Whether or not to print
    more information and warnings. **kwargs â€” passed to the `self.tokenize()` method'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•'
- en: Returns
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š
- en: '`input_ids` â€” List of token ids to be fed to a model.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰ŒIDåˆ—è¡¨ã€‚'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`bbox` â€” List of bounding boxes to be fed to a model.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„è¾¹ç•Œæ¡†åˆ—è¡¨ã€‚'
- en: '`token_type_ids` â€” List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *â€œtoken_type_idsâ€* is in `self.model_input_names`).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰Œç±»å‹IDåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*â€œtoken_type_idsâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ç‰Œç±»å‹IDæ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`attention_mask` â€” List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *â€œattention_maskâ€* is
    in `self.model_input_names`).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` â€” æŒ‡å®šå“ªäº›ä»¤ç‰Œåº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*â€œattention_maskâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›è’™ç‰ˆæ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`labels` â€” List of labels to be fed to a model. (when `word_labels` is specified).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`word_labels`æ—¶ï¼‰ã€‚'
- en: '`overflowing_tokens` â€” List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` â€” æº¢å‡ºä»¤ç‰Œåºåˆ—çš„åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`num_truncated_tokens` â€” Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` â€” æˆªæ–­çš„æ ‡è®°æ•°ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`special_tokens_mask` â€” List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—æ ‡è®°ï¼ˆå½“`add_special_tokens=True`å’Œ`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚'
- en: '`length` â€” The length of the inputs (when `return_length=True`).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰ã€‚'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œæ ‡è®°åŒ–å’Œä¸ºæ¨¡å‹å‡†å¤‡ï¼Œå…·æœ‰å•è¯çº§åˆ«æ ‡å‡†åŒ–è¾¹ç•Œæ¡†å’Œå¯é€‰æ ‡ç­¾ã€‚
- en: '#### `save_vocabulary`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L410)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L410)'
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: LayoutLMv2TokenizerFast
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2TokenizerFast
- en: '### `class transformers.LayoutLMv2TokenizerFast`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2TokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L70)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L70)'
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” File containing the vocabulary.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) â€” åŒ…å«è¯æ±‡è¡¨çš„æ–‡ä»¶ã€‚'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    lowercase the input when tokenizing.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *optional*, defaults to `True`) â€” åœ¨æ ‡è®°åŒ–æ—¶æ˜¯å¦å°†è¾“å…¥è½¬æ¢ä¸ºå°å†™ã€‚'
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) â€” The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) â€” The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) â€” åˆ†éš”ç¬¦æ ‡è®°ï¼Œåœ¨ä»å¤šä¸ªåºåˆ—æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºæ–‡æœ¬å’Œé—®é¢˜çš„é—®é¢˜å›ç­”ã€‚å®ƒä¹Ÿç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) â€” The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚'
- en: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) â€” The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) â€” åœ¨è¿›è¡Œåºåˆ—åˆ†ç±»ï¼ˆå¯¹æ•´ä¸ªåºåˆ—è€Œä¸æ˜¯æ¯ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼‰æ—¶ä½¿ç”¨çš„åˆ†ç±»å™¨æ ‡è®°ã€‚å½“ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œå®ƒæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) â€” The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) â€” ç”¨äºå±è”½å€¼çš„æ ‡è®°ã€‚åœ¨ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚'
- en: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [CLS] token.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[CLS]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`)
    â€” The bounding box to use for the special [SEP] token.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`)
    â€” ç”¨äºç‰¹æ®Š[SEP]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” The
    bounding box to use for the special [PAD] token.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) â€” ç”¨äºç‰¹æ®Š[PAD]æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚'
- en: '`pad_token_label` (`int`, *optional*, defaults to -100) â€” The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorchâ€™s
    CrossEntropyLoss.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_label` (`int`, *optional*, defaults to -100) â€” ç”¨äºå¡«å……æ ‡è®°çš„æ ‡ç­¾ã€‚é»˜è®¤ä¸º-100ï¼Œè¿™æ˜¯PyTorchçš„CrossEntropyLossçš„`ignore_index`ã€‚'
- en: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to only label the first subword, in case word labels are provided.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä»…æ ‡è®°ç¬¬ä¸€ä¸ªå­è¯ï¼Œå¦‚æœæä¾›äº†å•è¯æ ‡ç­¾ã€‚'
- en: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not to tokenize Chinese characters. This should likely be deactivated for Japanese
    (see [this issue](https://github.com/huggingface/transformers/issues/328)).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦æ ‡è®°åŒ–ä¸­æ–‡å­—ç¬¦ã€‚è¿™å¯èƒ½åº”è¯¥åœ¨æ—¥è¯­ä¸­åœç”¨ï¼ˆå‚è§[æ­¤é—®é¢˜](https://github.com/huggingface/transformers/issues/328)ï¼‰ã€‚'
- en: '`strip_accents` (`bool`, *optional*) â€” Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original LayoutLMv2).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strip_accents` (`bool`, *optional*) â€” æ˜¯å¦å»é™¤æ‰€æœ‰é‡éŸ³ç¬¦å·ã€‚å¦‚æœæœªæŒ‡å®šæ­¤é€‰é¡¹ï¼Œåˆ™å°†ç”±`lowercase`çš„å€¼ç¡®å®šï¼ˆä¸åŸå§‹LayoutLMv2ä¸­çš„æƒ…å†µç›¸åŒï¼‰ã€‚'
- en: Construct a â€œfastâ€ LayoutLMv2 tokenizer (backed by HuggingFaceâ€™s *tokenizers*
    library). Based on WordPiece.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªâ€œå¿«é€Ÿâ€LayoutLMv2åˆ†è¯å™¨ï¼ˆç”±HuggingFaceçš„*tokenizers*åº“æ”¯æŒï¼‰ã€‚åŸºäºWordPieceã€‚
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `__call__`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L179)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L179)'
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå•ä¸ªç¤ºä¾‹çš„å•è¯æˆ–ä¸€æ‰¹ç¤ºä¾‹çš„é—®é¢˜ï¼‰æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼ˆå•è¯æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`text_pair` (`List[str]`, `List[List[str]]`) â€” The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„æ ‡è®°åŒ–å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) â€” å•è¯çº§åˆ«çš„è¾¹ç•Œæ¡†ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”æ ‡å‡†åŒ–ä¸º0-1000çš„æ¯”ä¾‹ã€‚'
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) â€” Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) â€” å•è¯çº§åˆ«çš„æ•´æ•°æ ‡ç­¾ï¼ˆç”¨äºè¯¸å¦‚FUNSDã€CORDç­‰æ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä½¿ç”¨ç›¸å¯¹äºå…¶æ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: å¡«å……åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰: ä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦çš„åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`: æˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚è¿™å°†é€æ ‡è®°æˆªæ–­ï¼Œå¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™ä»è¾ƒé•¿åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: æˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: æˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰: ä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¸­çš„ä¸€ä¸ªéœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, é»˜è®¤ä¸º0) â€” å¦‚æœè®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œå¹¶ä¸”ä¸`max_length`ä¸€èµ·ä½¿ç”¨ï¼Œå½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>=
    7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨Tensor Coresç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚'
- en: '`return_token_type_ids` (`bool`, *optional*) â€” Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä»¤ç‰Œç±»å‹IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›ä»¤ç‰Œç±»å‹IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) â€” Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„ä»¤ç‰Œåºåˆ—ã€‚å¦‚æœæä¾›ä¸€å¯¹è¾“å…¥idåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy
    = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„ä»¤ç‰Œã€‚'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return special tokens mask information.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç‰¹æ®Šä»¤ç‰Œæ©ç ä¿¡æ¯ã€‚'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æ¯ä¸ªä»¤ç‰Œçš„`(char_start, char_end)`ã€‚'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Pythonâ€™s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚
- en: '`return_length` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) â€” Whether or not to print
    more information and warnings. **kwargs â€” passed to the `self.tokenize()` method'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•'
- en: Returns
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸¦æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š
- en: '`input_ids` â€” List of token ids to be fed to a model.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„ä»¤ç‰Œidåˆ—è¡¨ã€‚'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`bbox` â€” List of bounding boxes to be fed to a model.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` â€” è¦æä¾›ç»™æ¨¡å‹çš„è¾¹ç•Œæ¡†åˆ—è¡¨ã€‚'
- en: '`token_type_ids` â€” List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *â€œtoken_type_idsâ€* is in `self.model_input_names`).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„ä»¤ç‰Œç±»å‹idåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–è€…`self.model_input_names`ä¸­åŒ…å«*â€œtoken_type_idsâ€*æ—¶ï¼‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`attention_mask` â€” List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *â€œattention_maskâ€* is
    in `self.model_input_names`).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` â€” æŒ‡å®šå“ªäº›ä»¤ç‰Œåº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–è€…`self.model_input_names`ä¸­åŒ…å«*â€œattention_maskâ€*æ—¶ï¼‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`labels` â€” List of labels to be fed to a model. (when `word_labels` is specified).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` â€” è¦æä¾›ç»™æ¨¡å‹çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`word_labels`æ—¶ï¼‰ã€‚'
- en: '`overflowing_tokens` â€” List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` â€” æº¢å‡ºçš„ä»¤ç‰Œåºåˆ—åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`num_truncated_tokens` â€” Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` â€” æˆªæ–­çš„ä»¤ç‰Œæ•°é‡ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`special_tokens_mask` â€” List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šä»¤ç‰Œï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—ä»¤ç‰Œï¼ˆå½“`add_special_tokens=True`å¹¶ä¸”`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚'
- en: '`length` â€” The length of the inputs (when `return_length=True`).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰ã€‚'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œåˆ†è¯å’Œå‡†å¤‡æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«å•è¯çº§åˆ«çš„å½’ä¸€åŒ–è¾¹ç•Œæ¡†å’Œå¯é€‰æ ‡ç­¾ã€‚
- en: LayoutLMv2Processor
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2Processor
- en: '### `class transformers.LayoutLMv2Processor`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2Processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L27)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L27)'
- en: '[PRE20]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image_processor` (`LayoutLMv2ImageProcessor`, *optional*) â€” An instance of
    [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor).
    The image processor is a required input.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`ï¼ˆ`LayoutLMv2ImageProcessor`ï¼Œ*å¯é€‰*ï¼‰â€” [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)çš„å®ä¾‹ã€‚å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` (`LayoutLMv2Tokenizer` or `LayoutLMv2TokenizerFast`, *optional*)
    â€” An instance of [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast).
    The tokenizer is a required input.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ`LayoutLMv2Tokenizer`æˆ–`LayoutLMv2TokenizerFast`ï¼Œ*å¯é€‰*ï¼‰â€” [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)æˆ–[LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)çš„å®ä¾‹ã€‚æ ‡è®°å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a LayoutLMv2 processor which combines a LayoutLMv2 image processor
    and a LayoutLMv2 tokenizer into a single processor.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªLayoutLMv2å¤„ç†å™¨ï¼Œå°†LayoutLMv2å›¾åƒå¤„ç†å™¨å’ŒLayoutLMv2æ ‡è®°å™¨åˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚
- en: '[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    offers all the functionalities you need to prepare data for the model.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)æä¾›äº†å‡†å¤‡æ¨¡å‹æ•°æ®æ‰€éœ€çš„æ‰€æœ‰åŠŸèƒ½ã€‚'
- en: It first uses [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    to resize document images to a fixed size, and optionally applies OCR to get words
    and normalized bounding boxes. These are then provided to [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast),
    which turns the words and bounding boxes into token-level `input_ids`, `attention_mask`,
    `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which
    are turned into token-level `labels` for token classification tasks (such as FUNSD,
    CORD).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒé¦–å…ˆä½¿ç”¨[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)å°†æ–‡æ¡£å›¾åƒè°ƒæ•´ä¸ºå›ºå®šå¤§å°ï¼Œå¹¶å¯é€‰æ‹©åº”ç”¨OCRä»¥è·å–å•è¯å’Œå½’ä¸€åŒ–è¾¹ç•Œæ¡†ã€‚ç„¶åå°†å®ƒä»¬æä¾›ç»™[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)æˆ–[LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)ï¼Œå°†å•è¯å’Œè¾¹ç•Œæ¡†è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`input_ids`ã€`attention_mask`ã€`token_type_ids`ã€`bbox`ã€‚å¯é€‰åœ°ï¼Œå¯ä»¥æä¾›æ•´æ•°`word_labels`ï¼Œè¿™äº›æ ‡ç­¾å°†è½¬æ¢ä¸ºç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚FUNSDã€CORDï¼‰çš„æ ‡è®°çº§åˆ«`labels`ã€‚
- en: '#### `__call__`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L69)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L69)'
- en: '[PRE21]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This method first forwards the `images` argument to [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__).
    In case [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    was initialized with `apply_ocr` set to `True`, it passes the obtained words and
    bounding boxes along with the additional arguments to [**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)
    and returns the output, together with resized `images`. In case [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    was initialized with `apply_ocr` set to `False`, it passes the words (`text`/`text_pair`)
    and `boxes` specified by the user along with the additional arguments to [__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)
    and returns the output, together with resized `images`.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•é¦–å…ˆå°†`images`å‚æ•°è½¬å‘åˆ°[**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚å¦‚æœ[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)åˆå§‹åŒ–æ—¶`apply_ocr`è®¾ç½®ä¸º`True`ï¼Œå®ƒå°†è·å–çš„å•è¯å’Œè¾¹ç•Œæ¡†è¿åŒå…¶ä»–å‚æ•°ä¼ é€’ç»™[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)å¹¶è¿”å›è¾“å‡ºï¼Œä»¥åŠè°ƒæ•´å¤§å°åçš„`images`ã€‚å¦‚æœ[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)åˆå§‹åŒ–æ—¶`apply_ocr`è®¾ç½®ä¸º`False`ï¼Œå®ƒå°†ç”¨æˆ·æŒ‡å®šçš„å•è¯ï¼ˆ`text`/`text_pair`ï¼‰å’Œ`boxes`è¿åŒå…¶ä»–å‚æ•°ä¼ é€’ç»™[__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)å¹¶è¿”å›è¾“å‡ºï¼Œä»¥åŠè°ƒæ•´å¤§å°åçš„`images`ã€‚
- en: Please refer to the docstring of the above two methods for more information.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒä¸Šè¿°ä¸¤ä¸ªæ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: LayoutLMv2Model
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2Model
- en: '### `class transformers.LayoutLMv2Model`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L688)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L688)'
- en: '[PRE22]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare LayoutLMv2 Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„LayoutLMv2æ¨¡å‹å˜æ¢å™¨ï¼Œè¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L802)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L802)'
- en: '[PRE23]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`bbox` (`torch.LongTensor` of shape `((batch_size, sequence_length), 4)`, *optional*)
    â€” Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` (`torch.LongTensor` of shape `((batch_size, sequence_length), 4)`, *optional*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚é€‰æ‹©èŒƒå›´åœ¨`[0, config.max_2d_position_embeddings-1]`å†…ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯(x0, y0,
    x1, y1)æ ¼å¼çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­(x0, y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1)è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚'
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) â€” Batch of document images.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    æˆ– `detectron.structures.ImageList`ï¼Œå…¶`tensors`çš„å½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” æ–‡æ¡£å›¾åƒçš„æ‰¹å¤„ç†ã€‚'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…çš„æ©ç å€¼ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´åœ¨`[0, config.max_position_embeddings - 1]`å†…ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„ç‰¹å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…çš„æ©ç å€¼ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE24]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: LayoutLMv2ForSequenceClassification
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2ForSequenceClassification
- en: '### `class transformers.LayoutLMv2ForSequenceClassification`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2ForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L944)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L944)'
- en: '[PRE25]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: LayoutLMv2 Model with a sequence classification head on top (a linear layer
    on top of the concatenation of the final hidden state of the [CLS] token, average-pooled
    initial visual embeddings and average-pooled final visual embeddings, e.g. for
    document image classification tasks such as the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)
    dataset.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv2æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªåºåˆ—åˆ†ç±»å¤´ï¼ˆåœ¨[CLS]æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€å¹³å‡æ± åŒ–çš„åˆå§‹è§†è§‰åµŒå…¥å’Œå¹³å‡æ± åŒ–çš„æœ€ç»ˆè§†è§‰åµŒå…¥çš„ä¸²è”ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼Œä¾‹å¦‚ç”¨äºæ–‡æ¡£å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå¦‚[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)æ•°æ®é›†ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L967)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L967)'
- en: '[PRE26]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) â€” Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚é€‰æ‹©èŒƒå›´ä¸º `[0, config.max_2d_position_embeddings-1]`ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯ (x0,
    y0, x1, y1) æ ¼å¼çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­ (x0, y0) å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œè€Œ (x1, y1) è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚'
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) â€” Batch of document images.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    æˆ– `detectron.structures.ImageList`ï¼Œå…¶ `tensors` çš„å½¢çŠ¶ä¸º `(batch_size, num_channels,
    height, width)`) â€” æ–‡æ¡£å›¾åƒçš„æ‰¹å¤„ç†ã€‚'
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` èŒƒå›´å†…ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºæœªè¢«é®è”½çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºè¢«é®è”½çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨ `[0, 1]` èŒƒå›´å†…ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º `[0, config.max_position_embeddings -
    1]`ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„ç‰¹å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†
    *input_ids* ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨
    `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ
    `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾› `labels` æ—¶è¿”å›)
    â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE27]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: LayoutLMv2ForTokenClassification
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2ForTokenClassification
- en: '### `class transformers.LayoutLMv2ForTokenClassification`'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2ForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1126)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1126)'
- en: '[PRE28]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: LayoutLMv2 Model with a token classification head on top (a linear layer on
    top of the text part of the hidden states) e.g. for sequence labeling (information
    extraction) tasks such as [FUNSD](https://guillaumejaume.github.io/FUNSD/), [SROIE](https://rrc.cvc.uab.es/?ch=13),
    [CORD](https://github.com/clovaai/cord) and [Kleister-NDA](https://github.com/applicaai/kleister-nda).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨LayoutLMv2æ¨¡å‹çš„é¡¶éƒ¨å…·æœ‰æ ‡è®°åˆ†ç±»å¤´éƒ¨ï¼ˆéšè—çŠ¶æ€çš„æ–‡æœ¬éƒ¨åˆ†ä¸Šçš„çº¿æ€§å±‚ï¼‰çš„æ¨¡å‹ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—æ ‡è®°ï¼ˆä¿¡æ¯æå–ï¼‰ä»»åŠ¡çš„[FUNSD](https://guillaumejaume.github.io/FUNSD/)ã€[SROIE](https://rrc.cvc.uab.es/?ch=13)ã€[CORD](https://github.com/clovaai/cord)å’Œ[Kleister-NDA](https://github.com/applicaai/kleister-nda)ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1149)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1149)'
- en: '[PRE29]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) â€” Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_2d_position_embeddings-1]`ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯ä¸€ä¸ªè§„èŒƒåŒ–ç‰ˆæœ¬ï¼Œæ ¼å¼ä¸º(x0,
    y0, x1, y1)ï¼Œå…¶ä¸­(x0, y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1)è¡¨ç¤ºå³ä¸‹è§’çš„ä½ç½®ã€‚'
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) â€” Batch of document images.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`æˆ–`detectron.structures.ImageList`ï¼Œå…¶`tensors`å½¢çŠ¶ä¸º`(batch_size,
    num_channels, height, width)`ï¼‰â€” æ‰¹é‡æ–‡æ¡£å›¾åƒã€‚'
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`ï¼Œ*å¯é€‰*ï¼‰â€”
    æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 å¯¹åº”äº*å¥å­A*çš„æ ‡è®°ã€‚
- en: 1 corresponds to a *sentence B* token.
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äº*å¥å­B*çš„æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`batch_size, sequence_length`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0,
    config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`ä¹‹é—´ã€‚'
- en: Returns
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)æˆ–`tuple(torch.FloatTensor)`ã€‚'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification loss.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    â€” Classification scores (before SoftMax).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`ï¼‰â€”
    åˆ†ç±»åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºä¹‹ä¸€ï¼Œ+
    æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE30]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: LayoutLMv2ForQuestionAnswering
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutLMv2ForQuestionAnswering
- en: '### `class transformers.LayoutLMv2ForQuestionAnswering`'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutLMv2ForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1258)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1258)'
- en: '[PRE31]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: LayoutLMv2 Model with a span classification head on top for extractive question-answering
    tasks such as [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of
    the text part of the hidden-states output to compute `span start logits` and `span
    end logits`).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv2æ¨¡å‹ï¼Œåœ¨å…¶é¡¶éƒ¨å…·æœ‰ç”¨äºæå–é—®ç­”ä»»åŠ¡çš„è·¨åº¦åˆ†ç±»å¤´ï¼Œä¾‹å¦‚[DocVQA](https://rrc.cvc.uab.es/?ch=17)ï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„æ–‡æœ¬éƒ¨åˆ†é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼Œç”¨äºè®¡ç®—`è·¨åº¦èµ·å§‹å¯¹æ•°`å’Œ`è·¨åº¦ç»“æŸå¯¹æ•°`ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1280)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1280)'
- en: '[PRE32]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) â€” Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`batch_size, sequence_length`çš„`torch.LongTensor`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    â€” Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, 4)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„è¾¹ç•Œæ¡†ã€‚åœ¨èŒƒå›´`[0,
    config.max_2d_position_embeddings-1]`ä¸­é€‰æ‹©ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯(x0, y0, x1, y1)æ ¼å¼çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­(x0,
    y0)å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œè€Œ(x1, y1)è¡¨ç¤ºè¾¹ç•Œæ¡†å³ä¸‹è§’çš„ä½ç½®ã€‚'
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) â€” Batch of document images.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`æˆ–`detectron.structures.ImageList`ï¼Œå…¶`tensors`çš„å½¢çŠ¶ä¸º`(batch_size,
    num_channels, height, width)`ï¼‰ â€” æ–‡æ¡£å›¾åƒçš„æ‰¹å¤„ç†ã€‚'
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`batch_size, sequence_length`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`æ©ç `çš„æ ‡è®°ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`æ©ç `çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›è’™ç‰ˆï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`batch_size, sequence_length`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰
    â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*çš„æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `batch_size, sequence_length`ï¼Œ*å¯é€‰*)
    â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*)
    â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† *input_ids* ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    â€” Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦èµ·å§‹ä½ç½®çš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦
    (`sequence_length`)ã€‚è¶…å‡ºåºåˆ—èŒƒå›´çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€”
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦ç»“æŸä½ç½®çš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦
    (`sequence_length`)ã€‚è¶…å‡ºåºåˆ—èŒƒå›´çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚'
- en: Returns
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æŸå¤±` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” æ€»è·¨åº¦æŠ½å–æŸå¤±æ˜¯èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„äº¤å‰ç†µä¹‹å’Œã€‚'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Span-start scores (before SoftMax).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€”
    è·¨åº¦èµ·å§‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Span-end scores (before SoftMax).'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€” è·¨åº¦ç»“æŸåˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True`
    æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True`
    æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length,
    sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: In this example below, we give the LayoutLMv2 model an image (of texts) and
    ask it a question. It will give us a prediction of what it thinks the answer is
    (the span of the answer within the texts parsed from the image).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ç»™ LayoutLMv2 æ¨¡å‹ä¸€ä¸ªå›¾åƒï¼ˆåŒ…å«æ–‡æœ¬ï¼‰å¹¶å‘å…¶æé—®ã€‚å®ƒä¼šç»™å‡ºä¸€ä¸ªé¢„æµ‹ï¼Œå³å®ƒè®¤ä¸ºç­”æ¡ˆåœ¨ä»å›¾åƒä¸­è§£æçš„æ–‡æœ¬ä¸­çš„ä½ç½®ã€‚
- en: '[PRE33]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
