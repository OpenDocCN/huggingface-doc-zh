- en: LLM prompting guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMæç¤ºæŒ‡å—
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models such as Falcon, LLaMA, etc. are pretrained transformer
    models initially trained to predict the next token given some input text. They
    typically have billions of parameters and have been trained on trillions of tokens
    for an extended period of time. As a result, these models become quite powerful
    and versatile, and you can use them to solve multiple NLP tasks out of the box
    by instructing the models with natural language prompts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åƒFalconã€LLaMAç­‰å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯é¢„è®­ç»ƒçš„å˜å‹å™¨æ¨¡å‹ï¼Œæœ€åˆè®­ç»ƒç”¨äºé¢„æµ‹ç»™å®šä¸€äº›è¾“å…¥æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚å®ƒä»¬é€šå¸¸å…·æœ‰æ•°åäº¿ä¸ªå‚æ•°ï¼Œå¹¶ä¸”å·²ç»åœ¨é•¿æ—¶é—´å†…è®­ç»ƒäº†æ•°ä¸‡äº¿ä¸ªæ ‡è®°ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹å˜å¾—éå¸¸å¼ºå¤§å’Œå¤šåŠŸèƒ½ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç”¨è‡ªç„¶è¯­è¨€æç¤ºæŒ‡å¯¼æ¨¡å‹æ¥è§£å†³å¤šä¸ªNLPä»»åŠ¡ã€‚
- en: Designing such prompts to ensure the optimal output is often called â€œprompt
    engineeringâ€. Prompt engineering is an iterative process that requires a fair
    amount of experimentation. Natural languages are much more flexible and expressive
    than programming languages, however, they can also introduce some ambiguity. At
    the same time, prompts in natural language are quite sensitive to changes. Even
    minor modifications in prompts can lead to wildly different outputs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡è¿™æ ·çš„æç¤ºä»¥ç¡®ä¿æœ€ä½³è¾“å‡ºé€šå¸¸è¢«ç§°ä¸ºâ€œæç¤ºå·¥ç¨‹â€ã€‚æç¤ºå·¥ç¨‹æ˜¯ä¸€ä¸ªéœ€è¦å¤§é‡å®éªŒçš„è¿­ä»£è¿‡ç¨‹ã€‚è‡ªç„¶è¯­è¨€æ¯”ç¼–ç¨‹è¯­è¨€æ›´åŠ çµæ´»å’Œè¡¨è¾¾ä¸°å¯Œï¼Œä½†ä¹Ÿå¯èƒ½å¼•å…¥ä¸€äº›æ­§ä¹‰ã€‚åŒæ—¶ï¼Œè‡ªç„¶è¯­è¨€ä¸­çš„æç¤ºå¯¹å˜åŒ–éå¸¸æ•æ„Ÿã€‚å³ä½¿æç¤ºä¸­è¿›è¡Œè½»å¾®ä¿®æ”¹ä¹Ÿå¯èƒ½å¯¼è‡´æˆªç„¶ä¸åŒçš„è¾“å‡ºã€‚
- en: While there is no exact recipe for creating prompts to match all cases, researchers
    have worked out a number of best practices that help to achieve optimal results
    more consistently.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ²¡æœ‰ç¡®åˆ‡çš„é…æ–¹å¯ä»¥åˆ›å»ºé€‚ç”¨äºæ‰€æœ‰æƒ…å†µçš„æç¤ºï¼Œä½†ç ”ç©¶äººå‘˜å·²ç»åˆ¶å®šå‡ºä¸€äº›æœ€ä½³å®è·µï¼Œæœ‰åŠ©äºæ›´ä¸€è‡´åœ°å®ç°æœ€ä½³ç»“æœã€‚
- en: 'This guide covers the prompt engineering best practices to help you craft better
    LLM prompts and solve various NLP tasks. Youâ€™ll learn:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—æ¶µç›–äº†æç¤ºå·¥ç¨‹çš„æœ€ä½³å®è·µï¼Œä»¥å¸®åŠ©æ‚¨åˆ¶ä½œæ›´å¥½çš„LLMæç¤ºå¹¶è§£å†³å„ç§NLPä»»åŠ¡ã€‚æ‚¨å°†å­¦åˆ°ï¼š
- en: '[Basics of prompting](#basic-prompts)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æç¤ºçš„åŸºç¡€çŸ¥è¯†](#basic-prompts)'
- en: '[Best practices of LLM prompting](#best-practices-of-llm-prompting)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLMæç¤ºçš„æœ€ä½³å®è·µ](#best-practices-of-llm-prompting)'
- en: '[Advanced prompting techniques: few-shot prompting and chain-of-thought](#advanced-prompting-techniques)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é«˜çº§æç¤ºæŠ€æœ¯ï¼šå°‘æ ·æœ¬æç¤ºå’Œæ€ç»´é“¾](#advanced-prompting-techniques)'
- en: '[When to fine-tune instead of prompting](#prompting-vs-fine-tuning)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½•æ—¶è¿›è¡Œå¾®è°ƒè€Œä¸æ˜¯æç¤º](#prompting-vs-fine-tuning)'
- en: 'Prompt engineering is only a part of the LLM output optimization process. Another
    essential component is choosing the optimal text generation strategy. You can
    customize how your LLM selects each of the subsequent tokens when generating the
    text without modifying any of the trainable parameters. By tweaking the text generation
    parameters, you can reduce repetition in the generated text and make it more coherent
    and human-sounding. Text generation strategies and parameters are out of scope
    for this guide, but you can learn more about these topics in the following guides:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹ä»…æ˜¯LLMè¾“å‡ºä¼˜åŒ–è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚å¦ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†æ˜¯é€‰æ‹©æœ€ä½³çš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥ã€‚æ‚¨å¯ä»¥è‡ªå®šä¹‰LLMåœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å¦‚ä½•é€‰æ‹©æ¯ä¸ªåç»­æ ‡è®°ï¼Œè€Œæ— éœ€ä¿®æ”¹ä»»ä½•å¯è®­ç»ƒå‚æ•°ã€‚é€šè¿‡è°ƒæ•´æ–‡æœ¬ç”Ÿæˆå‚æ•°ï¼Œæ‚¨å¯ä»¥å‡å°‘ç”Ÿæˆæ–‡æœ¬ä¸­çš„é‡å¤ï¼Œå¹¶ä½¿å…¶æ›´è¿è´¯å’Œæ›´å…·äººç±»å£°éŸ³ã€‚æ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œå‚æ•°è¶…å‡ºäº†æœ¬æŒ‡å—çš„èŒƒå›´ï¼Œä½†æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹æŒ‡å—ä¸­äº†è§£æ›´å¤šç›¸å…³ä¸»é¢˜ï¼š
- en: '[Generation with LLMs](../llm_tutorial)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨LLMè¿›è¡Œç”Ÿæˆ](../llm_tutorial)'
- en: '[Text generation strategies](../generation_strategies)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies)'
- en: Basics of prompting
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤ºçš„åŸºç¡€çŸ¥è¯†
- en: Types of models
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å‹ç±»å‹
- en: 'The majority of modern LLMs are decoder-only transformers. Some examples include:
    [LLaMA](../model_doc/llama), [Llama2](../model_doc/llama2), [Falcon](../model_doc/falcon),
    [GPT2](../model_doc/gpt2). However, you may encounter encoder-decoder transformer
    LLMs as well, for instance, [Flan-T5](../model_doc/flan-t5) and [BART](../model_doc/bart).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£LLMå¤§å¤šæ•°æ˜¯ä»…è§£ç å™¨çš„å˜å‹å™¨ã€‚ä¸€äº›ä¾‹å­åŒ…æ‹¬ï¼š[LLaMA](../model_doc/llama), [Llama2](../model_doc/llama2),
    [Falcon](../model_doc/falcon), [GPT2](../model_doc/gpt2)ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯èƒ½é‡åˆ°ç¼–ç å™¨-è§£ç å™¨å˜å‹å™¨LLMï¼Œä¾‹å¦‚[Flan-T5](../model_doc/flan-t5)å’Œ[BART](../model_doc/bart)ã€‚
- en: Encoder-decoder-style models are typically used in generative tasks where the
    output **heavily** relies on the input, for example, in translation and summarization.
    The decoder-only models are used for all other types of generative tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨é£æ ¼çš„æ¨¡å‹é€šå¸¸ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œå…¶ä¸­è¾“å‡º**ä¸¥é‡**ä¾èµ–äºè¾“å…¥ï¼Œä¾‹å¦‚ç¿»è¯‘å’Œæ€»ç»“ã€‚è§£ç å™¨æ¨¡å‹ç”¨äºæ‰€æœ‰å…¶ä»–ç±»å‹çš„ç”Ÿæˆä»»åŠ¡ã€‚
- en: When using a pipeline to generate text with an LLM, itâ€™s important to know what
    type of LLM you are using, because they use different pipelines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ç®¡é“ç”ŸæˆLLMæ–‡æœ¬æ—¶ï¼Œäº†è§£æ‚¨æ­£åœ¨ä½¿ç”¨çš„LLMç±»å‹å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨ä¸åŒçš„ç®¡é“ã€‚
- en: 'Run inference with decoder-only models with the `text-generation` pipeline:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`text-generation`ç®¡é“è¿è¡Œä»…è§£ç å™¨æ¨¡å‹çš„æ¨ç†ï¼š
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To run inference with an encoder-decoder, use the `text2text-generation` pipeline:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¿›è¡Œæ¨ç†ï¼Œè¯·ä½¿ç”¨`text2text-generation`ç®¡é“ï¼š
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Base vs instruct/chat models
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºç¡€ç‰ˆ vs æŒ‡å¯¼/èŠå¤©ç‰ˆæ¨¡å‹
- en: 'Most of the recent LLM checkpoints available on ğŸ¤— Hub come in two versions:
    base and instruct (or chat). For example, [`tiiuae/falcon-7b`](https://huggingface.co/tiiuae/falcon-7b)
    and [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Hubä¸Šæä¾›çš„å¤§å¤šæ•°æœ€æ–°LLMæ£€æŸ¥ç‚¹éƒ½æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼šåŸºç¡€ç‰ˆå’ŒæŒ‡å¯¼ç‰ˆï¼ˆæˆ–èŠå¤©ç‰ˆï¼‰ã€‚ä¾‹å¦‚ï¼Œ[`tiiuae/falcon-7b`](https://huggingface.co/tiiuae/falcon-7b)
    å’Œ [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)ã€‚
- en: Base models are excellent at completing the text when given an initial prompt,
    however, they are not ideal for NLP tasks where they need to follow instructions,
    or for conversational use. This is where the instruct (chat) versions come in.
    These checkpoints are the result of further fine-tuning of the pre-trained base
    versions on instructions and conversational data. This additional fine-tuning
    makes them a better choice for many NLP tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡å‹åœ¨ç»™å®šåˆå§‹æç¤ºæ—¶å®Œæˆæ–‡æœ¬çš„èƒ½åŠ›éå¸¸å‡ºè‰²ï¼Œä½†æ˜¯å®ƒä»¬å¹¶ä¸é€‚åˆéœ€è¦éµå¾ªæŒ‡ä»¤æˆ–ç”¨äºå¯¹è¯çš„NLPä»»åŠ¡ã€‚è¿™å°±æ˜¯æŒ‡å¯¼ï¼ˆèŠå¤©ï¼‰ç‰ˆæœ¬çš„ç”¨æ­¦ä¹‹åœ°ã€‚è¿™äº›æ£€æŸ¥ç‚¹æ˜¯åœ¨é¢„è®­ç»ƒåŸºç¡€ç‰ˆæœ¬ä¸Šè¿›ä¸€æ­¥å¾®è°ƒæŒ‡ä»¤å’Œå¯¹è¯æ•°æ®çš„ç»“æœã€‚è¿™ç§é¢å¤–çš„å¾®è°ƒä½¿å®ƒä»¬æˆä¸ºè®¸å¤šNLPä»»åŠ¡çš„æ›´å¥½é€‰æ‹©ã€‚
- en: Letâ€™s illustrate some simple prompts that you can use with [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)
    to solve some common NLP tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸¾ä¾‹è¯´æ˜ä¸€äº›ç®€å•çš„æç¤ºï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)æ¥è§£å†³ä¸€äº›å¸¸è§çš„NLPä»»åŠ¡ã€‚
- en: NLP tasks
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡
- en: 'First, letâ€™s set up the environment:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®ç¯å¢ƒï¼š
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, letâ€™s load the model with the appropriate pipeline (`"text-generation"`):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨é€‚å½“çš„ç®¡é“ï¼ˆ`"text-generation"`ï¼‰åŠ è½½æ¨¡å‹ï¼š
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that Falcon models were trained using the `bfloat16` datatype, so we recommend
    you use the same. This requires a recent version of CUDA and works best on modern
    cards.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒFalconæ¨¡å‹æ˜¯ä½¿ç”¨`bfloat16`æ•°æ®ç±»å‹è®­ç»ƒçš„ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®æ‚¨ä¹Ÿä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹ã€‚è¿™éœ€è¦ä¸€ä¸ªæœ€æ–°ç‰ˆæœ¬çš„CUDAï¼Œå¹¶ä¸”åœ¨ç°ä»£æ˜¾å¡ä¸Šæ•ˆæœæœ€ä½³ã€‚
- en: Now that we have the model loaded via the pipeline, letâ€™s explore how you can
    use prompts to solve NLP tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡ç®¡é“åŠ è½½äº†æ¨¡å‹ï¼Œè®©æˆ‘ä»¬æ¢è®¨å¦‚ä½•ä½¿ç”¨æç¤ºæ¥è§£å†³NLPä»»åŠ¡ã€‚
- en: Text classification
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ†ç±»
- en: 'One of the most common forms of text classification is sentiment analysis,
    which assigns a label like â€œpositiveâ€, â€œnegativeâ€, or â€œneutralâ€ to a sequence
    of text. Letâ€™s write a prompt that instructs the model to classify a given text
    (a movie review). Weâ€™ll start by giving the instruction, and then specifying the
    text to classify. Note that instead of leaving it at that, weâ€™re also adding the
    beginning of the response - `"Sentiment: "`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ†ç±»ä¸­æœ€å¸¸è§çš„å½¢å¼ä¹‹ä¸€æ˜¯æƒ…æ„Ÿåˆ†æï¼Œå®ƒä¸ºä¸€æ®µæ–‡æœ¬åˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼Œæ¯”å¦‚â€œç§¯æâ€ã€â€œæ¶ˆæâ€æˆ–â€œä¸­æ€§â€ã€‚è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªæç¤ºï¼ŒæŒ‡ç¤ºæ¨¡å‹å¯¹ç»™å®šçš„æ–‡æœ¬ï¼ˆç”µå½±è¯„è®ºï¼‰è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å°†ä»ç»™å‡ºæŒ‡ä»¤å¼€å§‹ï¼Œç„¶åæŒ‡å®šè¦åˆ†ç±»çš„æ–‡æœ¬ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ä»…ä»…æ­¢æ­¥äºæ­¤ï¼Œè¿˜æ·»åŠ äº†å“åº”çš„å¼€å¤´
    - `"æƒ…æ„Ÿï¼š"`ï¼š
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As a result, the output contains a classification label from the list we have
    provided in the instructions, and it is a correct one!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¾“å‡ºåŒ…å«äº†æˆ‘ä»¬åœ¨æŒ‡ä»¤ä¸­æä¾›çš„åˆ—è¡¨ä¸­çš„ä¸€ä¸ªåˆ†ç±»æ ‡ç­¾ï¼Œè€Œä¸”æ˜¯æ­£ç¡®çš„ï¼
- en: You may notice that in addition to the prompt, we pass a `max_new_tokens` parameter.
    It controls the number of tokens the model shall generate, and it is one of the
    many text generation parameters that you can learn about in [Text generation strategies](../generation_strategies)
    guide.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½æ³¨æ„åˆ°ï¼Œé™¤äº†æç¤ºä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä¼ é€’äº†ä¸€ä¸ª`max_new_tokens`å‚æ•°ã€‚å®ƒæ§åˆ¶æ¨¡å‹åº”è¯¥ç”Ÿæˆçš„æ ‡è®°æ•°é‡ï¼Œè¿™æ˜¯æ‚¨å¯ä»¥åœ¨[æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies)æŒ‡å—ä¸­äº†è§£çš„è®¸å¤šæ–‡æœ¬ç”Ÿæˆå‚æ•°ä¹‹ä¸€ã€‚
- en: Named Entity Recognition
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å‘½åå®ä½“è¯†åˆ«
- en: 'Named Entity Recognition (NER) is a task of finding named entities in a piece
    of text, such as a person, location, or organization. Letâ€™s modify the instructions
    in the prompt to make the LLM perform this task. Here, letâ€™s also set `return_full_text
    = False` so that output doesnâ€™t contain the prompt:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ˜¯åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°å‘½åå®ä½“çš„ä»»åŠ¡ï¼Œæ¯”å¦‚äººç‰©ã€åœ°ç‚¹æˆ–ç»„ç»‡ã€‚è®©æˆ‘ä»¬ä¿®æ”¹æç¤ºä¸­çš„æŒ‡ä»¤ï¼Œè®©LLMæ‰§è¡Œè¿™ä¸ªä»»åŠ¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜è®¾ç½®`return_full_text
    = False`ï¼Œè¿™æ ·è¾“å‡ºå°±ä¸åŒ…å«æç¤ºäº†ï¼š
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the model correctly identified two named entities from the given
    text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæ¨¡å‹æ­£ç¡®è¯†åˆ«äº†ç»™å®šæ–‡æœ¬ä¸­çš„ä¸¤ä¸ªå‘½åå®ä½“ã€‚
- en: Translation
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¿»è¯‘
- en: 'Another task LLMs can perform is translation. You can choose to use encoder-decoder
    models for this task, however, here, for the simplicity of the examples, weâ€™ll
    keep using Falcon-7b-instruct, which does a decent job. Once again, hereâ€™s how
    you can write a basic prompt to instruct a model to translate a piece of text
    from English to Italian:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LLMå¯ä»¥æ‰§è¡Œçš„å¦ä¸€ä¸ªä»»åŠ¡æ˜¯ç¿»è¯‘ã€‚æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ¥æ‰§è¡Œæ­¤ä»»åŠ¡ï¼Œä½†æ˜¯åœ¨è¿™é‡Œï¼Œä¸ºäº†ç®€åŒ–ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨Falcon-7b-instructï¼Œå®ƒåšå¾—ç›¸å½“ä¸é”™ã€‚å†æ¬¡ï¼Œè¿™æ˜¯æ‚¨å¦‚ä½•ç¼–å†™ä¸€ä¸ªåŸºæœ¬æç¤ºï¼ŒæŒ‡ç¤ºæ¨¡å‹å°†ä¸€æ®µæ–‡æœ¬ä»è‹±è¯­ç¿»è¯‘æˆæ„å¤§åˆ©è¯­ï¼š
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here weâ€™ve added a `do_sample=True` and `top_k=10` to allow the model to be
    a bit more flexible when generating output.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ·»åŠ äº†`do_sample=True`å’Œ`top_k=10`ï¼Œä»¥å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºæ—¶æ›´åŠ çµæ´»ã€‚
- en: Text summarization
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ–‡æœ¬æ‘˜è¦
- en: Similar to the translation, text summarization is another generative task where
    the output **heavily** relies on the input, and encoder-decoder models can be
    a better choice. However, decoder-style models can be used for this task as well.
    Previously, we have placed the instructions at the very beginning of the prompt.
    However, the very end of the prompt can also be a suitable location for instructions.
    Typically, itâ€™s better to place the instruction on one of the extreme ends.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç¿»è¯‘ç±»ä¼¼ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯å¦ä¸€ä¸ªç”Ÿæˆä»»åŠ¡ï¼Œè¾“å‡º**ä¸¥é‡**ä¾èµ–äºè¾“å…¥ï¼Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å¯èƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œè§£ç å™¨é£æ ¼çš„æ¨¡å‹ä¹Ÿå¯ä»¥ç”¨äºè¿™ä¸ªä»»åŠ¡ã€‚ä»¥å‰ï¼Œæˆ‘ä»¬å°†æŒ‡ä»¤æ”¾åœ¨æç¤ºçš„å¼€å¤´ã€‚ç„¶è€Œï¼Œæç¤ºçš„æœ€åä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªåˆé€‚çš„ä½ç½®æ¥æ”¾ç½®æŒ‡ä»¤ã€‚é€šå¸¸ï¼Œæœ€å¥½å°†æŒ‡ä»¤æ”¾åœ¨ä¸¤ç«¯ä¹‹ä¸€ã€‚
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Question answering
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é—®ç­”
- en: 'For question answering task we can structure the prompt into the following
    logical components: instructions, context, question, and the leading word or phrase
    (`"Answer:"`) to nudge the model to start generating the answer:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥å°†æç¤ºç»“æ„åŒ–ä¸ºä»¥ä¸‹é€»è¾‘ç»„ä»¶ï¼šæŒ‡ä»¤ã€ä¸Šä¸‹æ–‡ã€é—®é¢˜å’Œå¼•å¯¼è¯æˆ–çŸ­è¯­ï¼ˆ`"Answer:"`ï¼‰ï¼Œä»¥ä¿ƒä½¿æ¨¡å‹å¼€å§‹ç”Ÿæˆç­”æ¡ˆï¼š
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Reasoning
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Reasoning is one of the most difficult tasks for LLMs, and achieving good results
    often requires applying advanced prompting techniques, like [Chain-of-though](#chain-of-thought).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†æ˜¯LLMä¸­æœ€å›°éš¾çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œè¦å–å¾—è‰¯å¥½çš„ç»“æœé€šå¸¸éœ€è¦åº”ç”¨é«˜çº§æç¤ºæŠ€æœ¯ï¼Œæ¯”å¦‚[æ€ç»´é“¾](#chain-of-thought)ã€‚
- en: 'Letâ€™s try if we can make a model reason about a simple arithmetics task with
    a basic prompt:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥è®©æ¨¡å‹é€šè¿‡ä¸€ä¸ªåŸºæœ¬æç¤ºæ¥æ¨ç†ä¸€ä¸ªç®€å•çš„ç®—æœ¯ä»»åŠ¡ï¼š
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Correct! Letâ€™s increase the complexity a little and see if we can still get
    away with a basic prompt:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®ï¼è®©æˆ‘ä»¬ç¨å¾®å¢åŠ ä¸€ç‚¹å¤æ‚æ€§ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦ä»ç„¶å¯ä»¥é€šè¿‡ä¸€ä¸ªåŸºæœ¬æç¤ºæ¥å®Œæˆï¼š
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is a wrong answer, it should be 12\. In this case, this can be due to the
    prompt being too basic, or due to the choice of model, after all weâ€™ve picked
    the smallest version of Falcon. Reasoning is difficult for models of all sizes,
    but larger models are likely to perform better.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé”™è¯¯ç­”æ¡ˆï¼Œåº”è¯¥æ˜¯12ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæç¤ºè¿‡äºåŸºç¡€ï¼Œæˆ–è€…æ˜¯å› ä¸ºæ¨¡å‹é€‰æ‹©ä¸å½“ï¼Œæ¯•ç«Ÿæˆ‘ä»¬é€‰æ‹©äº†Falconçš„æœ€å°ç‰ˆæœ¬ã€‚å¯¹äºæ‰€æœ‰å¤§å°çš„æ¨¡å‹æ¥è¯´ï¼Œæ¨ç†éƒ½æ˜¯å›°éš¾çš„ï¼Œä½†æ›´å¤§çš„æ¨¡å‹å¯èƒ½è¡¨ç°æ›´å¥½ã€‚
- en: Best practices of LLM prompting
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMæç¤ºçš„æœ€ä½³å®è·µ
- en: 'In this section of the guide we have compiled a list of best practices that
    tend to improve the prompt results:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—çš„è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬ç¼–åˆ¶äº†ä¸€ä»½å€¾å‘äºæ”¹å–„æç¤ºç»“æœçš„æœ€ä½³å®è·µæ¸…å•ï¼š
- en: When choosing the model to work with, the latest and most capable models are
    likely to perform better.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹æ—¶ï¼Œæœ€æ–°å’Œæœ€æœ‰èƒ½åŠ›çš„æ¨¡å‹å¯èƒ½è¡¨ç°æ›´å¥½ã€‚
- en: Start with a simple and short prompt, and iterate from there.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»ä¸€ä¸ªç®€å•è€ŒçŸ­çš„æç¤ºå¼€å§‹ï¼Œç„¶åé€æ­¥è¿­ä»£ã€‚
- en: Put the instructions at the beginning of the prompt, or at the very end. When
    working with large context, models apply various optimizations to prevent Attention
    complexity from scaling quadratically. This may make a model more attentive to
    the beginning or end of a prompt than the middle.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æŒ‡ä»¤æ”¾åœ¨æç¤ºçš„å¼€å¤´æˆ–æœ€åã€‚åœ¨å¤„ç†å¤§é‡ä¸Šä¸‹æ–‡æ—¶ï¼Œæ¨¡å‹ä¼šåº”ç”¨å„ç§ä¼˜åŒ–æªæ–½ï¼Œä»¥é˜²æ­¢æ³¨æ„åŠ›å¤æ‚åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚è¿™å¯èƒ½ä¼šä½¿æ¨¡å‹æ›´åŠ å…³æ³¨æç¤ºçš„å¼€å¤´æˆ–ç»“å°¾ï¼Œè€Œä¸æ˜¯ä¸­é—´éƒ¨åˆ†ã€‚
- en: Clearly separate instructions from the text they apply to - more on this in
    the next section.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æŒ‡ä»¤ä¸å…¶é€‚ç”¨çš„æ–‡æœ¬æ¸…æ™°åˆ†å¼€-æ›´å¤šå†…å®¹è¯·å‚è§ä¸‹ä¸€èŠ‚ã€‚
- en: Be specific and descriptive about the task and the desired outcome - its format,
    length, style, language, etc.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ä»»åŠ¡å’ŒæœŸæœ›ç»“æœè¿›è¡Œå…·ä½“å’Œæè¿°æ€§çš„è¯´æ˜-å…¶æ ¼å¼ã€é•¿åº¦ã€é£æ ¼ã€è¯­è¨€ç­‰ã€‚
- en: Avoid ambiguous descriptions and instructions.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¿å…æ¨¡æ£±ä¸¤å¯çš„æè¿°å’ŒæŒ‡ä»¤ã€‚
- en: Favor instructions that say â€œwhat to doâ€ instead of those that say â€œwhat not
    to doâ€.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´å€¾å‘äºè¯´â€œè¦åšä»€ä¹ˆâ€è€Œä¸æ˜¯è¯´â€œä¸è¦åšä»€ä¹ˆâ€çš„æŒ‡ä»¤ã€‚
- en: â€œLeadâ€ the output in the right direction by writing the first word (or even
    begin the first sentence for the model).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ç¼–å†™ç¬¬ä¸€ä¸ªå•è¯ï¼ˆç”šè‡³å¼€å§‹ç¬¬ä¸€ä¸ªå¥å­ï¼‰æ¥â€œå¼•å¯¼â€è¾“å‡ºæœç€æ­£ç¡®æ–¹å‘å‘å±•ã€‚
- en: Use advanced techniques like [Few-shot prompting](#few-shot-prompting) and [Chain-of-thought](#chain-of-thought)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é«˜çº§æŠ€æœ¯ï¼Œå¦‚[å°‘æ ·æœ¬æç¤º](#å°‘æ ·æœ¬æç¤º)å’Œ[æ€ç»´é“¾](#æ€ç»´é“¾)
- en: Test your prompts with different models to assess their robustness.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸åŒæ¨¡å‹æµ‹è¯•æ‚¨çš„æç¤ºï¼Œä»¥è¯„ä¼°å…¶ç¨³å¥æ€§ã€‚
- en: Version and track the performance of your prompts.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰ˆæœ¬å’Œè·Ÿè¸ªæç¤ºçš„æ€§èƒ½ã€‚
- en: Advanced prompting techniques
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜çº§æç¤ºæŠ€æœ¯
- en: Few-shot prompting
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°‘æ ·æœ¬æç¤º
- en: The basic prompts in the sections above are the examples of â€œzero-shotâ€ prompts,
    meaning, the model has been given instructions and context, but no examples with
    solutions. LLMs that have been fine-tuned on instruction datasets, generally perform
    well on such â€œzero-shotâ€ tasks. However, you may find that your task has more
    complexity or nuance, and, perhaps, you have some requirements for the output
    that the model doesnâ€™t catch on just from the instructions. In this case, you
    can try the technique called few-shot prompting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°éƒ¨åˆ†çš„åŸºæœ¬æç¤ºæ˜¯â€œé›¶æ ·æœ¬â€æç¤ºçš„ç¤ºä¾‹ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å·²ç»è·å¾—äº†æŒ‡ä»¤å’Œä¸Šä¸‹æ–‡ï¼Œä½†æ²¡æœ‰å¸¦æœ‰è§£å†³æ–¹æ¡ˆçš„ç¤ºä¾‹ã€‚é€šå¸¸åœ¨æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„LLMåœ¨è¿™ç§â€œé›¶æ ·æœ¬â€ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°æ‚¨çš„ä»»åŠ¡æ›´åŠ å¤æ‚æˆ–å¾®å¦™ï¼Œä¹Ÿè®¸æ‚¨å¯¹æ¨¡å‹æ²¡æœ‰ä»æŒ‡ä»¤ä¸­æ•æ‰åˆ°çš„è¾“å‡ºæœ‰ä¸€äº›è¦æ±‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥å°è¯•ç§°ä¸ºå°‘æ ·æœ¬æç¤ºçš„æŠ€æœ¯ã€‚
- en: In few-shot prompting, we provide examples in the prompt giving the model more
    context to improve the performance. The examples condition the model to generate
    the output following the patterns in the examples.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°‘æ ·æœ¬æç¤ºä¸­ï¼Œæˆ‘ä»¬åœ¨æç¤ºä¸­æä¾›ç¤ºä¾‹ï¼Œä¸ºæ¨¡å‹æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä»¥æé«˜æ€§èƒ½ã€‚è¿™äº›ç¤ºä¾‹ä¼šè®©æ¨¡å‹ç”Ÿæˆéµå¾ªç¤ºä¾‹æ¨¡å¼çš„è¾“å‡ºã€‚
- en: 'Hereâ€™s an example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the above code snippet we used a single example to demonstrate the desired
    output to the model, so this can be called a â€œone-shotâ€ prompting. However, depending
    on the task complexity you may need to use more than one example.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç¤ºä¾‹æ¥å‘æ¨¡å‹å±•ç¤ºæ‰€éœ€çš„è¾“å‡ºï¼Œå› æ­¤è¿™å¯ä»¥ç§°ä¸ºâ€œä¸€æ¬¡æ€§â€æç¤ºã€‚ç„¶è€Œï¼Œæ ¹æ®ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œæ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨å¤šä¸ªç¤ºä¾‹ã€‚
- en: 'Limitations of the few-shot prompting technique:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å°‘æ ·æœ¬æç¤ºæŠ€æœ¯çš„å±€é™æ€§ï¼š
- en: While LLMs can pick up on the patterns in the examples, these technique doesnâ€™t
    work well on complex reasoning tasks
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è™½ç„¶LLMå¯ä»¥æ•æ‰åˆ°ç¤ºä¾‹ä¸­çš„æ¨¡å¼ï¼Œä½†è¿™äº›æŠ€æœ¯åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šæ•ˆæœä¸ä½³
- en: Few-shot prompting requires creating lengthy prompts. Prompts with large number
    of tokens can increase computation and latency. Thereâ€™s also a limit to the length
    of the prompts.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°‘æ ·æœ¬æç¤ºéœ€è¦åˆ›å»ºè¾ƒé•¿çš„æç¤ºã€‚å…·æœ‰å¤§é‡æ ‡è®°çš„æç¤ºå¯èƒ½ä¼šå¢åŠ è®¡ç®—å’Œå»¶è¿Ÿã€‚æç¤ºçš„é•¿åº¦ä¹Ÿæœ‰é™åˆ¶ã€‚
- en: Sometimes when given a number of examples, models can learn patterns that you
    didnâ€™t intend them to learn, e.g. that the third movie review is always negative.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œå½“ç»™å®šå¤šä¸ªç¤ºä¾‹æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ æ‚¨å¹¶éæ‰“ç®—è®©å®ƒå­¦ä¹ çš„æ¨¡å¼ï¼Œä¾‹å¦‚ç¬¬ä¸‰ä¸ªç”µå½±è¯„è®ºæ€»æ˜¯è´Ÿé¢çš„ã€‚
- en: Chain-of-thought
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ€ç»´é“¾
- en: Chain-of-thought (CoT) prompting is a technique that nudges a model to produce
    intermediate reasoning steps thus improving the results on complex reasoning tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒä¿ƒä½¿æ¨¡å‹äº§ç”Ÿä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜å¤æ‚æ¨ç†ä»»åŠ¡çš„ç»“æœã€‚
- en: 'There are two ways of steering a model to producing the reasoning steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥å¼•å¯¼æ¨¡å‹äº§ç”Ÿæ¨ç†æ­¥éª¤ï¼š
- en: few-shot prompting by illustrating examples with detailed answers to questions,
    showing the model how to work through a problem.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ç”¨è¯¦ç»†ç­”æ¡ˆè¯´æ˜ç¤ºä¾‹æ¥è¿›è¡Œå°‘æ ·æœ¬æç¤ºï¼Œå‘æ¨¡å‹å±•ç¤ºå¦‚ä½•è§£å†³é—®é¢˜ã€‚
- en: by instructing the model to reason by adding phrases like â€œLetâ€™s think step
    by stepâ€ or â€œTake a deep breath and work through the problem step by step.â€
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æ·»åŠ çŸ­è¯­ï¼Œå¦‚â€œè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°æ€è€ƒâ€æˆ–â€œæ·±å‘¼å¸ï¼Œä¸€æ­¥ä¸€æ­¥åœ°è§£å†³é—®é¢˜â€ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
- en: 'If we apply the CoT technique to the muffins example from the [reasoning section](#reasoning)
    and use a larger model, such as (`tiiuae/falcon-180B-chat`) which you can play
    with in the [HuggingChat](https://huggingface.co/chat/), weâ€™ll get a significant
    improvement on the reasoning result:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†CoTæŠ€æœ¯åº”ç”¨äº[æ¨ç†éƒ¨åˆ†](#æ¨ç†)ä¸­çš„æ¾é¥¼ç¤ºä¾‹ï¼Œå¹¶ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼Œä¾‹å¦‚ï¼ˆ`tiiuae/falcon-180B-chat`ï¼‰ï¼Œæ‚¨å¯ä»¥åœ¨[HuggingChat](https://huggingface.co/chat/)ä¸­å°è¯•ï¼Œæˆ‘ä»¬å°†åœ¨æ¨ç†ç»“æœä¸Šè·å¾—æ˜¾è‘—çš„æ”¹è¿›ï¼š
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Prompting vs fine-tuning
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤º vs å¾®è°ƒ
- en: 'You can achieve great results by optimizing your prompts, however, you may
    still ponder whether fine-tuning a model would work better for your case. Here
    are some scenarios when fine-tuning a smaller model may be a preferred option:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¼˜åŒ–æ‚¨çš„æç¤ºï¼Œæ‚¨å¯ä»¥å–å¾—å‡ºè‰²çš„ç»“æœï¼Œä½†æ˜¯æ‚¨å¯èƒ½ä»ç„¶åœ¨è€ƒè™‘æ˜¯å¦å¾®è°ƒæ¨¡å‹å¯¹æ‚¨çš„æƒ…å†µæ›´æœ‰æ•ˆã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¾®è°ƒè¾ƒå°æ¨¡å‹å¯èƒ½æ˜¯é¦–é€‰çš„æƒ…å†µï¼š
- en: Your domain is wildly different from what LLMs were pre-trained on and extensive
    prompt optimization did not yield sufficient results.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨çš„é¢†åŸŸä¸LLMsé¢„å…ˆè®­ç»ƒçš„é¢†åŸŸå¤§ç›¸å¾„åº­ï¼Œå¹¿æ³›çš„æç¤ºä¼˜åŒ–å¹¶æœªäº§ç”Ÿè¶³å¤Ÿçš„ç»“æœã€‚
- en: You need your model to work well in a low-resource language.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦æ‚¨çš„æ¨¡å‹åœ¨èµ„æºç¨€ç¼ºçš„è¯­è¨€ä¸­è¡¨ç°è‰¯å¥½ã€‚
- en: You need the model to be trained on sensitive data that is under strict regulations.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦è®­ç»ƒæ¨¡å‹çš„æ•°æ®æ˜¯å—ä¸¥æ ¼ç›‘ç®¡çš„æ•æ„Ÿæ•°æ®ã€‚
- en: You have to use a small model due to cost, privacy, infrastructure or other
    limitations.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºæˆæœ¬ã€éšç§ã€åŸºç¡€è®¾æ–½æˆ–å…¶ä»–é™åˆ¶ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨å°å‹æ¨¡å‹ã€‚
- en: In all of the above examples, you will need to make sure that you either already
    have or can easily obtain a large enough domain-specific dataset at a reasonable
    cost to fine-tune a model. You will also need to have enough time and resources
    to fine-tune a model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°æ‰€æœ‰ç¤ºä¾‹ä¸­ï¼Œæ‚¨éœ€è¦ç¡®ä¿æ‚¨å·²ç»æ‹¥æœ‰æˆ–å¯ä»¥è½»æ¾è·å¾—è¶³å¤Ÿå¤§çš„é¢†åŸŸç‰¹å®šæ•°æ®é›†ï¼Œä»¥åˆç†çš„æˆæœ¬æ¥å¾®è°ƒæ¨¡å‹ã€‚æ‚¨è¿˜éœ€è¦æœ‰è¶³å¤Ÿçš„æ—¶é—´å’Œèµ„æºæ¥å¾®è°ƒæ¨¡å‹ã€‚
- en: If the above examples are not the case for you, optimizing prompts can prove
    to be more beneficial.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸Šè¿°ç¤ºä¾‹ä¸é€‚ç”¨äºæ‚¨ï¼Œä¼˜åŒ–æç¤ºå¯èƒ½ä¼šæ›´æœ‰ç›Šã€‚
