- en: Learning Tools (Experimental ğŸ§ª)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å­¦ä¹ å·¥å…·ï¼ˆå®éªŒæ€§ ğŸ§ªï¼‰
- en: 'Original text: [https://huggingface.co/docs/trl/learning_tools](https://huggingface.co/docs/trl/learning_tools)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/trl/learning_tools](https://huggingface.co/docs/trl/learning_tools)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Using Large Language Models (LLMs) with tools has been a popular topic recently
    with awesome works such as [ToolFormer](https://arxiv.org/abs/2302.04761) and
    [ToolBench](https://arxiv.org/pdf/2305.16504.pdf). In TRL, we provide a simple
    example of how to teach LLM to use tools with reinforcement learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å·¥å…·çš„è¯é¢˜å¤‡å—å…³æ³¨ï¼Œå¦‚[ToolFormer](https://arxiv.org/abs/2302.04761)å’Œ[ToolBench](https://arxiv.org/pdf/2305.16504.pdf)ç­‰å‡ºè‰²çš„ä½œå“ã€‚åœ¨TRLä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ•™æˆLLMä½¿ç”¨å·¥å…·ã€‚
- en: 'Hereâ€™s an overview of the scripts in the [trl repository](https://github.com/lvwerra/trl/tree/main/examples/research_projects/tools):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯[trlå­˜å‚¨åº“](https://github.com/lvwerra/trl/tree/main/examples/research_projects/tools)ä¸­è„šæœ¬çš„æ¦‚è¿°ï¼š
- en: '| File | Description |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡ä»¶ | æè¿° |'
- en: '| --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [`calculator.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/calculator.py)
    | Script to train LLM to use a calculator with reinforcement learning. |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [`calculator.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/calculator.py)
    | ç”¨äºè®­ç»ƒLLMä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä½¿ç”¨è®¡ç®—å™¨çš„è„šæœ¬ã€‚ |'
- en: '| [`triviaqa.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/triviaqa.py)
    | Script to train LLM to use a wiki tool to answer questions. |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| [`triviaqa.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/triviaqa.py)
    | ç”¨äºè®­ç»ƒLLMä½¿ç”¨ç»´åŸºå·¥å…·å›ç­”é—®é¢˜çš„è„šæœ¬ã€‚ |'
- en: '| [`python_interpreter.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/python_interpreter.py)
    | Script to train LLM to use python interpreter to solve math puzzles. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| [`python_interpreter.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/python_interpreter.py)
    | ç”¨äºè®­ç»ƒLLMä½¿ç”¨Pythonè§£é‡Šå™¨è§£å†³æ•°å­¦éš¾é¢˜çš„è„šæœ¬ã€‚ |'
- en: Note that the scripts above rely heavily on the `TextEnvironment` API which
    is still under active development. The API may change in the future. Please see
    [`TextEnvironment`](text_environment) for the related docs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸Šè¿°è„šæœ¬ä¸¥é‡ä¾èµ–äºä»åœ¨ç§¯æå¼€å‘ä¸­çš„`TextEnvironment` APIã€‚APIå¯èƒ½ä¼šåœ¨æœªæ¥å‘ç”Ÿå˜åŒ–ã€‚è¯·å‚é˜…[`TextEnvironment`](text_environment)ä»¥è·å–ç›¸å…³æ–‡æ¡£ã€‚
- en: Learning to Use a Calculator
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ ä½¿ç”¨è®¡ç®—å™¨
- en: 'The rough idea is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è‡´æƒ³æ³•å¦‚ä¸‹ï¼š
- en: 'Load a tool such as [ybelkada/simple-calculator](https://huggingface.co/spaces/ybelkada/simple-calculator)
    that parse a text calculation like `"14 + 34"` and return the calulated number:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€ä¸ªå·¥å…·ï¼Œæ¯”å¦‚[ybelkada/simple-calculator](https://huggingface.co/spaces/ybelkada/simple-calculator)ï¼Œè§£ææ–‡æœ¬è®¡ç®—å¦‚`"14
    + 34"`å¹¶è¿”å›è®¡ç®—åçš„æ•°å­—ï¼š
- en: '[PRE0]'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define a reward function that returns a positive reward if the tool returns
    the correct answer. In the script we create a dummy reward function like `reward_fn
    = lambda x: 1`, but we override the rewards directly later.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'å®šä¹‰ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œå¦‚æœå·¥å…·è¿”å›æ­£ç¡®ç­”æ¡ˆåˆ™è¿”å›æ­£å¥–åŠ±ã€‚åœ¨è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè™šæ‹Ÿçš„å¥–åŠ±å‡½æ•°ï¼Œå¦‚`reward_fn = lambda x: 1`ï¼Œä½†æˆ‘ä»¬ç¨åç›´æ¥è¦†ç›–äº†å¥–åŠ±ã€‚'
- en: Create a prompt on how to use the tools
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºå¦‚ä½•ä½¿ç”¨å·¥å…·çš„æç¤º
- en: '[PRE1]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Create a `trl.TextEnvironment` with the model
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¨¡å‹åˆ›å»ºä¸€ä¸ª`trl.TextEnvironment`
- en: '[PRE2]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then generate some data such as `tasks = ["\n\nWhat is 13.1-3?", "\n\nWhat is
    4*3?"]` and run the environment with `queries, responses, masks, rewards, histories
    = env.run(tasks)`. The environment will look for the `<call>` token in the prompt
    and append the tool output to the response; it will also return the mask associated
    with the response. You can further use the `histories` to visualize the interaction
    between the model and the tool; `histories[0].show_text()` will show the text
    with color-coded tool output and `histories[0].show_tokens(tokenizer)` will show
    visualize the tokens. ![](../Images/e9b6e7b4e250b332fcbd9626573e3f25.png)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åç”Ÿæˆä¸€äº›æ•°æ®ï¼Œå¦‚`tasks = ["\n\nWhat is 13.1-3?", "\n\nWhat is 4*3?"]`ï¼Œå¹¶ä½¿ç”¨`queries,
    responses, masks, rewards, histories = env.run(tasks)`è¿è¡Œç¯å¢ƒã€‚ç¯å¢ƒå°†æŸ¥æ‰¾æç¤ºä¸­çš„`<call>`æ ‡è®°ï¼Œå¹¶å°†å·¥å…·è¾“å‡ºé™„åŠ åˆ°å“åº”ä¸­ï¼›å®ƒè¿˜å°†è¿”å›ä¸å“åº”ç›¸å…³çš„æ©ç ã€‚æ‚¨å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨`histories`æ¥å¯è§†åŒ–æ¨¡å‹å’Œå·¥å…·ä¹‹é—´çš„äº¤äº’ï¼›`histories[0].show_text()`å°†æ˜¾ç¤ºå¸¦æœ‰é¢œè‰²ç¼–ç å·¥å…·è¾“å‡ºçš„æ–‡æœ¬ï¼Œ`histories[0].show_tokens(tokenizer)`å°†æ˜¾ç¤ºå¯è§†åŒ–çš„æ ‡è®°ã€‚![](../Images/e9b6e7b4e250b332fcbd9626573e3f25.png)
- en: Finally, we can train the model with `train_stats = ppo_trainer.step(queries,
    responses, rewards, masks)`. The trainer will use the mask to ignore the tool
    output when computing the loss, make sure to pass that argument to `step`.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`train_stats = ppo_trainer.step(queries, responses, rewards, masks)`æ¥è®­ç»ƒæ¨¡å‹ã€‚è®­ç»ƒå™¨å°†ä½¿ç”¨æ©ç åœ¨è®¡ç®—æŸå¤±æ—¶å¿½ç•¥å·¥å…·è¾“å‡ºï¼Œè¯·ç¡®ä¿å°†è¯¥å‚æ•°ä¼ é€’ç»™`step`ã€‚
- en: Experiment results
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®éªŒç»“æœ
- en: We trained a model with the above script for 10 random seeds. You can reproduce
    the run with the following command. Feel free to remove the `--slurm-*` arguments
    if you donâ€™t have access to a slurm cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°è„šæœ¬å¯¹æ¨¡å‹è¿›è¡Œäº†10ä¸ªéšæœºç§å­çš„è®­ç»ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é‡ç°è¿è¡Œã€‚å¦‚æœæ‚¨æ— æ³•è®¿é—®slurmé›†ç¾¤ï¼Œè¯·éšæ—¶åˆ é™¤`--slurm-*`å‚æ•°ã€‚
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can then use [`openrlbenchmark`](https://github.com/openrlbenchmark/openrlbenchmark)
    which generates the following plot.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[`openrlbenchmark`](https://github.com/openrlbenchmark/openrlbenchmark)ç”Ÿæˆä»¥ä¸‹å›¾è¡¨ã€‚
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/6cbe96fa603fe61845da0a4c0fbf4641.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cbe96fa603fe61845da0a4c0fbf4641.png)'
- en: As we can see, while 1-2 experiments crashed for some reason, most of the runs
    obtained near perfect proficiency in the calculator task.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè™½ç„¶æœ‰1-2æ¬¡å®éªŒç”±äºæŸç§åŸå› å´©æºƒï¼Œä½†å¤§å¤šæ•°è¿è¡Œåœ¨è®¡ç®—å™¨ä»»åŠ¡ä¸­è·å¾—äº†æ¥è¿‘å®Œç¾çš„ç†Ÿç»ƒåº¦ã€‚
- en: '(Early Experiments ğŸ§ª): learning to use a wiki tool for question answering'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ï¼ˆæ—©æœŸå®éªŒ ğŸ§ªï¼‰ï¼šå­¦ä¹ ä½¿ç”¨ç»´åŸºå·¥å…·å›ç­”é—®é¢˜
- en: In the [ToolFormer](https://arxiv.org/abs/2302.04761) paper, it shows an interesting
    use case that utilizes a Wikipedia Search tool to help answer questions. In this
    section, we attempt to perform similar experiments but uses RL instead to teach
    the model to use a wiki tool on the [TriviaQA](https://nlp.cs.washington.edu/triviaqa/)
    dataset.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ToolFormer](https://arxiv.org/abs/2302.04761)è®ºæ–‡ä¸­ï¼Œå±•ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„ç”¨ä¾‹ï¼Œåˆ©ç”¨ç»´åŸºç™¾ç§‘æœç´¢å·¥å…·æ¥å¸®åŠ©å›ç­”é—®é¢˜ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°è¯•è¿›è¡Œç±»ä¼¼çš„å®éªŒï¼Œä½†æ˜¯ä½¿ç”¨RLæ¥æ•™æˆæ¨¡å‹å¦‚ä½•åœ¨[TriviaQA](https://nlp.cs.washington.edu/triviaqa/)æ•°æ®é›†ä¸Šä½¿ç”¨ç»´åŸºå·¥å…·ã€‚
- en: '**Note that many settings are different so the results are not directly comparable.**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯·æ³¨æ„ï¼Œè®¸å¤šè®¾ç½®ä¸åŒï¼Œå› æ­¤ç»“æœä¸èƒ½ç›´æ¥è¿›è¡Œæ¯”è¾ƒã€‚**'
- en: Building a search index
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ„å»ºæœç´¢ç´¢å¼•
- en: Since [ToolFormer](https://arxiv.org/abs/2302.04761) did not open source, we
    needed to first replicate the search index. It is mentioned in their paper that
    the authors built the search index using a BM25 retriever that indexes the Wikipedia
    dump from [KILT](https://github.com/facebookresearch/KILT)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº[ToolFormer](https://arxiv.org/abs/2302.04761)æ²¡æœ‰å¼€æºï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆå¤åˆ¶æœç´¢ç´¢å¼•ã€‚åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­æåˆ°ï¼Œä½œè€…ä½¿ç”¨BM25æ£€ç´¢å™¨æ„å»ºäº†æœç´¢ç´¢å¼•ï¼Œè¯¥æ£€ç´¢å™¨ä»[KILT](https://github.com/facebookresearch/KILT)çš„ç»´åŸºç™¾ç§‘è½¬å‚¨ä¸­æ£€ç´¢
- en: Fortunately, [`pyserini`](https://github.com/castorini/pyserini) already implements
    the BM25 retriever and provides a prebuilt index for the KILT Wikipedia dump.
    We can use the following code to search the index.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œ[`pyserini`](https://github.com/castorini/pyserini)å·²ç»å®ç°äº†BM25æ£€ç´¢å™¨ï¼Œå¹¶ä¸ºKILTç»´åŸºç™¾ç§‘è½¬å‚¨æä¾›äº†é¢„æ„å»ºçš„ç´¢å¼•ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æœç´¢ç´¢å¼•ã€‚
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We then basically deployed this snippet as a Hugging Face space [here](https://huggingface.co/spaces/vwxyzjn/pyserini-wikipedia-kilt-doc),
    so that we can use the space as a `transformers.Tool` later.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šå°†è¿™ä¸ªç‰‡æ®µéƒ¨ç½²ä¸ºHugging Faceç©ºé—´[è¿™é‡Œ](https://huggingface.co/spaces/vwxyzjn/pyserini-wikipedia-kilt-doc)ï¼Œä»¥ä¾¿ç¨åå¯ä»¥å°†ç©ºé—´ç”¨ä½œ`transformers.Tool`ã€‚
- en: '![](../Images/99bd431ca68ab971be553352b4f02e67.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99bd431ca68ab971be553352b4f02e67.png)'
- en: Experiment settings
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®éªŒè®¾ç½®
- en: 'We use the following settings:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹è®¾ç½®ï¼š
- en: use the `bigcode/starcoderbase` model as the base model
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`bigcode/starcoderbase`æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹
- en: use the `pyserini-wikipedia-kilt-doc` space as the wiki tool and only uses the
    first paragrahs of the search result, allowing the `TextEnvironment` to obtain
    at most `max_tool_reponse=400` response tokens from the tool.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`pyserini-wikipedia-kilt-doc`ç©ºé—´ä½œä¸ºç»´åŸºå·¥å…·ï¼Œå¹¶ä»…ä½¿ç”¨æœç´¢ç»“æœçš„ç¬¬ä¸€æ®µï¼Œå…è®¸`TextEnvironment`ä»å·¥å…·ä¸­æœ€å¤šè·å¾—`max_tool_reponse=400`ä¸ªå“åº”æ ‡è®°ã€‚
- en: test if the response contain the answer string, if so, give a reward of 1, otherwise,
    give a reward of 0.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµ‹è¯•å›å¤æ˜¯å¦åŒ…å«ç­”æ¡ˆå­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å¥–åŠ±1ï¼Œå¦åˆ™ï¼Œå¥–åŠ±0ã€‚
- en: notice this is a simplified evaluation criteria. In [ToolFormer](https://arxiv.org/abs/2302.04761),
    the authors checks if the first 20 words of the response contain the correct answer.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è¯„ä¼°æ ‡å‡†ã€‚åœ¨[ToolFormer](https://arxiv.org/abs/2302.04761)ä¸­ï¼Œä½œè€…æ£€æŸ¥å›å¤çš„å‰20ä¸ªå•è¯æ˜¯å¦åŒ…å«æ­£ç¡®ç­”æ¡ˆã€‚
- en: used the following prompt that demonstrates the usage of the wiki tool.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»¥ä¸‹æ¼”ç¤ºäº†ç»´åŸºå·¥å…·çš„ç”¨æ³•çš„æç¤ºã€‚
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Result and Discussion
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»“æœå’Œè®¨è®º
- en: Our experiments show that the agent can learn to use the wiki tool to answer
    questions. The learning curves would go up mostly, but one of the experiment did
    crash.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä»£ç†å¯ä»¥å­¦ä¼šä½¿ç”¨ç»´åŸºå·¥å…·æ¥å›ç­”é—®é¢˜ã€‚å­¦ä¹ æ›²çº¿å¤§å¤šä¼šä¸Šå‡ï¼Œä½†å…¶ä¸­ä¸€ä¸ªå®éªŒå´©æºƒäº†ã€‚
- en: '![](../Images/a85eaae9f29d7074402801be154c2d57.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a85eaae9f29d7074402801be154c2d57.png)'
- en: Wandb report is [here](https://wandb.ai/costa-huang/cleanRL/reports/TriviaQA-Final-Experiments--Vmlldzo1MjY0ODk5)
    for further inspection.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: WandbæŠ¥å‘Šåœ¨[è¿™é‡Œ](https://wandb.ai/costa-huang/cleanRL/reports/TriviaQA-Final-Experiments--Vmlldzo1MjY0ODk5)è¿›è¡Œè¿›ä¸€æ­¥æ£€æŸ¥ã€‚
- en: 'Note that the correct rate of the trained model is on the low end, which could
    be due to the following reasons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè®­ç»ƒæ¨¡å‹çš„æ­£ç¡®ç‡è¾ƒä½ï¼Œå¯èƒ½æ˜¯ç”±äºä»¥ä¸‹åŸå› ï¼š
- en: '**incorrect searches:** When given the question `"What is Bruce Willis'' real
    first name?"` if the model searches for `Bruce Willis`, our wiki tool returns
    â€œPatrick Poivey (born 18 February 1948) is a French actor. He is especially known
    for his voice: he is the French dub voice of Bruce Willis since 1988.`But a correct
    search should be`Walter Bruce Willis (born March 19, 1955) is an American former
    actor. He achieved fame with a leading role on the comedy-drama series Moonlighting
    (1985â€“1989) and appeared in over a hundred films, gaining recognition as an action
    hero after his portrayal of John McClane in the Die Hard franchise (1988â€“2013)
    and other roles.[1][2]â€'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é”™è¯¯çš„æœç´¢:** å½“ç»™å‡ºé—®é¢˜`â€œå¸ƒé²æ–¯Â·å¨åˆ©æ–¯çš„çœŸå®åå­—æ˜¯ä»€ä¹ˆï¼Ÿâ€`æ—¶ï¼Œå¦‚æœæ¨¡å‹æœç´¢`å¸ƒé²æ–¯Â·å¨åˆ©æ–¯`ï¼Œæˆ‘ä»¬çš„ç»´åŸºå·¥å…·è¿”å›â€œå¸•ç‰¹é‡Œå…‹Â·æ™®ç“¦ï¼ˆå‡ºç”Ÿäº1948å¹´2æœˆ18æ—¥ï¼‰æ˜¯ä¸€ä½æ³•å›½æ¼”å‘˜ã€‚ä»–ä»¥ä»–çš„å£°éŸ³è€Œé—»åï¼šè‡ª1988å¹´ä»¥æ¥ï¼Œä»–ä¸€ç›´æ˜¯å¸ƒé²æ–¯Â·å¨åˆ©æ–¯çš„æ³•è¯­é…éŸ³ã€‚â€ä½†æ­£ç¡®çš„æœç´¢åº”è¯¥æ˜¯`æ²ƒå°”ç‰¹Â·å¸ƒé²æ–¯Â·å¨åˆ©æ–¯ï¼ˆå‡ºç”Ÿäº1955å¹´3æœˆ19æ—¥ï¼‰æ˜¯ä¸€ä½ç¾å›½å‰æ¼”å‘˜ã€‚ä»–å› åœ¨å–œå‰§å‰§é›†ã€Šæœˆå…‰å…‰ã€‹ï¼ˆ1985-1989ï¼‰ä¸­æ‹…ä»»ä¸»è§’è€Œæˆåï¼Œå¹¶åœ¨ä¸€ç™¾å¤šéƒ¨ç”µå½±ä¸­å‡ºæ¼”ï¼Œå› åœ¨ã€Šè™èƒ†é¾™å¨ã€‹ç³»åˆ—ï¼ˆ1988-2013ï¼‰ä¸­æ‰®æ¼”çº¦ç¿°Â·éº¦å…‹è±æ©è€Œè¢«è®¤å¯ä¸ºåŠ¨ä½œè‹±é›„ä»¥åŠå…¶ä»–è§’è‰²è€Œè·å¾—è®¤å¯ã€‚`'
- en: '![](../Images/36b96862e37367132a174f261df0cb60.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36b96862e37367132a174f261df0cb60.png)'
- en: '**unnecessarily long response**: The wiki tool by default sometimes output
    very long sequences. E.g., when the wiki tool searches for â€œBrown Actâ€'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸å¿…è¦çš„é•¿å›å¤:** é»˜è®¤æƒ…å†µä¸‹ï¼Œç»´åŸºå·¥å…·æœ‰æ—¶ä¼šè¾“å‡ºéå¸¸é•¿çš„åºåˆ—ã€‚ä¾‹å¦‚ï¼Œå½“ç»´åŸºå·¥å…·æœç´¢â€œå¸ƒæœ—æ³•æ¡ˆâ€æ—¶'
- en: Our wiki tool returns â€œThe Ralph M. Brown Act, located at California Government
    Code 54950 â€œet seq.â€, is an act of the California State Legislature, authored
    by Assemblymember Ralph M. Brown and passed in 1953, that guarantees the publicâ€™s
    right to attend and participate in meetings of local legislative bodies.â€
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç»´åŸºå·¥å…·è¿”å›â€œæ‹‰å°”å¤«Â·MÂ·å¸ƒæœ—æ³•æ¡ˆï¼Œä½äºåŠ åˆ©ç¦å°¼äºšæ”¿åºœæ³•å…¸54950â€œç­‰ç­‰â€ï¼Œæ˜¯åŠ åˆ©ç¦å°¼äºšå·è®®ä¼šçš„ä¸€é¡¹æ³•æ¡ˆï¼Œç”±è®®å‘˜æ‹‰å°”å¤«Â·MÂ·å¸ƒæœ—æ’°å†™å¹¶äº1953å¹´é€šè¿‡ï¼Œä¿è¯äº†å…¬ä¼—æœ‰æƒå‚åŠ å’Œå‚ä¸åœ°æ–¹ç«‹æ³•æœºæ„çš„ä¼šè®®ã€‚â€
- en: '[ToolFormer](https://arxiv.org/abs/2302.04761)â€™s wiki tool returns â€œThe Ralph
    M. Brown Act is an act of the California State Legislature that guarantees the
    publicâ€™s right to attend and participate in meetings of local legislative bodies.â€
    which is more succinct.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ToolFormer](https://arxiv.org/abs/2302.04761)çš„ç»´åŸºå·¥å…·è¿”å›â€œæ‹‰å°”å¤«Â·MÂ·å¸ƒæœ—æ³•æ¡ˆæ˜¯åŠ åˆ©ç¦å°¼äºšå·è®®ä¼šçš„ä¸€é¡¹æ³•æ¡ˆï¼Œä¿è¯äº†å…¬ä¼—æœ‰æƒå‚åŠ å’Œå‚ä¸åœ°æ–¹ç«‹æ³•æœºæ„çš„ä¼šè®®ã€‚â€æ›´ä¸ºç®€æ´ã€‚'
- en: '![](../Images/03829057729a612fe5243cc49785765b.png)'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/03829057729a612fe5243cc49785765b.png)'
- en: '(Early Experiments ğŸ§ª): solving math puzzles with python interpreter'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ï¼ˆæ—©æœŸå®éªŒğŸ§ªï¼‰ï¼šä½¿ç”¨Pythonè§£é‡Šå™¨è§£å†³æ•°å­¦éš¾é¢˜
- en: 'In this section, we attempt to teach the model to use a python interpreter
    to solve math puzzles. The rough idea is to give the agent a prompt like the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°è¯•æ•™å¯¼æ¨¡å‹ä½¿ç”¨Pythonè§£é‡Šå™¨æ¥è§£å†³æ•°å­¦éš¾é¢˜ã€‚å¤§è‡´çš„æƒ³æ³•æ˜¯ç»™äºˆä»£ç†ä¸€ä¸ªç±»ä¼¼ä»¥ä¸‹çš„æç¤ºï¼š
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training experiment can be found at [https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y](https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®éªŒå¯ä»¥åœ¨[https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y](https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y)æ‰¾åˆ°
- en: '![](../Images/07e24d84156093ad57f614545aaa5ab2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e24d84156093ad57f614545aaa5ab2.png)'
