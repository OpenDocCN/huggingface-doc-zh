- en: NLLB-MOE
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLLB-MOE
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb-moe](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb-moe)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb-moe](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb-moe)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered
    Machine Translation](https://arxiv.org/abs/2207.04672) by Marta R. Costa-jussà,
    James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe
    Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume
    Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip
    Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon
    Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov,
    Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre
    Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'NLLB模型是由Marta R. Costa-jussà、James Cross、Onur Çelebi、Maha Elbayad、Kenneth Heafield、Kevin
    Heffernan、Elahe Kalbassi、Janice Lam、Daniel Licht、Jean Maillard、Anna Sun、Skyler
    Wang、Guillaume Wenzek、Al Youngblood、Bapi Akula、Loic Barrault、Gabriel Mejia Gonzalez、Prangthip
    Hansanti、John Hoffman、Semarley Jarrett、Kaushik Ram Sadagopan、Dirk Rowe、Shannon
    Spruit、Chau Tran、Pierre Andrews、Necip Fazil Ayan、Shruti Bhosale、Sergey Edunov、Angela
    Fan、Cynthia Gao、Vedanuj Goswami、Francisco Guzmán、Philipp Koehn、Alexandre Mourachko、Christophe
    Ropers、Safiyyah Saleem、Holger Schwenk和Jeff Wang在[No Language Left Behind: Scaling
    Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672)中提出的。'
- en: 'The abstract of the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Driven by the goal of eradicating language barriers on a global scale, machine
    translation has solidified itself as a key focus of artificial intelligence research
    today. However, such efforts have coalesced around a small subset of languages,
    leaving behind the vast majority of mostly low-resource languages. What does it
    take to break the 200 language barrier while ensuring safe, high quality results,
    all while keeping ethical considerations in mind? In No Language Left Behind,
    we took on this challenge by first contextualizing the need for low-resource language
    translation support through exploratory interviews with native speakers. Then,
    we created datasets and models aimed at narrowing the performance gap between
    low and high-resource languages. More specifically, we developed a conditional
    compute model based on Sparsely Gated Mixture of Experts that is trained on data
    obtained with novel and effective data mining techniques tailored for low-resource
    languages. We propose multiple architectural and training improvements to counteract
    overfitting while training on thousands of tasks. Critically, we evaluated the
    performance of over 40,000 different translation directions using a human-translated
    benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark
    covering all languages in Flores-200 to assess translation safety. Our model achieves
    an improvement of 44% BLEU relative to the previous state-of-the-art, laying important
    groundwork towards realizing a universal translation system.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*受到在全球范围内消除语言障碍的目标驱动，机器翻译已经巩固自己作为当今人工智能研究的重点。然而，这些努力已经围绕着一小部分语言展开，抛弃了大多数主要是低资源语言的语言。要突破200种语言障碍，同时确保安全、高质量的结果，同时考虑伦理因素，需要什么？在《No
    Language Left Behind》中，我们通过首先通过与母语者的探索性访谈来将对低资源语言翻译支持的需求进行情境化，然后创建了旨在缩小低资源语言与高资源语言之间性能差距的数据集和模型。更具体地说，我们开发了一个基于Sparsely
    Gated Mixture of Experts的条件计算模型，该模型是通过针对低资源语言量身定制的新颖和有效的数据挖掘技术获得的数据进行训练的。我们提出了多种架构和训练改进措施，以抵消在数千个任务上训练时的过拟合。至关重要的是，我们使用人工翻译的基准Flores-200评估了超过40,000个不同的翻译方向的性能，并结合了一个涵盖Flores-200中所有语言的新型毒性基准来评估翻译的安全性。我们的模型相对于先前的最先进技术实现了44%的BLEU改进，为实现通用翻译系统奠定了重要基础。*'
- en: This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).
    The original code can be found [here](https://github.com/facebookresearch/fairseq).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[Arthur Zucker](https://huggingface.co/ArthurZ)贡献。原始代码可以在[这里](https://github.com/facebookresearch/fairseq)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: M2M100ForConditionalGeneration is the base model for both NLLB and NLLB MoE
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M2M100ForConditionalGeneration是NLLB和NLLB MoE的基础模型
- en: The NLLB-MoE is very similar to the NLLB model, but it’s feed forward layer
    is based on the implementation of SwitchTransformers.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLLB-MoE与NLLB模型非常相似，但其前馈层基于SwitchTransformers的实现。
- en: The tokenizer is the same as the NLLB models.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器与NLLB模型相同。
- en: Implementation differences with SwitchTransformers
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与SwitchTransformers的实现差异
- en: The biggest difference is the way the tokens are routed. NLLB-MoE uses a `top-2-gate`
    which means that for each input, only the top two experts are selected based on
    the highest predicted probabilities from the gating network, and the remaining
    experts are ignored. In `SwitchTransformers`, only the top-1 probabilities are
    computed, which means that tokens have less probability of being forwarded. Moreover,
    if a token is not routed to any expert, `SwitchTransformers` still adds its unmodified
    hidden states (kind of like a residual connection) while they are masked in `NLLB`’s
    top-2 routing mechanism.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的区别在于令牌路由的方式。NLLB-MoE使用`top-2-gate`，这意味着对于每个输入，只选择两个最高预测概率的专家，其余专家将被忽略。在`SwitchTransformers`中，只计算了前两个最高概率，这意味着令牌被转发的概率较低。此外，如果一个令牌没有路由到任何专家，`SwitchTransformers`仍然会添加其未修改的隐藏状态（类似于残差连接），而在`NLLB`的top-2路由机制中，它们被屏蔽。
- en: Generating with NLLB-MoE
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NLLB-MoE生成
- en: The available checkpoints require around 350GB of storage. Make sure to use
    `accelerate` if you do not have enough RAM on your machine.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的检查点需要约350GB的存储空间。如果您的计算机内存不足，请确保使用`accelerate`。
- en: While generating the target text set the `forced_bos_token_id` to the target
    language id. The following example shows how to translate English to French using
    the *facebook/nllb-200-distilled-600M* model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成目标文本集时，将`forced_bos_token_id`设置为目标语言id。以下示例显示如何使用*facebook/nllb-200-distilled-600M*模型将英语翻译成法语。
- en: Note that we’re using the BCP-47 code for French `fra_Latn`. See [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)
    for the list of all BCP-47 in the Flores 200 dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用法语的BCP-47代码`fra_Latn`。请参阅[Flores 200数据集中所有BCP-47的列表](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generating from any other language than English
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从除英语以外的任何其他语言生成
- en: English (`eng_Latn`) is set as the default language from which to translate.
    In order to specify that you’d like to translate from a different language, you
    should specify the BCP-47 code in the `src_lang` keyword argument of the tokenizer
    initialization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 英语（`eng_Latn`）被设置为默认语言进行翻译。为了指定您希望从其他语言翻译，您应该在分词器初始化的`src_lang`关键字参数中指定BCP-47代码。
- en: 'See example below for a translation from romanian to german:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见下面的示例，将罗马尼亚语翻译成德语：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Resources
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Translation task guide](../tasks/translation)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[翻译任务指南](../tasks/translation)'
- en: '[Summarization task guide](../tasks/summarization)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[摘要任务指南](../tasks/summarization)'
- en: NllbMoeConfig
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NllbMoeConfig
- en: '### `class transformers.NllbMoeConfig`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NllbMoeConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/configuration_nllb_moe.py#L27)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/configuration_nllb_moe.py#L27)'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 50265) — Vocabulary size of the
    NllbMoe model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)
    or'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 50265) — NllbMoe模型的词汇量。定义了在调用[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)时可以表示的不同标记数量。'
- en: '`d_model` (`int`, *optional*, defaults to 1024) — Dimensionality of the layers
    and the pooler layer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, defaults to 1024) — 层和池化器层的维度。'
- en: '`encoder_layers` (`int`, *optional*, defaults to 12) — Number of encoder layers.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers` (`int`, *optional*, defaults to 12) — 编码器层数。'
- en: '`decoder_layers` (`int`, *optional*, defaults to 12) — Number of decoder layers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers` (`int`, *optional*, defaults to 12) — 解码器层数。'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer解码器中每个注意力层的注意力头数。'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 4096) — Dimensionality of
    the “intermediate” (often named feed-forward) layer in decoder.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, defaults to 4096) — 解码器中“中间”（通常称为前馈）层的维度。'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 4096) — Dimensionality of
    the “intermediate” (often named feed-forward) layer in encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, defaults to 4096) — 编码器中“中间”（通常称为前馈）层的维度。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — 全连接层内激活的dropout比率。'
- en: '`classifier_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for classifier.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_dropout` (`float`, *optional*, defaults to 0.0) — 分类器的dropout比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 1024) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 1024) — 该模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop
    probability for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](https://arxiv.org/abs/1909.11556)。'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop
    probability for the decoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 解码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](https://arxiv.org/abs/1909.11556)。'
- en: '`second_expert_policy` ( `str`, *optional*, default to `"all"`) — The policy
    used for the sampling the probability of being sampled to a second expert for
    each token.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`second_expert_policy` (`str`, *optional*, default to `"all"`) — 用于对每个标记采样到第二专家的概率进行采样的策略。'
- en: '`normalize_router_prob_before_dropping` (`bool`, *optional*, defaults to `True`)
    — Whether or not to normalize the router probabilities before applying a mask
    based on the experts capacity (capacity dropping).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_router_prob_before_dropping` (`bool`, *optional*, defaults to `True`)
    — 是否在应用基于专家容量的掩码之前对路由器概率进行归一化（容量降低）。'
- en: '`batch_prioritized_routing` (`bool`, *optional*, defaults to `True`) — Whether
    or not to orders the tokens by their router probabilities before capacity dropping.
    This means that the tokens that have the highest probabilities will be routed
    before other tokens that might be further in the sequence.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_prioritized_routing` (`bool`, *optional*, 默认为`True`) — 是否按照路由器概率对令牌进行排序以进行容量丢弃。这意味着具有最高概率的令牌将在其他可能在序列中更远的令牌之前路由。'
- en: '`moe_eval_capacity_token_fraction` (`float`, *optional*, defaults to 1.0) —
    Fraction of tokens as capacity during validation, if set to negative, uses the
    same as training. Should be in range: (0.0, 1.0].'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`moe_eval_capacity_token_fraction` (`float`, *optional*, 默认为1.0) — 验证期间作为容量的令牌分数，如果设置为负数，则使用与训练相同的值。应在范围内：(0.0,
    1.0]。'
- en: '`num_experts` (`int`, *optional*, defaults to 128) — Number of experts for
    each NllbMoeSparseMlp layer.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_experts` (`int`, *optional*, 默认为128) — 每个NllbMoeSparseMlp层的专家数量。'
- en: '`expert_capacity` (`int`, *optional*, defaults to 64) — Number of tokens that
    can be stored in each expert.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expert_capacity` (`int`, *optional*, 默认为64) — 每个专家可以存储的令牌数量。'
- en: '`encoder_sparse_step` (`int`, *optional*, defaults to 4) — Frequency of the
    sparse layers in the encoder. 4 means that one out of 4 layers will be sparse.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_sparse_step` (`int`, *optional*, defaults to 4) — 编码器中稀疏层的频率。4表示每4层中会有一层是稀疏的。'
- en: '`decoder_sparse_step` (`int`, *optional*, defaults to 4) — Frequency of the
    sparse layers in the decoder. 4 means that one out of 4 layers will be sparse.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_sparse_step` (`int`, *optional*, defaults to 4) — 解码器中稀疏层的频率。4表示每4层中会有一层是稀疏的。'
- en: '`router_dtype` (`str`, *optional*, default to `"float32"`) — The `dtype` used
    for the routers. It is preferable to keep the `dtype` to `"float32"` as specified
    in the *selective precision* discussion in [the paper](https://arxiv.org/abs/2101.03961).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_dtype` (`str`, *optional*, 默认为`"float32"`) — 用于路由器的`dtype`。最好保持`dtype`为`"float32"`，如[论文](https://arxiv.org/abs/2101.03961)中的*selective
    precision*讨论中所指定的。'
- en: '`router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) —
    Whether to ignore padding tokens when routing. if `False`, the padding tokens
    are not routed to any experts.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_ignore_padding_tokens` (`bool`, *optional*, 默认为`False`) — 在路由时是否忽略填充令牌。如果为`False`，则填充令牌不会路由到任何专家。'
- en: '`router_bias` (`bool`, *optional*, defaults to `False`) — Whether or not the
    classifier of the router should have a bias.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_bias` (`bool`, *optional*, 默认为`False`) — 路由器的分类器是否应具有偏差。'
- en: '`moe_token_dropout` (`float`, *optional*, defualt ot 0.2) — Masking rate for
    MoE expert output masking (EOM), which is implemented via a Dropout2d on the expert
    outputs.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`moe_token_dropout` (`float`, *optional*, 默认为0.2) — MoE专家输出掩码（EOM）的掩码率，通过对专家输出进行Dropout2d实现。'
- en: '`output_router_logits` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the router logits. Only set to `True` to get the auxiliary loss
    when training.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_router_logits` (`bool`, *optional*, 默认为`False`) — 是否返回路由器logits。仅在训练时设置为`True`以获得辅助损失。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: This is the configuration class to store the configuration of a [NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel).
    It is used to instantiate an NLLB-MoE model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the NLLB-MoE [facebook/nllb-moe-54b](https://huggingface.co/facebook/nllb-moe-54b)
    architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)的配置。它用于根据指定的参数实例化一个NLLB-MoE模型，定义模型架构。使用默认值实例化配置将产生类似于NLLB-MoE
    [facebook/nllb-moe-54b](https://huggingface.co/facebook/nllb-moe-54b) 架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: NllbMoeTop2Router
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NllbMoeTop2Router
- en: '### `class transformers.NllbMoeTop2Router`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NllbMoeTop2Router`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L217)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L217)'
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Router using tokens choose top-2 experts assignment.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器使用令牌选择前两个专家的分配。
- en: This router uses the same mechanism as in NLLB-MoE from the fairseq repository.
    Items are sorted by router_probs and then routed to their choice of expert until
    the expert’s expert_capacity is reached. **There is no guarantee that each token
    is processed by an expert**, or that each expert receives at least one token.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此路由器使用与fairseq存储库中的NLLB-MoE相同的机制。项目按照router_probs排序，然后路由到其选择的专家，直到达到专家的expert_capacity。**不能保证每个令牌都由专家处理**，也不能保证每个专家至少收到一个令牌。
- en: The router combining weights are also returned to make sure that the states
    that are not updated will be masked.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 还返回路由器组合权重，以确保未更新的状态将被掩码。
- en: '#### `route_tokens`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `route_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L258)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L258)'
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Computes the `dispatch_mask` and the `dispatch_weights` for each experts. The
    masks are adapted to the expert capacity.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个专家的`dispatch_mask`和`dispatch_weights`。这些掩码会根据专家的容量进行调整。
- en: '#### `forward`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L343)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L343)'
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.Tensor`) — (batch_size, sequence_length, hidden_dim)
    from which router probabilities are computed.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`torch.Tensor`) — (batch_size, sequence_length, hidden_dim)
    用于计算路由器概率。'
- en: Returns
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: top_1_mask (`torch.Tensor` of shape (batch_size, sequence_length))
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: top_1_mask (`torch.Tensor`，形状为(batch_size, sequence_length))
- en: 'Index tensor of shape [batch_size, sequence_length] corresponding to the expert
    selected for each token using the top1 probabilities of the router. router_probabilities
    (`torch.Tensor` of shape (batch_size, sequence_length, nump_experts)): Tensor
    of shape (batch_size, sequence_length, num_experts) corresponding to the probabilities
    for each token and expert. Used for routing tokens to experts. router_logits (`torch.Tensor`
    of shape (batch_size, sequence_length))): Logits tensor of shape (batch_size,
    sequence_length, num_experts) corresponding to raw router logits. This is used
    later for computing router z-loss.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为[batch_size, sequence_length]的索引张量，对应于使用路由器的top1概率为每个标记选择的专家。router_probabilities
    (`torch.Tensor`，形状为(batch_size, sequence_length, nump_experts))：形状为(batch_size,
    sequence_length, num_experts)的张量，对应于每个标记和专家的概率。用于将标记路由到专家。router_logits (`torch.Tensor`，形状为(batch_size,
    sequence_length))：形状为(batch_size, sequence_length, num_experts)的原始路由器logits张量。稍后用于计算路由器z-loss。
- en: The hidden states are reshaped to simplify the computation of the router probabilities
    (combining weights for each experts.)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态被重新整形以简化路由器概率的计算（为每个专家组合权重）。
- en: NllbMoeSparseMLP
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NllbMoeSparseMLP
- en: '### `class transformers.NllbMoeSparseMLP`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NllbMoeSparseMLP`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L394)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L394)'
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Implementation of the NLLB-MoE sparse MLP module.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: NLLB-MoE稀疏MLP模块的实现。
- en: '#### `forward`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L410)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L410)'
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.Tensor` of shape `(batch_size, sequence_length, hidden_dim)`)
    — The hidden states'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`torch.Tensor`，形状为`(batch_size, sequence_length, hidden_dim)`)
    — 隐藏状态'
- en: '`padding_mask` (`torch.Tensor`, *optional*, defaults to `False`) — Attention
    mask. Can be in the causal form or not.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_mask` (`torch.Tensor`，*可选*，默认为`False`) — 注意力掩码。可以是因果形式或非因果形式。'
- en: Returns
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: hidden_states (`torch.Tensor` of shape `(batch_size, sequence_length, hidden_dim)`)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: hidden_states (`torch.Tensor`，形状为`(batch_size, sequence_length, hidden_dim)`)
- en: 'Updated hidden states router_logits (`torch.Tensor` of shape `(batch_size,
    sequence_length, num_experts)`): Needed for computing the loss'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的隐藏状态路由器logits (`torch.Tensor`，形状为`(batch_size, sequence_length, num_experts)`)：用于计算损失
- en: The goal of this forward pass is to have the same number of operation as the
    equivalent `NllbMoeDenseActDense` (mlp) layer. This means that all of the hidden
    states should be processed at most twice ( since we are using a top_2 gating mecanism).
    This means that we keep the complexity to O(batch_size x sequence_length x hidden_dim)
    instead of O(num_experts x batch_size x sequence_length x hidden_dim).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递的目标是具有与等效的`NllbMoeDenseActDense`（mlp）层相同数量的操作。这意味着所有隐藏状态最多应该被处理两次（因为我们使用了一个top_2门控机制）。这意味着我们将复杂度保持在O(batch_size
    x sequence_length x hidden_dim)而不是O(num_experts x batch_size x sequence_length
    x hidden_dim)。
- en: 1- Get the `router_probs` from the `router`. The shape of the `router_mask`
    is `(batch_size X sequence_length, num_expert)` and corresponds to the boolean
    version of the `router_probs`. The inputs are masked using the `router_mask`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 从`router`获取`router_probs`。`router_mask`的形状为`(batch_size X sequence_length,
    num_expert)`，对应于`router_probs`的布尔版本。使用`router_mask`对输入进行掩码处理。
- en: 2- Dispatch the hidden_states to its associated experts. The router probabilities
    are used to weight the contribution of each experts when updating the masked hidden
    states.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 将隐藏状态分派给其关联的专家。路由器概率用于在更新掩码隐藏状态时加权每个专家的贡献。
- en: NllbMoeModel
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NllbMoeModel
- en: '### `class transformers.NllbMoeModel`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NllbMoeModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1452)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1452)'
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare NllbMoe Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 裸NllbMoe模型输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1490)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1490)'
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Returns
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.Seq2SeqMoEModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.Seq2SeqMoEModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.Seq2SeqMoEModelOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig))
    and inputs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.Seq2SeqMoEModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或者`config.return_dict=False`）包含根据配置（[NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型解码器最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或者`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个
    + 每一层的输出一个）。'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`decoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_router_logits` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_router_logits=True`或者`config.add_router_probs=True`时返回)
    — 形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。'
- en: Router logits of the decoder model, useful to compute the auxiliary loss for
    Mixture of Experts models.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器模型的路由器logits，用于计算混合专家模型的辅助损失。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或者`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个
    + 每一层的输出一个）。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`encoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_router_logits`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_router_logits=True`或`config.add_router_probs=True`时返回）—
    形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。'
- en: Router logits of the encoder model, useful to compute the auxiliary loss and
    the z_loss for the sparse modules.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器模型的路由器logits，用于计算辅助损失和稀疏模块的z_loss。
- en: The [NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)
    forward method, overrides the `__call__` special method.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: The [NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)
    forward method, overrides the `__call__` special method.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: NllbMoeForConditionalGeneration
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NllbMoeForConditionalGeneration
- en: '### `class transformers.NllbMoeForConditionalGeneration`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NllbMoeForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1590)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1590)'
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The NllbMoe Model with a language modeling head. Can be used for summarization.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: NllbMoe模型带有一个语言建模头。可用于摘要。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1619)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1619)'
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 输入序列标记在词汇表中的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0,
    1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被屏蔽的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被屏蔽的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`torch.LongTensor` 的形状为 `(batch_size, target_sequence_length)`，*optional*)
    — 词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入标记？](../glossary#decoder-input-ids)'
- en: NllbMoe uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NllbMoe 使用 `eos_token_id` 作为 `decoder_input_ids` 生成的起始标记。如果使用 `past_key_values`，可选择仅输入最后的
    `decoder_input_ids`（参见 `past_key_values`）。
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.LongTensor` 的形状为 `(batch_size, target_sequence_length)`，*optional*)
    — 默认行为：生成一个忽略 `decoder_input_ids` 中填充标记的张量。因果掩码也将默认使用。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor` 的形状为 `(encoder_layers, encoder_attention_heads)`，*optional*)
    — 编码器中注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — 解码器中注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules in
    the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` 的形状为 `(decoder_layers, decoder_attention_heads)`，*optional*)
    — 解码器中交叉注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括 (`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` 的形状为
    `(batch_size, sequence_length, hidden_size)`，*optional*) 是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递 `use_cache=True`
    或 `config.use_cache=True` 时返回) — 长度为 `config.n_layers` 的 `tuple(torch.FloatTensor)`
    元组，每个元组有 2 个形状为 `(batch_size, num_heads, sequence_length, embed_size_per_head)`
    的张量和额外的 2 个形状为 `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`
    的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见 `past_key_values` 输入）。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（即未将其过去键值状态提供给此模型的那些）的形状为
    `(batch_size, 1)`，而不是形状为 `(batch_size, sequence_length)` 的所有 `decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` 的形状为 `(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您希望更好地控制如何将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length,
    hidden_size)`，*可选的*) — 可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则只需输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果要更好地控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选的*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选的*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选的*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`output_router_logits` (`bool`, *optional*) — Whether or not to return the
    logits of all the routers. They are useful for computing the router loss, and
    should not be returned during inference.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_router_logits` (`bool`, *可选的*) — 是否返回所有路由器的对数。它们对于计算路由器损失很有用，在推断期间不应返回。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选的*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should either
    be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选的*) — 用于计算掩码语言建模损失的标签。索引应该在`[0,
    ..., config.vocab_size]`范围内，或者为-100（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`范围内的标记。'
- en: Returns
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`transformers.modeling_outputs.Seq2SeqMoEOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.Seq2SeqMoEOutput`或者`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.Seq2SeqMoEOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig))
    and inputs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.Seq2SeqMoEOutput`或者一个`torch.FloatTensor`元组（如果传递`return_dict=False`或者当`config.return_dict=False`时）包含各种元素，取决于配置（[NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`, *可选的*, 当提供`labels`时返回) — 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *可选的*, 当传递`use_cache=True`或者当`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个层的解码器的隐藏状态加上初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[NllbMoeForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: '`decoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Router logits of the decoder model, useful to compute the auxiliary loss for
    Mixture of Experts models.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`。'
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 编码器模型最后一层的隐藏状态序列。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器每一层输出的隐藏状态，以及初始嵌入输出。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器模型的路由器logits，用于计算混合专家模型的辅助损失和z_loss。
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器模型的路由器logits，用于计算混合专家模型的辅助损失。
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后使用，用于计算自注意力头中的加权平均值。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后使用，用于计算自注意力头中的加权平均值。
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个函数，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。
- en: '`encoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
- en: Router logits of the encoder model, useful to compute the auxiliary loss and
    z_loss for Mixture of Experts models.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后使用，用于计算交叉注意力头中的加权平均值。
- en: The [NllbMoeForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在传递`output_router_logits=True`或`config.add_router_probs=True`时返回，`encoder_router_logits`是一个元组，包含每一层的`torch.FloatTensor`，形状为`(batch_size,
    sequence_length, num_experts)`。
- en: 'Translation example:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`。'
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
