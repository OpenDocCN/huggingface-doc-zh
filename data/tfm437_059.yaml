- en: Export to TorchScript
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¼å‡ºåˆ°TorchScript
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/torchscript](https://huggingface.co/docs/transformers/v4.37.2/en/torchscript)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/torchscript](https://huggingface.co/docs/transformers/v4.37.2/en/torchscript)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This is the very beginning of our experiments with TorchScript and we are still
    exploring its capabilities with variable-input-size models. It is a focus of interest
    to us and we will deepen our analysis in upcoming releases, with more code examples,
    a more flexible implementation, and benchmarks comparing Python-based codes with
    compiled TorchScript.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬ä½¿ç”¨TorchScriptçš„å®éªŒçš„å¼€å§‹ï¼Œæˆ‘ä»¬ä»åœ¨æ¢ç´¢å…¶å¯¹äºå¯å˜è¾“å…¥å¤§å°æ¨¡å‹çš„èƒ½åŠ›ã€‚è¿™æ˜¯æˆ‘ä»¬æ„Ÿå…´è¶£çš„ç„¦ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨å³å°†å‘å¸ƒçš„ç‰ˆæœ¬ä¸­æ·±å…¥åˆ†æï¼Œæä¾›æ›´å¤šä»£ç ç¤ºä¾‹ï¼Œæ›´çµæ´»çš„å®ç°ä»¥åŠå°†Pythonä»£ç ä¸ç¼–è¯‘åçš„TorchScriptè¿›è¡Œæ¯”è¾ƒçš„åŸºå‡†æµ‹è¯•ã€‚
- en: 'According to the [TorchScript documentation](https://pytorch.org/docs/stable/jit.html):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®[TorchScriptæ–‡æ¡£](https://pytorch.org/docs/stable/jit.html)ï¼š
- en: TorchScript is a way to create serializable and optimizable models from PyTorch
    code.
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TorchScriptæ˜¯ä¸€ç§ä»PyTorchä»£ç åˆ›å»ºå¯åºåˆ—åŒ–å’Œå¯ä¼˜åŒ–æ¨¡å‹çš„æ–¹æ³•ã€‚
- en: There are two PyTorch modules, [JIT and TRACE](https://pytorch.org/docs/stable/jit.html),
    that allow developers to export their models to be reused in other programs like
    efficiency-oriented C++ programs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ä¸ªPyTorchæ¨¡å—[JITå’ŒTRACE](https://pytorch.org/docs/stable/jit.html)ï¼Œå…è®¸å¼€å‘äººå‘˜å°†ä»–ä»¬çš„æ¨¡å‹å¯¼å‡ºä»¥ä¾¿åœ¨å…¶ä»–ç¨‹åºä¸­é‡å¤ä½¿ç”¨ï¼Œæ¯”å¦‚é¢å‘æ•ˆç‡çš„C++ç¨‹åºã€‚
- en: We provide an interface that allows you to export ğŸ¤— Transformers models to TorchScript
    so they can be reused in a different environment than PyTorch-based Python programs.
    Here, we explain how to export and use our models using TorchScript.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå…è®¸æ‚¨å°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºåˆ°TorchScriptï¼Œä»¥ä¾¿åœ¨ä¸åŸºäºPyTorchçš„Pythonç¨‹åºä¸åŒçš„ç¯å¢ƒä¸­é‡å¤ä½¿ç”¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è§£é‡Šäº†å¦‚ä½•ä½¿ç”¨TorchScriptå¯¼å‡ºå’Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: 'Exporting a model requires two things:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼å‡ºæ¨¡å‹éœ€è¦ä¸¤ä»¶äº‹ï¼š
- en: model instantiation with the `torchscript` flag
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`torchscript`æ ‡å¿—å®ä¾‹åŒ–æ¨¡å‹
- en: a forward pass with dummy inputs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è™šæ‹Ÿè¾“å…¥è¿›è¡Œå‰å‘ä¼ é€’
- en: These necessities imply several things developers should be careful about as
    detailed below.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¿…éœ€å“æ„å‘³ç€å¼€å‘äººå‘˜åº”è¯¥æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ã€‚
- en: TorchScript flag and tied weights
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TorchScriptæ ‡å¿—å’Œç»‘å®šæƒé‡
- en: The `torchscript` flag is necessary because most of the ğŸ¤— Transformers language
    models have tied weights between their `Embedding` layer and their `Decoding`
    layer. TorchScript does not allow you to export models that have tied weights,
    so it is necessary to untie and clone the weights beforehand.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchscript`æ ‡å¿—æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå¤§å¤šæ•°ğŸ¤— Transformersè¯­è¨€æ¨¡å‹çš„`Embedding`å±‚å’Œ`Decoding`å±‚ä¹‹é—´æœ‰ç»‘å®šæƒé‡ã€‚TorchScriptä¸å…è®¸æ‚¨å¯¼å‡ºå…·æœ‰ç»‘å®šæƒé‡çš„æ¨¡å‹ï¼Œå› æ­¤éœ€è¦åœ¨æ­¤ä¹‹å‰è§£å¼€å¹¶å…‹éš†æƒé‡ã€‚'
- en: Models instantiated with the `torchscript` flag have their `Embedding` layer
    and `Decoding` layer separated, which means that they should not be trained down
    the line. Training would desynchronize the two layers, leading to unexpected results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`torchscript`æ ‡å¿—å®ä¾‹åŒ–çš„æ¨¡å‹å°†å®ƒä»¬çš„`Embedding`å±‚å’Œ`Decoding`å±‚åˆ†å¼€ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸åº”è¯¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒä¼šä½¿è¿™ä¸¤å±‚ä¸åŒæ­¥ï¼Œå¯¼è‡´æ„å¤–ç»“æœã€‚
- en: This is not the case for models that do not have a language model head, as those
    do not have tied weights. These models can be safely exported without the `torchscript`
    flag.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ²¡æœ‰è¯­è¨€æ¨¡å‹å¤´çš„æ¨¡å‹ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ²¡æœ‰ç»‘å®šæƒé‡ã€‚è¿™äº›æ¨¡å‹å¯ä»¥å®‰å…¨åœ°å¯¼å‡ºè€Œä¸ä½¿ç”¨`torchscript`æ ‡å¿—ã€‚
- en: Dummy inputs and standard lengths
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è™šæ‹Ÿè¾“å…¥å’Œæ ‡å‡†é•¿åº¦
- en: The dummy inputs are used for a models forward pass. While the inputsâ€™ values
    are propagated through the layers, PyTorch keeps track of the different operations
    executed on each tensor. These recorded operations are then used to create the
    *trace* of the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è™šæ‹Ÿè¾“å…¥ç”¨äºæ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚å½“è¾“å…¥çš„å€¼é€šè¿‡å±‚ä¼ æ’­æ—¶ï¼ŒPyTorchä¼šè·Ÿè¸ªæ¯ä¸ªå¼ é‡ä¸Šæ‰§è¡Œçš„ä¸åŒæ“ä½œã€‚ç„¶åä½¿ç”¨è¿™äº›è®°å½•çš„æ“ä½œæ¥åˆ›å»ºæ¨¡å‹çš„*trace*ã€‚
- en: 'The trace is created relative to the inputsâ€™ dimensions. It is therefore constrained
    by the dimensions of the dummy input, and will not work for any other sequence
    length or batch size. When trying with a different size, the following error is
    raised:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªæ˜¯ç›¸å¯¹äºè¾“å…¥ç»´åº¦åˆ›å»ºçš„ã€‚å› æ­¤ï¼Œå®ƒå—è™šæ‹Ÿè¾“å…¥ç»´åº¦çš„é™åˆ¶ï¼Œå¹¶ä¸”å¯¹äºä»»ä½•å…¶ä»–åºåˆ—é•¿åº¦æˆ–æ‰¹é‡å¤§å°éƒ½ä¸èµ·ä½œç”¨ã€‚å½“å°è¯•ä½¿ç”¨ä¸åŒå¤§å°æ—¶ï¼Œä¼šå¼•å‘ä»¥ä¸‹é”™è¯¯ï¼š
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We recommended you trace the model with a dummy input size at least as large
    as the largest input that will be fed to the model during inference. Padding can
    help fill the missing values. However, since the model is traced with a larger
    input size, the dimensions of the matrix will also be large, resulting in more
    calculations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨è‡³å°‘ä¸æ¨ç†è¿‡ç¨‹ä¸­å°†é¦ˆé€åˆ°æ¨¡å‹çš„æœ€å¤§è¾“å…¥ä¸€æ ·å¤§çš„è™šæ‹Ÿè¾“å…¥å¤§å°æ¥è·Ÿè¸ªæ¨¡å‹ã€‚å¡«å……å¯ä»¥å¸®åŠ©å¡«è¡¥ç¼ºå¤±çš„å€¼ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹æ˜¯ä½¿ç”¨è¾ƒå¤§çš„è¾“å…¥å¤§å°è·Ÿè¸ªçš„ï¼ŒçŸ©é˜µçš„ç»´åº¦ä¹Ÿä¼šå¾ˆå¤§ï¼Œå¯¼è‡´æ›´å¤šçš„è®¡ç®—ã€‚
- en: Be careful of the total number of operations done on each input and follow the
    performance closely when exporting varying sequence-length models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ³¨æ„æ¯ä¸ªè¾“å…¥ä¸Šæ‰§è¡Œçš„æ€»æ“ä½œæ•°ï¼Œå¹¶åœ¨å¯¼å‡ºä¸åŒåºåˆ—é•¿åº¦æ¨¡å‹æ—¶å¯†åˆ‡å…³æ³¨æ€§èƒ½ã€‚
- en: Using TorchScript in Python
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨Pythonä¸­ä½¿ç”¨TorchScript
- en: This section demonstrates how to save and load models as well as how to use
    the trace for inference.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ¼”ç¤ºäº†å¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨è·Ÿè¸ªè¿›è¡Œæ¨ç†ã€‚
- en: Saving a model
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¿å­˜æ¨¡å‹
- en: 'To export a `BertModel` with TorchScript, instantiate `BertModel` from the
    `BertConfig` class and then save it to disk under the filename `traced_bert.pt`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯¼å‡ºå¸¦æœ‰TorchScriptçš„`BertModel`ï¼Œè¯·ä»`BertConfig`ç±»å®ä¾‹åŒ–`BertModel`ï¼Œç„¶åå°†å…¶ä¿å­˜åˆ°ç£ç›˜ä¸Šçš„æ–‡ä»¶åä¸º`traced_bert.pt`ï¼š
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading a model
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹
- en: 'Now you can load the previously saved `BertModel`, `traced_bert.pt`, from disk
    and use it on the previously initialised `dummy_input`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ‚¨å¯ä»¥åŠ è½½å…ˆå‰ä¿å­˜çš„`BertModel`ï¼Œ`traced_bert.pt`ï¼Œä»ç£ç›˜ä¸Šå¹¶åœ¨å…ˆå‰åˆå§‹åŒ–çš„`dummy_input`ä¸Šä½¿ç”¨å®ƒï¼š
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using a traced model for inference
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è·Ÿè¸ªæ¨¡å‹è¿›è¡Œæ¨ç†
- en: 'Use the traced model for inference by using its `__call__` dunder method:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨å…¶`__call__` dunderæ–¹æ³•å¯¹æ¨ç†ä½¿ç”¨è·Ÿè¸ªæ¨¡å‹ï¼š
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Deploy Hugging Face TorchScript models to AWS with the Neuron SDK
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Neuron SDKå°†Hugging Face TorchScriptæ¨¡å‹éƒ¨ç½²åˆ°AWS
- en: 'AWS introduced the [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/)
    instance family for low cost, high performance machine learning inference in the
    cloud. The Inf1 instances are powered by the AWS Inferentia chip, a custom-built
    hardware accelerator, specializing in deep learning inferencing workloads. [AWS
    Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) is the SDK
    for Inferentia that supports tracing and optimizing transformers models for deployment
    on Inf1\. The Neuron SDK provides:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: AWSæ¨å‡ºäº†[Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/)å®ä¾‹ç³»åˆ—ï¼Œç”¨äºåœ¨äº‘ä¸­è¿›è¡Œä½æˆæœ¬ã€é«˜æ€§èƒ½çš„æœºå™¨å­¦ä¹ æ¨ç†ã€‚Inf1å®ä¾‹ç”±AWS
    InferentiaèŠ¯ç‰‡æä¾›åŠ¨åŠ›ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºæ·±åº¦å­¦ä¹ æ¨ç†å·¥ä½œè´Ÿè½½çš„å®šåˆ¶ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#)æ˜¯ç”¨äºInferentiaçš„SDKï¼Œæ”¯æŒè·Ÿè¸ªå’Œä¼˜åŒ–transformersæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨Inf1ä¸Šéƒ¨ç½²ã€‚Neuron
    SDKæä¾›ï¼š
- en: Easy-to-use API with one line of code change to trace and optimize a TorchScript
    model for inference in the cloud.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ˜“äºä½¿ç”¨çš„APIï¼Œåªéœ€æ›´æ”¹ä¸€è¡Œä»£ç å³å¯è·Ÿè¸ªå’Œä¼˜åŒ–TorchScriptæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨äº‘ä¸­è¿›è¡Œæ¨ç†ã€‚
- en: Out of the box performance optimizations for [improved cost-performance](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/%3E).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é’ˆå¯¹[æ”¹è¿›çš„æˆæœ¬æ€§èƒ½](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/%3E)è¿›è¡Œå³æ’å³ç”¨çš„æ€§èƒ½ä¼˜åŒ–ã€‚
- en: Support for Hugging Face transformers models built with either [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)
    or [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ”¯æŒä½¿ç”¨[PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)æˆ–[TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)æ„å»ºçš„Hugging
    Face transformersæ¨¡å‹ã€‚
- en: Implications
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å½±å“
- en: Transformers models based on the [BERT (Bidirectional Encoder Representations
    from Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert)
    architecture, or its variants such as [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)
    and [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)
    run best on Inf1 for non-generative tasks such as extractive question answering,
    sequence classification, and token classification. However, text generation tasks
    can still be adapted to run on Inf1 according to this [AWS Neuron MarianMT tutorial](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html).
    More information about models that can be converted out of the box on Inferentia
    can be found in the [Model Architecture Fit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia)
    section of the Neuron documentation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäº[BERTï¼ˆæ¥è‡ªTransformersçš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼‰](https://huggingface.co/docs/transformers/main/model_doc/bert)æ¶æ„çš„transformersæ¨¡å‹ï¼Œæˆ–å…¶å˜ä½“ï¼Œå¦‚[distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)å’Œ[roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)åœ¨éç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æå–å¼é—®ç­”ã€åºåˆ—åˆ†ç±»å’Œæ ‡è®°åˆ†ç±»ï¼‰ä¸Šåœ¨Inf1ä¸Šè¿è¡Œæ•ˆæœæœ€ä½³ã€‚ç„¶è€Œï¼Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä»å¯ä»¥æ ¹æ®æ­¤[AWS
    Neuron MarianMTæ•™ç¨‹](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)è¿›è¡Œé€‚åº”ä»¥åœ¨Inf1ä¸Šè¿è¡Œã€‚æœ‰å…³å¯ä»¥ç›´æ¥åœ¨Inferentiaä¸Šè½¬æ¢çš„æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…Neuronæ–‡æ¡£çš„[æ¨¡å‹æ¶æ„é€‚é…](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia)éƒ¨åˆ†ã€‚
- en: Dependencies
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¾èµ–
- en: Using AWS Neuron to convert models requires a [Neuron SDK environment](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)
    which comes preconfigured on [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨AWS Neuronè½¬æ¢æ¨¡å‹éœ€è¦ä¸€ä¸ª[Neuron SDKç¯å¢ƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)ï¼Œè¯¥ç¯å¢ƒé¢„å…ˆé…ç½®åœ¨[AWSæ·±åº¦å­¦ä¹ AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)ä¸Šã€‚
- en: Converting a model for AWS Neuron
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹è½¬æ¢ä¸ºAWS Neuron
- en: 'Convert a model for AWS NEURON using the same code from [Using TorchScript
    in Python](torchscript#using-torchscript-in-python) to trace a `BertModel`. Import
    the `torch.neuron` framework extension to access the components of the Neuron
    SDK through a Python API:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸[åœ¨Pythonä¸­ä½¿ç”¨TorchScript](torchscript#using-torchscript-in-python)ç›¸åŒçš„ä»£ç æ¥ä¸ºAWS
    NEURONè½¬æ¢æ¨¡å‹ï¼Œä»¥è·Ÿè¸ª`BertModel`ã€‚å¯¼å…¥`torch.neuron`æ¡†æ¶æ‰©å±•ä»¥é€šè¿‡Python APIè®¿é—®Neuron SDKçš„ç»„ä»¶ï¼š
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You only need to modify the following line:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åªéœ€è¦ä¿®æ”¹ä»¥ä¸‹è¡Œï¼š
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This enables the Neuron SDK to trace the model and optimize it for Inf1 instances.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—Neuron SDKèƒ½å¤Ÿè·Ÿè¸ªæ¨¡å‹å¹¶ä¸ºInf1å®ä¾‹è¿›è¡Œä¼˜åŒ–ã€‚
- en: To learn more about AWS Neuron SDK features, tools, example tutorials and latest
    updates, please see the [AWS NeuronSDK documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æœ‰å…³AWS Neuron SDKåŠŸèƒ½ã€å·¥å…·ã€ç¤ºä¾‹æ•™ç¨‹å’Œæœ€æ–°æ›´æ–°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[AWS NeuronSDKæ–‡æ¡£](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)ã€‚
