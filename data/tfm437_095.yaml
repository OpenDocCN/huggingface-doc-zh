- en: What 🤗 Transformers can do
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗 Transformers 能做什么
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/task_summary](https://huggingface.co/docs/transformers/v4.37.2/en/task_summary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/task_summary](https://huggingface.co/docs/transformers/v4.37.2/en/task_summary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🤗 Transformers is a library of pretrained state-of-the-art models for natural
    language processing (NLP), computer vision, and audio and speech processing tasks.
    Not only does the library contain Transformer models, but it also has non-Transformer
    models like modern convolutional networks for computer vision tasks. If you look
    at some of the most popular consumer products today, like smartphones, apps, and
    televisions, odds are that some kind of deep learning technology is behind it.
    Want to remove a background object from a picture taken by your smartphone? This
    is an example of a panoptic segmentation task (don’t worry if you don’t know what
    this means yet, we’ll describe it in the following sections!).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers 是一个预训练的最先进模型库，用于自然语言处理（NLP）、计算机视觉以及音频和语音处理任务。这个库不仅包含了Transformer模型，还有像现代卷积网络这样的非Transformer模型，用于计算机视觉任务。如果你看一下今天最流行的消费产品，比如智能手机、应用和电视，很可能背后都有某种深度学习技术。想要从智能手机拍摄的照片中移除背景物体？这就是一个全景分割任务的例子（如果你还不知道这是什么，不用担心，我们将在接下来的部分中描述！）。
- en: This page provides an overview of the different speech and audio, computer vision,
    and NLP tasks that can be solved with the 🤗 Transformers library in just three
    lines of code!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这个页面提供了关于🤗 Transformers库中可以用三行代码解决的不同语音和音频、计算机视觉和NLP任务的概述！
- en: Audio
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音频
- en: Audio and speech processing tasks are a little different from the other modalities
    mainly because audio as an input is a continuous signal. Unlike text, a raw audio
    waveform can’t be neatly split into discrete chunks the way a sentence can be
    divided into words. To get around this, the raw audio signal is typically sampled
    at regular intervals. If you take more samples within an interval, the sampling
    rate is higher, and the audio more closely resembles the original audio source.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和语音处理任务与其他模态有些不同，主要是因为音频作为输入是一个连续信号。与文本不同，原始音频波形不能像句子可以被分成单词那样整齐地分割。为了解决这个问题，原始音频信号通常以固定间隔进行采样。如果在一个间隔内取更多样本，采样率就更高，音频更接近原始音频源。
- en: Previous approaches preprocessed the audio to extract useful features from it.
    It is now more common to start audio and speech processing tasks by directly feeding
    the raw audio waveform to a feature encoder to extract an audio representation.
    This simplifies the preprocessing step and allows the model to learn the most
    essential features.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的方法是对音频进行预处理，从中提取有用的特征。现在更常见的做法是直接将原始音频波形输入特征编码器，以提取音频表示，开始音频和语音处理任务。这简化了预处理步骤，并允许模型学习最关键的特征。
- en: Audio classification
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频分类
- en: 'Audio classification is a task that labels audio data from a predefined set
    of classes. It is a broad category with many specific applications, some of which
    include:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 音频分类是一个从预定义类别集中为音频数据贴上标签的任务。这是一个广泛的类别，有许多具体应用，其中一些包括：
- en: 'acoustic scene classification: label audio with a scene label (“office”, “beach”,
    “stadium”)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声学场景分类：为音频贴上场景标签（“办公室”，“海滩”，“体育场”）
- en: 'acoustic event detection: label audio with a sound event label (“car horn”,
    “whale calling”, “glass breaking”)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声学事件检测：为音频贴上声音事件标签（“汽车喇叭”，“鲸鱼呼叫”，“玻璃破碎”）
- en: 'tagging: label audio containing multiple sounds (birdsongs, speaker identification
    in a meeting)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记：为包含多个声音的音频贴上标签（鸟鸣声，会议中的发言人识别）
- en: 'music classification: label music with a genre label (“metal”, “hip-hop”, “country”)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐分类：为音乐贴上流派标签（“金属”，“嘻哈”，“乡村”）
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Automatic speech recognition
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动语音识别
- en: Automatic speech recognition (ASR) transcribes speech into text. It is one of
    the most common audio tasks due partly to speech being such a natural form of
    human communication. Today, ASR systems are embedded in “smart” technology products
    like speakers, phones, and cars. We can ask our virtual assistants to play music,
    set reminders, and tell us the weather.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别（ASR）将语音转录为文本。由于语音是一种自然的人类交流形式，它是最常见的音频任务之一。今天，ASR系统嵌入在“智能”技术产品中，如扬声器、手机和汽车。我们可以要求虚拟助手播放音乐，设置提醒，并告诉我们天气。
- en: But one of the key challenges Transformer architectures have helped with is
    in low-resource languages. By pretraining on large amounts of speech data, finetuning
    the model on only one hour of labeled speech data in a low-resource language can
    still produce high-quality results compared to previous ASR systems trained on
    100x more labeled data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但Transformer架构帮助解决的关键挑战之一是低资源语言。通过在大量语音数据上进行预训练，然后在低资源语言中仅对一个小时的标记语音数据进行微调，仍然可以产生与之前在100倍更多标记数据上训练的ASR系统相比的高质量结果。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Computer vision
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: One of the first and earliest successful computer vision tasks was recognizing
    images of zip code numbers using a [convolutional neural network (CNN)](glossary#convolution).
    An image is composed of pixels, and each pixel has a numerical value. This makes
    it easy to represent an image as a matrix of pixel values. Each particular combination
    of pixel values describes the colors of an image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最早成功的计算机视觉任务之一是使用卷积神经网络（CNN）识别邮政编码数字的图像。一幅图像由像素组成，每个像素都有一个数值。这使得将图像表示为像素值矩阵变得容易。每个像素值组合描述了图像的颜色。
- en: 'Two general ways computer vision tasks can be solved are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉任务可以通过两种一般方式解决：
- en: Use convolutions to learn the hierarchical features of an image from low-level
    features to high-level abstract things.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用卷积来学习图像的层次特征，从低级特征到高级抽象事物。
- en: Split an image into patches and use a Transformer to gradually learn how each
    image patch is related to each other to form an image. Unlike the bottom-up approach
    favored by a CNN, this is kind of like starting out with a blurry image and then
    gradually bringing it into focus.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像分割成补丁，并使用Transformer逐渐学习每个图像补丁如何相互关联以形成图像。与CNN所青睐的自下而上方法不同，这有点像从模糊的图像开始，然后逐渐使其聚焦。
- en: Image classification
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像分类
- en: 'Image classification labels an entire image from a predefined set of classes.
    Like most classification tasks, there are many practical use cases for image classification,
    some of which include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类从预定义类别集合中为整个图像标记。与大多数分类任务一样，图像分类有许多实际用例，其中一些包括：
- en: 'healthcare: label medical images to detect disease or monitor patient health'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健：标记医学图像以检测疾病或监测患者健康
- en: 'environment: label satellite images to monitor deforestation, inform wildland
    management or detect wildfires'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境：标记卫星图像以监测森林砍伐，通知野地管理或检测野火
- en: 'agriculture: label images of crops to monitor plant health or satellite images
    for land use monitoring'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 农业：标记作物图像以监测植物健康或卫星图像用于土地利用监测
- en: 'ecology: label images of animal or plant species to monitor wildlife populations
    or track endangered species'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生态学：标记动物或植物物种的图像以监测野生动物种群或跟踪濒危物种
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Object detection
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标检测
- en: 'Unlike image classification, object detection identifies multiple objects within
    an image and the objects’ positions in an image (defined by the bounding box).
    Some example applications of object detection include:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像分类不同，目标检测识别图像中的多个对象以及对象在图像中的位置（由边界框定义）。目标检测的一些示例应用包括：
- en: 'self-driving vehicles: detect everyday traffic objects such as other vehicles,
    pedestrians, and traffic lights'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶车辆：检测日常交通对象，如其他车辆，行人和交通灯
- en: 'remote sensing: disaster monitoring, urban planning, and weather forecasting'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遥感：灾害监测，城市规划和天气预报
- en: 'defect detection: detect cracks or structural damage in buildings, and manufacturing
    defects'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺陷检测：检测建筑物中的裂缝或结构损坏，以及制造缺陷
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Image segmentation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像分割
- en: 'Image segmentation is a pixel-level task that assigns every pixel in an image
    to a class. It differs from object detection, which uses bounding boxes to label
    and predict objects in an image because segmentation is more granular. Segmentation
    can detect objects at a pixel-level. There are several types of image segmentation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是一个像素级任务，将图像中的每个像素分配给一个类别。它与目标检测不同，后者使用边界框来标记和预测图像中的对象，因为分割更加细粒化。分割可以在像素级别检测对象。有几种类型的图像分割：
- en: 'instance segmentation: in addition to labeling the class of an object, it also
    labels each distinct instance of an object (“dog-1”, “dog-2”)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例分割：除了标记对象的类别外，还标记对象的每个不同实例（“狗-1”，“狗-2”）
- en: 'panoptic segmentation: a combination of semantic and instance segmentation;
    it labels each pixel with a semantic class **and** each distinct instance of an
    object'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全景分割：语义和实例分割的结合；它使用语义类别标记每个像素**和**对象的每个不同实例
- en: Segmentation tasks are helpful in self-driving vehicles to create a pixel-level
    map of the world around them so they can navigate safely around pedestrians and
    other vehicles. It is also useful for medical imaging, where the task’s finer
    granularity can help identify abnormal cells or organ features. Image segmentation
    can also be used in ecommerce to virtually try on clothes or create augmented
    reality experiences by overlaying objects in the real world through your camera.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 分割任务在自动驾驶车辆中非常有用，可以为它们创建周围世界的像素级地图，以便它们可以安全地绕过行人和其他车辆。在医学成像中也很有用，任务的更细粒度可以帮助识别异常细胞或器官特征。图像分割还可以用于电子商务，通过在真实世界中通过您的相机叠加对象来虚拟试穿衣服或创建增强现实体验。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Depth estimation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度估计
- en: Depth estimation predicts the distance of each pixel in an image from the camera.
    This computer vision task is especially important for scene understanding and
    reconstruction. For example, in self-driving cars, vehicles need to understand
    how far objects like pedestrians, traffic signs, and other vehicles are to avoid
    obstacles and collisions. Depth information is also helpful for constructing 3D
    representations from 2D images and can be used to create high-quality 3D representations
    of biological structures or buildings.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 深度估计预测图像中每个像素与相机之间的距离。这种计算机视觉任务对于场景理解和重建尤为重要。例如，在自动驾驶汽车中，车辆需要了解行人、交通标志和其他车辆等物体的距离，以避免障碍和碰撞。深度信息还有助于从2D图像构建3D表示，并可用于创建生物结构或建筑物的高质量3D表示。
- en: 'There are two approaches to depth estimation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深度估计有两种方法：
- en: 'stereo: depths are estimated by comparing two images of the same image from
    slightly different angles'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 立体：通过比较略有不同角度拍摄的两幅图像来估计深度
- en: 'monocular: depths are estimated from a single image'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单眼：从单个图像估计深度
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Natural language processing
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: NLP tasks are among the most common types of tasks because text is such a natural
    way for us to communicate. To get text into a format recognized by a model, it
    needs to be tokenized. This means dividing a sequence of text into separate words
    or subwords (tokens) and then converting these tokens into numbers. As a result,
    you can represent a sequence of text as a sequence of numbers, and once you have
    a sequence of numbers, it can be input into a model to solve all sorts of NLP
    tasks!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: NLP任务是最常见的任务类型之一，因为文本是我们进行交流的一种自然方式。要将文本转换为模型识别的格式，需要对其进行标记化。这意味着将文本序列分割为单独的单词或子词（标记），然后将这些标记转换为数字。因此，您可以将文本序列表示为数字序列，一旦您有了数字序列，就可以将其输入到模型中以解决各种NLP任务！
- en: Text classification
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本分类
- en: 'Like classification tasks in any modality, text classification labels a sequence
    of text (it can be sentence-level, a paragraph, or a document) from a predefined
    set of classes. There are many practical applications for text classification,
    some of which include:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何模态中的分类任务一样，文本分类将文本序列（可以是句子级、段落或文档）从预定义类别集中标记。文本分类有许多实际应用，其中一些包括：
- en: 'sentiment analysis: label text according to some polarity like `positive` or
    `negative` which can inform and support decision-making in fields like politics,
    finance, and marketing'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析：根据`积极`或`消极`等极性为文本标记，可以在政治、金融和营销等领域支持决策
- en: 'content classification: label text according to some topic to help organize
    and filter information in news and social media feeds (`weather`, `sports`, `finance`,
    etc.)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容分类：根据某个主题为文本标记，以帮助组织和过滤新闻和社交媒体信息流中的信息（如`天气`、`体育`、`金融`等）
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Token classification
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记分类
- en: In any NLP task, text is preprocessed by separating the sequence of text into
    individual words or subwords. These are known as [tokens](glossary#token). Token
    classification assigns each token a label from a predefined set of classes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何自然语言处理任务中，文本都会被预处理，将文本序列分割成单个单词或子词。这些被称为[标记](术语表#标记)。标记分类为每个标记分配一个来自预定义类别集的标签。
- en: 'Two common types of token classification are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 两种常见的标记分类类型是：
- en: 'named entity recognition (NER): label a token according to an entity category
    like organization, person, location or date. NER is especially popular in biomedical
    settings, where it can label genes, proteins, and drug names.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）：根据实体类别（如组织、个人、地点或日期）为标记分配标签。NER在生物医学领域特别受欢迎，可以标记基因、蛋白质和药物名称。
- en: 'part-of-speech tagging (POS): label a token according to its part-of-speech
    like noun, verb, or adjective. POS is useful for helping translation systems understand
    how two identical words are grammatically different (bank as a noun versus bank
    as a verb).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注（POS）：根据其词性（名词、动词或形容词）为标记分配标签。POS对于帮助翻译系统理解两个相同单词在语法上的不同之处（名词“银行”与动词“存款”）非常有用。
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Question answering
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问答
- en: Question answering is another token-level task that returns an answer to a question,
    sometimes with context (open-domain) and other times without context (closed-domain).
    This task happens whenever we ask a virtual assistant something like whether a
    restaurant is open. It can also provide customer or technical support and help
    search engines retrieve the relevant information you’re asking for.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 问答是另一个标记级任务，它返回一个问题的答案，有时包含上下文（开放域），有时不包含上下文（封闭域）。每当我们询问虚拟助手像餐厅是否营业这样的问题时，这个任务就会发生。它还可以提供客户或技术支持，并帮助搜索引擎检索您所询问的相关信息。
- en: 'There are two common types of question answering:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 问答有两种常见类型：
- en: 'extractive: given a question and some context, the answer is a span of text
    from the context the model must extract'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽取式：给定一个问题和一些上下文，答案是模型必须从上下文中提取的文本片段
- en: 'abstractive: given a question and some context, the answer is generated from
    the context; this approach is handled by the [Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline)
    instead of the [QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline)
    shown below'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽象式：给定一个问题和一些上下文，从上下文生成答案；这种方法由[Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline)处理，而不是下面显示的[QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline)
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Summarization
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Summarization creates a shorter version of a text from a longer one while trying
    to preserve most of the meaning of the original document. Summarization is a sequence-to-sequence
    task; it outputs a shorter text sequence than the input. There are a lot of long-form
    documents that can be summarized to help readers quickly understand the main points.
    Legislative bills, legal and financial documents, patents, and scientific papers
    are a few examples of documents that could be summarized to save readers time
    and serve as a reading aid.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要创建一个较短的文本版本，从较长的文本中提取，同时尽量保留原始文档的大部分含义。摘要是一个序列到序列的任务；它输出比输入更短的文本序列。有很多长篇文档可以被摘要，以帮助读者快速理解主要观点。法案、法律和财务文件、专利和科学论文是一些可以被摘要以节省读者时间并作为阅读辅助的文档示例。
- en: 'Like question answering, there are two types of summarization:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与问答类似，摘要有两种类型：
- en: 'extractive: identify and extract the most important sentences from the original
    text'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽取式：识别并提取原始文本中最重要的句子
- en: 'abstractive: generate the target summary (which may include new words not in
    the input document) from the original text; the [SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline)
    uses the abstractive approach'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽象式：从原始文本生成目标摘要（可能包含输入文档中没有的新单词）；[SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline)使用抽象式方法
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Translation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 翻译
- en: Translation converts a sequence of text in one language to another. It is important
    in helping people from different backgrounds communicate with each other, help
    translate content to reach wider audiences, and even be a learning tool to help
    people learn a new language. Along with summarization, translation is a sequence-to-sequence
    task, meaning the model receives an input sequence and returns a target output
    sequence.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译将一种语言中的文本序列转换为另一种语言。它对帮助来自不同背景的人们相互沟通、帮助翻译内容以扩大受众、甚至作为帮助人们学习新语言的工具都非常重要。翻译与摘要一样，是一个序列到序列的任务，意味着模型接收一个输入序列并返回一个目标输出序列。
- en: In the early days, translation models were mostly monolingual, but recently,
    there has been increasing interest in multilingual models that can translate between
    many pairs of languages.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，翻译模型主要是单语的，但最近，对于能够在许多语言对之间进行翻译的多语言模型越来越感兴趣。
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Language modeling
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言建模
- en: Language modeling is a task that predicts a word in a sequence of text. It has
    become a very popular NLP task because a pretrained language model can be finetuned
    for many other downstream tasks. Lately, there has been a lot of interest in large
    language models (LLMs) which demonstrate zero- or few-shot learning. This means
    the model can solve tasks it wasn’t explicitly trained to do! Language models
    can be used to generate fluent and convincing text, though you need to be careful
    since the text may not always be accurate.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模是一种预测文本序列中单词的任务。它已经成为非常流行的自然语言处理任务，因为预训练的语言模型可以为许多其他下游任务进行微调。最近，对大型语言模型（LLMs）产生了很大兴趣，这些模型展示了零或少量样本学习。这意味着模型可以解决它没有明确训练的任务！语言模型可用于生成流畅且令人信服的文本，尽管您需要小心，因为文本可能并不总是准确的。
- en: 'There are two types of language modeling:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的语言建模：
- en: 'causal: the model’s objective is to predict the next token in a sequence, and
    future tokens are masked'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果：模型的目标是预测序列中的下一个标记，未来的标记被掩盖。
- en: '[PRE11]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'masked: the model’s objective is to predict a masked token in a sequence with
    full access to the tokens in the sequence'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩码：模型的目标是在完全访问序列中的标记的情况下预测序列中的掩码标记
- en: '[PRE12]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Multimodal
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态
- en: Multimodal tasks require a model to process multiple data modalities (text,
    image, audio, video) to solve a particular problem. Image captioning is an example
    of a multimodal task where the model takes an image as input and outputs a sequence
    of text describing the image or some properties of the image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态任务需要模型处理多种数据模态（文本、图像、音频、视频）来解决特定问题。图像字幕是一个多模态任务的例子，模型以图像作为输入，并输出描述图像或图像某些属性的文本序列。
- en: Although multimodal models work with different data types or modalities, internally,
    the preprocessing steps help the model convert all the data types into embeddings
    (vectors or list of numbers that holds meaningful information about the data).
    For a task like image captioning, the model learns relationships between image
    embeddings and text embeddings.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多模态模型处理不同的数据类型或模态，但在内部，预处理步骤帮助模型将所有数据类型转换为嵌入（包含有关数据的有意义信息的向量或数字列表）。对于像图像字幕这样的任务，模型学习图像嵌入和文本嵌入之间的关系。
- en: Document question answering
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文档问答
- en: Document question answering is a task that answers natural language questions
    from a document. Unlike a token-level question answering task which takes text
    as input, document question answering takes an image of a document as input along
    with a question about the document and returns an answer. Document question answering
    can be used to parse structured documents and extract key information from it.
    In the example below, the total amount and change due can be extracted from a
    receipt.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 文档问答是一个从文档中回答自然语言问题的任务。与以文本作为输入的标记级别问答任务不同，文档问答以文档的图像作为输入，同时提出关于文档的问题，并返回一个答案。文档问答可用于解析结构化文档并从中提取关键信息。在下面的示例中，可以从收据中提取总金额和找零金额。
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Hopefully, this page has given you some more background information about all
    the types of tasks in each modality and the practical importance of each one.
    In the next [section](tasks_explained), you’ll learn **how** 🤗 Transformers work
    to solve these tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本页为您提供了有关每种模态中所有任务类型及其实际重要性的更多背景信息。在下一节中，您将了解🤗变压器是如何解决这些任务的。
