- en: MusicGen
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MusicGen
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: The MusicGen model was proposed in the paper [Simple and Controllable Music
    Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai
    Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre DÃ©fossez.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: MusicGenæ¨¡å‹æ˜¯ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi
    Adiå’ŒAlexandre DÃ©fossezåœ¨è®ºæ–‡[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºçš„ã€‚
- en: MusicGen is a single stage auto-regressive Transformer model capable of generating
    high-quality music samples conditioned on text descriptions or audio prompts.
    The text descriptions are passed through a frozen text encoder model to obtain
    a sequence of hidden-state representations. MusicGen is then trained to predict
    discrete audio tokens, or *audio codes*, conditioned on these hidden-states. These
    audio tokens are then decoded using an audio compression model, such as EnCodec,
    to recover the audio waveform.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: MusicGenæ˜¯ä¸€ä¸ªå•é˜¶æ®µè‡ªå›å½’Transformeræ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„éŸ³ä¹æ ·æœ¬ï¼Œå…¶æ¡ä»¶æ˜¯æ–‡æœ¬æè¿°æˆ–éŸ³é¢‘æç¤ºã€‚æ–‡æœ¬æè¿°é€šè¿‡ä¸€ä¸ªå†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ä¼ é€’ï¼Œä»¥è·å¾—ä¸€ç³»åˆ—éšè—çŠ¶æ€è¡¨ç¤ºã€‚ç„¶åè®­ç»ƒMusicGenæ¥é¢„æµ‹ç¦»æ•£çš„éŸ³é¢‘æ ‡è®°ï¼Œæˆ–ç§°ä¸º*éŸ³é¢‘ä»£ç *ï¼Œè¿™äº›æ ‡è®°æ˜¯é€šè¿‡éŸ³é¢‘å‹ç¼©æ¨¡å‹ï¼ˆå¦‚EnCodecï¼‰è§£ç ä»¥æ¢å¤éŸ³é¢‘æ³¢å½¢ã€‚
- en: Through an efficient token interleaving pattern, MusicGen does not require a
    self-supervised semantic representation of the text/audio prompts, thus eliminating
    the need to cascade multiple models to predict a set of codebooks (e.g. hierarchically
    or upsampling). Instead, it is able to generate all the codebooks in a single
    forward pass.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é«˜æ•ˆçš„æ ‡è®°äº¤é”™æ¨¡å¼ï¼ŒMusicGenä¸éœ€è¦è‡ªç›‘ç£çš„æ–‡æœ¬/éŸ³é¢‘æç¤ºè¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œæ¶ˆé™¤äº†é¢„æµ‹ä¸€ç»„ç ä¹¦ï¼ˆä¾‹å¦‚åˆ†å±‚æˆ–ä¸Šé‡‡æ ·ï¼‰æ‰€éœ€çº§è”å¤šä¸ªæ¨¡å‹çš„éœ€è¦ã€‚ç›¸åï¼Œå®ƒèƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­ç”Ÿæˆæ‰€æœ‰ç ä¹¦ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*We tackle the task of conditional music generation. We introduce MusicGen,
    a single Language Model (LM) that operates over several streams of compressed
    discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised
    of a single-stage transformer LM together with efficient token interleaving patterns,
    which eliminates the need for cascading several models, e.g., hierarchically or
    upsampling. Following this approach, we demonstrate how MusicGen can generate
    high-quality samples, while being conditioned on textual description or melodic
    features, allowing better controls over the generated output. We conduct extensive
    empirical evaluation, considering both automatic and human studies, showing the
    proposed approach is superior to the evaluated baselines on a standard text-to-music
    benchmark. Through ablation studies, we shed light over the importance of each
    of the components comprising MusicGen.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬è§£å†³äº†æ¡ä»¶éŸ³ä¹ç”Ÿæˆçš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†MusicGenï¼Œä¸€ä¸ªå•ä¸€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œå®ƒåœ¨å‡ ä¸ªæµçš„å‹ç¼©ç¦»æ•£éŸ³ä¹è¡¨ç¤ºï¼ˆå³æ ‡è®°ï¼‰ä¸Šè¿è¡Œã€‚ä¸ä»¥å¾€çš„å·¥ä½œä¸åŒï¼ŒMusicGenç”±å•é˜¶æ®µTransformer
    LMå’Œé«˜æ•ˆçš„æ ‡è®°äº¤é”™æ¨¡å¼ç»„æˆï¼Œæ¶ˆé™¤äº†çº§è”å¤šä¸ªæ¨¡å‹çš„éœ€è¦ï¼Œä¾‹å¦‚åˆ†å±‚æˆ–ä¸Šé‡‡æ ·ã€‚éµå¾ªè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å±•ç¤ºäº†MusicGenå¦‚ä½•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬æè¿°æˆ–æ—‹å¾‹ç‰¹å¾çš„æ¡ä»¶ä¸‹ï¼Œå…è®¸æ›´å¥½åœ°æ§åˆ¶ç”Ÿæˆçš„è¾“å‡ºã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼Œè€ƒè™‘äº†è‡ªåŠ¨å’Œäººç±»ç ”ç©¶ï¼Œæ˜¾ç¤ºæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ ‡å‡†æ–‡æœ¬åˆ°éŸ³ä¹åŸºå‡†ä¸Šä¼˜äºè¯„ä¼°çš„åŸºçº¿ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬é˜æ˜äº†æ„æˆMusicGençš„æ¯ä¸ªç»„ä»¶çš„é‡è¦æ€§ã€‚*'
- en: This model was contributed by [sanchit-gandhi](https://huggingface.co/sanchit-gandhi).
    The original code can be found [here](https://github.com/facebookresearch/audiocraft).
    The pre-trained checkpoints can be found on the [Hugging Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen-).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç”±[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/audiocraft)æ‰¾åˆ°ã€‚é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å¯ä»¥åœ¨[Hugging
    Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen-)ä¸Šæ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: 'After downloading the original checkpoints from [here](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#importing--exporting-models)
    , you can convert them using the **conversion script** available at `src/transformers/models/musicgen/convert_musicgen_transformers.py`
    with the following command:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ä»[è¿™é‡Œ](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#importing--exporting-models)ä¸‹è½½åŸå§‹æ£€æŸ¥ç‚¹åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä½äº`src/transformers/models/musicgen/convert_musicgen_transformers.py`çš„**è½¬æ¢è„šæœ¬**è¿›è¡Œè½¬æ¢ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generation
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆ
- en: 'MusicGen is compatible with two generation modes: greedy and sampling. In practice,
    sampling leads to significantly better results than greedy, thus we encourage
    sampling mode to be used where possible. Sampling is enabled by default, and can
    be explicitly specified by setting `do_sample=True` in the call to `MusicgenForConditionalGeneration.generate()`,
    or by overriding the modelâ€™s generation config (see below).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: MusicGenå…¼å®¹ä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼šè´ªå©ªå’ŒæŠ½æ ·ã€‚å®é™…ä¸Šï¼ŒæŠ½æ ·æ¯”è´ªå©ªäº§ç”Ÿçš„ç»“æœæ˜¾è‘—æ›´å¥½ï¼Œå› æ­¤æˆ‘ä»¬é¼“åŠ±å°½å¯èƒ½ä½¿ç”¨æŠ½æ ·æ¨¡å¼ã€‚æŠ½æ ·é»˜è®¤å¯ç”¨ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨`MusicgenForConditionalGeneration.generate()`æ—¶è®¾ç½®`do_sample=True`æ¥æ˜ç¡®æŒ‡å®šï¼Œæˆ–é€šè¿‡è¦†ç›–æ¨¡å‹çš„ç”Ÿæˆé…ç½®ï¼ˆè§ä¸‹æ–‡ï¼‰æ¥æŒ‡å®šã€‚
- en: Generation is limited by the sinusoidal positional embeddings to 30 second inputs.
    Meaning, MusicGen cannot generate more than 30 seconds of audio (1503 tokens),
    and input audio passed by Audio-Prompted Generation contributes to this limit
    so, given an input of 20 seconds of audio, MusicGen cannot generate more than
    10 seconds of additional audio.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå—æ­£å¼¦ä½ç½®åµŒå…¥çš„é™åˆ¶ï¼Œè¾“å…¥é™åˆ¶ä¸º30ç§’ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒMusicGenä¸èƒ½ç”Ÿæˆè¶…è¿‡30ç§’çš„éŸ³é¢‘ï¼ˆ1503ä¸ªæ ‡è®°ï¼‰ï¼Œè¾“å…¥éŸ³é¢‘é€šè¿‡éŸ³é¢‘æç¤ºç”Ÿæˆä¹Ÿä¼šå¯¹æ­¤é™åˆ¶æœ‰æ‰€è´¡çŒ®ï¼Œå› æ­¤ï¼Œç»™å®š20ç§’çš„éŸ³é¢‘è¾“å…¥ï¼ŒMusicGenä¸èƒ½ç”Ÿæˆè¶…è¿‡é¢å¤–10ç§’çš„éŸ³é¢‘ã€‚
- en: Transformers supports both mono (1-channel) and stereo (2-channel) variants
    of MusicGen. The mono channel versions generate a single set of codebooks. The
    stereo versions generate 2 sets of codebooks, 1 for each channel (left/right),
    and each set of codebooks is decoded independently through the audio compression
    model. The audio streams for each channel are combined to give the final stereo
    output.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Transformersæ”¯æŒMusicGençš„å•å£°é“ï¼ˆ1é€šé“ï¼‰å’Œç«‹ä½“å£°ï¼ˆ2é€šé“ï¼‰å˜ä½“ã€‚å•å£°é“ç‰ˆæœ¬ç”Ÿæˆä¸€ç»„ä»£ç ä¹¦ã€‚ç«‹ä½“å£°ç‰ˆæœ¬ç”Ÿæˆ2ç»„ä»£ç ä¹¦ï¼Œæ¯ä¸ªé€šé“ï¼ˆå·¦/å³ï¼‰å„ä¸€ä¸ªï¼Œå¹¶ä¸”æ¯ç»„ä»£ç ä¹¦é€šè¿‡éŸ³é¢‘å‹ç¼©æ¨¡å‹ç‹¬ç«‹è§£ç ã€‚æ¯ä¸ªé€šé“çš„éŸ³é¢‘æµåˆå¹¶ä»¥äº§ç”Ÿæœ€ç»ˆçš„ç«‹ä½“å£°è¾“å‡ºã€‚
- en: Unconditional Generation
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ— æ¡ä»¶ç”Ÿæˆ
- en: 'The inputs for unconditional (or â€˜nullâ€™) generation can be obtained through
    the method `MusicgenForConditionalGeneration.get_unconditional_inputs()`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ— æ¡ä»¶ï¼ˆæˆ–'null'ï¼‰ç”Ÿæˆçš„è¾“å…¥å¯ä»¥é€šè¿‡æ–¹æ³•`MusicgenForConditionalGeneration.get_unconditional_inputs()`è·å¾—ï¼š
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The audio outputs are a three-dimensional Torch tensor of shape `(batch_size,
    num_channels, sequence_length)`. To listen to the generated audio samples, you
    can either play them in an ipynb notebook:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘è¾“å‡ºæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_channels, sequence_length)`çš„ä¸‰ç»´Torchå¼ é‡ã€‚è¦å¬ç”Ÿæˆçš„éŸ³é¢‘æ ·æœ¬ï¼Œå¯ä»¥åœ¨ipynbç¬”è®°æœ¬ä¸­æ’­æ”¾å®ƒä»¬ï¼š
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or save them as a `.wav` file using a third-party library, e.g. `scipy`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“ï¼ˆä¾‹å¦‚`scipy`ï¼‰å°†å®ƒä»¬ä¿å­˜ä¸º`.wav`æ–‡ä»¶ï¼š
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Text-Conditional Generation
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡æœ¬æ¡ä»¶ç”Ÿæˆ
- en: 'The model can generate an audio sample conditioned on a text prompt through
    use of the [MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)
    to pre-process the inputs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¯ä»¥é€šè¿‡ä½¿ç”¨[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)é¢„å¤„ç†è¾“å…¥æ¥ç”Ÿæˆå—æ–‡æœ¬æç¤ºæ¡ä»¶çš„éŸ³é¢‘æ ·æœ¬ï¼š
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `guidance_scale` is used in classifier free guidance (CFG), setting the
    weighting between the conditional logits (which are predicted from the text prompts)
    and the unconditional logits (which are predicted from an unconditional or â€˜nullâ€™
    prompt). Higher guidance scale encourages the model to generate samples that are
    more closely linked to the input prompt, usually at the expense of poorer audio
    quality. CFG is enabled by setting `guidance_scale > 1`. For best results, use
    `guidance_scale=3` (default).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`guidance_scale`ç”¨äºåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆCFGï¼‰ï¼Œè®¾ç½®æ¡ä»¶å¯¹æ•°ï¼ˆä»æ–‡æœ¬æç¤ºé¢„æµ‹ï¼‰å’Œæ— æ¡ä»¶å¯¹æ•°ï¼ˆä»æ— æ¡ä»¶æˆ–''null''æç¤ºé¢„æµ‹ï¼‰ä¹‹é—´çš„æƒé‡ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ä¸è¾“å…¥æç¤ºå¯†åˆ‡ç›¸å…³çš„æ ·æœ¬ï¼Œé€šå¸¸ä»¥éŸ³é¢‘è´¨é‡è¾ƒå·®ä¸ºä»£ä»·ã€‚é€šè¿‡è®¾ç½®`guidance_scale
    > 1`å¯ç”¨CFGã€‚ä¸ºè·å¾—æœ€ä½³ç»“æœï¼Œè¯·ä½¿ç”¨`guidance_scale=3`ï¼ˆé»˜è®¤å€¼ï¼‰ã€‚'
- en: Audio-Prompted Generation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: éŸ³é¢‘æç¤ºç”Ÿæˆ
- en: 'The same [MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)
    can be used to pre-process an audio prompt that is used for audio continuation.
    In the following example, we load an audio file using the ğŸ¤— Datasets library,
    which can be pip installed through the command below:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åŒçš„[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)å¯ç”¨äºé¢„å¤„ç†ç”¨äºéŸ³é¢‘å»¶ç»­çš„éŸ³é¢‘æç¤ºã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ğŸ¤—
    Datasetsåº“åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œpipå®‰è£…ï¼š
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For batched audio-prompted generation, the generated `audio_values` can be
    post-processed to remove padding by using the [MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)
    class:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰¹é‡éŸ³é¢‘æç¤ºç”Ÿæˆï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)ç±»å¯¹ç”Ÿæˆçš„`audio_values`è¿›è¡Œåå¤„ç†ï¼Œä»¥å»é™¤å¡«å……ï¼š
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Generation Configuration
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿæˆé…ç½®
- en: 'The default parameters that control the generation process, such as sampling,
    guidance scale and number of generated tokens, can be found in the modelâ€™s generation
    config, and updated as desired:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹çš„é»˜è®¤å‚æ•°ï¼Œä¾‹å¦‚é‡‡æ ·ã€å¼•å¯¼æ¯”ä¾‹å’Œç”Ÿæˆçš„æ ‡è®°æ•°é‡ï¼Œå¯ä»¥åœ¨æ¨¡å‹çš„ç”Ÿæˆé…ç½®ä¸­æ‰¾åˆ°ï¼Œå¹¶æ ¹æ®éœ€è¦è¿›è¡Œæ›´æ–°ï¼š
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that any arguments passed to the generate method will **supersede** those
    in the generation config, so setting `do_sample=False` in the call to generate
    will supersede the setting of `model.generation_config.do_sample` in the generation
    config.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¼ é€’ç»™ç”Ÿæˆæ–¹æ³•çš„ä»»ä½•å‚æ•°éƒ½å°†**è¦†ç›–**ç”Ÿæˆé…ç½®ä¸­çš„å‚æ•°ï¼Œå› æ­¤åœ¨è°ƒç”¨ç”Ÿæˆæ—¶è®¾ç½®`do_sample=False`å°†è¦†ç›–ç”Ÿæˆé…ç½®ä¸­`model.generation_config.do_sample`çš„è®¾ç½®ã€‚
- en: Model Structure
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹ç»“æ„
- en: 'The MusicGen model can be de-composed into three distinct stages:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MusicGenæ¨¡å‹å¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªä¸åŒçš„é˜¶æ®µï¼š
- en: 'Text encoder: maps the text inputs to a sequence of hidden-state representations.
    The pre-trained MusicGen models use a frozen text encoder from either T5 or Flan-T5'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç¼–ç å™¨ï¼šå°†æ–‡æœ¬è¾“å…¥æ˜ å°„åˆ°ä¸€ç³»åˆ—éšè—çŠ¶æ€è¡¨ç¤ºã€‚é¢„è®­ç»ƒçš„MusicGenæ¨¡å‹ä½¿ç”¨æ¥è‡ªT5æˆ–Flan-T5çš„å†»ç»“æ–‡æœ¬ç¼–ç å™¨
- en: 'MusicGen decoder: a language model (LM) that auto-regressively generates audio
    tokens (or codes) conditional on the encoder hidden-state representations'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MusicGenè§£ç å™¨ï¼šä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œæ ¹æ®ç¼–ç å™¨éšè—çŠ¶æ€è¡¨ç¤ºè‡ªå›å½’ç”ŸæˆéŸ³é¢‘æ ‡è®°ï¼ˆæˆ–ä»£ç ï¼‰
- en: 'Audio encoder/decoder: used to encode an audio prompt to use as prompt tokens,
    and recover the audio waveform from the audio tokens predicted by the decoder'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨ï¼šç”¨äºå°†éŸ³é¢‘æç¤ºç¼–ç ä¸ºæç¤ºæ ‡è®°ï¼Œå¹¶é€šè¿‡è§£ç å™¨é¢„æµ‹çš„éŸ³é¢‘æ ‡è®°æ¢å¤éŸ³é¢‘æ³¢å½¢
- en: 'Thus, the MusicGen model can either be used as a standalone decoder model,
    corresponding to the class [MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM),
    or as a composite model that includes the text encoder and audio encoder/decoder,
    corresponding to the class [MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration).
    If only the decoder needs to be loaded from the pre-trained checkpoint, it can
    be loaded by first specifying the correct config, or be accessed through the `.decoder`
    attribute of the composite model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒMusicGenæ¨¡å‹å¯ä»¥ä½œä¸ºç‹¬ç«‹çš„è§£ç å™¨æ¨¡å‹ä½¿ç”¨ï¼Œå¯¹åº”äºç±»[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)ï¼Œæˆ–ä½œä¸ºåŒ…å«æ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨çš„å¤åˆæ¨¡å‹ä½¿ç”¨ï¼Œå¯¹åº”äºç±»[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)ã€‚å¦‚æœåªéœ€ä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹åŠ è½½è§£ç å™¨ï¼Œåˆ™å¯ä»¥é¦–å…ˆæŒ‡å®šæ­£ç¡®çš„é…ç½®ï¼Œæˆ–é€šè¿‡å¤åˆæ¨¡å‹çš„`.decoder`å±æ€§è®¿é—®ï¼š
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since the text encoder and audio encoder/decoder models are frozen during training,
    the MusicGen decoder [MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)
    can be trained standalone on a dataset of encoder hidden-states and audio codes.
    For inference, the trained decoder can be combined with the frozen text encoder
    and audio encoder/decoders to recover the composite [MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)
    model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´è¢«å†»ç»“ï¼ŒMusicGenè§£ç å™¨[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)å¯ä»¥åœ¨ç¼–ç å™¨éšè—çŠ¶æ€å’ŒéŸ³é¢‘ä»£ç çš„æ•°æ®é›†ä¸Šç‹¬ç«‹è®­ç»ƒã€‚å¯¹äºæ¨æ–­ï¼Œè®­ç»ƒå¥½çš„è§£ç å™¨å¯ä»¥ä¸å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨ç»“åˆï¼Œä»¥æ¢å¤å¤åˆ[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)æ¨¡å‹ã€‚
- en: 'Tips:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼š
- en: MusicGen is trained on the 32kHz checkpoint of Encodec. You should ensure you
    use a compatible version of the Encodec model.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MusicGenæ˜¯åœ¨Encodecçš„32kHzæ£€æŸ¥ç‚¹ä¸Šè®­ç»ƒçš„ã€‚æ‚¨åº”ç¡®ä¿ä½¿ç”¨Encodecæ¨¡å‹çš„å…¼å®¹ç‰ˆæœ¬ã€‚
- en: Sampling mode tends to deliver better results than greedy - you can toggle sampling
    with the variable `do_sample` in the call to `MusicgenForConditionalGeneration.generate()`
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡‡æ ·æ¨¡å¼å¾€å¾€æ¯”è´ªå©ªæ¨¡å¼æä¾›æ›´å¥½çš„ç»“æœ - æ‚¨å¯ä»¥åœ¨è°ƒç”¨`MusicgenForConditionalGeneration.generate()`æ—¶ä½¿ç”¨å˜é‡`do_sample`åˆ‡æ¢é‡‡æ ·ã€‚
- en: MusicgenDecoderConfig
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicgenDecoderConfig
- en: '### `class transformers.MusicgenDecoderConfig`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MusicgenDecoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L30)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L30)'
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 2048) â€” Vocabulary size of the
    MusicgenDecoder model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling `MusicgenDecoder`.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º2048ï¼‰â€” MusicgenDecoderæ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨`MusicgenDecoder`æ—¶ä¼ é€’çš„`inputs_ids`å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 1024) â€” Dimensionality of the
    layers and the pooler layer.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1024ï¼‰â€” å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 24) â€” Number of decoder
    layers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º24ï¼‰â€” è§£ç å™¨å±‚æ•°ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 16) â€” Number of attention
    heads for each attention layer in the Transformer block.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º16ï¼‰â€” Transformerå—ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`ffn_dim` (`int`, *optional*, defaults to 4096) â€” Dimensionality of the â€œintermediateâ€
    (often named feed-forward) layer in the Transformer block.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ffn_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4096ï¼‰â€” Transformerå—ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    â€” The non-linear activation function (function or string) in the decoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function`ï¼ˆ`str`æˆ–`function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"gelu"`ï¼‰â€” è§£ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for all fully connected layers in the embeddings, text_encoder, and pooler.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1ï¼‰â€” åµŒå…¥ã€æ–‡æœ¬ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for the attention probabilities.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” å…¨è¿æ¥å±‚å†…éƒ¨æ¿€æ´»çš„dropoutæ¯”ç‡ã€‚'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 2048) â€” The maximum
    sequence length that this model might ever be used with. Typically, set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º2048ï¼‰â€” è¯¥æ¨¡å‹å¯èƒ½è¢«ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå°†å…¶è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚512ã€1024æˆ–2048ï¼‰ã€‚'
- en: '`initializer_factor` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.02ï¼‰â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.0) â€” The LayerDrop probability
    for the decoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” è§£ç å™¨çš„LayerDropæ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LayerDropè®ºæ–‡](è§[https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚'
- en: '`scale_embedding` (`bool`, *optional*, defaults to `False`) â€” Scale embeddings
    by diving by sqrt(hidden_size).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_embedding`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” é€šè¿‡å°†å…¶é™¤ä»¥sqrt(hidden_size)æ¥ç¼©æ”¾åµŒå…¥ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether the model should
    return the last key/values attentions (not used by all models)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚'
- en: '`num_codebooks` (`int`, *optional*, defaults to 4) â€” The number of parallel
    codebooks forwarded to the model.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codebooks`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4ï¼‰â€” è½¬å‘åˆ°æ¨¡å‹çš„å¹¶è¡Œç ä¹¦çš„æ•°é‡ã€‚'
- en: '`tie_word_embeddings(bool,` *optional*, defaults to `False`) â€” Whether input
    and output word embeddings should be tied.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tie_word_embeddings(bool,` *å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” è¾“å…¥å’Œè¾“å‡ºè¯åµŒå…¥æ˜¯å¦åº”è¯¥ç»‘å®šã€‚'
- en: '`audio_channels` (`int`, *optional*, defaults to 1 â€” Number of channels in
    the audio data. Either 1 for mono or 2 for stereo. Stereo models generate a separate
    audio stream for the left/right output channels. Mono models generate a single
    audio stream output.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” éŸ³é¢‘æ•°æ®ä¸­çš„é€šé“æ•°ã€‚å•å£°é“ä¸º1ï¼Œç«‹ä½“å£°ä¸º2ã€‚ç«‹ä½“å£°æ¨¡å‹ä¸ºå·¦/å³è¾“å‡ºé€šé“ç”Ÿæˆå•ç‹¬çš„éŸ³é¢‘æµã€‚å•å£°é“æ¨¡å‹ç”Ÿæˆå•ä¸ªéŸ³é¢‘æµè¾“å‡ºã€‚'
- en: This is the configuration class to store the configuration of an `MusicgenDecoder`.
    It is used to instantiate a MusicGen decoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the MusicGen [facebook/musicgen-small](https://huggingface.co/facebook/musicgen-small)
    architecture.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨ `MusicgenDecoder` é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª MusicGen è§£ç å™¨ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº
    MusicGen [facebook/musicgen-small](https://huggingface.co/facebook/musicgen-small)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: MusicgenConfig
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicgenConfig
- en: '### `class transformers.MusicgenConfig`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MusicgenConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L139)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L139)'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`kwargs` (*optional*) â€” Dictionary of keyword arguments. Notably:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) â€” å…³é”®å­—å‚æ•°çš„å­—å…¸ã€‚ç‰¹åˆ«æ˜¯ï¼š'
- en: '`text_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) â€” An instance of a configuration object that defines the text encoder
    config.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) â€” å®šä¹‰æ–‡æœ¬ç¼–ç å™¨é…ç½®çš„é…ç½®å¯¹è±¡å®ä¾‹ã€‚'
- en: '`audio_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) â€” An instance of a configuration object that defines the audio encoder
    config.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) â€” å®šä¹‰éŸ³é¢‘ç¼–ç å™¨é…ç½®çš„é…ç½®å¯¹è±¡å®ä¾‹ã€‚'
- en: '`decoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) â€” An instance of a configuration object that defines the decoder config.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) â€” å®šä¹‰è§£ç å™¨é…ç½®çš„é…ç½®å¯¹è±¡å®ä¾‹ã€‚'
- en: This is the configuration class to store the configuration of a [MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel).
    It is used to instantiate a MusicGen model according to the specified arguments,
    defining the text encoder, audio encoder and MusicGen decoder configs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨ [MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel)
    é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª MusicGen æ¨¡å‹ï¼Œå®šä¹‰æ–‡æœ¬ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨å’Œ MusicGen è§£ç å™¨é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#### `from_sub_models_config`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_sub_models_config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L217)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L217)'
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Returns
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)'
- en: An instance of a configuration object
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡å®ä¾‹
- en: Instantiate a [MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)
    (or a derived class) from text encoder, audio encoder and decoder configurations.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ–‡æœ¬ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨å’Œè§£ç å™¨é…ç½®å®ä¾‹åŒ–ä¸€ä¸ª [MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚
- en: MusicgenProcessor
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicgenProcessor
- en: '### `class transformers.MusicgenProcessor`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MusicgenProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L26)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L26)'
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`feature_extractor` (`EncodecFeatureExtractor`) â€” An instance of [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` (`EncodecFeatureExtractor`) â€” ä¸€ä¸ª [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor)
    çš„å®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯ä¸€ä¸ªå¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` (`T5Tokenizer`) â€” An instance of [T5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer).
    The tokenizer is a required input.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`T5Tokenizer`) â€” ä¸€ä¸ª [T5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer)
    çš„å®ä¾‹ã€‚è¿™æ˜¯ä¸€ä¸ªå¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a MusicGen processor which wraps an EnCodec feature extractor and
    a T5 tokenizer into a single processor class.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª MusicGen å¤„ç†å™¨ï¼Œå°† EnCodec ç‰¹å¾æå–å™¨å’Œ T5 åˆ†è¯å™¨å°è£…æˆä¸€ä¸ªå•ä¸€çš„å¤„ç†å™¨ç±»ã€‚
- en: '[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)
    offers all the functionalities of [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor)
    and `TTokenizer`. See `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor.decode)
    for more information.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)
    æä¾›äº† [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor)
    å’Œ `TTokenizer` çš„æ‰€æœ‰åŠŸèƒ½ã€‚æŸ¥çœ‹ `__call__()` å’Œ [decode()](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor.decode)
    è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: '#### `batch_decode`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L90)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L90)'
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This method is used to decode either batches of audio outputs from the MusicGen
    model, or batches of token ids from the tokenizer. In the case of decoding token
    ids, this method forwards all its arguments to T5Tokenizerâ€™s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•ç”¨äºè§£ç æ¥è‡ªMusicGenæ¨¡å‹çš„éŸ³é¢‘è¾“å‡ºæ‰¹æ¬¡æˆ–æ¥è‡ªæ ‡è®°å™¨çš„æ ‡è®°idæ‰¹æ¬¡ã€‚åœ¨è§£ç æ ‡è®°idçš„æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°T5Tokenizerçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `decode`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L108)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L108)'
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This method forwards all its arguments to T5Tokenizerâ€™s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°T5Tokenizerçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: MusicgenModel
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicgenModel
- en: '### `class transformers.MusicgenModel`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MusicgenModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L835)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L835)'
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare Musicgen decoder model outputting raw hidden-states without any specific
    head on top.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„Musicgenè§£ç å™¨æ¨¡å‹ï¼Œè¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚
- en: The Musicgen model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)
    by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve,
    Yossi Adi, Alexandre DÃ©fossez. It is an encoder decoder transformer trained on
    the task of conditional music generation
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Musicgenæ¨¡å‹æ˜¯ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi
    Adiã€Alexandre DÃ©fossezåœ¨[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ¡ä»¶éŸ³ä¹ç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L855)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L855)'
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary, corresponding to the sequence
    of audio codes.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size * num_codebooks, sequence_length)`çš„`torch.LongTensor`ï¼‰â€”
    è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ï¼Œå¯¹åº”äºéŸ³é¢‘ä»£ç åºåˆ—ã€‚'
- en: Indices can be obtained by encoding an audio prompt with an audio encoder model
    to predict audio codes, such as with the [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel).
    See [EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)
    for details.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹å¯¹éŸ³é¢‘æç¤ºè¿›è¡Œç¼–ç ä»¥é¢„æµ‹éŸ³é¢‘ä»£ç ï¼Œå¯ä»¥è·å¾—ç´¢å¼•ï¼Œä¾‹å¦‚ä½¿ç”¨[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: The `input_ids` will automatically be converted from shape `(batch_size * num_codebooks,
    target_sequence_length)` to `(batch_size, num_codebooks, target_sequence_length)`
    in the forward pass. If you obtain audio codes from an audio encoding model, such
    as [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel),
    ensure that the number of frames is equal to 1, and that you reshape the audio
    codes from `(frames, batch_size, num_codebooks, target_sequence_length)` to `(batch_size
    * num_codebooks, target_sequence_length)` prior to passing them as `input_ids`.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`input_ids`å°†åœ¨å‰å‘ä¼ é€’ä¸­è‡ªåŠ¨ä»å½¢çŠ¶`(batch_size * num_codebooks, target_sequence_length)`è½¬æ¢ä¸º`(batch_size,
    num_codebooks, target_sequence_length)`ã€‚å¦‚æœæ‚¨ä»éŸ³é¢‘ç¼–ç æ¨¡å‹ï¼ˆå¦‚[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ï¼‰è·å–éŸ³é¢‘ä»£ç ï¼Œè¯·ç¡®ä¿å¸§æ•°ç­‰äº1ï¼Œå¹¶ä¸”åœ¨å°†å…¶ä½œä¸º`input_ids`ä¼ é€’ä¹‹å‰ï¼Œå°†éŸ³é¢‘ä»£ç ä»`(frames,
    batch_size, num_codebooks, target_sequence_length)`é‡å¡‘ä¸º`(batch_size * num_codebooks,
    target_sequence_length)`ã€‚'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention of the decoder.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—çš„è¾“å‡ºã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`encoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`,
    *optional*) â€” Mask to avoid performing cross-attention on padding tokens indices
    of encoder input_ids. Mask values selected in `[0, 1]`:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥æ ‡è®°çš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules in
    the decoder to avoid performing cross-attention on hidden heads. Mask values selected
    in `[0, 1]`:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ï¼Œä»¥é¿å…åœ¨éšè—å¤´éƒ¨ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€”
    é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: The [MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel)
    forward method, overrides the `__call__` special method.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: MusicgenForCausalLM
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicgenForCausalLM
- en: '### `class transformers.MusicgenForCausalLM`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MusicgenForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L906)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L906)'
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰â€”
    åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The MusicGen decoder model with a language modelling head on top.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: MusicGenè§£ç å™¨æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ã€‚
- en: The Musicgen model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)
    by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve,
    Yossi Adi, Alexandre DÃ©fossez. It is an encoder decoder transformer trained on
    the task of conditional music generation
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Musicgenæ¨¡å‹ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi
    Adiã€Alexandre DÃ©fossezåœ¨[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºã€‚è¿™æ˜¯ä¸€ä¸ªåœ¨æ¡ä»¶éŸ³ä¹ç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“å®ç°çš„é€šç”¨æ–¹æ³•ï¼Œä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L942)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L942)'
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary, corresponding to the sequence
    of audio codes.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size * num_codebooks, sequence_length)`çš„`torch.LongTensor`ï¼‰â€”
    è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ï¼Œå¯¹åº”äºéŸ³é¢‘ä»£ç åºåˆ—ã€‚'
- en: Indices can be obtained by encoding an audio prompt with an audio encoder model
    to predict audio codes, such as with the [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel).
    See [EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)
    for details.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡ä½¿ç”¨éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹å¯¹éŸ³é¢‘æç¤ºè¿›è¡Œç¼–ç æ¥è·å–ç´¢å¼•ï¼Œä»¥é¢„æµ‹éŸ³é¢‘ä»£ç ï¼Œä¾‹å¦‚ä½¿ç”¨[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ã€‚æŸ¥çœ‹[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)è·å–è¯¦ç»†ä¿¡æ¯ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: The `input_ids` will automatically be converted from shape `(batch_size * num_codebooks,
    target_sequence_length)` to `(batch_size, num_codebooks, target_sequence_length)`
    in the forward pass. If you obtain audio codes from an audio encoding model, such
    as [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel),
    ensure that the number of frames is equal to 1, and that you reshape the audio
    codes from `(frames, batch_size, num_codebooks, target_sequence_length)` to `(batch_size
    * num_codebooks, target_sequence_length)` prior to passing them as `input_ids`.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`input_ids`å°†åœ¨å‰å‘ä¼ é€’ä¸­è‡ªåŠ¨ä»å½¢çŠ¶`(batch_size * num_codebooks, target_sequence_length)`è½¬æ¢ä¸º`(batch_size,
    num_codebooks, target_sequence_length)`ã€‚å¦‚æœæ‚¨ä»éŸ³é¢‘ç¼–ç æ¨¡å‹ï¼ˆä¾‹å¦‚[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ï¼‰è·å–éŸ³é¢‘ä»£ç ï¼Œè¯·ç¡®ä¿å¸§æ•°ç­‰äº1ï¼Œå¹¶ä¸”åœ¨å°†éŸ³é¢‘ä»£ç ä»`(frames,
    batch_size, num_codebooks, target_sequence_length)`é‡å¡‘ä¸º`(batch_size * num_codebooks,
    target_sequence_length)`ä¹‹å‰ï¼Œå°†å…¶ä½œä¸º`input_ids`ä¼ é€’ã€‚'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤º`æœªè¢«æ©ç›–`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention of the decoder.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`encoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`,
    *optional*) â€” Mask to avoid performing cross-attention on padding tokens indices
    of encoder input_ids. Mask values selected in `[0, 1]`:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥IDçš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤º`æœªè¢«æ©ç›–`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç›–`ã€‚
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules in
    the decoder to avoid performing cross-attention on hidden heads. Mask values selected
    in `[0, 1]`:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” ç”¨äºå°†è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ï¼Œä»¥é¿å…åœ¨éšè—å¤´éƒ¨ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„æœ€å`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” ç”¨äºè¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹å†…éƒ¨**ç§»åŠ¨**æ ‡ç­¾ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels = input_ids`ã€‚ç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆè¢«`masked`ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0,
    ..., config.vocab_size]`ä¸­çš„æ ‡ç­¾ã€‚'
- en: 'Returns: [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig))
    and inputs.'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è¿”å›ï¼š[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–`tuple(torch.FloatTensor)`ï¼šä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼Œæˆ–è€…å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚ '
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss.'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€”
    è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´éƒ¨çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+
    ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—è¾“å‡ºã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+
    ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: MusicgenForConditionalGeneration
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicgenForConditionalGeneration
- en: '### `class transformers.MusicgenForConditionalGeneration`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MusicgenForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1404)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1404)'
- en: '[PRE21]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The composite MusicGen model with a text encoder, audio encoder and Musicgen
    decoder, for music generation tasks with one or both of text and audio prompts.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰æ–‡æœ¬ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨å’ŒMusicgenè§£ç å™¨çš„å¤åˆMusicGenæ¨¡å‹ï¼Œç”¨äºå…·æœ‰æ–‡æœ¬å’Œ/æˆ–éŸ³é¢‘æç¤ºçš„éŸ³ä¹ç”Ÿæˆä»»åŠ¡ã€‚
- en: The Musicgen model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)
    by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve,
    Yossi Adi, Alexandre DÃ©fossez. It is an encoder decoder transformer trained on
    the task of conditional music generation
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Musicgenæ¨¡å‹ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi
    Adiã€Alexandre DÃ©fossezåœ¨[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ¡ä»¶éŸ³ä¹ç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨transformerã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚è¯·æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1760)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1760)'
- en: '[PRE22]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¦‚æœæä¾›å¡«å……ï¼Œåˆ™é»˜è®¤å°†å¿½ç•¥ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0,
    1]`ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ã€‚
- en: 0 for tokens that are `masked`.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size * num_codebooks,
    target_sequence_length)`, *optional*) â€” Indices of decoder input sequence tokens
    in the vocabulary, corresponding to the sequence of audio codes.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size * num_codebooks, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ï¼Œå¯¹åº”äºéŸ³é¢‘ä»£ç åºåˆ—ã€‚'
- en: Indices can be obtained by encoding an audio prompt with an audio encoder model
    to predict audio codes, such as with the [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel).
    See [EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)
    for details.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡ä½¿ç”¨éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹å¯¹éŸ³é¢‘æç¤ºè¿›è¡Œç¼–ç ä»¥é¢„æµ‹éŸ³é¢‘ä»£ç æ¥è·å–ç´¢å¼•ï¼Œä¾‹å¦‚ä½¿ç”¨[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)ã€‚
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)'
- en: The `decoder_input_ids` will automatically be converted from shape `(batch_size
    * num_codebooks, target_sequence_length)` to `(batch_size, num_codebooks, target_sequence_length)`
    in the forward pass. If you obtain audio codes from an audio encoding model, such
    as [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel),
    ensure that the number of frames is equal to 1, and that you reshape the audio
    codes from `(frames, batch_size, num_codebooks, target_sequence_length)` to `(batch_size
    * num_codebooks, target_sequence_length)` prior to passing them as `decoder_input_ids`.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`å°†åœ¨å‰å‘ä¼ é€’ä¸­è‡ªåŠ¨ä»å½¢çŠ¶`(batch_size * num_codebooks, target_sequence_length)`è½¬æ¢ä¸º`(batch_size,
    num_codebooks, target_sequence_length)`ã€‚å¦‚æœæ‚¨ä»éŸ³é¢‘ç¼–ç æ¨¡å‹ï¼ˆå¦‚[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ï¼‰è·å–éŸ³é¢‘ä»£ç ï¼Œè¯·ç¡®ä¿å¸§æ•°ç­‰äº1ï¼Œå¹¶ä¸”åœ¨å°†éŸ³é¢‘ä»£ç ä»`(frames,
    batch_size, num_codebooks, target_sequence_length)`é‡å¡‘ä¸º`(batch_size * num_codebooks,
    target_sequence_length)`ä¹‹å‰ï¼Œå°†å…¶ä½œä¸º`decoder_input_ids`ä¼ é€’ã€‚'
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ã€‚'
- en: 1 indicates the head is `not masked`,
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è§£ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ã€‚'
- en: 1 indicates the head is `not masked`,
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules in
    the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” åœ¨è§£ç å™¨ä¸­å°†äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`,
    *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*optional*)æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«å¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚è¿™å¾ˆæœ‰ç”¨ï¼Œå¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig))
    and inputs.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€”
    è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰-
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰-
    é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ªå’Œæ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¯å±‚è§£ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰-
    æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ªå’Œæ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¯å±‚ç¼–ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE23]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
