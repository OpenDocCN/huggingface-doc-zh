- en: Brief introduction to RL documentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RL文档简介
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/rl-documentation](https://huggingface.co/learn/deep-rl-course/unitbonus3/rl-documentation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unitbonus3/rl-documentation](https://huggingface.co/learn/deep-rl-course/unitbonus3/rl-documentation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this advanced topic, we address the question: **how should we monitor and
    keep track of powerful reinforcement learning agents that we are training in the
    real world and interfacing with humans?**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个高级主题中，我们探讨一个问题：**我们应该如何监控和跟踪我们在现实世界中训练并与人类交互的强化学习代理？**
- en: As machine learning systems have increasingly impacted modern life, the **call
    for the documentation of these systems has grown**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统对现代生活的影响日益增加，**对这些系统的文档化需求也在增长**。
- en: Such documentation can cover aspects such as the training data used — where
    it is stored, when it was collected, who was involved, etc. — or the model optimization
    framework — the architecture, evaluation metrics, relevant papers, etc. — and
    more.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的文档可以涵盖诸如使用的训练数据 - 存储位置、收集时间、参与者等 - 或模型优化框架 - 架构、评估指标、相关论文等 - 等方面。
- en: Today, model cards and datasheets are becoming increasingly available. For example,
    on the Hub (see documentation [here](https://huggingface.co/docs/hub/model-cards)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，模型卡片和数据表格越来越普遍。例如，在Hub上（查看文档[这里](https://huggingface.co/docs/hub/model-cards)）。
- en: If you click on a [popular model on the Hub](https://huggingface.co/models),
    you can learn about its creation process.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您点击[Hub上的热门模型](https://huggingface.co/models)，您可以了解其创建过程。
- en: These model and data specific logs are designed to be completed when the model
    or dataset are created, leaving them to go un-updated when these models are built
    into evolving systems in the future. ​
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特定于模型和数据的日志被设计为在创建模型或数据集时完成，当这些模型被构建到未来的演变系统中时，它们将不再更新。​
- en: Motivating Reward Reports
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激励奖励报告
- en: Reinforcement learning systems are fundamentally designed to optimize based
    on measurements of reward and time. While the notion of a reward function can
    be mapped nicely to many well-understood fields of supervised learning (via a
    loss function), understanding of how machine learning systems evolve over time
    is limited.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习系统基本上是为了基于奖励和时间的测量进行优化而设计的。虽然奖励函数的概念可以很好地映射到许多众所周知的监督学习领域（通过损失函数），但对机器学习系统随时间如何演变的理解是有限的。
- en: To that end, the authors introduce [*Reward Reports for Reinforcement Learning*](https://www.notion.so/Brief-introduction-to-RL-documentation-b8cbda5a6f5242338e0756e6bef72af4)
    (the pithy naming is designed to mirror the popular papers *Model Cards for Model
    Reporting* and *Datasheets for Datasets*). The goal is to propose a type of documentation
    focused on the **human factors of reward** and **time-varying feedback systems**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，作者们介绍了《强化学习奖励报告》（简洁的命名旨在模仿流行论文《模型报告的模型卡片》和《数据集的数据表格》）。目标是提出一种关注**奖励的人类因素**和**时变反馈系统**的文档类型。
- en: Building on the documentation frameworks for [model cards](https://arxiv.org/abs/1810.03993)
    and [datasheets](https://arxiv.org/abs/1803.09010) proposed by Mitchell et al.
    and Gebru et al., we argue the need for Reward Reports for AI systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mitchell等人和Gebru等人提出的[模型卡片](https://arxiv.org/abs/1810.03993)和[数据表格](https://arxiv.org/abs/1803.09010)的文档框架基础上，我们认为AI系统需要奖励报告。
- en: '**Reward Reports** are living documents for proposed RL deployments that demarcate
    design choices.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励报告**是为拟议的RL部署而设计的活动文档，标明设计选择。'
- en: However, many questions remain about the applicability of this framework to
    different RL applications, roadblocks to system interpretability, and the resonances
    between deployed supervised machine learning systems and the sequential decision-making
    utilized in RL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，关于这种框架在不同RL应用中的适用性、系统可解释性的障碍以及部署的监督机器学习系统与RL中使用的顺序决策之间的共鸣仍有许多问题。
- en: At a minimum, Reward Reports are an opportunity for RL practitioners to deliberate
    on these questions and begin the work of deciding how to resolve them in practice.
    ​
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，奖励报告是供RL从业者思考这些问题并开始决定如何在实践中解决它们的机会。​
- en: Capturing temporal behavior with documentation
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过文档捕捉时间行为
- en: The core piece specific to documentation designed for RL and feedback-driven
    ML systems is a *change-log*. The change-log updates information from the designer
    (changed training parameters, data, etc.) along with noticed changes from the
    user (harmful behavior, unexpected responses, etc.).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为RL和反馈驱动的ML系统设计的文档的核心部分是*变更日志*。变更日志更新了设计者的信息（更改的训练参数、数据等）以及用户的注意到的变化（有害行为、意外响应等）。
- en: The change log is accompanied by update triggers that encourage monitoring these
    effects.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 变更日志伴随着更新触发器，鼓励监控这些影响。
- en: Contributing
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贡献
- en: Some of the most impactful RL-driven systems are multi-stakeholder in nature
    and behind the closed doors of private corporations. These corporations are largely
    without regulation, so the burden of documentation falls on the public.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最有影响力的RL驱动系统是多利益相关者性质的，并且在私营公司的闭门之后。这些公司大多没有监管，因此文档的负担落在公众身上。
- en: If you are interested in contributing, we are building Reward Reports for popular
    machine learning systems on a public record on [GitHub](https://github.com/RewardReports/reward-reports).
    ​ For further reading, you can visit the Reward Reports [paper](https://arxiv.org/abs/2204.10817)
    or look [an example report](https://github.com/RewardReports/reward-reports/tree/main/examples).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣贡献，我们正在为流行的机器学习系统在[GitHub](https://github.com/RewardReports/reward-reports)上建立奖励报告的公开记录。更多阅读，请访问奖励报告[论文](https://arxiv.org/abs/2204.10817)或查看[示例报告](https://github.com/RewardReports/reward-reports/tree/main/examples)。
- en: Author
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者
- en: This section was written by [Nathan Lambert](https://twitter.com/natolambert)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本节由[Nathan Lambert](https://twitter.com/natolambert)撰写
