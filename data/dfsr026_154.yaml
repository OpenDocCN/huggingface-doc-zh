- en: Image-to-Video Generation with PIA (Personalized Image Animator)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PIA进行图像到视频生成（个性化图像动画）
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/pia](https://huggingface.co/docs/diffusers/api/pipelines/pia)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/pia](https://huggingface.co/docs/diffusers/api/pipelines/pia)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: '[PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image
    Models](https://arxiv.org/abs/2312.13964) by Yiming Zhang, Zhening Xing, Yanhong
    Zeng, Youqing Fang, Kai Chen'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[PIA：通过文本到图像模型中的即插即用模块个性化图像动画](https://arxiv.org/abs/2312.13964) 作者：Yiming
    Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen'
- en: Recent advancements in personalized text-to-image (T2I) models have revolutionized
    content creation, empowering non-experts to generate stunning images with unique
    styles. While promising, adding realistic motions into these personalized images
    by text poses significant challenges in preserving distinct styles, high-fidelity
    details, and achieving motion controllability by text. In this paper, we present
    PIA, a Personalized Image Animator that excels in aligning with condition images,
    achieving motion controllability by text, and the compatibility with various personalized
    T2I models without specific tuning. To achieve these goals, PIA builds upon a
    base T2I model with well-trained temporal alignment layers, allowing for the seamless
    transformation of any personalized T2I model into an image animation model. A
    key component of PIA is the introduction of the condition module, which utilizes
    the condition frame and inter-frame affinity as input to transfer appearance information
    guided by the affinity hint for individual frame synthesis in the latent space.
    This design mitigates the challenges of appearance-related image alignment within
    and allows for a stronger focus on aligning with motion-related guidance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化文本到图像（T2I）模型的最新进展已经彻底改变了内容创作，使非专家能够以独特风格生成令人惊叹的图像。虽然有所希望，但是通过文本向这些个性化图像添加逼真的动作在保留独特风格、高保真细节和通过文本实现动作可控性方面面临着重大挑战。在本文中，我们提出了PIA，一种个性化图像动画生成器，擅长与条件图像对齐，通过文本实现动作可控性，并且与各种个性化T2I模型兼容而无需特定调整。为了实现这些目标，PIA基于一个基础T2I模型构建了经过良好训练的时间对齐层，允许将任何个性化T2I模型无缝转换为图像动画模型。PIA的一个关键组件是引入条件模块，该模块利用条件帧和帧间亲和性作为输入，通过亲和性提示指导个别帧在潜在空间中进行外观信息传递，从而减轻了外观相关图像对齐的挑战，并允许更加专注于与动作相关的指导对齐。
- en: '[Project page](https://pi-animator.github.io/)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[项目页面](https://pi-animator.github.io/)'
- en: Available Pipelines
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的流水线
- en: '| Pipeline | Tasks | Demo |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 流水线 | 任务 | 演示 |'
- en: '| --- | --- | :-: |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | :-: |'
- en: '| [PIAPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pia/pipeline_pia.py)
    | *Image-to-Video Generation with PIA* |  |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| [PIAPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pia/pipeline_pia.py)
    | *使用PIA进行图像到视频生成* |  |'
- en: Available checkpoints
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的检查点
- en: Motion Adapter checkpoints for PIA can be found under the [OpenMMLab org](https://huggingface.co/openmmlab/PIA-condition-adapter).
    These checkpoints are meant to work with any model based on Stable Diffusion 1.5
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PIA的Motion Adapter检查点可以在[OpenMMLab org](https://huggingface.co/openmmlab/PIA-condition-adapter)下找到。这些检查点旨在与基于Stable
    Diffusion 1.5的任何模型一起使用。
- en: Usage example
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示例
- en: PIA works with a MotionAdapter checkpoint and a Stable Diffusion 1.5 model checkpoint.
    The MotionAdapter is a collection of Motion Modules that are responsible for adding
    coherent motion across image frames. These modules are applied after the Resnet
    and Attention blocks in the Stable Diffusion UNet. In addition to the motion modules,
    PIA also replaces the input convolution layer of the SD 1.5 UNet model with a
    9 channel input convolution layer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: PIA可以与MotionAdapter检查点和Stable Diffusion 1.5模型检查点一起使用。MotionAdapter是一组负责在图像帧之间添加连贯动作的运动模块。这些模块应用于Stable
    Diffusion UNet中的Resnet和Attention块之后。除了运动模块，PIA还将SD 1.5 UNet模型的输入卷积层替换为一个9通道输入卷积层。
- en: The following example demonstrates how to use PIA to generate a video from a
    single image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何使用PIA从单个图像生成视频。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are some sample outputs:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例输出：
- en: '| masterpiece, bestquality, sunset. ![cat in a field](../Images/0e6cae78c21b7faca444db4e5ac98357.png)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 杰作，最佳质量，日落。 ![田野中的猫](../Images/0e6cae78c21b7faca444db4e5ac98357.png) |'
- en: If you plan on using a scheduler that can clip samples, make sure to disable
    it by setting `clip_sample=False` in the scheduler as this can also have an adverse
    effect on generated samples. Additionally, the PIA checkpoints can be sensitive
    to the beta schedule of the scheduler. We recommend setting this to `linear`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用可以剪辑样本的调度程序，请确保在调度程序中设置`clip_sample=False`以禁用它，因为这也可能对生成的样本产生不利影响。此外，PIA的检查点可能对调度程序的beta调度敏感。我们建议将其设置为`linear`。
- en: Using FreeInit
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用FreeInit
- en: '[FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537)
    by Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[FreeInit：填补视频扩散模型中的初始化差距](https://arxiv.org/abs/2312.07537) 作者：Tianxing Wu,
    Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu。'
- en: FreeInit is an effective method that improves temporal consistency and overall
    quality of videos generated using video-diffusion-models without any addition
    training. It can be applied to PIA, AnimateDiff, ModelScope, VideoCrafter and
    various other video generation models seamlessly at inference time, and works
    by iteratively refining the latent-initialization noise. More details can be found
    it the paper.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: FreeInit是一种有效的方法，可以在不进行任何额外训练的情况下改善使用视频扩散模型生成的视频的时间一致性和整体质量。它可以在推理时无缝应用于PIA、AnimateDiff、ModelScope、VideoCrafter和其他各种视频生成模型，并通过迭代地优化潜在初始化噪声来工作。更多细节可以在论文中找到。
- en: The following example demonstrates the usage of FreeInit.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了使用FreeInit的方法。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '| masterpiece, bestquality, sunset. ![cat in a field](../Images/2c58eb14c2ee5ab1bb455398e9a71b14.png)
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 杰作，最佳质量，日落。 ![田野中的猫](../Images/2c58eb14c2ee5ab1bb455398e9a71b14.png) |'
- en: FreeInit is not really free - the improved quality comes at the cost of extra
    computation. It requires sampling a few extra times depending on the `num_iters`
    parameter that is set when enabling it. Setting the `use_fast_sampling` parameter
    to `True` can improve the overall performance (at the cost of lower quality compared
    to when `use_fast_sampling=False` but still better results than vanilla video
    generation models).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: FreeInit并不是真正免费的 - 改进的质量是以额外计算为代价的。它需要根据启用时设置的`num_iters`参数进行几次额外采样。将`use_fast_sampling`参数设置为`True`可以提高整体性能（以降低质量为代价，与`use_fast_sampling=False`相比仍然比普通视频生成模型有更好的结果）。
- en: PIAPipeline
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PIAPipeline
- en: '### `class diffusers.PIAPipeline`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.PIAPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L212)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L212)'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — 变分自动编码器（VAE）模型，用于对图像进行编码和解码到和从潜在表示中。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — 用于对文本进行标记化的[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    used to create a UNetMotionModel to denoise the encoded video latents.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 用于创建UNetMotionModel以去噪编码视频潜在的[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。'
- en: '`motion_adapter` (`MotionAdapter`) — A `MotionAdapter` to be used in combination
    with `unet` to denoise the encoded video latents.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_adapter` (`MotionAdapter`) — 与`unet`结合使用的`MotionAdapter`，用于去噪编码视频潜在。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 与`unet`结合使用的调度器，用于去噪编码图像潜在。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: Pipeline for text-to-video generation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本到视频生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。检查超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道还继承了以下加载方法：
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于加载文本反演嵌入的[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于加载LoRA权重的[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于保存LoRA权重的[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于加载IP适配器的[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
- en: '#### `__call__`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L948)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L948)'
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image` (`PipelineImageInput`) — The input image to be used for video generation.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`PipelineImageInput`) — 用于视频生成的输入图像。'
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`, *可选*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`strength` (`float`, *optional*, defaults to 1.0) — Indicates extent to transform
    the reference `image`. Must be between 0 and 1.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength` (`float`, *可选*, 默认为1.0) — 表示转换参考`image`的程度。必须在0和1之间。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated video.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成视频的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated video.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成视频的像素宽度。'
- en: '`num_frames` (`int`, *optional*, defaults to 16) — The number of video frames
    that are generated. Defaults to 16 frames which at 8 frames per seconds amounts
    to 2 seconds of video.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_frames` (`int`, *可选*, 默认为16) — 生成的视频帧数。默认为16帧，每秒8帧，相当于2秒的视频。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality videos at the expense
    of slower inference.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的视频，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str`或`List[str]`, *可选*) — 指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale
    < 1`）将被忽略。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *可选*, 默认为0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta（η）。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`或`List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for video generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`. Latents
    should be of shape `(batch_size, num_channel, num_frames, height, width)`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成的嘈杂潜在变量，用作视频生成的输入。可以用来调整相同生成与不同提示。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在变量张量，潜在变量应具有形状`(batch_size,
    num_channel, num_frames, height, width)`。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image — (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters. motion_scale — (`int`, *optional*, defaults
    to 0): Parameter that controls the amount and type of motion that is added to
    the image. Increasing the value increases the amount of motion, while specific
    ranges of values control the type of motion that is added. Must be between 0 and
    8. Set between 0-2 to only increase the amount of motion. Set between 3-5 to create
    looping motion. Set between 6-8 to perform motion with image style transfer.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`negative_prompt`输入参数生成`negative_prompt_embeds`。ip_adapter_image
    — (`PipelineImageInput`, *可选*): 用于与IP适配器一起使用的可选图像输入。motion_scale — (`int`, *可选*,
    默认为0): 控制添加到图像中的运动量和类型的参数。增加值会增加运动量，而特定范围的值控制添加的运动类型。必须在0和8之间。设置在0-2之间只增加运动量。设置在3-5之间创建循环运动。设置在6-8之间执行带有图像风格转移的运动。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated video. Choose between `torch.FloatTensor`, `PIL.Image` or `np.array`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"pil"`) — 生成视频的输出格式。选择`torch.FloatTensor`、`PIL.Image`或`np.array`之间的选项。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    instead of a plain tuple.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)而不是普通元组。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 如果指定，则将传递给[`AttentionProcessor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`self.processor`的kwargs字典。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用预终层的输出来计算提示嵌入。'
- en: '`callback_on_step_end` (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Callable`, *可选*) — 在推理过程中每个降噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。'
- en: Returns
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    or `tuple`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)或`tuple`'
- en: If `return_dict` is `True`, [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated frames.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的帧的列表。
- en: The call function to the pipeline for generation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `disable_free_init`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_free_init`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L620)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L620)'
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Disables the FreeInit mechanism if enabled.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，则禁用FreeInit机制。
- en: '#### `disable_freeu`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L568)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L568)'
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，则禁用FreeU机制。
- en: '#### `disable_vae_slicing`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L520)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L520)'
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片的VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `disable_vae_tiling`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L537)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L537)'
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用平铺的VAE解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。
- en: '#### `enable_free_init`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_free_init`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L576)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L576)'
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_iters` (`int`, *optional*, defaults to `3`) — Number of FreeInit noise
    re-initialization iterations.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iters` (`int`, *可选*，默认为`3`) — FreeInit噪声重新初始化迭代次数。'
- en: '`use_fast_sampling` (`bool`, *optional*, defaults to `False`) — Whether or
    not to speedup sampling procedure at the cost of probably lower quality results.
    Enables the “Coarse-to-Fine Sampling” strategy, as mentioned in the paper, if
    set to `True`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_fast_sampling` (`bool`, *可选*，默认为`False`) — 是否加快采样过程以降低可能的质量。如果设置为`True`，则启用“粗到细采样”策略，如论文中所述。'
- en: '`method` (`str`, *optional*, defaults to `butterworth`) — Must be one of `butterworth`,
    `ideal` or `gaussian` to use as the filtering method for the FreeInit low pass
    filter.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`method` (`str`, *可选*，默认为`butterworth`) — 必须是`butterworth`、`ideal`或`gaussian`中的一个，用作FreeInit低通滤波器的滤波方法。'
- en: '`order` (`int`, *optional*, defaults to `4`) — Order of the filter used in
    `butterworth` method. Larger values lead to `ideal` method behaviour whereas lower
    values lead to `gaussian` method behaviour.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`order` (`int`, *可选*，默认为`4`) — `butterworth`方法中使用的滤波器的阶数。较大的值导致`ideal`方法行为，而较小的值导致`gaussian`方法行为。'
- en: '`spatial_stop_frequency` (`float`, *optional*, defaults to `0.25`) — Normalized
    stop frequency for spatial dimensions. Must be between 0 to 1\. Referred to as
    `d_s` in the original implementation.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spatial_stop_frequency` (`float`, *可选*，默认为`0.25`) — 空间维度的归一化停止频率。必须介于0到1之间。在原始实现中称为`d_s`。'
- en: '`temporal_stop_frequency` (`float`, *optional*, defaults to `0.25`) — Normalized
    stop frequency for temporal dimensions. Must be between 0 to 1\. Referred to as
    `d_t` in the original implementation.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temporal_stop_frequency` (`float`, *可选*，默认为`0.25`) — 时间维度的归一化停止频率。必须介于0到1之间。在原始实现中称为`d_t`。'
- en: '`generator` (`torch.Generator`, *optional*, defaults to `0.25`) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make FreeInit generation deterministic.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`, *可选*，默认为`0.25`) — 用于使FreeInit生成确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: Enables the FreeInit mechanism as in [https://arxiv.org/abs/2312.07537](https://arxiv.org/abs/2312.07537).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 启用FreeInit机制，如[https://arxiv.org/abs/2312.07537](https://arxiv.org/abs/2312.07537)。
- en: This implementation has been adapted from the [official repository](https://github.com/TianxingWu/FreeInit).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现已经从[官方存储库](https://github.com/TianxingWu/FreeInit)进行了调整。
- en: '#### `enable_freeu`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L545)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L545)'
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1` (`float`) — 第1阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效果”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2` (`float`) — 第2阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效果”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1` (`float`) — 第1阶段的缩放因子，用于放大骨干特征的贡献。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2` (`float`) — 第2阶段的缩放因子，用于放大骨干特征的贡献。'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后缀表示它们被应用的阶段。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如Stable Diffusion
    v1、v2和Stable Diffusion XL）的值组合。
- en: '#### `enable_vae_slicing`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L512)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L512)'
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片的VAE解码。启用此选项时，VAE将将输入张量分割成片段，以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。
- en: '#### `enable_vae_tiling`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L528)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L528)'
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 启用平铺的VAE解码。启用此选项时，VAE将将输入张量分割成瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。
- en: '#### `encode_prompt`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L281)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L281)'
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 每个提示应生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用分类器自由引导或不使用'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于引导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用引导（即如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *可选*) — 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从CLIP跳过的层数。值为1意味着将使用前一层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: enable_freeu
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用自由u
- en: disable_freeu
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用自由u
- en: enable_free_init
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用自由初始化
- en: disable_free_init
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用自由初始化
- en: enable_vae_slicing
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用VAE切片
- en: disable_vae_slicing
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用VAE切片
- en: enable_vae_tiling
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用VAE平铺
- en: disable_vae_tiling
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用VAE平铺
- en: PIAPipelineOutput
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PIAPipelineOutput
- en: '### `class diffusers.pipelines.pia.PIAPipelineOutput`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.pipelines.pia.PIAPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L197)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pia/pipeline_pia.py#L197)'
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`frames` (`torch.Tensor`, `np.ndarray`, or List[PIL.Image.Image]) —'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frames` (`torch.Tensor`, `np.ndarray`, 或 List[PIL.Image.Image]) —'
- en: '`Nested` list of length `batch_size` with denoised PIL image sequences of length
    `num_frames`, —'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度为`batch_size`的`Nested`列表，其中包含长度为`num_frames`的去噪PIL图像序列， —
- en: '`NumPy` array of shape `(batch_size, num_frames, channels, height, width, —'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状为`(batch_size, num_frames, channels, height, width)`的`NumPy`数组 —
- en: '`Torch` tensor of shape `(batch_size, num_frames, channels, height, width)`.
    —'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状为`(batch_size, num_frames, channels, height, width)`的`Torch`张量。 —
- en: Output class for PIAPipeline.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PIAPipeline的输出类。
