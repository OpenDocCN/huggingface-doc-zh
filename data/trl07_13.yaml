- en: Supervised Fine-tuning Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨
- en: 'Original text: [https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL
    we provide an easy-to-use API to create your SFT models and train them with few
    lines of code on your dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç›‘ç£å¾®è°ƒï¼ˆæˆ–ç®€ç§°SFTï¼‰æ˜¯RLHFä¸­çš„ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚åœ¨TRLä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„APIæ¥åˆ›å»ºæ‚¨çš„SFTæ¨¡å‹ï¼Œå¹¶ç”¨å‡ è¡Œä»£ç åœ¨æ‚¨çš„æ•°æ®é›†ä¸Šè®­ç»ƒå®ƒä»¬ã€‚
- en: Check out a complete flexible example at [`examples/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft.py).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[`examples/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft.py)ä¸­æŸ¥çœ‹ä¸€ä¸ªå®Œæ•´çµæ´»çš„ç¤ºä¾‹ã€‚
- en: Quickstart
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå¼€å§‹
- en: 'If you have a dataset hosted on the ğŸ¤— Hub, you can easily fine-tune your SFT
    model using [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) from TRL.
    Let us assume your dataset is `imdb`, the text you want to predict is inside the
    `text` field of the dataset, and you want to fine-tune the `facebook/opt-350m`
    model. The following code-snippet takes care of all the data pre-processing and
    training for you:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨ğŸ¤— Hubä¸Šæ‰˜ç®¡äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)ä»TRLè½»æ¾å¾®è°ƒæ‚¨çš„SFTæ¨¡å‹ã€‚å‡è®¾æ‚¨çš„æ•°æ®é›†æ˜¯`imdb`ï¼Œæ‚¨è¦é¢„æµ‹çš„æ–‡æœ¬ä½äºæ•°æ®é›†çš„`text`å­—æ®µä¸­ï¼Œæ‚¨æƒ³è¦å¾®è°ƒ`facebook/opt-350m`æ¨¡å‹ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µä¼šå¤„ç†æ‰€æœ‰çš„æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒå·¥ä½œï¼š
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Make sure to pass a correct value for `max_seq_length` as the default value
    will be set to `min(tokenizer.model_max_length, 1024)`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä¸º`max_seq_length`ä¼ é€’æ­£ç¡®çš„å€¼ï¼Œå› ä¸ºé»˜è®¤å€¼å°†è®¾ç½®ä¸º`min(tokenizer.model_max_length, 1024)`ã€‚
- en: 'You can also construct a model outside of the trainer and pass it as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥åœ¨è®­ç»ƒå¸ˆä¹‹å¤–æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’å¦‚ä¸‹ï¼š
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The above snippets will use the default training arguments from the [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)
    class. If you want to modify that, make sure to create your own `TrainingArguments`
    object and pass it to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    constructor as it is done on the [`supervised_finetuning.py` script](https://github.com/huggingface/trl/blob/main/examples/stack_llama/scripts/supervised_finetuning.py)
    on the stack-llama example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä»£ç ç‰‡æ®µå°†ä½¿ç”¨[`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)ç±»ä¸­çš„é»˜è®¤è®­ç»ƒå‚æ•°ã€‚å¦‚æœæ‚¨æƒ³ä¿®æ”¹å®ƒï¼Œè¯·ç¡®ä¿åˆ›å»ºè‡ªå·±çš„`TrainingArguments`å¯¹è±¡å¹¶å°†å…¶ä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ„é€ å‡½æ•°ï¼Œå°±åƒåœ¨[`supervised_finetuning.py`è„šæœ¬](https://github.com/huggingface/trl/blob/main/examples/stack_llama/scripts/supervised_finetuning.py)ä¸­çš„stack-llamaç¤ºä¾‹ä¸­æ‰€åšçš„é‚£æ ·ã€‚
- en: Advanced usage
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜çº§ç”¨æ³•
- en: Train on completions only
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»…åœ¨å®Œæˆä¸Šè®­ç»ƒ
- en: 'You can use the `DataCollatorForCompletionOnlyLM` to train your model on the
    generated prompts only. Note that this works only in the case when `packing=False`.
    To instantiate that collator for instruction data, pass a response template and
    the tokenizer. Here is an example of how it would work to fine-tune `opt-350m`
    on completions only on the CodeAlpaca dataset:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`DataCollatorForCompletionOnlyLM`ä»…åœ¨ç”Ÿæˆæç¤ºä¸Šè®­ç»ƒæ‚¨çš„æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™ä»…åœ¨`packing=False`çš„æƒ…å†µä¸‹æœ‰æ•ˆã€‚ä¸ºäº†ä¸ºæŒ‡ä»¤æ•°æ®å®ä¾‹åŒ–è¯¥æ”¶é›†å™¨ï¼Œä¼ é€’ä¸€ä¸ªå“åº”æ¨¡æ¿å’Œåˆ†è¯å™¨ã€‚ä»¥ä¸‹æ˜¯åœ¨CodeAlpacaæ•°æ®é›†ä¸Šä»…å¯¹å®Œæˆè¿›è¡Œå¾®è°ƒ`opt-350m`çš„å·¥ä½œç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To instantiate that collator for assistant style conversation data, pass a
    response template, an instruction template and the tokenizer. Here is an example
    of how it would work to fine-tune `opt-350m` on assistant completions only on
    the Open Assistant Guanaco dataset:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºåŠ©æ‰‹é£æ ¼å¯¹è¯æ•°æ®å®ä¾‹åŒ–è¯¥æ”¶é›†å™¨æ—¶ï¼Œè¯·ä¼ é€’ä¸€ä¸ªå“åº”æ¨¡æ¿ã€ä¸€ä¸ªæŒ‡ä»¤æ¨¡æ¿å’Œåˆ†è¯å™¨ã€‚ä»¥ä¸‹æ˜¯åœ¨Open Assistant Guanacoæ•°æ®é›†ä¸Šä»…å¯¹åŠ©æ‰‹å®Œæˆè¿›è¡Œå¾®è°ƒ`opt-350m`çš„å·¥ä½œç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Make sure to have a `pad_token_id` which is different from `eos_token_id` which
    can result in the model not properly predicting EOS (End of Sentence) tokens during
    generation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æœ‰ä¸€ä¸ª`pad_token_id`ï¼Œå®ƒä¸`eos_token_id`ä¸åŒï¼Œè¿™å¯ä»¥é˜²æ­¢æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ— æ³•æ­£ç¡®é¢„æµ‹EOSï¼ˆå¥å­ç»“æŸï¼‰æ ‡è®°ã€‚
- en: Using token_ids directly for response_template
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç›´æ¥ä½¿ç”¨token_idsä½œä¸ºresponse_template
- en: 'Some tokenizers like Llama 2 (`meta-llama/Llama-2-XXb-hf`) tokenize sequences
    differently depending whether they have context or not. For example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›åˆ†è¯å™¨ï¼ˆå¦‚Llama 2ï¼ˆ`meta-llama/Llama-2-XXb-hf`ï¼‰ï¼‰æ ¹æ®æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡è€Œä¸åŒåœ°å¯¹åºåˆ—è¿›è¡Œåˆ†è¯ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this case, and due to lack of context in `response_template`, the same string
    (â€### Assistant:â€) is tokenized differently:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äº`response_template`ä¸­ç¼ºä¹ä¸Šä¸‹æ–‡ï¼Œç›¸åŒçš„å­—ç¬¦ä¸²ï¼ˆâ€œ### Assistant:â€ï¼‰è¢«ä¸åŒåœ°åˆ†è¯ï¼š
- en: 'Text (with context): `[2277, 29937, 4007, 22137, 29901]`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰ï¼š`[2277, 29937, 4007, 22137, 29901]`
- en: '`response_template` (without context): `[835, 4007, 22137, 29901]`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response_template`ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰ï¼š`[835, 4007, 22137, 29901]`'
- en: 'This will lead to an error when the `DataCollatorForCompletionOnlyLM` does
    not find the `response_template` in the dataset example text:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å½“`DataCollatorForCompletionOnlyLM`åœ¨æ•°æ®é›†ç¤ºä¾‹æ–‡æœ¬ä¸­æ‰¾ä¸åˆ°`response_template`æ—¶ï¼Œè¿™å°†å¯¼è‡´é”™è¯¯ï¼š
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To solve this, you can tokenize the `response_template` with the same context
    than in the dataset, truncate it as needed and pass the `token_ids` directly to
    the `response_template` argument of the `DataCollatorForCompletionOnlyLM` class.
    For example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸æ•°æ®é›†ä¸­ç›¸åŒä¸Šä¸‹æ–‡çš„æ–¹å¼å¯¹`response_template`è¿›è¡Œåˆ†è¯ï¼Œæ ¹æ®éœ€è¦æˆªæ–­å®ƒï¼Œå¹¶å°†`token_ids`ç›´æ¥ä¼ é€’ç»™`DataCollatorForCompletionOnlyLM`ç±»çš„`response_template`å‚æ•°ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Add Special Tokens for Chat Format
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸ºèŠå¤©æ ¼å¼æ·»åŠ ç‰¹æ®Šæ ‡è®°
- en: 'Adding special tokens to a language model is crucial for training chat models.
    These tokens are added between the different roles in a conversation, such as
    the user, assistant, and system and help the model recognize the structure and
    flow of a conversation. This setup is essential for enabling the model to generate
    coherent and contextually appropriate responses in a chat environment. The `setup_chat_format()`
    function in `trl` easily sets up a model and tokenizer for conversational AI tasks.
    This function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºè¯­è¨€æ¨¡å‹æ·»åŠ ç‰¹æ®Šæ ‡è®°å¯¹äºè®­ç»ƒèŠå¤©æ¨¡å‹è‡³å…³é‡è¦ã€‚è¿™äº›æ ‡è®°è¢«æ·»åŠ åœ¨å¯¹è¯ä¸­çš„ä¸åŒè§’è‰²ä¹‹é—´ï¼Œå¦‚ç”¨æˆ·ã€åŠ©æ‰‹å’Œç³»ç»Ÿï¼Œå¹¶å¸®åŠ©æ¨¡å‹è¯†åˆ«å¯¹è¯çš„ç»“æ„å’Œæµç¨‹ã€‚è¿™ç§è®¾ç½®å¯¹äºä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨èŠå¤©ç¯å¢ƒä¸­ç”Ÿæˆè¿è´¯å’Œä¸Šä¸‹æ–‡é€‚å½“çš„å“åº”è‡³å…³é‡è¦ã€‚`trl`ä¸­çš„`setup_chat_format()`å‡½æ•°å¯ä»¥è½»æ¾ä¸ºä¼šè¯AIä»»åŠ¡è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™ä¸ªå‡½æ•°ï¼š
- en: Adds special tokens to the tokenizer, e.g. `<|im_start|>` and `<|im_end|>`,
    to indicate the start and end of a conversation.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘æ ‡è®°åŒ–å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä¾‹å¦‚`<|im_start|>`å’Œ`<|imm_end|>`ï¼Œä»¥æŒ‡ç¤ºä¼šè¯çš„å¼€å§‹å’Œç»“æŸã€‚
- en: Resizes the modelâ€™s embedding layer to accommodate the new tokens.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒæ•´æ¨¡å‹çš„åµŒå…¥å±‚ä»¥å®¹çº³æ–°çš„æ ‡è®°ã€‚
- en: Sets the `chat_template` of the tokenizer, which is used to format the input
    data into a chat-like format. The default is `chatml` from OpenAI.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾ç½®ä»¤ç‰ŒåŒ–å™¨çš„`chat_template`ï¼Œç”¨äºå°†è¾“å…¥æ•°æ®æ ¼å¼åŒ–ä¸ºç±»ä¼¼èŠå¤©çš„æ ¼å¼ã€‚é»˜è®¤å€¼ä¸ºOpenAIçš„`chatml`ã€‚
- en: '*optionally* you can pass `resize_to_multiple_of` to resize the embedding layer
    to a multiple of the `resize_to_multiple_of` argument, e.g. 64\. If you want to
    see more formats being supported in the future, please open a GitHub issue on
    [trl](https://github.com/huggingface/trl)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å¯é€‰*æ‚¨å¯ä»¥ä¼ é€’`resize_to_multiple_of`æ¥å°†åµŒå…¥å±‚è°ƒæ•´ä¸º`resize_to_multiple_of`å‚æ•°çš„å€æ•°ï¼Œä¾‹å¦‚64ã€‚å¦‚æœæ‚¨å¸Œæœ›å°†æ¥æ”¯æŒæ›´å¤šæ ¼å¼ï¼Œè¯·åœ¨[trl](https://github.com/huggingface/trl)ä¸Šæ‰“å¼€GitHubé—®é¢˜'
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With our model and tokenizer set up, we can now fine-tune our model on a conversational
    dataset. Below is an example of how a dataset can be formatted for fine-tuning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†æˆ‘ä»¬çš„æ¨¡å‹å’Œåˆ†è¯å™¨è®¾ç½®ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨å¯¹è¯æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ ¼å¼åŒ–æ•°æ®é›†è¿›è¡Œå¾®è°ƒçš„ç¤ºä¾‹ã€‚
- en: Dataset format support
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æ ¼å¼æ”¯æŒ
- en: 'The [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) supports popular
    dataset formats. This allows you to pass the dataset to the trainer without any
    pre-processing directly. The following formats are supported:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ”¯æŒæµè¡Œçš„æ•°æ®é›†æ ¼å¼ã€‚è¿™ä½¿æ‚¨å¯ä»¥ç›´æ¥å°†æ•°æ®é›†ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œæ— éœ€ä»»ä½•é¢„å¤„ç†ã€‚æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š'
- en: conversational format
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹è¯æ ¼å¼
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: instruction format
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡ä»¤æ ¼å¼
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If your dataset uses one of the above formats, you can directly pass it to the
    trainer without pre-processing. The [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    will then format the dataset for you using the defined format from the modelâ€™s
    tokenizer with the [apply_chat_template](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models)
    method.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ•°æ®é›†ä½¿ç”¨ä¸Šè¿°æ ¼å¼ä¹‹ä¸€ï¼Œæ‚¨å¯ä»¥ç›´æ¥å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨è€Œæ— éœ€é¢„å¤„ç†ã€‚[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)å°†ä½¿ç”¨æ¨¡å‹çš„åˆ†è¯å™¨ä¸­å®šä¹‰çš„æ ¼å¼ä½¿ç”¨[apply_chat_template](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models)æ–¹æ³•ä¸ºæ‚¨æ ¼å¼åŒ–æ•°æ®é›†ã€‚
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If the dataset is not in one those format you can either preprocess the dataset
    to match the formatting or pass a formatting function to the SFTTrainer to do
    it for you. Letâ€™s have a look.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ•°æ®é›†ä¸æ˜¯è¿™äº›æ ¼å¼ä¹‹ä¸€ï¼Œæ‚¨å¯ä»¥é¢„å¤„ç†æ•°æ®é›†ä»¥åŒ¹é…æ ¼å¼ï¼Œæˆ–è€…å°†æ ¼å¼åŒ–å‡½æ•°ä¼ é€’ç»™SFTTrainerä»¥ä»£æ›¿ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚
- en: Format your input prompts
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ¼å¼åŒ–æ‚¨çš„è¾“å…¥æç¤º
- en: 'For instruction fine-tuning, it is quite common to have two columns inside
    the dataset: one for the prompt & the other for the response. This allows people
    to format examples like [Stanford-Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
    did as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŒ‡ä»¤å¾®è°ƒï¼Œé€šå¸¸åœ¨æ•°æ®é›†ä¸­æœ‰ä¸¤åˆ—ï¼šä¸€ä¸ªç”¨äºæç¤ºï¼Œå¦ä¸€ä¸ªç”¨äºå“åº”ã€‚è¿™ä½¿äººä»¬å¯ä»¥åƒ[Stanford-Alpaca](https://github.com/tatsu-lab/stanford_alpaca)é‚£æ ·æ ¼å¼åŒ–ç¤ºä¾‹ï¼š
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let us assume your dataset has two fields, `question` and `answer`. Therefore
    you can just run:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ‚¨çš„æ•°æ®é›†æœ‰ä¸¤ä¸ªå­—æ®µï¼Œ`question`å’Œ`answer`ã€‚å› æ­¤æ‚¨å¯ä»¥ç›´æ¥è¿è¡Œï¼š
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To preperly format your input make sure to process all the examples by looping
    over them and returning a list of processed text. Check out a full example on
    how to use SFTTrainer on alpaca dataset [here](https://github.com/huggingface/trl/pull/444#issue-1760952763)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ­£ç¡®æ ¼å¼åŒ–æ‚¨çš„è¾“å…¥ï¼Œè¯·ç¡®ä¿é€šè¿‡å¾ªç¯å¤„ç†æ‰€æœ‰ç¤ºä¾‹å¹¶è¿”å›å¤„ç†è¿‡çš„æ–‡æœ¬åˆ—è¡¨ã€‚æŸ¥çœ‹å¦‚ä½•åœ¨alpacaæ•°æ®é›†ä¸Šä½¿ç”¨SFTTrainerçš„å®Œæ•´ç¤ºä¾‹[è¿™é‡Œ](https://github.com/huggingface/trl/pull/444#issue-1760952763)
- en: Packing dataset ( ConstantLengthDataset )
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰“åŒ…æ•°æ®é›†ï¼ˆConstantLengthDatasetï¼‰
- en: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) supports *example
    packing*, where multiple short examples are packed in the same input sequence
    to increase training efficiency. This is done with the `ConstantLengthDataset`
    utility class that returns constant length chunks of tokens from a stream of examples.
    To enable the usage of this dataset class, simply pass `packing=True` to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    constructor.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ”¯æŒ*ç¤ºä¾‹æ‰“åŒ…*ï¼Œå…¶ä¸­å¤šä¸ªçŸ­ç¤ºä¾‹æ‰“åŒ…åœ¨åŒä¸€è¾“å…¥åºåˆ—ä¸­ä»¥å¢åŠ è®­ç»ƒæ•ˆç‡ã€‚è¿™æ˜¯é€šè¿‡`ConstantLengthDataset`å®ç”¨ç¨‹åºç±»å®Œæˆçš„ï¼Œè¯¥ç±»ä»ç¤ºä¾‹æµä¸­è¿”å›å¸¸é•¿åº¦çš„ä»¤ç‰Œå—ã€‚è¦å¯ç”¨æ­¤æ•°æ®é›†ç±»çš„ä½¿ç”¨ï¼Œåªéœ€å°†`packing=True`ä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ„é€ å‡½æ•°ã€‚'
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that if you use a packed dataset and if you pass `max_steps` in the training
    arguments you will probably train your models for more than few epochs, depending
    on the way you have configured the packed dataset and the training protocol. Double
    check that you know and understand what you are doing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨ä½¿ç”¨æ‰“åŒ…æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒå‚æ•°ä¸­ä¼ é€’äº†`max_steps`ï¼Œåˆ™æ‚¨å¯èƒ½ä¼šè®­ç»ƒæ¨¡å‹å¤šä¸ªå‘¨æœŸï¼Œå…·ä½“å–å†³äºæ‚¨å¦‚ä½•é…ç½®æ‰“åŒ…æ•°æ®é›†å’Œè®­ç»ƒåè®®ã€‚è¯·ç¡®ä¿æ‚¨çŸ¥é“å¹¶ç†è§£æ‚¨æ­£åœ¨åšä»€ä¹ˆã€‚
- en: Customize your prompts using packed dataset
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰“åŒ…æ•°æ®é›†è‡ªå®šä¹‰æ‚¨çš„æç¤º
- en: 'If your dataset has several fields that you want to combine, for example if
    the dataset has `question` and `answer` fields and you want to combine them, you
    can pass a formatting function to the trainer that will take care of that. For
    example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ•°æ®é›†æœ‰å‡ ä¸ªå­—æ®µè¦åˆå¹¶ï¼Œä¾‹å¦‚æ•°æ®é›†æœ‰`question`å’Œ`answer`å­—æ®µï¼Œæ‚¨æƒ³è¦åˆå¹¶å®ƒä»¬ï¼Œæ‚¨å¯ä»¥å°†æ ¼å¼åŒ–å‡½æ•°ä¼ é€’ç»™è®­ç»ƒå™¨æ¥å¤„ç†ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also customize the `ConstantLengthDataset` much more by directly passing
    the arguments to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    constructor. Please refer to that classâ€™ signature for more information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€šè¿‡ç›´æ¥å°†å‚æ•°ä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ„é€ å‡½æ•°æ¥æ›´å¤šåœ°å®šåˆ¶`ConstantLengthDataset`ã€‚è¯·å‚è€ƒè¯¥ç±»çš„ç­¾åä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: Control over the pretrained model
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„æ§åˆ¶
- en: You can directly pass the kwargs of the `from_pretrained()` method to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer).
    For example, if you want to load a model in a different precision, analogous to
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ç›´æ¥å°†`from_pretrained()`æ–¹æ³•çš„kwargsä¼ é€’ç»™[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¦ä»¥ä¸åŒçš„ç²¾åº¦åŠ è½½æ¨¡å‹ï¼Œç±»ä¼¼äº
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that all keyword arguments of `from_pretrained()` are supported.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œ`from_pretrained()`çš„æ‰€æœ‰å…³é”®å­—å‚æ•°éƒ½å—æ”¯æŒã€‚
- en: Training adapters
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒé€‚é…å™¨
- en: We also support a tight integration with ğŸ¤— PEFT library so that any user can
    conveniently train adapters and share them on the Hub instead of training the
    entire model
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æ”¯æŒä¸ğŸ¤— PEFTåº“çš„ç´§å¯†é›†æˆï¼Œä»¥ä¾¿ä»»ä½•ç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°è®­ç»ƒé€‚é…å™¨å¹¶åœ¨Hubä¸Šå…±äº«å®ƒä»¬ï¼Œè€Œä¸æ˜¯è®­ç»ƒæ•´ä¸ªæ¨¡å‹
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can also continue training your `PeftModel`. For that, first load a `PeftModel`
    outside `SFTTrainer` and pass it directly to the trainer without the `peft_config`
    argument being passed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ç»§ç»­è®­ç»ƒæ‚¨çš„`PeftModel`ã€‚ä¸ºæ­¤ï¼Œè¯·é¦–å…ˆåœ¨`SFTTrainer`ä¹‹å¤–åŠ è½½`PeftModel`ï¼Œå¹¶ç›´æ¥å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œè€Œæ— éœ€ä¼ é€’`peft_config`å‚æ•°ã€‚
- en: Training adapters with base 8 bit models
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŸºç¡€8ä½æ¨¡å‹è®­ç»ƒé€‚é…å™¨
- en: 'For that you need to first load your 8bit model outside the Trainer and pass
    a `PeftConfig` to the trainer. For example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦é¦–å…ˆåœ¨Trainerä¹‹å¤–åŠ è½½æ‚¨çš„8ä½æ¨¡å‹ï¼Œå¹¶å°†`PeftConfig`ä¼ é€’ç»™è®­ç»ƒå™¨ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using Flash Attention and Flash Attention 2
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Flash Attentionå’ŒFlash Attention 2
- en: You can benefit from Flash Attention 1 & 2 using SFTTrainer out of the box with
    minimal changes of code. First, to make sure you have all the latest features
    from transformers, install transformers from source
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨SFTTrainerä¸­çš„Flash Attention 1å’Œ2æ¥è·ç›Šï¼Œåªéœ€è¿›è¡Œæœ€å°‘çš„ä»£ç æ›´æ”¹ã€‚é¦–å…ˆï¼Œä¸ºäº†ç¡®ä¿æ‚¨æ‹¥æœ‰æ¥è‡ªtransformersçš„æ‰€æœ‰æœ€æ–°åŠŸèƒ½ï¼Œè¯·ä»æºä»£ç å®‰è£…transformers
- en: '[PRE19]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that Flash Attention only works on GPU now and under half-precision regime
    (when using adapters, base model loaded in half-precision) Note also both features
    are perfectly compatible with other tools such as quantization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒFlash Attentionç°åœ¨ä»…åœ¨GPUä¸Šè¿è¡Œï¼Œå¹¶ä¸”åœ¨åŠç²¾åº¦åˆ¶åº¦ä¸‹è¿è¡Œï¼ˆå½“ä½¿ç”¨é€‚é…å™¨æ—¶ï¼ŒåŸºç¡€æ¨¡å‹ä»¥åŠç²¾åº¦åŠ è½½ï¼‰ã€‚è¿˜è¯·æ³¨æ„ï¼Œè¿™ä¸¤ä¸ªåŠŸèƒ½ä¸é‡åŒ–ç­‰å…¶ä»–å·¥å…·å®Œå…¨å…¼å®¹ã€‚
- en: Using Flash-Attention 1
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Flash-Attention 1
- en: 'For Flash Attention 1 you can use the `BetterTransformer` API and force-dispatch
    the API to use Flash Attention kernel. First, install the latest optimum package:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºFlash Attention 1ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`BetterTransformer` APIå¹¶å¼ºåˆ¶åˆ†æ´¾APIä»¥ä½¿ç”¨Flash Attentionå†…æ ¸ã€‚é¦–å…ˆï¼Œå®‰è£…æœ€æ–°çš„optimumåŒ…ï¼š
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once you have loaded your model, wrap the `trainer.train()` call under the
    `with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):`
    context manager:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹åï¼Œå°†`trainer.train()`è°ƒç”¨åŒ…è£…åœ¨`with torch.backends.cuda.sdp_kernel(enable_flash=True,
    enable_math=False, enable_mem_efficient=False):`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ï¼š
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that you cannot train your model using Flash Attention 1 on an arbitrary
    dataset as `torch.scaled_dot_product_attention` does not support training with
    padding tokens if you use Flash Attention kernels. Therefore you can only use
    that feature with `packing=True`. If your dataset contains padding tokens, consider
    switching to Flash Attention 2 integration.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœä½¿ç”¨Flash Attentionå†…æ ¸ï¼Œæ‚¨ä¸èƒ½åœ¨ä»»æ„æ•°æ®é›†ä¸Šè®­ç»ƒæ‚¨çš„æ¨¡å‹ï¼Œå› ä¸º`torch.scaled_dot_product_attention`ä¸æ”¯æŒä½¿ç”¨å¡«å……ä»¤ç‰Œè¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œæ‚¨åªèƒ½åœ¨`packing=True`çš„æƒ…å†µä¸‹ä½¿ç”¨è¯¥åŠŸèƒ½ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†åŒ…å«å¡«å……ä»¤ç‰Œï¼Œè¯·è€ƒè™‘åˆ‡æ¢åˆ°Flash
    Attention 2é›†æˆã€‚
- en: Below are some numbers you can get in terms of speedup and memory efficiency,
    using Flash Attention 1, on a single NVIDIA-T4 16GB.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨å•ä¸ªNVIDIA-T4 16GBä¸Šä½¿ç”¨Flash Attention 1å¯ä»¥è·å¾—çš„ä¸€äº›åŠ é€Ÿå’Œå†…å­˜æ•ˆç‡æ–¹é¢çš„æ•°å­—ã€‚
- en: '| use_flash_attn_1 | model_name | max_seq_len | batch_size | time per training
    step |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| use_flash_attn_1 | model_name | max_seq_len | batch_size | time per training
    step |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| x | facebook/opt-350m | 2048 | 8 | ~59.1s |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| x | facebook/opt-350m | 2048 | 8 | ~59.1s |'
- en: '|  | facebook/opt-350m | 2048 | 8 | **OOM** |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | facebook/opt-350m | 2048 | 8 | **OOM** |'
- en: '| x | facebook/opt-350m | 2048 | 4 | ~30.3s |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| x | facebook/opt-350m | 2048 | 4 | ~30.3s |'
- en: '|  | facebook/opt-350m | 2048 | 4 | ~148.9s |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | facebook/opt-350m | 2048 | 4 | ~148.9s |'
- en: Using Flash Attention-2
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Flash Attention-2
- en: 'To use Flash Attention 2, first install the latest `flash-attn` package:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨Flash Attention 2ï¼Œé¦–å…ˆå®‰è£…æœ€æ–°çš„`flash-attn`åŒ…ï¼š
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And add `use_flash_attention_2=True` when calling `from_pretrained`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨`from_pretrained`æ—¶æ·»åŠ `use_flash_attention_2=True`ï¼š
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you donâ€™t use quantization, make sure your model is loaded in half-precision
    and dispatch your model on a supported GPU device. After loading your model, you
    can either train it as it is, or attach adapters and train adapters on it in case
    your model is quantized.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸ä½¿ç”¨é‡åŒ–ï¼Œè¯·ç¡®ä¿æ‚¨çš„æ¨¡å‹ä»¥åŠç²¾åº¦åŠ è½½ï¼Œå¹¶å°†æ‚¨çš„æ¨¡å‹åˆ†æ´¾åˆ°æ”¯æŒçš„GPUè®¾å¤‡ä¸Šã€‚åŠ è½½æ¨¡å‹åï¼Œæ‚¨å¯ä»¥æŒ‰åŸæ ·è®­ç»ƒå®ƒï¼Œæˆ–è€…åœ¨æ‚¨çš„æ¨¡å‹ç»è¿‡é‡åŒ–çš„æƒ…å†µä¸‹ï¼Œé™„åŠ é€‚é…å™¨å¹¶åœ¨å…¶ä¸Šè®­ç»ƒé€‚é…å™¨ã€‚
- en: In contrary to Flash Attention 1, the integration makes it possible to train
    your model on an arbitrary dataset that also includes padding tokens.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Flash Attention 1ç›¸åï¼Œé›†æˆä½¿å¾—å¯ä»¥åœ¨åŒ…å«å¡«å……ä»¤ç‰Œçš„ä»»æ„æ•°æ®é›†ä¸Šè®­ç»ƒæ‚¨çš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚
- en: Enhance modelâ€™s performances using NEFTune
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨NEFTuneå¢å¼ºæ¨¡å‹æ€§èƒ½
- en: 'NEFTune is a technique to boost the performance of chat models and was introduced
    by the paper [â€œNEFTune: Noisy Embeddings Improve Instruction Finetuningâ€](https://arxiv.org/abs/2310.05914)
    from Jain et al. it consists of adding noise to the embedding vectors during training.
    According to the abstract of the paper:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'NEFTuneæ˜¯ä¸€ç§æé«˜èŠå¤©æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯ï¼Œç”±Jainç­‰äººåœ¨è®ºæ–‡["NEFTune: Noisy Embeddings Improve Instruction
    Finetuning"](https://arxiv.org/abs/2310.05914)ä¸­ä»‹ç»ã€‚å®ƒåŒ…æ‹¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘åµŒå…¥å‘é‡æ·»åŠ å™ªå£°ã€‚æ ¹æ®è®ºæ–‡çš„æ‘˜è¦ï¼š'
- en: Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval,
    which rises to 64.69% using noisy embeddings. NEFTune also improves over strong
    baselines on modern instruction datasets. Models trained with Evol-Instruct see
    a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8%
    improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat
    benefit from additional training with NEFTune.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Alpacaå¯¹LLaMA-2-7Bè¿›è¡Œæ ‡å‡†å¾®è°ƒï¼Œåœ¨AlpacaEvalä¸Šè¾¾åˆ°29.79%ï¼Œä½¿ç”¨å˜ˆæ‚çš„åµŒå…¥æé«˜åˆ°64.69%ã€‚NEFTuneè¿˜æ”¹å–„äº†ç°ä»£æŒ‡ä»¤æ•°æ®é›†ä¸Šçš„å¼ºåŸºçº¿ã€‚ä½¿ç”¨Evol-Instructè®­ç»ƒçš„æ¨¡å‹çœ‹åˆ°äº†10%çš„æ”¹è¿›ï¼Œä½¿ç”¨ShareGPTçœ‹åˆ°äº†8%çš„æ”¹è¿›ï¼Œä½¿ç”¨OpenPlatypusçœ‹åˆ°äº†8%çš„æ”¹è¿›ã€‚å³ä½¿æ˜¯è¿›ä¸€æ­¥é€šè¿‡RLHFç²¾ç»†è°ƒæ•´çš„å¼ºå¤§æ¨¡å‹ï¼Œå¦‚LLaMA-2-Chatï¼Œä¹Ÿå—ç›ŠäºNEFTuneçš„é¢å¤–è®­ç»ƒã€‚
- en: '![](../Images/52cfc6d8e877350238b9e92fa688af9a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52cfc6d8e877350238b9e92fa688af9a.png)'
- en: To use it in `SFTTrainer` simply pass `neftune_noise_alpha` when creating your
    `SFTTrainer` instance. Note that to avoid any surprising behaviour, NEFTune is
    disabled after training to retrieve back the original behaviour of the embedding
    layer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨`SFTTrainer`ä¸­ä½¿ç”¨å®ƒï¼Œåªéœ€åœ¨åˆ›å»º`SFTTrainer`å®ä¾‹æ—¶ä¼ é€’`neftune_noise_alpha`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†é¿å…ä»»ä½•æ„å¤–è¡Œä¸ºï¼Œåœ¨è®­ç»ƒåç¦ç”¨NEFTuneä»¥æ¢å¤åµŒå…¥å±‚çš„åŸå§‹è¡Œä¸ºã€‚
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We have tested NEFTune by training `mistralai/Mistral-7B-v0.1` on the [OpenAssistant
    dataset](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) and
    validated that using NEFTune led to a performance boost of ~25% on MT Bench.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡åœ¨[OpenAssistantæ•°æ®é›†](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)ä¸Šè®­ç»ƒ`mistralai/Mistral-7B-v0.1`æ¥æµ‹è¯•NEFTuneï¼Œå¹¶éªŒè¯ä½¿ç”¨NEFTuneå¯¼è‡´MT
    Benchæ€§èƒ½æå‡çº¦25%ã€‚
- en: '![](../Images/2041067e08b1835ad3fb89fb4c7255c5.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2041067e08b1835ad3fb89fb4c7255c5.png)'
- en: Note however, that the amount of performance gain is *dataset dependent* and
    in particular, applying NEFTune on synthetic datasets like [UltraChat](https://huggingface.co/datasets/stingning/ultrachat)
    typically produces smaller gains.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¯·æ³¨æ„ï¼Œæ€§èƒ½å¢ç›Šçš„æ•°é‡æ˜¯*æ•°æ®é›†ç›¸å…³*çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆæˆæ•°æ®é›†ï¼ˆå¦‚[UltraChat](https://huggingface.co/datasets/stingning/ultrachat)ï¼‰ä¸Šåº”ç”¨
    NEFTune é€šå¸¸ä¼šäº§ç”Ÿè¾ƒå°çš„å¢ç›Šã€‚
- en: Accelerate fine-tuning 2x using unsloth
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ unsloth åŠ é€Ÿå¾®è°ƒ 2 å€
- en: 'You can further accelerate QLoRA / LoRA (2x faster, 60% less memory) using
    the [`unsloth`](https://github.com/unslothai/unsloth) library that is fully compatible
    with `SFTTrainer`. Currently `unsloth` supports only Llama (Yi, TinyLlama, Qwen,
    Deepseek etc) and Mistral architectures. Some benchmarks on 1x A100 listed below:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨å®Œå…¨å…¼å®¹`SFTTrainer`çš„[`unsloth`](https://github.com/unslothai/unsloth)åº“è¿›ä¸€æ­¥åŠ é€Ÿ
    QLoRA / LoRAï¼ˆé€Ÿåº¦æé«˜ 2 å€ï¼Œå†…å­˜å‡å°‘ 60%ï¼‰ã€‚ç›®å‰ï¼Œ`unsloth`ä»…æ”¯æŒ Llamaï¼ˆYiã€TinyLlamaã€Qwenã€Deepseek
    ç­‰ï¼‰å’Œ Mistral æ¶æ„ã€‚ä»¥ä¸‹æ˜¯ 1x A100 çš„ä¸€äº›åŸºå‡†æµ‹è¯•ï¼š
- en: '| 1 A100 40GB | Dataset | ğŸ¤— | ğŸ¤— + Flash Attention 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM saved
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 1 A100 40GB | æ•°æ®é›† | ğŸ¤— | ğŸ¤— + é—ªå­˜æ³¨æ„åŠ› 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM å·²ä¿å­˜ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Code Llama 34b | Slim Orca | 1x | 1.01x | **1.94x** | -22.7% |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Code Llama 34b | ç˜¦é²¸é±¼ | 1x | 1.01x | **1.94x** | -22.7% |'
- en: '| Llama-2 7b | Slim Orca | 1x | 0.96x | **1.87x** | -39.3% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 7b | ç˜¦é²¸é±¼ | 1x | 0.96x | **1.87x** | -39.3% |'
- en: '| Mistral 7b | Slim Orca | 1x | 1.17x | **1.88x** | -65.9% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7b | ç˜¦é²¸é±¼ | 1x | 1.17x | **1.88x** | -65.9% |'
- en: '| Tiny Llama 1.1b | Alpaca | 1x | 1.55x | **2.74x** | -57.8% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Tiny Llama 1.1b | ç¾Šé©¼ | 1x | 1.55x | **2.74x** | -57.8% |'
- en: 'First install `unsloth` according to the [official documentation](https://github.com/unslothai/unsloth).
    Once installed, you can incorporate unsloth into your workflow in a very simple
    manner; instead of loading `AutoModelForCausalLM`, you just need to load a `FastLanguageModel`
    as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://github.com/unslothai/unsloth)å®‰è£…`unsloth`ã€‚å®‰è£…å®Œæˆåï¼Œæ‚¨å¯ä»¥éå¸¸ç®€å•åœ°å°†
    unsloth æ•´åˆåˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼›åªéœ€åŠ è½½`FastLanguageModel`ï¼Œè€Œä¸æ˜¯åŠ è½½`AutoModelForCausalLM`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The saved model is fully compatible with Hugging Faceâ€™s transformers library.
    Learn more about unsloth in their [official repository](https://github.com/unslothai/unsloth).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜çš„æ¨¡å‹ä¸ Hugging Face çš„ transformers åº“å®Œå…¨å…¼å®¹ã€‚åœ¨ä»–ä»¬çš„[å®˜æ–¹å­˜å‚¨åº“](https://github.com/unslothai/unsloth)ä¸­äº†è§£æ›´å¤šå…³äº
    unsloth çš„ä¿¡æ¯ã€‚
- en: Best practices
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€ä½³å®è·µ
- en: 'Pay attention to the following best practices when training a model with that
    trainer:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨è¯¥è®­ç»ƒå™¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹æœ€ä½³å®è·µï¼š
- en: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) always pads by default
    the sequences to the `max_seq_length` argument of the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer).
    If none is passed, the trainer will retrieve that value from the tokenizer. Some
    tokenizers do not provide default value, so there is a check to retrieve the minimum
    between 2048 and that value. Make sure to check it before training.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) é»˜è®¤æƒ…å†µä¸‹æ€»æ˜¯å°†åºåˆ—å¡«å……åˆ°[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)çš„`max_seq_length`å‚æ•°ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’å€¼ï¼Œè®­ç»ƒå™¨å°†ä»åˆ†è¯å™¨ä¸­æ£€ç´¢è¯¥å€¼ã€‚ä¸€äº›åˆ†è¯å™¨ä¸æä¾›é»˜è®¤å€¼ï¼Œå› æ­¤æœ‰ä¸€ä¸ªæ£€æŸ¥ä»¥æ£€ç´¢
    2048 å’Œè¯¥å€¼ä¹‹é—´çš„æœ€å°å€¼ã€‚åœ¨è®­ç»ƒä¹‹å‰ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥ã€‚'
- en: For training adapters in 8bit, you might need to tweak the arguments of the
    `prepare_model_for_kbit_training` method from PEFT, hence we advise users to use
    `prepare_in_int8_kwargs` field, or create the `PeftModel` outside the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    and pass it.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºåœ¨ 8 ä½ä¸­è®­ç»ƒé€‚é…å™¨ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´ PEFT çš„`prepare_model_for_kbit_training`æ–¹æ³•çš„å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨`prepare_in_int8_kwargs`å­—æ®µï¼Œæˆ–åœ¨[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)ä¹‹å¤–åˆ›å»º`PeftModel`å¹¶ä¼ é€’å®ƒã€‚
- en: For a more memory-efficient training using adapters, you can load the base model
    in 8bit, for that simply add `load_in_8bit` argument when creating the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer),
    or create a base model in 8bit outside the trainer and pass it.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´èŠ‚çœå†…å­˜åœ°ä½¿ç”¨é€‚é…å™¨è¿›è¡Œè®­ç»ƒï¼Œæ‚¨å¯ä»¥åœ¨ 8 ä½ä¸­åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œåªéœ€åœ¨åˆ›å»º[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)æ—¶æ·»åŠ `load_in_8bit`å‚æ•°ï¼Œæˆ–åœ¨è®­ç»ƒå™¨ä¹‹å¤–åˆ›å»ºä¸€ä¸ª
    8 ä½çš„åŸºç¡€æ¨¡å‹å¹¶ä¼ é€’å®ƒã€‚
- en: If you create a model outside the trainer, make sure to not pass to the trainer
    any additional keyword arguments that are relative to `from_pretrained()` method.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨è®­ç»ƒå™¨ä¹‹å¤–åˆ›å»ºæ¨¡å‹ï¼Œè¯·ç¡®ä¿ä¸è¦å‘è®­ç»ƒå™¨ä¼ é€’ç›¸å¯¹äº`from_pretrained()`æ–¹æ³•çš„ä»»ä½•é¢å¤–å…³é”®å­—å‚æ•°ã€‚
- en: GPTQ Conversion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTQ è½¬æ¢
- en: You may experience some issues with GPTQ Quantization after completing training.
    Lowering `gradient_accumulation_steps` to `4` will resolve most issues during
    the quantization process to GPTQ format.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®Œæˆè®­ç»ƒåï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ä¸€äº› GPTQ é‡åŒ–çš„é—®é¢˜ã€‚å°†`gradient_accumulation_steps`é™ä½åˆ°`4`å°†è§£å†³å¤§éƒ¨åˆ†åœ¨é‡åŒ–è¿‡ç¨‹ä¸­åˆ°
    GPTQ æ ¼å¼çš„é—®é¢˜ã€‚
- en: SFTTrainer
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SFTTrainer
- en: '### `class trl.SFTTrainer`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.SFTTrainer`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)'
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]) â€” The model
    to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the
    model name to load from cache or download. The model can be also converted to
    a `PeftModel` if a `PeftConfig` object is passed to the `peft_config` argument.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`ï¼ˆUnion[`transformers.PreTrainedModel`, `nn.Module`, `str`ï¼‰â€” è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥æ˜¯`PreTrainedModel`ï¼Œ`torch.nn.Module`æˆ–åŒ…å«è¦ä»ç¼“å­˜åŠ è½½æˆ–ä¸‹è½½çš„æ¨¡å‹åç§°çš„å­—ç¬¦ä¸²ã€‚å¦‚æœå°†`PeftConfig`å¯¹è±¡ä¼ é€’ç»™`peft_config`å‚æ•°ï¼Œåˆ™æ¨¡å‹ä¹Ÿå¯ä»¥è½¬æ¢ä¸º`PeftModel`ã€‚'
- en: '`args` (Optional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments))
    â€” The arguments to tweak for training. Please refer to the official documentation
    of `transformers.TrainingArguments` for more information.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args`ï¼ˆOptional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments)ï¼‰â€”
    ç”¨äºè°ƒæ•´è®­ç»ƒçš„å‚æ•°ã€‚è¯·å‚è€ƒ`transformers.TrainingArguments`çš„å®˜æ–¹æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: '`data_collator` (Optional`transformers.DataCollator`) â€” The data collator to
    use for training.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_collator`ï¼ˆå¯é€‰`transformers.DataCollator`ï¼‰â€” ç”¨äºè®­ç»ƒçš„æ•°æ®æ•´ç†å™¨ã€‚'
- en: '`train_dataset` (Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset))
    â€” The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset`
    to create their dataset.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset` (Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset))
    â€” ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚æˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨`trl.trainer.ConstantLengthDataset`æ¥åˆ›å»ºä»–ä»¬çš„æ•°æ®é›†ã€‚'
- en: '`eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]])
    â€” The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset`
    to create their dataset.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]])
    â€” ç”¨äºè¯„ä¼°çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨`trl.trainer.ConstantLengthDataset`æ¥åˆ›å»ºä»–ä»¬çš„æ•°æ®é›†ã€‚'
- en: '`tokenizer` (Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” The tokenizer to use for training. If not specified, the tokenizer associated
    to the model will be used.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” ç”¨äºè®­ç»ƒçš„åˆ†è¯å™¨ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨ä¸æ¨¡å‹å…³è”çš„åˆ†è¯å™¨ã€‚'
- en: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) â€” The model initializer
    to use for training. If None is specified, the default model initializer will
    be used.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) â€” ç”¨äºè®­ç»ƒçš„æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚å¦‚æœæŒ‡å®šä¸ºNoneï¼Œåˆ™å°†ä½¿ç”¨é»˜è®¤çš„æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚'
- en: '`compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional*
    defaults to None) â€” The function used to compute metrics during evaluation. It
    should return a dictionary mapping metric names to metric values. If not specified,
    only the loss will be computed during evaluation.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional*
    defaults to None) â€” ç”¨äºåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­è®¡ç®—æŒ‡æ ‡çš„å‡½æ•°ã€‚å®ƒåº”è¯¥è¿”å›ä¸€ä¸ªå°†æŒ‡æ ‡åç§°æ˜ å°„åˆ°æŒ‡æ ‡å€¼çš„å­—å…¸ã€‚å¦‚æœæœªæŒ‡å®šï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­åªä¼šè®¡ç®—æŸå¤±ã€‚'
- en: '`callbacks` (`List[transformers.TrainerCallback]`) â€” The callbacks to use for
    training.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callbacks` (`List[transformers.TrainerCallback]`) â€” ç”¨äºè®­ç»ƒçš„å›è°ƒå‡½æ•°ã€‚'
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    â€” The optimizer and scheduler to use for training.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    â€” ç”¨äºè®­ç»ƒçš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚'
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    â€” The function to use to preprocess the logits before computing the metrics.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    â€” ç”¨äºåœ¨è®¡ç®—æŒ‡æ ‡ä¹‹å‰é¢„å¤„ç†logitsçš„å‡½æ•°ã€‚'
- en: '`peft_config` (`Optional[PeftConfig]`) â€” The PeftConfig object to use to initialize
    the PeftModel.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config` (`Optional[PeftConfig]`) â€” ç”¨äºåˆå§‹åŒ–PeftModelçš„PeftConfigå¯¹è±¡ã€‚'
- en: '`dataset_text_field` (`Optional[str]`) â€” The name of the text field of the
    dataset, in case this is passed by a user, the trainer will automatically create
    a `ConstantLengthDataset` based on the `dataset_text_field` argument.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_text_field` (`Optional[str]`) â€” æ•°æ®é›†çš„æ–‡æœ¬å­—æ®µçš„åç§°ï¼Œå¦‚æœç”¨æˆ·ä¼ é€’äº†è¿™ä¸ªå‚æ•°ï¼Œè®­ç»ƒå™¨å°†è‡ªåŠ¨åŸºäº`dataset_text_field`å‚æ•°åˆ›å»ºä¸€ä¸ª`ConstantLengthDataset`ã€‚'
- en: '`formatting_func` (`Optional[Callable]`) â€” The formatting function to be used
    for creating the `ConstantLengthDataset`.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`formatting_func` (`Optional[Callable]`) â€” ç”¨äºåˆ›å»º`ConstantLengthDataset`çš„æ ¼å¼åŒ–å‡½æ•°ã€‚'
- en: '`max_seq_length` (`Optional[int]`) â€” The maximum sequence length to use for
    the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults
    to `512`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_length` (`Optional[int]`) â€” ç”¨äº`ConstantLengthDataset`å’Œè‡ªåŠ¨åˆ›å»ºæ•°æ®é›†çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é»˜è®¤ä¸º`512`ã€‚'
- en: '`infinite` (`Optional[bool]`) â€” Whether to use an infinite dataset or not.
    Defaults to `False`.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`infinite` (`Optional[bool]`) â€” æ˜¯å¦ä½¿ç”¨æ— é™æ•°æ®é›†ã€‚é»˜è®¤ä¸º`False`ã€‚'
- en: '`num_of_sequences` (`Optional[int]`) â€” The number of sequences to use for the
    `ConstantLengthDataset`. Defaults to `1024`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_of_sequences` (`Optional[int]`) â€” ç”¨äº`ConstantLengthDataset`çš„åºåˆ—æ•°ã€‚é»˜è®¤ä¸º`1024`ã€‚'
- en: '`chars_per_token` (`Optional[float]`) â€” The number of characters per token
    to use for the `ConstantLengthDataset`. Defaults to `3.6`. You can check how this
    is computed in the stack-llama example: [https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chars_per_token` (`Optional[float]`) â€” ç”¨äº`ConstantLengthDataset`çš„æ¯ä¸ªæ ‡è®°çš„å­—ç¬¦æ•°ã€‚é»˜è®¤ä¸º`3.6`ã€‚æ‚¨å¯ä»¥åœ¨stack-llamaç¤ºä¾‹ä¸­æŸ¥çœ‹å¦‚ä½•è®¡ç®—è¿™ä¸ªå€¼ï¼š[https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53)ã€‚'
- en: '`packing` (`Optional[bool]`) â€” Used only in case `dataset_text_field` is passed.
    This argument is used by the `ConstantLengthDataset` to pack the sequences of
    the dataset.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packing` (`Optional[bool]`) â€” ä»…åœ¨ä¼ é€’äº†`dataset_text_field`çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚è¿™ä¸ªå‚æ•°ç”±`ConstantLengthDataset`ç”¨äºæ‰“åŒ…æ•°æ®é›†çš„åºåˆ—ã€‚'
- en: '`dataset_num_proc` (`Optional[int]`) â€” The number of workers to use to tokenize
    the data. Only used when `packing=False`. Defaults to None.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_num_proc` (`Optional[int]`) â€” ç”¨äºæ ‡è®°åŒ–æ•°æ®çš„å·¥ä½œè¿›ç¨‹æ•°ã€‚ä»…åœ¨`packing=False`æ—¶ä½¿ç”¨ã€‚é»˜è®¤ä¸ºNoneã€‚'
- en: '`dataset_batch_size` (`int`) â€” The number of examples to tokenize per batch.
    If batch_size <= 0 or batch_size == None, tokenize the full dataset as a single
    batch. Defaults to 1000.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_batch_size` (`int`) â€” æ¯æ‰¹è¦æ ‡è®°åŒ–çš„ç¤ºä¾‹æ•°ã€‚å¦‚æœbatch_size <= 0æˆ–batch_size == Noneï¼Œåˆ™å°†æ•´ä¸ªæ•°æ®é›†æ ‡è®°ä¸ºå•ä¸ªæ‰¹æ¬¡ã€‚é»˜è®¤ä¸º1000ã€‚'
- en: '`neftune_noise_alpha` (`Optional[float]`) â€” If not `None`, this will activate
    NEFTune noise embeddings. This has been proven to drastically improve model performances
    for instruction fine-tuning. Check out the original paper here: [https://arxiv.org/abs/2310.05914](https://arxiv.org/abs/2310.05914)
    and the original code here: [https://github.com/neelsjain/NEFTune](https://github.com/neelsjain/NEFTune)
    model_init_kwargs â€” (`Optional[Dict]`, *optional*): Dict of Optional kwargs to
    pass when instantiating the model from a string dataset_kwargs â€” (`Optional[Dict]`,
    *optional*): Dict of Optional kwargs to pass when creating packed or non-packed
    datasets'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neftune_noise_alpha` (`Optional[float]`) â€” å¦‚æœä¸æ˜¯ `None`ï¼Œåˆ™ä¼šæ¿€æ´» NEFTune å™ªå£°åµŒå…¥ã€‚è¿™å·²è¢«è¯æ˜æå¤§åœ°æé«˜äº†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚æŸ¥çœ‹åŸå§‹è®ºæ–‡ï¼š[https://arxiv.org/abs/2310.05914](https://arxiv.org/abs/2310.05914)
    å’ŒåŸå§‹ä»£ç ï¼š[https://github.com/neelsjain/NEFTune](https://github.com/neelsjain/NEFTune)
    model_init_kwargs â€” (`Optional[Dict]`, *optional*): ä¼ é€’ç»™å®ä¾‹åŒ–æ¨¡å‹æ—¶çš„å¯é€‰å‚æ•°å­—å…¸ dataset_kwargs
    â€” (`Optional[Dict]`, *optional*): åˆ›å»ºæ‰“åŒ…æˆ–éæ‰“åŒ…æ•°æ®é›†æ—¶ä¼ é€’çš„å¯é€‰å‚æ•°å­—å…¸'
- en: Class definition of the Supervised Finetuning Trainer (SFT Trainer). This class
    is a wrapper around the `transformers.Trainer` class and inherits all of its attributes
    and methods. The trainer takes care of properly initializing the PeftModel in
    case a user passes a `PeftConfig` object.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨ï¼ˆSFT Trainerï¼‰çš„ç±»å®šä¹‰ã€‚è¿™ä¸ªç±»æ˜¯ `transformers.Trainer` ç±»çš„åŒ…è£…å™¨ï¼Œå¹¶ç»§æ‰¿äº†å®ƒçš„æ‰€æœ‰å±æ€§å’Œæ–¹æ³•ã€‚è®­ç»ƒå™¨è´Ÿè´£åœ¨ç”¨æˆ·ä¼ é€’
    `PeftConfig` å¯¹è±¡æ—¶æ­£ç¡®åˆå§‹åŒ– PeftModelã€‚
- en: ConstantLengthDataset
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConstantLengthDataset
- en: '### `class trl.trainer.ConstantLengthDataset`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.trainer.ConstantLengthDataset`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/utils.py#L355)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/utils.py#L355)'
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tokenizer` (`transformers.PreTrainedTokenizer`) â€” The processor used for processing
    the data.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`transformers.PreTrainedTokenizer`) â€” ç”¨äºå¤„ç†æ•°æ®çš„å¤„ç†å™¨ã€‚'
- en: '`dataset` (`dataset.Dataset`) â€” Dataset with text files.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset` (`dataset.Dataset`) â€” åŒ…å«æ–‡æœ¬æ–‡ä»¶çš„æ•°æ®é›†ã€‚'
- en: '`dataset_text_field` (`str`, **optional**) â€” Name of the field in the dataset
    that contains the text. Used only if `formatting_func` is `None`.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_text_field` (`str`, **optional**) â€” æ•°æ®é›†ä¸­åŒ…å«æ–‡æœ¬çš„å­—æ®µåç§°ã€‚ä»…åœ¨ `formatting_func`
    ä¸º `None` æ—¶ä½¿ç”¨ã€‚'
- en: '`formatting_func` (`Callable`, **optional**) â€” Function that formats the text
    before tokenization. Usually it is recommended to have follows a certain pattern
    such as `"### Question: {question} ### Answer: {answer}"`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`formatting_func` (`Callable`, **optional**) â€” åœ¨åˆ†è¯ä¹‹å‰æ ¼å¼åŒ–æ–‡æœ¬çš„å‡½æ•°ã€‚é€šå¸¸å»ºè®®éµå¾ªç‰¹å®šæ¨¡å¼ï¼Œå¦‚ `"###
    é—®é¢˜ï¼š{question} ### ç­”æ¡ˆï¼š{answer}"`'
- en: '`infinite` (`bool`, *optional*, defaults to `False`) â€” If True the iterator
    is reset after dataset reaches end else stops.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`infinite` (`bool`, *optional*, defaults to `False`) â€” å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ•°æ®é›†åˆ°è¾¾æœ«å°¾åé‡ç½®è¿­ä»£å™¨ï¼Œå¦åˆ™åœæ­¢ã€‚'
- en: '`seq_length` (`int`, *optional*, defaults to `1024`) â€” Length of token sequences
    to return.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seq_length` (`int`, *optional*, defaults to `1024`) â€” è¦è¿”å›çš„ä»¤ç‰Œåºåˆ—çš„é•¿åº¦ã€‚'
- en: '`num_of_sequences` (`int`, *optional*, defaults to `1024`) â€” Number of token
    sequences to keep in buffer.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_of_sequences` (`int`, *optional*, defaults to `1024`) â€” åœ¨ç¼“å†²åŒºä¸­ä¿ç•™çš„ä»¤ç‰Œåºåˆ—æ•°ã€‚'
- en: '`chars_per_token` (`int`, *optional*, defaults to `3.6`) â€” Number of characters
    per token used to estimate number of tokens in text buffer.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chars_per_token` (`int`, *optional*, defaults to `3.6`) â€” ç”¨äºä¼°è®¡æ–‡æœ¬ç¼“å†²åŒºä¸­ä»¤ç‰Œæ•°é‡çš„æ¯ä¸ªä»¤ç‰Œçš„å­—ç¬¦æ•°ã€‚'
- en: '`eos_token_id` (`int`, *optional*, defaults to `0`) â€” Id of the end of sequence
    token if the passed tokenizer does not have an EOS token.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, defaults to `0`) â€” å¦‚æœä¼ é€’çš„åˆ†è¯å™¨æ²¡æœ‰ EOS æ ‡è®°ï¼Œåˆ™ä¸ºåºåˆ—ç»“æŸæ ‡è®°çš„
    IDã€‚'
- en: '`shuffle` (â€˜boolâ€™, *optional*, defaults to True) â€” Shuffle the examples before
    they are returned'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle` (â€˜boolâ€™, *optional*, defaults to True) â€” åœ¨è¿”å›ç¤ºä¾‹ä¹‹å‰å¯¹ç¤ºä¾‹è¿›è¡Œæ´—ç‰Œ'
- en: '`append_concat_token` (â€˜boolâ€™, *optional*, defaults to True) â€” If true, appends
    `eos_token_id` at the end of each sample being packed.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`append_concat_token` (â€˜boolâ€™, *optional*, defaults to True) â€” å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ¯ä¸ªè¢«æ‰“åŒ…çš„æ ·æœ¬æœ«å°¾é™„åŠ 
    `eos_token_id`ã€‚'
- en: '`add_special_tokens` (â€˜boolâ€™, *optional*, defaults to True) â€” If true, tokenizers
    adds special tokens to each sample being packed.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (â€˜boolâ€™, *optional*, defaults to True) â€” å¦‚æœä¸º Trueï¼Œåˆ™åˆ†è¯å™¨ä¼šä¸ºæ¯ä¸ªè¢«æ‰“åŒ…çš„æ ·æœ¬æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚'
- en: Iterable dataset that returns constant length chunks of tokens from stream of
    text files. The dataset also formats the text before tokenization with a specific
    format that is provided by the user.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è¿­ä»£æ•°æ®é›†ï¼Œä»æ–‡æœ¬æ–‡ä»¶æµä¸­è¿”å›æ’å®šé•¿åº¦çš„ä»¤ç‰Œå—ã€‚æ•°æ®é›†è¿˜ä¼šåœ¨åˆ†è¯ä¹‹å‰ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç‰¹å®šæ ¼å¼å¯¹æ–‡æœ¬è¿›è¡Œæ ¼å¼åŒ–ã€‚
