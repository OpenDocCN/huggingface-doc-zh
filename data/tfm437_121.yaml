- en: Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The pipelines are a great and easy way to use models for inference. These pipelines
    are objects that abstract most of the complex code from the library, offering
    a simple API dedicated to several tasks, including Named Entity Recognition, Masked
    Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.
    See the [task summary](../task_summary) for examples of use.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 管道是使用模型进行推断的一种很好且简单的方式。这些管道是抽象出库中大部分复杂代码的对象，提供了专门用于多个任务的简单 API，包括命名实体识别、掩码语言建模、情感分析、特征提取和问答。查看[任务摘要](../task_summary)以获取使用示例。
- en: 'There are two categories of pipeline abstractions to be aware about:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种要注意的管道抽象类别：
- en: The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    which is the most powerful object encapsulating all other pipelines.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    是封装所有其他管道的最强大对象。'
- en: Task-specific pipelines are available for [audio](#audio), [computer vision](#computer-vision),
    [natural language processing](#natural-language-processing), and [multimodal](#multimodal)
    tasks.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对[音频](#audio)、[计算机视觉](#computer-vision)、[自然语言处理](#natural-language-processing)和[多模态](#multimodal)任务提供了特定任务的管道。
- en: The pipeline abstraction
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道抽象
- en: The *pipeline* abstraction is a wrapper around all the other available pipelines.
    It is instantiated as any other pipeline but can provide additional quality of
    life.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*pipeline* 抽象是围绕所有其他可用管道的包装器。它像任何其他管道一样实例化，但可以提供额外的生活质量。'
- en: 'Simple call on one item:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简单调用一个项目：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to use a specific model from the [hub](https://huggingface.co)
    you can ignore the task if the model on the hub already defines it:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用来自[hub](https://huggingface.co)的特定模型，可以忽略任务，如果 hub 上的模型已经定义了它：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To call a pipeline on many items, you can call it with a *list*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要在多个项目上调用管道，可以使用*列表*调用它。
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To iterate over full datasets it is recommended to use a `dataset` directly.
    This means you don’t need to allocate the whole dataset at once, nor do you need
    to do batching yourself. This should work just as fast as custom loops on GPU.
    If it doesn’t don’t hesitate to create an issue.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要遍历完整数据集，建议直接使用`dataset`。这意味着您不需要一次性分配整个数据集，也不需要自己进行批处理。这应该与 GPU 上的自定义循环一样快。如果不是，请不要犹豫创建一个问题。
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For ease of use, a generator is also possible:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便使用，也可以使用生成器：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `transformers.pipeline`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.pipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/__init__.py#L531)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/__init__.py#L531)'
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`task` (`str`) — The task defining which pipeline will be returned. Currently
    accepted tasks are:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`) — 定义将返回哪个管道的任务。当前接受的任务有：'
- en: '`"audio-classification"`: will return a [AudioClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AudioClassificationPipeline).'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"audio-classification"`: 将返回一个[AudioClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AudioClassificationPipeline)。'
- en: '`"automatic-speech-recognition"`: will return a [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline).'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"automatic-speech-recognition"`: 将返回一个[AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)。'
- en: '`"conversational"`: will return a [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"conversational"`: 将返回一个[ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline)。'
- en: '`"depth-estimation"`: will return a [DepthEstimationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.DepthEstimationPipeline).'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"depth-estimation"`: 将返回一个[DepthEstimationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.DepthEstimationPipeline)。'
- en: '`"document-question-answering"`: will return a [DocumentQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.DocumentQuestionAnsweringPipeline).'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"document-question-answering"`: 将返回一个[DocumentQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.DocumentQuestionAnsweringPipeline)。'
- en: '`"feature-extraction"`: will return a [FeatureExtractionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FeatureExtractionPipeline).'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"feature-extraction"`: 将返回一个[FeatureExtractionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FeatureExtractionPipeline)。'
- en: '`"fill-mask"`: will return a [FillMaskPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FillMaskPipeline):.'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"fill-mask"`: 将返回一个[FillMaskPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FillMaskPipeline)。'
- en: '`"image-classification"`: will return a [ImageClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageClassificationPipeline).'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"image-classification"`: 将返回一个[ImageClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageClassificationPipeline)。'
- en: '`"image-segmentation"`: will return a [ImageSegmentationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageSegmentationPipeline).'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"image-segmentation"`: 将返回一个[ImageSegmentationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageSegmentationPipeline)。'
- en: '`"image-to-image"`: will return a [ImageToImagePipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageToImagePipeline).'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"image-to-image"`: 将返回一个[ImageToImagePipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageToImagePipeline)。'
- en: '`"image-to-text"`: will return a [ImageToTextPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageToTextPipeline).'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"image-to-text"`: 将返回一个[ImageToTextPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageToTextPipeline)。'
- en: '`"mask-generation"`: will return a [MaskGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.MaskGenerationPipeline).'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mask-generation"`: 将返回一个[MaskGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.MaskGenerationPipeline)。'
- en: '`"object-detection"`: will return a [ObjectDetectionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ObjectDetectionPipeline).'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"object-detection"`：将返回一个[ObjectDetectionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ObjectDetectionPipeline)。'
- en: '`"question-answering"`: will return a [QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline).'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"question-answering"`：将返回一个[QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline)。'
- en: '`"summarization"`: will return a [SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline).'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"summarization"`：将返回一个[SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline)。'
- en: '`"table-question-answering"`: will return a [TableQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TableQuestionAnsweringPipeline).'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"table-question-answering"`：将返回一个[TableQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TableQuestionAnsweringPipeline)。'
- en: '`"text2text-generation"`: will return a [Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline).'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"text2text-generation"`：将返回一个[Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline)。'
- en: '`"text-classification"` (alias `"sentiment-analysis"` available): will return
    a [TextClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextClassificationPipeline).'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"text-classification"`（别名`"sentiment-analysis"`可用）：将返回一个[TextClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextClassificationPipeline)。'
- en: '`"text-generation"`: will return a [TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextGenerationPipeline):.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"text-generation"`：将返回一个[TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextGenerationPipeline)。'
- en: '`"text-to-audio"` (alias `"text-to-speech"` available): will return a [TextToAudioPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextToAudioPipeline):.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"text-to-audio"`（别名`"text-to-speech"`可用）：将返回一个[TextToAudioPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextToAudioPipeline)。'
- en: '`"token-classification"` (alias `"ner"` available): will return a [TokenClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TokenClassificationPipeline).'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"token-classification"`（别名`"ner"`可用）：将返回一个[TokenClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TokenClassificationPipeline)。'
- en: '`"translation"`: will return a [TranslationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TranslationPipeline).'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"translation"`：将返回一个[TranslationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TranslationPipeline)。'
- en: '`"translation_xx_to_yy"`: will return a [TranslationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TranslationPipeline).'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"translation_xx_to_yy"`：将返回一个[TranslationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TranslationPipeline)。'
- en: '`"video-classification"`: will return a [VideoClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.VideoClassificationPipeline).'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"video-classification"`：将返回一个[VideoClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.VideoClassificationPipeline)。'
- en: '`"visual-question-answering"`: will return a [VisualQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.VisualQuestionAnsweringPipeline).'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"visual-question-answering"`：将返回一个[VisualQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.VisualQuestionAnsweringPipeline)。'
- en: '`"zero-shot-classification"`: will return a [ZeroShotClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline).'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"zero-shot-classification"`：将返回一个[ZeroShotClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline)。'
- en: '`"zero-shot-image-classification"`: will return a [ZeroShotImageClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotImageClassificationPipeline).'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"zero-shot-image-classification"`：将返回一个[ZeroShotImageClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotImageClassificationPipeline)。'
- en: '`"zero-shot-audio-classification"`: will return a [ZeroShotAudioClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotAudioClassificationPipeline).'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"zero-shot-audio-classification"`：将返回一个[ZeroShotAudioClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotAudioClassificationPipeline)。'
- en: '`"zero-shot-object-detection"`: will return a [ZeroShotObjectDetectionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotObjectDetectionPipeline).'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"zero-shot-object-detection"`：将返回一个[ZeroShotObjectDetectionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotObjectDetectionPipeline)。'
- en: '`model` (`str` or [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel),
    *optional*) — The model that will be used by the pipeline to make predictions.
    This can be a model identifier or an actual instance of a pretrained model inheriting
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    (for PyTorch) or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    (for TensorFlow).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`str`或[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)，*可选*）—
    该模型将被管道用于进行预测。这可以是一个模型标识符或一个实际的继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的预训练模型实例。'
- en: If not provided, the default for the `task` will be loaded.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未提供，`task`的默认值将被加载。
- en: '`config` (`str` or [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — The configuration that will be used by the pipeline to instantiate
    the model. This can be a model identifier or an actual pretrained model configuration
    inheriting from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`str`或[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，*可选*）—
    该配置将被管道用于实例化模型。这可以是模型标识符或实际的预训练模型配置，继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)。'
- en: If not provided, the default configuration file for the requested model will
    be used. That means that if `model` is given, its default configuration will be
    used. However, if `model` is not supplied, this `task`’s default model’s config
    is used instead.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未提供，则将使用请求的模型的默认配置文件。这意味着如果提供了`model`，将使用其默认配置。但是，如果未提供`model`，则将使用此`task`的默认模型配置。
- en: '`tokenizer` (`str` or [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer),
    *optional*) — The tokenizer that will be used by the pipeline to encode data for
    the model. This can be a model identifier or an actual pretrained tokenizer inheriting
    from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`str`或[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，*可选*）—
    该分词器将被管道用于对模型的数据进行编码。这可以是模型标识符或实际的预训练分词器，继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: If not provided, the default tokenizer for the given `model` will be loaded
    (if it is a string). If `model` is not specified or not a string, then the default
    tokenizer for `config` is loaded (if it is a string). However, if `config` is
    also not given or not a string, then the default tokenizer for the given `task`
    will be loaded.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未提供，则将加载给定`model`的默认分词器（如果是字符串）。如果未指定`model`或不是字符串，则将加载`config`的默认分词器（如果是字符串）。但是，如果也未提供`config`或不是字符串，则将加载给定`task`的默认分词器。
- en: '`feature_extractor` (`str` or `PreTrainedFeatureExtractor`, *optional*) — The
    feature extractor that will be used by the pipeline to encode data for the model.
    This can be a model identifier or an actual pretrained feature extractor inheriting
    from `PreTrainedFeatureExtractor`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`（`str`或`PreTrainedFeatureExtractor`，*可选*）— 该特征提取器将被管道用于对模型的数据进行编码。这可以是模型标识符或实际的预训练特征提取器，继承自`PreTrainedFeatureExtractor`。'
- en: Feature extractors are used for non-NLP models, such as Speech or Vision models
    as well as multi-modal models. Multi-modal models will also require a tokenizer
    to be passed.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征提取器用于非NLP模型，例如语音或视觉模型以及多模态模型。多模态模型还需要传递一个分词器。
- en: If not provided, the default feature extractor for the given `model` will be
    loaded (if it is a string). If `model` is not specified or not a string, then
    the default feature extractor for `config` is loaded (if it is a string). However,
    if `config` is also not given or not a string, then the default feature extractor
    for the given `task` will be loaded.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未提供，则将加载给定`model`的默认特征提取器（如果是字符串）。如果未指定`model`或不是字符串，则将加载`config`的默认特征提取器（如果是字符串）。但是，如果也未提供`config`或不是字符串，则将加载给定`task`的默认特征提取器。
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示PyTorch，也可以是`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — When passing a task
    name or a string model identifier: The specific model version to use. It can be
    a branch name, a tag name, or a commit id, since we use a git-based system for
    storing models and other artifacts on huggingface.co, so `revision` can be any
    identifier allowed by git.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision`（`str`，*可选*，默认为`"main"`）— 当传递任务名称或字符串模型标识符时：要使用的特定模型版本。它可以是分支名称、标签名称或提交ID，因为我们在huggingface.co上使用基于git的系统存储模型和其他工件，所以`revision`可以是git允许的任何标识符。'
- en: '`use_fast` (`bool`, *optional*, defaults to `True`) — Whether or not to use
    a Fast tokenizer if possible (a [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_fast`（`bool`，*可选*，默认为`True`）— 是否尽可能使用快速分词器（[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）。'
- en: '`use_auth_token` (`str` or *bool*, *optional*) — The token to use as HTTP bearer
    authorization for remote files. If `True`, will use the token generated when running
    `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auth_token`（`str`或*bool*，*可选*）— 用作远程文件的HTTP bearer授权的令牌。如果为`True`，将使用运行`huggingface-cli
    login`时生成的令牌（存储在`~/.huggingface`中）。'
- en: '`device` (`int` or `str` or `torch.device`) — Defines the device (*e.g.*, `"cpu"`,
    `"cuda:1"`, `"mps"`, or a GPU ordinal rank like `1`) on which this pipeline will
    be allocated.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`或`str`或`torch.device`）— 定义此管道将分配到的设备（例如，`"cpu"`，`"cuda:1"`，`"mps"`，或类似`1`的GPU序数等）。'
- en: '`device_map` (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*)
    — Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate`
    library is present, set `device_map="auto"` to compute the most optimized `device_map`
    automatically (see [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)
    for more information).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map`（`str`或`Dict[str, Union[int, str, torch.device]`，*可选*）— 直接作为`model_kwargs`发送（只是一个更简单的快捷方式）。当存在`accelerate`库时，设置`device_map="auto"`以自动计算最优化的`device_map`（有关更多信息，请参见[这里](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)）。'
- en: Do not use `device_map` AND `device` at the same time as they will conflict
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不要同时使用`device_map`和`device`，因为它们会发生冲突
- en: '`torch_dtype` (`str` or `torch.dtype`, *optional*) — Sent directly as `model_kwargs`
    (just a simpler shortcut) to use the available precision for this model (`torch.float16`,
    `torch.bfloat16`, … or `"auto"`).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_dtype`（`str`或`torch.dtype`，*可选*）- 直接发送为`model_kwargs`（只是一个更简单的快捷方式）以使用此模型的可用精度（`torch.float16`，`torch.bfloat16`，...或`"auto"`）。'
- en: '`trust_remote_code` (`bool`, *optional*, defaults to `False`) — Whether or
    not to allow for custom code defined on the Hub in their own modeling, configuration,
    tokenization or even pipeline files. This option should only be set to `True`
    for repositories you trust and in which you have read the code, as it will execute
    code present on the Hub on your local machine.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trust_remote_code`（`bool`，*可选*，默认为`False`）- 是否允许在Hub上定义的自定义代码在其自己的建模、配置、标记化甚至管道文件中执行。此选项应仅对您信任的存储库设置为`True`，并且您已经阅读了代码，因为它将在本地机器上执行Hub上存在的代码。'
- en: '`model_kwargs` (`Dict[str, Any]`, *optional*) — Additional dictionary of keyword
    arguments passed along to the model’s `from_pretrained(..., **model_kwargs)` function.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_kwargs`（`Dict[str, Any]`，*可选*）- 传递给模型的`from_pretrained(..., **model_kwargs)`函数的其他关键字参数字典。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the specific pipeline init (see the documentation for the corresponding
    pipeline class for possible values).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）- 传递给特定管道初始化的其他关键字参数（请参阅相应管道类的文档以获取可能的值）。'
- en: Returns
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)'
- en: A suitable pipeline for the task.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 适合任务的管道。
- en: Utility factory method to build a [Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 构建[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)的实用工厂方法。
- en: 'Pipelines are made of:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 管道由以下组成：
- en: A [tokenizer](tokenizer) in charge of mapping raw textual input to token.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责将原始文本输入映射到标记的[分词器](tokenizer)。
- en: A [model](model) to make predictions from the inputs.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从输入中进行预测的[模型](model)。
- en: Some (optional) post processing for enhancing model’s output.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些（可选的）后处理以增强模型的输出。
- en: 'Examples:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pipeline batching
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道批处理
- en: All pipelines can use batching. This will work whenever the pipeline uses its
    streaming ability (so when passing lists or `Dataset` or `generator`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有管道都可以使用批处理。每当管道使用其流式处理能力时（因此当传递列表或`Dataset`或`generator`时），它将起作用。
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: However, this is not automatically a win for performance. It can be either a
    10x speedup or 5x slowdown depending on hardware, data and the actual model being
    used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不自动意味着性能提升。它可能是10倍的加速或5倍的减速，取决于硬件、数据和实际使用的模型。
- en: 'Example where it’s mostly a speedup:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 主要是加速的示例：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Example where it’s most a slowdown:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 主要是减速的示例：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is a occasional very long sentence compared to the other. In that case,
    the **whole** batch will need to be 400 tokens long, so the whole batch will be
    [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on bigger
    batches, the program simply crashes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他句子相比，这是一个偶尔非常长的句子。在这种情况下，整个批次将需要400个标记长，因此整个批次将是[64, 400]而不是[64, 4]，导致严重减速。更糟糕的是，在更大的批次上，程序会直接崩溃。
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'There are no good (general) solutions for this problem, and your mileage may
    vary depending on your use cases. Rule of thumb:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题没有好的（通用）解决方案，您的使用情况可能会有所不同。经验法则：
- en: 'For users, a rule of thumb is:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用户，一个经验法则是：
- en: '**Measure performance on your load, with your hardware. Measure, measure, and
    keep measuring. Real numbers are the only way to go.**'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在您的负载上测量性能，使用您的硬件。测量，测量，继续测量。真实数字是唯一的方法。**'
- en: If you are latency constrained (live product doing inference), don’t batch.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您受到延迟约束（进行推断的实时产品），则不要批处理。
- en: If you are using CPU, don’t batch.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在使用CPU，则不要批处理。
- en: 'If you are using throughput (you want to run your model on a bunch of static
    data), on GPU, then:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在使用吞吐量（希望在一堆静态数据上运行模型），在GPU上，则：
- en: If you have no clue about the size of the sequence_length (“natural” data),
    by default don’t batch, measure and try tentatively to add it, add OOM checks
    to recover when it will fail (and it will at some point if you don’t control the
    sequence_length.)
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您对序列长度的大小一无所知（“自然”数据），默认情况下不要批处理，测量并尝试试探性地添加它，添加OOM检查以在失败时恢复（如果您不控制序列长度，它将在某个时候失败）。
- en: If your sequence_length is super regular, then batching is more likely to be
    VERY interesting, measure and push it until you get OOMs.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的序列长度非常规则，则批处理更有可能非常有趣，测量并推动它直到出现OOM。
- en: The larger the GPU the more likely batching is going to be more interesting
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU越大，批处理就越有可能更有趣
- en: As soon as you enable batching, make sure you can handle OOMs nicely.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦启用批处理，请确保您可以很好地处理OOM。
- en: Pipeline chunk batching
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道块批处理
- en: '`zero-shot-classification` and `question-answering` are slightly specific in
    the sense, that a single input might yield multiple forward pass of a model. Under
    normal circumstances, this would yield issues with `batch_size` argument.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`zero-shot-classification`和`question-answering`在某种意义上略有特殊，因为单个输入可能会导致模型的多次前向传递。在正常情况下，这将导致`batch_size`参数出现问题。'
- en: 'In order to circumvent this issue, both of these pipelines are a bit specific,
    they are `ChunkPipeline` instead of regular `Pipeline`. In short:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了规避这个问题，这两个管道都有点特殊，它们是`ChunkPipeline`而不是常规的`Pipeline`。简而言之：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now becomes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在变成了：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This should be very transparent to your code because the pipelines are used
    in the same way.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这对您的代码应该非常透明，因为管道的使用方式相同。
- en: This is a simplified view, since the pipeline can handle automatically the batch
    to ! Meaning you don’t have to care about how many forward passes you inputs are
    actually going to trigger, you can optimize the `batch_size` independently of
    the inputs. The caveats from the previous section still apply.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简化的视图，因为管道可以自动处理批处理！这意味着您无需关心实际将触发多少前向传递，您可以独立于输入优化`batch_size`。前一节中的注意事项仍然适用。
- en: Pipeline custom code
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道自定义代码
- en: If you want to override a specific pipeline.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要覆盖特定管道。
- en: Don’t hesitate to create an issue for your task at hand, the goal of the pipeline
    is to be easy to use and support most cases, so `transformers` could maybe support
    your use case.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 不要犹豫为您手头的任务创建一个问题，管道的目标是易于使用并支持大多数情况，因此`transformers`可能支持您的用例。
- en: 'If you want to try simply you can:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想简单尝试，可以：
- en: Subclass your pipeline of choice
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子类化您选择的管道
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That should enable you to do all the custom code you want.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该使您能够执行所有您想要的自定义代码。
- en: Implementing a pipeline
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现管道
- en: '[Implementing a new pipeline](../add_new_pipeline)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[实现新管道](../add_new_pipeline)'
- en: Audio
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音频
- en: Pipelines available for audio tasks include the following.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 音频任务可用的管道包括以下内容。
- en: AudioClassificationPipeline
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频分类管道
- en: '### `class transformers.AudioClassificationPipeline`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AudioClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L66)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L66)'
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该模型将由管道用于进行预测。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该tokenizer将被管道用于为模型编码数据。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为 -1) — CPU/GPU支持的设备序数。将其设置为 -1 将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Audio classification pipeline using any `AutoModelForAudioClassification`. This
    pipeline predicts the class of a raw waveform or an audio file. In case of an
    audio file, ffmpeg should be installed to support multiple audio formats.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`AutoModelForAudioClassification`的音频分类管道。该管道预测原始波形或音频文件的类别。在音频文件的情况下，应安装ffmpeg以支持多种音频格式。
- en: 'Example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"audio-classification"`.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载此管道："audio-classification"。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=audio-classification).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?filter=audio-classification)上查看可用模型的列表。
- en: '#### `__call__`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L103)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L103)'
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`np.ndarray` or `bytes` or `str` or `dict`) — The inputs is either
    :'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs` (`np.ndarray` 或 `bytes` 或 `str` 或 `dict`) — 输入可以是：'
- en: '`str` that is the filename of the audio file, the file will be read at the
    correct sampling rate to get the waveform using *ffmpeg*. This requires *ffmpeg*
    to be installed on the system.'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`str` — 音频文件的文件名，文件将以正确的采样率读取以获取波形，使用*ffmpeg*。这需要在系统上安装*ffmpeg*。'
- en: '`bytes` it is supposed to be the content of an audio file and is interpreted
    by *ffmpeg* in the same way.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bytes` 应该是音频文件的内容，并由*ffmpeg*以相同方式解释。'
- en: (`np.ndarray` of shape (n, ) of type `np.float32` or `np.float64`) Raw audio
    at the correct sampling rate (no further check will be done)
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`np.ndarray`，形状为(n, )，类型为`np.float32`或`np.float64`) — 在正确的采样率下的原始音频（不会进行进一步检查）
- en: '`dict` form can be used to pass raw audio sampled at arbitrary `sampling_rate`
    and let this pipeline do the resampling. The dict must be either be in the format
    `{"sampling_rate": int, "raw": np.array}`, or `{"sampling_rate": int, "array":
    np.array}`, where the key `"raw"` or `"array"` is used to denote the raw audio
    waveform.'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dict` — 可以用于传递以任意`sampling_rate`采样的原始音频，并让此管道进行重新采样。字典必须是以下格式之一：`{"sampling_rate":
    int, "raw": np.array}`或`{"sampling_rate": int, "array": np.array}`，其中键`"raw"`或`"array"`用于表示原始音频波形。'
- en: '`top_k` (`int`, *optional*, defaults to None) — The number of top labels that
    will be returned by the pipeline. If the provided number is `None` or higher than
    the number of labels available in the model configuration, it will default to
    the number of labels.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`，*可选*，默认为None) — 管道将返回的顶部标签数。如果提供的数字为`None`或高于模型配置中可用标签的数量，则默认为标签数。'
- en: Returns
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list of `dict` with the following keys
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一个带有以下键的`dict`列表
- en: '`label` (`str`) — The label predicted.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label` (`str`) — 预测的标签。'
- en: '`score` (`float`) — The corresponding probability.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` (`float`) — 相应的概率。'
- en: Classify the sequence(s) given as inputs. See the [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)
    documentation for more information.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定的输入序列进行分类。有关更多信息，请参阅[AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)文档。
- en: AutomaticSpeechRecognitionPipeline
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutomaticSpeechRecognitionPipeline
- en: '### `class transformers.AutomaticSpeechRecognitionPipeline`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AutomaticSpeechRecognitionPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L134)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L134)'
- en: '[PRE18]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该模型将被管道用于进行预测。这需要是一个继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`feature_extractor` ([SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor))
    — The feature extractor that will be used by the pipeline to encode waveform for
    the model.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` ([SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor))
    — 该特征提取器将被管道用于为模型编码波形。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该分词器将被管道用于为模型编码数据。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`decoder` (`pyctcdecode.BeamSearchDecoderCTC`, *optional*) — [PyCTCDecode’s
    BeamSearchDecoderCTC](https://github.com/kensho-technologies/pyctcdecode/blob/2fd33dc37c4111417e08d89ccd23d28e9b308d19/pyctcdecode/decoder.py#L180)
    can be passed for language model boosted decoding. See [Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)
    for more information.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` (`pyctcdecode.BeamSearchDecoderCTC`, *可选*) — 可以传递[PyCTCDecode的BeamSearchDecoderCTC](https://github.com/kensho-technologies/pyctcdecode/blob/2fd33dc37c4111417e08d89ccd23d28e9b308d19/pyctcdecode/decoder.py#L180)以进行语言模型增强解码。有关更多信息，请参阅[Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)。'
- en: '`chunk_length_s` (`float`, *optional*, defaults to 0) — The input length for
    in each chunk. If `chunk_length_s = 0` then chunking is disabled (default).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_length_s` (`float`，*可选*，默认为0) — 每个块中的输入长度。如果`chunk_length_s = 0`，则禁用分块（默认）。'
- en: For more information on how to effectively use `chunk_length_s`, please have
    a look at the [ASR chunking blog post](https://huggingface.co/blog/asr-chunking).
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关如何有效使用 `chunk_length_s` 的更多信息，请查看[ASR分块博文](https://huggingface.co/blog/asr-chunking)。
- en: '`stride_length_s` (`float`, *optional*, defaults to `chunk_length_s / 6`) —
    The length of stride on the left and right of each chunk. Used only with `chunk_length_s
    > 0`. This enables the model to *see* more context and infer letters better than
    without this context but the pipeline discards the stride bits at the end to make
    the final reconstitution as perfect as possible.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride_length_s` (`float`, *可选*, 默认为 `chunk_length_s / 6`) — 每个块左右两侧的步幅长度。仅在
    `chunk_length_s > 0` 时使用。这使得模型能够*看到*更多的上下文，并比没有上下文更好地推断字母，但管道会丢弃末尾的步幅位，以使最终重构尽可能完美。'
- en: For more information on how to effectively use `stride_length_s`, please have
    a look at the [ASR chunking blog post](https://huggingface.co/blog/asr-chunking).
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关如何有效使用 `stride_length_s` 的更多信息，请查看[ASR分块博文](https://huggingface.co/blog/asr-chunking)。
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed. If no framework
    is specified, will default to the one currently installed. If no framework is
    specified and both frameworks are installed, will default to the framework of
    the `model`, or to PyTorch if no model is provided.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表PyTorch或 `"tf"` 代表TensorFlow。指定的框架必须已安装。如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则默认使用
    `model` 的框架，或者如果未提供模型，则默认使用PyTorch。'
- en: '`device` (Union[`int`, `torch.device`], *optional*) — Device ordinal for CPU/GPU
    supports. Setting this to `None` will leverage CPU, a positive will run the model
    on the associated CUDA device id.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (Union[`int`, `torch.device`], *可选*) — CPU/GPU支持的设备序数。将其设置为 `None`
    将使用CPU，将其设置为正数将在关联的CUDA设备上运行模型。'
- en: '`torch_dtype` (Union[`int`, `torch.dtype`], *optional*) — The data-type (dtype)
    of the computation. Setting this to `None` will use float32 precision. Set to
    `torch.float16` or `torch.bfloat16` to use half-precision in the respective dtypes.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_dtype` (Union[`int`, `torch.dtype`], *可选*) — 计算的数据类型（dtype）。将其设置为 `None`
    将使用float32精度。设置为 `torch.float16` 或 `torch.bfloat16` 将使用相应dtype的半精度。'
- en: Pipeline that aims at extracting spoken text contained within some audio.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在从某些音频中提取包含的口语文本的管道。
- en: The input can be either a raw waveform or a audio file. In case of the audio
    file, ffmpeg should be installed for to support multiple audio formats
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可以是原始波形或音频文件。在音频文件的情况下，需要安装ffmpeg以支持多种音频格式
- en: 'Example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: '#### `__call__`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L229)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L229)'
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`np.ndarray` or `bytes` or `str` or `dict`) — The inputs is either
    :'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs` (`np.ndarray` 或 `bytes` 或 `str` 或 `dict`) — 输入可以是：'
- en: '`str` that is either the filename of a local audio file, or a public URL address
    to download the audio file. The file will be read at the correct sampling rate
    to get the waveform using *ffmpeg*. This requires *ffmpeg* to be installed on
    the system.'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`str`，可以是本地音频文件的文件名，也可以是下载音频文件的公共URL地址。文件将以正确的采样率读取，以使用*ffmpeg*获取波形。这需要系统上安装*ffmpeg*。'
- en: '`bytes` it is supposed to be the content of an audio file and is interpreted
    by *ffmpeg* in the same way.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bytes` 应该是音频文件内容，并由*ffmpeg*以相同方式解释。'
- en: (`np.ndarray` of shape (n, ) of type `np.float32` or `np.float64`) Raw audio
    at the correct sampling rate (no further check will be done)
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`np.ndarray` 的形状为 (n, )，类型为 `np.float32` 或 `np.float64`) — 以正确采样率的原始音频（不会进行进一步检查）
- en: '`dict` form can be used to pass raw audio sampled at arbitrary `sampling_rate`
    and let this pipeline do the resampling. The dict must be in the format `{"sampling_rate":
    int, "raw": np.array}` with optionally a `"stride": (left: int, right: int)` than
    can ask the pipeline to treat the first `left` samples and last `right` samples
    to be ignored in decoding (but used at inference to provide more context to the
    model). Only use `stride` with CTC models.'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '可以使用`dict`形式传递以任意`sampling_rate`采样的原始音频，并让此管道进行重新采样。字典必须采用格式 `{"sampling_rate":
    int, "raw": np.array}`，可选地包含一个 `"stride": (left: int, right: int)`，可以要求管道在解码时忽略前
    `left` 个样本和最后 `right` 个样本（但在推理中使用以向模型提供更多上下文）。仅在CTC模型中使用 `stride`。'
- en: '`return_timestamps` (*optional*, `str` or `bool`) — Only available for pure
    CTC models (Wav2Vec2, HuBERT, etc) and the Whisper model. Not available for other
    sequence-to-sequence models.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_timestamps` (*可选*, `str` 或 `bool`) — 仅适用于纯CTC模型（Wav2Vec2、HuBERT等）和Whisper模型。不适用于其他序列到序列模型。'
- en: 'For CTC models, timestamps can take one of two formats:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于CTC模型，时间戳可以采用以下两种格式之一：
- en: '`"char"`: the pipeline will return timestamps along the text for every character
    in the text. For instance, if you get `[{"text": "h", "timestamp": (0.5, 0.6)},
    {"text": "i", "timestamp": (0.7, 0.9)}]`, then it means the model predicts that
    the letter “h” was spoken after `0.5` and before `0.6` seconds.'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"char"`: 管道将为文本中的每个字符返回时间戳。例如，如果您得到 `[{"text": "h", "timestamp": (0.5, 0.6)},
    {"text": "i", "timestamp": (0.7, 0.9)}]`，则表示模型预测字母“h”在 `0.5` 秒后和 `0.6` 秒前被发音。'
- en: '`"word"`: the pipeline will return timestamps along the text for every word
    in the text. For instance, if you get `[{"text": "hi ", "timestamp": (0.5, 0.9)},
    {"text": "there", "timestamp": (1.0, 1.5)}]`, then it means the model predicts
    that the word “hi” was spoken after `0.5` and before `0.9` seconds.'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"word"`: 管道将为文本中的每个单词返回时间戳。例如，如果您得到 `[{"text": "hi ", "timestamp": (0.5, 0.9)},
    {"text": "there", "timestamp": (1.0, 1.5)}]`，则表示模型预测单词“hi”在 `0.5` 秒后和 `0.9` 秒前被发音。'
- en: 'For the Whisper model, timestamps can take one of two formats:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于Whisper模型，时间戳可以采用以下两种格式之一：
- en: '`"word"`: same as above for word-level CTC timestamps. Word-level timestamps
    are predicted through the *dynamic-time warping (DTW)* algorithm, an approximation
    to word-level timestamps by inspecting the cross-attention weights.'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"word"`: 与单词级CTC时间戳相同。单词级时间戳通过*动态时间规整（DTW）*算法预测，通过检查交叉注意力权重来近似单词级时间戳。'
- en: '`True`: the pipeline will return timestamps along the text for *segments* of
    words in the text. For instance, if you get `[{"text": " Hi there!", "timestamp":
    (0.5, 1.5)}]`, then it means the model predicts that the segment “Hi there!” was
    spoken after `0.5` and before `1.5` seconds. Note that a segment of text refers
    to a sequence of one or more words, rather than individual words as with word-level
    timestamps.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`：管道将在文本中为*单词段*返回时间戳。例如，如果您获得`[{"text": " Hi there!", "timestamp": (0.5,
    1.5)}]`，则表示模型预测段“Hi there!”在`0.5`秒后和`1.5`秒前被说出。请注意，文本段指的是一个或多个单词的序列，而不是单词级时间戳。'
- en: '`generate_kwargs` (`dict`, *optional*) — The dictionary of ad-hoc parametrization
    of `generate_config` to be used for the generation call. For a complete overview
    of generate, check the [following guide](https://huggingface.co/docs/transformers/en/main_classes/text_generation).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_kwargs` (`dict`, *可选*) — 用于生成调用的`generate_config`的自定义参数字典。有关generate的完整概述，请查看[以下指南](https://huggingface.co/docs/transformers/en/main_classes/text_generation)。'
- en: '`max_new_tokens` (`int`, *optional*) — The maximum numbers of tokens to generate,
    ignoring the number of tokens in the prompt.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens` (`int`, *可选*) — 要生成的最大标记数，忽略提示中的标记数。'
- en: Returns
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict`'
- en: 'A dictionary with the following keys:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 具有以下键的字典：
- en: '`text` (`str`): The recognized text.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`): 识别的文本。'
- en: '`chunks` (*optional(, `List[Dict]`) When using `return_timestamps`, the `chunks`
    will become a list containing all the various text chunks identified by the model,*
    e.g.* `[{"text": "hi ", "timestamp": (0.5, 0.9)}, {"text": "there", "timestamp":
    (1.0, 1.5)}]`. The original full text can roughly be recovered by doing `"".join(chunk["text"]
    for chunk in output["chunks"])`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunks` (*可选(, `List[Dict]`) 当使用`return_timestamps`时，`chunks`将变成一个包含模型识别的各种文本块的列表，例如`[{"text":
    "hi ", "timestamp": (0.5, 0.9)}, {"text": "there", "timestamp": (1.0, 1.5)}]`。原始完整文本可以通过`"".join(chunk["text"]
    for chunk in output["chunks"])`来粗略恢复。'
- en: Transcribe the audio sequence(s) given as inputs to text. See the [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)
    documentation for more information.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 将给定的音频序列转录为文本。有关更多信息，请参阅[AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)文档。
- en: TextToAudioPipeline
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本到音频管道
- en: '### `class transformers.TextToAudioPipeline`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TextToAudioPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L27)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L27)'
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Text-to-audio generation pipeline using any `AutoModelForTextToWaveform` or
    `AutoModelForTextToSpectrogram`. This pipeline generates an audio file from an
    input text and optional other conditional inputs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`AutoModelForTextToWaveform`或`AutoModelForTextToSpectrogram`的文本到音频生成管道。此管道从输入文本和可选的其他条件输入生成音频文件。
- en: 'Example:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline tutorial](../pipeline_tutorial)中使用管道的基础知识
- en: You can specify parameters passed to the model by using `TextToAudioPipeline.__call__.forward_params`
    or `TextToAudioPipeline.__call__.generate_kwargs`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用`TextToAudioPipeline.__call__.forward_params`或`TextToAudioPipeline.__call__.generate_kwargs`来指定传递给模型的参数。
- en: 'Example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE23]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifiers: `"text-to-speech"` or `"text-to-audio"`.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道目前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载：`"text-to-speech"`或`"text-to-audio"`。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-to-speech).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?filter=text-to-speech)上查看可用模型列表。
- en: '#### `__call__`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L160)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L160)'
- en: '[PRE24]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_inputs` (`str` or `List[str]`) — The text(s) to generate.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_inputs` (`str`或`List[str]`) — 要生成的文本。'
- en: '`forward_params` (`dict`, *optional*) — Parameters passed to the model generation/forward
    method. `forward_params` are always passed to the underlying model.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward_params` (`dict`, *可选*) — 传递给模型生成/前向方法的参数。`forward_params`始终传递给底层模型。'
- en: '`generate_kwargs` (`dict`, *optional*) — The dictionary of ad-hoc parametrization
    of `generate_config` to be used for the generation call. For a complete overview
    of generate, check the [following guide](https://huggingface.co/docs/transformers/en/main_classes/text_generation).
    `generate_kwargs` are only passed to the underlying model if the latter is a generative
    model.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_kwargs` (`dict`, *可选*) — 用于生成调用的`generate_config`的自定义参数字典。有关generate的完整概述，请查看[以下指南](https://huggingface.co/docs/transformers/en/main_classes/text_generation)。`generate_kwargs`仅在底层模型是生成模型时才传递给底层模型。'
- en: Returns
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `dict` or a list of `dict`
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`dict`或`dict`的列表
- en: 'The dictionaries have two keys:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 字典有两个键：
- en: '`audio` (`np.ndarray` of shape `(nb_channels, audio_length)`) — The generated
    audio waveform.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio` (`np.ndarray`，形状为`(nb_channels, audio_length)`) — 生成的音频波形。'
- en: '`sampling_rate` (`int`) — The sampling rate of the generated audio waveform.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`) — 生成的音频波形的采样率。'
- en: Generates speech/audio from the inputs. See the [TextToAudioPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextToAudioPipeline)
    documentation for more information.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入生成语音/音频。有关更多信息，请参阅[TextToAudioPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextToAudioPipeline)文档。
- en: ZeroShotAudioClassificationPipeline
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeroShotAudioClassificationPipeline
- en: '### `class transformers.ZeroShotAudioClassificationPipeline`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ZeroShotAudioClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L32)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L32)'
- en: '[PRE25]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，用于PyTorch，以及继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的模型，用于TensorFlow。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 管道将用于为模型编码数据的分词器。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str`或`ModelCard`, *可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表PyTorch 或 `"tf"` 代表TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，将默认使用`model`的框架，或者如果未提供模型，则默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为8) — 当管道将使用 *DataLoader*（传递数据集时，在Pytorch模型的GPU上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为1) — 当管道将使用 *DataLoader*（传递数据集时，在Pytorch模型的GPU上），要使用的批次大小，对于推断来说，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为-1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Zero shot audio classification pipeline using `ClapModel`. This pipeline predicts
    the class of an audio when you provide an audio and a set of `candidate_labels`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ClapModel`进行零射击音频分类管道。此管道在提供音频和一组`candidate_labels`时预测音频的类。
- en: 'Example:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
    This audio classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-audio-classification"`. See the
    list of available models on [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-audio-classification).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在[pipeline教程](../pipeline_tutorial)中了解如何使用管道的基础知识。此音频分类管道目前可以通过以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载："zero-shot-audio-classification"。在[huggingface.co/models](https://huggingface.co/models?filter=zero-shot-audio-classification)上查看可用模型的列表。
- en: '#### `__call__`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L64)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L64)'
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`audios` (`str`, `List[str]`, `np.array` or `List[np.array]`) — The pipeline
    handles three types of inputs:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audios` (`str`, `List[str]`, `np.array`或`List[np.array]`) — 管道处理三种类型的输入：'
- en: A string containing a http link pointing to an audio
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向音频的http链接的字符串
- en: A string containing a local path to an audio
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含音频本地路径的字符串
- en: An audio loaded in numpy
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载在numpy中的音频
- en: '`candidate_labels` (`List[str]`) — The candidate labels for this audio'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`candidate_labels` (`List[str]`) — 此音频的候选标签'
- en: '`hypothesis_template` (`str`, *optional*, defaults to `"This is a sound of
    {}"`) — The sentence used in cunjunction with *candidate_labels* to attempt the
    audio classification by replacing the placeholder with the candidate_labels. Then
    likelihood is estimated by using logits_per_audio'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hypothesis_template`（`str`，*可选*，默认为`"This is a sound of {}"`）- 与*candidate_labels*一起使用的句子，通过将占位符替换为candidate_labels尝试音频分类。然后通过使用logits_per_audio来估计可能性。'
- en: Assign labels to the audio(s) passed as inputs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为传入的音频分配标签。
- en: Computer vision
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Pipelines available for computer vision tasks include the following.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉任务可用的管道包括以下内容。
- en: DepthEstimationPipeline
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DepthEstimationPipeline
- en: '### `class transformers.DepthEstimationPipeline`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DepthEstimationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L22)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L22)'
- en: '[PRE28]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）-
    该模型将被管道用于进行预测。这需要是一个继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）-
    该tokenizer将被管道用于对数据进行编码以供模型使用。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str`或`ModelCard`，*可选*）- 为此管道的模型分配的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）- 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为`""`）- 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`（`int`，*可选*，默认为8）- 当管道将使用*DataLoader*（在传递数据集时，在PyTorch模型的GPU上）时，要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为1）- 当管道将使用*DataLoader*（在传递数据集时，在PyTorch模型的GPU上）时，要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）-
    负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为-1）- CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为`False`）- 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Depth estimation pipeline using any `AutoModelForDepthEstimation`. This pipeline
    predicts the depth of an image.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`AutoModelForDepthEstimation`的深度估计管道。该管道预测图像的深度。
- en: 'Example:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE29]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识。
- en: 'This depth estimation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"depth-estimation"`.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 此深度估计管道目前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中加载："depth-estimation"。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=depth-estimation).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?filter=depth-estimation)上查看可用模型的列表。
- en: '#### `__call__`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L53)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L53)'
- en: '[PRE30]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：'
- en: A string containing a http link pointing to an image
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的http链接的字符串
- en: A string containing a local path to an image
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的本地路径的字符串
- en: An image loaded in PIL directly
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接加载的PIL图像
- en: 'The pipeline accepts either a single image or a batch of images, which must
    then be passed as a string. Images in a batch must all be in the same format:
    all as http links, all as local paths, or all as PIL images.'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该管道接受单个图像或一批图像，然后必须将它们作为字符串传递。批处理中的图像必须全部采用相同的格式：全部作为http链接，全部作为本地路径，或全部作为PIL图像。
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`, *可选*, 默认为5) — 管道将返回的前几个标签的数量。如果提供的数字高于模型配置中可用的标签数量，则将默认为标签数量。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *可选*, 默认为None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为None，则不设置超时，调用可能会永远阻塞。'
- en: Assign labels to the image(s) passed as inputs.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 为传递的图像分配标签。
- en: ImageClassificationPipeline
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageClassificationPipeline
- en: '### `class transformers.ImageClassificationPipeline`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L50)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L50)'
- en: '[PRE31]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，对于PyTorch和继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 管道将用于为模型编码数据的分词器。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示PyTorch，也可以是`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch框架。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为`""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为8) — 当管道将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为1) — 当管道将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的批次大小，对于推断，这并不总是有益，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为-1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function_to_apply` (`str`, *可选*, 默认为`"default"`) — 用于从模型输出中提取分数的函数。接受四个不同的值：'
- en: '`"default"`: if the model has a single label, will apply the sigmoid function
    on the output. If the model has several labels, will apply the softmax function
    on the output.'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"default"`: 如果模型只有一个标签，将在输出上应用sigmoid函数。如果模型有多个标签，将在输出上应用softmax函数。'
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sigmoid`: 在输出上应用sigmoid函数。'
- en: '`"softmax"`: Applies the softmax function on the output.'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"softmax"`: 在输出上应用softmax函数。'
- en: '`"none"`: Does not apply any function on the output.'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`: 不在输出上应用任何函数。'
- en: Image classification pipeline using any `AutoModelForImageClassification`. This
    pipeline predicts the class of an image.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何 `AutoModelForImageClassification` 的图像分类管道。此管道预测图像的类别。
- en: 'Example:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE32]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在 [pipeline教程](../pipeline_tutorial) 中使用管道的基础知识。
- en: 'This image classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"image-classification"`.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图像分类管道目前可以通过 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    使用以下任务标识符进行加载：`"image-classification"`。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=image-classification).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [huggingface.co/models](https://huggingface.co/models?filter=image-classification)
    上可用模型的列表。
- en: '#### `__call__`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L111)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L111)'
- en: '[PRE33]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：'
- en: A string containing a http link pointing to an image
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的http链接的字符串
- en: A string containing a local path to an image
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的本地路径的字符串
- en: An image loaded in PIL directly
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接加载的PIL图像
- en: 'The pipeline accepts either a single image or a batch of images, which must
    then be passed as a string. Images in a batch must all be in the same format:
    all as http links, all as local paths, or all as PIL images.'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管道接受单个图像或一批图像，然后必须将它们作为字符串传递。批处理中的图像必须全部采用相同的格式：全部作为http链接，全部作为本地路径，或全部作为PIL图像。
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function_to_apply` (`str`, *可选*, 默认为`"default"`) — 应用于模型输出以检索分数的函数。接受四个不同的值：'
- en: 'If this argument is not specified, then it will apply the following functions
    according to the number of labels:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定此参数，则将根据标签数应用以下函数：
- en: If the model has a single label, will apply the sigmoid function on the output.
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型只有一个标签，将在输出上应用sigmoid函数。
- en: If the model has several labels, will apply the softmax function on the output.
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型有多个标签，将在输出上应用softmax函数。
- en: 'Possible values are:'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能的值有：
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sigmoid`: 在输出上应用sigmoid函数。'
- en: '`"softmax"`: Applies the softmax function on the output.'
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"softmax"`: 在输出上应用softmax函数。'
- en: '`"none"`: Does not apply any function on the output.'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`: 不在输出上应用任何函数。'
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`, *可选*, 默认为5) — 管道将返回的前k个标签数。如果提供的数字高于模型配置中可用的标签数，则默认为标签数。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *可选*, 默认为None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为None，则不设置超时，调用可能永远阻塞。'
- en: Assign labels to the image(s) passed as inputs.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为传入的图像分配标签。
- en: ImageSegmentationPipeline
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageSegmentationPipeline
- en: '### `class transformers.ImageSegmentationPipeline`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageSegmentationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L30)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L30)'
- en: '[PRE34]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于PyTorch是 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    是TensorFlow。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 管道将用于为模型编码数据的分词器。此对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表PyTorch，或者 `"tf"` 代表TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, defaults to `""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, defaults to 8) — 当管道将使用*DataLoader*（在传递数据集时，对于Pytorch模型在GPU上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用*DataLoader*（在传递数据集时，对于Pytorch模型在GPU上），要使用的批处理大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *optional*, defaults to -1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，将其设置为正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *optional*, defaults to `False`) — 指示管道输出应以二进制格式（即pickle）还是原始文本格式发生的标志。'
- en: Image segmentation pipeline using any `AutoModelForXXXSegmentation`. This pipeline
    predicts masks of objects and their classes.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`AutoModelForXXXSegmentation`的图像分割管道。该管道预测对象及其类别的掩模。
- en: 'Example:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE35]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This image segmentation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"image-segmentation"`.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 此图像分割管道目前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载："image-segmentation"。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=image-segmentation).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?filter=image-segmentation)上查看可用模型的列表。
- en: '#### `__call__`'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L97)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L97)'
- en: '[PRE36]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`str`, `List[str]`, `PIL.Image`或`List[PIL.Image]`) — 该管道处理三种类型的图像：'
- en: A string containing an HTTP(S) link pointing to an image
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的HTTP(S)链接的字符串
- en: A string containing a local path to an image
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含图像本地路径的字符串
- en: An image loaded in PIL directly
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接在PIL中加载的图像
- en: 'The pipeline accepts either a single image or a batch of images. Images in
    a batch must all be in the same format: all as HTTP(S) links, all as local paths,
    or all as PIL images.'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该管道接受单个图像或一批图像。批处理中的图像必须全部采用相同的格式：全部作为HTTP(S)链接，全部作为本地路径，或全部作为PIL图像。
- en: '`subtask` (`str`, *optional*) — Segmentation task to be performed, choose [`semantic`,
    `instance` and `panoptic`] depending on model capabilities. If not set, the pipeline
    will attempt tp resolve in the following order: `panoptic`, `instance`, `semantic`.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subtask` (`str`, *optional*) — 要执行的分割任务，根据模型的能力选择[`semantic`、`instance`和`panoptic`]。如果未设置，管道将尝试按以下顺序解析：`panoptic`、`instance`、`semantic`。'
- en: '`threshold` (`float`, *optional*, defaults to 0.9) — Probability threshold
    to filter out predicted masks.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *optional*, defaults to 0.9) — 用于过滤预测掩模的概率阈值。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — 在将预测掩模转换为二进制值时使用的阈值。'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.5) — Mask
    overlap threshold to eliminate small, disconnected segments.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.5) — 用于消除小的、不连续段的掩模重叠阈值。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *optional*, defaults to None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为None，则不设置超时，并且调用可能会永远阻塞。'
- en: Perform segmentation (detect masks & classes) in the image(s) passed as inputs.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在作为输入传递的图像中执行分割（检测掩模和类别）。
- en: ImageToImagePipeline
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageToImagePipeline
- en: '### `class transformers.ImageToImagePipeline`'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageToImagePipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L39)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L39)'
- en: '[PRE37]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，用于PyTorch，以及继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的模型，用于TensorFlow。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该tokenizer将被管道用于为模型编码数据。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str`或`ModelCard`, *可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示PyTorch，也可以是`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架，并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为`""`) — 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为8) — 当管道将使用*DataLoader*（在传递数据集时，在PyTorch模型的GPU上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为1) — 当管道将使用*DataLoader*（在传递数据集时，在PyTorch模型的GPU上），要使用的批处理大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为-1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Image to Image pipeline using any `AutoModelForImageToImage`. This pipeline
    generates an image based on a previous image input.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`AutoModelForImageToImage`的图像到图像管道。该管道基于先前的图像输入生成图像。
- en: 'Example:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE38]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This image to image pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"image-to-image"`.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 此图像到图像管道目前可以从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中使用以下任务标识符加载：`"image-to-image"`。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=image-to-image).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[huggingface.co/models](https://huggingface.co/models?filter=image-to-image)上可用模型的列表。
- en: '#### `__call__`'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L87)'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L87)'
- en: '[PRE39]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`str`, `List[str]`, `PIL.Image`或`List[PIL.Image]`) — 该管道处理三种类型的图像：'
- en: A string containing a http link pointing to an image
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的http链接的字符串
- en: A string containing a local path to an image
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含本地图像路径的字符串
- en: An image loaded in PIL directly
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接加载的PIL图像
- en: 'The pipeline accepts either a single image or a batch of images, which must
    then be passed as a string. Images in a batch must all be in the same format:
    all as http links, all as local paths, or all as PIL images.'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管道接受单个图像或一批图像，然后必须将它们作为字符串传递。批处理中的图像必须全部采用相同的格式：全部作为http链接，全部作为本地路径，或全部作为PIL图像。
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is used and the
    call may block forever.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *可选*, 默认为None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为None，则不使用超时，调用可能会永远阻塞。'
- en: Transform the image(s) passed as inputs.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 转换传递的图像。
- en: ObjectDetectionPipeline
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ObjectDetectionPipeline
- en: '### `class transformers.ObjectDetectionPipeline`'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ObjectDetectionPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L26)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L26)'
- en: '[PRE40]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，用于 PyTorch，以及继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    的模型，用于 TensorFlow。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 管道将用于为模型编码数据的分词器。此对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *optional*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch，也可以是 `"tf"`
    代表 TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则将默认使用 PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch
    模型的 GPU 上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用 *DataLoader*（在传递数据集时，在
    Pytorch 模型的 GPU 上），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *optional*, 默认为 -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的
    CUDA 设备 id 上运行模型。您也可以传递原生的 `torch.device` 或一个 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *optional*, 默认为 `False`) — 指示管道输出应以二进制格式（即 pickle）还是原始文本格式发生的标志。'
- en: Object detection pipeline using any `AutoModelForObjectDetection`. This pipeline
    predicts bounding boxes of objects and their classes.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何 `AutoModelForObjectDetection` 的对象检测管道。此管道预测对象的边界框和它们的类别。
- en: 'Example:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE41]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [pipeline 教程](../pipeline_tutorial) 中了解有关使用管道的基础知识
- en: 'This object detection pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"object-detection"`.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 此对象检测管道目前可以通过以下任务标识符从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    加载："object-detection"。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=object-detection).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [huggingface.co/models](https://huggingface.co/models?filter=object-detection)
    上可用模型的列表。
- en: '#### `__call__`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L72)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L72)'
- en: '[PRE42]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：'
- en: A string containing an HTTP(S) link pointing to an image
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的 HTTP(S) 链接的字符串
- en: A string containing a local path to an image
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含本地图像路径的字符串
- en: An image loaded in PIL directly
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接加载的 PIL 图像
- en: 'The pipeline accepts either a single image or a batch of images. Images in
    a batch must all be in the same format: all as HTTP(S) links, all as local paths,
    or all as PIL images.'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管道接受单个图像或一批图像。批处理中的图像必须全部采用相同的格式：全部作为 HTTP(S) 链接，全部作为本地路径，或全部作为 PIL 图像。
- en: '`threshold` (`float`, *optional*, defaults to 0.9) — The probability necessary
    to make a prediction.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *optional*, 默认为 0.9) — 进行预测所需的概率。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout`（`float`，*可选*，默认为 None）— 从网络获取图像的最大等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能永远阻塞。'
- en: Detect objects (bounding boxes & classes) in the image(s) passed as inputs.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 检测作为输入传递的图像中的对象（边界框和类）。
- en: VideoClassificationPipeline
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视频分类管道
- en: '### `class transformers.VideoClassificationPipeline`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VideoClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L21)'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L21)'
- en: '[PRE43]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Parameters
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）—
    该模型将被管道用于进行预测。这需要是继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于 TensorFlow 是继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）—
    该管道将用于为模型编码数据的分词器。此对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str` 或 `ModelCard`，*可选*）— 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）— 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为 `""`）— 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`（`int`，*可选*，默认为 8）— 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU
    上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为 1）— 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的 GPU
    上），要使用的批次大小，对于推断，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）—
    负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为 -1）— 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA
    设备 id 上运行模型。您也可以传递本机 `torch.device` 或 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为 `False`）— 指示管道输出是否以二进制格式（即 pickle）或原始文本格式发生的标志。'
- en: Video classification pipeline using any `AutoModelForVideoClassification`. This
    pipeline predicts the class of a video.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何 `AutoModelForVideoClassification` 的视频分类管道。此管道预测视频的类别。
- en: 'This video classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"video-classification"`.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 当前可以使用以下任务标识符从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    加载此视频分类管道：`"video-classification"`。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=video-classification).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [huggingface.co/models](https://huggingface.co/models?filter=video-classification)
    上可用模型的列表。
- en: '#### `__call__`'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L51)'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L51)'
- en: '[PRE44]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`videos` (`str`, `List[str]`) — The pipeline handles three types of videos:'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`videos`（`str`，`List[str]`）— 管道处理三种类型的视频：'
- en: A string containing a http link pointing to a video
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向视频的 http 链接的字符串
- en: A string containing a local path to a video
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含视频本地路径的字符串
- en: 'The pipeline accepts either a single video or a batch of videos, which must
    then be passed as a string. Videos in a batch must all be in the same format:
    all as http links or all as local paths.'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该管道接受单个视频或一批视频，然后必须将其作为字符串传递。批处理中的视频必须全部采用相同的格式：全部作为http链接或全部作为本地路径。
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`, *optional*, defaults to 5) — 管道将返回的前k个标签的数量。如果提供的数字高于模型配置中可用的标签数量，则默认为标签数量。'
- en: '`num_frames` (`int`, *optional*, defaults to `self.model.config.num_frames`)
    — The number of frames sampled from the video to run the classification on. If
    not provided, will default to the number of frames specified in the model configuration.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_frames` (`int`, *optional*, defaults to `self.model.config.num_frames`)
    — 从视频中采样的帧数，用于进行分类。如果未提供，则将默认为模型配置中指定的帧数。'
- en: '`frame_sampling_rate` (`int`, *optional*, defaults to 1) — The sampling rate
    used to select frames from the video. If not provided, will default to 1, i.e.
    every frame will be used.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frame_sampling_rate` (`int`, *optional*, defaults to 1) — 用于从视频中选择帧的采样率。如果未提供，则将默认为1，即将使用每一帧。'
- en: Assign labels to the video(s) passed as inputs.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 为传入的视频分配标签。
- en: ZeroShotImageClassificationPipeline
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeroShotImageClassificationPipeline
- en: '### `class transformers.ZeroShotImageClassificationPipeline`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ZeroShotImageClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L32)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L32)'
- en: '[PRE45]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的PyTorch模型和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的TensorFlow模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该tokenizer将被管道用于对数据进行编码以供模型使用。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` or `ModelCard`, *optional*) — 为该管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *optional*) — 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，则将默认为当前安装的框架。如果未指定框架且两个框架都已安装，则将默认为`model`的框架，或者如果未提供模型，则将默认为PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, defaults to `""`) — 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, defaults to 8) — 当管道将使用*DataLoader*（传递数据集时，在Pytorch模型的GPU上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用*DataLoader*（传递数据集时，在Pytorch模型的GPU上），要使用的批处理大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *optional*, defaults to -1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *optional*, defaults to `False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本形式发生的标志。'
- en: Zero shot image classification pipeline using `CLIPModel`. This pipeline predicts
    the class of an image when you provide an image and a set of `candidate_labels`.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`CLIPModel`进行零样本图像分类管道。该管道在提供图像和一组`candidate_labels`时预测图像的类别。
- en: 'Example:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE46]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在[pipeline tutorial](../pipeline_tutorial)中了解有关使用管道的基础知识
- en: 'This image classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-image-classification"`.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 当前可以使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)从中加载此图像分类管道的任务标识符为："zero-shot-image-classification"。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-image-classification).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?filter=zero-shot-image-classification)上查看可用模型列表。
- en: '#### `__call__`'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L76)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L76)'
- en: '[PRE47]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — 管道处理三种类型的图像：'
- en: A string containing a http link pointing to an image
  id: totrans-506
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的http链接的字符串
- en: A string containing a local path to an image
  id: totrans-507
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的本地路径的字符串
- en: An image loaded in PIL directly
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接在PIL中加载的图像
- en: '`candidate_labels` (`List[str]`) — The candidate labels for this image'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`candidate_labels` (`List[str]`) — 该图像的候选标签'
- en: '`hypothesis_template` (`str`, *optional*, defaults to `"This is a photo of
    {}"`) — The sentence used in cunjunction with *candidate_labels* to attempt the
    image classification by replacing the placeholder with the candidate_labels. Then
    likelihood is estimated by using logits_per_image'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hypothesis_template` (`str`, *可选*, 默认为`"This is a photo of {}"`) — 与*candidate_labels*一起使用的句子，通过替换占位符与候选标签尝试图像分类。然后通过使用logits_per_image来估计可能性'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *可选*, 默认为None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为None，则不设置超时，调用可能会永远阻塞。'
- en: Assign labels to the image(s) passed as inputs.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 为传入的图像分配标签。
- en: ZeroShotObjectDetectionPipeline
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeroShotObjectDetectionPipeline
- en: '### `class transformers.ZeroShotObjectDetectionPipeline`'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ZeroShotObjectDetectionPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L22)'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L22)'
- en: '[PRE48]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，对于TensorFlow，需要是继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该tokenizer将被管道用于对数据进行编码以供模型使用。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` or `ModelCard`, *可选*) — 为该管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为8) — 当管道将使用*DataLoader*（当传递数据集时，对于PyTorch模型在GPU上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为1) — 当管道将使用*DataLoader*（当传递数据集时，对于PyTorch模型在GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为-1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备id上运行模型。您也可以传递本机的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Zero shot object detection pipeline using `OwlViTForObjectDetection`. This pipeline
    predicts bounding boxes of objects when you provide an image and a set of `candidate_labels`.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`OwlViTForObjectDetection`进行零击目标检测管道。当您提供一张图像和一组`candidate_labels`时，此管道会预测对象的边界框。
- en: 'Example:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE49]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This object detection pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-object-detection"`.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标检测管道目前可以从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中加载，使用以下任务标识符："zero-shot-object-detection"。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-object-detection).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?filter=zero-shot-object-detection)上查看可用模型列表。
- en: '#### `__call__`'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L65)'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L65)'
- en: '[PRE50]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image` (`str`, `PIL.Image` or `List[Dict[str, Any]]`) — The pipeline handles
    three types of images:'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`str`, `PIL.Image`或`List[Dict[str, Any]]`) — 该管道处理三种类型的图像：'
- en: A string containing an http url pointing to an image
  id: totrans-540
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的http url的字符串
- en: A string containing a local path to an image
  id: totrans-541
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的本地路径的字符串
- en: An image loaded in PIL directly
  id: totrans-542
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接加载的PIL图像
- en: 'You can use this parameter to send directly a list of images, or a dataset
    or a generator like so:'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以使用此参数直接发送图像列表，数据集或生成器，如下所示：
- en: Detect objects (bounding boxes & classes) in the image(s) passed as inputs.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 检测传入的图像中的对象（边界框和类）。
- en: Natural Language Processing
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Pipelines available for natural language processing tasks include the following.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 用于自然语言处理任务的管道包括以下内容。
- en: ConversationalPipeline
  id: totrans-547
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对话管道
- en: '### `class transformers.Conversation`'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Conversation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L18)'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L18)'
- en: '[PRE51]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Parameters
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`messages` (Union[str, List[Dict[str, str]]], *optional*) — The initial messages
    to start the conversation, either a string, or a list of dicts containing “role”
    and “content” keys. If a string is passed, it is interpreted as a single message
    with the “user” role.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`messages` (Union[str, List[Dict[str, str]]], *可选*) — 开始对话的初始消息，可以是一个字符串，也可以是包含“role”和“content”键的字典列表。如果传递了一个字符串，它将被解释为具有“user”角色的单个消息。'
- en: '`conversation_id` (`uuid.UUID`, *optional*) — Unique identifier for the conversation.
    If not provided, a random UUID4 id will be assigned to the conversation.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conversation_id` (`uuid.UUID`, *可选*) — 对话的唯一标识符。如果未提供，将为对话分配一个随机的UUID4 id。'
- en: Utility class containing a conversation and its history. This class is meant
    to be used as an input to the [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
    The conversation contains several utility functions to manage the addition of
    new user inputs and generated model responses.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 包含对话及其历史记录的实用类。此类旨在用作[ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline)的输入。对话包含几个实用函数，用于管理新用户输入和生成模型响应的添加。
- en: 'Usage:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：
- en: '[PRE52]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '#### `add_user_input`'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_user_input`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L89)'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L89)'
- en: '[PRE53]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Add a user input to the conversation for the next round. This is a legacy method
    that assumes that inputs must alternate user/assistant/user/assistant, and so
    will not add multiple user messages in succession. We recommend just using `add_message`
    with role “user” instead.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 为下一轮对话添加用户输入。这是一个传统方法，假设输入必须交替用户/助手/用户/助手，因此不会连续添加多个用户消息。我们建议只使用带有“user”角色的`add_message`。
- en: '#### `append_response`'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `append_response`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L110)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L110)'
- en: '[PRE54]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This is a legacy method. We recommend just using `add_message` with an appropriate
    role instead.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个传统方法。我们建议只使用带有适当角色的`add_message`。
- en: '#### `mark_processed`'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `mark_processed`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L116)'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L116)'
- en: '[PRE55]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This is a legacy method, as the Conversation no longer distinguishes between
    processed and unprocessed user input. We set a counter here to keep behaviour
    mostly backward-compatible, but in general you should just read the messages directly
    when writing new code.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个传统方法，因为Conversation不再区分已处理和未处理的用户输入。我们在这里设置一个计数器，以保持行为大体向后兼容，但通常在编写新代码时，您应该直接阅读消息。
- en: '### `class transformers.ConversationalPipeline`'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConversationalPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L194)'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L194)'
- en: '[PRE56]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）
    — 管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，对于TensorFlow，需要是继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）
    — 管道将用于为模型编码数据的分词器。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str`或`ModelCard`，*可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示PyTorch，也可以是`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`，默认为`""`) — 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为8) — 当管道将使用*DataLoader*（当传递数据集时，在PyTorch模型的GPU上），要使用的工作程序数。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为1) — 当管道将使用*DataLoader*（当传递数据集时，在PyTorch模型的GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）
    — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为-1) — 用于CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为`False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: '`min_length_for_response` (`int`, *optional*, defaults to 32) — The minimum
    length (in number of tokens) for a response.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_length_for_response` (`int`, *可选*, 默认为32) — 响应的最小长度（标记数）。'
- en: '`minimum_tokens` (`int`, *optional*, defaults to 10) — The minimum length of
    tokens to leave for a response.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minimum_tokens` (`int`, *可选*, 默认为10) — 留给响应的最小标记长度。'
- en: Multi-turn conversational pipeline.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 多轮对话管道。
- en: 'Example:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE57]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 学习有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This conversational pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"conversational"`.'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 当前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载此对话管道：`"conversational"`。
- en: This pipeline can be used with any model that has a [chat template](https://huggingface.co/docs/transformers/chat_templating)
    set.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可与具有[聊天模板](https://huggingface.co/docs/transformers/chat_templating)设置的任何模型一起使用。
- en: '#### `__call__`'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L262)'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L262)'
- en: '[PRE58]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`conversations` (a [Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)
    or a list of [Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation))
    — Conversation to generate responses for. Inputs can also be passed as a list
    of dictionaries with `role` and `content` keys - in this case, they will be converted
    to `Conversation` objects automatically. Multiple conversations in either format
    may be passed as a list.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conversations`（一个[Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)或[Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)列表）
    — 用于生成响应的对话。输入也可以作为带有`role`和`content`键的字典列表传递 - 在这种情况下，它们将自动转换为`Conversation`对象。可以作为列表传递任何格式的多个对话。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output. generate_kwargs
    — Additional keyword arguments to pass along to the generate method of the model
    (see the generate method corresponding to your framework [here](./model#generative-models)).'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*，默认为`False`）— 是否清除文本输出中可能存在的额外空格。generate_kwargs
    — 要传递给模型的generate方法的其他关键字参数（请参阅与您的框架对应的generate方法[此处](./model#generative-models)）。'
- en: Returns
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)
    or a list of [Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '[对话](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)或[对话](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)列表'
- en: Conversation(s) with updated generated responses for those containing a new
    user input.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 包含新用户输入的对话的更新生成响应的对话。
- en: Generate responses for the conversation(s) given as inputs.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 为输入的对话生成响应。
- en: FillMaskPipeline
  id: totrans-602
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FillMaskPipeline
- en: '### `class transformers.FillMaskPipeline`'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FillMaskPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L22)'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L22)'
- en: '[PRE59]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parameters
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）—
    流水线将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）—
    流水线将用于为模型编码数据的分词器。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str`或`ModelCard`，*可选*）— 为该流水线的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为`""`）— 用于流水线的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`（`int`，*可选*，默认为8）— 当流水线将使用*DataLoader*（在传递数据集时，在PyTorch模型的GPU上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为1）— 当流水线将使用*DataLoader*（在传递数据集时，在PyTorch模型的GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用流水线进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）—
    负责解析提供的流水线参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为-1）— CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为`False`）— 指示流水线输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: '`top_k` (`int`, defaults to 5) — The number of predictions to return.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k`（`int`，默认为5）— 要返回的预测数量。'
- en: '`targets` (`str` or `List[str]`, *optional*) — When passed, the model will
    limit the scores to the passed targets instead of looking up in the whole vocab.
    If the provided targets are not in the model vocab, they will be tokenized and
    the first resulting token will be used (with a warning, and that might be slower).'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targets`（`str`或`List[str]`，*可选*）— 当传递时，模型将限制分数到传递的目标，而不是在整个词汇表中查找。如果提供的目标不在模型词汇表中，它们将被标记化，第一个生成的标记将被使用（带有警告，可能会更慢）。'
- en: Masked language modeling prediction pipeline using any `ModelWithLMHead`. See
    the [masked language modeling examples](../task_summary#masked-language-modeling)
    for more information.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`ModelWithLMHead`的掩码语言建模预测管道。有关更多信息，请参阅[掩码语言建模示例](../task_summary#masked-language-modeling)。
- en: 'Example:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE60]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This mask filling pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"fill-mask"`.'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 此掩码填充管道目前可以使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)从以下任务标识符加载："fill-mask"。
- en: The models that this pipeline can use are models that have been trained with
    a masked language modeling objective, which includes the bi-directional models
    in the library. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=fill-mask).
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已经使用掩码语言建模目标进行训练的模型，其中包括库中的双向模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=fill-mask)上可用模型的最新列表。
- en: 'This pipeline only works for inputs with exactly one token masked. Experimental:
    We added support for multiple masks. The returned values are raw model output,
    and correspond to disjoint probabilities where one might expect joint probabilities
    (See [discussion](https://github.com/huggingface/transformers/pull/10222)).'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道仅适用于具有一个掩码标记的输入。实验性：我们添加了对多个掩码的支持。返回的值是原始模型输出，并对应于不相交的概率，其中一个可能期望联合概率（请参见[讨论](https://github.com/huggingface/transformers/pull/10222)）。
- en: 'This pipeline now supports tokenizer_kwargs. For example try:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道现在支持tokenizer_kwargs。例如尝试：
- en: '[PRE61]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#### `__call__`'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L248)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L248)'
- en: '[PRE62]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`str` or `List[str]`) — One or several texts (or one list of prompts)
    with masked tokens.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args`（`str`或`List[str]`）- 一个或多个文本（或一个提示列表）带有掩码标记。'
- en: '`targets` (`str` or `List[str]`, *optional*) — When passed, the model will
    limit the scores to the passed targets instead of looking up in the whole vocab.
    If the provided targets are not in the model vocab, they will be tokenized and
    the first resulting token will be used (with a warning, and that might be slower).'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targets`（`str`或`List[str]`，*可选*）- 当传递时，模型将限制分数到传递的目标，而不是在整个词汇表中查找。如果提供的目标不在模型词汇表中，它们将被标记化，并且将使用第一个生成的标记（带有警告，并且可能会更慢）。'
- en: '`top_k` (`int`, *optional*) — When passed, overrides the number of predictions
    to return.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k`（`int`，*可选*）- 当传递时，将覆盖要返回的预测数量。'
- en: Returns
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list or a list of list of `dict`
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表或字典列表
- en: 'Each result comes as list of dictionaries with the following keys:'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都以字典列表的形式呈现，具有以下键：
- en: '`sequence` (`str`) — The corresponding input with the mask token prediction.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence`（`str`）- 具有掩码标记预测的相应输入。'
- en: '`score` (`float`) — The corresponding probability.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score`（`float`）- 相应的概率。'
- en: '`token` (`int`) — The predicted token id (to replace the masked one).'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`（`int`）- 预测的标记id（用于替换掩码标记）。'
- en: '`token_str` (`str`) — The predicted token (to replace the masked one).'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_str`（`str`）- 预测的标记（用于替换掩码标记）。'
- en: Fill the masked token in the text(s) given as inputs.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 填充作为输入的文本中的掩码标记。
- en: QuestionAnsweringPipeline
  id: totrans-644
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QuestionAnsweringPipeline
- en: '### `class transformers.QuestionAnsweringPipeline`'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.QuestionAnsweringPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L224)'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L224)'
- en: '[PRE63]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Parameters
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）-
    该模型将被管道用于进行预测。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）-
    该tokenizer将被管道用于为模型编码数据。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str`或`ModelCard`，*可选*）- 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）- 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，则将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为`""`）- 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, 默认为8) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch
    模型的 GPU 上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, 默认为1) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch
    模型的 GPU 上），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *optional*, 默认为-1) — 用于 CPU/GPU 支持的设备序数。将其设置为-1将利用 CPU，正数将在关联的
    CUDA 设备 id 上运行模型。您也可以传递原生的 `torch.device` 或一个 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *optional*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。'
- en: Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question
    answering examples](../task_summary#question-answering) for more information.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何 `ModelForQuestionAnswering` 的问答管道。有关更多信息，请参阅[问答示例](../task_summary#question-answering)。
- en: 'Example:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE64]'
  id: totrans-662
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何在[pipeline tutorial](../pipeline_tutorial)中使用管道的基础知识。
- en: 'This question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"question-answering"`.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 此问答管道目前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中加载："question-answering"。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a question answering task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=question-answering).
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在问答任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=question-answering)上可用模型的最新列表。
- en: '#### `__call__`'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L343)'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L343)'
- en: '[PRE65]'
  id: totrans-668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`SquadExample` or a list of `SquadExample`) — One or several `SquadExample`
    containing the question and context.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`SquadExample` 或 `SquadExample` 列表) — 包含问题和上下文的一个或多个 `SquadExample`。'
- en: '`X` (`SquadExample` or a list of `SquadExample`, *optional*) — One or several
    `SquadExample` containing the question and context (will be treated the same way
    as if passed as the first positional argument).'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X` (`SquadExample` 或 `SquadExample` 列表, *optional*) — 包含问题和上下文的一个或多个 `SquadExample`（将被视为第一个位置参数传递时的相同方式）。'
- en: '`data` (`SquadExample` or a list of `SquadExample`, *optional*) — One or several
    `SquadExample` containing the question and context (will be treated the same way
    as if passed as the first positional argument).'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data` (`SquadExample` 或 `SquadExample` 列表, *optional*) — 包含问题和上下文的一个或多个 `SquadExample`（将被视为第一个位置参数传递时的相同方式）。'
- en: '`question` (`str` or `List[str]`) — One or several question(s) (must be used
    in conjunction with the `context` argument).'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question` (`str` 或 `List[str]`) — 一个或多个问题（必须与 `context` 参数一起使用）。'
- en: '`context` (`str` or `List[str]`) — One or several context(s) associated with
    the question(s) (must be used in conjunction with the `question` argument).'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context` (`str` 或 `List[str]`) — 与问题相关联的一个或多个上下文（必须与 `question` 参数一起使用）。'
- en: '`topk` (`int`, *optional*, defaults to 1) — The number of answers to return
    (will be chosen by order of likelihood). Note that we return less than topk answers
    if there are not enough options available within the context.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topk` (`int`, *optional*, 默认为1) — 要返回的答案数量（将按可能性顺序选择）。请注意，如果在上下文中没有足够的选项可用，我们将返回少于
    topk 个答案。'
- en: '`doc_stride` (`int`, *optional*, defaults to 128) — If the context is too long
    to fit with the question for the model, it will be split in several chunks with
    some overlap. This argument controls the size of that overlap.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_stride` (`int`, *optional*, 默认为128) — 如果上下文对于模型来说太长而无法与问题匹配，它将被分成几个具有一些重叠的块。此参数控制该重叠的大小。'
- en: '`max_answer_len` (`int`, *optional*, defaults to 15) — The maximum length of
    predicted answers (e.g., only answers with a shorter length are considered).'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_answer_len` (`int`, *optional*, 默认为15) — 预测答案的最大长度（例如，只考虑长度较短的答案）。'
- en: '`max_seq_len` (`int`, *optional*, defaults to 384) — The maximum length of
    the total sentence (context + question) in tokens of each chunk passed to the
    model. The context will be split in several chunks (using `doc_stride` as overlap)
    if needed.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_len` (`int`, *optional*, 默认为384) — 每个传递给模型的块中的总句子长度（上下文 + 问题）的最大长度。如果需要，上下文将被分成几个块（使用
    `doc_stride` 作为重叠）。'
- en: '`max_question_len` (`int`, *optional*, defaults to 64) — The maximum length
    of the question after tokenization. It will be truncated if needed.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_question_len` (`int`, *optional*, 默认为64) — 在标记化后问题的最大长度。如果需要，它将被截断。'
- en: '`handle_impossible_answer` (`bool`, *optional*, defaults to `False`) — Whether
    or not we accept impossible as an answer.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`handle_impossible_answer` (`bool`, *optional*, 默认为 `False`) — 是否接受不可能作为答案。'
- en: '`align_to_words` (`bool`, *optional*, defaults to `True`) — Attempts to align
    the answer to real words. Improves quality on space separated langages. Might
    hurt on non-space-separated languages (like Japanese or Chinese)'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`align_to_words` (`bool`, *可选*, 默认为 `True`) — 尝试将答案与实际单词对齐。提高了空格分隔语言的质量。可能会对非空格分隔的语言（如日语或中文）造成伤害'
- en: Returns
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `dict` or a list of `dict`
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `dict` 或一个 `dict` 的列表
- en: 'Each result comes as a dictionary with the following keys:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都作为一个带有以下键的字典：
- en: '`score` (`float`) — The probability associated to the answer.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` (`float`) — 与答案相关联的概率。'
- en: '`start` (`int`) — The character start index of the answer (in the tokenized
    version of the input).'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start` (`int`) — 答案的字符起始索引（在输入的标记化版本中）。'
- en: '`end` (`int`) — The character end index of the answer (in the tokenized version
    of the input).'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end` (`int`) — 答案的字符结束索引（在输入的标记化版本中）。'
- en: '`answer` (`str`) — The answer to the question.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer` (`str`) — 问题的答案。'
- en: Answer the question(s) given as inputs by using the context(s).
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用输入的上下文回答提出的问题。
- en: '#### `create_sample`'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_sample`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L278)'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L278)'
- en: '[PRE66]'
  id: totrans-692
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Parameters
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`question` (`str` or `List[str]`) — The question(s) asked.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question` (`str` 或 `List[str]`) — 提出的问题。'
- en: '`context` (`str` or `List[str]`) — The context(s) in which we will look for
    the answer.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context` (`str` 或 `List[str]`) — 我们将在其中寻找答案的上下文。'
- en: Returns
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: One or a list of `SquadExample`
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 一个或多个 `SquadExample`
- en: The corresponding `SquadExample` grouping question and context.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的 `SquadExample` 分组问题和上下文。
- en: QuestionAnsweringPipeline leverages the `SquadExample` internally. This helper
    method encapsulate all the logic for converting question(s) and context(s) to
    `SquadExample`.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: QuestionAnsweringPipeline 在内部利用 `SquadExample`。这个辅助方法封装了将问题和上下文转换为 `SquadExample`
    的所有逻辑。
- en: We currently support extractive question answering.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前支持抽取式问答。
- en: '#### `span_to_answer`'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `span_to_answer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L630)'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L630)'
- en: '[PRE67]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`) — The actual context to extract the answer from.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`) — 从中提取答案的实际上下文。'
- en: '`start` (`int`) — The answer starting token index.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start` (`int`) — 答案起始标记索引。'
- en: '`end` (`int`) — The answer end token index.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end` (`int`) — 答案结束标记索引。'
- en: Returns
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: Dictionary like `{‘answer’
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 `{‘answer’
- en: 'str, ‘start’: int, ‘end’: int}`'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 'str, ‘start’: int, ‘end’: int}`'
- en: When decoding from token probabilities, this method maps token indexes to actual
    word in the initial context.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 从标记概率解码时，此方法将标记索引映射到初始上下文中的实际单词。
- en: SummarizationPipeline
  id: totrans-712
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SummarizationPipeline
- en: '### `class transformers.SummarizationPipeline`'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SummarizationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L216)'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L216)'
- en: '[PRE68]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Parameters
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 流水线将用于进行预测的模型。这需要是继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于 PyTorch 是 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 流水线将用于为模型编码数据的分词器。此对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为此流水线的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-721
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 用于流水线的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为 8) — 当流水线将使用 *DataLoader*（当传递数据集时，在 Pytorch
    模型的 GPU 上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为 1) — 当流水线将使用 *DataLoader*（当传递数据集时，在 Pytorch
    模型的 GPU 上），要使用的批量大小，对于推断，这并不总是有益的，请阅读 [使用流水线进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为 -1) — CPU/GPU支持的设备序号。将其设置为-1将利用CPU，设置为正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的
    `torch.device` 或 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式进行的标志。'
- en: Summarize news articles and other documents.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 总结新闻文章和其他文档。
- en: 'This summarizing pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"summarization"`.'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '当前可以使用以下任务标识符从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    加载此总结管道: `"summarization"`.'
- en: The models that this pipeline can use are models that have been fine-tuned on
    a summarization task, which is currently, ’*bart-large-cnn*’, ’*t5-small*’, ’*t5-base*’,
    ’*t5-large*’, ’*t5-3b*’, ’*t5-11b*’. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=summarization).
    For a list of available parameters, see the [following documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在摘要任务上进行了微调的模型，目前有 ’*bart-large-cnn*’, ’*t5-small*’, ’*t5-base*’, ’*t5-large*’,
    ’*t5-3b*’, ’*t5-11b*’。查看[huggingface.co/models](https://huggingface.co/models?filter=summarization)上可用模型的最新列表。有关可用参数的列表，请参阅[以下文档](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
- en: 'Usage:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '用法:'
- en: '[PRE69]'
  id: totrans-732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '#### `__call__`'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L245)'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L245)'
- en: '[PRE70]'
  id: totrans-735
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Parameters
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`documents` (*str* or `List[str]`) — One or several articles (or one list of
    articles) to summarize.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`documents` (*str* 或 `List[str]`) — 要总结的一个或多个文章（或一组文章）。'
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    include the decoded texts in the outputs'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_text` (`bool`, *可选*, 默认为 `True`) — 是否在输出中包含解码后的文本'
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the tensors of predictions (as token indices) in the outputs.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`bool`, *可选*, 默认为 `False`) — 是否在输出中包含预测的张量（作为标记索引）。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output. generate_kwargs
    — Additional keyword arguments to pass along to the generate method of the model
    (see the generate method corresponding to your framework [here](./model#generative-models)).'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`, *可选*, 默认为 `False`) — 是否清除文本输出中可能存在的额外空格。generate_kwargs
    — 传递给模型的generate方法的额外关键字参数（请参阅您框架对应的generate方法[此处](./model#generative-models)）。'
- en: Returns
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list or a list of list of `dict`
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `dict` 列表或 `dict` 列表的列表
- en: 'Each result comes as a dictionary with the following keys:'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '每个结果都以以下键的字典形式呈现:'
- en: '`summary_text` (`str`, present when `return_text=True`) — The summary of the
    corresponding input.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_text` (`str`, 当 `return_text=True` 时存在) — 相应输入的摘要。'
- en: '`summary_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the summary.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True`
    时存在) — 摘要的标记ID。'
- en: Summarize the text(s) given as inputs.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入的文本进行总结。
- en: TableQuestionAnsweringPipeline
  id: totrans-747
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TableQuestionAnsweringPipeline
- en: '### `class transformers.TableQuestionAnsweringPipeline`'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TableQuestionAnsweringPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L87)'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L87)'
- en: '[PRE71]'
  id: totrans-750
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Parameters
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该管道将用于进行预测的模型。这需要是继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于TensorFlow是继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该管道将用于为模型编码数据的分词器。此对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表PyTorch 或 `"tf"` 代表TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-756
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则将默认使用 PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, defaults to `""`) — 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, defaults to 8) — 当管道将使用 *DataLoader*（传递数据集时，在
    Pytorch 模型的 GPU 上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用 *DataLoader*（传递数据集时，在
    Pytorch 模型的 GPU 上），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*） — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *optional*, defaults to -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1
    将利用 CPU，正数将在关联的 CUDA 设备 id 上运行模型。您也可以传递原生 `torch.device` 或 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *optional*, defaults to `False`) — 标志，指示管道输出应以二进制格式（即
    pickle）或原始文本形式发生。'
- en: Table Question Answering pipeline using a `ModelForTableQuestionAnswering`.
    This pipeline is only available in PyTorch.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ModelForTableQuestionAnswering` 的表格问答管道。此管道仅在 PyTorch 中可用。
- en: 'Example:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE72]'
  id: totrans-765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在 [pipeline 教程](../pipeline_tutorial) 中使用管道的基础知识
- en: 'This tabular question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"table-question-answering"`.'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 此表格问答管道目前可以从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    中加载，使用以下任务标识符：`"table-question-answering"`。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a tabular question answering task. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=table-question-answering).
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在表格问答任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?filter=table-question-answering)
    上提供的可用模型的最新列表。
- en: '#### `__call__`'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L270)'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L270)'
- en: '[PRE73]'
  id: totrans-771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Parameters
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`table` (`pd.DataFrame` or `Dict`) — Pandas DataFrame or dictionary that will
    be converted to a DataFrame containing all the table values. See above for an
    example of dictionary.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table` (`pd.DataFrame` 或 `Dict`) — 将转换为包含所有表格值的 DataFrame 的 Pandas DataFrame
    或字典。请参阅上面的字典示例。'
- en: '`query` (`str` or `List[str]`) — Query or list of queries that will be sent
    to the model alongside the table.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query` (`str` 或 `List[str]`) — 将发送到模型的查询或查询列表，以及表格一起。'
- en: '`sequential` (`bool`, *optional*, defaults to `False`) — Whether to do inference
    sequentially or as a batch. Batching is faster, but models like SQA require the
    inference to be done sequentially to extract relations within sequences, given
    their conversational nature.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequential` (`bool`, *optional*, defaults to `False`) — 是否按顺序进行推理还是批处理。批处理更快，但像
    SQA 这样的模型需要按顺序进行推理，以提取序列中的关系，考虑到它们的对话性质。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-777
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-778
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到使用参数 `max_length` 指定的最大长度或模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-779
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）: 不填充（即可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or `TapasTruncationStrategy`, *optional*, defaults
    to `False`) — Activates and controls truncation. Accepts the following values:'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 `TapasTruncationStrategy`, *optional*, defaults
    to `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''drop_rows_to_fit''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate row by row, removing rows
    from the table.'
  id: totrans-781
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''drop_rows_to_fit''`: 截断到使用参数 `max_length` 指定的最大长度或模型的最大可接受输入长度（如果未提供该参数）。这将逐行截断，从表中删除行。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-782
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_truncate''`（默认）：不截断（即，可以输出序列长度大于模型最大可接受输入大小的批次）。'
- en: Returns
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A dictionary or a list of dictionaries containing results
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 包含结果的字典或字典列表
- en: 'Each result is a dictionary with the following keys:'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果是一个带有以下键的字典：
- en: '`answer` (`str`) — The answer of the query given the table. If there is an
    aggregator, the answer will be preceded by `AGGREGATOR >`.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer`（`str`）— 给定表格的查询答案。如果有聚合器，答案将以`AGGREGATOR >`开头。'
- en: '`coordinates` (`List[Tuple[int, int]]`) — Coordinates of the cells of the answers.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coordinates`（`List[Tuple[int, int]]`）— 答案单元格的坐标。'
- en: '`cells` (`List[str]`) — List of strings made up of the answer cell values.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cells`（`List[str]`）— 由答案单元格值组成的字符串列表。'
- en: '`aggregator` (`str`) — If the model has an aggregator, this returns the aggregator.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregator`（`str`）— 如果模型有聚合器，则返回聚合器。'
- en: 'Answers queries according to a table. The pipeline accepts several types of
    inputs which are detailed below:'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表格回答查询。管道接受以下几种类型的输入，详细信息如下：
- en: '`pipeline(table, query)`'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline(table, query)`'
- en: '`pipeline(table, [query])`'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline(table, [query])`'
- en: '`pipeline(table=table, query=query)`'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline(table=table, query=query)`'
- en: '`pipeline(table=table, query=[query])`'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline(table=table, query=[query])`'
- en: '`pipeline({"table": table, "query": query})`'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline({"table": table, "query": query})`'
- en: '`pipeline({"table": table, "query": [query]})`'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline({"table": table, "query": [query]})`'
- en: '`pipeline([{"table": table, "query": query}, {"table": table, "query": query}])`'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline([{"table": table, "query": query}, {"table": table, "query": query}])`'
- en: 'The `table` argument should be a dict or a DataFrame built from that dict,
    containing the whole table:'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '`table`参数应该是一个从该字典构建的字典或DataFrame，包含整个表格：'
- en: 'Example:'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE74]'
  id: totrans-800
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This dictionary can be passed in as such, or can be converted to a pandas DataFrame:'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将此字典作为参数传递，或者可以将其转换为pandas DataFrame：
- en: 'Example:'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE75]'
  id: totrans-803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: TextClassificationPipeline
  id: totrans-804
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TextClassificationPipeline
- en: '### `class transformers.TextClassificationPipeline`'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TextClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L34)'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L34)'
- en: '[PRE76]'
  id: totrans-807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Parameters
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）—
    该模型将被管道用于进行预测。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，对于PyTorch是[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）—
    该tokenizer将被管道用于为模型编码数据。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str`或`ModelCard`，*可选*）— 为该管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为`""`）— 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`（`int`，*可选*，默认为8）— 当管道将使用*DataLoader*（在PyTorch模型的GPU上传递数据集时），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为1）— 当管道将使用*DataLoader*（传递数据集时，在PyTorch模型的GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）—
    负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为-1）— CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为`False`）— 指示管道输出应以二进制格式（即pickle）还是原始文本格式发生的标志。'
- en: '`return_all_scores` (`bool`, *optional*, defaults to `False`) — Whether to
    return all prediction scores or just the one of the predicted class.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_all_scores`（`bool`，*可选*，默认为`False`）— 是否返回所有预测分数还是仅返回预测类的分数。'
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function_to_apply`（`str`，*可选*，默认为`"default"`）— 用于应用于模型输出以检索分数的函数。接受四个不同的值：'
- en: '`"default"`: if the model has a single label, will apply the sigmoid function
    on the output. If the model has several labels, will apply the softmax function
    on the output.'
  id: totrans-822
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"default"`：如果模型有一个标签，则将在输出上应用sigmoid函数。如果模型有多个标签，则将在输出上应用softmax函数。'
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  id: totrans-823
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"sigmoid"`：在输出上应用sigmoid函数。'
- en: '`"softmax"`: Applies the softmax function on the output.'
  id: totrans-824
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"softmax"`：在输出上应用softmax函数。'
- en: '`"none"`: Does not apply any function on the output.'
  id: totrans-825
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`：不在输出上应用任何函数。'
- en: Text classification pipeline using any `ModelForSequenceClassification`. See
    the [sequence classification examples](../task_summary#sequence-classification)
    for more information.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`ModelForSequenceClassification`的文本分类管道。有关更多信息，请参阅[序列分类示例](../task_summary#sequence-classification)。
- en: 'Example:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 例：
- en: '[PRE77]'
  id: totrans-828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This text classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"sentiment-analysis"` (for classifying sequences
    according to positive or negative sentiments).'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 此文本分类管道目前可以从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中使用以下任务标识符加载：`"sentiment-analysis"`（用于根据积极或消极情绪对序列进行分类）。
- en: If multiple classification labels are available (`model.config.num_labels >=
    2`), the pipeline will run a softmax over the results. If there is a single label,
    the pipeline will run a sigmoid over the result.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有多个分类标签可用（`model.config.num_labels >= 2`），则管道将对结果运行softmax。如果只有一个标签，则管道将对结果运行sigmoid。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a sequence classification task. See the up-to-date list of available models on
    [huggingface.co/models](https://huggingface.co/models?filter=text-classification).
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在序列分类任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=text-classification)上可用模型的最新列表。
- en: '#### `__call__`'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L122)'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L122)'
- en: '[PRE78]'
  id: totrans-835
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Parameters
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`) — One or
    several texts to classify. In order to use text pairs for your classification,
    you can send a dictionary containing `{"text", "text_pair"}` keys, or a list of
    those.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args`（`str`或`List[str]`或`Dict[str]`或`List[Dict[str]]`）— 要分类的一个或多个文本。为了在分类中使用文本对，可以发送包含`{"text"，"text_pair"}`键的字典，或者这些字典的列表。'
- en: '`top_k` (`int`, *optional*, defaults to `1`) — How many results to return.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k`（`int`，*可选*，默认为`1`）— 要返回多少结果。'
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function_to_apply`（`str`，*可选*，默认为`"default"`）— 用于应用于模型输出以检索分数的函数。接受四个不同的值：'
- en: 'If this argument is not specified, then it will apply the following functions
    according to the number of labels:'
  id: totrans-840
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定此参数，则将根据标签数应用以下函数：
- en: If the model has a single label, will apply the sigmoid function on the output.
  id: totrans-841
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型有一个标签，将在输出上应用sigmoid函数。
- en: If the model has several labels, will apply the softmax function on the output.
  id: totrans-842
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型有多个标签，将在输出上应用softmax函数。
- en: 'Possible values are:'
  id: totrans-843
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能的值为：
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  id: totrans-844
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"sigmoid"`：在输出上应用sigmoid函数。'
- en: '`"softmax"`: Applies the softmax function on the output.'
  id: totrans-845
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"softmax"`：在输出上应用softmax函数。'
- en: '`"none"`: Does not apply any function on the output.'
  id: totrans-846
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`：不在输出上应用任何函数。'
- en: Returns
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list or a list of list of `dict`
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 一个列表或`dict`的列表
- en: 'Each result comes as list of dictionaries with the following keys:'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都作为带有以下键的字典列表：
- en: '`label` (`str`) — The label predicted.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`（`str`）— 预测的标签。'
- en: '`score` (`float`) — The corresponding probability.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score`（`float`）— 相应的概率。'
- en: If `top_k` is used, one such dictionary is returned per label.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用`top_k`，则每个标签返回一个这样的字典。
- en: Classify the text(s) given as inputs.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定的文本进行分类。
- en: TextGenerationPipeline
  id: totrans-854
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TextGenerationPipeline
- en: '### `class transformers.TextGenerationPipeline`'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TextGenerationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L23)'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L23)'
- en: '[PRE79]'
  id: totrans-857
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Parameters
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该pipeline将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，对于PyTorch和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的TensorFlow。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该pipeline将用于为模型编码数据的分词器。该对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为此pipeline的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表PyTorch 或 `"tf"` 代表TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 用于该pipeline的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为 8) — 当pipeline将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为 1) — 当pipeline将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的批次大小，对于推断来说，这并不总是有益的，请阅读[使用pipeline进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的pipeline参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为 -1) — CPU/GPU支持的设备序数。将其设置为 -1 将使用 CPU，设置为正数将在关联的CUDA设备上运行模型。您也可以传递原生的
    `torch.device` 或一个 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为 `False`) — 指示pipeline输出是否应以二进制格式（即pickle）或原始文本形式发生的标志。'
- en: Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts
    the words that will follow a specified text prompt.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何 `ModelWithLMHead` 的语言生成pipeline。该pipeline预测将跟随指定文本提示的单词。
- en: 'Example:'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE80]'
  id: totrans-872
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial).
    You can pass text generation parameters to this pipeline to control stopping criteria,
    decoding strategy, and more. Learn more about text generation parameters in [Text
    generation strategies](../generation_strategies) and [Text generation](text_generation).
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用pipeline的基础知识。您可以将文本生成参数传递给此pipeline以控制停止标准、解码策略等。在[文本生成策略](../generation_strategies)和[文本生成](text_generation)中了解更多关于文本生成参数的信息。
- en: 'This language generation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"text-generation"`.'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '目前可以使用以下任务标识符从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    加载此语言生成pipeline: `"text-generation"`.'
- en: The models that this pipeline can use are models that have been trained with
    an autoregressive language modeling objective, which includes the uni-directional
    models in the library (e.g. gpt2). See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-generation).
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 该pipeline可以使用已经训练过自回归语言建模目标的模型，包括库中的单向模型（例如gpt2）。查看[huggingface.co/models](https://huggingface.co/models?filter=text-generation)上可用模型的列表。
- en: '#### `__call__`'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L178)'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L178)'
- en: '[PRE81]'
  id: totrans-878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Parameters
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`str` or `List[str]`) — One or several prompts (or one list of prompts)
    to complete.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`str` 或 `List[str]`) — 一个或多个提示（或一个提示列表）以完成。'
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the tensors of predictions (as token indices) in the outputs. If set
    to `True`, the decoded text is not returned.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`bool`, *可选*, 默认为 `False`) — 是否在输出中返回预测的张量（作为标记索引）。如果设置为
    `True`，则不返回解码后的文本。'
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return the decoded texts in the outputs.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_text` (`bool`, *可选*, 默认为 `True`) — 是否返回输出中解码后的文本。'
- en: '`return_full_text` (`bool`, *optional*, defaults to `True`) — If set to `False`
    only added text is returned, otherwise the full text is returned. Only meaningful
    if *return_text* is set to True.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_full_text` (`bool`, *可选*, 默认为 `True`) — 如果设置为 `False`，则只返回添加的文本，否则返回完整的文本。仅在
    *return_text* 设置为 True 时有意义。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`, *可选*, 默认为 `False`) — 是否清除文本输出中的潜在额外空格。'
- en: '`prefix` (`str`, *optional*) — Prefix added to prompt.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix` (`str`, *可选*) — 添加到提示的前缀。'
- en: '`handle_long_generation` (`str`, *optional*) — By default, this pipelines does
    not handle long generation (ones that exceed in one form or the other the model
    maximum length). There is no perfect way to adress this (more info :[https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227](https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227)).
    This provides common strategies to work around that problem depending on your
    use case.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`handle_long_generation` (`str`, *可选*) — 默认情况下，此管道不处理长生成（超过模型最大长度的生成）。没有完美的解决方法（更多信息：[https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227](https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227)）。这提供了根据您的用例解决该问题的常见策略。'
- en: '`None` : default strategy where nothing in particular happens'
  id: totrans-887
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`None`：默认策略，没有特别的操作'
- en: '`"hole"`: Truncates left of input, and leaves a gap wide enough to let generation
    happen (might truncate a lot of the prompt and not suitable when generation exceed
    the model capacity)'
  id: totrans-888
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hole"`：截断输入的左侧，并留下足够宽的间隙以进行生成（可能会截断大部分提示，当生成超出模型容量时不适用）'
- en: generate_kwargs — Additional keyword arguments to pass along to the generate
    method of the model (see the generate method corresponding to your framework [here](./model#generative-models)).
  id: totrans-889
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: generate_kwargs — 传递给模型的 generate 方法的额外关键字参数（查看您框架对应的 generate 方法[这里](./model#generative-models)）。
- en: Returns
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list or a list of list of `dict`
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `dict` 或 `dict` 的列表
- en: 'Returns one of the following dictionaries (cannot return a combination of both
    `generated_text` and `generated_token_ids`):'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下字典之一（不能返回 `generated_text` 和 `generated_token_ids` 的组合）：
- en: '`generated_text` (`str`, present when `return_text=True`) — The generated text.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generated_text` (`str`, 当 `return_text=True` 时存在) — 生成的文本。'
- en: '`generated_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the generated text.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generated_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True`
    时存在) — 生成文本的标记 id。'
- en: Complete the prompt(s) given as inputs.
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 完成给定的提示作为输入。
- en: Text2TextGenerationPipeline
  id: totrans-896
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本到文本生成管道
- en: '### `class transformers.Text2TextGenerationPipeline`'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Text2TextGenerationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L25)'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L25)'
- en: '[PRE82]'
  id: totrans-899
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Parameters
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该管道将用于进行预测的模型。这需要是继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于 TensorFlow 则是继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该管道将用于为模型编码数据的分词器。该对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为该管道的模型指定的模型卡片。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-905
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 用于该管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用 *DataLoader*（在传递数据集时，在 PyTorch
    模型的 GPU 上）时，要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用 *DataLoader*（传递数据集时，在 Pytorch 模型的
    GPU 上），要使用的批次大小，对于推断来说，这并不总是有益的，请阅读 [使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为 -1) — CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA
    设备 id 上运行模型。您也可以传递原生的 `torch.device` 或 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生的标志。'
- en: Pipeline for text to text generation using seq2seq models.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 seq2seq 模型进行文本到文本生成的管道。
- en: 'Example:'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE83]'
  id: totrans-914
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial).
    You can pass text generation parameters to this pipeline to control stopping criteria,
    decoding strategy, and more. Learn more about text generation parameters in [Text
    generation strategies](../generation_strategies) and [Text generation](text_generation).
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何在 [管道教程](../pipeline_tutorial) 中使用管道的基础知识。您可以将文本生成参数传递给此管道，以控制停止条件、解码策略等。在
    [文本生成策略](../generation_strategies) 和 [文本生成](text_generation) 中了解更多关于文本生成参数的信息。
- en: 'This Text2TextGenerationPipeline pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"text2text-generation"`.'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 此 Text2TextGenerationPipeline 管道目前可以通过以下任务标识符从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    加载："text2text-generation"。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a translation task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text2text-generation).
    For a list of available parameters, see the [following documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在翻译任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?filter=text2text-generation)
    上可用模型的最新列表。有关可用参数的列表，请参阅 [以下文档](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
- en: 'Usage:'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：
- en: '[PRE84]'
  id: totrans-919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '#### `__call__`'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L138)'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L138)'
- en: '[PRE85]'
  id: totrans-922
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Parameters
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`str` or `List[str]`) — Input text for the encoder.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`str` 或 `List[str]`) — 编码器的输入文本。'
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the tensors of predictions (as token indices) in the outputs.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`bool`, *可选*, 默认为 `False`) — 是否在输出中包含预测的张量（作为标记索引）。'
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    include the decoded texts in the outputs.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_text` (`bool`, *可选*, 默认为 `True`) — 是否在输出中包含解码后的文本。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`, *可选*, 默认为 `False`) — 是否清除文本输出中可能存在的额外空格。'
- en: '`truncation` (`TruncationStrategy`, *optional*, defaults to `TruncationStrategy.DO_NOT_TRUNCATE`)
    — The truncation strategy for the tokenization within the pipeline. `TruncationStrategy.DO_NOT_TRUNCATE`
    (default) will never truncate, but it is sometimes desirable to truncate the input
    to fit the model’s max_length instead of throwing an error down the line. generate_kwargs
    — Additional keyword arguments to pass along to the generate method of the model
    (see the generate method corresponding to your framework [here](./model#generative-models)).'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`TruncationStrategy`, *可选*, 默认为 `TruncationStrategy.DO_NOT_TRUNCATE`)
    — 管道内部标记化的截断策略。`TruncationStrategy.DO_NOT_TRUNCATE`（默认）永远不会截断，但有时希望将输入截断以适应模型的
    `max_length` 而不是在后续出错。generate_kwargs — 传递给模型的 generate 方法的其他关键字参数（请参阅您的框架对应的
    generate 方法 [此处](./model#generative-models)）。'
- en: Returns
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list or a list of list of `dict`
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表或字典列表
- en: 'Each result comes as a dictionary with the following keys:'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都作为一个带有以下键的字典。
- en: '`generated_text` (`str`, present when `return_text=True`) — The generated text.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generated_text` (`str`, 当 `return_text=True` 时存在) — 生成的文本。'
- en: '`generated_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the generated text.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generated_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True`
    时存在) — 生成文本的标记 id。'
- en: Generate the output text(s) using text(s) given as inputs.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定的文本作为输入生成输出文本。
- en: '#### `check_inputs`'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `check_inputs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L111)'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L111)'
- en: '[PRE86]'
  id: totrans-937
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Checks whether there might be something wrong with given input with regard to
    the model.
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 检查给定输入与模型相关是否存在问题。
- en: TokenClassificationPipeline
  id: totrans-939
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TokenClassificationPipeline
- en: '### `class transformers.TokenClassificationPipeline`'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TokenClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L61)'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L61)'
- en: '[PRE87]'
  id: totrans-942
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Parameters
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）—
    流水线将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的PyTorch模型和继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的TensorFlow模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）—
    流水线将用于对数据进行编码的分词器。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str`或`ModelCard`，*可选*）— 为此流水线的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`代表PyTorch或`"tf"`代表TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-948
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为`""`）— 用于流水线的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`（`int`，*可选*，默认为8）— 当流水线将使用*DataLoader*（传递数据集时，在PyTorch模型的GPU上）时，要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为1）— 当流水线将使用*DataLoader*（传递数据集时，在PyTorch模型的GPU上）时，要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用流水线进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）—
    负责解析提供的流水线参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为-1）— 用于CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为`False`）— 指示流水线输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: '`ignore_labels` (`List[str]`, defaults to `["O"]`) — A list of labels to ignore.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_labels`（`List[str]`，默认为`["O"]`）— 要忽略的标签列表。'
- en: '`grouped_entities` (`bool`, *optional*, defaults to `False`) — DEPRECATED,
    use `aggregation_strategy` instead. Whether or not to group the tokens corresponding
    to the same entity together in the predictions or not.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grouped_entities`（`bool`，*可选*，默认为`False`）— 已弃用，请改用`aggregation_strategy`。是否将对应于相同实体的标记分组在预测中一起还是不分组。'
- en: '`stride` (`int`, *optional*) — If stride is provided, the pipeline is applied
    on all the text. The text is split into chunks of size model_max_length. Works
    only with fast tokenizers and `aggregation_strategy` different from `NONE`. The
    value of this argument defines the number of overlapping tokens between chunks.
    In other words, the model will shift forward by `tokenizer.model_max_length -
    stride` tokens each step.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`（`int`，*可选*）— 如果提供了步幅，流水线将应用于所有文本。文本将被分割成大小为model_max_length的块。仅适用于快速分词器和`aggregation_strategy`不同于`NONE`的情况。此参数的值定义了块之间重叠标记的数量。换句话说，模型将在每一步中向前移动`tokenizer.model_max_length
    - stride`个标记。'
- en: '`aggregation_strategy` (`str`, *optional*, defaults to `"none"`) — The strategy
    to fuse (or not) tokens based on the model prediction.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregation_strategy`（`str`，*可选*，默认为`"none"`）— 基于模型预测融合（或不融合）标记的策略。'
- en: '“none” : Will simply not do any aggregation and simply return raw results from
    the model'
  id: totrans-959
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “none”：将简单地不执行任何聚合，并直接从模型返回原始结果
- en: '“simple” : Will attempt to group entities following the default schema. (A,
    B-TAG), (B, I-TAG), (C, I-TAG), (D, B-TAG2) (E, B-TAG2) will end up being [{“word”:
    ABC, “entity”: “TAG”}, {“word”: “D”, “entity”: “TAG2”}, {“word”: “E”, “entity”:
    “TAG2”}] Notice that two consecutive B tags will end up as different entities.
    On word based languages, we might end up splitting words undesirably : Imagine
    Microsoft being tagged as [{“word”: “Micro”, “entity”: “ENTERPRISE”}, {“word”:
    “soft”, “entity”: “NAME”}]. Look for FIRST, MAX, AVERAGE for ways to mitigate
    that and disambiguate words (on languages that support that meaning, which is
    basically tokens separated by a space). These mitigations will only work on real
    words, “New york” might still be tagged with two different entities.'
  id: totrans-960
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “simple”：将尝试按照默认模式对实体进行分组。（A，B-TAG），（B，I-TAG），（C，I-TAG），（D，B-TAG2）（E，B-TAG2）最终将成为[{“word”：ABC，“entity”：“TAG”}，{“word”：“D”，“entity”：“TAG2”}，{“word”：“E”，“entity”：“TAG2”}]请注意，两个连续的B标签将成为不同的实体。在基于单词的语言中，我们可能会不希望将单词拆分：想象一下将Microsoft标记为[{“word”：“Micro”，“entity”：“企业”}，{“word”：“soft”，“entity”：“名称”}]。查找FIRST、MAX、AVERAGE以了解减轻这种情况并消除单词歧义的方法（在支持该含义的语言上，基本上是由空格分隔的标记）。这些减轻措施仅适用于真实单词，“New
    york”可能仍然会被标记为两个不同的实体。
- en: '“first” : (works only on word based models) Will use the `SIMPLE` strategy
    except that words, cannot end up with different tags. Words will simply use the
    tag of the first token of the word when there is ambiguity.'
  id: totrans-961
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “first”：（仅适用于基于单词的模型）将使用`SIMPLE`策略，除非单词不能以不同的标签结束。当存在歧义时，单词将简单地使用单词的第一个标记的标签。
- en: '“average” : (works only on word based models) Will use the `SIMPLE` strategy
    except that words, cannot end up with different tags. scores will be averaged
    first across tokens, and then the maximum label is applied.'
  id: totrans-962
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “average”：（仅适用于基于单词的模型）将使用`SIMPLE`策略，除非单词不能以不同的标签结束。分数将首先在标记之间平均，然后应用最大标签。
- en: '“max” : (works only on word based models) Will use the `SIMPLE` strategy except
    that words, cannot end up with different tags. Word entity will simply be the
    token with the maximum score.'
  id: totrans-963
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “max”：（仅适用于基于单词的模型）将使用`SIMPLE`策略，除非单词不能以不同的标签结束。单词实体将简单地是具有最高分数的标记。
- en: Named Entity Recognition pipeline using any `ModelForTokenClassification`. See
    the [named entity recognition examples](../task_summary#named-entity-recognition)
    for more information.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`ModelForTokenClassification`来进行命名实体识别管道。有关更多信息，请参阅[命名实体识别示例](../task_summary#named-entity-recognition)。
- en: 'Example:'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE88]'
  id: totrans-966
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This token recognition pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"ner"` (for predicting the classes of tokens
    in a sequence: person, organisation, location or miscellaneous).'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 此标记识别管道目前可以从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中加载，使用以下任务标识符："ner"（用于预测序列中的标记类别：人物、组织、地点或其他）。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a token classification task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=token-classification).
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在标记分类任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=token-classification)上可用模型的最新列表。
- en: '#### `__call__`'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L219)'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L219)'
- en: '[PRE89]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Parameters
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`str` or `List[str]`) — One or several texts (or one list of texts)
    for token classification.'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（`str`或`List[str]`）—一个或多个文本（或一个文本列表）用于标记分类。'
- en: Returns
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list or a list of list of `dict`
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表或字典列表的列表
- en: 'Each result comes as a list of dictionaries (one for each token in the corresponding
    input, or each entity if this pipeline was instantiated with an aggregation_strategy)
    with the following keys:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都作为字典列表（对应于相应输入中的每个标记，或者如果使用聚合策略实例化此管道，则对应于每个实体）返回，具有以下键：
- en: '`word` (`str`) — The token/word classified. This is obtained by decoding the
    selected tokens. If you want to have the exact string in the original sentence,
    use `start` and `end`.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word`（`str`）—分类的标记/单词。这是通过解码所选标记获得的。如果要获得原始句子中的确切字符串，请使用`start`和`end`。'
- en: '`score` (`float`) — The corresponding probability for `entity`.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score`（`float`）—`entity`的相应概率。'
- en: '`entity` (`str`) — The entity predicted for that token/word (it is named *entity_group*
    when *aggregation_strategy* is not `"none"`.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity`（`str`）—为该标记/单词预测的实体（当*aggregation_strategy*不是`"none"`时，它被命名为*entity_group*）。'
- en: '`index` (`int`, only present when `aggregation_strategy="none"`) — The index
    of the corresponding token in the sentence.'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`（`int`，仅在`aggregation_strategy="none"`时存在）—句子中对应标记的索引。'
- en: '`start` (`int`, *optional*) — The index of the start of the corresponding entity
    in the sentence. Only exists if the offsets are available within the tokenizer'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start`（`int`，*可选*）—句子中对应实体的起始索引。仅当分词器中存在偏移量时才存在'
- en: '`end` (`int`, *optional*) — The index of the end of the corresponding entity
    in the sentence. Only exists if the offsets are available within the tokenizer'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end`（`int`，*可选*）—句子中对应实体的结束索引。仅当分词器中存在偏移量时才存在'
- en: Classify each token of the text(s) given as inputs.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定的文本的每个标记进行分类。
- en: '#### `aggregate_words`'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `aggregate_words`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L470)'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L470)'
- en: '[PRE90]'
  id: totrans-987
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Override tokens from a given word that disagree to force agreement on word boundaries.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖给定单词中不同意的标记，以强制在单词边界上达成一致。
- en: 'Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with
    first strategy as microsoft| company| B-ENT I-ENT'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT 将使用第一种策略重写为microsoft| company|
    B-ENT I-ENT
- en: '#### `gather_pre_entities`'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `gather_pre_entities`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L356)'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L356)'
- en: '[PRE91]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Fuse various numpy arrays into dicts with all the information needed for aggregation
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 将各种numpy数组融合成包含所有聚合所需信息的字典
- en: '#### `group_entities`'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `group_entities`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L533)'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L533)'
- en: '[PRE92]'
  id: totrans-996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Parameters
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`entities` (`dict`) — The entities predicted by the pipeline.'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entities`（`dict`）— 管道预测的实体。'
- en: Find and group together the adjacent tokens with the same entity predicted.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 查找并将预测相同实体的相邻标记组合在一起。
- en: '#### `group_sub_entities`'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `group_sub_entities`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L498)'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L498)'
- en: '[PRE93]'
  id: totrans-1002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Parameters
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`entities` (`dict`) — The entities predicted by the pipeline.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entities`（`dict`）— 管道预测的实体。'
- en: Group together the adjacent tokens with the same entity predicted.
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测相同实体的相邻标记组合在一起。
- en: TranslationPipeline
  id: totrans-1006
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TranslationPipeline
- en: '### `class transformers.TranslationPipeline`'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TranslationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L286)'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L286)'
- en: '[PRE94]'
  id: totrans-1009
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Parameters
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）—
    管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，用于PyTorch，以及继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)的模型，用于TensorFlow。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）—
    管道将用于为模型编码数据的分词器。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str`或`ModelCard`，*可选*）— 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）— 要使用的框架，可以是`"pt"`表示PyTorch，也可以是`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1015
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，将默认使用`model`的框架，或者如果未提供模型，则默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为`""`）— 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`（`int`，*可选*，默认为8）— 当管道将使用*DataLoader*（当传递数据集时，在Pytorch模型的GPU上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为1）— 当管道将使用*DataLoader*（当传递数据集时，在Pytorch模型的GPU上），要使用的批量大小，对于推断，这并不总是有益，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）—
    负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为-1）— CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为`False`）— 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Translates from one language to another.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 从一种语言翻译成另一种语言。
- en: 'This translation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"translation_xx_to_yy"`.'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载此翻译管道：`"translation_xx_to_yy"`。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a translation task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=translation).
    For a list of available parameters, see the [following documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 该 pipeline 可以使用的模型是在翻译任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?filter=translation)
    上的可用模型的最新列表。有关可用参数的列表，请参阅[以下文档](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
- en: 'Usage:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 用法：
- en: '[PRE95]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '#### `__call__`'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L341)'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L341)'
- en: '[PRE96]'
  id: totrans-1029
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Parameters
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`str` or `List[str]`) — Texts to be translated.'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`str` 或 `List[str]`) — 要翻译的文本。'
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the tensors of predictions (as token indices) in the outputs.'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`bool`, *optional*, 默认为 `False`) — 是否在输出中包含预测的张量（作为 token
    索引）。'
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    include the decoded texts in the outputs.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_text` (`bool`, *optional*, 默认为 `True`) — 是否在输出中包含解码后的文本。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output.'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`, *optional*, 默认为 `False`) — 是否清理文本输出中的潜在额外空格。'
- en: '`src_lang` (`str`, *optional*) — The language of the input. Might be required
    for multilingual models. Will not have any effect for single pair translation
    models'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang` (`str`, *optional*) — 输入文本的语言。对于多语言模型可能是必需的。对于单对翻译模型没有任何效果'
- en: '`tgt_lang` (`str`, *optional*) — The language of the desired output. Might
    be required for multilingual models. Will not have any effect for single pair
    translation models generate_kwargs — Additional keyword arguments to pass along
    to the generate method of the model (see the generate method corresponding to
    your framework [here](./model#generative-models)).'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *optional*) — 期望输出的语言。对于多语言模型可能是必需的。对于单对翻译模型没有任何效果 generate_kwargs
    — 传递给模型的 generate 方法的其他关键字参数（请参阅您框架对应的 generate 方法[此处](./model#generative-models)）。'
- en: Returns
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: A list or a list of list of `dict`
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `dict` 列表或 `dict` 列表的列表
- en: 'Each result comes as a dictionary with the following keys:'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都作为一个带有以下键的字典：
- en: '`translation_text` (`str`, present when `return_text=True`) — The translation.'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`translation_text` (`str`, 当 `return_text=True` 时存在) — 翻译结果。'
- en: '`translation_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the translation.'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`translation_token_ids` (`torch.Tensor` 或 `tf.Tensor`, 当 `return_tensors=True`
    时存在) — 翻译的 token ids。'
- en: Translate the text(s) given as inputs.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译给定的文本。
- en: ZeroShotClassificationPipeline
  id: totrans-1043
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeroShotClassificationPipeline
- en: '### `class transformers.ZeroShotClassificationPipeline`'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ZeroShotClassificationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L46)'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L46)'
- en: '[PRE97]'
  id: totrans-1046
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Parameters
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 该 pipeline 将用于进行预测的模型。这需要是继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于 TensorFlow 则是 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 该 pipeline 将用于为模型编码数据的分词器。该对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *optional*) — 为该 pipeline 的模型指定的模型卡片。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表
    TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1052
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用 `model` 的框架，或者如果未提供模型，则将默认使用 PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 用于标识 pipeline 的任务。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, 默认为 8) — 当 pipeline 将使用 *DataLoader*（在传递数据集时，在
    PyTorch 模型的 GPU 上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用 *DataLoader*（当传递数据集时，在 Pytorch 模型的
    GPU 上），要使用的批量大小，对于推理来说，这并不总是有益的，请阅读 [使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为 -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的
    CUDA 设备 id 上运行模型。您也可以传递原生 `torch.device` 或 `str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为 `False`) — 标志，指示管道输出是否应以二进制格式（即 pickle）或原始文本格式发生。'
- en: NLI-based zero-shot classification pipeline using a `ModelForSequenceClassification`
    trained on NLI (natural language inference) tasks. Equivalent of `text-classification`
    pipelines, but these models don’t require a hardcoded number of potential classes,
    they can be chosen at runtime. It usually means it’s slower but it is **much**
    more flexible.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 NLI 的零样本分类管道使用在 NLI（自然语言推理）任务上训练的 `ModelForSequenceClassification`。相当于 `text-classification`
    管道，但这些模型不需要预先确定的潜在类别数量，可以在运行时选择。这通常意味着速度较慢，但**更**灵活。
- en: Any combination of sequences and labels can be passed and each combination will
    be posed as a premise/hypothesis pair and passed to the pretrained model. Then,
    the logit for *entailment* is taken as the logit for the candidate label being
    valid. Any NLI model can be used, but the id of the *entailment* label must be
    included in the model config’s :attr:*~transformers.PretrainedConfig.label2id*.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 可以传递任意序列和标签的组合，每个组合将被视为前提/假设对，并传递给预训练模型。然后，*蕴涵* 的逻辑被视为候选标签有效的逻辑。可以使用任何 NLI 模型，但
    *蕴涵* 标签的 id 必须包含在模型配置的 :attr:*~transformers.PretrainedConfig.label2id* 中。
- en: 'Example:'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE98]'
  id: totrans-1062
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [pipeline 教程](../pipeline_tutorial) 中了解有关使用管道的基础知识
- en: 'This NLI pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-classification"`.'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 此 NLI 管道目前可以从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    中使用以下任务标识符加载：`"zero-shot-classification"`。
- en: The models that this pipeline can use are models that have been fine-tuned on
    an NLI task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?search=nli).
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在 NLI 任务上进行了微调的模型。请查看 [huggingface.co/models](https://huggingface.co/models?search=nli)
    上可用模型的最新列表。
- en: '#### `__call__`'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L163)'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L163)'
- en: '[PRE99]'
  id: totrans-1068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Parameters
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`str` or `List[str]`) — The sequence(s) to classify, will be truncated
    if the model input is too large.'
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`str` or `List[str]`) — 要分类的序列，如果模型输入过大，将被截断。'
- en: '`candidate_labels` (`str` or `List[str]`) — The set of possible class labels
    to classify each sequence into. Can be a single label, a string of comma-separated
    labels, or a list of labels.'
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`candidate_labels` (`str` or `List[str]`) — 用于将每个序列分类的可能类标签集。可以是单个标签、逗号分隔的标签字符串或标签列表。'
- en: '`hypothesis_template` (`str`, *optional*, defaults to `"This example is {}."`)
    — The template used to turn each label into an NLI-style hypothesis. This template
    must include a {} or similar syntax for the candidate label to be inserted into
    the template. For example, the default template is `"This example is {}."` With
    the candidate label `"sports"`, this would be fed into the model like `"<cls>
    sequence to classify <sep> This example is sports . <sep>"`. The default template
    works well in many cases, but it may be worthwhile to experiment with different
    templates depending on the task setting.'
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hypothesis_template` (`str`, *可选*, 默认为 `"This example is {}."`) — 用于将每个标签转换为
    NLI 风格假设的模板。此模板必须包含一个 {} 或类似的语法，以便将候选标签插入模板中。例如，默认模板是 `"This example is {}."`，使用候选标签
    `"sports"`，这将被馈送到模型中 `"<cls> sequence to classify <sep> This example is sports
    . <sep>"`。默认模板在许多情况下效果很好，但根据任务设置的不同，尝试不同模板可能是值得的。'
- en: '`multi_label` (`bool`, *optional*, defaults to `False`) — Whether or not multiple
    candidate labels can be true. If `False`, the scores are normalized such that
    the sum of the label likelihoods for each sequence is 1\. If `True`, the labels
    are considered independent and probabilities are normalized for each candidate
    by doing a softmax of the entailment score vs. the contradiction score.'
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multi_label` (`bool`, *可选*, 默认为 `False`) — 是否可以存在多个候选标签为真。如果为 `False`，则对每个序列的标签可能性进行归一化，使得每个序列的标签可能性之和为
    1。如果为 `True`，则将标签视为独立的，并通过对蕴涵分数与矛盾分数进行 softmax，对每个候选进行概率归一化。'
- en: Returns
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `dict` or a list of `dict`
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `dict` 或 `dict` 列表
- en: 'Each result comes as a dictionary with the following keys:'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都作为一个带有以下键的字典：
- en: '`sequence` (`str`) — The sequence for which this is the output.'
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence` (`str`) — 这是输出的序列。'
- en: '`labels` (`List[str]`) — The labels sorted by order of likelihood.'
  id: totrans-1078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`List[str]`) — 按可能性顺序排序的标签。'
- en: '`scores` (`List[float]`) — The probabilities for each of the labels.'
  id: totrans-1079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores` (`List[float]`) — 每个标签的概率。'
- en: Classify the sequence(s) given as inputs. See the [ZeroShotClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline)
    documentation for more information.
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定的序列进行分类。有关更多信息，请参阅[ZeroShotClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline)文档。
- en: Multimodal
  id: totrans-1081
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态
- en: Pipelines available for multimodal tasks include the following.
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于多模态任务的管道包括以下内容。
- en: DocumentQuestionAnsweringPipeline
  id: totrans-1083
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文档问答管道
- en: '### `class transformers.DocumentQuestionAnsweringPipeline`'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DocumentQuestionAnsweringPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L101)'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L101)'
- en: '[PRE100]'
  id: totrans-1086
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Parameters
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，对于PyTorch是[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 管道将用于为模型编码数据的分词器。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *optional*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表
    TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1092
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`，默认为 `""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用*DataLoader*（在传递数据集时，对于Pytorch模型在GPU上），要使用的工作人员数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, 默认为 1) — 当管道将使用*DataLoader*（在传递数据集时，对于Pytorch模型在GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *optional*, 默认为 -1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *optional*, 默认为 `False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Document Question Answering pipeline using any `AutoModelForDocumentQuestionAnswering`.
    The inputs/outputs are similar to the (extractive) question answering pipeline;
    however, the pipeline takes an image (and optional OCR’d words/boxes) as input
    instead of text context.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何`AutoModelForDocumentQuestionAnswering`的文档问答管道。输入/输出与(抽取式)问答管道类似；但是，该管道将图像（和可选的OCR单词/框）作为输入，而不是文本上下文。
- en: 'Example:'
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE101]'
  id: totrans-1101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This document question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"document-question-answering"`.'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 此文档问答管道目前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载：`"document-question-answering"`。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a document question answering task. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=document-question-answering).
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在文档问答任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=document-question-answering)上可用模型的最新列表。
- en: '#### `__call__`'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L194)'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L194)'
- en: '[PRE102]'
  id: totrans-1107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Parameters
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image` (`str` or `PIL.Image`) — The pipeline handles three types of images:'
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`str`或`PIL.Image`) — 管道处理三种类型的图像：'
- en: A string containing a http link pointing to an image
  id: totrans-1110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的http链接的字符串
- en: A string containing a local path to an image
  id: totrans-1111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含图像本地路径的字符串
- en: An image loaded in PIL directly
  id: totrans-1112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接在PIL中加载的图像
- en: The pipeline accepts either a single image or a batch of images. If given a
    single image, it can be broadcasted to multiple questions.
  id: totrans-1113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管道接受单个图像或一批图像。如果给定单个图像，则可以广播到多个问题。
- en: '`question` (`str`) — A question to ask of the document.'
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question` (`str`) — 要问的问题。'
- en: '`word_boxes` (`List[str, Tuple[float, float, float, float]]`, *optional*) —
    A list of words and bounding boxes (normalized 0->1000). If you provide this optional
    input, then the pipeline will use these words and boxes instead of running OCR
    on the image to derive them for models that need them (e.g. LayoutLM). This allows
    you to reuse OCR’d results across many invocations of the pipeline without having
    to re-run it each time.'
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_boxes` (`List[str, Tuple[float, float, float, float]]`, *可选*) — 一组单词和边界框（标准化为0->1000）。如果提供此可选输入，则管道将使用这些单词和框，而不是在图像上运行OCR来为需要它们的模型（例如LayoutLM）派生它们。这允许您在管道的许多调用之间重用OCR的结果，而无需每次重新运行它。'
- en: '`top_k` (`int`, *optional*, defaults to 1) — The number of answers to return
    (will be chosen by order of likelihood). Note that we return less than top_k answers
    if there are not enough options available within the context.'
  id: totrans-1116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`, *可选*, 默认为1) — 要返回的答案数量（将按可能性顺序选择）。请注意，如果在上下文中没有足够的选项可用，我们将返回少于top_k个答案。'
- en: '`doc_stride` (`int`, *optional*, defaults to 128) — If the words in the document
    are too long to fit with the question for the model, it will be split in several
    chunks with some overlap. This argument controls the size of that overlap.'
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_stride` (`int`, *可选*, 默认为128) — 如果文档中的单词太长，无法与模型的问题匹配，它将被分成几个具有一些重叠的块。此参数控制该重叠的大小。'
- en: '`max_answer_len` (`int`, *optional*, defaults to 15) — The maximum length of
    predicted answers (e.g., only answers with a shorter length are considered).'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_answer_len` (`int`, *可选*, 默认为15) — 预测答案的最大长度（例如，只考虑长度较短的答案）。'
- en: '`max_seq_len` (`int`, *optional*, defaults to 384) — The maximum length of
    the total sentence (context + question) in tokens of each chunk passed to the
    model. The context will be split in several chunks (using `doc_stride` as overlap)
    if needed.'
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_len` (`int`, *可选*, 默认为384) — 每个传递给模型的块中的总句子长度（上下文+问题）的最大长度（以标记为单位）。如果需要，上下文将被分成几个块（使用`doc_stride`作为重叠）。'
- en: '`max_question_len` (`int`, *optional*, defaults to 64) — The maximum length
    of the question after tokenization. It will be truncated if needed.'
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_question_len` (`int`, *可选*, 默认为64) — 问题在标记化后的最大长度。如果需要，将被截断。'
- en: '`handle_impossible_answer` (`bool`, *optional*, defaults to `False`) — Whether
    or not we accept impossible as an answer.'
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`handle_impossible_answer` (`bool`, *可选*, 默认为`False`) — 是否接受不可能作为答案。'
- en: '`lang` (`str`, *optional*) — Language to use while running OCR. Defaults to
    english.'
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lang` (`str`, *可选*) — 运行OCR时要使用的语言。默认为英语。'
- en: '`tesseract_config` (`str`, *optional*) — Additional flags to pass to tesseract
    while running OCR.'
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tesseract_config` (`str`, *可选*) — 在运行OCR时传递给tesseract的附加标志。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *可选*, 默认为None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为None，则不设置超时，调用可能会永远阻塞。'
- en: Returns
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `dict` or a list of `dict`
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`dict`或一个`dict`列表
- en: 'Each result comes as a dictionary with the following keys:'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都作为一个带有以下键的字典：
- en: '`score` (`float`) — The probability associated to the answer.'
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` (`float`) — 与答案相关联的概率。'
- en: '`start` (`int`) — The start word index of the answer (in the OCR’d version
    of the input or provided `word_boxes`).'
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start` (`int`) — 答案的开始单词索引（在输入的OCR版本或提供的`word_boxes`中）。'
- en: '`end` (`int`) — The end word index of the answer (in the OCR’d version of the
    input or provided `word_boxes`).'
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end` (`int`) — 答案的结束单词索引（在输入的OCR版本或提供的`word_boxes`中）。'
- en: '`answer` (`str`) — The answer to the question.'
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer` (`str`) — 问题的答案。'
- en: '`words` (`list[int]`) — The index of each word/box pair that is in the answer'
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`words` (`list[int]`) — 答案中每个单词/框对的索引'
- en: Answer the question(s) given as inputs by using the document(s). A document
    is defined as an image and an optional list of (word, box) tuples which represent
    the text in the document. If the `word_boxes` are not provided, it will use the
    Tesseract OCR engine (if available) to extract the words and boxes automatically
    for LayoutLM-like models which require them as input. For Donut, no OCR is run.
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用文档回答输入的问题。文档被定义为一幅图像和一个可选的（单词，框）元组列表，表示文档中的文本。如果未提供`word_boxes`，它将使用Tesseract
    OCR引擎（如果可用）自动提取单词和框，以供需要它们作为输入的LayoutLM类似模型使用。对于Donut，不运行OCR。
- en: 'You can invoke the pipeline several ways:'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以多种方式调用管道：
- en: '`pipeline(image=image, question=question)`'
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline(image=image, question=question)`'
- en: '`pipeline(image=image, question=question, word_boxes=word_boxes)`'
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline(image=image, question=question, word_boxes=word_boxes)`'
- en: '`pipeline([{"image": image, "question": question}])`'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline([{"image": image, "question": question}])`'
- en: '`pipeline([{"image": image, "question": question, "word_boxes": word_boxes}])`'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline([{"image": image, "question": question, "word_boxes": word_boxes}])`'
- en: FeatureExtractionPipeline
  id: totrans-1139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FeatureExtractionPipeline
- en: '### `class transformers.FeatureExtractionPipeline`'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FeatureExtractionPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L7)'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L7)'
- en: '[PRE103]'
  id: totrans-1142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Parameters
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`模型` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 流水线将使用的模型来进行预测。这需要是一个继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于 PyTorch 是 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`分词器` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 流水线将用于为模型编码数据的分词器。此对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`模型卡` (`str` 或 `ModelCard`，*可选*) — 为此流水线的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`框架` (`str`，*可选*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表 TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用模型的框架，或者如果未提供模型，则默认使用 PyTorch。
- en: '`return_tensors` (`bool`, *optional*) — If `True`, returns a tensor according
    to the specified framework, otherwise returns a list.'
  id: totrans-1149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`bool`，*可选*) — 如果为 `True`，则根据指定的框架返回一个张量，否则返回一个列表。'
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`任务` (`str`，默认为 `""`) — 用于流水线的任务标识符。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*)
    — 负责解析提供的流水线参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id.'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`设备` (`int`，*可选*，默认为 -1) — 用于 CPU/GPU 支持的设备序数。将其设置为 -1 将利用 CPU，正数将在关联的 CUDA
    设备 id 上运行模型。'
- en: '`tokenize_kwargs` (`dict`, *optional*) — Additional dictionary of keyword arguments
    passed along to the tokenizer.'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize_kwargs` (`dict`，*可选*) — 传递给分词器的额外关键字参数的字典。'
- en: Feature extraction pipeline using no model head. This pipeline extracts the
    hidden states from the base transformer, which can be used as features in downstream
    tasks.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用没有模型头的特征提取流水线。此流水线从基础变换器中提取隐藏状态，可以用作下游任务中的特征。
- en: 'Example:'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE104]'
  id: totrans-1156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在 [流水线教程](../pipeline_tutorial) 中使用流水线的基础知识
- en: 'This feature extraction pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the task identifier: `"feature-extraction"`.'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 当前可以使用任务标识符 `"feature-extraction"` 从 [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    加载此特征提取流水线。
- en: All models may be used for this pipeline. See a list of all models, including
    community-contributed models on [huggingface.co/models](https://huggingface.co/models).
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都可以用于此流水线。查看包括社区贡献模型在内的所有模型列表，请访问 [huggingface.co/models](https://huggingface.co/models)。
- en: '#### `__call__`'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L96)'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L96)'
- en: '[PRE105]'
  id: totrans-1162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Parameters
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` (`str` or `List[str]`) — One or several texts (or one list of texts)
    to get the features of.'
  id: totrans-1164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`str` 或 `List[str]`) — 一个或多个文本（或一个文本列表）以获取特征。'
- en: Returns
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A nested list of `float`
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 一个嵌套的 `float` 列表
- en: The features computed by the model.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 模型计算的特征。
- en: Extract the features of the input(s).
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 提取输入的特征。
- en: ImageToTextPipeline
  id: totrans-1169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageToTextPipeline
- en: '### `class transformers.ImageToTextPipeline`'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageToTextPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L30)'
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L30)'
- en: '[PRE106]'
  id: totrans-1172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Parameters
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`模型` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 流水线将使用的模型来进行预测。这需要是一个继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于 PyTorch 是 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）
    — 管道将用于为模型编码数据的分词器。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str`或`ModelCard`，*optional*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`，*optional*) — 要使用的框架，可以是“pt”表示PyTorch或“tf”表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`，默认为`""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, 默认为8) — 当管道将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-1181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, 默认为1) — 当管道将使用*DataLoader*（在传递数据集时，在Pytorch模型的GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*optional*）
    — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *optional*, 默认为-1) — 用于CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-1184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *optional*, 默认为`False`) — 指示管道输出是否应以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Image To Text pipeline using a `AutoModelForVision2Seq`. This pipeline predicts
    a caption for a given image.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`AutoModelForVision2Seq`的图像到文本管道。此管道为给定图像预测标题。
- en: 'Example:'
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE107]'
  id: totrans-1187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用管道的基础知识
- en: 'This image to text pipeline can currently be loaded from pipeline() using the
    following task identifier: “image-to-text”.'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可以使用以下任务标识符从pipeline()加载此图像到文本管道：“image-to-text”。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?pipeline_tag=image-to-text).
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?pipeline_tag=image-to-text)上查看可用模型列表。
- en: '#### `__call__`'
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L83)'
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L83)'
- en: '[PRE108]'
  id: totrans-1193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Parameters
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-1195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`str`，`List[str]`，`PIL.Image`或`List[PIL.Image]`) — 管道处理三种类型的图像：'
- en: A string containing a HTTP(s) link pointing to an image
  id: totrans-1196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的HTTP(s)链接的字符串
- en: A string containing a local path to an image
  id: totrans-1197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的本地路径的字符串
- en: An image loaded in PIL directly
  id: totrans-1198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接加载的PIL图像
- en: The pipeline accepts either a single image or a batch of images.
  id: totrans-1199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该管道接受单个图像或一批图像。
- en: '`max_new_tokens` (`int`, *optional*) — The amount of maximum tokens to generate.
    By default it will use `generate` default.'
  id: totrans-1200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens` (`int`, *optional*) — 要生成的最大标记数量。默认情况下，它将使用`generate`的默认值。'
- en: '`generate_kwargs` (`Dict`, *optional*) — Pass it to send all of these arguments
    directly to `generate` allowing full control of this function.'
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_kwargs` (`Dict`, *optional*) — 将其传递给`generate`，以便直接将所有这些参数发送到`generate`，从而完全控制此函数。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-1202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *optional*, 默认为None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为None，则不设置超时，并且调用可能会永远阻塞。'
- en: Returns
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A list or a list of list of `dict`
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表或字典列表
- en: 'Each result comes as a dictionary with the following key:'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: 每个结果都以包含以下键的字典形式呈现：
- en: '`generated_text` (`str`) — The generated text.'
  id: totrans-1206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generated_text` (`str`) — 生成的文本。'
- en: Assign labels to the image(s) passed as inputs.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: 为传入的图像分配标签。
- en: MaskGenerationPipeline
  id: totrans-1208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MaskGenerationPipeline
- en: '### `class transformers.MaskGenerationPipeline`'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MaskGenerationPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L22)'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L22)'
- en: '[PRE109]'
  id: totrans-1211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Parameters
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）-
    该模型将被管道用于进行预测。这需要是一个继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）-
    该分词器将被管道用于为模型编码数据。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`feature_extractor` ([SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor))
    — The feature extractor that will be used by the pipeline to encode the input.'
  id: totrans-1215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`（[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)）-
    该特征提取器将被管道用于对输入进行编码。'
- en: '`points_per_batch` (*optional*, int, default to 64) — Sets the number of points
    run simultaneously by the model. Higher numbers may be faster but use more GPU
    memory.'
  id: totrans-1216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`points_per_batch`（*可选*，int，默认为64）- 设置模型同时运行的点数。较高的数字可能更快，但会使用更多的GPU内存。'
- en: '`output_bboxes_mask` (`bool`, *optional*, default to `False`) — Whether or
    not to output the bounding box predictions.'
  id: totrans-1217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_bboxes_mask`（`bool`，*可选*，默认为`False`）- 是否输出边界框预测。'
- en: '`output_rle_masks` (`bool`, *optional*, default to `False`) — Whether or not
    to output the masks in `RLE` format'
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_rle_masks`（`bool`，*可选*，默认为`False`）- 是否输出`RLE`格式的掩码'
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)）-
    该模型将被管道用于进行预测。这需要是一个继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）-
    该分词器将被管道用于为模型编码数据。该对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard`（`str`或`ModelCard`，*可选*）- 为该管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`（`str`，*可选*）- 要使用的框架，可以是 `"pt"` 代表PyTorch 或 `"tf"` 代表TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`（`str`，默认为`""`）- 用于该管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`（`int`，*可选*，默认为8）- 当管道将使用*DataLoader*（在传递数据集时，对于PyTorch模型在GPU上），要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-1226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`，*可选*，默认为1）- 当管道将使用*DataLoader*（在传递数据集时，对于PyTorch模型在GPU上），要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）-
    负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为-1）- CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或一个`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为`False`）—指示流水线输出是否以二进制格式（即pickle）或原始文本格式发生的标志。'
- en: Automatic mask generation for images using `SamForMaskGeneration`. This pipeline
    predicts binary masks for an image, given an image. It is a `ChunkPipeline` because
    you can seperate the points in a mini-batch in order to avoid OOM issues. Use
    the `points_per_batch` argument to control the number of points that will be processed
    at the same time. Default is `64`.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`SamForMaskGeneration`为图像生成自动蒙版。该流水线预测图像的二进制蒙版，给定一个图像。这是一个`ChunkPipeline`，因为您可以将小批量中的点分开，以避免OOM问题。使用`points_per_batch`参数来控制同时处理的点数。默认值为`64`。
- en: 'The pipeline works in 3 steps:'
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: 该流水线分为3个步骤：
- en: '`preprocess`: A grid of 1024 points evenly separated is generated along with
    bounding boxes and point labels. For more details on how the points and bounding
    boxes are created, check the `_generate_crop_boxes` function. The image is also
    preprocessed using the `image_processor`. This function `yields` a minibatch of
    `points_per_batch`.'
  id: totrans-1232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`preprocess`：生成1024个均匀分隔的点网格，以及边界框和点标签。有关如何创建点和边界框的详细信息，请检查`_generate_crop_boxes`函数。还使用`image_processor`对图像进行预处理。此函数`yield`一个`points_per_batch`的小批量。'
- en: '`forward`: feeds the outputs of `preprocess` to the model. The image embedding
    is computed only once. Calls both `self.model.get_image_embeddings` and makes
    sure that the gradients are not computed, and the tensors and models are on the
    same device.'
  id: totrans-1233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`forward`：将`preprocess`的输出馈送到模型。图像嵌入仅计算一次。调用`self.model.get_image_embeddings`并确保不计算梯度，张量和模型在同一设备上。'
- en: '`postprocess`: The most important part of the automatic mask generation happens
    here. Three steps are induced:'
  id: totrans-1234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`postprocess`：自动蒙版生成的最重要部分发生在这里。引入了三个步骤：'
- en: 'image_processor.postprocess_masks (run on each minibatch loop): takes in the
    raw output masks, resizes them according to the image size, and transforms there
    to binary masks.'
  id: totrans-1235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: image_processor.postprocess_masks（在每个小批量循环中运行）：接受原始输出蒙版，根据图像大小调整其大小，并将其转换为二进制蒙版。
- en: 'image_processor.filter_masks (on each minibatch loop): uses both `pred_iou_thresh`
    and `stability_scores`. Also applies a variety of filters based on non maximum
    suppression to remove bad masks.'
  id: totrans-1236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: image_processor.filter_masks（在每个小批量循环中）：同时使用`pred_iou_thresh`和`stability_scores`。还应用基于非最大抑制的各种过滤器，以消除不良蒙版。
- en: image_processor.postprocess_masks_for_amg applies the NSM on the mask to only
    keep relevant ones.
  id: totrans-1237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: image_processor.postprocess_masks_for_amg将NSM应用于蒙版，仅保留相关的蒙版。
- en: 'Example:'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE110]'
  id: totrans-1239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关在[pipeline教程](../pipeline_tutorial)中使用流水线的基础知识
- en: 'This segmentation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"mask-generation"`.'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 当前可以使用以下任务标识符从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)加载此分割流水线：`"mask-generation"`。
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=mask-generation).
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: 在[huggingface.co/models](https://huggingface.co/models?filter=mask-generation)上查看可用模型的列表。
- en: '#### `__call__`'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L135)'
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L135)'
- en: '[PRE111]'
  id: totrans-1245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Parameters
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (`np.ndarray` or `bytes` or `str` or `dict`) — Image or list of images.'
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（`np.ndarray`或`bytes`或`str`或`dict`）—图像或图像列表。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.0) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-1248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold`（`float`，*可选*，默认为0.0）—将预测的蒙版转换为二进制值时使用的阈值。'
- en: '`pred_iou_thresh` (`float`, *optional*, defaults to 0.88) — A filtering threshold
    in `[0,1]` applied on the model’s predicted mask quality.'
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_iou_thresh`（`float`，*可选*，默认为0.88）—在`[0,1]`上应用于模型预测的蒙版质量的过滤阈值。'
- en: '`stability_score_thresh` (`float`, *optional*, defaults to 0.95) — A filtering
    threshold in `[0,1]`, using the stability of the mask under changes to the cutoff
    used to binarize the model’s mask predictions.'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stability_score_thresh`（`float`，*可选*，默认为0.95）—在`[0,1]`中的过滤阈值，使用蒙版在截止值变化下的稳定性来对模型的蒙版预测进行二值化。'
- en: '`stability_score_offset` (`int`, *optional*, defaults to 1) — The amount to
    shift the cutoff when calculated the stability score.'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stability_score_offset`（`int`，*可选*，默认为1）—在计算稳定性分数时，偏移截止值的量。'
- en: '`crops_nms_thresh` (`float`, *optional*, defaults to 0.7) — The box IoU cutoff
    used by non-maximal suppression to filter duplicate masks.'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_nms_thresh`（`float`，*可选*，默认为0.7）—非极大值抑制使用的框IoU截止值，用于过滤重复蒙版。'
- en: '`crops_n_layers` (`int`, *optional*, defaults to 0) — If `crops_n_layers>0`,
    mask prediction will be run again on crops of the image. Sets the number of layers
    to run, where each layer has 2**i_layer number of image crops.'
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_n_layers`（`int`，*可选*，默认为0）—如果`crops_n_layers>0`，将再次在图像的裁剪上运行蒙版预测。设置要运行的层数，其中每一层具有2**i_layer数量的图像裁剪。'
- en: '`crop_overlap_ratio` (`float`, *optional*, defaults to `512 / 1500`) — Sets
    the degree to which crops overlap. In the first crop layer, crops will overlap
    by this fraction of the image length. Later layers with more crops scale down
    this overlap.'
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_overlap_ratio`（`float`，*可选*，默认为`512 / 1500`）—设置裁剪重叠的程度。在第一层裁剪中，裁剪将以图像长度的这一部分重叠。具有更多裁剪的后续层会缩小此重叠。'
- en: '`crop_n_points_downscale_factor` (`int`, *optional*, defaults to `1`) — The
    number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.'
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_n_points_downscale_factor`（`int`，*可选*，默认为`1`）—在第n层中采样的每边点数按`crop_n_points_downscale_factor**n`缩小。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-1256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout`（`float`，*可选*，默认为None）—从网络获取图像的最长时间（以秒为单位）。如果为None，则不设置超时，调用可能永远阻塞。'
- en: Returns
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict`'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict`'
- en: 'A dictionary with the following keys:'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有以下键的字典：
- en: '`mask` (`PIL.Image`) — A binary mask of the detected object as a PIL Image
    of shape `(width, height)` of the original image. Returns a mask filled with zeros
    if no object is found.'
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask` (`PIL.Image`) — 检测到的对象的二进制掩模，作为原始图像的形状为`(width, height)`的PIL图像。如果未找到对象，则返回填充有零的掩模。'
- en: '`score` (*optional* `float`) — Optionally, when the model is capable of estimating
    a confidence of the “object” described by the label and the mask.'
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` (*可选* `float`) — 可选地，当模型能够估计由标签和掩模描述的“对象”的置信度时。'
- en: Generates binary segmentation masks
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: 生成二进制分割掩模
- en: VisualQuestionAnsweringPipeline
  id: totrans-1263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VisualQuestionAnsweringPipeline
- en: '### `class transformers.VisualQuestionAnsweringPipeline`'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VisualQuestionAnsweringPipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L18)'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: '[`源代码`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L18)'
- en: '[PRE112]'
  id: totrans-1266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Parameters
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)（对于PyTorch）和[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)（对于TensorFlow）的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 管道将用于为模型编码数据的分词器。此对象继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *可选*) — 为此管道的模型指定的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *可选*) — 要使用的框架，可以是`"pt"`表示PyTorch或`"tf"`表示TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，则将默认使用`model`的框架，或者如果未提供模型，则将默认使用PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-1274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *可选*, 默认为 8) — 当管道将使用*DataLoader*（在GPU上为Pytorch模型传递数据集时）时，要使用的工作程序数量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-1275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为 1) — 当管道将使用*DataLoader*（在GPU上为Pytorch模型传递数据集时）时，要使用的批次大小，对于推断，这并不总是有益的，请阅读[使用管道进行批处理](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *可选*) — 负责解析提供的管道参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-1277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, *可选*, 默认为 -1) — CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递原生的`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output` (`bool`, *可选*, 默认为 `False`) — 标志指示管道输出是否以二进制格式（即pickle）或原始文本形式发生。'
- en: Visual Question Answering pipeline using a `AutoModelForVisualQuestionAnswering`.
    This pipeline is currently only available in PyTorch.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`AutoModelForVisualQuestionAnswering`的视觉问答管道。此管道目前仅在PyTorch中可用。
- en: 'Example:'
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE113]'
  id: totrans-1281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 在[pipeline教程](../pipeline_tutorial)中了解如何使用管道的基础知识
- en: 'This visual question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifiers: `"visual-question-answering", "vqa"`.'
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 此视觉问答管道目前可以从[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中加载，使用以下任务标识符："visual-question-answering",
    "vqa"。
- en: The models that this pipeline can use are models that have been fine-tuned on
    a visual question answering task. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=visual-question-answering).
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道可以使用已在视觉问答任务上进行了微调的模型。请查看[huggingface.co/models](https://huggingface.co/models?filter=visual-question-answering)上可用模型的最新列表。
- en: '#### `__call__`'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L70)'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L70)'
- en: '[PRE114]'
  id: totrans-1287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Parameters
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  id: totrans-1289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`str`, `List[str]`, `PIL.Image` 或 `List[PIL.Image]`) — 管道处理三种类型的图像：'
- en: A string containing a http link pointing to an image
  id: totrans-1290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含指向图像的 http 链接的字符串
- en: A string containing a local path to an image
  id: totrans-1291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含本地图像路径的字符串
- en: An image loaded in PIL directly
  id: totrans-1292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接加载的 PIL 图像
- en: The pipeline accepts either a single image or a batch of images. If given a
    single image, it can be broadcasted to multiple questions.
  id: totrans-1293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该管道接受单个图像或一批图像。如果给定单个图像，则可以广播到多个问题。
- en: '`question` (`str`, `List[str]`) — The question(s) asked. If given a single
    question, it can be broadcasted to multiple images.'
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question` (`str`, `List[str]`) — 提出的问题。如果给定单个问题，则可以广播到多个图像。'
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  id: totrans-1295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` (`int`, *optional*, 默认为 5) — 管道将返回的前 k 个标签的数量。如果提供的数字高于模型配置中可用的标签数量，则默认为标签数量。'
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  id: totrans-1296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`float`, *optional*, 默认为 None) — 从网络获取图像的最长等待时间（以秒为单位）。如果为 None，则不设置超时，调用可能会永远阻塞。'
- en: Returns
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A dictionary or a list of dictionaries containing the result. The dictionaries
    contain the following keys
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 包含结果的字典或字典列表。字典包含以下键
- en: '`label` (`str`) — The label identified by the model.'
  id: totrans-1299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label` (`str`) — 模型识别的标签。'
- en: '`score` (`int`) — The score attributed by the model for that label.'
  id: totrans-1300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` (`int`) — 模型为该标签分配的分数。'
- en: 'Answers open-ended questions about images. The pipeline accepts several types
    of inputs which are detailed below:'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 回答关于图像的开放性问题。该管道接受下面详细说明的几种类型的输入：
- en: '`pipeline(image=image, question=question)`'
  id: totrans-1302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline(image=image, question=question)`'
- en: '`pipeline({"image": image, "question": question})`'
  id: totrans-1303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline({"image": image, "question": question})`'
- en: '`pipeline([{"image": image, "question": question}])`'
  id: totrans-1304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline([{"image": image, "question": question}])`'
- en: '`pipeline([{"image": image, "question": question}, {"image": image, "question":
    question}])`'
  id: totrans-1305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline([{"image": image, "question": question}, {"image": image, "question":
    question}])`'
- en: 'Parent class: Pipeline'
  id: totrans-1306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 父类：Pipeline
- en: '### `class transformers.Pipeline`'
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pipeline`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L749)'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L749)'
- en: '[PRE115]'
  id: totrans-1309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Parameters
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    或 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 管道将用于进行预测的模型。这需要是一个继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    的模型，对于 PyTorch 是 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 管道将用于为模型编码数据的分词器。该对象继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)。'
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  id: totrans-1313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modelcard` (`str` 或 `ModelCard`, *optional*) — 为该管道的模型分配的模型卡。'
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  id: totrans-1314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *optional*) — 要使用的框架，可以是 `"pt"` 代表 PyTorch 或 `"tf"` 代表
    TensorFlow。指定的框架必须已安装。'
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  id: totrans-1315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未指定框架，将默认使用当前安装的框架。如果未指定框架并且两个框架都已安装，将默认使用 `model` 的框架，或者如果未提供模型，则默认使用 PyTorch。
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  id: totrans-1316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, 默认为 `""`) — 用于管道的任务标识符。'
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  id: totrans-1317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers` (`int`, *optional*, 默认为 8) — 当管道将使用 *DataLoader*（在传递数据集时，在 Pytorch
    模型上使用 GPU），要使用的工作进程数。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  id: totrans-1318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, defaults to 1) — 当管道将使用 *DataLoader*（在传递数据集时，在
    Pytorch 模型上使用 GPU），要使用的批次大小，对于推理来说，这并不总是有益的，请阅读 [Batching with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    。'
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args_parser`（[ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler)，*可选*）-负责解析提供的流水线参数的对象的引用。'
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  id: totrans-1320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`int`，*可选*，默认为-1）-CPU/GPU支持的设备序数。将其设置为-1将利用CPU，正数将在关联的CUDA设备ID上运行模型。您也可以传递本机`torch.device`或`str`。'
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  id: totrans-1321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_output`（`bool`，*可选*，默认为`False`）-指示流水线输出应以二进制格式（即pickle）或原始文本发生的标志。'
- en: The Pipeline class is the class from which all pipelines inherit. Refer to this
    class for methods shared across different pipelines.
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline类是所有流水线继承的类。请参考此类以获取不同流水线共享的方法。
- en: 'Base class implementing pipelined operations. Pipeline workflow is defined
    as a sequence of the following operations:'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 实现流水线操作的基类。流水线工作流定义为以下操作序列：
- en: Input -> Tokenization -> Model Inference -> Post-Processing (task dependent)
    -> Output
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: 输入->标记化->模型推断->后处理（任务相关）->输出
- en: Pipeline supports running on CPU or GPU through the device argument (see below).
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline支持通过设备参数在CPU或GPU上运行（见下文）。
- en: Some pipeline, like for instance [FeatureExtractionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FeatureExtractionPipeline)
    (`'feature-extraction'`) output large tensor object as nested-lists. In order
    to avoid dumping such large structure as textual data we provide the `binary_output`
    constructor argument. If set to `True`, the output will be stored in the pickle
    format.
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: 某些流水线，例如[FeatureExtractionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FeatureExtractionPipeline)（`'feature-extraction'`）将大张量对象输出为嵌套列表。为了避免将这样大的结构转储为文本数据，我们提供了`binary_output`构造参数。如果设置为`True`，输出将以pickle格式存储。
- en: '#### `check_model_type`'
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `check_model_type`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L984)'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L984)'
- en: '[PRE116]'
  id: totrans-1329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Parameters
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`supported_models` (`List[str]` or `dict`) — The list of models supported by
    the pipeline, or a dictionary with model class values.'
  id: totrans-1331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`supported_models`（`List[str]`或`dict`）-流水线支持的模型列表，或具有模型类值的字典。'
- en: Check if the model class is in supported by the pipeline.
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型类是否受流水线支持。
- en: '#### `device_placement`'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `device_placement`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L923)'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L923)'
- en: '[PRE117]'
  id: totrans-1335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Context Manager allowing tensor allocation on the user-specified device in framework
    agnostic way.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文管理器，以框架不可知的方式在用户指定的设备上分配张量。
- en: 'Examples:'
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE118]'
  id: totrans-1338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '#### `ensure_tensor_on_device`'
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `ensure_tensor_on_device`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L950)'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L950)'
- en: '[PRE119]'
  id: totrans-1341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Parameters
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`inputs` (keyword arguments that should be `torch.Tensor`, the rest is ignored)
    — The tensors to place on `self.device`.'
  id: totrans-1343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`（应为`torch.Tensor`的关键字参数，其余部分将被忽略）-要放置在`self.device`上的张量。'
- en: '`Recursive` on lists **only**. —'
  id: totrans-1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅对列表进行**递归**。
- en: Returns
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict[str, torch.Tensor]`'
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, torch.Tensor]`'
- en: The same as `inputs` but on the proper device.
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 与`inputs`相同，但在适当的设备上。
- en: Ensure PyTorch tensors are on the specified device.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 确保PyTorch张量位于指定设备上。
- en: '#### `postprocess`'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `postprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1047)'
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1047)'
- en: '[PRE120]'
  id: totrans-1351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Postprocess will receive the raw outputs of the `_forward` method, generally
    tensors, and reformat them into something more friendly. Generally it will output
    a list or a dict or results (containing just strings and numbers).
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理将接收`_forward`方法的原始输出，通常是张量，并将其重新格式化为更友好的形式。通常它将输出一个包含字符串和数字的列表或结果字典。
- en: '#### `predict`'
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `predict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L917)'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L917)'
- en: '[PRE121]'
  id: totrans-1355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Scikit / Keras interface to transformers’ pipelines. This method will forward
    to **call**().
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit / Keras接口到transformers的流水线。此方法将转发到**call**()。
- en: '#### `preprocess`'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1026)'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1026)'
- en: '[PRE122]'
  id: totrans-1359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Preprocess will take the `input_` of a specific pipeline and return a dictionary
    of everything necessary for `_forward` to run properly. It should contain at least
    one tensor, but might have arbitrary other items.
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理将获取特定流水线的`input_`并返回一个包含一切必要内容以使`_forward`正确运行的字典。它应至少包含一个张量，但可能有任意其他项目。
- en: '#### `save_pretrained`'
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L861)'
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L861)'
- en: '[PRE123]'
  id: totrans-1363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: Parameters
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str`) — A path to the directory where to saved. It will
    be created if it doesn’t exist.'
  id: totrans-1365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`（`str`）-要保存的目录的路径。如果不存在，将创建它。'
- en: '`safe_serialization` (`str`) — Whether to save the model using `safetensors`
    or the traditional way for PyTorch or Tensorflow.'
  id: totrans-1366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`（`str`）-是否使用`safetensors`保存模型，还是使用PyTorch或Tensorflow的传统方式。'
- en: Save the pipeline’s model and tokenizer.
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: 保存流水线的模型和分词器。
- en: '#### `transform`'
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transform`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L911)'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L911)'
- en: '[PRE124]'
  id: totrans-1370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Scikit / Keras interface to transformers’ pipelines. This method will forward
    to **call**().
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit / Keras接口到transformers的管道。这种方法将转发到**call**()。
