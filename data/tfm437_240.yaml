- en: Transformer XL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer XL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/transfo-xl](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/transfo-xl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/transfo-xl](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/transfo-xl)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This model is in maintenance mode only, so we won’t accept any new PRs changing
    its code. This model was deprecated due to security issues linked to `pickle.load`.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型只处于维护模式，因此我们不会接受任何新的更改其代码的PR。由于与`pickle.load`相关的安全问题，此模型已被弃用。
- en: We recommend switching to more recent models for improved security.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议切换到更近期的模型以提高安全性。
- en: In case you would still like to use `TransfoXL` in your experiments, we recommend
    using the [Hub checkpoint](https://huggingface.co/transfo-xl-wt103) with a specific
    revision to ensure you are downloading safe files from the Hub.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仍然希望在实验中使用`TransfoXL`，我们建议使用[Hub检查点](https://huggingface.co/transfo-xl-wt103)，并使用特定的修订版本以确保您从Hub下载安全文件。
- en: 'You will need to set the environment variable `TRUST_REMOTE_CODE` to `True`
    in order to allow the usage of `pickle.load()`:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许使用`pickle.load()`，您需要将环境变量`TRUST_REMOTE_CODE`设置为`True`。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you run into any issues running this model, please reinstall the last version
    that supported this model: v4.35.0. You can do so by running the following command:
    `pip install -U transformers==4.35.0`.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.35.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.35.0`。
- en: '[![Models](../Images/d365cfba2e11c71f505634a4ef7ad83e.png)](https://huggingface.co/models?filter=transfo-xl)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/transfo-xl-wt103)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Models](../Images/d365cfba2e11c71f505634a4ef7ad83e.png)](https://huggingface.co/models?filter=transfo-xl)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/transfo-xl-wt103)'
- en: Overview
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Transformer-XL model was proposed in [Transformer-XL: Attentive Language
    Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang
    Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
    It’s a causal (uni-directional) transformer with relative positioning (sinusoïdal)
    embeddings which can reuse previously computed hidden-states to attend to longer
    context (memory). This model also uses adaptive softmax inputs and outputs (tied).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer-XL模型是由Zihang Dai、Zhilin Yang、Yiming Yang、Jaime Carbonell、Quoc V.
    Le、Ruslan Salakhutdinov在[Transformer-XL: Attentive Language Models Beyond a Fixed-Length
    Context](https://arxiv.org/abs/1901.02860)中提出的。它是一个因果（单向）变压器，具有相对定位（正弦）嵌入，可以重用先前计算的隐藏状态以便于更长的上下文（记忆）。该模型还使用自适应softmax输入和输出（绑定）。'
- en: 'The abstract from the paper is the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的摘要如下：
- en: '*Transformers have a potential of learning longer-term dependency, but are
    limited by a fixed-length context in the setting of language modeling. We propose
    a novel neural architecture Transformer-XL that enables learning dependency beyond
    a fixed length without disrupting temporal coherence. It consists of a segment-level
    recurrence mechanism and a novel positional encoding scheme. Our method not only
    enables capturing longer-term dependency, but also resolves the context fragmentation
    problem. As a result, Transformer-XL learns dependency that is 80% longer than
    RNNs and 450% longer than vanilla Transformers, achieves better performance on
    both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers
    during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity
    to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word,
    and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103,
    Transformer-XL manages to generate reasonably coherent, novel text articles with
    thousands of tokens.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*变压器有潜力学习更长期的依赖关系，但在语言建模设置中受到固定长度上下文的限制。我们提出了一种新颖的神经架构Transformer-XL，它能够在不破坏时间连贯性的情况下实现超越固定长度的依赖关系学习。它包括一个段级循环机制和一种新颖的位置编码方案。我们的方法不仅能够捕捉更长期的依赖关系，还能解决上下文碎片化问题。因此，Transformer-XL学习的依赖关系比RNN长80%，比普通变压器长450%，在短序列和长序列上表现更好，并且在评估过程中比普通变压器快1800多倍。值得注意的是，我们将bpc/perplexity的最新结果改进到了enwiki8的0.99，text8的1.08，WikiText-103的18.3，One
    Billion Word的21.8，Penn Treebank的54.5（无需微调）。当仅在WikiText-103上训练时，Transformer-XL能够生成具有数千个标记的合理连贯的新文章。*'
- en: This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The
    original code can be found [here](https://github.com/kimiyoung/transformer-xl).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[thomwolf](https://huggingface.co/thomwolf)贡献的。原始代码可以在[这里](https://github.com/kimiyoung/transformer-xl)找到。
- en: Usage tips
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: Transformer-XL uses relative sinusoidal positional embeddings. Padding can be
    done on the left or on the right. The original implementation trains on SQuAD
    with padding on the left, therefore the padding defaults are set to left.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer-XL使用相对正弦位置嵌入。填充可以在左侧或右侧进行。原始实现在左侧进行填充训练SQuAD，因此填充默认设置为左侧。
- en: Transformer-XL is one of the few models that has no sequence length limit.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer-XL是少数没有序列长度限制的模型之一。
- en: Same as a regular GPT model, but introduces a recurrence mechanism for two consecutive
    segments (similar to a regular RNNs with two consecutive inputs). In this context,
    a segment is a number of consecutive tokens (for instance 512) that may span across
    multiple documents, and segments are fed in order to the model.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与常规的GPT模型相同，但引入了两个连续片段的循环机制（类似于具有两个连续输入的常规RNN）。在这个上下文中，一个片段是一系列连续的标记（例如512个），可能跨越多个文档，片段按顺序馈送到模型中。
- en: Basically, the hidden states of the previous segment are concatenated to the
    current input to compute the attention scores. This allows the model to pay attention
    to information that was in the previous segment as well as the current one. By
    stacking multiple attention layers, the receptive field can be increased to multiple
    previous segments.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本上，前一个片段的隐藏状态被连接到当前输入以计算注意力分数。这使得模型能够关注前一个片段和当前片段中的信息。通过堆叠多个注意力层，可以将感受野扩展到多个先前的片段。
- en: This changes the positional embeddings to positional relative embeddings (as
    the regular positional embeddings would give the same results in the current input
    and the current hidden state at a given position) and needs to make some adjustments
    in the way attention scores are computed.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将把位置嵌入改为位置相对嵌入（因为常规位置嵌入会在给定位置的当前输入和当前隐藏状态中产生相同的结果），并且需要对计算注意力分数的方式进行一些调整。
- en: 'TransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in
    PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PyTorch中的一个错误，TransformerXL与*torch.nn.DataParallel*不兼容，请参阅[问题＃36035](https://github.com/pytorch/pytorch/issues/36035)
- en: Resources
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Causal language modeling task guide](../tasks/language_modeling)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[因果语言建模任务指南](../tasks/language_modeling)'
- en: TransfoXLConfig
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TransfoXLConfig
- en: '### `class transformers.TransfoXLConfig`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TransfoXLConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/configuration_transfo_xl.py#L29)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/configuration_transfo_xl.py#L29)'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 267735) — Vocabulary size of the
    BERT model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [TransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLModel)
    or [TFTransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLModel).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, 默认为267735) — BERT模型的词汇量。定义了在调用[TransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLModel)或[TFTransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLModel)时可以由`inputs_ids`表示的不同标记数量。'
- en: '`cutoffs` (`List[int]`, *optional*, defaults to `[20000, 40000, 200000]`) —
    Cutoffs for the adaptive softmax.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cutoffs` (`List[int]`, *optional*, 默认为`[20000, 40000, 200000]`) — 自适应softmax的截断值。'
- en: '`d_model` (`int`, *optional*, defaults to 1024) — Dimensionality of the model’s
    hidden states.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, 默认为1024) — 模型隐藏状态的维度。'
- en: '`d_embed` (`int`, *optional*, defaults to 1024) — Dimensionality of the embeddings'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_embed` (`int`, *optional*, 默认为1024) — 嵌入的维度'
- en: '`n_head` (`int`, *optional*, defaults to 16) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head` (`int`, *optional*, 默认为16) — Transformer编码器中每个注意力层的注意力头数量。'
- en: '`d_head` (`int`, *optional*, defaults to 64) — Dimensionality of the model’s
    heads.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_head` (`int`, *optional*, 默认为64) — 模型头部的维度。'
- en: '`d_inner` (`int`, *optional*, defaults to 4096) — Inner dimension in FF'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_inner` (`int`, *optional*, 默认为4096) — FF中的内部维度'
- en: '`div_val` (`int`, *optional*, defaults to 4) — Divident value for adapative
    input and softmax'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`div_val` (`int`, *optional*, 默认为4) — 自适应输入和softmax的除数值'
- en: '`pre_lnorm` (`boolean`, *optional*, defaults to `False`) — Whether or not to
    apply LayerNorm to the input instead of the output in the blocks.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pre_lnorm` (`boolean`, *optional*, 默认为`False`) — 是否在块中将LayerNorm应用于输入而不是输出。'
- en: '`n_layer` (`int`, *optional*, defaults to 18) — Number of hidden layers in
    the Transformer encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer` (`int`, *optional*, 默认为18) — Transformer编码器中的隐藏层数量。'
- en: '`mem_len` (`int`, *optional*, defaults to 1600) — Length of the retained previous
    heads.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mem_len` (`int`, *optional*, 默认为1600) — 保留的先前头部的长度。'
- en: '`clamp_len` (`int`, *optional*, defaults to 1000) — Use the same pos embeddings
    after clamp_len.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clamp_len` (`int`, *optional*, 默认为1000) — 在clamp_len之后使用相同的位置嵌入。'
- en: '`same_length` (`boolean`, *optional*, defaults to `True`) — Whether or not
    to use the same attn length for all tokens'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`same_length` (`boolean`, *optional*, 默认为`True`) — 是否对所有标记使用相同的注意力长度'
- en: '`proj_share_all_but_first` (`boolean`, *optional*, defaults to `True`) — True
    to share all but first projs, False not to share.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proj_share_all_but_first` (`boolean`, *optional*, 默认为`True`) — True表示共享除第一个proj之外的所有proj，False表示不共享。'
- en: '`attn_type` (`int`, *optional*, defaults to 0) — Attention type. 0 for Transformer-XL,
    1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_type` (`int`, *optional*, 默认为0) — 注意力类型。0表示Transformer-XL，1表示Shaw等人，2表示Vaswani等人，3表示Al
    Rfou等人。'
- en: '`sample_softmax` (`int`, *optional*, defaults to -1) — Number of samples in
    the sampled softmax.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_softmax` (`int`, *optional*, 默认为-1) — 采样softmax中的样本数量。'
- en: '`adaptive` (`boolean`, *optional*, defaults to `True`) — Whether or not to
    use adaptive softmax.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adaptive` (`boolean`, *optional*, 默认为`True`) — 是否使用自适应softmax。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`dropatt` (`float`, *optional*, defaults to 0.0) — The dropout ratio for the
    attention probabilities.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropatt` (`float`, *optional*, 默认为0.0) — 注意力概率的丢弃比率。'
- en: '`untie_r` (`boolean`, *optional*, defaults to `True`) — Whether ot not to untie
    relative position biases.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`untie_r` (`boolean`, *optional*, 默认为`True`) — 是否解开相对位置偏差。'
- en: '`init` (`str`, *optional*, defaults to `"normal"`) — Parameter initializer
    to use.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init` (`str`, *optional*, 默认为`"normal"`) — 要使用的参数初始化器。'
- en: '`init_range` (`float`, *optional*, defaults to 0.01) — Parameters initialized
    by U(-init_range, init_range).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_range` (`float`, *optional*, 默认为0.01) — 由U(-init_range, init_range)初始化的参数。'
- en: '`proj_init_std` (`float`, *optional*, defaults to 0.01) — Parameters initialized
    by N(0, init_std)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proj_init_std` (`float`, *optional*, 默认为0.01) — 由N(0, init_std)初始化的参数'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — Parameters initialized
    by N(0, init_std)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, 默认为0.02) — 由N(0, init_std)初始化的参数'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) — The epsilon
    to use in the layer normalization layers'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`, *optional*, 默认为1e-05) — 在层归一化层中使用的epsilon'
- en: '`eos_token_id` (`int`, *optional*, defaults to 0) — End of stream token id.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, 默认为0) — 流结束标记id。'
- en: This is the configuration class to store the configuration of a [TransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLModel)
    or a [TFTransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLModel).
    It is used to instantiate a Transformer-XL model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the TransfoXL [transfo-xl-wt103](https://huggingface.co/transfo-xl-wt103)
    architecture.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[TransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLModel)或[TFTransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLModel)配置的配置类。它用于根据指定的参数实例化一个Transformer-XL模型，定义模型架构。使用默认值实例化配置将产生类似于TransfoXL
    [transfo-xl-wt103](https://huggingface.co/transfo-xl-wt103)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TransfoXLTokenizer
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TransfoXLTokenizer
- en: '### `class transformers.TransfoXLTokenizer`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TransfoXLTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py#L124)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py#L124)'
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`special` (`List[str]`, *optional*) — A list of special tokens (to be treated
    by the original implementation of this tokenizer).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special` (`List[str]`，*可选*) — 特殊标记的列表（由此分词器的原始实现处理）。'
- en: '`min_freq` (`int`, *optional*, defaults to 0) — The minimum number of times
    a token has to be present in order to be kept in the vocabulary (otherwise it
    will be mapped to `unk_token`).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_freq` (`int`，*可选*，默认为0) — 标记必须出现的最小次数，以便保留在词汇表中（否则将映射到`unk_token`）。'
- en: '`max_size` (`int`, *optional*) — The maximum size of the vocabulary. If left
    unset, it will default to the size of the vocabulary found after excluding the
    tokens according to the `min_freq` rule.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_size` (`int`，*可选*) — 词汇表的最大大小。如果未设置，它将默认为根据`min_freq`规则排除标记后找到的词汇表的大小。'
- en: '`lower_case` (`bool`, *optional*, defaults to `False`) — Whether or not to
    lowercase the input when tokenizing.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lower_case` (`bool`，*可选*，默认为`False`) — 在标记化时是否将输入转换为小写。'
- en: '`delimiter` (`str`, *optional*) — The delimiter used between tokens.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delimiter` (`str`，*可选*) — 标记之间使用的分隔符。'
- en: '`vocab_file` (`str`, *optional*) — File containing the vocabulary (from the
    original implementation).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`，*可选*) — 包含词汇表的文件（来自原始实现）。'
- en: '`pretrained_vocab_file` (`str`, *optional*) — File containing the vocabulary
    as saved with the `save_pretrained()` method.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_vocab_file` (`str`，*可选*) — 包含使用`save_pretrained()`方法保存的词汇表的文件。'
- en: '`never_split` (`List[str]`, *optional*) — List of tokens that should never
    be split. If no list is specified, will simply use the existing special tokens.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`never_split` (`List[str]`，*可选*) — 不应拆分的标记列表。如果未指定列表，将简单地使用现有的特殊标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`，*可选*，默认为`"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`eos_token` (`str`, *optional*, defaults to `"<eos>"`) — The end of sequence
    token.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`，*可选*，默认为`"<eos>"`) — 序列结束标记。'
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `[''<formula>'']`)
    — A list of additional special tokens (for the HuggingFace functionality).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`，*可选*，默认为`[''<formula>'']`) — 附加特殊标记的列表（用于HuggingFace功能）。'
- en: '`language` (`str`, *optional*, defaults to `"en"`) — The language of this tokenizer
    (used for mose preprocessing).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language` (`str`，*可选*，默认为`"en"`) — 此分词器的语言（用于更多预处理）。'
- en: Construct a Transformer-XL tokenizer adapted from Vocab class in [the original
    code](https://github.com/kimiyoung/transformer-xl). The Transformer-XL tokenizer
    is a word-level tokenizer (no sub-word tokenization).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个从原始代码中的Vocab类调整的Transformer-XL分词器，原始代码在[这里](https://github.com/kimiyoung/transformer-xl)。Transformer-XL分词器是一个单词级的分词器（没有子词分词）。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考这个超类以获取有关这些方法的更多信息。
- en: '#### `save_vocabulary`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py#L328)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py#L328)'
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: TransfoXL specific outputs
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TransfoXL特定输出
- en: '### `class transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L599)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L599)'
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态的序列。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码。已经计算过其过去的令牌id不应该作为输入id传递给此模型。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型输出的隐藏状态，以及初始嵌入输出。
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型输出的基类，可能还包含过去的键/值（用于加速顺序解码）。
- en: '### `class transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L664)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L664)'
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`losses` (`torch.FloatTensor` of shape *(batch_size, sequence_length-1)*, *optional*,
    returned when `labels` is provided) — Language modeling losses (not reduced).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`losses` (`torch.FloatTensor`，形状为*(batch_size, sequence_length-1)*，*可选的*，当提供`labels`时返回)
    — 语言建模损失（未减少）。'
- en: '`prediction_scores` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.vocab_size)`) — Prediction scores of the language modeling head (scores
    for each vocabulary token after SoftMax).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_scores` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    config.vocab_size)`) — 语言建模头的预测分数（SoftMax后每个词汇标记的分数）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码。已经计算过其过去的令牌id不应该作为输入id传递给此模型。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`loss` (`torch.FloatTensor` of shape `()`, *optional*, returned when `labels`
    is provided) — Reduced language modeling loss.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`()`，*可选的*，当提供`labels`时返回) — 减少的语言建模损失。'
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型输出的基类，可能还包含过去的键/值（用于加速顺序解码）。
- en: '### `class transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L672)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L672)'
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`)
    — 模型最后一层的隐藏状态序列。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[tf.Tensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码。已经计算过其过去的令牌id不应该作为输入id传递给此模型。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出的基类，可能还包含过去的键/值（以加速顺序解码）。
- en: '### `class transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L703)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L703)'
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`losses` (`tf.Tensor` of shape *(batch_size, sequence_length-1)*, *optional*,
    returned when `labels` is provided) — Language modeling losses (not reduced).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`losses`（形状为*(batch_size, sequence_length-1)*的`tf.Tensor`，*可选*，当提供`labels`时返回）-
    语言建模损失（未减少）。'
- en: '`prediction_scores` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token after SoftMax).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_scores`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）-
    语言建模头的预测分数（SoftMax后每个词汇标记的分数）。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[tf.Tensor]`）- 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码。将其过去传递给此模型的令牌ID不应作为输入ID传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Base class for model’s outputs that may also contain a past key/values (to speed
    up sequential decoding).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出的基类，可能还包含过去的键/值（以加速顺序解码）。
- en: PytorchHide Pytorch content
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: TransfoXLModel
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TransfoXLModel
- en: '### `class transformers.TransfoXLModel`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TransfoXLModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L760)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L760)'
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）-
    模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Bert Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的Bert模型变压器输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L867)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L867)'
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks) as computed by the model
    (see `mems` output below). Can be used to speed up sequential decoding. The token
    ids which have their mems given to this model should not be passed as `input_ids`
    as they have already been computed.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含由模型计算的预先计算的隐藏状态（注意力块中的键和值）（请参见下面的`mems`输出）。可用于加速顺序解码。将其过去给此模型的标记ID不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*)
    — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    and inputs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput)或`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层输出的隐藏状态序列。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码（查看`mems`输入）。将其过去传递给此模型的标记ID不应作为输入ID传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 后的注意力权重，用于计算自注意力头部的加权平均值。
- en: The [TransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLModel)
    forward method, overrides the `__call__` special method.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[TransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLModel)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: TransfoXLLMHeadModel
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TransfoXLLMHeadModel
- en: '### `class transformers.TransfoXLLMHeadModel`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TransfoXLLMHeadModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L997)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L997)'
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The Transformer-XL Model with a language modeling head on top (adaptive softmax
    with weights tied to the adaptive input embeddings)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有语言建模头部的 Transformer-XL 模型（自适应 softmax，其权重与自适应输入嵌入绑定）
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L1060)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L1060)'
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。查看 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    了解详情。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[输入 ID 是什么？](../glossary#input-ids)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks) as computed by the model
    (see `mems` output below). Can be used to speed up sequential decoding. The token
    ids which have their mems given to this model should not be passed as `input_ids`
    as they have already been computed.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — 包含由模型计算的预计算隐藏状态（注意力块中的键和值），如模型（见下面的
    `mems` 输出）所计算的。可用于加速顺序解码。将其 mems 给定给此模型的令牌 id 不应作为 `input_ids` 传递，因为它们已经被计算过了。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被屏蔽。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids`
    索引转换为关联向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于语言建模的标签。请注意，模型内部**已经移动**标签，即您可以设置`labels
    = input_ids`。在`[-100, 0, ..., config.vocab_size]`中选择索引。所有设置为`-100`的标签都被忽略（掩码），损失仅计算`[0,
    ..., config.vocab_size]`中的标签。'
- en: Returns
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    and inputs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput)
    或 `torch.FloatTensor` 元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）和输入的各种元素。'
- en: '`losses` (`torch.FloatTensor` of shape *(batch_size, sequence_length-1)*, *optional*,
    returned when `labels` is provided) — Language modeling losses (not reduced).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`losses` (`torch.FloatTensor`，形状为*(batch_size, sequence_length-1)*，*可选*，当提供`labels`时返回)
    — 语言建模损失（未减少）。'
- en: '`prediction_scores` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.vocab_size)`) — Prediction scores of the language modeling head (scores
    for each vocabulary token after SoftMax).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_scores` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    config.vocab_size)`） — 语言建模头的预测分数（SoftMax后每个词汇标记的分数）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码（见`mems`输入）。将其过去传递给此模型的令牌id不应作为输入id传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`loss` (`torch.FloatTensor` of shape `()`, *optional*, returned when `labels`
    is provided) Reduced language modeling loss.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`()`，*可选*，当提供`labels`时返回) 减少的语言建模损失。'
- en: The [TransfoXLLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[TransfoXLLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: TransfoXLForSequenceClassification
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TransfoXLForSequenceClassification
- en: '### `class transformers.TransfoXLForSequenceClassification`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TransfoXLForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L1177)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L1177)'
- en: '[PRE15]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The Transformer-XL Model transformer with a sequence classification head on
    top (linear layer).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer-XL 模型，顶部带有一个序列分类头（线性层）。
- en: '[TransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-1) do.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[TransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification)
    使用最后一个令牌进行分类，就像其他因果模型（例如 GPT-1）一样。'
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它在最后一个令牌上进行分类，因此需要知道最后一个令牌的位置。如果在配置中定义了 `pad_token_id`，则会找到每行中不是填充令牌的最后一个令牌。如果未定义
    `pad_token_id`，则会简单地取批处理中每行的最后一个值。当传递 `inputs_embeds` 而不是 `input_ids` 时，它无法猜测填充令牌，因此会执行相同操作（取批处理中每行的最后一个值）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L1201)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L1201)'
- en: '[PRE16]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列令牌的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks) as computed by the model
    (see `mems` output below). Can be used to speed up sequential decoding. The token
    ids which have their mems given to this model should not be passed as `input_ids`
    as they have already been computed.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — 包含由模型计算得出的预先计算的隐藏状态（注意力块中的键和值），可以用于加速顺序解码。将这些隐藏状态传递给模型的令牌
    id 不应作为 `input_ids` 传递，因为它们已经被计算过。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被屏蔽。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids`
    索引转换为关联向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast`
    or `tuple(torch.FloatTensor)`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast`或`torch.FloatTensor`元组'
- en: A `transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    and inputs.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`损失` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`） — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`mems` (`List[torch.FloatTensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[torch.FloatTensor]`，长度为`config.n_layers`) — 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码（查看`mems`输入）。将其过去给予此模型的令牌id不应作为输入id传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[TransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of single-label classification:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签分类示例：
- en: '[PRE17]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Example of multi-label classification:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类示例：
- en: '[PRE18]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: TensorFlowHide TensorFlow content
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: TFTransfoXLModel
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFTransfoXLModel
- en: '### `class transformers.TFTransfoXLModel`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFTransfoXLModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L851)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L851)'
- en: '[PRE19]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Bert Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的Bert模型变压器输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该可以“正常工作”
    - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可用于收集所有输入张量放在第一个位置参数中：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个`input_ids`的张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个按照文档字符串中给定顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个包含一个或多个与文档字符串中给定输入名称相关联的输入张量的字典：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L860)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L860)'
- en: '[PRE20]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`）-
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是input IDs？](../glossary#input-ids)'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks) as computed by the model
    (see `mems` output below). Can be used to speed up sequential decoding. The token
    ids which have their mems given to this model should not be passed as `input_ids`
    as they have already been computed.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[tf.Tensor]`）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值）（请参见下面的`mems`输出）。可用于加速顺序解码。将其`mems`给定给此模型的标记id不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`head_mask` (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`或`Numpy
    array`，*可选*）- 用于使自注意力模块中的选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`inputs_embeds` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`或`Numpy
    array`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *optional*, defaults to `False`) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput)
    或 `tuple(tf.Tensor)`'
- en: A [transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    and inputs.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput)
    或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）和输入的不同元素。'
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`）
    — 模型最后一层的隐藏状态序列。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[tf.Tensor]`，长度为`config.n_layers`) — 包含预计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码（参见`mems`输入）。将过去给定给该模型的令牌id不应作为输入id传递，因为它们已经被计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFTransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLModel)
    forward method, overrides the `__call__` special method.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFTransfoXLModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE21]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: TFTransfoXLLMHeadModel
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFTransfoXLLMHeadModel
- en: '### `class transformers.TFTransfoXLLMHeadModel`'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFTransfoXLLMHeadModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L892)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L892)'
- en: '[PRE22]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）-
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Transformer-XL Model with a language modeling head on top (adaptive softmax
    with weights tied to the adaptive input embeddings)
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 具有顶部语言建模头部的Transformer-XL模型（自适应softmax，其权重与自适应输入嵌入相关联）
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以“正常工作”
    - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可用于在第一个位置参数中收集所有输入张量：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个包含`input_ids`的张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L928)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L928)'
- en: '[PRE23]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy数组`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks) as computed by the model
    (see `mems` output below). Can be used to speed up sequential decoding. The token
    ids which have their mems given to this model should not be passed as `input_ids`
    as they have already been computed.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems`（长度为`config.n_layers`的`List[tf.Tensor]`）- 包含由模型计算得到的预计算隐藏状态（自注意力块中的键和值）（请参见下面的`mems`输出）。可用于加速顺序解码。将其mems给定给此模型的标记ID不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`head_mask` (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`或`Numpy数组`，*可选*）-
    用于使自注意力模块中选择的头部失效的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部“未被掩码”，
- en: 0 indicates the head is `masked`.
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`masked`。
- en: '`inputs_embeds` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor`或`Numpy数组`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这很有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。这个参数可以在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`，*optional*，默认为`False`) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    and inputs.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）和输入的各种元素。
- en: '`losses` (`tf.Tensor` of shape *(batch_size, sequence_length-1)*, *optional*,
    returned when `labels` is provided) — Language modeling losses (not reduced).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`losses` (`tf.Tensor`，形状为*(batch_size, sequence_length-1)*，*optional*, 当提供`labels`时返回)
    — 语言建模损失（未减少）。'
- en: '`prediction_scores` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token after SoftMax).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_scores` (`tf.Tensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax后每个词汇标记的分数）。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — 包含预先计算的隐藏状态（注意力块中的键和值）。可以用于加速顺序解码。将其过去传递给该模型的令牌id不应作为输入id传递，因为它们已经被计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出处的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFTransfoXLLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel)
    forward method, overrides the `__call__` special method.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFTransfoXLLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默忽略它们。
- en: 'Example:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE24]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: TFTransfoXLForSequenceClassification
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFTransfoXLForSequenceClassification
- en: '### `class transformers.TFTransfoXLForSequenceClassification`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFTransfoXLForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L1006)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L1006)'
- en: '[PRE25]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）
    - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Transfo XL Model transformer with a sequence classification head on top
    (linear layer).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: Transfo XL模型变压器，顶部带有序列分类头（线性层）。
- en: '[TFTransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-1,GPT-2) do.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFTransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification)使用最后一个标记来进行分类，就像其他因果模型（例如GPT-1、GPT-2）一样。'
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它在最后一个标记上进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，它会找到每行中不是填充标记的最后一个标记。如果未定义`pad_token_id`，它会简单地取批处理中每行的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，因此它会执行相同操作（取批处理中每行的最后一个值）。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种输入格式：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以正常工作
    - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras的`Functional`
    API创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个按照文档字符串中给定顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像将输入传递给任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L1041)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L1041)'
- en: '[PRE26]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`tf.Tensor` 或形状为 `(batch_size, sequence_length)` 的 `Numpy array`)
    — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    和 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks) as computed by the model
    (see `mems` output below). Can be used to speed up sequential decoding. The token
    ids which have their mems given to this model should not be passed as `input_ids`
    as they have already been computed.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[tf.Tensor]`，长度为 `config.n_layers`) — 包含由模型计算得到的预计算隐藏状态（注意力块中的键和值）（请参见下面的
    `mems` 输出）。可用于加速顺序解码。将其 `mems` 给予此模型的标记 id 不应作为 `input_ids` 传递，因为它们已经计算过。'
- en: '`head_mask` (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`tf.Tensor` 或形状为 `(num_heads,)` 或 `(num_layers, num_heads)` 的
    `Numpy array`，*可选*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在 `[0, 1]` 之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `masked`。
- en: 0 indicates the head is `masked`.
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `masked`。
- en: '`inputs_embeds` (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`tf.Tensor` 或形状为 `(batch_size, sequence_length, hidden_size)`
    的 `Numpy array`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids`
    索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *可选*，默认为 `False`) — 是否在训练模式下使用模型（一些模块，如 dropout 模块，在训练和评估之间有不同的行为）。'
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the cross entropy classification loss. Indices should be
    in `[0, ..., config.vocab_size - 1]`.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 用于计算交叉熵分类损失的标签。索引应在
    `[0, ..., config.vocab_size - 1]` 中。'
- en: Returns
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast`
    or `tuple(tf.Tensor)`'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast`
    或 `tuple(tf.Tensor)`'
- en: A `transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig))
    and inputs.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast`
    或 `tf.Tensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[TransfoXLConfig](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 分类（如果 `config.num_labels==1`
    则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor`，形状为 `(batch_size, config.num_labels)`) — 分类（如果 `config.num_labels==1`
    则为回归）得分（SoftMax 之前）。'
- en: '`mems` (`List[tf.Tensor]` of length `config.n_layers`) — Contains pre-computed
    hidden-states (key and values in the attention blocks). Can be used (see `mems`
    input) to speed up sequential decoding. The token ids which have their past given
    to this model should not be passed as input ids as they have already been computed.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mems` (`List[tf.Tensor]`，长度为 `config.n_layers`) — 包含预计算的隐藏状态（注意力块中的键和值）。可用于（参见
    `mems` 输入）加速顺序解码。将其过去给予此模型的标记 id 不应作为输入 id 传递，因为它们已经计算过。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFTransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFTransfoXLForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE27]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Internal Layers
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内部层
- en: '### `class transformers.AdaptiveEmbedding`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AdaptiveEmbedding`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L399)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py#L399)'
- en: '[PRE29]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '### `class transformers.TFAdaptiveEmbedding`'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFAdaptiveEmbedding`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L344)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py#L344)'
- en: '[PRE30]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
