- en: UPerNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UPerNet
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/upernet)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: The UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221)
    by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a
    general framework to effectively segment a wide range of concepts from images,
    leveraging any vision backbone like [ConvNeXt](convnext) or [Swin](swin).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: UPerNetæ¨¡å‹æ˜¯ç”±è‚–ç‰¹ã€åˆ˜è‹±æˆã€å‘¨åšç£Šã€å§œç‰å®ã€å­™åšåœ¨[ç»Ÿä¸€æ„ŸçŸ¥è§£æç”¨äºåœºæ™¯ç†è§£](https://arxiv.org/abs/1807.10221)ä¸­æå‡ºçš„ã€‚UPerNetæ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»å›¾åƒä¸­åˆ†å‰²å„ç§æ¦‚å¿µï¼Œåˆ©ç”¨ä»»ä½•è§†è§‰éª¨å¹²ï¼Œå¦‚[ConvNeXt](convnext)æˆ–[Swin](swin)ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Humans recognize the visual world at multiple levels: we effortlessly categorize
    scenes and detect objects inside, while also identifying the textures and surfaces
    of the objects along with their different compositional parts. In this paper,
    we study a new task called Unified Perceptual Parsing, which requires the machine
    vision systems to recognize as many visual concepts as possible from a given image.
    A multi-task framework called UPerNet and a training strategy are developed to
    learn from heterogeneous image annotations. We benchmark our framework on Unified
    Perceptual Parsing and show that it is able to effectively segment a wide range
    of concepts from images. The trained networks are further applied to discover
    visual knowledge in natural scenes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*äººç±»åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šè¯†åˆ«è§†è§‰ä¸–ç•Œï¼šæˆ‘ä»¬è½»æ¾åœ°å¯¹åœºæ™¯è¿›è¡Œåˆ†ç±»ï¼Œå¹¶æ£€æµ‹å…¶ä¸­çš„å¯¹è±¡ï¼ŒåŒæ—¶è¿˜è¯†åˆ«å¯¹è±¡çš„çº¹ç†å’Œè¡¨é¢ä»¥åŠå®ƒä»¬ä¸åŒçš„ç»„æˆéƒ¨åˆ†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªç§°ä¸ºç»Ÿä¸€æ„ŸçŸ¥è§£æçš„æ–°ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡è¦æ±‚æœºå™¨è§†è§‰ç³»ç»Ÿä»ç»™å®šå›¾åƒä¸­å°½å¯èƒ½è¯†åˆ«å°½å¯èƒ½å¤šçš„è§†è§‰æ¦‚å¿µã€‚å¼€å‘äº†ä¸€ä¸ªåä¸ºUPerNetçš„å¤šä»»åŠ¡æ¡†æ¶å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥ä»å¼‚æ„å›¾åƒæ³¨é‡Šä¸­å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ç»Ÿä¸€æ„ŸçŸ¥è§£æä¸Šå¯¹æˆ‘ä»¬çš„æ¡†æ¶è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶å±•ç¤ºå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å›¾åƒä¸­åˆ†å‰²å„ç§æ¦‚å¿µã€‚è®­ç»ƒçš„ç½‘ç»œè¿›ä¸€æ­¥åº”ç”¨äºå‘ç°è‡ªç„¶åœºæ™¯ä¸­çš„è§†è§‰çŸ¥è¯†ã€‚*'
- en: '![drawing](../Images/a144fe8a7a6a0326f562f0adcb5e8255.png) UPerNet framework.
    Taken from the [original paper](https://arxiv.org/abs/1807.10221).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![å›¾ç¤º](../Images/a144fe8a7a6a0326f562f0adcb5e8255.png) UPerNet æ¡†æ¶ã€‚å–è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/1807.10221)ã€‚'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code is based on OpenMMLabâ€™s mmsegmentation [here](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç åŸºäºOpenMMLabçš„mmsegmentation
    [è¿™é‡Œ](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py)ã€‚
- en: Usage examples
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç¤ºä¾‹
- en: 'UPerNet is a general framework for semantic segmentation. It can be used with
    any vision backbone, like so:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: UPerNetæ˜¯è¯­ä¹‰åˆ†å‰²çš„é€šç”¨æ¡†æ¶ã€‚å¯ä»¥ä¸ä»»ä½•è§†è§‰éª¨å¹²ä¸€èµ·ä½¿ç”¨ï¼Œå¦‚ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To use another vision backbone, like [ConvNeXt](convnext), simply instantiate
    the model with the appropriate backbone:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨å¦ä¸€ä¸ªè§†è§‰éª¨å¹²ï¼Œå¦‚[ConvNeXt](convnext)ï¼Œåªéœ€ä½¿ç”¨é€‚å½“çš„éª¨å¹²å®ä¾‹åŒ–æ¨¡å‹ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that this will randomly initialize all the weights of the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™å°†éšæœºåˆå§‹åŒ–æ¨¡å‹çš„æ‰€æœ‰æƒé‡ã€‚
- en: Resources
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with UPerNet.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨UPerNetã€‚
- en: Demo notebooks for UPerNet can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UPerNetçš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet)æ‰¾åˆ°ã€‚
- en: '[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)æ”¯æŒã€‚'
- en: 'See also: [Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æŒ‡å—](../tasks/semantic_segmentation)
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: UperNetConfig
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UperNetConfig
- en: '### `class transformers.UperNetConfig`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UperNetConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/configuration_upernet.py#L26)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/configuration_upernet.py#L26)'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`backbone_config` (`PretrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`)
    â€” The configuration of the backbone model.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` (`PretrainedConfig`æˆ–`dict`, *å¯é€‰*, é»˜è®¤ä¸º`ResNetConfig()`) â€”
    éª¨å¹²æ¨¡å‹çš„é…ç½®ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 512) â€” The number of hidden units
    in the convolutional layers.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” å·ç§¯å±‚ä¸­éšè—å•å…ƒçš„æ•°é‡ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) â€” Pooling
    scales used in Pooling Pyramid Module applied on the last feature map.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool_scales` (`Tuple[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[1, 2, 3, 6]`) â€” åº”ç”¨äºæœ€åç‰¹å¾å›¾çš„Pooling Pyramid
    Moduleä¸­ä½¿ç”¨çš„æ± åŒ–å°ºåº¦ã€‚'
- en: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” Whether to
    use an auxiliary head during training.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_head` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” è®­ç»ƒæœŸé—´æ˜¯å¦ä½¿ç”¨è¾…åŠ©å¤´ã€‚'
- en: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” Weight of
    the cross-entropy loss of the auxiliary head.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss_weight` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.4) â€” è¾…åŠ©å¤´çš„äº¤å‰ç†µæŸå¤±çš„æƒé‡ã€‚'
- en: '`auxiliary_channels` (`int`, *optional*, defaults to 256) â€” Number of channels
    to use in the auxiliary head.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_channels` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º256) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„é€šé“æ•°ã€‚'
- en: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) â€” Number of convolutional
    layers to use in the auxiliary head.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_num_convs` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„å·ç§¯å±‚æ•°ã€‚'
- en: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) â€” Whether
    to concatenate the output of the auxiliary head with the input before the classification
    layer.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_concat_input` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨åˆ†ç±»å±‚ä¹‹å‰å°†è¾…åŠ©å¤´çš„è¾“å‡ºä¸è¾“å…¥è¿æ¥ã€‚'
- en: '`loss_ignore_index` (`int`, *optional*, defaults to 255) â€” The index that is
    ignored by the loss function.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_ignore_index` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º255) â€” æŸå¤±å‡½æ•°å¿½ç•¥çš„ç´¢å¼•ã€‚'
- en: This is the configuration class to store the configuration of an [UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation).
    It is used to instantiate an UperNet model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the UperNet [openmmlab/upernet-convnext-tiny](https://huggingface.co/openmmlab/upernet-convnext-tiny)
    architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªUperNetæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºUperNet
    [openmmlab/upernet-convnext-tiny](https://huggingface.co/openmmlab/upernet-convnext-tiny)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Examples:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: UperNetForSemanticSegmentation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UperNetForSemanticSegmentation
- en: '### `class transformers.UperNetForSemanticSegmentation`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UperNetForSemanticSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L343)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L343)'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`This` model is a PyTorch [torch.nn.Module](https â€”//pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`This`æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https â€”//pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚ä½¿ç”¨'
- en: '`it` as a regular PyTorch Module and refer to the PyTorch documentation for
    all matter related to general usage and â€” behavior. â€” config ([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig)):
    Model configuration class with all the parameters of the model. Initializing with
    a config file does not load the weights associated with the model, only the configuration.
    Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`it`ä½œä¸ºå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚é…ç½®([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig))ï¼šæ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: UperNet framework leveraging any vision backbone e.g. for ADE20k, CityScapes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: UperNetæ¡†æ¶åˆ©ç”¨ä»»ä½•è§†è§‰éª¨å¹²ï¼Œä¾‹å¦‚ADE20kï¼ŒCityScapesã€‚
- en: '#### `forward`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L360)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/upernet/modeling_upernet.py#L360)'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [SegformerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor.__call__)
    for details.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹ä¼šå¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[SegformerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor.__call__)ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers in case the backbone has them. See `attentions`
    under returned tensors for more detail.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›éª¨å¹²å…·æœ‰æ³¨æ„åŠ›å¼ é‡çš„æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers of the backbone. See `hidden_states` under returned
    tensors for more detail.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›éª¨å¹²çš„æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, height, width)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig))
    and inputs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`, *å¯é€‰çš„*, å½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” Classification scores for each pixel.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” æ¯ä¸ªåƒç´ çš„åˆ†ç±»åˆ†æ•°ã€‚'
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip warning="{true}">è¿”å›çš„logitsä¸ä¸€å®šä¸ä½œä¸ºè¾“å…¥ä¼ é€’çš„`pixel_values`å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡æ’å€¼å¹¶åœ¨ç”¨æˆ·éœ€è¦å°†logitsè°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°æ—¶ä¸¢å¤±ä¸€äº›è´¨é‡ã€‚æ‚¨åº”å§‹ç»ˆæ£€æŸ¥logitsçš„å½¢çŠ¶å¹¶æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚</tip>
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, patch_size, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+
    ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[UperNetForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
