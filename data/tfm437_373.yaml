- en: ViLT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ViLT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vilt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vilt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vilt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vilt)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The ViLT model was proposed in [ViLT: Vision-and-Language Transformer Without
    Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae
    Kim, Bokyung Son, Ildoo Kim. ViLT incorporates text embeddings into a Vision Transformer
    (ViT), allowing it to have a minimal design for Vision-and-Language Pre-training
    (VLP).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'ViLT模型是由Wonjae Kim、Bokyung Son、Ildoo Kim在[ViLT: Vision-and-Language Transformer
    Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)中提出的。ViLT将文本嵌入集成到Vision
    Transformer（ViT）中，使其在视觉和语言预训练（VLP）中具有最小设计。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Vision-and-Language Pre-training (VLP) has improved performance on various
    joint vision-and-language downstream tasks. Current approaches to VLP heavily
    rely on image feature extraction processes, most of which involve region supervision
    (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although
    disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed,
    that simply extracting input features requires much more computation than the
    multimodal interaction steps; and (2) expressive power, as it is upper bounded
    to the expressive power of the visual embedder and its predefined visual vocabulary.
    In this paper, we present a minimal VLP model, Vision-and-Language Transformer
    (ViLT), monolithic in the sense that the processing of visual inputs is drastically
    simplified to just the same convolution-free manner that we process textual inputs.
    We show that ViLT is up to tens of times faster than previous VLP models, yet
    with competitive or better downstream task performance.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*视觉和语言预训练（VLP）在各种联合视觉和语言下游任务上提高了性能。当前的VLP方法主要依赖于图像特征提取过程，其中大部分涉及区域监督（例如目标检测）和卷积架构（例如ResNet）。尽管文献中忽略了这一点，但我们发现这在效率/速度方面存在问题，仅仅提取输入特征就需要比多模态交互步骤更多的计算量；以及表达能力方面，因为它受到视觉嵌入器及其预定义视觉词汇表表达能力的上限。在本文中，我们提出了一个最小的VLP模型，Vision-and-Language
    Transformer（ViLT），在处理视觉输入方面与处理文本输入的无卷积方式相同，从而使其成为一个整体。我们展示ViLT比以前的VLP模型快数十倍，但在下游任务性能上具有竞争力或更好。*'
- en: '![drawing](../Images/39474e1425b5e4960e6fcf4a616a954b.png) ViLT architecture.
    Taken from the [original paper](https://arxiv.org/abs/2102.03334).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ViLT架构。取自[原始论文](https://arxiv.org/abs/2102.03334)。
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/dandelin/ViLT).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/dandelin/ViLT)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: The quickest way to get started with ViLT is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViLT)
    (which showcase both inference and fine-tuning on custom data).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用ViLT的最快方法是查看[示例笔记本](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViLT)（展示了对自定义数据的推理和微调）。
- en: ViLT is a model that takes both `pixel_values` and `input_ids` as input. One
    can use [ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)
    to prepare data for the model. This processor wraps a image processor (for the
    image modality) and a tokenizer (for the language modality) into one.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViLT是一个同时接受`pixel_values`和`input_ids`作为输入的模型。可以使用[ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)为模型准备数据。该处理器将图像处理器（用于图像模态）和标记器（用于语言模态）封装在一起。
- en: 'ViLT is trained with images of various sizes: the authors resize the shorter
    edge of input images to 384 and limit the longer edge to under 640 while preserving
    the aspect ratio. To make batching of images possible, the authors use a `pixel_mask`
    that indicates which pixel values are real and which are padding. [ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)
    automatically creates this for you.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViLT使用各种尺寸的图像进行训练：作者将输入图像的较短边调整为384，并将较长边限制在640以下，同时保持纵横比。为了使图像的批处理成为可能，作者使用一个`pixel_mask`指示哪些像素值是真实的，哪些是填充的。[ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)会自动为您创建这个。
- en: The design of ViLT is very similar to that of a standard Vision Transformer
    (ViT). The only difference is that the model includes additional embedding layers
    for the language modality.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViLT的设计与标准Vision Transformer（ViT）非常相似。唯一的区别是该模型包括用于语言模态的额外嵌入层。
- en: The PyTorch version of this model is only available in torch 1.10 and higher.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此模型的PyTorch版本仅在torch 1.10及更高版本中可用。
- en: ViltConfig
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltConfig
- en: '### `class transformers.ViltConfig`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/configuration_vilt.py#L28)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/configuration_vilt.py#L28)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    text part of the model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为30522）—模型文本部分的词汇量。定义了在调用[ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel)时可以表示的不同标记数量。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling [ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel).
    This is used when encoding text.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size`（`int`，*可选*，默认为2）—在调用[ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel)时传递的`token_type_ids`的词汇量。在编码文本时使用。'
- en: '`modality_type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary
    size of the modalities passed when calling [ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel).
    This is used after concatening the embeddings of the text and image modalities.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modality_type_vocab_size` (`int`, *optional*, defaults to 2) — 在调用[ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel)时传递的模态词汇表大小。在连接文本和图像模态的嵌入后使用。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 40) — The maximum
    sequence length that this model might ever be used with.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 40) — 该模型可能使用的最大序列长度。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数量。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化层中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化层中所有全连接层的dropout概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。'
- en: '`image_size` (`int`, *optional*, defaults to 384) — The size (resolution) of
    each image.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 384) — 每张图片的大小（分辨率）。'
- en: '`patch_size` (`int`, *optional*, defaults to 32) — The size (resolution) of
    each patch.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 32) — 每个补丁的大小（分辨率）。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) — 输入通道数。'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。'
- en: '`max_image_length` (`int`, *optional*, defaults to -1) — The maximum number
    of patches to take as input for the Transformer encoder. If set to a positive
    integer, the encoder will sample `max_image_length` patches at maximum. If set
    to -1, will not be taken into account.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_image_length` (`int`, *optional*, defaults to -1) — 用于Transformer编码器的输入最大补丁数。如果设置为正整数，编码器将最多采样`max_image_length`个补丁。如果设置为-1，则不会考虑。'
- en: '`num_images` (`int`, *optional*, defaults to -1) — The number of images to
    use for natural language visual reasoning. If set to a positive integer, will
    be used by [ViltForImagesAndTextClassification](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForImagesAndTextClassification)
    for defining the classifier head.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images` (`int`, *optional*, defaults to -1) — 用于自然语言视觉推理的图像数量。如果设置为正整数，将被[ViltForImagesAndTextClassification](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForImagesAndTextClassification)用于定义分类器头。'
- en: This is the configuration class to store the configuration of a `ViLTModel`.
    It is used to instantiate an ViLT model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ViLT [dandelin/vilt-b32-mlm](https://huggingface.co/dandelin/vilt-b32-mlm)
    architecture.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储`ViLTModel`配置的配置类。根据指定的参数实例化ViLT模型，定义模型架构。使用默认值实例化配置将产生类似于ViLT[dandelin/vilt-b32-mlm](https://huggingface.co/dandelin/vilt-b32-mlm)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ViltFeatureExtractor
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltFeatureExtractor
- en: '### `class transformers.ViltFeatureExtractor`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/feature_extraction_vilt.py#L26)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/feature_extraction_vilt.py#L26)'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `__call__`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess an image or a batch of images.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理一张图片或一批图片。
- en: ViltImageProcessor
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltImageProcessor
- en: '### `class transformers.ViltImageProcessor`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/image_processing_vilt.py#L120)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/image_processing_vilt.py#L120)'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, defaults to `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的
    `size`。可以被 `preprocess` 方法中的 `do_resize` 参数覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 384}`):
    Resize the shorter side of the input to `size["shortest_edge"]`. The longer side
    will be limited to under `int((1333 / 800) * size["shortest_edge"])` while preserving
    the aspect ratio. Only has an effect if `do_resize` is set to `True`. Can be overridden
    by the `size` parameter in the `preprocess` method.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 384}`):
    调整输入的较短边到 `size["shortest_edge"]`。较长边将被限制在 `int((1333 / 800) * size["shortest_edge"])`
    以下，同时保持纵横比。仅在 `do_resize` 设置为 `True` 时有效。可以被 `preprocess` 方法中的 `size` 参数覆盖。'
- en: '`size_divisor` (`int`, *optional*, defaults to 32) — The size by which to make
    sure both the height and width can be divided. Only has an effect if `do_resize`
    is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess`
    method.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size_divisor` (`int`, *optional*, defaults to 32) — 用于确保高度和宽度都可以被整除的大小。仅在
    `do_resize` 设置为 `True` 时有效。可以被 `preprocess` 方法中的 `size_divisor` 参数覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`)
    — Resampling filter to use if resizing the image. Only has an effect if `do_resize`
    is set to `True`. Can be overridden by the `resample` parameter in the `preprocess`
    method.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`)
    — 如果调整图像大小，则使用的重采样滤波器。仅在 `do_resize` 设置为 `True` 时有效。可以被 `preprocess` 方法中的 `resample`
    参数覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Wwhether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, defaults to `True`) — 是否按指定比例 `rescale_factor`
    重新缩放图像。可以被 `preprocess` 方法中的 `do_rescale` 参数覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Only has an effect if `do_rescale` is set
    to `True`. Can be overridden by the `rescale_factor` parameter in the `preprocess`
    method.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — 如果重新缩放图像，则使用的缩放因子。仅在
    `do_rescale` 设置为 `True` 时有效。可以被 `preprocess` 方法中的 `rescale_factor` 参数覆盖。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, defaults to `True`) — 是否对图像进行归一化。可以被 `preprocess`
    方法中的 `do_normalize` 参数覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — 如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean`
    参数覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method. Can be overridden by
    the `image_std` parameter in the `preprocess` method.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — 如果对图像进行归一化，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std`
    参数覆盖。'
- en: '`do_pad` (`bool`, *optional*, defaults to `True`) — Whether to pad the image
    to the `(max_height, max_width)` of the images in the batch. Can be overridden
    by the `do_pad` parameter in the `preprocess` method.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_pad` (`bool`, *optional*, defaults to `True`) — 是否将图像填充到批次中图像的（最大高度，最大宽度）。可以被
    `preprocess` 方法中的 `do_pad` 参数覆盖。'
- en: Constructs a ViLT image processor.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 ViLT 图像处理器。
- en: '#### `preprocess`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/image_processing_vilt.py#L338)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/image_processing_vilt.py#L338)'
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个图像或批量图像，像素值范围从 0 到 255。如果传入像素值在 0 到 1
    之间的图像，请设置 `do_rescale=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Controls the
    size of the image after `resize`. The shortest edge of the image is resized to
    `size["shortest_edge"]` whilst preserving the aspect ratio. If the longest edge
    of this resized image is > `int(size["shortest_edge"] * (1333 / 800))`, then the
    image is resized again to make the longest edge equal to `int(size["shortest_edge"]
    * (1333 / 800))`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — 控制 `resize`
    后图像的大小。图像的最短边被调整为 `size["shortest_edge"]`，同时保持纵横比。如果调整后图像的最长边 > `int(size["shortest_edge"]
    * (1333 / 800))`，则再次调整图像，使最长边等于 `int(size["shortest_edge"] * (1333 / 800))`。'
- en: '`size_divisor` (`int`, *optional*, defaults to `self.size_divisor`) — The image
    is resized to a size that is a multiple of this value.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size_divisor` (`int`, *optional*, defaults to `self.size_divisor`) — 图像被调整为该值的倍数。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    — Resampling filter to use if resizing the image. Only has an effect if `do_resize`
    is set to `True`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    — 如果调整图像大小，则使用的重采样滤波器。仅在 `do_resize` 设置为 `True` 时有效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale`（`bool`，*可选*，默认为`self.do_rescale`）— 是否将图像值重新缩放在[0 - 1]之间。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor`（`float`，*可选*，默认为`self.rescale_factor`）— 如果`do_rescale`设置为`True`，用于重新缩放图像的重新缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize`（`bool`，*可选*，默认为`self.do_normalize`）— 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to normalize the image by if `do_normalize` is set to `True`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean`（`float`或`List[float]`，*可选*，默认为`self.image_mean`）— 如果`do_normalize`设置为`True`，用于归一化图像的图像均值。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to normalize the image by if `do_normalize` is set
    to `True`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std`（`float`或`List[float]`，*可选*，默认为`self.image_std`）— 如果`do_normalize`设置为`True`，用于归一化图像的图像标准差。'
- en: '`do_pad` (`bool`, *optional*, defaults to `self.do_pad`) — Whether to pad the
    image to the (max_height, max_width) in the batch. If `True`, a pixel mask is
    also created and returned.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_pad`（`bool`，*可选*，默认为`self.do_pad`）— 是否将图像填充到批处理中的（max_height，max_width）。如果为`True`，还会创建并返回像素掩码。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`或`TensorType`，*可选*）— 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：返回一个`np.ndarray`列表。
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW`或`''tf''`：返回类型为`tf.Tensor`的批处理。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH`或`''pt''`：返回类型为`torch.Tensor`的批处理。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY`或`''np''`：返回类型为`np.ndarray`的批处理。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX`或`''jax''`：返回类型为`jax.numpy.ndarray`的批处理。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format`（`ChannelDimension`或`str`，*可选*，默认为`ChannelDimension.FIRST`）— 输出图像的通道维度格式。可以是以下之一：'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format`（`ChannelDimension`或`str`，*可选*）— 输入图像的通道维度格式。如果未设置，则从输入图像中推断出通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`或`ChannelDimension.NONE`：图像以（高度，宽度）格式。'
- en: Preprocess an image or batch of images.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或图像批处理。
- en: ViltProcessor
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltProcessor
- en: '### `class transformers.ViltProcessor`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/processing_vilt.py#L27)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/processing_vilt.py#L27)'
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` (`ViltImageProcessor`, *optional*) — An instance of [ViltImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltImageProcessor).
    The image processor is a required input.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`（`ViltImageProcessor`，*可选*）— 一个[ViltImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltImageProcessor)的实例。图像处理器是必需的输入。'
- en: '`tokenizer` (`BertTokenizerFast`, *optional*) — An instance of [‘BertTokenizerFast`].
    The tokenizer is a required input.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`BertTokenizerFast`，*可选*）— 一个[‘BertTokenizerFast`]的实例。分词器是必需的输入。'
- en: Constructs a ViLT processor which wraps a BERT tokenizer and ViLT image processor
    into a single processor.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个ViLT处理器，将BERT分词器和ViLT图像处理器封装成一个处理器。
- en: '[ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)
    offers all the functionalities of [ViltImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltImageProcessor)
    and [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor.__call__)
    and `decode()` for more information.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViltProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor)提供了[ViltImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltImageProcessor)和[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)的所有功能。有关更多信息，请参阅[**call**()](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltProcessor.__call__)和`decode()`的文档字符串。'
- en: '#### `__call__`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/processing_vilt.py#L64)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/processing_vilt.py#L64)'
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This method uses [ViltImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    method to prepare image(s) for the model, and [BertTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    to prepare text for the model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法使用[ViltImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)方法准备模型的图像，并使用[BertTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)准备模型的文本。
- en: Please refer to the docstring of the above two methods for more information.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅上述两种方法的文档字符串。
- en: ViltModel
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltModel
- en: '### `class transformers.ViltModel`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L711)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L711)'
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare ViLT Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 裸 ViLT 模型变压器输出原始隐藏状态，没有特定的头部。此模型是 PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L743)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L743)'
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `({0})`) — 词汇表中输入序列标记的索引。可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在
    `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被“遮蔽”的标记，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被“遮蔽”的标记。[什么是注意力掩码？](../glossary#attention-mask)
- en: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — 段落标记索引，用于指示输入的第一部分和第二部分。索引在
    `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 标记。[什么是标记类型 ID？](../glossary#token-type-ids)
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取像素值。有关详细信息，请参阅 [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — 用于避免在填充像素值上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示真实的像素（即未被遮蔽），
- en: 0 for pixels that are padding (i.e. `masked`). `What are attention masks? <../glossary.html#attention-mask>`__
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示填充的像素（即“遮蔽”）。`什么是注意力掩码？<../glossary.html#attention-mask>`__
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被“遮蔽”。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是模型内部的嵌入查找矩阵，这将非常有用。'
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `pixel_values`。如果您想要更多控制如何将 `pixel_values`
    转换为补丁嵌入，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回的张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    and inputs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回通过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel)
    forward method, overrides the `__call__` special method.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViltModel](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ViltForMaskedLM
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltForMaskedLM
- en: '### `class transformers.ViltForMaskedLM`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltForMaskedLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L875)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L875)'
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: ViLT Model with a language modeling head on top as done during pretraining.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，ViLT模型在顶部带有一个语言建模头。
- en: This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch的`torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L899)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L899)'
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。可以使用
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入
    ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示标记是 `未掩码`，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示 `被掩码` 的标记。[什么是注意力掩码？](../glossary#attention-mask)
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 段标记索引，指示输入的第一部分和第二部分。索引选在 `[0, 1]`：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应一个 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应一个 *句子 B* 标记。[什么是标记类型 ID？](../glossary#token-type-ids)
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height,
    width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取像素值。有关详细信息，请参阅 [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask` (`torch.LongTensor`，形状为 `(batch_size, height, width)`，*可选*) —
    用于避免在填充像素值上执行注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示像素是真实的（即 `未掩码`），
- en: 0 for pixels that are padding (i.e. `masked`). `What are attention masks? <../glossary.html#attention-mask>`__
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示填充的像素（即 `被掩码`）。`什么是注意力掩码？ <../glossary.html#attention-mask>`__
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*)
    — 用于使自注意力模块的选定头部失效的掩码。掩码值选在 `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部是 `未掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是 `被掩码`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor`，形状为 `(batch_size, num_patches, hidden_size)`，*可选*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递 `pixel_values`。如果您想要更多控制权来将 `pixel_values` 转换为补丁嵌入，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: '`labels` (*torch.LongTensor* of shape *(batch_size, sequence_length)*, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    *[-100, 0, …, config.vocab_size]* (see *input_ids* docstring) Tokens with indices
    set to *-100* are ignored (masked), the loss is only computed for the tokens with
    labels in *[0, …, config.vocab_size]*'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（*torch.LongTensor*，形状为 *(batch_size, sequence_length)*，*可选*) — 用于计算掩码语言建模损失的标签。索引应在
    *[-100, 0, …, config.vocab_size]* 范围内（参见 *input_ids* 文档字符串）。索引设置为 *-100* 的标记将被忽略（被掩码），损失仅计算具有标签在
    *[0, …, config.vocab_size]* 范围内的标记。'
- en: Returns
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    and inputs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`
    时）包含根据配置（[ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 掩码语言建模（MLM）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层，则为嵌入输出 + 每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力 softmax 之后的注意力权重。
- en: The [ViltForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViltForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForMaskedLM)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ViltForQuestionAnswering
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltForQuestionAnswering
- en: '### `class transformers.ViltForQuestionAnswering`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1050)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1050)'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法加载模型权重。'
- en: Vilt Model transformer with a classifier head on top (a linear layer on top
    of the final hidden state of the [CLS] token) for visual question answering, e.g.
    for VQAv2.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Vilt 模型变压器，顶部带有分类器头（在 [CLS] 标记的最终隐藏状态之上的线性层），用于视觉问答，例如用于 VQAv2。
- en: This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是 PyTorch 的 `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1075)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1075)'
- en: '[PRE15]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `({0})`) — 词汇表中输入序列标记的索引。可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。查看 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    获取详细信息。[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为 `({0})`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在
    `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩码的标记为 1，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被掩码`的标记为0。[什么是注意力掩码？](../glossary#attention-mask)
- en: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`({0})`的`torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引选定在`[0,
    1]`之间：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*的标记，
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*的标记。[什么是标记类型ID？](../glossary#token-type-ids)
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免对填充像素值执行注意力的掩码。掩码值选定在`[0,
    1]`之间：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素为1表示是真实像素（即`未被掩码`），
- en: 0 for pixels that are padding (i.e. `masked`). `What are attention masks? <../glossary.html#attention-mask>`__
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于填充像素（即`masked`）的像素为0。`什么是注意力掩码？<../glossary.html#attention-mask>`__
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部`被掩码`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`({0}, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵。'
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds`（形状为`(batch_size, num_patches, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`pixel_values`。如果您想要更多控制如何将`pixel_values`转换为补丁嵌入，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。查看返回的张量中的`attentions`以获取更多细节。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。查看返回的张量中的`hidden_states`以获取更多细节。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`labels` (`torch.FloatTensor` of shape `(batch_size, num_labels)`, *optional*)
    — Labels for computing the visual question answering loss. This tensor must be
    either a one-hot encoding of all answers that are applicable for a given example
    in the batch, or a soft encoding indicating which answers are applicable, where
    1.0 is the highest score.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, num_labels)`的`torch.FloatTensor`，*可选*）— 用于计算视觉问答损失的标签。此张量必须是给定示例批次中适用的所有答案的独热编码，或者指示哪些答案适用的软编码，其中1.0是最高分。'
- en: Returns
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    and inputs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者当`config.return_dict=False`时），包括根据配置（[ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）— 分类（如果`config.num_labels==1`则为回归）分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ViltForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViltForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForQuestionAnswering)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ViltForImagesAndTextClassification
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltForImagesAndTextClassification
- en: '### `class transformers.ViltForImagesAndTextClassification`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.ViltForImagesAndTextClassification` 类'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1265)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1265)'
- en: '[PRE17]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `({0})`) — 词汇表中输入序列标记的索引。可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入
    ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为 `({0})`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在
    `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被掩码的标记，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被掩码的标记。[什么是注意力掩码？](../glossary#attention-mask)
- en: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为 `({0})`，*可选*) — 段标记索引，用于指示输入的第一部分和第二部分。索引在
    `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 的标记，
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 的标记。[什么是标记类型 ID？](../glossary#token-type-ids)
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_images, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_images, num_channels,
    height, width)`) — 像素值。像素值可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取。有关详细信息，请参阅 [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, num_images, height,
    width)`, *optional*) — Mask to avoid performing attention on padding pixel values.
    Mask values selected in `[0, 1]`:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask` (`torch.LongTensor`，形状为 `(batch_size, num_images, height, width)`，*可选*)
    — 用于避免在填充像素值上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示真实的像素（即 `未被掩码`），
- en: 0 for pixels that are padding (i.e. `masked`). `What are attention masks? <../glossary.html#attention-mask>`__
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示填充的像素（即 `被掩码`）。`什么是注意力掩码？ <../glossary.html#attention-mask>`__
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*)
    — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部是 `未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是 `被掩码`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_images, num_patches,
    hidden_size)`, *optional*) — Optionally, instead of passing `pixel_values`, you
    can choose to directly pass an embedded representation. This is useful if you
    want more control over how to convert `pixel_values` into patch embeddings.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_images, num_patches,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`pixel_values`。如果您想要更多控制如何将`pixel_values`转换为补丁嵌入，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Vilt Model transformer with a classifier head on top for natural language visual
    reasoning, e.g. NLVR2.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Vilt模型变压器，顶部带有自然语言视觉推理的分类器头部，例如NLVR2。
- en: '#### `forward`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1290)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1290)'
- en: '[PRE18]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `({0})`) — 词汇表中输入序列标记的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0,
    1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的标记，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`masked`的标记。[什么是注意力掩码？](../glossary#attention-mask)
- en: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在`[0,
    1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。[什么是标记类型ID？](../glossary#token-type-ids)
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — 用于避免在填充像素值上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示真实的像素（即`not masked`），
- en: 0 for pixels that are padding (i.e. `masked`). `What are attention masks? <../glossary.html#attention-mask>`__
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示填充像素（即`masked`）。`什么是注意力掩码？<../glossary.html#attention-mask>`__
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`pixel_values`。如果您想要更多控制如何将`pixel_values`转换为补丁嵌入，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Binary
    classification labels.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 二元分类标签。'
- en: Returns
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.vilt.modeling_vilt.ViltForImagesAndTextClassificationOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.vilt.modeling_vilt.ViltForImagesAndTextClassificationOutput`
    或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.vilt.modeling_vilt.ViltForImagesAndTextClassificationOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    and inputs.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.vilt.modeling_vilt.ViltForImagesAndTextClassificationOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    分类（如果`config.num_labels==1`则为回归）分数（SoftMax之前）。'
- en: '`hidden_states` (`List[tuple(torch.FloatTensor)]`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — List of tuples of `torch.FloatTensor` (one for each image-text pair, each tuple
    containing the output of the embeddings + one for the output of each layer) of
    shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model
    at the output of each layer plus the initial embedding outputs.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`List[tuple(torch.FloatTensor)]`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 一个元组列表，每个元组包含每个图像-文本对的`torch.FloatTensor`输出（每个元组包含嵌入的输出和每个层的输出），形状为`(batch_size,
    sequence_length, hidden_size)`。模型在每个层的输出处的隐藏状态加上初始嵌入输出。'
- en: '`attentions` (`List[tuple(torch.FloatTensor)]`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — List of tuples of `torch.FloatTensor`
    (one for each image-text pair, each tuple containing the attention weights of
    shape `(batch_size, num_heads, sequence_length, sequence_length)`. Attentions
    weights after the attention softmax, used to compute the weighted average in the
    self-attention heads.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`List[tuple(torch.FloatTensor)]`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 一个元组列表，每个元组包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的注意力权重的`torch.FloatTensor`（每个图像-文本对一个元组）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。'
- en: The [ViltForImagesAndTextClassification](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForImagesAndTextClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViltForImagesAndTextClassification](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForImagesAndTextClassification)前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ViltForImageAndTextRetrieval
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltForImageAndTextRetrieval
- en: '### `class transformers.ViltForImageAndTextRetrieval`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltForImageAndTextRetrieval`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1163)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1163)'
- en: '[PRE20]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Vilt Model transformer with a classifier head on top (a linear layer on top
    of the final hidden state of the [CLS] token) for image-to-text or text-to-image
    retrieval, e.g. MSCOCO and F30K.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Vilt模型变压器，顶部带有分类器头（在[CLS]标记的最终隐藏状态之上的线性层），用于图像到文本或文本到图像检索，例如MSCOCO和F30K。
- en: This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是一个 PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1182)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1182)'
- en: '[PRE21]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `({0})`) — 词汇表中输入序列标记的索引。可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入
    ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在
    `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示 `未掩码` 的标记，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `掩码` 的标记。[什么是注意力掩码？](../glossary#attention-mask)
- en: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引在
    `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 标记。[什么是标记类型 ID？](../glossary#token-type-ids)
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取像素值。有关详细信息，请参阅 [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — 用于避免在填充像素值上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示真实的像素（即 `未掩码`），
- en: 0 for pixels that are padding (i.e. `masked`). `What are attention masks? <../glossary.html#attention-mask>`__
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示填充的像素（即 `掩码`）。`什么是注意力掩码？ <../glossary.html#attention-mask>`__
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部 `未掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部 `掩码`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为与模型的内部嵌入查找矩阵相关的向量，则这是有用的。'
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `pixel_values`。如果您想要更多控制如何将 `pixel_values`
    转换为补丁嵌入，则这是有用的。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    are currently not supported.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 目前不支持标签。'
- en: Returns
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    and inputs.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）- 分类（如果`config.num_labels==1`则为回归）得分（在SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力权重在注意力softmax之后。
- en: The [ViltForImageAndTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForImageAndTextRetrieval)
    forward method, overrides the `__call__` special method.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViltForImageAndTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForImageAndTextRetrieval)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ViltForTokenClassification
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViltForTokenClassification
- en: '### `class transformers.ViltForTokenClassification`'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ViltForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1406)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1406)'
- en: '[PRE23]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig)）-
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: ViLT Model with a token classification head on top (a linear layer on top of
    the final hidden-states of the text tokens) e.g. for Named-Entity-Recognition
    (NER) tasks.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ViLT模型在顶部有一个标记分类头（文本标记的最终隐藏状态上的线性层），例如用于命名实体识别（NER）任务。
- en: This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch的`torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1426)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vilt/modeling_vilt.py#L1426)'
- en: '[PRE24]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`({0})`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`({0})`的`torch.FloatTensor`，*可选*）—用于避免在填充标记索引上执行注意力的遮罩。选择的遮罩值在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被遮罩`的标记，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被遮罩`的标记。[什么是注意力遮罩？](../glossary#attention-mask)
- en: '`token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`({0})`的`torch.LongTensor`，*可选*）—段标记索引，指示输入的第一部分和第二部分。索引在`[0,
    1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。[什么是标记类型ID？](../glossary#token-type-ids)
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[ViltImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）—用于避免在填充像素值上执行注意力的遮罩。选择的遮罩值在`[0,
    1]`中：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示真实的像素（即`未被遮罩`），
- en: 0 for pixels that are padding (i.e. `masked`). `What are attention masks? <../glossary.html#attention-mask>`__
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示填充的像素（即`被遮罩`）。`什么是注意力遮罩？<../glossary.html#attention-mask>`__
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—用于使自注意力模块的选定头部失效的遮罩。选择的遮罩值在`[0,
    1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被遮罩`，
- en: 0 indicates the head is `masked`.
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部`被遮罩`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`({0}, hidden_size)`的`torch.FloatTensor`，*可选*）—可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds`（形状为`(batch_size, num_patches, hidden_size)`的`torch.FloatTensor`，*可选*）—可选地，可以直接传递嵌入表示，而不是传递`pixel_values`。如果您想要更多控制如何将`pixel_values`转换为补丁嵌入，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）—是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）—是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）—是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, text_sequence_length)`,
    *optional*) — Labels for computing the token classification loss. Indices should
    be in `[0, ..., config.num_labels - 1]`.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, text_sequence_length)`的`torch.LongTensor`，*可选*）—用于计算标记分类损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`中。'
- en: Returns
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或者`tuple(torch.FloatTensor)`，
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig))
    and inputs.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或者一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或者`config.return_dict=False`时）包括各种元素，取决于配置（[ViltConfig](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）—分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）—分类分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力 softmax 后的注意力权重。
- en: The [ViltForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[ViltForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/vilt#transformers.ViltForTokenClassification)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数中定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。
