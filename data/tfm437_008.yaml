- en: Preprocess
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing](https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing](https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can train a model on a dataset, it needs to be preprocessed into
    the expected model input format. Whether your data is text, images, or audio,
    they need to be converted and assembled into batches of tensors. 🤗 Transformers
    provides a set of preprocessing classes to help prepare your data for the model.
    In this tutorial, you’ll learn that for:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可以在数据集上训练模型之前，需要将其预处理为预期的模型输入格式。无论您的数据是文本、图像还是音频，都需要将其转换并组装成张量批次。🤗 Transformers提供了一组预处理类来帮助准备数据供模型使用。在本教程中，您将了解到：
- en: Text, use a [Tokenizer](./main_classes/tokenizer) to convert text into a sequence
    of tokens, create a numerical representation of the tokens, and assemble them
    into tensors.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本，使用[Tokenizer](./main_classes/tokenizer)将文本转换为一系列标记，创建标记的数值表示，并将它们组装成张量。
- en: Speech and audio, use a [Feature extractor](./main_classes/feature_extractor)
    to extract sequential features from audio waveforms and convert them into tensors.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音和音频，使用[Feature extractor](./main_classes/feature_extractor)从音频波形中提取序列特征并将其转换为张量。
- en: Image inputs use a [ImageProcessor](./main_classes/image) to convert images
    into tensors.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像输入使用[ImageProcessor](./main_classes/image)将图像转换为张量。
- en: Multimodal inputs, use a [Processor](./main_classes/processors) to combine a
    tokenizer and a feature extractor or image processor.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态输入，使用[Processor](./main_classes/processors)来结合一个分词器和一个特征提取器或图像处理器。
- en: '`AutoProcessor` **always** works and automatically chooses the correct class
    for the model you’re using, whether you’re using a tokenizer, image processor,
    feature extractor or processor.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`AutoProcessor` **总是**有效，并自动选择您正在使用的模型的正确类别，无论您是使用分词器、图像处理器、特征提取器还是处理器。'
- en: 'Before you begin, install 🤗 Datasets so you can load some datasets to experiment
    with:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请安装🤗数据集，以便加载一些数据集进行实验：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Natural Language Processing
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '[https://www.youtube-nocookie.com/embed/Yffk5aydLzg](https://www.youtube-nocookie.com/embed/Yffk5aydLzg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/Yffk5aydLzg](https://www.youtube-nocookie.com/embed/Yffk5aydLzg)'
- en: The main tool for preprocessing textual data is a [tokenizer](main_classes/tokenizer).
    A tokenizer splits text into *tokens* according to a set of rules. The tokens
    are converted into numbers and then tensors, which become the model inputs. Any
    additional inputs required by the model are added by the tokenizer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理文本数据的主要工具是[tokenizer](main_classes/tokenizer)。分词器根据一组规则将文本分割为*标记*。这些标记被转换为数字，然后成为模型输入的张量。分词器会添加模型所需的任何额外输入。
- en: If you plan on using a pretrained model, it’s important to use the associated
    pretrained tokenizer. This ensures the text is split the same way as the pretraining
    corpus, and uses the same corresponding tokens-to-index (usually referred to as
    the *vocab*) during pretraining.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打算使用预训练模型，重要的是使用相关的预训练分词器。这确保文本被分割的方式与预训练语料库相同，并且在预训练期间使用相同的对应标记索引（通常称为*词汇表*）。
- en: 'Get started by loading a pretrained tokenizer with the [AutoTokenizer.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)
    method. This downloads the *vocab* a model was pretrained with:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[AutoTokenizer.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)方法加载预训练的分词器来开始。这会下载模型预训练时使用的*词汇表*：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then pass your text to the tokenizer:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将您的文本传递给分词器：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The tokenizer returns a dictionary with three important items:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器返回一个包含三个重要项目的字典：
- en: '[input_ids](glossary#input-ids) are the indices corresponding to each token
    in the sentence.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[input_ids](glossary#input-ids)是句子中每个标记对应的索引。'
- en: '[attention_mask](glossary#attention-mask) indicates whether a token should
    be attended to or not.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[attention_mask](glossary#attention-mask)指示一个标记是否应该被关注。'
- en: '[token_type_ids](glossary#token-type-ids) identifies which sequence a token
    belongs to when there is more than one sequence.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[token_type_ids](glossary#token-type-ids)标识一个标记属于哪个序列，当有多个序列时。'
- en: 'Return your input by decoding the `input_ids`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解码`input_ids`返回您的输入：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier
    and separator) - to the sentence. Not all models need special tokens, but if they
    do, the tokenizer automatically adds them for you.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，分词器添加了两个特殊标记 - `CLS`和`SEP`（分类器和分隔符）- 到句子中。并非所有模型都需要特殊标记，但如果需要，分词器会自动为您添加它们。
- en: 'If there are several sentences you want to preprocess, pass them as a list
    to the tokenizer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有几个句子需要预处理，将它们作为列表传递给分词器：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pad
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充
- en: Sentences aren’t always the same length which can be an issue because tensors,
    the model inputs, need to have a uniform shape. Padding is a strategy for ensuring
    tensors are rectangular by adding a special *padding token* to shorter sentences.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 句子长度不总是相同，这可能是一个问题，因为张量，即模型输入，需要具有统一的形状。填充是一种确保张量是矩形的策略，通过向较短的句子添加一个特殊的*填充标记*。
- en: 'Set the `padding` parameter to `True` to pad the shorter sequences in the batch
    to match the longest sequence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将`padding`参数设置为`True`，以将批次中较短的序列填充到与最长序列相匹配的长度：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first and third sentences are now padded with `0`’s because they are shorter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一句和第三句现在用`0`填充，因为它们较短。
- en: Truncation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 截断
- en: On the other end of the spectrum, sometimes a sequence may be too long for a
    model to handle. In this case, you’ll need to truncate the sequence to a shorter
    length.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有时一个序列可能太长，模型无法处理。在这种情况下，您需要将序列截断为较短的长度。
- en: 'Set the `truncation` parameter to `True` to truncate a sequence to the maximum
    length accepted by the model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将`truncation`参数设置为`True`，将序列截断为模型接受的最大长度：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Check out the [Padding and truncation](./pad_truncation) concept guide to learn
    more different padding and truncation arguments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[填充和截断](./pad_truncation)概念指南，了解更多不同的填充和截断参数。
- en: Build tensors
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建张量
- en: Finally, you want the tokenizer to return the actual tensors that get fed to
    the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您希望分词器返回实际馈送到模型的张量。
- en: 'Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for
    TensorFlow:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将`return_tensors`参数设置为`pt`以供PyTorch使用，或设置为`tf`以供TensorFlow使用：
- en: PytorchHide Pytorch content
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Different pipelines support tokenizer arguments in their `__call__()` differently.
    `text-2-text-generation` pipelines support (i.e. pass on) only `truncation`. `text-generation`
    pipelines support `max_length`, `truncation`, `padding` and `add_special_tokens`.
    In `fill-mask` pipelines, tokenizer arguments can be passed in the `tokenizer_kwargs`
    argument (dictionary).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的管道以不同的方式在其`__call__()`中支持分词器参数。`text-2-text-generation`管道仅支持（即传递）`truncation`。`text-generation`管道支持`max_length`、`truncation`、`padding`和`add_special_tokens`。在`fill-mask`管道中，分词器参数可以在`tokenizer_kwargs`参数（字典）中传递。
- en: Audio
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音频
- en: For audio tasks, you’ll need a [feature extractor](main_classes/feature_extractor)
    to prepare your dataset for the model. The feature extractor is designed to extract
    features from raw audio data, and convert them into tensors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音频任务，您将需要一个[特征提取器](main_classes/feature_extractor)来准备您的数据集以供模型使用。特征提取器旨在从原始音频数据中提取特征，并将其转换为张量。
- en: 'Load the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset
    (see the 🤗 [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub)
    for more details on how to load a dataset) to see how you can use a feature extractor
    with audio datasets:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 加载[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)数据集（查看🤗[Datasets教程](https://huggingface.co/docs/datasets/load_hub)以获取有关如何加载数据集的更多详细信息）以查看如何在音频数据集中使用特征提取器：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Access the first element of the `audio` column to take a look at the input.
    Calling the `audio` column automatically loads and resamples the audio file:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 访问`audio`列的第一个元素以查看输入。调用`audio`列会自动加载和重新采样音频文件：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This returns three items:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回三个项目：
- en: '`array` is the speech signal loaded - and potentially resampled - as a 1D array.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`array`是加载的语音信号 - 可能已重新采样 - 作为1D数组。'
- en: '`path` points to the location of the audio file.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`指向音频文件的位置。'
- en: '`sampling_rate` refers to how many data points in the speech signal are measured
    per second.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`指的是每秒测量的语音信号中有多少数据点。'
- en: For this tutorial, you’ll use the [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)
    model. Take a look at the model card, and you’ll learn Wav2Vec2 is pretrained
    on 16kHz sampled speech audio. It is important your audio data’s sampling rate
    matches the sampling rate of the dataset used to pretrain the model. If your data’s
    sampling rate isn’t the same, then you need to resample your data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将使用[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)模型。查看模型卡片，您将了解到Wav2Vec2是在16kHz采样的语音音频上进行预训练的。重要的是，您的音频数据的采样率要与用于预训练模型的数据集的采样率匹配。如果您的数据采样率不同，则需要对数据进行重新采样。
- en: 'Use 🤗 Datasets’ [cast_column](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.cast_column)
    method to upsample the sampling rate to 16kHz:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用🤗 Datasets的[cast_column](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.cast_column)方法将采样率上采样至16kHz：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Call the `audio` column again to resample the audio file:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次调用`audio`列以重新采样音频文件：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, load a feature extractor to normalize and pad the input. When padding
    textual data, a `0` is added for shorter sequences. The same idea applies to audio
    data. The feature extractor adds a `0` - interpreted as silence - to `array`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载一个特征提取器来对输入进行归一化和填充。在填充文本数据时，会为较短的序列添加`0`。相同的思想也适用于音频数据。特征提取器会向`array`中添加一个`0`
    - 被解释为静音。
- en: 'Load the feature extractor with [AutoFeatureExtractor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoFeatureExtractor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained)加载特征提取器：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Pass the audio `array` to the feature extractor. We also recommend adding the
    `sampling_rate` argument in the feature extractor in order to better debug any
    silent errors that may occur.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将音频`array`传递给特征提取器。我们还建议在特征提取器中添加`sampling_rate`参数，以更好地调试可能发生的任何静默错误。
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Just like the tokenizer, you can apply padding or truncation to handle variable
    sequences in a batch. Take a look at the sequence length of these two audio samples:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与分词器一样，您可以应用填充或截断来处理批处理中的可变序列。查看这两个音频样本的序列长度：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create a function to preprocess the dataset so the audio samples are the same
    lengths. Specify a maximum sample length, and the feature extractor will either
    pad or truncate the sequences to match it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个函数来预处理数据集，使音频样本具有相同的长度。指定最大样本长度，特征提取器将填充或截断序列以匹配它：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Apply the `preprocess_function` to the first few examples in the dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据集中的前几个示例应用`preprocess_function`：
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The sample lengths are now the same and match the specified maximum length.
    You can pass your processed dataset to the model now!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在样本长度相同并与指定的最大长度匹配。现在可以将处理过的数据集传递给模型了！
- en: '[PRE18]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Computer vision
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: For computer vision tasks, you’ll need an [image processor](main_classes/image_processor)
    to prepare your dataset for the model. Image preprocessing consists of several
    steps that convert images into the input expected by the model. These steps include
    but are not limited to resizing, normalizing, color channel correction, and converting
    images to tensors.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算机视觉任务，您将需要一个[图像处理器](main_classes/image_processor)来准备您的数据集以供模型使用。图像预处理包括几个步骤，将图像转换为模型期望的输入。这些步骤包括但不限于调整大小、归一化、颜色通道校正以及将图像转换为张量。
- en: 'Image preprocessing often follows some form of image augmentation. Both image
    preprocessing and image augmentation transform image data, but they serve different
    purposes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图像预处理通常遵循某种形式的图像增强。图像预处理和图像增强都会转换图像数据，但它们有不同的目的：
- en: Image augmentation alters images in a way that can help prevent overfitting
    and increase the robustness of the model. You can get creative in how you augment
    your data - adjust brightness and colors, crop, rotate, resize, zoom, etc. However,
    be mindful not to change the meaning of the images with your augmentations.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像增强以一种可以帮助防止过拟合并增加模型鲁棒性的方式改变图像。您可以在数据增强中发挥创造力 - 调整亮度和颜色，裁剪，旋转，调整大小，缩放等。但是，请注意不要通过增强改变图像的含义。
- en: Image preprocessing guarantees that the images match the model’s expected input
    format. When fine-tuning a computer vision model, images must be preprocessed
    exactly as when the model was initially trained.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像预处理确保图像与模型期望的输入格式匹配。在微调计算机视觉模型时，图像必须与模型最初训练时的预处理方式完全相同。
- en: You can use any library you like for image augmentation. For image preprocessing,
    use the `ImageProcessor` associated with the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用任何您喜欢的库进行图像增强。对于图像预处理，请使用与模型关联的`ImageProcessor`。
- en: 'Load the [food101](https://huggingface.co/datasets/food101) dataset (see the
    🤗 [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub) for more
    details on how to load a dataset) to see how you can use an image processor with
    computer vision datasets:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 加载[food101](https://huggingface.co/datasets/food101)数据集（请参阅🤗[数据集教程](https://huggingface.co/docs/datasets/load_hub)以获取有关如何加载数据集的更多详细信息），以查看如何在计算机视觉数据集中使用图像处理器：
- en: Use 🤗 Datasets `split` parameter to only load a small sample from the training
    split since the dataset is quite large!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用🤗数据集`split`参数仅加载训练集中的一小部分样本，因为数据集非常大！
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, take a look at the image with 🤗 Datasets [`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)
    feature:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看一下带有🤗数据集[`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)特征的图像：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/63a998c1115de762652ef3c59b938a3f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63a998c1115de762652ef3c59b938a3f.png)'
- en: 'Load the image processor with [AutoImageProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoImageProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained)加载图像处理器：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: First, let’s add some image augmentation. You can use any library you prefer,
    but in this tutorial, we’ll use torchvision’s [`transforms`](https://pytorch.org/vision/stable/transforms.html)
    module. If you’re interested in using another data augmentation library, learn
    how in the [Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)
    or [Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们添加一些图像增强。您可以使用任何您喜欢的库，但在本教程中，我们将使用torchvision的[`transforms`](https://pytorch.org/vision/stable/transforms.html)模块。如果您有兴趣使用其他数据增强库，请在[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)或[Kornia
    notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)中学习如何使用。
- en: Here we use [`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)
    to chain together a couple of transforms - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)
    and [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html).
    Note that for resizing, we can get the image size requirements from the `image_processor`.
    For some models, an exact height and width are expected, for others only the `shortest_edge`
    is defined.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们使用[`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)来链接一些转换
    - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)和[`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)。请注意，对于调整大小，我们可以从`image_processor`获取图像大小要求。对于某些模型，期望精确的高度和宽度，对于其他模型只定义了`shortest_edge`。
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The model accepts [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)
    as its input. `ImageProcessor` can take care of normalizing the images, and generating
    appropriate tensors. Create a function that combines image augmentation and image
    preprocessing for a batch of images and generates `pixel_values`:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型接受[`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)作为其输入。`ImageProcessor`可以负责归一化图像，并生成适当的张量。创建一个函数，将图像增强和图像预处理组合为一批图像，并生成`pixel_values`：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the example above we set `do_resize=False` because we have already resized
    the images in the image augmentation transformation, and leveraged the `size`
    attribute from the appropriate `image_processor`. If you do not resize images
    during image augmentation, leave this parameter out. By default, `ImageProcessor`
    will handle the resizing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们设置了`do_resize=False`，因为我们已经在图像增强转换中调整了图像的大小，并利用了适当的`image_processor`的`size`属性。如果您在图像增强期间不调整图像大小，请省略此参数。默认情况下，`ImageProcessor`将处理调整大小。
- en: If you wish to normalize images as a part of the augmentation transformation,
    use the `image_processor.image_mean`, and `image_processor.image_std` values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果希望将图像归一化作为增强转换的一部分，请使用`image_processor.image_mean`和`image_processor.image_std`值。
- en: 'Then use 🤗 Datasets[set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    to apply the transforms on the fly:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用🤗数据集[set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)来动态应用转换：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now when you access the image, you’ll notice the image processor has added `pixel_values`.
    You can pass your processed dataset to the model now!
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在当您访问图像时，您会注意到图像处理器已添加了`pixel_values`。现在您可以将处理过的数据集传递给模型了！
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here is what the image looks like after the transforms are applied. The image
    has been randomly cropped and it’s color properties are different.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用转换后的图像样子。图像已被随机裁剪，其颜色属性不同。
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/eee368b92fc814b0667081ba147249f4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eee368b92fc814b0667081ba147249f4.png)'
- en: For tasks like object detection, semantic segmentation, instance segmentation,
    and panoptic segmentation, `ImageProcessor` offers post processing methods. These
    methods convert model’s raw outputs into meaningful predictions such as bounding
    boxes, or segmentation maps.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标检测、语义分割、实例分割和全景分割等任务，`ImageProcessor`提供后处理方法。这些方法将模型的原始输出转换为有意义的预测，如边界框或分割地图。
- en: Pad
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充
- en: In some cases, for instance, when fine-tuning [DETR](./model_doc/detr), the
    model applies scale augmentation at training time. This may cause images to be
    different sizes in a batch. You can use `DetrImageProcessor.pad()` from [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    and define a custom `collate_fn` to batch images together.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，例如在微调[DETR](./model_doc/detr)时，模型会在训练时应用尺度增强。这可能导致批处理中的图像大小不同。您可以使用来自[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)的`DetrImageProcessor.pad()`，并定义一个自定义的`collate_fn`来将图像批处理在一起。
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Multimodal
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态
- en: For tasks involving multimodal inputs, you’ll need a [processor](main_classes/processors)
    to prepare your dataset for the model. A processor couples together two processing
    objects such as as tokenizer and feature extractor.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及多模态输入的任务，您将需要一个[处理器](主要类/处理器)来为模型准备您的数据集。处理器将两个处理对象（如标记器和特征提取器）耦合在一起。
- en: 'Load the [LJ Speech](https://huggingface.co/datasets/lj_speech) dataset (see
    the 🤗 [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub) for more
    details on how to load a dataset) to see how you can use a processor for automatic
    speech recognition (ASR):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 加载[LJ Speech](https://huggingface.co/datasets/lj_speech)数据集（查看🤗[数据集教程](https://huggingface.co/docs/datasets/load_hub)以获取有关如何加载数据集的更多详细信息），以查看如何使用处理器进行自动语音识别（ASR）：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For ASR, you’re mainly focused on `audio` and `text` so you can remove the
    other columns:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ASR，您主要关注`音频`和`文本`，因此可以删除其他列：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now take a look at the `audio` and `text` columns:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看一下`音频`和`文本`列：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Remember you should always [resample](preprocessing#audio) your audio dataset’s
    sampling rate to match the sampling rate of the dataset used to pretrain a model!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你应该始终[重新采样](预处理#音频)你的音频数据集的采样率，以匹配用于预训练模型的数据集的采样率！
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Load a processor with [AutoProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained)加载一个处理器：
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create a function to process the audio data contained in `array` to `input_values`,
    and tokenize `text` to `labels`. These are the inputs to the model:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来处理`array`中包含的音频数据为`input_values`，并将`文本`标记化为`标签`。这些是模型的输入：
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Apply the `prepare_dataset` function to a sample:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`prepare_dataset`函数应用到一个样本中：
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The processor has now added `input_values` and `labels`, and the sampling rate
    has also been correctly downsampled to 16kHz. You can pass your processed dataset
    to the model now!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器现在已经添加了`input_values`和`labels`，采样率也已经正确降采样到16kHz。现在您可以将处理过的数据集传递给模型了！
