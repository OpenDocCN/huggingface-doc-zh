- en: Training on TPUs with ğŸ¤— Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨TPUä¸Šä½¿ç”¨ğŸ¤— Accelerateè¿›è¡Œè®­ç»ƒ
- en: 'Original text: [https://huggingface.co/docs/accelerate/concept_guides/training_tpu](https://huggingface.co/docs/accelerate/concept_guides/training_tpu)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/concept_guides/training_tpu](https://huggingface.co/docs/accelerate/concept_guides/training_tpu)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Training on TPUs can be slightly different from training on multi-gpu, even
    with ğŸ¤— Accelerate. This guide aims to show you where you should be careful and
    why, as well as the best practices in general.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨TPUä¸Šè®­ç»ƒå¯èƒ½ä¸åœ¨å¤šGPUä¸Šè®­ç»ƒç•¥æœ‰ä¸åŒï¼Œå³ä½¿ä½¿ç”¨ğŸ¤— Accelerateã€‚æœ¬æŒ‡å—æ—¨åœ¨å‘æ‚¨å±•ç¤ºåº”è¯¥æ³¨æ„çš„åœ°æ–¹ä»¥åŠåŸå› ï¼Œä»¥åŠä¸€èˆ¬çš„æœ€ä½³å®è·µã€‚
- en: Training in a Notebook
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ç¬”è®°æœ¬ä¸­è®­ç»ƒ
- en: The main carepoint when training on TPUs comes from the [notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher).
    As mentioned in the [notebook tutorial](../usage_guides/notebook), you need to
    restructure your training code into a function that can get passed to the [notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)
    function and be careful about not declaring any tensors on the GPU.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨TPUä¸Šè®­ç»ƒæ—¶çš„ä¸»è¦å…³æ³¨ç‚¹æ¥è‡ª[notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)ã€‚å¦‚åœ¨[notebookæ•™ç¨‹](../usage_guides/notebook)ä¸­æåˆ°çš„ï¼Œæ‚¨éœ€è¦å°†è®­ç»ƒä»£ç é‡æ„ä¸ºä¸€ä¸ªå¯ä»¥ä¼ é€’ç»™[notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)å‡½æ•°çš„å‡½æ•°ï¼Œå¹¶æ³¨æ„ä¸è¦åœ¨GPUä¸Šå£°æ˜ä»»ä½•å¼ é‡ã€‚
- en: While on a TPU that last part is not as important, a critical part to understand
    is that when you launch code from a notebook you do so through a process called
    **forking**. When launching from the command-line, you perform **spawning**, where
    a python process is not currently running and you *spawn* a new process in. Since
    your Jupyter notebook is already utilizing a python process, you need to *fork*
    a new process from it to launch your code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨TPUä¸Šï¼Œæœ€åä¸€éƒ¨åˆ†å¹¶ä¸é‚£ä¹ˆé‡è¦ï¼Œéœ€è¦ç†è§£çš„ä¸€ä¸ªå…³é”®éƒ¨åˆ†æ˜¯ï¼Œå½“æ‚¨ä»ç¬”è®°æœ¬å¯åŠ¨ä»£ç æ—¶ï¼Œæ‚¨æ˜¯é€šè¿‡ä¸€ç§ç§°ä¸º**forking**çš„è¿‡ç¨‹è¿›è¡Œçš„ã€‚å½“ä»å‘½ä»¤è¡Œå¯åŠ¨æ—¶ï¼Œæ‚¨æ‰§è¡Œ**spawning**ï¼Œå…¶ä¸­pythonè¿›ç¨‹å½“å‰æœªè¿è¡Œï¼Œæ‚¨*ç”Ÿæˆ*ä¸€ä¸ªæ–°è¿›ç¨‹ã€‚ç”±äºæ‚¨çš„Jupyterç¬”è®°æœ¬å·²ç»åœ¨ä½¿ç”¨pythonè¿›ç¨‹ï¼Œå› æ­¤æ‚¨éœ€è¦ä»ä¸­*fork*ä¸€ä¸ªæ–°è¿›ç¨‹æ¥å¯åŠ¨æ‚¨çš„ä»£ç ã€‚
- en: Where this becomes important is in regard to declaring your model. On forked
    TPU processes, it is recommended that you instantiate your model *once* and pass
    this into your training function. This is different than training on GPUs where
    you create `n` models that have their gradients synced and back-propagated at
    certain moments. Instead, one model instance is shared between all the nodes and
    it is passed back and forth. This is important especially when training on low-resource
    TPUs such as those provided in Kaggle kernels or on Google Colaboratory.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç‚¹çš„é‡è¦æ€§åœ¨äºå£°æ˜æ‚¨çš„æ¨¡å‹ã€‚åœ¨åˆ†å‰çš„TPUè¿›ç¨‹ä¸­ï¼Œå»ºè®®æ‚¨å®ä¾‹åŒ–æ‚¨çš„æ¨¡å‹*ä¸€æ¬¡*ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™æ‚¨çš„è®­ç»ƒå‡½æ•°ã€‚è¿™ä¸åœ¨GPUä¸Šè®­ç»ƒæ—¶åˆ›å»º`n`ä¸ªåœ¨æŸäº›æ—¶åˆ»åŒæ­¥æ¢¯åº¦å¹¶åå‘ä¼ æ’­çš„æ¨¡å‹ä¸åŒã€‚ç›¸åï¼Œä¸€ä¸ªæ¨¡å‹å®ä¾‹åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¹‹é—´å…±äº«ï¼Œå¹¶æ¥å›ä¼ é€’ã€‚è¿™åœ¨è®­ç»ƒä½èµ„æºTPUæ—¶å°¤ä¸ºé‡è¦ï¼Œä¾‹å¦‚åœ¨Kaggleå†…æ ¸æˆ–Google
    Colaboratoryä¸­æä¾›çš„TPUä¸Šã€‚
- en: 'Below is an example of a training function passed to the [notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)
    if training on CPUs or GPUs:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¼ é€’ç»™[notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)çš„è®­ç»ƒå‡½æ•°ç¤ºä¾‹ï¼Œå¦‚æœåœ¨CPUæˆ–GPUä¸Šè®­ç»ƒï¼š
- en: This code snippet is based off the one from the `simple_nlp_example` notebook
    found [here](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb)
    with slight modifications for the sake of simplicity
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ä»£ç ç‰‡æ®µåŸºäº`simple_nlp_example`ç¬”è®°æœ¬ä¸­çš„ä»£ç ï¼Œç¨ä½œä¿®æ”¹ä»¥ç®€åŒ–
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `notebook_launcher` will default to 8 processes if ğŸ¤— Accelerate has been
    configured for a TPU
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœğŸ¤— Accelerateå·²é…ç½®ä¸ºTPUï¼Œåˆ™`notebook_launcher`å°†é»˜è®¤ä¸º8ä¸ªè¿›ç¨‹
- en: 'If you use this example and declare the model *inside* the training loop, then
    on a low-resource system you will potentially see an error like:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨æ­¤ç¤ºä¾‹å¹¶åœ¨å…¶ä¸­å£°æ˜æ¨¡å‹ï¼Œåˆ™åœ¨ä½èµ„æºç³»ç»Ÿä¸Šå¯èƒ½ä¼šçœ‹åˆ°é”™è¯¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This error is *extremely* cryptic but the basic explanation is you ran out
    of system RAM. You can avoid this entirely by reconfiguring the training function
    to accept a single `model` argument, and declare it in an outside cell:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé”™è¯¯*æå…¶*éš¾ä»¥ç†è§£ï¼Œä½†åŸºæœ¬è§£é‡Šæ˜¯æ‚¨çš„ç³»ç»ŸRAMç”¨å®Œäº†ã€‚æ‚¨å¯ä»¥é€šè¿‡é‡æ–°é…ç½®è®­ç»ƒå‡½æ•°ä»¥æ¥å—å•ä¸ª`model`å‚æ•°å¹¶åœ¨å¤–éƒ¨å•å…ƒæ ¼ä¸­å£°æ˜å®ƒæ¥å®Œå…¨é¿å…è¿™ç§æƒ…å†µï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And finally calling the training function with:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åè°ƒç”¨è®­ç»ƒå‡½æ•°ï¼š
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The above workaround is only needed when launching a TPU instance from a Jupyter
    Notebook on a low-resource server such as Google Colaboratory or Kaggle. If using
    a script or launching on a much beefier server declaring the model beforehand
    is not needed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°è§£å†³æ–¹æ³•ä»…åœ¨ä»Jupyter Notebookå¯åŠ¨TPUå®ä¾‹æ—¶éœ€è¦ï¼Œä¾‹å¦‚åœ¨Google Colaboratoryæˆ–Kaggleç­‰ä½èµ„æºæœåŠ¡å™¨ä¸Šã€‚å¦‚æœä½¿ç”¨è„šæœ¬æˆ–åœ¨æ›´å¼ºå¤§çš„æœåŠ¡å™¨ä¸Šå¯åŠ¨ï¼Œåˆ™ä¸éœ€è¦äº‹å…ˆå£°æ˜æ¨¡å‹ã€‚
- en: Mixed Precision and Global Variables
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦å’Œå…¨å±€å˜é‡
- en: As mentioned in the [mixed precision tutorial](../usage_guides/mixed_precision),
    ğŸ¤— Accelerate supports fp16 and bf16, both of which can be used on TPUs. That being
    said, ideally `bf16` should be utilized as it is extremely efficient to use.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚åœ¨[æ··åˆç²¾åº¦æ•™ç¨‹](../usage_guides/mixed_precision)ä¸­æåˆ°çš„ï¼ŒğŸ¤— Accelerateæ”¯æŒfp16å’Œbf16ï¼Œä¸¤è€…éƒ½å¯ä»¥åœ¨TPUä¸Šä½¿ç”¨ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç†æƒ³æƒ…å†µä¸‹åº”è¯¥ä½¿ç”¨`bf16`ï¼Œå› ä¸ºå®ƒéå¸¸é«˜æ•ˆã€‚
- en: There are two â€œlayersâ€ when using `bf16` and ğŸ¤— Accelerate on TPUs, at the base
    level and at the operation level.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨TPUä¸Šä½¿ç”¨`bf16`å’ŒğŸ¤— Accelerateæ—¶ï¼Œæœ‰ä¸¤ä¸ªâ€œå±‚â€ï¼Œå³åŸºæœ¬çº§åˆ«å’Œæ“ä½œçº§åˆ«ã€‚
- en: 'At the base level, this is enabled when passing `mixed_precision="bf16"` to
    `Accelerator`, such as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºæœ¬çº§åˆ«ä¸Šï¼Œå½“å°†`mixed_precision="bf16"`ä¼ é€’ç»™`Accelerator`æ—¶ï¼Œè¿™æ˜¯å¯ç”¨çš„ï¼Œä¾‹å¦‚ï¼š
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By default, this will cast `torch.float` and `torch.double` to `bfloat16` on
    TPUs. The specific configuration being set is an environmental variable of `XLA_USE_BF16`
    is set to `1`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨TPUä¸Šå°†`torch.float`å’Œ`torch.double`è½¬æ¢ä¸º`bfloat16`ã€‚è®¾ç½®çš„å…·ä½“é…ç½®æ˜¯å°†ç¯å¢ƒå˜é‡`XLA_USE_BF16`è®¾ç½®ä¸º`1`ã€‚
- en: There is a further configuration you can perform which is setting the `XLA_DOWNCAST_BF16`
    environmental variable. If set to `1`, then `torch.float` is `bfloat16` and `torch.double`
    is `float32`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æ‰§è¡Œè¿›ä¸€æ­¥çš„é…ç½®ï¼Œå³è®¾ç½®`XLA_DOWNCAST_BF16`ç¯å¢ƒå˜é‡ã€‚å¦‚æœè®¾ç½®ä¸º`1`ï¼Œé‚£ä¹ˆ`torch.float`æ˜¯`bfloat16`ï¼Œ`torch.double`æ˜¯`float32`ã€‚
- en: 'This is performed in the `Accelerator` object when passing `downcast_bf16=True`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åœ¨ä¼ é€’`downcast_bf16=True`æ—¶åœ¨`Accelerator`å¯¹è±¡ä¸­æ‰§è¡Œçš„ã€‚
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using downcasting instead of bf16 everywhere is good for when you are trying
    to calculate metrics, log values, and more where raw bf16 tensors would be unusable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—æŒ‡æ ‡ã€è®°å½•å€¼ç­‰éœ€è¦ä½¿ç”¨åŸå§‹bf16å¼ é‡æ—¶ï¼Œä½¿ç”¨downcastingè€Œä¸æ˜¯bf16æ˜¯å¾ˆå¥½çš„é€‰æ‹©ã€‚
- en: Training Times on TPUs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPUä¸Šçš„è®­ç»ƒæ—¶é—´
- en: As you launch your script, you may notice that training seems exceptionally
    slow at first. This is because TPUs first run through a few batches of data to
    see how much memory to allocate before finally utilizing this configured memory
    allocation extremely efficiently.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨å¯åŠ¨è„šæœ¬æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°è®­ç»ƒä¸€å¼€å§‹ä¼¼ä¹å¼‚å¸¸ç¼“æ…¢ã€‚è¿™æ˜¯å› ä¸ºTPUé¦–å…ˆè¿è¡Œå‡ æ‰¹æ•°æ®ï¼Œä»¥æŸ¥çœ‹éœ€è¦åˆ†é…å¤šå°‘å†…å­˜ï¼Œç„¶åæ‰èƒ½æå…¶é«˜æ•ˆåœ°åˆ©ç”¨è¿™ä¸ªé…ç½®çš„å†…å­˜åˆ†é…ã€‚
- en: If you notice that your evaluation code to calculate the metrics of your model
    takes longer due to a larger batch size being used, it is recommended to keep
    the batch size the same as the training data if it is too slow. Otherwise the
    memory will reallocate to this new batch size after the first few iterations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ³¨æ„åˆ°è¯„ä¼°ä»£ç è®¡ç®—æ¨¡å‹æŒ‡æ ‡æ‰€éœ€çš„æ—¶é—´æ›´é•¿ï¼Œè¿™æ˜¯å› ä¸ºä½¿ç”¨äº†æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œå»ºè®®å¦‚æœé€Ÿåº¦å¤ªæ…¢ï¼Œåˆ™ä¿æŒæ‰¹é‡å¤§å°ä¸è®­ç»ƒæ•°æ®ç›¸åŒã€‚å¦åˆ™ï¼Œåœ¨å‰å‡ æ¬¡è¿­ä»£ä¹‹åï¼Œå†…å­˜å°†é‡æ–°åˆ†é…åˆ°è¿™ä¸ªæ–°çš„æ‰¹é‡å¤§å°ã€‚
- en: Just because the memory is allocated does not mean it will be used or that the
    batch size will increase when going back to your training dataloader.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ä»…å› ä¸ºå†…å­˜è¢«åˆ†é…å¹¶ä¸æ„å‘³ç€å®ƒä¼šè¢«ä½¿ç”¨ï¼Œæˆ–è€…å½“è¿”å›åˆ°è®­ç»ƒæ•°æ®åŠ è½½å™¨æ—¶æ‰¹é‡å¤§å°ä¼šå¢åŠ ã€‚
