- en: Qwen2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Qwen2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/qwen2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/qwen2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/qwen2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/qwen2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Qwen2 is the new model series of large language models from the Qwen team. Previously,
    we released the Qwen series, including Qwen-72B, Qwen-1.8B, Qwen-VL, Qwen-Audio,
    etc.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen2是Qwen团队推出的大型语言模型新系列。之前，我们发布了Qwen系列，包括Qwen-72B、Qwen-1.8B、Qwen-VL、Qwen-Audio等。
- en: Model Details
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型详情
- en: Qwen2 is a language model series including decoder language models of different
    model sizes. For each size, we release the base language model and the aligned
    chat model. It is based on the Transformer architecture with SwiGLU activation,
    attention QKV bias, group query attention, mixture of sliding window attention
    and full attention, etc. Additionally, we have an improved tokenizer adaptive
    to multiple natural languages and codes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen2是一个包含不同模型大小的解码器语言模型的系列。对于每个大小，我们发布基础语言模型和对齐的聊天模型。它基于Transformer架构，具有SwiGLU激活、注意力QKV偏置、组查询注意力、滑动窗口注意力和全注意力混合等。此外，我们还有一个适应多种自然语言和代码的改进的分词器。
- en: Usage tips
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: '`Qwen2-7B-beta` and `Qwen2-7B-Chat-beta` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`Qwen2-7B-beta`和`Qwen2-7B-Chat-beta`可以在[Huggingface Hub](https://huggingface.co/Qwen)上找到'
- en: In the following, we demonstrate how to use `Qwen2-7B-Chat-beta` for the inference.
    Note that we have used the ChatML format for dialog, in this demo we show how
    to leverage `apply_chat_template` for this purpose.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们演示如何使用`Qwen2-7B-Chat-beta`进行推断。请注意，我们已经使用ChatML格式进行对话，在此演示中，我们展示了如何利用`apply_chat_template`来实现这一目的。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Qwen2Config
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qwen2Config
- en: '### `class transformers.Qwen2Config`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Qwen2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/configuration_qwen2.py#L28)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/configuration_qwen2.py#L28)'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 151936) — Vocabulary size of the
    Qwen2 model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [Qwen2Model](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Model)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为151936）— Qwen2模型的词汇量。定义了在调用[Qwen2Model](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Model)时可以表示的不同标记的数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) — Dimension of the hidden
    representations.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*，默认为4096）— 隐藏表示的维度。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 22016) — Dimension of the
    MLP representations.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size`（`int`，*可选*，默认为22016）— MLP表示的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 32) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers`（`int`，*可选*，默认为32）— Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`（`int`，*可选*，默认为32）— Transformer编码器中每个注意力层的注意力头数。'
- en: '`num_key_value_heads` (`int`, *optional*, defaults to 32) — This is the number
    of key_value heads that should be used to implement Grouped Query Attention. If
    `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention
    (MHA), if `num_key_value_heads=1 the model will use Multi Query Attention (MQA)
    otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint,
    each group key and value head should be constructed by meanpooling all the original
    heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf).
    If it is not specified, will default to` 32`.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_key_value_heads`（`int`，*可选*，默认为32）— 这是应该用于实现Grouped Query Attention的key_value头的数量。如果`num_key_value_heads=num_attention_heads`，模型将使用Multi
    Head Attention（MHA），如果`num_key_value_heads=1`，模型将使用Multi Query Attention（MQA），否则将使用GQA。将多头检查点转换为GQA检查点时，应通过均值池化构建每个组键和值头，以包含该组中所有原始头的平均值。有关更多详细信息，请查看[此论文](https://arxiv.org/pdf/2305.13245.pdf)。如果未指定，将默认为`32`。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) — The
    non-linear activation function (function or string) in the decoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act`（`str`或`function`，*可选*，默认为`"silu"`) — 解码器中的非线性激活函数（函数或字符串）。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 32768) — The maximum
    sequence length that this model might ever be used with.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings`（`int`，*可选*，默认为32768）— 该模型可能会使用的最大序列长度。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range`（`float`，*可选*，默认为0.02）— 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。'
- en: '`rms_norm_eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used
    by the rms normalization layers.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rms_norm_eps`（`float`，*可选*，默认为1e-06）— rms归一化层使用的epsilon。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*，默认为`True`）— 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在`config.is_decoder=True`时相关。'
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether the
    model’s input and output word embeddings should be tied.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tie_word_embeddings`（`bool`，*可选*，默认为`False`）— 模型的输入和输出词嵌入是否应该绑定。'
- en: '`rope_theta` (`float`, *optional*, defaults to 10000.0) — The base period of
    the RoPE embeddings.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rope_theta`（`float`，*可选*，默认为10000.0）— RoPE嵌入的基本周期。'
- en: '`use_sliding_window` (`bool`, *optional*, defaults to `False`) — Whether to
    use sliding window attention.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_sliding_window`（`bool`，*可选*，默认为`False`）— 是否使用滑动窗口注意力。'
- en: '`sliding_window` (`int`, *optional*, defaults to 4096) — Sliding window attention
    (SWA) window size. If not specified, will default to `4096`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sliding_window`（`int`，*可选*，默认为4096）— 滑动窗口注意力（SWA）窗口大小。如果未指定，将默认为`4096`。'
- en: '`max_window_layers` (`int`, *optional*, defaults to 28) — The number of layers
    that use SWA (Sliding Window Attention). The bottom layers use SWA while the top
    use full attention.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_window_layers` (`int`, *可选*, 默认为28) — 使用SWA（滑动窗口注意力）的层数。底层使用SWA，而顶层使用完全注意力。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *可选*, 默认为0.0) — 注意力概率的丢弃比率。'
- en: This is the configuration class to store the configuration of a [Qwen2Model](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Model).
    It is used to instantiate a Qwen2 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of Qwen2-7B-beta [Qwen/Qwen2-7B-beta](https://huggingface.co/Qwen/Qwen2-7B-beta).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置类，用于存储[Qwen2Model](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Model)的配置。它用于根据指定的参数实例化一个Qwen2模型，定义模型架构。使用默认值实例化配置将产生类似于Qwen2-7B-beta
    [Qwen/Qwen2-7B-beta](https://huggingface.co/Qwen/Qwen2-7B-beta)的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Qwen2Tokenizer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qwen2Tokenizer
- en: '### `class transformers.Qwen2Tokenizer`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Qwen2Tokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/tokenization_qwen2.py#L87)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/tokenization_qwen2.py#L87)'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件的路径。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors` (`str`, *可选*, 默认为`"replace"`) — 解码字节为UTF-8时要遵循的范例。查看[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)以获取更多信息。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。'
- en: '`bos_token` (`str`, *optional*) — The beginning of sequence token. Not applicable
    for this tokenizer.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*）--序列标记的开头。不适用于此标记器。'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--序列结束标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The token
    used for padding, for example when batching sequences of different lengths.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--用于填充的令牌，例如，在批处理不同长度的序列时。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not the model should cleanup the spaces that were added when splitting
    the input text during the tokenization process. Not applicable to this tokenizer,
    since tokenization does not add spaces.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`, *可选*, 默认为`False`) — 模型是否应清除在分词过程中拆分输入文本时添加的空格。对于这个分词器不适用，因为分词不会添加空格。'
- en: '`split_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the special tokens should be split during the tokenization process. The
    default behavior is to not split special tokens. This means that if `<|endoftext|>`
    is the `eos_token`, then `tokenizer.tokenize("<|endoftext|>") = [''<|endoftext|>`].
    Otherwise, if `split_special_tokens=True`, then `tokenizer.tokenize("<|endoftext|>")`
    will be give `[''<'', ''|'', ''endo'', ''ft'', ''ext'', ''|'', ''>'']`. This argument
    is only supported for `slow` tokenizers for the moment.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_special_token`（`bool`，*optional*，默认为`False`）--在标记化过程中是否应拆分特殊标记。默认行为是不拆分特殊标记。这意味着，如果`<|endoftext|>`是`eos_token`，则`tokenizer.tokenize("<|endoftext|>")
    = [''<|endoftext|>'']`。否则，如果`split_special_tokens=True`，则`tokenizer.tokenize("<|endoftext|>")`将被赋予`[''''<'''',
    ''''|'''', ''''endo'''', ''''ft'''', ''''ext'''', ''''|'''', ''''>'''']`。这一论点目前只支持“慢速”标记化器。'
- en: Construct a Qwen2 tokenizer. Based on byte-level Byte-Pair-Encoding.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Qwen2分词器。基于字节级字节对编码。
- en: Same with GPT2Tokenzier, this tokenizer has been trained to treat spaces like
    parts of the tokens so a word will
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT2Tokenizer相同，这个分词器已经训练过，将空格视为标记的一部分，因此一个单词会
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子开头（无空格）或不在句子开头时，可能会以不同方式编码：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is expected.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预期的。
- en: You should not use GPT2Tokenizer instead, because of the different pretokenization
    rules.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您不应该使用GPT2Tokenizer，因为其具有不同的pretokenization规则。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。
- en: '#### `save_vocabulary`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/tokenization_qwen2.py#L314)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/tokenization_qwen2.py#L314)'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Qwen2TokenizerFast
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qwen2TokenizerFast
- en: '### `class transformers.Qwen2TokenizerFast`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Qwen2TokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/tokenization_qwen2_fast.py#L44)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/tokenization_qwen2_fast.py#L44)'
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`, *optional*) — Path to the vocabulary file.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`, *可选*) — 词汇文件的路径。'
- en: '`merges_file` (`str`, *optional*) — Path to the merges file.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`, *可选*) — 合并文件的路径。'
- en: '`tokenizer_file` (`str`, *optional*) — Path to [tokenizers](https://github.com/huggingface/tokenizers)
    file (generally has a .json extension) that contains everything needed to load
    the tokenizer.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file` (`str`, *可选*) — 包含加载分词器所需的所有内容的[tokenizers](https://github.com/huggingface/tokenizers)文件的路径（通常具有.json扩展名）。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead. Not applicable to this tokenizer.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。不适用于此标记器。'
- en: '`bos_token` (`str`, *optional*) — The beginning of sequence token. Not applicable
    for this tokenizer.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*）--序列标记的开头。不适用于此标记器。'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--序列结束标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The token
    used for padding, for example when batching sequences of different lengths.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--用于填充的令牌，例如，在批处理不同长度的序列时。'
- en: Construct a “fast” Qwen2 tokenizer (backed by HuggingFace’s *tokenizers* library).
    Based on byte-level Byte-Pair-Encoding.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速”Qwen2分词器（由HuggingFace的*tokenizers*库支持）。基于字节级字节对编码。
- en: Same with GPT2Tokenzier, this tokenizer has been trained to treat spaces like
    parts of the tokens so a word will
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT2Tokenizer相同，这个分词器已经训练过，将空格视为标记的一部分，因此一个单词会
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子开头（无空格）或不在句子开头时，可能会以不同方式编码：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is expected.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预期的。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。
- en: Qwen2Model
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qwen2Model
- en: '### `class transformers.Qwen2Model`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Qwen2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L913)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L913)'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Qwen2Config](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights. config — Qwen2Config'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Qwen2Config](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Config)）—模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。config
    — Qwen2Config'
- en: The bare Qwen2 Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Qwen2模型输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer
    is a `Qwen2DecoderLayer`
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由*config.num_hidden_layers*层组成的Transformer解码器。每一层都是一个`Qwen2DecoderLayer`。
- en: '#### `forward`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L947)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L947)'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）—词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）—用于避免在填充标记索引上执行注意力的掩码。选择在`[0,
    1]`中的掩码值：'
- en: 1 for tokens that are `not masked`,
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被“掩盖”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被“掩盖”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，可选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您想要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: 1 indicates the head is `not masked`,
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—每个输入序列标记在位置嵌入中的位置索引。在范围`[0,
    config.n_positions - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`Cache`或`tuple(tuple(torch.FloatTensor))`，*可选*）—预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在先前解码阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。'
- en: 'Two formats are allowed:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 允许两种格式：
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度为`config.n_layers`的元组的元组（`torch.FloatTensor`），每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量。这也被称为传统缓存格式。
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统缓存格式。
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入最后的`input_ids`（即不将其过去的键值状态提供给此模型的那些）的形状为`(batch_size,
    1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: The [Qwen2Model](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[Qwen2Model](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Model)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: Qwen2ForCausalLM
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qwen2ForCausalLM
- en: '### `class transformers.Qwen2ForCausalLM`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Qwen2ForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1095)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1095)'
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `forward`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1125)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1125)'
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0,
    1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩码的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩码的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，可选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: 1 indicates the head is `not masked`,
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`Cache`或`tuple(tuple(torch.FloatTensor))`, *optional*) —
    预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在解码的先前阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。'
- en: 'Two formats are allowed:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 允许两种格式：
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量。这也被称为传统缓存格式。
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，将返回传统缓存格式。
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`input_ids`（那些没有将其过去键值状态提供给此模型的）的形状为`(batch_size,
    1)`的张量，而不是所有形状为`(batch_size, sequence_length)`的`input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为关联向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: 'Args — labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '参数 — labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
    用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`或-100（参见`input_ids`文档）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`中的标记。'
- en: Returns
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Qwen2Config](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Config))
    and inputs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[Qwen2Config](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Config)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）-
    语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）-
    长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出+每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出处的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Qwen2ForCausalLM](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2ForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[Qwen2ForCausalLM](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2ForCausalLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Qwen2ForSequenceClassification
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qwen2ForSequenceClassification
- en: '### `class transformers.Qwen2ForSequenceClassification`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Qwen2ForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1281)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1281)'
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Qwen2Config](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Qwen2Config](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2Config)）-
    模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Qwen2 Model transformer with a sequence classification head on top (linear
    layer).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen2模型变压器，顶部带有序列分类头（线性层）。
- en: '[Qwen2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[Qwen2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification)使用最后一个标记进行分类，就像其他因果模型（例如GPT-2）一样。'
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它对最后一个标记进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，则会找到每行中不是填充标记的最后一个标记。如果未定义`pad_token_id`，则会简单地取每行批次中的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，因此会执行相同操作（取每行批次中的最后一个值）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1312)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/qwen2/modeling_qwen2.py#L1312)'
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）- 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）- 避免对填充标记索引执行注意力的掩码。掩码值选择在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示标记是`未掩码`，
- en: 0 for tokens that are `masked`.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则可选择仅输入最后的`decoder_input_ids`（请参见`past_key_values`）。
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表1。
- en: 1 indicates the head is `not masked`,
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`未掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`掩码`。
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）-
    输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`Cache`或`tuple(tuple(torch.FloatTensor))`，*可选*）- 预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。这通常包括模型在解码的先前阶段返回的`past_key_values`，当`use_cache=True`或`config.use_cache=True`时。'
- en: 'Two formats are allowed:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 允许两种格式：
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)实例；
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元组`tuple(torch.FloatTensor)`的长度为`config.n_layers`，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量。这也被称为传统的缓存格式。
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型将输出与输入相同的缓存格式。如果没有传递`past_key_values`，则将返回传统的缓存格式。
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`input_ids`（即那些没有将它们的过去键值状态提供给该模型的输入）的形状为`(batch_size,
    1)`，而不是形状为`(batch_size, sequence_length)`的所有`input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）-
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型内部的嵌入查找矩阵，这将非常有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: The [Qwen2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[Qwen2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
