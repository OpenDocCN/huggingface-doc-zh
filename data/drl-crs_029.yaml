- en: What is RL? A short recap
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？简短回顾
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/what-is-rl](https://huggingface.co/learn/deep-rl-course/unit2/what-is-rl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit2/what-is-rl](https://huggingface.co/learn/deep-rl-course/unit2/what-is-rl)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In RL, we build an agent that can **make smart decisions**. For instance, an
    agent that **learns to play a video game.** Or a trading agent that **learns to
    maximize its benefits** by deciding on **what stocks to buy and when to sell.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们构建一个能够**做出明智决策**的agent。例如，一个**学会玩视频游戏**的agent。或者一个**通过决定**买入哪些股票和何时卖出**来最大化利益**的交易agent。
- en: '![RL process](../Images/75d6b22440b9c5d7d5b653bfb590c8e0.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习过程](../Images/75d6b22440b9c5d7d5b653bfb590c8e0.png)'
- en: To make intelligent decisions, our agent will learn from the environment by **interacting
    with it through trial and error** and receiving rewards (positive or negative) **as
    unique feedback.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出智能决策，我们的agent将通过与环境的互动来学习，通过试错来接收奖励（正面或负面）作为独特的反馈。
- en: Its goal **is to maximize its expected cumulative reward** (because of the reward
    hypothesis).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 它的目标**是最大化其期望累积奖励**（因为奖励假设）。
- en: '**The agent’s decision-making process is called the policy π:** given a state,
    a policy will output an action or a probability distribution over actions. That
    is, given an observation of the environment, a policy will provide an action (or
    multiple probabilities for each action) that the agent should take.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**agent的决策过程称为策略π**：给定一个状态，策略将输出一个动作或动作的概率分布。也就是说，给定环境的观察，策略将提供一个代理应该采取的动作（或每个动作的多个概率）。'
- en: '![Policy](../Images/eb29b1cbd14496d16a36efc7ba1b5298.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![策略](../Images/eb29b1cbd14496d16a36efc7ba1b5298.png)'
- en: '**Our goal is to find an optimal policy π*** , aka., a policy that leads to
    the best expected cumulative reward.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的目标是找到一个最优策略π**，即，一个导致最佳期望累积奖励的策略。'
- en: 'And to find this optimal policy (hence solving the RL problem), there **are
    two main types of RL methods**:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到这个最优策略（从而解决强化学习问题），有两种主要类型的强化学习方法：
- en: '*Policy-based methods*: **Train the policy directly** to learn which action
    to take given a state.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于策略的方法*：**直接训练策略**，学习在给定状态下采取哪个动作。'
- en: '*Value-based methods*: **Train a value function** to learn **which state is
    more valuable** and use this value function **to take the action that leads to
    it.**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于价值的方法*：**训练一个价值函数**，学习**哪个状态更有价值**，并使用这个价值函数**采取导致它的动作**。'
- en: '![Two RL approaches](../Images/ea1be9a83e2be724fd67093267c9958b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![两种强化学习方法](../Images/ea1be9a83e2be724fd67093267c9958b.png)'
- en: And in this unit, **we’ll dive deeper into the value-based methods.**
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个单元中，我们将深入探讨基于价值的方法。
