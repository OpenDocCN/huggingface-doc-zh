- en: Tokenizers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ‡è®°å™¨
- en: 'Original text: [https://huggingface.co/docs/tokenizers/index](https://huggingface.co/docs/tokenizers/index)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/tokenizers/index](https://huggingface.co/docs/tokenizers/index)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Fast State-of-the-art tokenizers, optimized for both research and production
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«é€Ÿçš„æœ€å…ˆè¿›çš„æ ‡è®°å™¨ï¼Œé’ˆå¯¹ç ”ç©¶å’Œç”Ÿäº§è¿›è¡Œäº†ä¼˜åŒ–ã€‚
- en: '[ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers) provides an implementation
    of todayâ€™s most used tokenizers, with a focus on performance and versatility.
    These tokenizers are also used in [ğŸ¤— Transformers](https://github.com/huggingface/transformers).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)æä¾›äº†å½“ä»Šæœ€å¸¸ç”¨çš„æ ‡è®°å™¨çš„å®ç°ï¼Œé‡ç‚¹æ”¾åœ¨æ€§èƒ½å’Œçµæ´»æ€§ä¸Šã€‚è¿™äº›æ ‡è®°å™¨ä¹Ÿç”¨äº[ğŸ¤—
    Transformers](https://github.com/huggingface/transformers)ã€‚'
- en: 'Main features:'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸»è¦ç‰¹ç‚¹ï¼š
- en: Train new vocabularies and tokenize, using todayâ€™s most used tokenizers.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ–°è¯æ±‡å¹¶æ ‡è®°åŒ–ï¼Œä½¿ç”¨å½“ä»Šæœ€å¸¸ç”¨çš„æ ‡è®°å™¨ã€‚
- en: Extremely fast (both training and tokenization), thanks to the Rust implementation.
    Takes less than 20 seconds to tokenize a GB of text on a serverâ€™s CPU.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æå¿«ï¼ˆè®­ç»ƒå’Œæ ‡è®°åŒ–ï¼‰ï¼Œæ„Ÿè°¢Rustå®ç°ã€‚åœ¨æœåŠ¡å™¨çš„CPUä¸Šæ ‡è®°åŒ–1GBæ–‡æœ¬ä¸åˆ°20ç§’ã€‚
- en: Easy to use, but also extremely versatile.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ˜“äºä½¿ç”¨ï¼Œä½†ä¹Ÿéå¸¸çµæ´»ã€‚
- en: Designed for both research and production.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ—¨åœ¨ç”¨äºç ”ç©¶å’Œç”Ÿäº§ã€‚
- en: Full alignment tracking. Even with destructive normalization, itâ€™s always possible
    to get the part of the original sentence that corresponds to any token.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„å¯¹é½è·Ÿè¸ªã€‚å³ä½¿è¿›è¡Œäº†ç ´åæ€§è§„èŒƒåŒ–ï¼Œä¹Ÿå§‹ç»ˆå¯ä»¥è·å¾—ä¸ä»»ä½•æ ‡è®°å¯¹åº”çš„åŸå§‹å¥å­éƒ¨åˆ†ã€‚
- en: 'Does all the pre-processing: Truncation, Padding, add the special tokens your
    model needs.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ‰€æœ‰é¢„å¤„ç†å·¥ä½œï¼šæˆªæ–­ã€å¡«å……ã€æ·»åŠ æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šæ ‡è®°ã€‚
