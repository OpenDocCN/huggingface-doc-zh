- en: LoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA
- en: 'Original text: [https://huggingface.co/docs/peft/developer_guides/lora](https://huggingface.co/docs/peft/developer_guides/lora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/developer_guides/lora](https://huggingface.co/docs/peft/developer_guides/lora)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: LoRA is low-rank decomposition method to reduce the number of trainable parameters
    which speeds up finetuning large models and uses less memory. In PEFT, using LoRA
    is as easy as setting up a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    and wrapping it with [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    to create a trainable [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA是一种低秩分解方法，可以减少可训练参数的数量，加快微调大型模型的速度并减少内存使用。在PEFT中，使用LoRA就像设置[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)并将其与[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)结合使用，以创建可训练的[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)一样简单。
- en: This guide explores in more detail other options and features for using LoRA.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南更详细地探讨了使用LoRA的其他选项和功能。
- en: Initialization
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化
- en: The initialization of LoRA weights is controlled by the parameter `init_lora_weights`
    in [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig).
    By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and
    zeros for weight B resulting in an identity transform (same as the reference [implementation](https://github.com/microsoft/LoRA)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA权重的初始化由[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)中的参数`init_lora_weights`控制。默认情况下，PEFT使用Kaiming-uniform初始化LoRA权重A，并使用零初始化权重B，从而产生恒等变换（与参考[实现](https://github.com/microsoft/LoRA)相同）。
- en: It is also possible to pass `init_lora_weights="gaussian"`. As the name suggests,
    this initializes weight A with a Gaussian distribution and zeros for weight B
    (this is how [Diffusers](https://huggingface.co/docs/diffusers/index) initializes
    LoRA weights).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以传递`init_lora_weights="gaussian"`。正如名称所示，这将使用高斯分布初始化权重A，并将权重B初始化为零（这是[Diffusers](https://huggingface.co/docs/diffusers/index)初始化LoRA权重的方式）。
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There is also an option to set `init_lora_weights=False` which is useful for
    debugging and testing. This should be the only time you use this option. When
    choosing this option, the LoRA weights are initialized such that they do *not*
    result in an identity transform.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个选项可以设置`init_lora_weights=False`，这对调试和测试很有用。这应该是您使用此选项的唯一时机。选择此选项时，LoRA权重将被初始化，使其*不*导致恒等变换。
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: LoftQ
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoftQ
- en: 'When quantizing the base model for QLoRA training, consider using the [LoftQ
    initialization](https://arxiv.org/abs/2310.08659), which has been shown to improve
    performance when training quantized models. The idea is that the LoRA weights
    are initialized such that the quantization error is minimized. If you’re using
    LoftQ, *do not* quantize the base model. You should set up a `LoftQConfig` instead:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在为QLoRA训练量化基础模型时，考虑使用[LoftQ初始化](https://arxiv.org/abs/2310.08659)，已经证明在训练量化模型时可以提高性能。其思想是初始化LoRA权重，使量化误差最小化。如果使用LoftQ，*不要*量化基础模型。您应该设置`LoftQConfig`：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Learn more about how PEFT works with quantization in the [Quantization](quantization)
    guide.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 了解有关PEFT如何与量化一起工作的更多信息，请参阅[量化](quantization)指南。
- en: Rank-stabilized LoRA
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 秩稳定的LoRA
- en: Another way to initialize [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    is with the [rank-stabilized LoRA (rsLoRA)](https://huggingface.co/papers/2312.03732)
    method. The LoRA architecture scales each adapter during every forward pass by
    a fixed scalar which is set at initialization and depends on the rank `r`. The
    scalar is given by `lora_alpha/r` in the original implementation, but rsLoRA uses
    `lora_alpha/math.sqrt(r)` which stabilizes the adapters and increases the performance
    potential from using a higher `r`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种初始化[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)的方法是使用[rank-stabilized
    LoRA (rsLoRA)](https://huggingface.co/papers/2312.03732)方法。LoRA架构在每次前向传递期间通过固定标量按比例缩放每个适配器，该标量在初始化时设置，并取决于秩`r`。标量由原始实现中的`lora_alpha/r`给出，但rsLoRA使用`lora_alpha/math.sqrt(r)`，这稳定了适配器并增加了使用更高`r`的性能潜力。
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: QLoRA-style training
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QLoRA风格的训练
- en: The default LoRA settings in PEFT add trainable weights to the query and value
    layers of each attention block. But [QLoRA](https://hf.co/papers/2305.14314),
    which adds trainable weights to all the linear layers of a transformer model,
    can provide performance equal to a fully finetuned model. To apply LoRA to all
    the linear layers, like in QLoRA, set `target_modules="all-linear"` (easier than
    specifying individual modules by name which can vary depending on the architecture).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT中的默认LoRA设置会向每个注意力块的查询和值层添加可训练权重。但是[QLoRA](https://hf.co/papers/2305.14314)会向变压器模型的所有线性层添加可训练权重，可以提供与完全微调模型相等的性能。要将LoRA应用于所有线性层，就像在QLoRA中一样，请设置`target_modules="all-linear"`（比根据架构的不同而变化的单独指定模块名称更容易）。
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Merge adapters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并适配器
- en: While LoRA is significantly smaller and faster to train, you may encounter latency
    issues during inference due to separately loading the base model and the LoRA
    adapter. To eliminate latency, use the [merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)
    function to merge the adapter weights with the base model. This allows you to
    use the newly merged model as a standalone model. The [merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)
    function doesn’t keep the adapter weights in memory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LoRA明显更小更快，但在推理过程中可能会遇到延迟问题，因为需要分别加载基础模型和LoRA适配器。为了消除延迟，使用[merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)函数将适配器权重与基础模型合并。这样可以将新合并的模型用作独立模型。[merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)函数不会在内存中保留适配器权重。
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you need to keep a copy of the weights so you can unmerge the adapter later
    or delete and load different ones, you should use the [merge_adapter()](/docs/peft/v0.8.2/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_adapter)
    function instead. Now you have the option to use [unmerge_adapter()](/docs/peft/v0.8.2/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unmerge_adapter)
    to return the base model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要保留权重的副本，以便稍后取消合并适配器或删除并加载不同的适配器，应该使用merge_adapter()函数。现在您可以选择使用unmerge_adapter()来返回基础模型。
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The [add_weighted_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.add_weighted_adapter)
    function is useful for merging multiple LoRAs into a new adapter based on a user
    provided weighting scheme in the `weights` parameter. Below is an end-to-end example.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: add_weighted_adapter()函数对于根据用户提供的权重方案将多个LoRA合并成一个新的适配器非常有用，权重方案在`weights`参数中指定。下面是一个端到端的示例。
- en: 'First load the base model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载基础模型：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we load the first adapter:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载第一个适配器：
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then load a different adapter and merge it with the first one:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载一个不同的适配器并将其与第一个合并：
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are several supported methods for `combination_type`. Refer to the [documentation](../package_reference/lora#peft.LoraModel.add_weighted_adapter)
    for more details. Note that “svd” as the `combination_type` is not supported when
    using `torch.float16` or `torch.bfloat16` as the datatype.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种支持的`combination_type`方法。有关更多详细信息，请参阅文档。请注意，在使用`torch.float16`或`torch.bfloat16`作为数据类型时，“svd”作为`combination_type`是不受支持的。
- en: 'Now, perform inference:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，执行推理：
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Load adapters
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载适配器
- en: Adapters can be loaded onto a pretrained model with [load_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.load_adapter),
    which is useful for trying out different adapters whose weights aren’t merged.
    Set the active adapter weights with the [set_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.set_adapter)
    function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器可以使用load_adapter()加载到预训练模型中，这对于尝试不合并权重的不同适配器非常有用。使用set_adapter()函数设置活动适配器的权重。
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To return the base model, you could use [unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.unload)
    to unload all of the LoRA modules or [delete_adapter()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.delete_adapter)
    to delete the adapter entirely.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要返回基础模型，您可以使用unload()来卸载所有的LoRA模块，或者使用delete_adapter()来完全删除适配器。
- en: '[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
