- en: Optimizing LLMs for Speed and Memory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–LLMsçš„é€Ÿåº¦å’Œå†…å­˜
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial_optimization](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial_optimization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial_optimization](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial_optimization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) such as GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b),
    and [Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf) are rapidly advancing
    in their ability to tackle human-centric tasks, establishing themselves as essential
    tools in modern knowledge-based industries. Deploying these models in real-world
    tasks remains challenging, however:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¸å¦‚GPT3/4ã€[Falcon](https://huggingface.co/tiiuae/falcon-40b)å’Œ[Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf)ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œèƒ½å¤Ÿå¤„ç†ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼Œæˆä¸ºç°ä»£çŸ¥è¯†å‹äº§ä¸šä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ä»»åŠ¡ä¸­éƒ¨ç½²è¿™äº›æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼š
- en: To exhibit near-human text understanding and generation capabilities, LLMs currently
    require to be composed of billions of parameters (see [Kaplan et al](https://arxiv.org/abs/2001.08361),
    [Wei et. al](https://arxiv.org/abs/2206.07682)). This consequently amplifies the
    memory demands for inference.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†å±•ç¤ºæ¥è¿‘äººç±»æ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œç›®å‰LLMséœ€è¦ç”±æ•°åäº¿å‚æ•°ç»„æˆï¼ˆå‚è§[Kaplanç­‰äºº](https://arxiv.org/abs/2001.08361)ï¼Œ[Weiç­‰äºº](https://arxiv.org/abs/2206.07682)ï¼‰ã€‚è¿™è¿›ä¸€æ­¥å¢åŠ äº†æ¨æ–­çš„å†…å­˜éœ€æ±‚ã€‚
- en: In many real-world tasks, LLMs need to be given extensive contextual information.
    This necessitates the modelâ€™s capability to manage very long input sequences during
    inference.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸­ï¼ŒLLMséœ€è¦æä¾›å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™è¦æ±‚æ¨¡å‹åœ¨æ¨æ–­è¿‡ç¨‹ä¸­èƒ½å¤Ÿå¤„ç†éå¸¸é•¿çš„è¾“å…¥åºåˆ—ã€‚
- en: The crux of these challenges lies in augmenting the computational and memory
    capabilities of LLMs, especially when handling expansive input sequences.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æŒ‘æˆ˜çš„å…³é”®åœ¨äºå¢å¼ºLLMsçš„è®¡ç®—å’Œå†…å­˜èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åºå¤§çš„è¾“å…¥åºåˆ—æ—¶ã€‚
- en: 'In this guide, we will go over the effective techniques for efficient LLM deployment:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»é«˜æ•ˆLLMéƒ¨ç½²çš„æœ‰æ•ˆæŠ€æœ¯ï¼š
- en: '**Lower Precision:** Research has shown that operating at reduced numerical
    precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve
    computational advantages without a considerable decline in model performance.'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½ç²¾åº¦ï¼š**ç ”ç©¶è¡¨æ˜ï¼Œä»¥é™ä½çš„æ•°å€¼ç²¾åº¦ï¼Œå³[8ä½å’Œ4ä½](./main_classes/quantization.md)å¯ä»¥åœ¨ä¸æ˜¾è‘—é™ä½æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹å®ç°è®¡ç®—ä¼˜åŠ¿ã€‚'
- en: '**Flash Attention:** Flash Attention is a variation of the attention algorithm
    that not only provides a more memory-efficient approach but also realizes increased
    efficiency due to optimized GPU memory utilization.'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¿«é—ªæ³¨æ„åŠ›ï¼š**å¿«é—ªæ³¨æ„åŠ›æ˜¯æ³¨æ„åŠ›ç®—æ³•çš„ä¸€ç§å˜ä½“ï¼Œä¸ä»…æä¾›äº†æ›´èŠ‚çœå†…å­˜çš„æ–¹æ³•ï¼Œè¿˜é€šè¿‡ä¼˜åŒ–GPUå†…å­˜åˆ©ç”¨ç‡å®ç°äº†å¢åŠ çš„æ•ˆç‡ã€‚'
- en: '**Architectural Innovations:** Considering that LLMs are always deployed in
    the same way during inference, namely autoregressive text generation with a long
    input context, specialized model architectures have been proposed that allow for
    more efficient inference. The most important advancement in model architectures
    hereby are [Alibi](https://arxiv.org/abs/2108.12409), [Rotary embeddings](https://arxiv.org/abs/2104.09864),
    [Multi-Query Attention (MQA)](https://arxiv.org/abs/1911.02150) and [Grouped-Query-Attention
    (GQA)]((https://arxiv.org/abs/2305.13245)).'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¶æ„åˆ›æ–°ï¼š**è€ƒè™‘åˆ°LLMsåœ¨æ¨æ–­è¿‡ç¨‹ä¸­å§‹ç»ˆä»¥ç›¸åŒæ–¹å¼éƒ¨ç½²ï¼Œå³å…·æœ‰é•¿è¾“å…¥ä¸Šä¸‹æ–‡çš„è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆï¼Œå·²ç»æå‡ºäº†ä¸“é—¨çš„æ¨¡å‹æ¶æ„ï¼Œå…è®¸æ›´é«˜æ•ˆçš„æ¨æ–­ã€‚åœ¨æ¨¡å‹æ¶æ„æ–¹é¢æœ€é‡è¦çš„è¿›å±•æ˜¯[Alibi](https://arxiv.org/abs/2108.12409)ã€[Rotary
    embeddings](https://arxiv.org/abs/2104.09864)ã€[å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰](https://arxiv.org/abs/1911.02150)å’Œ[åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰](https://arxiv.org/abs/2305.13245)ã€‚'
- en: Throughout this guide, we will offer an analysis of auto-regressive generation
    from a tensorâ€™s perspective. We delve into the pros and cons of adopting lower
    precision, provide a comprehensive exploration of the latest attention algorithms,
    and discuss improved LLM architectures. While doing so, we run practical examples
    showcasing each of the feature improvements.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä»å¼ é‡çš„è§’åº¦å¯¹è‡ªå›å½’ç”Ÿæˆè¿›è¡Œåˆ†æã€‚æˆ‘ä»¬æ·±å…¥æ¢è®¨é‡‡ç”¨ä½ç²¾åº¦çš„åˆ©å¼Šï¼Œå…¨é¢æ¢ç´¢æœ€æ–°çš„æ³¨æ„åŠ›ç®—æ³•ï¼Œå¹¶è®¨è®ºæ”¹è¿›çš„LLMæ¶æ„ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿è¡Œå®é™…ç¤ºä¾‹å±•ç¤ºæ¯ä¸ªåŠŸèƒ½æ”¹è¿›ã€‚
- en: 1\. Lower Precision
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. ä½ç²¾åº¦
- en: Memory requirements of LLMs can be best understood by seeing the LLM as a set
    of weight matrices and vectors and the text inputs as a sequence of vectors. In
    the following, the definition *weights* will be used to signify all model weight
    matrices and vectors.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†LLMè§†ä¸ºä¸€ç»„æƒé‡çŸ©é˜µå’Œå‘é‡ï¼Œå°†æ–‡æœ¬è¾“å…¥è§†ä¸ºä¸€ç³»åˆ—å‘é‡ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£LLMsçš„å†…å­˜éœ€æ±‚ã€‚åœ¨æ¥ä¸‹æ¥çš„å†…å®¹ä¸­ï¼Œå®šä¹‰*æƒé‡*å°†ç”¨äºè¡¨ç¤ºæ‰€æœ‰æ¨¡å‹æƒé‡çŸ©é˜µå’Œå‘é‡ã€‚
- en: 'At the time of writing this guide, LLMs consist of at least a couple billion
    parameters. Each parameter thereby is made of a decimal number, e.g. `4.5689`
    which is usually stored in either [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format),
    [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), or [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)
    format. This allows us to easily compute the memory requirement to load the LLM
    into memory:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ’°å†™æœ¬æŒ‡å—æ—¶ï¼ŒLLMsè‡³å°‘åŒ…å«æ•°åäº¿å‚æ•°ã€‚å› æ­¤ï¼Œæ¯ä¸ªå‚æ•°ç”±ä¸€ä¸ªåè¿›åˆ¶æ•°ç»„æˆï¼Œä¾‹å¦‚`4.5689`ï¼Œé€šå¸¸ä»¥[float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)ã€[bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)æˆ–[float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)æ ¼å¼å­˜å‚¨ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾è®¡ç®—åŠ è½½LLMåˆ°å†…å­˜æ‰€éœ€çš„å†…å­˜é‡ï¼š
- en: '*Loading the weights of a model having X billion parameters requires roughly
    4* X GB of VRAM in float32 precision*'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*åŠ è½½å…·æœ‰Xåäº¿å‚æ•°çš„æ¨¡å‹çš„æƒé‡å¤§çº¦éœ€è¦4*X GBçš„VRAMï¼Œç²¾åº¦ä¸ºfloat32*'
- en: 'Nowadays, models are however rarely trained in full float32 precision, but
    usually in bfloat16 precision or less frequently in float16 precision. Therefore
    the rule of thumb becomes:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä»Šï¼Œæ¨¡å‹å¾ˆå°‘ä»¥å®Œæ•´çš„float32ç²¾åº¦è¿›è¡Œè®­ç»ƒï¼Œè€Œé€šå¸¸ä»¥bfloat16ç²¾åº¦æˆ–æ›´å°‘çš„float16ç²¾åº¦è¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œç»éªŒæ³•åˆ™å˜ä¸ºï¼š
- en: '*Loading the weights of a model having X billion parameters requires roughly
    2* X GB of VRAM in bfloat16/float16 precision*'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*åŠ è½½å…·æœ‰Xåäº¿å‚æ•°çš„æ¨¡å‹çš„æƒé‡å¤§çº¦éœ€è¦2*X GBçš„VRAMï¼Œç²¾åº¦ä¸ºbfloat16/float16*'
- en: For shorter text inputs (less than 1024 tokens), the memory requirement for
    inference is very much dominated by the memory requirement to load the weights.
    Therefore, for now, letâ€™s assume that the memory requirement for inference is
    equal to the memory requirement to load the model into the GPU VRAM.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾ƒçŸ­çš„æ–‡æœ¬è¾“å…¥ï¼ˆå°‘äº1024ä¸ªæ ‡è®°ï¼‰ï¼Œæ¨ç†çš„å†…å­˜éœ€æ±‚ä¸»è¦å—åŠ è½½æƒé‡çš„å†…å­˜éœ€æ±‚æ”¯é…ã€‚å› æ­¤ï¼Œç°åœ¨è®©æˆ‘ä»¬å‡è®¾æ¨ç†çš„å†…å­˜éœ€æ±‚ç­‰äºå°†æ¨¡å‹åŠ è½½åˆ°GPU VRAMä¸­çš„å†…å­˜éœ€æ±‚ã€‚
- en: 'To give some examples of how much VRAM it roughly takes to load a model in
    bfloat16:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¾‹è¯´æ˜åŠ è½½ bfloat16 æ¨¡å‹å¤§è‡´éœ€è¦å¤šå°‘ VRAMï¼š
- en: '**GPT3** requires 2 * 175 GB = **350 GB** VRAM'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT3 éœ€è¦ 2 * 175 GB = 350 GB VRAM
- en: '[**Bloom**](https://huggingface.co/bigscience/bloom) requires 2 * 176 GB =
    **352 GB** VRAM'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bloom](https://huggingface.co/bigscience/bloom) éœ€è¦ 2 * 176 GB = 352 GB VRAM'
- en: '[**Llama-2-70b**](https://huggingface.co/meta-llama/Llama-2-70b-hf) requires
    2 * 70 GB = **140 GB** VRAM'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama-2-70b](https://huggingface.co/meta-llama/Llama-2-70b-hf) éœ€è¦ 2 * 70 GB
    = 140 GB VRAM'
- en: '[**Falcon-40b**](https://huggingface.co/tiiuae/falcon-40b) requires 2 * 40
    GB = **80 GB** VRAM'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon-40b](https://huggingface.co/tiiuae/falcon-40b) éœ€è¦ 2 * 40 GB = 80 GB
    VRAM'
- en: '[**MPT-30b**](https://huggingface.co/mosaicml/mpt-30b) requires 2 * 30 GB =
    **60 GB** VRAM'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MPT-30b](https://huggingface.co/mosaicml/mpt-30b) éœ€è¦ 2 * 30 GB = 60 GB VRAM'
- en: '[**bigcode/starcoder**](https://huggingface.co/bigcode/starcoder) requires
    2 * 15.5 = **31 GB** VRAM'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bigcode/starcoder](https://huggingface.co/bigcode/starcoder) éœ€è¦ 2 * 15.5 =
    31 GB VRAM'
- en: As of writing this document, the largest GPU chip on the market is the A100
    & H100 offering 80GB of VRAM. Most of the models listed before require more than
    80GB just to be loaded and therefore necessarily require [tensor parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#tensor-parallelism)
    and/or [pipeline parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆªè‡³æ’°å†™æœ¬æ–‡æ—¶ï¼Œå¸‚åœºä¸Šæœ€å¤§çš„GPUèŠ¯ç‰‡æ˜¯ A100 & H100ï¼Œæä¾› 80GB çš„ VRAMã€‚ä¹‹å‰åˆ—å‡ºçš„å¤§å¤šæ•°æ¨¡å‹éœ€è¦è¶…è¿‡ 80GB çš„å†…å­˜æ‰èƒ½åŠ è½½ï¼Œå› æ­¤å¿…ç„¶éœ€è¦å¼ é‡å¹¶è¡Œå¤„ç†å’Œ/æˆ–ç®¡é“å¹¶è¡Œå¤„ç†ã€‚
- en: ğŸ¤— Transformers does not support tensor parallelism out of the box as it requires
    the model architecture to be written in a specific way. If youâ€™re interested in
    writing models in a tensor-parallelism-friendly way, feel free to have a look
    at [the text-generation-inference library](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersä¸æ”¯æŒå¼ é‡å¹¶è¡Œå¤„ç†ï¼Œå› ä¸ºå®ƒè¦æ±‚æ¨¡å‹æ¶æ„ä»¥ç‰¹å®šæ–¹å¼ç¼–å†™ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£ä»¥å¼ é‡å¹¶è¡Œå‹å¥½çš„æ–¹å¼ç¼–å†™æ¨¡å‹ï¼Œè¯·éšæ—¶æŸ¥çœ‹[æ–‡æœ¬ç”Ÿæˆæ¨ç†åº“](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling)ã€‚
- en: Naive pipeline parallelism is supported out of the box. For this, simply load
    the model with `device="auto"` which will automatically place the different layers
    on the available GPUs as explained [here](https://huggingface.co/docs/accelerate/v0.22.0/en/concept_guides/big_model_inference).
    Note, however that while very effective, this naive pipeline parallelism does
    not tackle the issues of GPU idling. For this more advanced pipeline parallelism
    is required as explained [here](https://huggingface.co/docs/transformers/en/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¤©çœŸçš„ç®¡é“å¹¶è¡Œå¤„ç†æ˜¯å¼€ç®±å³ç”¨çš„ã€‚ä¸ºæ­¤ï¼Œåªéœ€ä½¿ç”¨ `device="auto"` åŠ è½½æ¨¡å‹ï¼Œå®ƒå°†è‡ªåŠ¨å°†ä¸åŒçš„å±‚æ”¾ç½®åœ¨å¯ç”¨çš„GPUä¸Šï¼Œå¦‚æ­¤å¤„æ‰€è¿°ã€‚è¯·æ³¨æ„ï¼Œå°½ç®¡éå¸¸æœ‰æ•ˆï¼Œä½†è¿™ç§å¤©çœŸçš„ç®¡é“å¹¶è¡Œå¤„ç†å¹¶æœªè§£å†³GPUç©ºé—²çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œéœ€è¦æ›´é«˜çº§çš„ç®¡é“å¹¶è¡Œå¤„ç†ï¼Œå¦‚æ­¤å¤„æ‰€è¿°ã€‚
- en: If you have access to an 8 x 80GB A100 node, you could load BLOOM as follows
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯ä»¥è®¿é—®ä¸€ä¸ª 8 x 80GB A100 èŠ‚ç‚¹ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼åŠ è½½ BLOOM
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By using `device_map="auto"` the attention layers would be equally distributed
    over all available GPUs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨ `device_map="auto"`ï¼Œæ³¨æ„åŠ›å±‚å°†å‡åŒ€åˆ†å¸ƒåœ¨æ‰€æœ‰å¯ç”¨çš„GPUä¸Šã€‚
- en: In this guide, we will use [bigcode/octocoder](https://huggingface.co/bigcode/octocoder)
    as it can be run on a single 40 GB A100 GPU device chip. Note that all memory
    and speed optimizations that we will apply going forward, are equally applicable
    to models that require model or tensor parallelism.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[bigcode/octocoder](https://huggingface.co/bigcode/octocoder)ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨å•ä¸ª
    40 GB A100 GPU è®¾å¤‡èŠ¯ç‰‡ä¸Šè¿è¡Œã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†è¦åº”ç”¨çš„æ‰€æœ‰å†…å­˜å’Œé€Ÿåº¦ä¼˜åŒ–éƒ½åŒæ ·é€‚ç”¨äºéœ€è¦æ¨¡å‹æˆ–å¼ é‡å¹¶è¡Œå¤„ç†çš„æ¨¡å‹ã€‚
- en: Since the model is loaded in bfloat16 precision, using our rule of thumb above,
    we would expect the memory requirement to run inference with `bigcode/octocoder`
    to be around 31 GB VRAM. Letâ€™s give it a try.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¨¡å‹ä»¥ bfloat16 ç²¾åº¦åŠ è½½ï¼Œæ ¹æ®æˆ‘ä»¬ä¸Šé¢çš„ç»éªŒæ³•åˆ™ï¼Œæˆ‘ä»¬é¢„è®¡ä½¿ç”¨ `bigcode/octocoder` è¿è¡Œæ¨ç†çš„å†…å­˜éœ€æ±‚çº¦ä¸º 31 GB
    VRAMã€‚è®©æˆ‘ä»¬è¯•ä¸€è¯•ã€‚
- en: We first load the model and tokenizer and then pass both to Transformersâ€™ [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)
    object.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç„¶åå°†ä¸¤è€…ä¼ é€’ç»™Transformersçš„[ç®¡é“](https://huggingface.co/docs/transformers/main_classes/pipelines)å¯¹è±¡ã€‚
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Output**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE4]python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024
    / 1024\n[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4]python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024
    / 1024\n[PRE5]'
- en: Nice, we can now directly use the result to convert bytes into Gigabytes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨ç»“æœå°†å­—èŠ‚è½¬æ¢ä¸ºåƒå…†å­—èŠ‚ã€‚
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Letâ€™s call [`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html)
    to measure the peak GPU memory allocation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è°ƒç”¨[`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html)æ¥æµ‹é‡GPUå†…å­˜åˆ†é…çš„å³°å€¼ã€‚
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Output**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Close enough to our back-of-the-envelope computation! We can see the number
    is not exactly correct as going from bytes to kilobytes requires a multiplication
    of 1024 instead of 1000\. Therefore the back-of-the-envelope formula can also
    be understood as an â€œat most X GBâ€ computation. Note that if we had tried to run
    the model in full float32 precision, a whopping 64 GB of VRAM would have been
    required.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è¿‘æˆ‘ä»¬ç²—ç•¥è®¡ç®—çš„ç»“æœï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°å­—å¹¶ä¸å®Œå…¨æ­£ç¡®ï¼Œå› ä¸ºä»å­—èŠ‚åˆ°åƒå­—èŠ‚éœ€è¦ä¹˜ä»¥1024è€Œä¸æ˜¯1000ã€‚å› æ­¤ï¼Œç²—ç•¥è®¡ç®—å…¬å¼ä¹Ÿå¯ä»¥ç†è§£ä¸ºâ€œæœ€å¤šXGBâ€çš„è®¡ç®—ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ä»¥å®Œæ•´çš„float32ç²¾åº¦è¿è¡Œæ¨¡å‹ï¼Œå°†éœ€è¦64GBçš„VRAMã€‚
- en: Almost all models are trained in bfloat16 nowadays, there is no reason to run
    the model in full float32 precision if [your GPU supports bfloat16](https://discuss.pytorch.org/t/bfloat16-native-support/117155/5).
    Float32 wonâ€™t give better inference results than the precision that was used to
    train the model.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å‡ ä¹æ‰€æœ‰æ¨¡å‹ç°åœ¨éƒ½æ˜¯åœ¨bfloat16ä¸­è®­ç»ƒçš„ï¼Œå¦‚æœ[æ‚¨çš„GPUæ”¯æŒbfloat16](https://discuss.pytorch.org/t/bfloat16-native-support/117155/5)ï¼Œå°±æ²¡æœ‰ç†ç”±ä»¥å®Œæ•´çš„float32ç²¾åº¦è¿è¡Œæ¨¡å‹ã€‚Float32ä¸ä¼šæ¯”ç”¨äºè®­ç»ƒæ¨¡å‹çš„ç²¾åº¦æä¾›æ›´å¥½çš„æ¨æ–­ç»“æœã€‚
- en: If you are unsure in which format the model weights are stored on the Hub, you
    can always look into the checkpointâ€™s config under `"torch_dtype"`, *e.g.* [here](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21).
    It is recommended to set the model to the same precision type as written in the
    config when loading with `from_pretrained(..., torch_dtype=...)` except when the
    original type is float32 in which case one can use both `float16` or `bfloat16`
    for inference.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç¡®å®šæ¨¡å‹æƒé‡ä»¥å“ªç§æ ¼å¼å­˜å‚¨åœ¨Hubä¸Šï¼Œæ‚¨å¯ä»¥éšæ—¶æŸ¥çœ‹æ£€æŸ¥ç‚¹çš„é…ç½®ï¼Œåœ¨`"torch_dtype"`ä¸‹ï¼Œä¾‹å¦‚[è¿™é‡Œ](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21)ã€‚å»ºè®®åœ¨ä½¿ç”¨`from_pretrained(...,
    torch_dtype=...)`åŠ è½½æ¨¡å‹æ—¶ï¼Œå°†æ¨¡å‹è®¾ç½®ä¸ºä¸é…ç½®ä¸­å†™å…¥çš„ç›¸åŒç²¾åº¦ç±»å‹ï¼Œé™¤éåŸå§‹ç±»å‹ä¸ºfloat32ï¼Œæ­¤æ—¶å¯ä»¥åœ¨æ¨æ–­ä¸­ä½¿ç”¨`float16`æˆ–`bfloat16`ã€‚
- en: Letâ€™s define a `flush(...)` function to free all allocated memory so that we
    can accurately measure the peak allocated GPU memory.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`flush(...)`å‡½æ•°æ¥é‡Šæ”¾æ‰€æœ‰åˆ†é…çš„å†…å­˜ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å‡†ç¡®åœ°æµ‹é‡åˆ†é…çš„GPUå†…å­˜å³°å€¼ã€‚
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Letâ€™s call it now for the next experiment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä¸ºä¸‹ä¸€ä¸ªå®éªŒè°ƒç”¨å®ƒã€‚
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the recent version of the accelerate library, you can also use an utility
    method called `release_memory()`
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€è¿‘çš„accelerateåº“ç‰ˆæœ¬ä¸­ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¸€ä¸ªåä¸º`release_memory()`çš„å®ç”¨æ–¹æ³•
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now what if your GPU does not have 32 GB of VRAM? It has been found that model
    weights can be quantized to 8-bit or 4-bits without a significant loss in performance
    (see [Dettmers et al.](https://arxiv.org/abs/2208.07339)). Model can be quantized
    to even 3 or 2 bits with an acceptable loss in performance as shown in the recent
    [GPTQ paper](https://arxiv.org/abs/2210.17323) ğŸ¤¯.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆå¦‚æœæ‚¨çš„GPUæ²¡æœ‰32GBçš„VRAMæ€ä¹ˆåŠï¼Ÿå·²ç»å‘ç°æ¨¡å‹æƒé‡å¯ä»¥é‡åŒ–ä¸º8ä½æˆ–4ä½è€Œä¸ä¼šæ˜¾è‘—é™ä½æ€§èƒ½ï¼ˆå‚è§[Dettmersç­‰äºº](https://arxiv.org/abs/2208.07339)ï¼‰ã€‚æ­£å¦‚æœ€è¿‘çš„[GPTQè®ºæ–‡](https://arxiv.org/abs/2210.17323)æ‰€ç¤ºï¼Œæ¨¡å‹å¯ä»¥é‡åŒ–ä¸º3ä½æˆ–2ä½ï¼Œæ€§èƒ½æŸå¤±æ˜¯å¯ä»¥æ¥å—çš„ğŸ¤¯ã€‚
- en: Without going into too many details, quantization schemes aim at reducing the
    precision of weights while trying to keep the modelâ€™s inference results as accurate
    as possible (*a.k.a* as close as possible to bfloat16). Note that quantization
    works especially well for text generation since all we care about is choosing
    the *set of most likely next tokens* and donâ€™t really care about the exact values
    of the next token *logit* distribution. All that matters is that the next token
    *logit* distribution stays roughly the same so that an `argmax` or `topk` operation
    gives the same results.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ·±å…¥ç»†èŠ‚ï¼Œé‡åŒ–æ–¹æ¡ˆæ—¨åœ¨é™ä½æƒé‡çš„ç²¾åº¦ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿æŒæ¨¡å‹æ¨æ–­ç»“æœçš„å‡†ç¡®æ€§ï¼ˆå³å°½å¯èƒ½æ¥è¿‘bfloat16ï¼‰ã€‚è¯·æ³¨æ„ï¼Œé‡åŒ–åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢ç‰¹åˆ«æœ‰æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬åªå…³å¿ƒé€‰æ‹©*æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°é›†*ï¼Œå¹¶ä¸çœŸæ­£å…³å¿ƒä¸‹ä¸€ä¸ªæ ‡è®°*logit*åˆ†å¸ƒçš„ç¡®åˆ‡å€¼ã€‚é‡è¦çš„æ˜¯ä¸‹ä¸€ä¸ªæ ‡è®°*logit*åˆ†å¸ƒä¿æŒå¤§è‡´ç›¸åŒï¼Œä»¥ä¾¿`argmax`æˆ–`topk`æ“ä½œç»™å‡ºç›¸åŒçš„ç»“æœã€‚
- en: 'There are various quantization techniques, which we wonâ€™t discuss in detail
    here, but in general, all quantization techniques work as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å„ç§é‡åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬è¿™é‡Œä¸ä¼šè¯¦ç»†è®¨è®ºï¼Œä½†æ€»çš„æ¥è¯´ï¼Œæ‰€æœ‰é‡åŒ–æŠ€æœ¯çš„å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼š
- en: Quantize all weights to the target precision
  id: totrans-58
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰æƒé‡é‡åŒ–ä¸ºç›®æ ‡ç²¾åº¦
- en: Load the quantized weights, and pass the input sequence of vectors in bfloat16
    precision
  id: totrans-59
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½é‡åŒ–çš„æƒé‡ï¼Œå¹¶ä»¥bfloat16ç²¾åº¦ä¼ é€’è¾“å…¥åºåˆ—çš„å‘é‡
- en: Dynamically dequantize weights to bfloat16 to perform the computation with their
    input vectors in bfloat16 precision
  id: totrans-60
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ¨æ€å°†æƒé‡å»é‡åŒ–ä¸ºbfloat16ï¼Œä»¥bfloat16ç²¾åº¦æ‰§è¡Œè®¡ç®—
- en: 'In a nutshell, this means that *inputs-weight matrix* multiplications, with<math><semantics><mrow><mi>X</mi></mrow>
    <annotation encoding="application/x-tex">X</annotation></semantics></math> X being
    the *inputs*,<math><semantics><mrow><mi>W</mi></mrow> <annotation encoding="application/x-tex">W</annotation></semantics></math>
    W being a weight matrix and<math><semantics><mrow><mi>Y</mi></mrow> <annotation
    encoding="application/x-tex">Y</annotation></semantics></math> Y being the output:
    <math display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>âˆ—</mo><mi>W</mi></mrow>
    <annotation encoding="application/x-tex">Y = X * W</annotation></semantics></math>
    Y=Xâˆ—W'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œè¿™æ„å‘³ç€*è¾“å…¥-æƒé‡çŸ©é˜µ*ä¹˜æ³•ï¼Œå…¶ä¸­<math><semantics><mrow><mi>X</mi></mrow> <annotation
    encoding="application/x-tex">X</annotation></semantics></math> Xæ˜¯*è¾“å…¥*ï¼Œ<math><semantics><mrow><mi>W</mi></mrow>
    <annotation encoding="application/x-tex">W</annotation></semantics></math> Wæ˜¯æƒé‡çŸ©é˜µï¼Œ<math><semantics><mrow><mi>Y</mi></mrow>
    <annotation encoding="application/x-tex">Y</annotation></semantics></math> Yæ˜¯è¾“å‡ºï¼š<math
    display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>âˆ—</mo><mi>W</mi></mrow>
    <annotation encoding="application/x-tex">Y = X * W</annotation></semantics></math>
    Y=Xâˆ—W
- en: are changed to <math display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>âˆ—</mo><mtext>dequantize</mtext><mo
    stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">Y = X * \text{dequantize}(W)</annotation></semantics></math>
    Y=Xâˆ—dequantize(W)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¢«æ”¹å˜ä¸º<math display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>âˆ—</mo><mtext>dequantize</mtext><mo
    stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">Y = X * \text{dequantize}(W)</annotation></semantics></math>
    Y=Xâˆ—dequantize(W)
- en: for every matrix multiplication. Dequantization and re-quantization is performed
    sequentially for all weight matrices as the inputs run through the network graph.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªçŸ©é˜µä¹˜æ³•ã€‚å½“è¾“å…¥é€šè¿‡ç½‘ç»œå›¾æ—¶ï¼Œæƒé‡çŸ©é˜µçš„åé‡åŒ–å’Œé‡æ–°é‡åŒ–æ˜¯æŒ‰é¡ºåºæ‰§è¡Œçš„ã€‚
- en: Therefore, inference time is often **not** reduced when using quantized weights,
    but rather increases. Enough theory, letâ€™s give it a try! To quantize the weights
    with Transformers, you need to make sure that the [`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes)
    library is installed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½“ä½¿ç”¨é‡åŒ–æƒé‡æ—¶ï¼Œæ¨ç†æ—¶é—´é€šå¸¸**ä¸ä¼š**å‡å°‘ï¼Œè€Œæ˜¯å¢åŠ ã€‚è¶³å¤Ÿçš„ç†è®ºï¼Œè®©æˆ‘ä»¬è¯•ä¸€è¯•ï¼è¦ä½¿ç”¨Transformersé‡åŒ–æƒé‡ï¼Œæ‚¨éœ€è¦ç¡®ä¿å·²å®‰è£…[`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes)åº“ã€‚
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can then load models in 8-bit quantization by simply adding a `load_in_8bit=True`
    flag to `from_pretrained`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç®€å•åœ°åœ¨`from_pretrained`ä¸­æ·»åŠ `load_in_8bit=True`æ ‡å¿—æ¥åŠ è½½8ä½é‡åŒ–çš„æ¨¡å‹ã€‚
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, letâ€™s run our example again and measure the memory usage.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å†æ¬¡è¿è¡Œæˆ‘ä»¬çš„ç¤ºä¾‹å¹¶æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µã€‚
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Output**:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE15]python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024
    / 1024\n[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE15]python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024
    / 1024\n[PRE16]'
- en: Nice, weâ€™re getting the same result as before, so no loss in accuracy! Letâ€™s
    look at how much memory was used this time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸ä¹‹å‰ç›¸åŒçš„ç»“æœï¼Œå› æ­¤åœ¨å‡†ç¡®æ€§ä¸Šæ²¡æœ‰æŸå¤±ï¼è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ¬¡ä½¿ç”¨äº†å¤šå°‘å†…å­˜ã€‚
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Output**:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Significantly less! Weâ€™re down to just a bit over 15 GBs and could therefore
    run this model on consumer GPUs like the 4090. Weâ€™re seeing a very nice gain in
    memory efficiency and more or less no degradation to the modelâ€™s output. However,
    we can also notice a slight slow-down during inference.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾è‘—å‡å°‘ï¼æˆ‘ä»¬åªå‰©ä¸‹ç•¥é«˜äº15GBï¼Œå› æ­¤å¯ä»¥åœ¨åƒ4090è¿™æ ·çš„æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œæ­¤æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å†…å­˜æ•ˆç‡ä¸Šè·å¾—äº†éå¸¸å¥½çš„æ”¶ç›Šï¼Œå‡ ä¹æ²¡æœ‰å¯¹æ¨¡å‹è¾“å‡ºçš„é™çº§ã€‚ä½†æ˜¯ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æˆ‘ä»¬ä¹Ÿå¯ä»¥æ³¨æ„åˆ°ç•¥å¾®å‡é€Ÿã€‚
- en: We delete the models and flush the memory again.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ é™¤æ¨¡å‹å¹¶å†æ¬¡æ¸…ç©ºå†…å­˜ã€‚
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Letâ€™s see what peak GPU memory consumption 4-bit quantization gives. Quantizing
    the model to 4-bit can be done with the same API as before - this time by passing
    `load_in_4bit=True` instead of `load_in_8bit=True`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹4ä½é‡åŒ–å¯¹GPUå†…å­˜æ¶ˆè€—çš„å³°å€¼ã€‚å°†æ¨¡å‹é‡åŒ–ä¸º4ä½å¯ä»¥é€šè¿‡ä¸ä¹‹å‰ç›¸åŒçš„APIå®Œæˆ - è¿™æ¬¡æ˜¯é€šè¿‡ä¼ é€’`load_in_4bit=True`è€Œä¸æ˜¯`load_in_8bit=True`æ¥å®Œæˆã€‚
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Output**:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE22]\ndef bytes_to_gigabytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE22]\ndef bytes_to_gigabytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n[PRE23]'
- en: Weâ€™re almost seeing the same output text as before - just the `python` is missing
    just before the code snippet. Letâ€™s see how much memory was required.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡ ä¹çœ‹åˆ°ä¸ä¹‹å‰ç›¸åŒçš„è¾“å‡ºæ–‡æœ¬ - åªæ˜¯åœ¨ä»£ç ç‰‡æ®µä¹‹å‰ç¼ºå°‘äº†`python`ã€‚è®©æˆ‘ä»¬çœ‹çœ‹éœ€è¦å¤šå°‘å†…å­˜ã€‚
- en: '[PRE24]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Output**:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Just 9.5GB! Thatâ€™s really not a lot for a >15 billion parameter model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰9.5GBï¼å¯¹äºä¸€ä¸ªè¶…è¿‡15äº¿å‚æ•°çš„æ¨¡å‹æ¥è¯´ï¼Œè¿™çœŸçš„ä¸å¤šã€‚
- en: While we see very little degradation in accuracy for our model here, 4-bit quantization
    can in practice often lead to different results compared to 8-bit quantization
    or full `bfloat16` inference. It is up to the user to try it out.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°æ¨¡å‹å‡†ç¡®æ€§å‡ ä¹æ²¡æœ‰ä¸‹é™ï¼Œä½†å®é™…ä¸Šï¼Œ4ä½é‡åŒ–é€šå¸¸ä¼šå¯¼è‡´ä¸8ä½é‡åŒ–æˆ–å®Œæ•´çš„`bfloat16`æ¨ç†ç›¸æ¯”äº§ç”Ÿä¸åŒçš„ç»“æœã€‚è¿™å–å†³äºç”¨æˆ·æ˜¯å¦å°è¯•ã€‚
- en: Also note that inference here was again a bit slower compared to 8-bit quantization
    which is due to the more aggressive quantization method used for 4-bit quantization
    leading to<math><semantics><mrow><mtext>quantize</mtext></mrow> <annotation encoding="application/x-tex">\text{quantize}</annotation></semantics></math>
    quantize and<math><semantics><mrow><mtext>dequantize</mtext></mrow> <annotation
    encoding="application/x-tex">\text{dequantize}</annotation></semantics></math>
    dequantize taking longer during inference.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜è¦æ³¨æ„ï¼Œä¸8ä½é‡åŒ–ç›¸æ¯”ï¼Œè¿™é‡Œçš„æ¨ç†é€Ÿåº¦å†æ¬¡ç¨æ…¢ä¸€äº›ï¼Œè¿™æ˜¯å› ä¸º4ä½é‡åŒ–ä½¿ç”¨äº†æ›´æ¿€è¿›çš„é‡åŒ–æ–¹æ³•ï¼Œå¯¼è‡´åœ¨æ¨ç†è¿‡ç¨‹ä¸­<math><semantics><mrow><mtext>é‡åŒ–</mtext></mrow></semantics></math>å’Œ<math><semantics><mrow><mtext>åé‡åŒ–</mtext></mrow></semantics></math>è¿‡ç¨‹éœ€è¦æ›´é•¿çš„æ—¶é—´ã€‚
- en: '[PRE26]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Overall, we saw that running OctoCoder in 8-bit precision reduced the required
    GPU VRAM from 32G GPU VRAM to only 15GB and running the model in 4-bit precision
    further reduces the required GPU VRAM to just a bit over 9GB.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°åœ¨8ä½ç²¾åº¦ä¸‹è¿è¡ŒOctoCoderå°†æ‰€éœ€çš„GPU VRAMä»32G GPU VRAMå‡å°‘åˆ°ä»…15GBï¼Œå¹¶ä¸”åœ¨4ä½ç²¾åº¦ä¸‹è¿è¡Œæ¨¡å‹è¿›ä¸€æ­¥å°†æ‰€éœ€çš„GPU
    VRAMå‡å°‘åˆ°ç•¥é«˜äº9GBã€‚
- en: 4-bit quantization allows the model to be run on GPUs such as RTX3090, V100,
    and T4 which are quite accessible for most people.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 4ä½é‡åŒ–ä½¿æ¨¡å‹å¯ä»¥åœ¨RTX3090ã€V100å’ŒT4ç­‰GPUä¸Šè¿è¡Œï¼Œè¿™å¯¹å¤§å¤šæ•°äººæ¥è¯´éå¸¸å®¹æ˜“è·å¾—ã€‚
- en: For more information on quantization and to see how one can quantize models
    to require even less GPU VRAM memory than 4-bit, we recommend looking into the
    [`AutoGPTQ`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60)
    implementation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³é‡åŒ–çš„æ›´å¤šä¿¡æ¯ä»¥åŠå¦‚ä½•å°†æ¨¡å‹é‡åŒ–ä»¥ä¾¿æ¯”4ä½æ›´å°‘åœ°ä½¿ç”¨GPU VRAMå†…å­˜ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹[`AutoGPTQ`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60)å®ç°ã€‚
- en: As a conclusion, it is important to remember that model quantization trades
    improved memory efficiency against accuracy and in some cases inference time.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ€åï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼Œæ¨¡å‹é‡åŒ–åœ¨å†…å­˜æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œäº†æƒè¡¡ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ä¼šå¢åŠ æ¨ç†æ—¶é—´ã€‚
- en: If GPU memory is not a constraint for your use case, there is often no need
    to look into quantization. However many GPUs simply canâ€™t run LLMs without quantization
    methods and in this case, 4-bit and 8-bit quantization schemes are extremely useful
    tools.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœGPUå†…å­˜å¯¹æ‚¨çš„ç”¨ä¾‹ä¸æ˜¯é™åˆ¶ï¼Œé€šå¸¸ä¸éœ€è¦è€ƒè™‘é‡åŒ–ã€‚ä½†æ˜¯è®¸å¤šGPUæ— æ³•åœ¨æ²¡æœ‰é‡åŒ–æ–¹æ³•çš„æƒ…å†µä¸‹è¿è¡ŒLLMsï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ4ä½å’Œ8ä½é‡åŒ–æ–¹æ¡ˆæ˜¯éå¸¸æœ‰ç”¨çš„å·¥å…·ã€‚
- en: For more in-detail usage information, we strongly recommend taking a look at
    the [Transformers Quantization Docs](https://huggingface.co/docs/transformers/main_classes/quantization#general-usage).
    Next, letâ€™s look into how we can improve computational and memory efficiency by
    using better algorithms and an improved model architecture.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´è¯¦ç»†çš„ä½¿ç”¨ä¿¡æ¯ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®æŸ¥çœ‹[Transformersé‡åŒ–æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/quantization#general-usage)ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é€šè¿‡ä½¿ç”¨æ›´å¥½çš„ç®—æ³•å’Œæ”¹è¿›çš„æ¨¡å‹æ¶æ„æ¥æé«˜è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚
- en: 2\. Flash Attention
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. é—ªå…‰å…³æ³¨
- en: Todayâ€™s top-performing LLMs share more or less the same fundamental architecture
    that consists of feed-forward layers, activation layers, layer normalization layers,
    and most crucially, self-attention layers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©è¡¨ç°æœ€ä½³çš„LLMsåŸºæœ¬ä¸Šå…±äº«ç›¸åŒçš„åŸºæœ¬æ¶æ„ï¼ŒåŒ…æ‹¬å‰é¦ˆå±‚ã€æ¿€æ´»å±‚ã€å±‚å½’ä¸€åŒ–å±‚ï¼Œä»¥åŠæœ€å…³é”®çš„è‡ªæ³¨æ„åŠ›å±‚ã€‚
- en: Self-attention layers are central to Large Language Models (LLMs) in that they
    enable the model to understand the contextual relationships between input tokens.
    However, the peak GPU memory consumption for self-attention layers grows *quadratically*
    both in compute and memory complexity with number of input tokens (also called
    *sequence length*) that we denote in the following by<math><semantics><mrow><mi>N</mi></mrow>
    <annotation encoding="application/x-tex">N</annotation></semantics></math> N .
    While this is not really noticeable for shorter input sequences (of up to 1000
    input tokens), it becomes a serious problem for longer input sequences (at around
    16000 input tokens).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›å±‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£è¾“å…¥æ ‡è®°ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›å±‚çš„å³°å€¼GPUå†…å­˜æ¶ˆè€—éšç€è¾“å…¥æ ‡è®°æ•°é‡ï¼ˆä¹Ÿç§°ä¸º*åºåˆ—é•¿åº¦*ï¼‰çš„å¢åŠ å‘ˆäºŒæ¬¡å¢é•¿ï¼Œæˆ‘ä»¬åœ¨ä¸‹æ–‡ä¸­ç”¨<math><semantics><mrow><mi>N</mi></mrow>
    <annotation encoding="application/x-tex">N</annotation></semantics></math> N è¡¨ç¤ºã€‚è™½ç„¶å¯¹äºè¾ƒçŸ­çš„è¾“å…¥åºåˆ—ï¼ˆæœ€å¤š1000ä¸ªè¾“å…¥æ ‡è®°ï¼‰è¿™å¹¶ä¸æ˜æ˜¾ï¼Œä½†å¯¹äºè¾ƒé•¿çš„è¾“å…¥åºåˆ—ï¼ˆå¤§çº¦16000ä¸ªè¾“å…¥æ ‡è®°ï¼‰åˆ™æˆä¸ºä¸€ä¸ªä¸¥é‡é—®é¢˜ã€‚
- en: 'Letâ€™s take a closer look. The formula to compute the output<math><semantics><mrow><mi
    mathvariant="bold">O</mi></mrow> <annotation encoding="application/x-tex">\mathbf{O}</annotation></semantics></math>
    O of a self-attention layer for an input<math><semantics><mrow><mi mathvariant="bold">X</mi></mrow>
    <annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
    X of length<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math>
    N is: <math display="block"><semantics><mrow><mtext mathvariant="bold">O</mtext><mo>=</mo><mtext>Attn</mtext><mo
    stretchy="false">(</mo><mi mathvariant="bold">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi
    mathvariant="bold">V</mi><mo>Ã—</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup><mo
    stretchy="false">)</mo><mtext>Â withÂ </mtext><mi mathvariant="bold">Q</mi><mo>=</mo><msub><mi
    mathvariant="bold">W</mi><mi>q</mi></msub><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi
    mathvariant="bold">V</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>v</mi></msub><mi
    mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo>=</mo><msub><mi
    mathvariant="bold">W</mi><mi>k</mi></msub><mi mathvariant="bold">X</mi></mrow>
    <annotation encoding="application/x-tex">\textbf{O} = \text{Attn}(\mathbf{X})
    = \mathbf{V} \times \text{Softmax}(\mathbf{QK}^T) \text{ with } \mathbf{Q} = \mathbf{W}_q
    \mathbf{X}, \mathbf{V} = \mathbf{W}_v \mathbf{X}, \mathbf{K} = \mathbf{W}_k \mathbf{X}</annotation></semantics></math>
    O=Attn(X)=VÃ—Softmax(QKT)Â withÂ Q=Wqâ€‹X,V=Wvâ€‹X,K=Wkâ€‹X <math><semantics><mrow><mi
    mathvariant="bold">X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo
    separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi
    mathvariant="normal">.</mi><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\mathbf{X}
    = (\mathbf{x}_1, ... \mathbf{x}_{N})</annotation></semantics></math> X=(x1â€‹,...xNâ€‹)
    is thereby the input sequence to the attention layer. The projections<math><semantics><mrow><mi
    mathvariant="bold">Q</mi></mrow> <annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics></math>
    Q and<math><semantics><mrow><mi mathvariant="bold">K</mi></mrow> <annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics></math>
    K will each consist of<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math>
    N vectors resulting in the<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi
    mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT being of size<math><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow>
    <annotation encoding="application/x-tex">N^2</annotation></semantics></math> N2
    .'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ã€‚è®¡ç®—è‡ªæ³¨æ„åŠ›å±‚å¯¹äºé•¿åº¦ä¸º<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math>
    N çš„è¾“å…¥<math><semantics><mrow><mi mathvariant="bold">X</mi></mrow> <annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
    X çš„è¾“å‡º<math><semantics><mrow><mi mathvariant="bold">O</mi></mrow> <annotation encoding="application/x-tex">\mathbf{O}</annotation></semantics></math>
    O çš„å…¬å¼æ˜¯ï¼š<math display="block"><semantics><mrow><mtext mathvariant="bold">O</mtext><mo>=</mo><mtext>Attn</mtext><mo
    stretchy="false">(</mo><mi mathvariant="bold">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi
    mathvariant="bold">V</mi><mo>Ã—</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup><mo
    stretchy="false">)</mo><mtext>Â withÂ </mtext><mi mathvariant="bold">Q</mi><mo>=</mo><msub><mi
    mathvariant="bold">W</mi><mi>q</mi></msub><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi
    mathvariant="bold">V</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>v</mi></msub><mi
    mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo>=</mo><msub><mi
    mathvariant="bold">W</mi><mi>k</mi></msub><mi mathvariant="bold">X</mi></mrow>
    <annotation encoding="application/x-tex">\textbf{O} = \text{Attn}(\mathbf{X})
    = \mathbf{V} \times \text{Softmax}(\mathbf{QK}^T) \text{ with } \mathbf{Q} = \mathbf{W}_q
    \mathbf{X}, \mathbf{V} = \mathbf{W}_v \mathbf{X}, \mathbf{K} = \mathbf{W}_k \mathbf{X}</annotation></semantics></math>
    O=Attn(X)=VÃ—Softmax(QKT)Â withÂ Q=Wqâ€‹X,V=Wvâ€‹X,K=Wkâ€‹X <math><semantics><mrow><mi
    mathvariant="bold">X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo
    separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi
    mathvariant="normal">.</mi><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\mathbf{X}
    = (\mathbf{x}_1, ... \mathbf{x}_{N})</annotation></semantics></math> X=(x1â€‹,...xNâ€‹)
    æ˜¯æ³¨æ„åŠ›å±‚çš„è¾“å…¥åºåˆ—ã€‚æŠ•å½±<math><semantics><mrow><mi mathvariant="bold">Q</mi></mrow> <annotation
    encoding="application/x-tex">\mathbf{Q}</annotation></semantics></math> Q å’Œ<math><semantics><mrow><mi
    mathvariant="bold">K</mi></mrow> <annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics></math>
    K å°†åˆ†åˆ«åŒ…å«<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math>
    N ä¸ªå‘é‡ï¼Œå¯¼è‡´<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT çš„å¤§å°ä¸º<math><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow> <annotation
    encoding="application/x-tex">N^2</annotation></semantics></math> N2 ã€‚
- en: LLMs usually have multiple attention heads, thus doing multiple self-attention
    computations in parallel. Assuming, the LLM has 40 attention heads and runs in
    bfloat16 precision, we can calculate the memory requirement to store the<math><semantics><mrow><mi
    mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="bold">T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK^T}</annotation></semantics></math>
    QKT matrices to be<math><semantics><mrow><mn>40</mn><mo>âˆ—</mo><mn>2</mn><mo>âˆ—</mo><msup><mi>N</mi><mn>2</mn></msup></mrow>
    <annotation encoding="application/x-tex">40 * 2 * N^2</annotation></semantics></math>
    40âˆ—2âˆ—N2 bytes. For<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow>
    <annotation encoding="application/x-tex">N=1000</annotation></semantics></math>
    N=1000 only around 50 MB of VRAM are needed, however, for<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>16000</mn></mrow>
    <annotation encoding="application/x-tex">N=16000</annotation></semantics></math>
    N=16000 we would need 19 GB of VRAM, and for<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn><mo
    separator="true">,</mo><mn>000</mn></mrow> <annotation encoding="application/x-tex">N=100,000</annotation></semantics></math>
    N=100,000 we would need almost 1TB just to store the<math><semantics><mrow><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT matrices.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsé€šå¸¸å…·æœ‰å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œå› æ­¤å¯ä»¥å¹¶è¡Œè¿›è¡Œå¤šä¸ªè‡ªæ³¨æ„åŠ›è®¡ç®—ã€‚å‡è®¾LLMæœ‰40ä¸ªæ³¨æ„åŠ›å¤´å¹¶ä¸”ä»¥bfloat16ç²¾åº¦è¿è¡Œï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å­˜å‚¨<math><semantics><mrow><mi
    mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="bold">T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK^T}</annotation></semantics></math>
    QKT çŸ©é˜µæ‰€éœ€çš„å†…å­˜ä¸º<math><semantics><mrow><mn>40</mn><mo>âˆ—</mo><mn>2</mn><mo>âˆ—</mo><msup><mi>N</mi><mn>2</mn></msup></mrow>
    <annotation encoding="application/x-tex">40 * 2 * N^2</annotation></semantics></math>
    40âˆ—2âˆ—N2 å­—èŠ‚ã€‚å¯¹äº<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow> <annotation
    encoding="application/x-tex">N=1000</annotation></semantics></math> N=1000ï¼Œåªéœ€è¦å¤§çº¦50MBçš„VRAMï¼Œç„¶è€Œï¼Œå¯¹äº<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>16000</mn></mrow>
    <annotation encoding="application/x-tex">N=16000</annotation></semantics></math>
    N=16000ï¼Œæˆ‘ä»¬å°†éœ€è¦19GBçš„VRAMï¼Œè€Œå¯¹äº<math><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn><mo
    separator="true">,</mo><mn>000</mn></mrow> <annotation encoding="application/x-tex">N=100,000</annotation></semantics></math>
    N=100,000ï¼Œæˆ‘ä»¬å°†éœ€è¦è¿‘1TBçš„VRAMæ¥å­˜å‚¨<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi
    mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT çŸ©é˜µã€‚
- en: Long story short, the default self-attention algorithm quickly becomes prohibitively
    memory-expensive for large input contexts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿è¯çŸ­è¯´ï¼Œå¯¹äºå¤§å‹è¾“å…¥ä¸Šä¸‹æ–‡æ¥è¯´ï¼Œé»˜è®¤çš„è‡ªæ³¨æ„åŠ›ç®—æ³•å¾ˆå¿«å˜å¾—å†…å­˜æ¶ˆè€—è¿‡é«˜ã€‚
- en: As LLMs improve in text comprehension and generation, they are applied to increasingly
    complex tasks. While models once handled the translation or summarization of a
    few sentences, they now manage entire pages, demanding the capability to process
    extensive input lengths.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€LLMsåœ¨æ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„æ”¹è¿›ï¼Œå®ƒä»¬è¢«åº”ç”¨äºè¶Šæ¥è¶Šå¤æ‚çš„ä»»åŠ¡ã€‚è™½ç„¶æ¨¡å‹æ›¾ç»å¤„ç†å‡ å¥è¯çš„ç¿»è¯‘æˆ–æ€»ç»“ï¼Œç°åœ¨å®ƒä»¬å¯ä»¥å¤„ç†æ•´é¡µçš„å†…å®¹ï¼Œéœ€è¦å¤„ç†å¹¿æ³›çš„è¾“å…¥é•¿åº¦ã€‚
- en: How can we get rid of the exorbitant memory requirements for large input lengths?
    We need a new way to compute the self-attention mechanism that gets rid of the<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">QK^T</annotation></semantics></math>
    QKT matrix. [Tri Dao et al.](https://arxiv.org/abs/2205.14135) developed exactly
    such a new algorithm and called it **Flash Attention**.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•æ‘†è„±å¤§å‹è¾“å…¥é•¿åº¦çš„è¿‡é«˜å†…å­˜éœ€æ±‚ï¼Ÿæˆ‘ä»¬éœ€è¦ä¸€ç§æ–°çš„æ–¹å¼æ¥è®¡ç®—è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘†è„±<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">QK^T</annotation></semantics></math>
    QKT çŸ©é˜µã€‚[Tri Daoç­‰äºº](https://arxiv.org/abs/2205.14135)å¼€å‘äº†ä¸€ç§å…¨æ–°çš„ç®—æ³•ï¼Œç§°ä¹‹ä¸º**Flash Attention**ã€‚
- en: 'In a nutshell, Flash Attention breaks the <math><semantics><mrow><mi mathvariant="bold">V</mi><mo>Ã—</mo><mtext>Softmax</mtext><mo
    stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{V} \times \text{Softmax}(\mathbf{QK}^T</annotation></semantics></math>VÃ—Softmax(QKT)
    computation apart and instead computes smaller chunks of the output by iterating
    over multiple softmax computation steps: <math display="block"><semantics><mrow><msub><mtext
    mathvariant="bold">O</mtext><mi>i</mi></msub><mo>â†</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup><mo>âˆ—</mo><msub><mtext
    mathvariant="bold">O</mtext><mi>i</mi></msub><mo>+</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup><mo>âˆ—</mo><msub><mi
    mathvariant="bold">V</mi><mi>j</mi></msub><mo>Ã—</mo><mtext>Softmax</mtext><mo
    stretchy="false">(</mo><msubsup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mi>T</mi></msubsup><mo stretchy="false">)</mo><mtext>Â forÂ multipleÂ </mtext><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi><mtext>Â iterations</mtext></mrow> <annotation
    encoding="application/x-tex">\textbf{O}_i \leftarrow s^a_{ij} * \textbf{O}_i +
    s^b_{ij} * \mathbf{V}_{j} \times \text{Softmax}(\mathbf{QK}^T_{i,j}) \text{ for
    multiple } i, j \text{ iterations}</annotation></semantics></math> Oiâ€‹â†sijaâ€‹âˆ—Oiâ€‹+sijbâ€‹âˆ—Vjâ€‹Ã—Softmax(QKi,jTâ€‹)Â forÂ multipleÂ i,jÂ iterations'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼ŒFlash Attentionå°†<math><semantics><mrow><mi mathvariant="bold">V</mi><mo>Ã—</mo><mtext>Softmax</mtext><mo
    stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{V} \times \text{Softmax}(\mathbf{QK}^T</annotation></semantics></math>VÃ—Softmax(QKT)è®¡ç®—åˆ†å¼€ï¼Œè€Œæ˜¯é€šè¿‡è¿­ä»£å¤šä¸ªsoftmaxè®¡ç®—æ­¥éª¤æ¥è®¡ç®—è¾“å‡ºçš„è¾ƒå°å—ï¼š<math
    display="block"><semantics><mrow><msub><mtext mathvariant="bold">O</mtext><mi>i</mi></msub><mo>â†</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup><mo>âˆ—</mo><msub><mtext
    mathvariant="bold">O</mtext><mi>i</mi></msub><mo>+</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup><mo>âˆ—</mo><msub><mi
    mathvariant="bold">V</mi><mi>j</mi></msub><mo>Ã—</mo><mtext>Softmax</mtext><mo
    stretchy="false">(</mo><msubsup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mrow><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi></mrow><mi>T</mi></msubsup><mo stretchy="false">)</mo><mtext>Â forÂ multipleÂ </mtext><mi>i</mi><mo
    separator="true">,</mo><mi>j</mi><mtext>Â iterations</mtext></mrow> <annotation
    encoding="application/x-tex">\textbf{O}_i \leftarrow s^a_{ij} * \textbf{O}_i +
    s^b_{ij} * \mathbf{V}_{j} \times \text{Softmax}(\mathbf{QK}^T_{i,j}) \text{ for
    multiple } i, j \text{ iterations}</annotation></semantics></math> Oiâ€‹â†sijaâ€‹âˆ—Oiâ€‹+sijbâ€‹âˆ—Vjâ€‹Ã—Softmax(QKi,jTâ€‹)Â forÂ multipleÂ i,jÂ iterations
- en: with<math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">s^a_{ij}</annotation></semantics></math>
    sijaâ€‹ and<math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">s^b_{ij}</annotation></semantics></math>
    sijbâ€‹ being some softmax normalization statistics that need to be recomputed for
    every<math><semantics><mrow><mi>i</mi></mrow> <annotation encoding="application/x-tex">i</annotation></semantics></math>
    i and<math><semantics><mrow><mi>j</mi></mrow> <annotation encoding="application/x-tex">j</annotation></semantics></math>
    j .
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">s^a_{ij}</annotation></semantics></math>å’Œ<math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">s^b_{ij}</annotation></semantics></math>æ˜¯ä¸€äº›éœ€è¦ä¸ºæ¯ä¸ª<math><semantics><mrow><mi>i</mi></mrow>
    <annotation encoding="application/x-tex">i</annotation></semantics></math>å’Œ<math><semantics><mrow><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">j</annotation></semantics></math>é‡æ–°è®¡ç®—çš„softmaxå½’ä¸€åŒ–ç»Ÿè®¡é‡ã€‚
- en: Please note that the whole Flash Attention is a bit more complex and is greatly
    simplified here as going in too much depth is out of scope for this guide. The
    reader is invited to take a look at the well-written [Flash Attention paper](https://arxiv.org/abs/2205.14135)
    for more details.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ•´ä¸ªFlash Attentionæœ‰ç‚¹å¤æ‚ï¼Œåœ¨è¿™é‡Œå¤§å¤§ç®€åŒ–äº†ï¼Œå› ä¸ºæ·±å…¥è®¨è®ºè¶…å‡ºäº†æœ¬æŒ‡å—çš„èŒƒå›´ã€‚è¯»è€…å¯ä»¥æŸ¥çœ‹å†™å¾—å¾ˆå¥½çš„[Flash Attentionè®ºæ–‡](https://arxiv.org/abs/2205.14135)ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: 'The main takeaway here is:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ä¸»è¦è¦ç‚¹æ˜¯ï¼š
- en: By keeping track of softmax normalization statistics and by using some smart
    mathematics, Flash Attention gives **numerical identical** outputs compared to
    the default self-attention layer at a memory cost that only increases linearly
    with<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math>
    N .
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€šè¿‡è·Ÿè¸ªsoftmaxå½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼Œå¹¶ä½¿ç”¨ä¸€äº›æ™ºèƒ½æ•°å­¦ï¼ŒFlash Attentionç»™å‡ºäº†ä¸é»˜è®¤è‡ªæ³¨æ„åŠ›å±‚**æ•°å€¼ç›¸åŒ**çš„è¾“å‡ºï¼Œè€Œå†…å­˜æˆæœ¬ä»…éš<math><semantics><mrow><mi>N</mi></mrow>
    <annotation encoding="application/x-tex">N</annotation></semantics></math>çº¿æ€§å¢åŠ ã€‚
- en: Looking at the formula, one would intuitively say that Flash Attention must
    be much slower compared to the default self-attention formula as more computation
    needs to be done. Indeed Flash Attention requires more FLOPs compared to normal
    attention as the softmax normalization statistics have to constantly be recomputed
    (see [paper](https://arxiv.org/abs/2205.14135) for more details if interested)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å…¬å¼æ¥çœ‹ï¼Œäººä»¬ç›´è§‰åœ°ä¼šè¯´Flash Attentionå¿…é¡»æ¯”é»˜è®¤çš„è‡ªæ³¨æ„åŠ›å…¬å¼æ…¢å¾—å¤šï¼Œå› ä¸ºéœ€è¦è¿›è¡Œæ›´å¤šçš„è®¡ç®—ã€‚äº‹å®ä¸Šï¼Œä¸æ™®é€šæ³¨æ„åŠ›ç›¸æ¯”ï¼ŒFlash Attentionéœ€è¦æ›´å¤šçš„FLOPsï¼Œå› ä¸ºsoftmaxå½’ä¸€åŒ–ç»Ÿè®¡é‡å¿…é¡»ä¸æ–­é‡æ–°è®¡ç®—ï¼ˆå¦‚æœæ„Ÿå…´è¶£ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/2205.14135)è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼‰
- en: However, Flash Attention is much faster in inference compared to default attention
    which comes from its ability to significantly reduce the demands on the slower,
    high-bandwidth memory of the GPU (VRAM), focusing instead on the faster on-chip
    memory (SRAM).
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¸é»˜è®¤æ³¨æ„åŠ›ç›¸æ¯”ï¼ŒFlash Attentionåœ¨æ¨ç†é€Ÿåº¦ä¸Šè¦å¿«å¾—å¤šï¼Œè¿™æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¯¹GPUï¼ˆVRAMï¼‰è¾ƒæ…¢ã€é«˜å¸¦å®½å†…å­˜çš„éœ€æ±‚ï¼Œè€Œæ˜¯ä¸“æ³¨äºæ›´å¿«çš„ç‰‡ä¸Šå†…å­˜ï¼ˆSRAMï¼‰ã€‚
- en: Essentially, Flash Attention makes sure that all intermediate write and read
    operations can be done using the fast *on-chip* SRAM memory instead of having
    to access the slower VRAM memory to compute the output vector<math><semantics><mrow><mi
    mathvariant="bold">O</mi></mrow> <annotation encoding="application/x-tex">\mathbf{O}</annotation></semantics></math>
    O .
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼ŒFlash Attentionç¡®ä¿æ‰€æœ‰ä¸­é—´å†™å…¥å’Œè¯»å–æ“ä½œéƒ½å¯ä»¥ä½¿ç”¨å¿«é€Ÿçš„*ç‰‡ä¸Š*SRAMå†…å­˜æ¥å®Œæˆï¼Œè€Œæ— éœ€è®¿é—®è¾ƒæ…¢çš„VRAMå†…å­˜æ¥è®¡ç®—è¾“å‡ºå‘é‡<math><semantics><mrow><mi
    mathvariant="bold">O</mi></mrow> <annotation encoding="application/x-tex">\mathbf{O}</annotation></semantics></math>ã€‚
- en: In practice, there is currently absolutely no reason to **not** use Flash Attention
    if available. The algorithm gives mathematically the same outputs, and is both
    faster and more memory-efficient.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå¦‚æœå¯ç”¨ï¼Œç›®å‰ç»å¯¹æ²¡æœ‰ç†ç”±**ä¸**ä½¿ç”¨Flash Attentionã€‚è¯¥ç®—æ³•åœ¨æ•°å­¦ä¸Šç»™å‡ºç›¸åŒçš„è¾“å‡ºï¼Œè€Œä¸”é€Ÿåº¦æ›´å¿«ï¼Œå†…å­˜æ•ˆç‡æ›´é«˜ã€‚
- en: Letâ€™s look at a practical example.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªå®é™…çš„ä¾‹å­ã€‚
- en: Our OctoCoder model now gets a significantly longer input prompt which includes
    a so-called *system prompt*. System prompts are used to steer the LLM into a better
    assistant that is tailored to the usersâ€™ task. In the following, we use a system
    prompt that will make OctoCoder a better coding assistant.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„OctoCoderæ¨¡å‹ç°åœ¨å¾—åˆ°äº†ä¸€ä¸ªæ˜æ˜¾æ›´é•¿çš„è¾“å…¥æç¤ºï¼Œå…¶ä¸­åŒ…æ‹¬æ‰€è°“çš„*ç³»ç»Ÿæç¤º*ã€‚ç³»ç»Ÿæç¤ºç”¨äºå¼•å¯¼LLMæˆä¸ºä¸€ä¸ªæ›´å¥½çš„åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºç”¨æˆ·çš„ä»»åŠ¡å®šåˆ¶ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç³»ç»Ÿæç¤ºï¼Œå°†ä½¿OctoCoderæˆä¸ºä¸€ä¸ªæ›´å¥½çš„ç¼–ç åŠ©æ‰‹ã€‚
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For demonstration purposes, we duplicate the system prompt by ten so that the
    input length is long enough to observe Flash Attentionâ€™s memory savings. We append
    the original text prompt `"Question: Please write a function in Python that transforms
    bytes to Giga bytes.\n\nAnswer: Here"`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬å°†ç³»ç»Ÿæç¤ºå¤åˆ¶åæ¬¡ï¼Œä»¥ä¾¿è¾“å…¥é•¿åº¦è¶³å¤Ÿé•¿ï¼Œä»¥è§‚å¯ŸFlash Attentionçš„å†…å­˜èŠ‚çœã€‚æˆ‘ä»¬é™„åŠ åŸå§‹æ–‡æœ¬æç¤º`"é—®é¢˜ï¼šè¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå°†å­—èŠ‚è½¬æ¢ä¸ºåƒå…†å­—èŠ‚çš„å‡½æ•°ã€‚\n\nç­”æ¡ˆï¼šåœ¨è¿™é‡Œ"`
- en: '[PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We instantiate our model again in bfloat16 precision.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†æ¬¡ä»¥bfloat16ç²¾åº¦å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Letâ€™s now run the model just like before *without Flash Attention* and measure
    the peak GPU memory requirement and inference time.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·è¿è¡Œæ¨¡å‹*ä¸ä½¿ç”¨Flash Attention*ï¼Œå¹¶æµ‹é‡GPUå†…å­˜éœ€æ±‚çš„å³°å€¼å’Œæ¨ç†æ—¶é—´ã€‚
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Output**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE32]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Weâ€™re getting the same output as before, however this time, the model repeats
    the answer multiple times until itâ€™s 60 tokens cut-off. This is not surprising
    as weâ€™ve repeated the system prompt ten times for demonstration purposes and thus
    cued the model to repeat itself.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†ä¸ä¹‹å‰ç›¸åŒçš„è¾“å‡ºï¼Œä½†æ˜¯è¿™ä¸€æ¬¡ï¼Œæ¨¡å‹ä¼šé‡å¤ç­”æ¡ˆå¤šæ¬¡ï¼Œç›´åˆ°è¾¾åˆ°60ä¸ªæ ‡è®°çš„æˆªæ­¢ã€‚è¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºæ¼”ç¤ºç›®çš„é‡å¤äº†ç³»ç»Ÿæç¤ºåæ¬¡ï¼Œä»è€Œæç¤ºæ¨¡å‹é‡å¤è‡ªå·±ã€‚
- en: '**Note** that the system prompt should not be repeated ten times in real-world
    applications - one time is enough!'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯·æ³¨æ„**ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œç³»ç»Ÿæç¤ºä¸åº”é‡å¤åæ¬¡-ä¸€æ¬¡å°±è¶³å¤Ÿäº†ï¼'
- en: Letâ€™s measure the peak GPU memory requirement.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æµ‹é‡GPUå†…å­˜éœ€æ±‚çš„å³°å€¼ã€‚
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Output**:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As we can see the peak GPU memory requirement is now significantly higher than
    in the beginning, which is largely due to the longer input sequence. Also the
    generation takes a little over a minute now.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå³°å€¼GPUå†…å­˜éœ€æ±‚ç°åœ¨æ¯”ä¸€å¼€å§‹æ˜¾ç€æ›´é«˜ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºæ›´é•¿çš„è¾“å…¥åºåˆ—ã€‚æ­¤å¤–ï¼Œç”Ÿæˆç°åœ¨éœ€è¦ä¸€åˆ†é’Ÿå¤šä¸€ç‚¹ã€‚
- en: We call `flush()` to free GPU memory for our next experiment.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è°ƒç”¨`flush()`æ¥é‡Šæ”¾GPUå†…å­˜ï¼Œä»¥ä¾¿è¿›è¡Œä¸‹ä¸€ä¸ªå®éªŒã€‚
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: For comparison, letâ€™s run the same function, but enable Flash Attention instead.
    To do so, we convert the model to [BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)
    and by doing so enabling PyTorchâ€™s [SDPA self-attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)
    which in turn is able to use Flash Attention.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œè®©æˆ‘ä»¬è¿è¡Œç›¸åŒçš„å‡½æ•°ï¼Œä½†å¯ç”¨Flash Attentionã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è½¬æ¢ä¸º[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)ï¼Œä»è€Œå¯ç”¨PyTorchçš„[SDPAè‡ªæ³¨æ„åŠ›](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼Œè¿›è€Œèƒ½å¤Ÿä½¿ç”¨Flash
    Attentionã€‚
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now we run the exact same code snippet as before and under the hood Transformers
    will make use of Flash Attention.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¿è¡Œä¸ä¹‹å‰å®Œå…¨ç›¸åŒçš„ä»£ç ç‰‡æ®µï¼Œåœ¨åº•å±‚Transformerså°†åˆ©ç”¨Flash Attentionã€‚
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '**Output**:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Weâ€™re getting the exact same result as before, but can observe a very significant
    speed-up thanks to Flash Attention.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†ä¸ä¹‹å‰å®Œå…¨ç›¸åŒçš„ç»“æœï¼Œä½†ç”±äºFlash Attentionï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°éå¸¸æ˜¾è‘—çš„åŠ é€Ÿã€‚
- en: Letâ€™s measure the memory consumption one last time.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æœ€åä¸€æ¬¡æµ‹é‡å†…å­˜æ¶ˆè€—ã€‚
- en: '[PRE39]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Output**:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE40]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: And weâ€™re almost back to our original 29GB peak GPU memory from the beginning.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡ ä¹å›åˆ°äº†æœ€åˆçš„29GB GPUå†…å­˜å³°å€¼ã€‚
- en: We can observe that we only use roughly 100MB more GPU memory when passing a
    very long input sequence with Flash Attention compared to passing a short input
    sequence as done in the beginning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä¸ä¸€å¼€å§‹ä¼ é€’çŸ­è¾“å…¥åºåˆ—ç›¸æ¯”ï¼Œä½¿ç”¨Flash Attentionä¼ é€’éå¸¸é•¿çš„è¾“å…¥åºåˆ—æ—¶ï¼Œæˆ‘ä»¬åªä½¿ç”¨äº†å¤§çº¦å¤š100MBçš„GPUå†…å­˜ã€‚
- en: '[PRE41]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: For more information on how to use Flash Attention, please have a look at [this
    doc page](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å¦‚ä½•ä½¿ç”¨Flash Attentionçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ­¤æ–‡æ¡£é¡µé¢](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2)ã€‚
- en: 3\. Architectural Innovations
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. æ¶æ„åˆ›æ–°
- en: 'So far we have looked into improving computational and memory efficiency by:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ç ”ç©¶äº†é€šè¿‡ä»¥ä¸‹æ–¹å¼æé«˜è®¡ç®—å’Œå†…å­˜æ•ˆç‡ï¼š
- en: Casting the weights to a lower precision format
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æƒé‡è½¬æ¢ä¸ºè¾ƒä½ç²¾åº¦æ ¼å¼
- en: Replacing the self-attention algorithm with a more memory- and compute efficient
    version
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨æ›´èŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºçš„ç‰ˆæœ¬æ›¿æ¢è‡ªæ³¨æ„åŠ›ç®—æ³•
- en: 'Letâ€™s now look into how we can change the architecture of an LLM so that it
    is most effective and efficient for task that require long text inputs, *e.g.*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ”¹å˜LLMçš„æ¶æ„ï¼Œä½¿å…¶å¯¹éœ€è¦é•¿æ–‡æœ¬è¾“å…¥çš„ä»»åŠ¡æœ€æœ‰æ•ˆå’Œé«˜æ•ˆï¼Œä¾‹å¦‚ï¼š
- en: Retrieval augmented Questions Answering,
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ£€ç´¢å¢å¼ºé—®ç­”ï¼Œ
- en: Summarization,
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»ç»“ï¼Œ
- en: Chat
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠå¤©
- en: Note that *chat* not only requires the LLM to handle long text inputs, but it
    also necessitates that the LLM is able to efficiently handle the back-and-forth
    dialogue between user and assistant (such as ChatGPT).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œ*chat*ä¸ä»…è¦æ±‚LLMå¤„ç†é•¿æ–‡æœ¬è¾“å…¥ï¼Œè¿˜è¦æ±‚LLMèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ç”¨æˆ·å’ŒåŠ©æ‰‹ä¹‹é—´çš„æ¥å›å¯¹è¯ï¼ˆä¾‹å¦‚ChatGPTï¼‰ã€‚
- en: Once trained, the fundamental LLM architecture is difficult to change, so it
    is important to make considerations about the LLMâ€™s tasks beforehand and accordingly
    optimize the modelâ€™s architecture. There are two important components of the model
    architecture that quickly become memory and/or performance bottlenecks for large
    input sequences.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒåŸºæœ¬çš„LLMæ¶æ„å¾ˆéš¾æ”¹å˜ï¼Œå› æ­¤åœ¨äº‹å…ˆè€ƒè™‘LLMçš„ä»»åŠ¡å¹¶ç›¸åº”åœ°ä¼˜åŒ–æ¨¡å‹æ¶æ„éå¸¸é‡è¦ã€‚æ¨¡å‹æ¶æ„çš„ä¸¤ä¸ªé‡è¦ç»„ä»¶å¾ˆå¿«æˆä¸ºå¤§å‹è¾“å…¥åºåˆ—çš„å†…å­˜å’Œ/æˆ–æ€§èƒ½ç“¶é¢ˆã€‚
- en: The positional embeddings
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ç½®åµŒå…¥
- en: The key-value cache
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é”®-å€¼ç¼“å­˜
- en: Letâ€™s go over each component in more detail
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°è®¨è®ºæ¯ä¸ªç»„ä»¶
- en: 3.1 Improving positional embeddings of LLMs
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 æ”¹è¿›LLMçš„ä½ç½®åµŒå…¥
- en: 'Self-attention puts each token in relation to each otherâ€™s tokens. As an example,
    the<math><semantics><mrow><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\text{Softmax}(\mathbf{QK}^T)</annotation></semantics></math>
    Softmax(QKT) matrix of the text input sequence *â€œHelloâ€, â€œIâ€, â€œloveâ€, â€œyouâ€* could
    look as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›å°†æ¯ä¸ªæ ‡è®°ä¸å…¶ä»–æ ‡è®°ç›¸å…³è”ã€‚ä¾‹å¦‚ï¼Œæ–‡æœ¬è¾“å…¥åºåˆ—çš„Softmax(QKT)çŸ©é˜µ*â€œHelloâ€, â€œIâ€, â€œloveâ€, â€œyouâ€*å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/a42237d68d8acd5442beddab2228c8d5.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a42237d68d8acd5442beddab2228c8d5.png)'
- en: Each word token is given a probability mass at which it attends all other word
    tokens and, therefore is put into relation with all other word tokens. E.g. the
    word *â€œloveâ€* attends to the word *â€œHelloâ€* with 5%, to *â€œIâ€* with 30%, and to
    itself with 65%.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå•è¯æ ‡è®°éƒ½è¢«èµ‹äºˆä¸€ä¸ªæ¦‚ç‡è´¨é‡ï¼Œç”¨äºå…³æ³¨æ‰€æœ‰å…¶ä»–å•è¯æ ‡è®°ï¼Œå› æ­¤ä¸æ‰€æœ‰å…¶ä»–å•è¯æ ‡è®°ç›¸å…³è”ã€‚ä¾‹å¦‚ï¼Œå•è¯*â€œloveâ€*å…³æ³¨å•è¯*â€œHelloâ€*çš„æ¦‚ç‡ä¸º5%ï¼Œå…³æ³¨*â€œIâ€*çš„æ¦‚ç‡ä¸º30%ï¼Œè‡ªèº«çš„æ¦‚ç‡ä¸º65%ã€‚
- en: A LLM based on self-attention, but without position embeddings would have great
    difficulties in understanding the positions of the text inputs to each other.
    This is because the probability score computed by<math><semantics><mrow><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT relates each word token to each other word token in<math><semantics><mrow><mi>O</mi><mo
    stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">O(1)</annotation></semantics></math> O(1) computations
    regardless of their relative positional distance to each other. Therefore, for
    the LLM without position embeddings each token appears to have the same distance
    to all other tokens, *e.g.* differentiating between *â€œHello I love youâ€* and *â€œYou
    love I helloâ€* would be very challenging.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè‡ªæ³¨æ„åŠ›çš„LLMï¼Œä½†æ²¡æœ‰ä½ç½®åµŒå…¥ï¼Œå°†åœ¨ç†è§£æ–‡æœ¬è¾“å…¥ä¹‹é—´çš„ä½ç½®æ–¹é¢é‡åˆ°å¾ˆå¤§å›°éš¾ã€‚è¿™æ˜¯å› ä¸ºç”±<math><semantics><mrow><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT è®¡ç®—çš„æ¦‚ç‡åˆ†æ•°å°†æ¯ä¸ªå•è¯æ ‡è®°ä¸å…¶ä»–å•è¯æ ‡è®°åœ¨<math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">O(1)</annotation></semantics></math>
    O(1) è®¡ç®—ä¸­ç›¸å…³è”ï¼Œè€Œä¸è€ƒè™‘å®ƒä»¬ä¹‹é—´çš„ç›¸å¯¹ä½ç½®è·ç¦»ã€‚å› æ­¤ï¼Œå¯¹äºæ²¡æœ‰ä½ç½®åµŒå…¥çš„LLMï¼Œæ¯ä¸ªæ ‡è®°ä¼¼ä¹ä¸æ‰€æœ‰å…¶ä»–æ ‡è®°å…·æœ‰ç›¸åŒçš„è·ç¦»ï¼Œä¾‹å¦‚ï¼ŒåŒºåˆ†â€œä½ å¥½ æˆ‘çˆ±ä½ â€å’Œâ€œä½ çˆ±æˆ‘
    ä½ å¥½â€å°†ä¼šéå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: For the LLM to understand sentence order, an additional *cue* is needed and
    is usually applied in the form of *positional encodings* (or also called *positional
    embeddings*). Positional encodings, encode the position of each token into a numerical
    presentation that the LLM can leverage to better understand sentence order.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®©LLMç†è§£å¥å­é¡ºåºï¼Œéœ€è¦é¢å¤–çš„*æç¤º*ï¼Œé€šå¸¸ä»¥*ä½ç½®ç¼–ç *ï¼ˆä¹Ÿç§°ä¸º*ä½ç½®åµŒå…¥*ï¼‰çš„å½¢å¼åº”ç”¨ã€‚ä½ç½®ç¼–ç å°†æ¯ä¸ªæ ‡è®°çš„ä½ç½®ç¼–ç ä¸ºLLMå¯ä»¥åˆ©ç”¨çš„æ•°å€¼è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°ç†è§£å¥å­é¡ºåºã€‚
- en: The authors of the [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762)
    paper introduced sinusoidal positional embeddings<math><semantics><mrow><mi mathvariant="bold">P</mi><mo>=</mo><msub><mi
    mathvariant="bold">p</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo
    separator="true">,</mo><msub><mi mathvariant="bold">p</mi><mi>N</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{P} = \mathbf{p}_1, \ldots, \mathbf{p}_N</annotation></semantics></math>
    P=p1â€‹,â€¦,pNâ€‹ . where each vector<math><semantics><mrow><msub><mi mathvariant="bold">p</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{p}_i</annotation></semantics></math>
    piâ€‹ is computed as a sinusoidal function of its position<math><semantics><mrow><mi>i</mi></mrow>
    <annotation encoding="application/x-tex">i</annotation></semantics></math> i .
    The positional encodings are then simply added to the input sequence vectors<math><semantics><mrow><mover
    accent="true"><mi mathvariant="bold">X</mi><mo>^</mo></mover><mo>=</mo><msub><mover
    accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mn>1</mn></msub><mo
    separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mover accent="true"><mi
    mathvariant="bold">x</mi><mo>^</mo></mover><mi>N</mi></msub></mrow> <annotation
    encoding="application/x-tex">\mathbf{\hat{X}} = \mathbf{\hat{x}}_1, \ldots, \mathbf{\hat{x}}_N</annotation></semantics></math>
    X^=x^1â€‹,â€¦,x^Nâ€‹ =<math><semantics><mrow><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo>+</mo><msub><mi
    mathvariant="bold">p</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo
    separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold">p</mi><mi>N</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{x}_1
    + \mathbf{p}_1, \ldots, \mathbf{x}_N + \mathbf{p}_N</annotation></semantics></math>
    x1â€‹+p1â€‹,â€¦,xNâ€‹+pNâ€‹ thereby cueing the model to better learn sentence order.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[*æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯ä½ æ‰€éœ€è¦çš„*](https://arxiv.org/abs/1706.03762)è®ºæ–‡çš„ä½œè€…ä»¬å¼•å…¥äº†æ­£å¼¦ä½ç½®åµŒå…¥<math><semantics><mrow><mi
    mathvariant="bold">P</mi><mo>=</mo><msub><mi mathvariant="bold">p</mi><mn>1</mn></msub><mo
    separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">p</mi><mi>N</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{P} = \mathbf{p}_1, \ldots, \mathbf{p}_N</annotation></semantics></math>
    P=p1â€‹,â€¦,pNâ€‹ ã€‚å…¶ä¸­æ¯ä¸ªå‘é‡<math><semantics><mrow><msub><mi mathvariant="bold">p</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{p}_i</annotation></semantics></math>
    piâ€‹ æ˜¯æ ¹æ®å…¶ä½ç½®<math><semantics><mrow><mi>i</mi></mrow> <annotation encoding="application/x-tex">i</annotation></semantics></math>
    i è®¡ç®—çš„æ­£å¼¦å‡½æ•°ã€‚ç„¶åå°†ä½ç½®ç¼–ç ç®€å•åœ°æ·»åŠ åˆ°è¾“å…¥åºåˆ—å‘é‡ä¸­<math><semantics><mrow><mover accent="true"><mi
    mathvariant="bold">X</mi><mo>^</mo></mover><mo>=</mo><msub><mover accent="true"><mi
    mathvariant="bold">x</mi><mo>^</mo></mover><mn>1</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo
    separator="true">,</mo><msub><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>N</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{\hat{X}} = \mathbf{\hat{x}}_1,
    \ldots, \mathbf{\hat{x}}_N</annotation></semantics></math> X^=x^1â€‹,â€¦,x^Nâ€‹ =<math><semantics><mrow><msub><mi
    mathvariant="bold">x</mi><mn>1</mn></msub><mo>+</mo><msub><mi mathvariant="bold">p</mi><mn>1</mn></msub><mo
    separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold">p</mi><mi>N</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{x}_1
    + \mathbf{p}_1, \ldots, \mathbf{x}_N + \mathbf{p}_N</annotation></semantics></math>
    x1â€‹+p1â€‹,â€¦,xNâ€‹+pNâ€‹ ä»è€Œæç¤ºæ¨¡å‹æ›´å¥½åœ°å­¦ä¹ å¥å­é¡ºåºã€‚'
- en: Instead of using fixed position embeddings, others (such as [Devlin et al.](https://arxiv.org/abs/1810.04805))
    used learned positional encodings for which the positional embeddings<math><semantics><mrow><mi
    mathvariant="bold">P</mi></mrow> <annotation encoding="application/x-tex">\mathbf{P}</annotation></semantics></math>
    P are learned during training.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–äººï¼ˆä¾‹å¦‚[Devlinç­‰äºº](https://arxiv.org/abs/1810.04805)ï¼‰ä½¿ç”¨äº†å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œè€Œä¸æ˜¯å›ºå®šçš„ä½ç½®åµŒå…¥ï¼Œè¿™äº›ä½ç½®åµŒå…¥åœ¨è®­ç»ƒæœŸé—´è¿›è¡Œå­¦ä¹ ã€‚
- en: 'Sinusoidal and learned position embeddings used to be the predominant methods
    to encode sentence order into LLMs, but a couple of problems related to these
    positional encodings were found:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¼¦å’Œå­¦ä¹ ä½ç½®åµŒå…¥æ›¾ç»æ˜¯å°†å¥å­é¡ºåºç¼–ç åˆ°LLMä¸­çš„ä¸»è¦æ–¹æ³•ï¼Œä½†å‘ç°äº†ä¸è¿™äº›ä½ç½®ç¼–ç ç›¸å…³çš„ä¸€äº›é—®é¢˜ï¼š
- en: Sinusoidal and learned position embeddings are both absolute positional embeddings,
    *i.e.* encoding a unique embedding for each position id:<math><semantics><mrow><mn>0</mn><mo
    separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><mi>N</mi></mrow>
    <annotation encoding="application/x-tex">0, \ldots, N</annotation></semantics></math>
    0,â€¦,N . As shown by [Huang et al.](https://arxiv.org/abs/2009.13658) and [Su et
    al.](https://arxiv.org/abs/2104.09864), absolute positional embeddings lead to
    poor LLM performance for long text inputs. For long text inputs, it is advantageous
    if the model learns the relative positional distance input tokens have to each
    other instead of their absolute position.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­£å¼¦å’Œå­¦ä¹ ä½ç½®åµŒå…¥éƒ½æ˜¯ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå³ä¸ºæ¯ä¸ªä½ç½®idç¼–ç ä¸€ä¸ªå”¯ä¸€çš„åµŒå…¥ï¼š<math><semantics><mrow><mn>0</mn><mo separator="true">,</mo><mo>â€¦</mo><mo
    separator="true">,</mo><mi>N</mi></mrow> <annotation encoding="application/x-tex">0,
    \ldots, N</annotation></semantics></math> 0,â€¦,Nã€‚æ­£å¦‚[Huangç­‰äºº](https://arxiv.org/abs/2009.13658)å’Œ[è‹ç­‰äºº](https://arxiv.org/abs/2104.09864)æ‰€ç¤ºï¼Œç»å¯¹ä½ç½®åµŒå…¥å¯¼è‡´é•¿æ–‡æœ¬è¾“å…¥çš„LLMæ€§èƒ½è¾ƒå·®ã€‚å¯¹äºé•¿æ–‡æœ¬è¾“å…¥ï¼Œå¦‚æœæ¨¡å‹å­¦ä¹ è¾“å…¥æ ‡è®°ä¹‹é—´çš„ç›¸å¯¹ä½ç½®è·ç¦»è€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ï¼Œå°†æ˜¯æœ‰åˆ©çš„ã€‚
- en: When using learned position embeddings, the LLM has to be trained on a fixed
    input length<math><semantics><mrow><mi>N</mi></mrow> <annotation encoding="application/x-tex">N</annotation></semantics></math>
    N, which makes it difficult to extrapolate to an input length longer than what
    it was trained on.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨å­¦ä¹ ä½ç½®åµŒå…¥æ—¶ï¼ŒLLMå¿…é¡»åœ¨å›ºå®šçš„è¾“å…¥é•¿åº¦<math><semantics><mrow><mi>N</mi></mrow> <annotation
    encoding="application/x-tex">N</annotation></semantics></math> Nä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™ä½¿å¾—éš¾ä»¥æ¨å¹¿åˆ°æ¯”å…¶è®­ç»ƒé•¿åº¦æ›´é•¿çš„è¾“å…¥ã€‚
- en: 'Recently, relative positional embeddings that can tackle the above mentioned
    problems have become more popular, most notably:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œèƒ½å¤Ÿè§£å†³ä¸Šè¿°é—®é¢˜çš„ç›¸å¯¹ä½ç½®åµŒå…¥å˜å¾—æ›´åŠ æµè¡Œï¼Œå…¶ä¸­æœ€è‘—åçš„æ˜¯ï¼š
- en: '[Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰](https://arxiv.org/abs/2104.09864)'
- en: '[ALiBi](https://arxiv.org/abs/2108.12409)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ALiBi](https://arxiv.org/abs/2108.12409)'
- en: Both *RoPE* and *ALiBi* argue that itâ€™s best to cue the LLM about sentence order
    directly in the self-attention algorithm as itâ€™s there that word tokens are put
    into relation with each other. More specifically, sentence order should be cued
    by modifying the<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi
    mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT computation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*RoPE*å’Œ*ALiBi*éƒ½è®¤ä¸ºæœ€å¥½ç›´æ¥åœ¨è‡ªæ³¨æ„åŠ›ç®—æ³•ä¸­æç¤ºLLMå…³äºå¥å­é¡ºåºï¼Œå› ä¸ºåœ¨é‚£é‡Œå•è¯æ ‡è®°å½¼æ­¤å…³è”ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå¥å­é¡ºåºåº”è¯¥é€šè¿‡ä¿®æ”¹<math><semantics><mrow><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKTè®¡ç®—æ¥æç¤ºã€‚'
- en: 'Without going into too many details, *RoPE* notes that positional information
    can be encoded into query-key pairs, *e.g.*<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    qiâ€‹ and<math><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{x}_j</annotation></semantics></math>
    xjâ€‹ by rotating each vector by an angle<math><semantics><mrow><mi>Î¸</mi><mo>âˆ—</mo><mi>i</mi></mrow>
    <annotation encoding="application/x-tex">\theta * i</annotation></semantics></math>
    Î¸âˆ—i and<math><semantics><mrow><mi>Î¸</mi><mo>âˆ—</mo><mi>j</mi></mrow> <annotation
    encoding="application/x-tex">\theta * j</annotation></semantics></math> Î¸âˆ—j respectively
    with<math><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">i, j</annotation></semantics></math>
    i,j describing each vectors sentence position: <math display="block"><semantics><mrow><msubsup><mover
    accent="true"><mi mathvariant="bold">q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><msub><mover
    accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>j</mi></msub><mo>=</mo><msubsup><mi
    mathvariant="bold">q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi mathvariant="bold">R</mi><mrow><mi>Î¸</mi><mo
    separator="true">,</mo><mi>i</mi><mo>âˆ’</mo><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub><mi
    mathvariant="normal">.</mi></mrow> <annotation encoding="application/x-tex">\mathbf{\hat{q}}_i^T
    \mathbf{\hat{x}}_j = \mathbf{{q}}_i^T \mathbf{R}_{\theta, i -j} \mathbf{{x}}_j.</annotation></semantics></math>
    q^â€‹iTâ€‹x^jâ€‹=qiTâ€‹RÎ¸,iâˆ’jâ€‹xjâ€‹. <math><semantics><mrow><msub><mi mathvariant="bold">R</mi><mrow><mi>Î¸</mi><mo
    separator="true">,</mo><mi>i</mi><mo>âˆ’</mo><mi>j</mi></mrow></msub></mrow> <annotation
    encoding="application/x-tex">\mathbf{R}_{\theta, i - j}</annotation></semantics></math>
    RÎ¸,iâˆ’jâ€‹ thereby represents a rotational matrix.<math><semantics><mrow><mi>Î¸</mi></mrow>
    <annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    Î¸ is *not* learned during training, but instead set to a pre-defined value that
    depends on the maximum input sequence length during training.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¯¦ç»†è®¨è®ºï¼Œ*RoPE*æŒ‡å‡ºä½ç½®ä¿¡æ¯å¯ä»¥è¢«ç¼–ç åˆ°æŸ¥è¯¢-é”®å¯¹ä¸­ï¼Œä¾‹å¦‚ï¼š<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    å’Œ<math><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{x}_j</annotation></semantics></math>
    ï¼Œé€šè¿‡å°†æ¯ä¸ªå‘é‡æ—‹è½¬ä¸€ä¸ªè§’åº¦<math><semantics><mrow><mi>Î¸</mi><mo>âˆ—</mo><mi>i</mi></mrow> <annotation
    encoding="application/x-tex">\theta * i</annotation></semantics></math> å’Œ<math><semantics><mrow><mi>Î¸</mi><mo>âˆ—</mo><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">\theta * j</annotation></semantics></math>
    ï¼Œå…¶ä¸­<math><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">i, j</annotation></semantics></math>
    æè¿°æ¯ä¸ªå‘é‡çš„å¥å­ä½ç½®ï¼š<math display="block"><semantics><mrow><msubsup><mover accent="true"><mi
    mathvariant="bold">q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><msub><mover
    accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>j</mi></msub><mo>=</mo><msubsup><mi
    mathvariant="bold">q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi mathvariant="bold">R</mi><mrow><mi>Î¸</mi><mo
    separator="true">,</mo><mi>i</mi><mo>âˆ’</mo><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub><mi
    mathvariant="normal">.</mi></mrow> <annotation encoding="application/x-tex">\mathbf{\hat{q}}_i^T
    \mathbf{\hat{x}}_j = \mathbf{{q}}_i^T \mathbf{R}_{\theta, i -j} \mathbf{{x}}_j.</annotation></semantics></math>
    q^â€‹iTâ€‹x^jâ€‹=qiTâ€‹RÎ¸,iâˆ’jâ€‹xjâ€‹ã€‚ <math><semantics><mrow><msub><mi mathvariant="bold">R</mi><mrow><mi>Î¸</mi><mo
    separator="true">,</mo><mi>i</mi><mo>âˆ’</mo><mi>j</mi></mrow></msub></mrow> <annotation
    encoding="application/x-tex">\mathbf{R}_{\theta, i - j}</annotation></semantics></math>
    RÎ¸,iâˆ’jâ€‹ä»£è¡¨ä¸€ä¸ªæ—‹è½¬çŸ©é˜µã€‚<math><semantics><mrow><mi>Î¸</mi></mrow> <annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    Î¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­*ä¸ä¼š*è¢«å­¦ä¹ ï¼Œè€Œæ˜¯è®¾ç½®ä¸ºä¸€ä¸ªé¢„å®šä¹‰çš„å€¼ï¼Œè¯¥å€¼å–å†³äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€å¤§è¾“å…¥åºåˆ—é•¿åº¦ã€‚
- en: By doing so, the propability score between<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    qiâ€‹ and<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>j</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_j</annotation></semantics></math>
    qjâ€‹ is only affected if<math><semantics><mrow><mi>i</mi><mo mathvariant="normal">â‰ </mo><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">i \ne j</annotation></semantics></math>
    iî€ =j and solely depends on the relative distance<math><semantics><mrow><mi>i</mi><mo>âˆ’</mo><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">i - j</annotation></semantics></math>
    iâˆ’j regardless of each vectorâ€™s specific positions<math><semantics><mrow><mi>i</mi></mrow>
    <annotation encoding="application/x-tex">i</annotation></semantics></math> i and<math><semantics><mrow><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">j</annotation></semantics></math> j .
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™æ ·åšï¼Œ<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    å’Œ<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>j</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_j</annotation></semantics></math>
    ä¹‹é—´çš„æ¦‚ç‡åˆ†æ•°åªæœ‰åœ¨<math><semantics><mrow><mi>i</mi><mo mathvariant="normal">â‰ </mo><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">i \ne j</annotation></semantics></math>
    æ—¶æ‰ä¼šå—åˆ°å½±å“ï¼Œå¹¶ä¸”ä»…å–å†³äºç›¸å¯¹è·ç¦»<math><semantics><mrow><mi>i</mi><mo>âˆ’</mo><mi>j</mi></mrow>
    <annotation encoding="application/x-tex">i - j</annotation></semantics></math>
    ï¼Œè€Œä¸è€ƒè™‘æ¯ä¸ªå‘é‡çš„å…·ä½“ä½ç½®<math><semantics><mrow><mi>i</mi></mrow> <annotation encoding="application/x-tex">i</annotation></semantics></math>
    å’Œ<math><semantics><mrow><mi>j</mi></mrow> <annotation encoding="application/x-tex">j</annotation></semantics></math>
    ã€‚
- en: '*RoPE* is used in multiple of todayâ€™s most important LLMs, such as:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*RoPE*è¢«ç”¨åœ¨å½“ä»Šä¸€äº›æœ€é‡è¦çš„LLMä¸­ï¼Œä¾‹å¦‚ï¼š'
- en: '[**Falcon**](https://huggingface.co/tiiuae/falcon-40b)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**çŒé¹°**](https://huggingface.co/tiiuae/falcon-40b)'
- en: '[**Llama**](https://arxiv.org/abs/2302.13971)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**å¤§ç¾Šé©¼**](https://arxiv.org/abs/2302.13971)'
- en: '[**PaLM**](https://arxiv.org/abs/2204.02311)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PaLM**](https://arxiv.org/abs/2204.02311)'
- en: As an alternative, *ALiBi* proposes a much simpler relative position encoding
    scheme. The relative distance that input tokens have to each other is added as
    a negative integer scaled by a pre-defined value `m` to each query-key entry of
    the<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT matrix right before the softmax computation.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œ*ALiBi* æå‡ºäº†ä¸€ç§æ›´ç®€å•çš„ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ¡ˆã€‚è¾“å…¥ä»¤ç‰Œä¹‹é—´çš„ç›¸å¯¹è·ç¦»è¢«æ·»åŠ ä¸ºè´Ÿæ•´æ•°ï¼Œä¹˜ä»¥é¢„å®šä¹‰å€¼ `m`ï¼Œå¹¶æ·»åŠ åˆ° softmax
    è®¡ç®—ä¹‹å‰çš„<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT çŸ©é˜µçš„æ¯ä¸ªæŸ¥è¯¢-é”®æ¡ç›®ä¸­ã€‚
- en: '![](../Images/b4adb6fd6b6aaa790ed5e4200191e405.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4adb6fd6b6aaa790ed5e4200191e405.png)'
- en: As shown in the [ALiBi](https://arxiv.org/abs/2108.12409) paper, this simple
    relative positional encoding allows the model to retain a high performance even
    at very long text input sequences.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚[ALiBi](https://arxiv.org/abs/2108.12409) è®ºæ–‡æ‰€ç¤ºï¼Œè¿™ç§ç®€å•çš„ç›¸å¯¹ä½ç½®ç¼–ç ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨éå¸¸é•¿çš„æ–‡æœ¬è¾“å…¥åºåˆ—ä¸­ä¿æŒé«˜æ€§èƒ½ã€‚
- en: '*ALiBi* is used in multiple of todayâ€™s most important LLMs, such as:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*ALiBi* åœ¨å½“ä»Šä¸€äº›æœ€é‡è¦çš„ LLM ä¸­ä½¿ç”¨ï¼Œä¾‹å¦‚ï¼š'
- en: '[**MPT**](https://huggingface.co/mosaicml/mpt-30b)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MPT**](https://huggingface.co/mosaicml/mpt-30b)'
- en: '[**BLOOM**](https://huggingface.co/bigscience/bloom)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**BLOOM**](https://huggingface.co/bigscience/bloom)'
- en: Both *RoPE* and *ALiBi* position encodings can extrapolate to input lengths
    not seen during training whereas it has been shown that extrapolation works much
    better out-of-the-box for *ALiBi* as compared to *RoPE*. For ALiBi, one simply
    increases the values of the lower triangular position matrix to match the length
    of the input sequence. For *RoPE*, keeping the same<math><semantics><mrow><mi>Î¸</mi></mrow>
    <annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    Î¸ that was used during training leads to poor results when passing text inputs
    much longer than those seen during training, *c.f* [Press et al.](https://arxiv.org/abs/2108.12409).
    However, the community has found a couple of effective tricks that adapt<math><semantics><mrow><mi>Î¸</mi></mrow>
    <annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    Î¸, thereby allowing *RoPE* position embeddings to work well for extrapolated text
    input sequences (see [here](https://github.com/huggingface/transformers/pull/24653)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*RoPE* å’Œ *ALiBi* ä½ç½®ç¼–ç éƒ½å¯ä»¥å¤–æ¨åˆ°è®­ç»ƒä¸­æœªè§è¿‡çš„è¾“å…¥é•¿åº¦ï¼Œç„¶è€Œå·²ç»è¯æ˜ç›¸å¯¹äº *RoPE*ï¼Œå¤–æ¨å¯¹äº *ALiBi* æ¥è¯´æ›´å®¹æ˜“ã€‚å¯¹äº
    ALiBiï¼Œåªéœ€å¢åŠ ä¸‹ä¸‰è§’ä½ç½®çŸ©é˜µçš„å€¼ä»¥åŒ¹é…è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚å¯¹äº *RoPE*ï¼Œä¿æŒè®­ç»ƒæœŸé—´ä½¿ç”¨çš„ç›¸åŒ<math><semantics><mrow><mi>Î¸</mi></mrow>
    <annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    Î¸ï¼Œåœ¨ä¼ é€’æ¯”è®­ç»ƒæœŸé—´çœ‹åˆ°çš„æ–‡æœ¬è¾“å…¥é•¿å¾—å¤šçš„æ–‡æœ¬è¾“å…¥æ—¶ä¼šå¯¼è‡´ç»“æœä¸ä½³ï¼Œ*å‚è§* [Press ç­‰äºº](https://arxiv.org/abs/2108.12409)ã€‚ç„¶è€Œï¼Œç¤¾åŒºå·²ç»å‘ç°äº†ä¸€äº›æœ‰æ•ˆçš„æŠ€å·§ï¼Œå¯ä»¥è°ƒæ•´<math><semantics><mrow><mi>Î¸</mi></mrow>
    <annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    Î¸ï¼Œä»è€Œä½¿ *RoPE* ä½ç½®åµŒå…¥é€‚ç”¨äºå¤–æ¨çš„æ–‡æœ¬è¾“å…¥åºåˆ—ï¼ˆå‚è§[è¿™é‡Œ](https://github.com/huggingface/transformers/pull/24653)ï¼‰ã€‚'
- en: 'Both RoPE and ALiBi are relative positional embeddings that are *not* learned
    during training, but instead are based on the following intuitions:'
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: RoPE å’Œ ALiBi éƒ½æ˜¯ç›¸å¯¹ä½ç½®åµŒå…¥ï¼Œå®ƒä»¬åœ¨è®­ç»ƒæœŸé—´ *ä¸* è¢«å­¦ä¹ ï¼Œè€Œæ˜¯åŸºäºä»¥ä¸‹ç›´è§‰ï¼š
- en: ''
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Positional cues about the text inputs should be given directly to the<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">QK^T</annotation></semantics></math>
    QKT matrix of the self-attention layer
  id: totrans-194
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºæ–‡æœ¬è¾“å…¥çš„ä½ç½®æç¤ºåº”ç›´æ¥æä¾›ç»™è‡ªæ³¨æ„åŠ›å±‚çš„<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">QK^T</annotation></semantics></math>
    QKT çŸ©é˜µ
- en: The LLM should be incentivized to learn a constant *relative* distance positional
    encodings have to each other
  id: totrans-195
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM åº”è¯¥è¢«æ¿€åŠ±å­¦ä¹ å¸¸æ•° *ç›¸å¯¹* è·ç¦»ä½ç½®ç¼–ç ä¹‹é—´çš„å…³ç³»
- en: The further text input tokens are from each other, the lower the probability
    of their query-value probability. Both RoPE and ALiBi lower the query-key probability
    of tokens far away from each other. RoPE by decreasing their vector product by
    increasing the angle between the query-key vectors. ALiBi by adding large negative
    numbers to the vector product
  id: totrans-196
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬è¾“å…¥ä»¤ç‰Œä¹‹é—´çš„è·ç¦»è¶Šè¿œï¼Œå®ƒä»¬çš„æŸ¥è¯¢-å€¼æ¦‚ç‡å°±è¶Šä½ã€‚RoPE å’Œ ALiBi éƒ½é™ä½äº†è¿œç¦»å½¼æ­¤çš„ä»¤ç‰Œçš„æŸ¥è¯¢-é”®æ¦‚ç‡ã€‚RoPE é€šè¿‡å¢åŠ æŸ¥è¯¢-é”®å‘é‡ä¹‹é—´çš„è§’åº¦æ¥å‡å°‘å®ƒä»¬çš„å‘é‡ç§¯ã€‚ALiBi
    é€šè¿‡å‘å‘é‡ç§¯æ·»åŠ å¤§çš„è´Ÿæ•°
- en: In conclusion, LLMs that are intended to be deployed in tasks that require handling
    large text inputs are better trained with relative positional embeddings, such
    as RoPE and ALiBi. Also note that even if an LLM with RoPE and ALiBi has been
    trained only on a fixed length of say<math><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>=</mo><mn>2048</mn></mrow>
    <annotation encoding="application/x-tex">N_1 = 2048</annotation></semantics></math>
    N1â€‹=2048 it can still be used in practice with text inputs much larger than<math><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub></mrow>
    <annotation encoding="application/x-tex">N_1</annotation></semantics></math> N1â€‹,
    like<math><semantics><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>=</mo><mn>8192</mn><mo>></mo><msub><mi>N</mi><mn>1</mn></msub></mrow>
    <annotation encoding="application/x-tex">N_2 = 8192 > N_1</annotation></semantics></math>
    N2â€‹=8192>N1â€‹ by extrapolating the positional embeddings.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œç”¨äºå¤„ç†å¤§æ–‡æœ¬è¾“å…¥çš„ä»»åŠ¡çš„ LLM æœ€å¥½ä½¿ç”¨ç›¸å¯¹ä½ç½®åµŒå…¥è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ RoPE å’Œ ALiBiã€‚è¿˜è¦æ³¨æ„ï¼Œå³ä½¿ä¸€ä¸ªå¸¦æœ‰ RoPE å’Œ ALiBi
    çš„ LLM åªåœ¨å›ºå®šé•¿åº¦çš„æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œæ¯”å¦‚<math><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>=</mo><mn>2048</mn></mrow>
    <annotation encoding="application/x-tex">N_1 = 2048</annotation></semantics></math>
    N1â€‹=2048ï¼Œå®ƒä»ç„¶å¯ä»¥åœ¨å®è·µä¸­ç”¨äºæ¯”<math><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub></mrow>
    <annotation encoding="application/x-tex">N_1</annotation></semantics></math> N1â€‹æ›´å¤§çš„æ–‡æœ¬è¾“å…¥ï¼Œæ¯”å¦‚<math><semantics><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>=</mo><mn>8192</mn><mo>></mo><msub><mi>N</mi><mn>1</mn></msub></mrow>
    <annotation encoding="application/x-tex">N_2 = 8192 > N_1</annotation></semantics></math>
    N2â€‹=8192>N1â€‹ï¼Œé€šè¿‡å¤–æ¨ä½ç½®åµŒå…¥ã€‚
- en: 3.2 The key-value cache
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 å…³é”®-å€¼ç¼“å­˜
- en: Auto-regressive text generation with LLMs works by iteratively putting in an
    input sequence, sampling the next token, appending the next token to the input
    sequence, and continuing to do so until the LLM produces a token that signifies
    that the generation has finished.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsçš„è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆé€šè¿‡è¿­ä»£åœ°è¾“å…¥ä¸€ä¸ªåºåˆ—ï¼ŒæŠ½æ ·ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå°†ä¸‹ä¸€ä¸ªæ ‡è®°é™„åŠ åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œå¹¶ç»§ç»­è¿™æ ·åšï¼Œç›´åˆ°LLMç”Ÿæˆä¸€ä¸ªè¡¨ç¤ºç”Ÿæˆç»“æŸçš„æ ‡è®°ã€‚
- en: Please have a look at [Transformerâ€™s Generate Text Tutorial](https://huggingface.co/docs/transformers/llm_tutorial#generate-text)
    to get a more visual explanation of how auto-regressive generation works.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹[Transformerç”Ÿæˆæ–‡æœ¬æ•™ç¨‹](https://huggingface.co/docs/transformers/llm_tutorial#generate-text)ï¼Œä»¥è·å¾—æ›´ç›´è§‚çš„è‡ªå›å½’ç”Ÿæˆå·¥ä½œåŸç†è§£é‡Šã€‚
- en: Letâ€™s run a quick code snippet to show how auto-regressive works in practice.
    We will simply take the most likely next token via `torch.argmax`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿è¡Œä¸€ä¸ªå¿«é€Ÿçš„ä»£ç ç‰‡æ®µï¼Œå±•ç¤ºè‡ªå›å½’åœ¨å®è·µä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘ä»¬å°†ç®€å•åœ°é€šè¿‡`torch.argmax`è·å–æœ€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Output**:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As we can see every time we increase the text input tokens by the just sampled
    token.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæ¯æ¬¡æˆ‘ä»¬é€šè¿‡åˆšåˆšæŠ½æ ·çš„æ ‡è®°å¢åŠ æ–‡æœ¬è¾“å…¥æ ‡è®°ã€‚
- en: With very few exceptions, LLMs are trained using the [causal language modeling
    objective](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling)
    and therefore mask the upper triangle matrix of the attention score - this is
    why in the two diagrams above the attention scores are left blank (*a.k.a* have
    0 probability). For a quick recap on causal language modeling you can refer to
    the [*Illustrated Self Attention blog*](https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æå°‘æ•°ä¾‹å¤–ï¼ŒLLMsæ˜¯ä½¿ç”¨[å› æœè¯­è¨€å»ºæ¨¡ç›®æ ‡](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling)è¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤ä¼šå±è”½æ³¨æ„åŠ›åˆ†æ•°çš„ä¸Šä¸‰è§’çŸ©é˜µ
    - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨ä¸Šè¿°ä¸¤ä¸ªå›¾è¡¨ä¸­ï¼Œæ³¨æ„åŠ›åˆ†æ•°ç•™ç©ºï¼ˆ*ä¹Ÿå°±æ˜¯*æ¦‚ç‡ä¸º0ï¼‰ã€‚å…³äºå› æœè¯­è¨€å»ºæ¨¡çš„å¿«é€Ÿå›é¡¾ï¼Œæ‚¨å¯ä»¥å‚è€ƒ[*Illustrated Self Attention
    blog*](https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention)ã€‚
- en: As a consequence, tokens *never* depend on previous tokens, more specifically
    the<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    qiâ€‹ vector is never put in relation with any key, values vectors<math><semantics><mrow><msub><mi
    mathvariant="bold">k</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi
    mathvariant="bold">v</mi><mi>j</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{k}_j,
    \mathbf{v}_j</annotation></semantics></math> kjâ€‹,vjâ€‹ if<math><semantics><mrow><mi>j</mi><mo>></mo><mi>i</mi></mrow>
    <annotation encoding="application/x-tex">j > i</annotation></semantics></math>
    j>i . Instead<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    qiâ€‹ only attends to previous key-value vectors<math><semantics><mrow><msub><mi
    mathvariant="bold">k</mi><mrow><mi>m</mi><mo><</mo><mi>i</mi></mrow></msub><mo
    separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mrow><mi>m</mi><mo><</mo><mi>i</mi></mrow></msub><mtext>Â ,Â forÂ </mtext><mi>m</mi><mo>âˆˆ</mo><mo
    stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mo>â€¦</mo><mi>i</mi><mo>âˆ’</mo><mn>1</mn><mo
    stretchy="false">}</mo></mrow> <annotation encoding="application/x-tex">\mathbf{k}_{m
    < i}, \mathbf{v}_{m < i} \text{ , for } m \in \{0, \ldots i - 1\}</annotation></semantics></math>
    km<iâ€‹,vm<iâ€‹Â ,Â forÂ mâˆˆ{0,â€¦iâˆ’1}. In order to reduce unnecessary computation, one
    can therefore cache each layerâ€™s key-value vectors for all previous timesteps.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ ‡è®°*æ°¸è¿œ*ä¸ä¾èµ–äºå…ˆå‰çš„æ ‡è®°ï¼Œæ›´å…·ä½“åœ°è¯´ï¼Œ<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    qiâ€‹å‘é‡æ°¸è¿œä¸ä¼šä¸ä»»ä½•é”®ã€å€¼å‘é‡<math><semantics><mrow><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub><mo
    separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{k}_j, \mathbf{v}_j</annotation></semantics></math>
    kjâ€‹,vjâ€‹ç›¸å…³è”ï¼Œå¦‚æœ<math><semantics><mrow><mi>j</mi><mo>></mo><mi>i</mi></mrow> <annotation
    encoding="application/x-tex">j > i</annotation></semantics></math> j>iã€‚ç›¸åï¼Œ<math><semantics><mrow><msub><mi
    mathvariant="bold">q</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math>
    qiâ€‹åªå…³æ³¨å…ˆå‰çš„é”®-å€¼å‘é‡<math><semantics><mrow><msub><mi mathvariant="bold">k</mi><mrow><mi>m</mi><mo><</mo><mi>i</mi></mrow></msub><mo
    separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mrow><mi>m</mi><mo><</mo><mi>i</mi></mrow></msub><mtext>Â ,Â forÂ </mtext><mi>m</mi><mo>âˆˆ</mo><mo
    stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mo>â€¦</mo><mi>i</mi><mo>âˆ’</mo><mn>1</mn><mo
    stretchy="false">}</mo></mrow> <annotation encoding="application/x-tex">\mathbf{k}_{m
    < i}, \mathbf{v}_{m < i} \text{ , for } m \in \{0, \ldots i - 1\}</annotation></semantics></math>
    km<iâ€‹,vm<iâ€‹Â ,Â forÂ mâˆˆ{0,â€¦iâˆ’1}ã€‚ä¸ºäº†å‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œå¯ä»¥ä¸ºæ¯ä¸€å±‚ç¼“å­˜æ‰€æœ‰å…ˆå‰æ—¶é—´æ­¥çš„é”®-å€¼å‘é‡ã€‚
- en: In the following, we will tell the LLM to make use of the key-value cache by
    retrieving and forwarding it for each forward pass. In Transformers, we can retrieve
    the key-value cache by passing the `use_cache` flag to the `forward` call and
    can then pass it with the current token.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‘Šè¯‰LLMåˆ©ç”¨é”®-å€¼ç¼“å­˜ï¼Œé€šè¿‡åœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­æ£€ç´¢å¹¶è½¬å‘å®ƒã€‚åœ¨Transformersä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘`forward`è°ƒç”¨ä¼ é€’`use_cache`æ ‡å¿—æ¥æ£€ç´¢é”®-å€¼ç¼“å­˜ï¼Œç„¶åå¯ä»¥å°†å…¶ä¸å½“å‰æ ‡è®°ä¸€èµ·ä¼ é€’ã€‚
- en: '[PRE44]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Output**:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE45]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As one can see, when using the key-value cache the text input tokens are *not*
    increased in length, but remain a single input vector. The length of the key-value
    cache on the other hand is increased by one at every decoding step.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚å¤§å®¶æ‰€çœ‹åˆ°çš„ï¼Œå½“ä½¿ç”¨é”®-å€¼ç¼“å­˜æ—¶ï¼Œæ–‡æœ¬è¾“å…¥æ ‡è®°çš„é•¿åº¦*ä¸ä¼š*å¢åŠ ï¼Œè€Œæ˜¯ä¿æŒä¸ºå•ä¸ªè¾“å…¥å‘é‡ã€‚å¦ä¸€æ–¹é¢ï¼Œé”®-å€¼ç¼“å­˜çš„é•¿åº¦åœ¨æ¯ä¸ªè§£ç æ­¥éª¤éƒ½ä¼šå¢åŠ ä¸€ä¸ªã€‚
- en: Making use of the key-value cache means that the<math><semantics><mrow><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT is essentially reduced to<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi
    mathvariant="bold">K</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{q}_c\mathbf{K}^T</annotation></semantics></math>
    qcâ€‹KT with<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_c</annotation></semantics></math>
    qcâ€‹ being the query projection of the currently passed input token which is *always*
    just a single vector.
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åˆ©ç”¨é”®å€¼ç¼“å­˜æ„å‘³ç€<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi
    mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKTåŸºæœ¬ä¸Šè¢«ç®€åŒ–ä¸º<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi
    mathvariant="bold">K</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{q}_c\mathbf{K}^T</annotation></semantics></math>
    qcâ€‹KTï¼Œå…¶ä¸­<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_c</annotation></semantics></math>
    qcâ€‹æ˜¯å½“å‰ä¼ é€’çš„è¾“å…¥ä»¤ç‰Œçš„æŸ¥è¯¢æŠ•å½±ï¼Œå®ƒ*å§‹ç»ˆ*åªæ˜¯ä¸€ä¸ªå•ä¸€å‘é‡ã€‚
- en: 'Using the key-value cache has two advantages:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é”®å€¼ç¼“å­˜æœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼š
- en: Significant increase in computational efficiency as less computations are performed
    compared to computing the full<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi
    mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT matrix. This leads to an increase in inference speed
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸è®¡ç®—å®Œæ•´çš„QKTçŸ©é˜µç›¸æ¯”ï¼Œè®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œå› ä¸ºè¿›è¡Œçš„è®¡ç®—è¾ƒå°‘ã€‚è¿™å¯¼è‡´æ¨ç†é€Ÿåº¦å¢åŠ ã€‚
- en: The maximum required memory is not increased quadratically with the number of
    generated tokens, but only increases linearly.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€éœ€çš„æœ€å¤§å†…å­˜å¹¶ä¸æ˜¯éšç€ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡çš„å¹³æ–¹å¢åŠ ï¼Œè€Œæ˜¯çº¿æ€§å¢åŠ ã€‚
- en: One should *always* make use of the key-value cache as it leads to identical
    results and a significant speed-up for longer input sequences. Transformers has
    the key-value cache enabled by default when making use of the text pipeline or
    the [`generate` method](https://huggingface.co/docs/transformers/main_classes/text_generation).
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åº”è¯¥*å§‹ç»ˆ*åˆ©ç”¨é”®å€¼ç¼“å­˜ï¼Œå› ä¸ºå®ƒä¼šäº§ç”Ÿç›¸åŒçš„ç»“æœï¼Œå¹¶ä¸”å¯¹äºè¾ƒé•¿çš„è¾“å…¥åºåˆ—ä¼šæ˜¾è‘—åŠ å¿«é€Ÿåº¦ã€‚å½“ä½¿ç”¨æ–‡æœ¬ç®¡é“æˆ–[`generate`æ–¹æ³•](https://huggingface.co/docs/transformers/main_classes/text_generation)æ—¶ï¼ŒTransformersé»˜è®¤å¯ç”¨é”®å€¼ç¼“å­˜ã€‚
- en: Note that, despite our advice to use key-value caches, your LLM output may be
    slightly different when you use them. This is a property of the matrix multiplication
    kernels themselves â€” you can read more about it [here](https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå°½ç®¡æˆ‘ä»¬å»ºè®®ä½¿ç”¨é”®å€¼ç¼“å­˜ï¼Œä½†å½“æ‚¨ä½¿ç”¨å®ƒä»¬æ—¶ï¼Œæ‚¨çš„LLMè¾“å‡ºå¯èƒ½ä¼šç•¥æœ‰ä¸åŒã€‚è¿™æ˜¯çŸ©é˜µä¹˜æ³•æ ¸å¿ƒæœ¬èº«çš„å±æ€§ â€” æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535)äº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: 3.2.1 Multi-round conversation
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 å¤šè½®å¯¹è¯
- en: The key-value cache is especially useful for applications such as chat where
    multiple passes of auto-regressive decoding are required. Letâ€™s look at an example.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: é”®å€¼ç¼“å­˜åœ¨éœ€è¦å¤šæ¬¡è‡ªå›å½’è§£ç çš„åº”ç”¨ç¨‹åºä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ã€‚
- en: '[PRE46]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In this chat, the LLM runs auto-regressive decoding twice:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå¯¹è¯ä¸­ï¼ŒLLMä¼šè‡ªå›å½’è§£ç ä¸¤æ¬¡ï¼š
- en: 'The first time, the key-value cache is empty and the input prompt is `"User:
    How many people live in France?"` and the model auto-regressively generates the
    text `"Roughly 75 million people live in France"` while increasing the key-value
    cache at every decoding step.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡ï¼Œé”®å€¼ç¼“å­˜ä¸ºç©ºï¼Œè¾“å…¥æç¤ºæ˜¯`"ç”¨æˆ·ï¼šæ³•å›½æœ‰å¤šå°‘äººå£ï¼Ÿ"`ï¼Œæ¨¡å‹ä¼šè‡ªå›å½’ç”Ÿæˆæ–‡æœ¬`"æ³•å›½å¤§çº¦æœ‰7500ä¸‡äººå£"ï¼ŒåŒæ—¶åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­å¢åŠ é”®å€¼ç¼“å­˜ã€‚
- en: 'The second time the input prompt is `"User: How many people live in France?
    \n Assistant: Roughly 75 million people live in France \n User: And how many in
    Germany?"`. Thanks to the cache, all key-value vectors for the first two sentences
    are already computed. Therefore the input prompt only consists of `"User: And
    how many in Germany?"`. While processing the shortened input prompt, itâ€™s computed
    key-value vectors are concatenated to the key-value cache of the first decoding.
    The second Assistantâ€™s answer `"Germany has ca. 81 million inhabitants"` is then
    auto-regressively generated with the key-value cache consisting of encoded key-value
    vectors of `"User: How many people live in France? \n Assistant: Roughly 75 million
    people live in France \n User: And how many are in Germany?"`.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ¬¡è¾“å…¥æç¤ºæ˜¯`"ç”¨æˆ·ï¼šæ³•å›½æœ‰å¤šå°‘äººå£ï¼Ÿ\nåŠ©æ‰‹ï¼šæ³•å›½å¤§çº¦æœ‰7500ä¸‡äººå£\nç”¨æˆ·ï¼šå¾·å›½æœ‰å¤šå°‘äººå£ï¼Ÿ"`ã€‚ç”±äºç¼“å­˜çš„å­˜åœ¨ï¼Œå‰ä¸¤ä¸ªå¥å­çš„æ‰€æœ‰é”®å€¼å‘é‡å·²ç»è®¡ç®—å®Œæ¯•ã€‚å› æ­¤ï¼Œè¾“å…¥æç¤ºåªåŒ…æ‹¬`"ç”¨æˆ·ï¼šå¾·å›½æœ‰å¤šå°‘äººå£ï¼Ÿ"`ã€‚åœ¨å¤„ç†ç¼©çŸ­çš„è¾“å…¥æç¤ºæ—¶ï¼Œå®ƒçš„è®¡ç®—é”®å€¼å‘é‡ä¼šä¸ç¬¬ä¸€æ¬¡è§£ç çš„é”®å€¼ç¼“å­˜è¿æ¥èµ·æ¥ã€‚ç„¶åç¬¬äºŒä¸ªåŠ©æ‰‹çš„å›ç­”`"å¾·å›½å¤§çº¦æœ‰8100ä¸‡å±…æ°‘"`ä¼šæ ¹æ®ç¼–ç çš„é”®å€¼å‘é‡`"ç”¨æˆ·ï¼šæ³•å›½æœ‰å¤šå°‘äººå£ï¼Ÿ\nåŠ©æ‰‹ï¼šæ³•å›½å¤§çº¦æœ‰7500ä¸‡äººå£\nç”¨æˆ·ï¼šå¾·å›½æœ‰å¤šå°‘äººå£ï¼Ÿ"`è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚
- en: 'Two things should be noted here:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸¤ç‚¹éœ€è¦æ³¨æ„ï¼š
- en: Keeping all the context is crucial for LLMs deployed in chat so that the LLM
    understands all the previous context of the conversation. E.g. for the example
    above the LLM needs to understand that the user refers to the population when
    asking `"And how many are in Germany"`.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºéƒ¨ç½²åœ¨èŠå¤©ä¸­çš„LLMæ¥è¯´ï¼Œä¿ç•™æ‰€æœ‰ä¸Šä¸‹æ–‡å¯¹äºLLMç†è§£å¯¹è¯çš„å…ˆå‰ä¸Šä¸‹æ–‡è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸Šé¢çš„ä¾‹å­ï¼ŒLLMéœ€è¦ç†è§£ç”¨æˆ·åœ¨è¯¢é—®`"å¾·å›½æœ‰å¤šå°‘äººå£ï¼Ÿ"`æ—¶æŒ‡çš„æ˜¯äººå£ã€‚
- en: The key-value cache is extremely useful for chat as it allows us to continuously
    grow the encoded chat history instead of having to re-encode the chat history
    again from scratch (as e.g. would be the case when using an encoder-decoder architecture).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é”®å€¼ç¼“å­˜å¯¹äºèŠå¤©éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬æŒç»­å¢åŠ ç¼–ç çš„èŠå¤©å†å²ï¼Œè€Œä¸å¿…é‡æ–°ä»å¤´å¼€å§‹é‡æ–°ç¼–ç èŠå¤©å†å²ï¼ˆä¾‹å¦‚ï¼Œå½“ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„æ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼‰ã€‚
- en: In `transformers`, a `generate` call will return `past_key_values` when `return_dict_in_generate=True`
    is passed, in addition to the default `use_cache=True`. Note that it is not yet
    available through the `pipeline` interface.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`transformers`ä¸­ï¼Œå½“ä¼ é€’`return_dict_in_generate=True`æ—¶ï¼Œ`generate`è°ƒç”¨å°†è¿”å›`past_key_values`ï¼Œé™¤äº†é»˜è®¤çš„`use_cache=True`ã€‚è¯·æ³¨æ„ï¼Œè¿™è¿˜ä¸é€‚ç”¨äº`pipeline`æ¥å£ã€‚
- en: '[PRE47]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Output**:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE48]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Great, no additional time is spent recomputing the same key and values for the
    attention layer! There is however one catch. While the required peak memory for
    the<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT matrix is significantly reduced, holding the key-value cache in memory can
    become very memory expensive for long input sequences or multi-turn chat. Remember
    that the key-value cache needs to store the key-value vectors for all previous
    input vectors<math><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mtext>,Â forÂ </mtext><mi>i</mi><mo>âˆˆ</mo><mo
    stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><mi>c</mi><mo>âˆ’</mo><mn>1</mn><mo
    stretchy="false">}</mo></mrow> <annotation encoding="application/x-tex">\mathbf{x}_i
    \text{, for } i \in \{1, \ldots, c - 1\}</annotation></semantics></math> xiâ€‹,Â forÂ iâˆˆ{1,â€¦,câˆ’1}
    for all self-attention layers and for all attention heads.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªå¥½äº†ï¼Œä¸éœ€è¦é¢å¤–çš„æ—¶é—´æ¥é‡æ–°è®¡ç®—æ³¨æ„åŠ›å±‚çš„ç›¸åŒé”®å’Œå€¼ï¼ç„¶è€Œï¼Œæœ‰ä¸€ä¸ªé—®é¢˜ã€‚è™½ç„¶<math><semantics><mrow><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT çŸ©é˜µæ‰€éœ€çš„å³°å€¼å†…å­˜æ˜¾è‘—å‡å°‘ï¼Œä½†åœ¨å†…å­˜ä¸­ä¿æŒé”®å€¼ç¼“å­˜å¯èƒ½ä¼šå¯¹é•¿è¾“å…¥åºåˆ—æˆ–å¤šè½®å¯¹è¯éå¸¸æ˜‚è´µã€‚è¯·è®°ä½ï¼Œé”®å€¼ç¼“å­˜éœ€è¦å­˜å‚¨æ‰€æœ‰å…ˆå‰è¾“å…¥å‘é‡çš„é”®å€¼å‘é‡<math><semantics><mrow><msub><mi
    mathvariant="bold">x</mi><mi>i</mi></msub><mtext>,Â forÂ </mtext><mi>i</mi><mo>âˆˆ</mo><mo
    stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><mi>c</mi><mo>âˆ’</mo><mn>1</mn><mo
    stretchy="false">}</mo></mrow> <annotation encoding="application/x-tex">\mathbf{x}_i
    \text{, for } i \in \{1, \ldots, c - 1\}</annotation></semantics></math> xiâ€‹,Â forÂ iâˆˆ{1,â€¦,câˆ’1}
    å¯¹äºæ‰€æœ‰è‡ªæ³¨æ„åŠ›å±‚å’Œæ‰€æœ‰æ³¨æ„åŠ›å¤´éƒ¨ã€‚
- en: 'Letâ€™s compute the number of float values that need to be stored in the key-value
    cache for the LLM `bigcode/octocoder` that we used before. The number of float
    values amounts to two times the sequence length times the number of attention
    heads times the attention head dimension and times the number of layers. Computing
    this for our LLM at a hypothetical input sequence length of 16000 gives:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¡ç®—ä¹‹å‰ä½¿ç”¨çš„LLM `bigcode/octocoder` éœ€è¦å­˜å‚¨åœ¨é”®å€¼ç¼“å­˜ä¸­çš„æµ®ç‚¹å€¼çš„æ•°é‡ã€‚æµ®ç‚¹å€¼çš„æ•°é‡ç­‰äºåºåˆ—é•¿åº¦ä¹˜ä»¥æ³¨æ„åŠ›å¤´æ•°ä¹˜ä»¥æ³¨æ„åŠ›å¤´ç»´åº¦ä¹˜ä»¥å±‚æ•°çš„ä¸¤å€ã€‚å¯¹äºæˆ‘ä»¬çš„LLMï¼Œåœ¨å‡è®¾è¾“å…¥åºåˆ—é•¿åº¦ä¸º16000æ—¶è®¡ç®—å¦‚ä¸‹ï¼š
- en: '[PRE49]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Output**:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼š'
- en: '[PRE50]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Roughly 8 billion float values! Storing 8 billion float values in `float16`
    precision requires around 15 GB of RAM which is circa half as much as the model
    weights themselves! Researchers have proposed two methods that allow to significantly
    reduce the memory cost of storing the key-value cache, which are explored in the
    next subsections.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦80äº¿ä¸ªæµ®ç‚¹å€¼ï¼ä»¥`float16`ç²¾åº¦å­˜å‚¨80äº¿ä¸ªæµ®ç‚¹å€¼éœ€è¦å¤§çº¦15 GBçš„å†…å­˜ï¼Œè¿™å¤§çº¦æ˜¯æ¨¡å‹æƒé‡æœ¬èº«çš„ä¸€åŠï¼ç ”ç©¶äººå‘˜æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘å­˜å‚¨é”®å€¼ç¼“å­˜çš„å†…å­˜æˆæœ¬ï¼Œè¿™å°†åœ¨æ¥ä¸‹æ¥çš„å°èŠ‚ä¸­æ¢è®¨ã€‚
- en: 3.2.2 Multi-Query-Attention (MQA)
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰
- en: '[Multi-Query-Attention](https://arxiv.org/abs/1911.02150) was proposed in Noam
    Shazeerâ€™s *Fast Transformer Decoding: One Write-Head is All You Need* paper. As
    the title says, Noam found out that instead of using `n_head` key-value projections
    weights, one can use a single head-value projection weight pair that is shared
    across all attention heads without that the modelâ€™s performance significantly
    degrades.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[Multi-Query-Attention](https://arxiv.org/abs/1911.02150) æ˜¯Noam Shazeeråœ¨*Fast
    Transformer Decoding: One Write-Head is All You Need*è®ºæ–‡ä¸­æå‡ºçš„ã€‚æ­£å¦‚æ ‡é¢˜æ‰€è¯´ï¼ŒNoam å‘ç°ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªå•ä¸€çš„å¤´å€¼æŠ•å½±æƒé‡å¯¹ï¼Œè€Œä¸æ˜¯ä½¿ç”¨`n_head`ä¸ªé”®å€¼æŠ•å½±æƒé‡ï¼Œè¿™ä¸ªå¯¹åœ¨æ‰€æœ‰æ³¨æ„åŠ›å¤´éƒ¨ä¹‹é—´å…±äº«ï¼Œè€Œä¸ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„æ€§èƒ½ã€‚'
- en: By using a single head-value projection weight pair, the key value vectors<math><semantics><mrow><msub><mi
    mathvariant="bold">k</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi
    mathvariant="bold">v</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\mathbf{k}_i,
    \mathbf{v}_i</annotation></semantics></math> kiâ€‹,viâ€‹ have to be identical across
    all attention heads which in turn means that we only need to store 1 key-value
    projection pair in the cache instead of `n_head` ones.
  id: totrans-240
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨å•ä¸ªå¤´å€¼æŠ•å½±æƒé‡å¯¹ï¼Œé”®å€¼å‘é‡<math><semantics><mrow><msub><mi mathvariant="bold">k</mi><mi>i</mi></msub><mo
    separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\mathbf{k}_i, \mathbf{v}_i</annotation></semantics></math>
    kiâ€‹,viâ€‹ åœ¨æ‰€æœ‰æ³¨æ„åŠ›å¤´éƒ¨ä¹‹é—´å¿…é¡»æ˜¯ç›¸åŒçš„ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªéœ€è¦åœ¨ç¼“å­˜ä¸­å­˜å‚¨1ä¸ªé”®å€¼æŠ•å½±å¯¹ï¼Œè€Œä¸æ˜¯`n_head`ä¸ªã€‚
- en: As most LLMs use between 20 and 100 attention heads, MQA significantly reduces
    the memory consumption of the key-value cache. For the LLM used in this notebook
    we could therefore reduce the required memory consumption from 15 GB to less than
    400 MB at an input sequence length of 16000.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¤§å¤šæ•°LLMä½¿ç”¨20åˆ°100ä¸ªæ³¨æ„åŠ›å¤´éƒ¨ï¼ŒMQAæ˜¾è‘—å‡å°‘äº†é”®å€¼ç¼“å­˜çš„å†…å­˜æ¶ˆè€—ã€‚å¯¹äºæœ¬ç¬”è®°æœ¬ä¸­ä½¿ç”¨çš„LLMï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†æ‰€éœ€çš„å†…å­˜æ¶ˆè€—ä»15 GBå‡å°‘åˆ°è¾“å…¥åºåˆ—é•¿åº¦ä¸º16000æ—¶çš„ä¸åˆ°400
    MBã€‚
- en: In addition to memory savings, MQA also leads to improved computational efficiency
    as explained in the following. In auto-regressive decoding, large key-value vectors
    need to be reloaded, concatenated with the current key-value vector pair to be
    then fed into the<math><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi
    mathvariant="bold">K</mi><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{q}_c\mathbf{K}^T</annotation></semantics></math>
    qcâ€‹KT computation at every step. For auto-regressive decoding, the required memory
    bandwidth for the constant reloading can become a serious time bottleneck. By
    reducing the size of the key-value vectors less memory needs to be accessed, thus
    reducing the memory bandwidth bottleneck. For more detail, please have a look
    at [Noamâ€™s paper](https://arxiv.org/abs/1911.02150).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†èŠ‚çœå†…å­˜å¤–ï¼ŒMQAè¿˜æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚åœ¨è‡ªå›å½’è§£ç ä¸­ï¼Œéœ€è¦é‡æ–°åŠ è½½å¤§çš„é”®å€¼å‘é‡ï¼Œå°†å…¶ä¸å½“å‰çš„é”®å€¼å‘é‡å¯¹è¿æ¥ï¼Œç„¶åå°†å…¶é¦ˆé€åˆ°æ¯ä¸€æ­¥çš„<math><semantics><mrow><msub><mi
    mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi mathvariant="bold">K</mi><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{q}_c\mathbf{K}^T</annotation></semantics></math>
    qcâ€‹KT è®¡ç®—ä¸­ã€‚å¯¹äºè‡ªå›å½’è§£ç ï¼Œå¸¸é‡é‡æ–°åŠ è½½æ‰€éœ€çš„å†…å­˜å¸¦å®½å¯èƒ½æˆä¸ºä¸¥é‡çš„æ—¶é—´ç“¶é¢ˆã€‚é€šè¿‡å‡å°é”®å€¼å‘é‡çš„å¤§å°ï¼Œå¯ä»¥å‡å°‘è®¿é—®çš„å†…å­˜é‡ï¼Œä»è€Œå‡å°‘å†…å­˜å¸¦å®½ç“¶é¢ˆã€‚æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œè¯·å‚é˜…[Noamçš„è®ºæ–‡](https://arxiv.org/abs/1911.02150)ã€‚
- en: The important part to understand here is that reducing the number of key-value
    attention heads to 1 only makes sense if a key-value cache is used. The peak memory
    consumption of the model for a single forward pass without key-value cache stays
    unchanged as every attention head still has a unique query vector so that each
    attention head still has a different<math><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi
    mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT matrix.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œéœ€è¦ç†è§£çš„é‡è¦éƒ¨åˆ†æ˜¯ï¼Œå°†å…³é”®å€¼æ³¨æ„åŠ›å¤´çš„æ•°é‡å‡å°‘åˆ°1åªæœ‰åœ¨ä½¿ç”¨å…³é”®å€¼ç¼“å­˜æ—¶æ‰æœ‰æ„ä¹‰ã€‚æ¨¡å‹åœ¨æ²¡æœ‰å…³é”®å€¼ç¼“å­˜çš„å•æ¬¡å‰å‘ä¼ é€’ä¸­çš„å³°å€¼å†…å­˜æ¶ˆè€—ä¿æŒä¸å˜ï¼Œå› ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´ä»ç„¶å…·æœ‰å”¯ä¸€çš„æŸ¥è¯¢å‘é‡ï¼Œå› æ­¤æ¯ä¸ªæ³¨æ„åŠ›å¤´ä»ç„¶å…·æœ‰ä¸åŒçš„<math><semantics><mrow><msup><mrow><mi
    mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{QK}^T</annotation></semantics></math>
    QKT çŸ©é˜µã€‚
- en: 'MQA has seen wide adoption by the community and is now used by many of the
    most popular LLMs:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: MQAå·²ç»è¢«ç¤¾åŒºå¹¿æ³›é‡‡ç”¨ï¼Œç°åœ¨è®¸å¤šæœ€å—æ¬¢è¿çš„LLMéƒ½åœ¨ä½¿ç”¨ï¼š
- en: '[**Falcon**](https://huggingface.co/tiiuae/falcon-40b)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Falcon**](https://huggingface.co/tiiuae/falcon-40b)'
- en: '[**PaLM**](https://arxiv.org/abs/2204.02311)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PaLM**](https://arxiv.org/abs/2204.02311)'
- en: '[**MPT**](https://huggingface.co/mosaicml/mpt-30b)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MPT**](https://huggingface.co/mosaicml/mpt-30b)'
- en: '[**BLOOM**](https://huggingface.co/bigscience/bloom)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**BLOOM**](https://huggingface.co/bigscience/bloom)'
- en: Also, the checkpoint used in this notebook - `bigcode/octocoder` - makes use
    of MQA.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæœ¬ç¬”è®°ä¸­ä½¿ç”¨çš„æ£€æŸ¥ç‚¹`bigcode/octocoder`ä½¿ç”¨äº†MQAã€‚
- en: 3.2.3 Grouped-Query-Attention (GQA)
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰
- en: '[Grouped-Query-Attention](https://arxiv.org/abs/2305.13245), as proposed by
    Ainslie et al. from Google, found that using MQA can often lead to quality degradation
    compared to using vanilla multi-key-value head projections. The paper argues that
    more model performance can be kept by less drastically reducing the number of
    query head projection weights. Instead of using just a single key-value projection
    weight, `n < n_head` key-value projection weights should be used. By choosing
    `n` to a significantly smaller value than `n_head`, such as 2,4 or 8 almost all
    of the memory and speed gains from MQA can be kept while sacrificing less model
    capacity and thus arguably less performance.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›](https://arxiv.org/abs/2305.13245)ï¼Œç”±è°·æ­Œçš„Ainslieç­‰äººæå‡ºï¼Œå‘ç°ä½¿ç”¨MQAä¸ä½¿ç”¨æ™®é€šçš„å¤šé”®å€¼å¤´æŠ•å½±ç›¸æ¯”ï¼Œé€šå¸¸ä¼šå¯¼è‡´è´¨é‡ä¸‹é™ã€‚è¯¥è®ºæ–‡è®¤ä¸ºï¼Œé€šè¿‡å‡å°‘æŸ¥è¯¢å¤´æŠ•å½±æƒé‡çš„æ•°é‡ï¼Œå¯ä»¥ä¿ç•™æ›´å¤šçš„æ¨¡å‹æ€§èƒ½ã€‚ä¸è¦ä»…ä½¿ç”¨å•ä¸ªé”®å€¼æŠ•å½±æƒé‡ï¼Œåº”ä½¿ç”¨`n
    < n_head`ä¸ªé”®å€¼æŠ•å½±æƒé‡ã€‚é€šè¿‡é€‰æ‹©`n`ä¸ºè¿œå°äº`n_head`çš„å€¼ï¼Œä¾‹å¦‚2ã€4æˆ–8ï¼Œå‡ ä¹å¯ä»¥ä¿ç•™æ¥è‡ªMQAçš„æ‰€æœ‰å†…å­˜å’Œé€Ÿåº¦å¢ç›Šï¼ŒåŒæ—¶ç‰ºç‰²è¾ƒå°‘çš„æ¨¡å‹å®¹é‡ï¼Œå› æ­¤å¯ä»¥è¯´æ˜¯æ€§èƒ½æ›´å¥½ã€‚'
- en: Moreover, the authors of GQA found out that existing model checkpoints can be
    *uptrained* to have a GQA architecture with as little as 5% of the original pre-training
    compute. While 5% of the original pre-training compute can still be a massive
    amount, GQA *uptraining* allows existing checkpoints to be useful for longer input
    sequences.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¤å¤–ï¼ŒGQAçš„ä½œè€…å‘ç°ï¼Œç°æœ‰çš„æ¨¡å‹æ£€æŸ¥ç‚¹å¯ä»¥é€šè¿‡ä»…ä½¿ç”¨åŸå§‹é¢„è®­ç»ƒè®¡ç®—é‡çš„5%è¿›è¡Œ*æ›´æ–°è®­ç»ƒ*ï¼Œä»¥å®ç°GQAæ¶æ„ã€‚è™½ç„¶åŸå§‹é¢„è®­ç»ƒè®¡ç®—é‡çš„5%ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ•°é‡ï¼Œä½†GQAçš„*æ›´æ–°è®­ç»ƒ*ä½¿ç°æœ‰çš„æ£€æŸ¥ç‚¹å¯ä»¥ç”¨äºæ›´é•¿çš„è¾“å…¥åºåˆ—ã€‚ '
- en: GQA was only recently proposed which is why there is less adoption at the time
    of writing this notebook. The most notable application of GQA is [Llama-v2](https://huggingface.co/meta-llama/Llama-2-70b-hf).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: GQAæ˜¯æœ€è¿‘æå‡ºçš„ï¼Œå› æ­¤åœ¨æ’°å†™æœ¬ç¬”è®°æ—¶é‡‡ç”¨çš„æƒ…å†µè¾ƒå°‘ã€‚GQAæœ€æ˜¾è‘—çš„åº”ç”¨æ˜¯[Llama-v2](https://huggingface.co/meta-llama/Llama-2-70b-hf)ã€‚
- en: As a conclusion, it is strongly recommended to make use of either GQA or MQA
    if the LLM is deployed with auto-regressive decoding and is required to handle
    large input sequences as is the case for example for chat.
  id: totrans-254
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œå¼ºçƒˆå»ºè®®åœ¨LLMéƒ¨ç½²è‡ªå›å½’è§£ç å¹¶éœ€è¦å¤„ç†å¤§å‹è¾“å…¥åºåˆ—çš„æƒ…å†µä¸‹ä½¿ç”¨GQAæˆ–MQAã€‚
- en: Conclusion
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The research community is constantly coming up with new, nifty ways to speed
    up inference time for ever-larger LLMs. As an example, one such promising research
    direction is [speculative decoding](https://arxiv.org/abs/2211.17192) where â€œeasy
    tokensâ€ are generated by smaller, faster language models and only â€œhard tokensâ€
    are generated by the LLM itself. Going into more detail is out of the scope of
    this notebook, but can be read upon in this [nice blog post](https://huggingface.co/blog/assisted-generation).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶ç•Œä¸æ–­æå‡ºæ–°çš„å·§å¦™æ–¹æ³•æ¥åŠ å¿«è¶Šæ¥è¶Šå¤§çš„LLMçš„æ¨ç†æ—¶é—´ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘æ˜¯[æ¨æµ‹è§£ç ](https://arxiv.org/abs/2211.17192)ï¼Œå…¶ä¸­è¾ƒå°ã€æ›´å¿«çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆâ€œç®€å•æ ‡è®°â€ï¼Œåªæœ‰LLMæœ¬èº«ç”Ÿæˆâ€œå›°éš¾æ ‡è®°â€ã€‚æ›´è¯¦ç»†çš„å†…å®¹è¶…å‡ºäº†æœ¬ç¬”è®°çš„èŒƒå›´ï¼Œä½†å¯ä»¥åœ¨è¿™ç¯‡[ä¸é”™çš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/assisted-generation)ä¸­é˜…è¯»ã€‚
- en: The reason massive LLMs such as GPT3/4, Llama-2-70b, Claude, PaLM can run so
    quickly in chat-interfaces such as [Hugging Face Chat](https://huggingface.co/chat/)
    or ChatGPT is to a big part thanks to the above-mentioned improvements in precision,
    algorithms, and architecture. Going forward, accelerators such as GPUs, TPUs,
    etcâ€¦ will only get faster and allow for more memory, but one should nevertheless
    always make sure to use the best available algorithms and architectures to get
    the most bang for your buck ğŸ¤—
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹LLMï¼ˆå¦‚GPT3/4ã€Llama-2-70bã€Claudeã€PaLMï¼‰èƒ½å¤Ÿåœ¨[Hugging Face Chat](https://huggingface.co/chat/)æˆ–ChatGPTç­‰èŠå¤©ç•Œé¢ä¸­è¿è¡Œå¦‚æ­¤è¿…é€Ÿï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¦å½’åŠŸäºä¸Šè¿°ç²¾åº¦ã€ç®—æ³•å’Œæ¶æ„çš„æ”¹è¿›ã€‚æœªæ¥ï¼ŒåƒGPUã€TPUç­‰åŠ é€Ÿå™¨å°†ä¼šå˜å¾—æ›´å¿«ï¼Œå…è®¸æ›´å¤šçš„å†…å­˜ï¼Œä½†ä»ç„¶åº”å§‹ç»ˆç¡®ä¿ä½¿ç”¨æœ€ä½³çš„å¯ç”¨ç®—æ³•å’Œæ¶æ„ï¼Œä»¥è·å¾—æœ€å¤§çš„æ€§ä»·æ¯”ğŸ¤—
