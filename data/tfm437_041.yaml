- en: Document Question Answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–‡æ¡£é—®ç­”
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/document_question_answering](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/document_question_answering)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/document_question_answering](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/document_question_answering)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Document Question Answering, also referred to as Document Visual Question Answering,
    is a task that involves providing answers to questions posed about document images.
    The input to models supporting this task is typically a combination of an image
    and a question, and the output is an answer expressed in natural language. These
    models utilize multiple modalities, including text, the positions of words (bounding
    boxes), and the image itself.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£é—®ç­”ï¼Œä¹Ÿç§°ä¸ºæ–‡æ¡£è§†è§‰é—®ç­”ï¼Œæ˜¯ä¸€ä¸ªæ¶‰åŠæä¾›å…³äºæ–‡æ¡£å›¾åƒçš„é—®é¢˜çš„ç­”æ¡ˆçš„ä»»åŠ¡ã€‚æ”¯æŒæ­¤ä»»åŠ¡çš„æ¨¡å‹çš„è¾“å…¥é€šå¸¸æ˜¯å›¾åƒå’Œé—®é¢˜çš„ç»„åˆï¼Œè¾“å‡ºæ˜¯ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„ç­”æ¡ˆã€‚è¿™äº›æ¨¡å‹åˆ©ç”¨å¤šç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å•è¯çš„ä½ç½®ï¼ˆè¾¹ç•Œæ¡†ï¼‰å’Œå›¾åƒæœ¬èº«ã€‚
- en: 'This guide illustrates how to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—è¯´æ˜äº†å¦‚ä½•ï¼š
- en: Fine-tune [LayoutLMv2](../model_doc/layoutlmv2) on the [DocVQA dataset](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ [DocVQA æ•°æ®é›†](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut)
    ä¸Šå¯¹ [LayoutLMv2](../model_doc/layoutlmv2) è¿›è¡Œå¾®è°ƒã€‚
- en: Use your fine-tuned model for inference.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­æ¼”ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
- en: '[LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3)'
- en: 'LayoutLMv2 solves the document question-answering task by adding a question-answering
    head on top of the final hidden states of the tokens, to predict the positions
    of the start and end tokens of the answer. In other words, the problem is treated
    as extractive question answering: given the context, extract which piece of information
    answers the question. The context comes from the output of an OCR engine, here
    it is Googleâ€™s Tesseract.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv2 é€šè¿‡åœ¨æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šæ·»åŠ ä¸€ä¸ªé—®é¢˜-å›ç­”å¤´æ¥è§£å†³æ–‡æ¡£é—®ç­”ä»»åŠ¡ï¼Œä»¥é¢„æµ‹ç­”æ¡ˆçš„å¼€å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®ã€‚æ¢å¥è¯è¯´ï¼Œè¿™ä¸ªé—®é¢˜è¢«è§†ä¸ºæŠ½å–å¼é—®ç­”ï¼šåœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œæå–å“ªä¸ªä¿¡æ¯ç‰‡æ®µå›ç­”é—®é¢˜ã€‚ä¸Šä¸‹æ–‡æ¥è‡ª
    OCR å¼•æ“çš„è¾“å‡ºï¼Œè¿™é‡Œæ˜¯ Google çš„ Tesseractã€‚
- en: Before you begin, make sure you have all the necessary libraries installed.
    LayoutLMv2 depends on detectron2, torchvision and tesseract.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚LayoutLMv2 ä¾èµ–äº detectron2ã€torchvision å’Œ tesseractã€‚
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once you have installed all of the dependencies, restart your runtime.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£…å®Œæ‰€æœ‰ä¾èµ–é¡¹åï¼Œè¯·é‡æ–°å¯åŠ¨æ‚¨çš„è¿è¡Œæ—¶ã€‚
- en: 'We encourage you to share your model with the community. Log in to your Hugging
    Face account to upload it to the ğŸ¤— Hub. When prompted, enter your token to log
    in:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚ç™»å½•åˆ°æ‚¨çš„ Hugging Face è´¦æˆ·ï¼Œå°†å…¶ä¸Šä¼ åˆ° ğŸ¤— Hubã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Letâ€™s define some global variables.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®šä¹‰ä¸€äº›å…¨å±€å˜é‡ã€‚
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Load the data
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®
- en: In this guide we use a small sample of preprocessed DocVQA that you can find
    on ğŸ¤— Hub. If youâ€™d like to use the full DocVQA dataset, you can register and download
    it on [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17). If you do so, to proceed
    with this guide check out [how to load files into a ğŸ¤— dataset](https://huggingface.co/docs/datasets/loading#local-and-remote-files).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå°æ ·æœ¬çš„é¢„å¤„ç† DocVQAï¼Œæ‚¨å¯ä»¥åœ¨ ğŸ¤— Hub ä¸Šæ‰¾åˆ°ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨å®Œæ•´çš„ DocVQA æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥åœ¨ [DocVQA
    ä¸»é¡µ](https://rrc.cvc.uab.es/?ch=17) ä¸Šæ³¨å†Œå¹¶ä¸‹è½½ã€‚å¦‚æœæ‚¨è¿™æ ·åšäº†ï¼Œè¦ç»§ç»­æœ¬æŒ‡å—ï¼Œè¯·æŸ¥çœ‹ [å¦‚ä½•å°†æ–‡ä»¶åŠ è½½åˆ° ğŸ¤— æ•°æ®é›†](https://huggingface.co/docs/datasets/loading#local-and-remote-files)ã€‚
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the dataset is split into train and test sets already. Take
    a look at a random example to familiarize yourself with the features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†å·²ç»åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æŸ¥çœ‹ä¸€ä¸ªéšæœºç¤ºä¾‹ï¼Œä»¥ç†Ÿæ‚‰ç‰¹å¾ã€‚
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Hereâ€™s what the individual fields represent:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å„ä¸ªå­—æ®µä»£è¡¨çš„å«ä¹‰ï¼š
- en: '`id`: the exampleâ€™s id'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id`ï¼šç¤ºä¾‹çš„ id'
- en: '`image`: a PIL.Image.Image object containing the document image'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼šåŒ…å«æ–‡æ¡£å›¾åƒçš„ PIL.Image.Image å¯¹è±¡'
- en: '`query`: the question string - natural language asked question, in several
    languages'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query`ï¼šé—®é¢˜å­—ç¬¦ä¸² - è‡ªç„¶è¯­è¨€æå‡ºçš„é—®é¢˜ï¼Œå¯ä»¥æ˜¯å¤šç§è¯­è¨€'
- en: '`answers`: a list of correct answers provided by human annotators'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answers`ï¼šäººç±»æ³¨é‡Šè€…æä¾›çš„æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨'
- en: '`words` and `bounding_boxes`: the results of OCR, which we will not use here'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`words` å’Œ `bounding_boxes`ï¼šOCR çš„ç»“æœï¼Œæˆ‘ä»¬è¿™é‡Œä¸ä¼šä½¿ç”¨'
- en: '`answer`: an answer matched by a different model which we will not use here'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer`ï¼šç”±å¦ä¸€ä¸ªæ¨¡å‹åŒ¹é…çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬è¿™é‡Œä¸ä¼šä½¿ç”¨'
- en: Letâ€™s leave only English questions, and drop the `answer` feature which appears
    to contain predictions by another model. Weâ€™ll also take the first of the answers
    from the set provided by the annotators. Alternatively, you can randomly sample
    it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åªä¿ç•™è‹±æ–‡é—®é¢˜ï¼Œå¹¶ä¸”åˆ é™¤åŒ…å«å¦ä¸€ä¸ªæ¨¡å‹é¢„æµ‹çš„ `answer` ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜å°†ä»æ³¨é‡Šè€…æä¾›çš„ç­”æ¡ˆé›†ä¸­å–ç¬¬ä¸€ä¸ªç­”æ¡ˆã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥éšæœºæŠ½æ ·ã€‚
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the LayoutLMv2 checkpoint that we use in this guide has been trained
    with `max_position_embeddings = 512` (you can find this information in the [checkpointâ€™s
    `config.json` file](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)).
    We can truncate the examples but to avoid the situation where the answer might
    be at the end of a large document and end up truncated, here weâ€™ll remove the
    few examples where the embedding is likely to end up longer than 512. If most
    of the documents in your dataset are long, you can implement a sliding window
    strategy - check out [this notebook](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)
    for details.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæœ¬æŒ‡å—ä¸­ä½¿ç”¨çš„ LayoutLMv2 æ£€æŸ¥ç‚¹å·²ç»è®­ç»ƒäº† `max_position_embeddings = 512`ï¼ˆæ‚¨å¯ä»¥åœ¨ [æ£€æŸ¥ç‚¹çš„ `config.json`
    æ–‡ä»¶](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)
    ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥æˆªæ–­ç¤ºä¾‹ï¼Œä½†ä¸ºäº†é¿å…ç­”æ¡ˆå¯èƒ½åœ¨å¤§å‹æ–‡æ¡£çš„æœ«å°¾å¹¶æœ€ç»ˆè¢«æˆªæ–­çš„æƒ…å†µï¼Œè¿™é‡Œæˆ‘ä»¬å°†åˆ é™¤å‡ ä¸ªç¤ºä¾‹ï¼Œå…¶ä¸­åµŒå…¥å¯èƒ½ä¼šè¶…è¿‡ 512ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†ä¸­å¤§å¤šæ•°æ–‡æ¡£å¾ˆé•¿ï¼Œæ‚¨å¯ä»¥å®ç°ä¸€ä¸ªæ»‘åŠ¨çª—å£ç­–ç•¥
    - è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ [æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ã€‚
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point letâ€™s also remove the OCR features from this dataset. These are
    a result of OCR for fine-tuning a different model. They would still require some
    processing if we wanted to use them, as they do not match the input requirements
    of the model we use in this guide. Instead, we can use the [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    on the original data for both OCR and tokenization. This way weâ€™ll get the inputs
    that match modelâ€™s expected input. If you want to process images manually, check
    out the [`LayoutLMv2` model documentation](../model_doc/layoutlmv2) to learn what
    input format the model expects.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶è®©æˆ‘ä»¬è¿˜ä»æ•°æ®é›†ä¸­åˆ é™¤OCRç‰¹å¾ã€‚è¿™äº›æ˜¯OCRçš„ç»“æœï¼Œç”¨äºå¾®è°ƒä¸åŒçš„æ¨¡å‹ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦ä½¿ç”¨å®ƒä»¬ï¼Œå®ƒä»¬ä»ç„¶éœ€è¦ä¸€äº›å¤„ç†ï¼Œå› ä¸ºå®ƒä»¬ä¸ç¬¦åˆæˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­ä½¿ç”¨çš„æ¨¡å‹çš„è¾“å…¥è¦æ±‚ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åŸå§‹æ•°æ®ä¸ŠåŒæ—¶ä½¿ç”¨[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)è¿›è¡ŒOCRå’Œæ ‡è®°åŒ–ã€‚è¿™æ ·æˆ‘ä»¬å°†å¾—åˆ°ä¸æ¨¡å‹é¢„æœŸè¾“å…¥åŒ¹é…çš„è¾“å…¥ã€‚å¦‚æœæ‚¨æƒ³æ‰‹åŠ¨å¤„ç†å›¾åƒï¼Œè¯·æŸ¥çœ‹[`LayoutLMv2`æ¨¡å‹æ–‡æ¡£](../model_doc/layoutlmv2)ä»¥äº†è§£æ¨¡å‹æœŸæœ›çš„è¾“å…¥æ ¼å¼ã€‚
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Finally, the data exploration wonâ€™t be complete if we donâ€™t peek at an image
    example.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¦‚æœæˆ‘ä»¬ä¸æŸ¥çœ‹ä¸€ä¸ªå›¾åƒç¤ºä¾‹ï¼Œæ•°æ®æ¢ç´¢å°±ä¸ä¼šå®Œæˆã€‚
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![DocVQA Image Example](../Images/b537ce4132491f7d258df4f7e115b34a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![DocVQAå›¾åƒç¤ºä¾‹](../Images/b537ce4132491f7d258df4f7e115b34a.png)'
- en: Preprocess the data
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ•°æ®
- en: The Document Question Answering task is a multimodal task, and you need to make
    sure that the inputs from each modality are preprocessed according to the modelâ€™s
    expectations. Letâ€™s start by loading the [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor),
    which internally combines an image processor that can handle image data and a
    tokenizer that can encode text data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£é—®ç­”ä»»åŠ¡æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ‚¨éœ€è¦ç¡®ä¿æ¯ç§æ¨¡æ€çš„è¾“å…¥éƒ½æŒ‰ç…§æ¨¡å‹çš„æœŸæœ›è¿›è¡Œé¢„å¤„ç†ã€‚è®©æˆ‘ä»¬ä»åŠ è½½[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)å¼€å§‹ï¼Œå®ƒå†…éƒ¨ç»“åˆäº†ä¸€ä¸ªå¯ä»¥å¤„ç†å›¾åƒæ•°æ®çš„å›¾åƒå¤„ç†å™¨å’Œä¸€ä¸ªå¯ä»¥ç¼–ç æ–‡æœ¬æ•°æ®çš„æ ‡è®°å™¨ã€‚
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Preprocessing document images
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ–‡æ¡£å›¾åƒ
- en: First, letâ€™s prepare the document images for the model with the help of the
    `image_processor` from the processor. By default, image processor resizes the
    images to 224x224, makes sure they have the correct order of color channels, applies
    OCR with tesseract to get words and normalized bounding boxes. In this tutorial,
    all of these defaults are exactly what we need. Write a function that applies
    the default image processing to a batch of images and returns the results of OCR.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬é€šè¿‡å¤„ç†å™¨ä¸­çš„`image_processor`æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ–‡æ¡£å›¾åƒã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå›¾åƒå¤„ç†å™¨å°†å›¾åƒè°ƒæ•´å¤§å°ä¸º224x224ï¼Œç¡®ä¿å®ƒä»¬å…·æœ‰æ­£ç¡®çš„é¢œè‰²é€šé“é¡ºåºï¼Œåº”ç”¨OCRä¸tesseractè·å–å•è¯å’Œå½’ä¸€åŒ–è¾¹ç•Œæ¡†ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‰€æœ‰è¿™äº›é»˜è®¤è®¾ç½®æ­£æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„ã€‚ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œå°†é»˜è®¤å›¾åƒå¤„ç†åº”ç”¨äºä¸€æ‰¹å›¾åƒï¼Œå¹¶è¿”å›OCRçš„ç»“æœã€‚
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To apply this preprocessing to the entire dataset in a fast way, use [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä»¥å¿«é€Ÿçš„æ–¹å¼å°†æ­¤é¢„å¤„ç†åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼Œè¯·ä½¿ç”¨[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)ã€‚
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Preprocessing text data
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ–‡æœ¬æ•°æ®
- en: Once we have applied OCR to the images, we need to encode the text part of the
    dataset to prepare it for the model. This involves converting the words and boxes
    that we got in the previous step to token-level `input_ids`, `attention_mask`,
    `token_type_ids` and `bbox`. For preprocessing text, weâ€™ll need the `tokenizer`
    from the processor.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬å¯¹å›¾åƒåº”ç”¨äº†OCRï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†è¿›è¡Œç¼–ç ï¼Œä»¥å‡†å¤‡æ¨¡å‹ä½¿ç”¨ã€‚è¿™æ¶‰åŠå°†æˆ‘ä»¬åœ¨ä¸Šä¸€æ­¥ä¸­è·å¾—çš„å•è¯å’Œæ¡†è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„`input_ids`ã€`attention_mask`ã€`token_type_ids`å’Œ`bbox`ã€‚å¯¹äºæ–‡æœ¬é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†éœ€è¦å¤„ç†å™¨ä¸­çš„`tokenizer`ã€‚
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: On top of the preprocessing mentioned above, we also need to add the labels
    for the model. For `xxxForQuestionAnswering` models in ğŸ¤— Transformers, the labels
    consist of the `start_positions` and `end_positions`, indicating which token is
    at the start and which token is at the end of the answer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä¸Šè¿°é¢„å¤„ç†ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸ºæ¨¡å‹æ·»åŠ æ ‡ç­¾ã€‚å¯¹äºğŸ¤— Transformersä¸­çš„`xxxForQuestionAnswering`æ¨¡å‹ï¼Œæ ‡ç­¾åŒ…æ‹¬`start_positions`å’Œ`end_positions`ï¼ŒæŒ‡ç¤ºç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸçš„æ ‡è®°åœ¨å“ªé‡Œã€‚
- en: Letâ€™s start with that. Define a helper function that can find a sublist (the
    answer split into words) in a larger list (the words list).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»è¿™é‡Œå¼€å§‹ã€‚å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå¯ä»¥åœ¨è¾ƒå¤§çš„åˆ—è¡¨ï¼ˆå•è¯åˆ—è¡¨ï¼‰ä¸­æ‰¾åˆ°ä¸€ä¸ªå­åˆ—è¡¨ï¼ˆç­”æ¡ˆæ‹†åˆ†ä¸ºå•è¯ï¼‰ã€‚
- en: This function will take two lists as input, `words_list` and `answer_list`.
    It will then iterate over the `words_list` and check if the current word in the
    `words_list` (words_list[i]) is equal to the first word of answer_list (answer_list[0])
    and if the sublist of `words_list` starting from the current word and of the same
    length as `answer_list` is equal `to answer_list`. If this condition is true,
    it means that a match has been found, and the function will record the match,
    its starting index (idx), and its ending index (idx + len(answer_list) - 1). If
    more than one match was found, the function will return only the first one. If
    no match is found, the function returns (`None`, 0, and 0).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‡½æ•°å°†æ¥å—ä¸¤ä¸ªåˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œ`words_list`å’Œ`answer_list`ã€‚ç„¶åï¼Œå®ƒå°†éå†`words_list`ï¼Œæ£€æŸ¥`words_list`ä¸­å½“å‰å•è¯ï¼ˆwords_list[i]ï¼‰æ˜¯å¦ç­‰äº`answer_list`çš„ç¬¬ä¸€ä¸ªå•è¯ï¼ˆanswer_list[0])ï¼Œä»¥åŠä»å½“å‰å•è¯å¼€å§‹ä¸”ä¸`answer_list`ç›¸åŒé•¿åº¦çš„`words_list`å­åˆ—è¡¨æ˜¯å¦ç­‰äº`answer_list`ã€‚å¦‚æœè¿™ä¸ªæ¡ä»¶ä¸ºçœŸï¼Œè¡¨ç¤ºæ‰¾åˆ°äº†åŒ¹é…ï¼Œå‡½æ•°å°†è®°å½•åŒ¹é…åŠå…¶èµ·å§‹ç´¢å¼•ï¼ˆidxï¼‰å’Œç»“æŸç´¢å¼•ï¼ˆidx
    + len(answer_list) - 1ï¼‰ã€‚å¦‚æœæ‰¾åˆ°äº†å¤šä¸ªåŒ¹é…ï¼Œå‡½æ•°å°†ä»…è¿”å›ç¬¬ä¸€ä¸ªã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œå‡½æ•°å°†è¿”å›ï¼ˆ`None`ï¼Œ0å’Œ0ï¼‰ã€‚
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To illustrate how this function finds the position of the answer, letâ€™s use
    it on an example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜æ­¤å‡½æ•°å¦‚ä½•æ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®ï¼Œè®©æˆ‘ä»¬åœ¨ä¸€ä¸ªç¤ºä¾‹ä¸Šä½¿ç”¨å®ƒï¼š
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once examples are encoded, however, they will look like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¸€æ—¦ç¤ºä¾‹è¢«ç¼–ç ï¼Œå®ƒä»¬å°†çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Weâ€™ll need to find the position of the answer in the encoded input.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ç¼–ç è¾“å…¥ä¸­ç­”æ¡ˆçš„ä½ç½®ã€‚
- en: '`token_type_ids` tells us which tokens are part of the question, and which
    ones are part of the documentâ€™s words.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`å‘Šè¯‰æˆ‘ä»¬å“ªäº›æ ‡è®°å±äºé—®é¢˜ï¼Œå“ªäº›å±äºæ–‡æ¡£çš„å•è¯ã€‚'
- en: '`tokenizer.cls_token_id` will help find the special token at the beginning
    of the input.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer.cls_token_id`å°†å¸®åŠ©æ‰¾åˆ°è¾“å…¥å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`word_ids` will help match the answer found in the original `words` to the
    same answer in the full encoded input and determine the start/end position of
    the answer in the encoded input.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_ids`å°†å¸®åŠ©å°†åŸå§‹`words`ä¸­æ‰¾åˆ°çš„ç­”æ¡ˆä¸å®Œå…¨ç¼–ç è¾“å…¥ä¸­çš„ç›¸åŒç­”æ¡ˆè¿›è¡ŒåŒ¹é…ï¼Œå¹¶ç¡®å®šç¼–ç è¾“å…¥ä¸­ç­”æ¡ˆçš„èµ·å§‹/ç»“æŸä½ç½®ã€‚'
- en: 'With that in mind, letâ€™s create a function to encode a batch of examples in
    the dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™ä¸ªæƒ³æ³•ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¯¹æ•°æ®é›†ä¸­çš„ä¸€æ‰¹ç¤ºä¾‹è¿›è¡Œç¼–ç ï¼š
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we have this preprocessing function, we can encode the entire dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼–ç ï¼š
- en: '[PRE19]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Letâ€™s check what the features of the encoded dataset look like:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ç¼–ç æ•°æ®é›†çš„ç‰¹å¾æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼š
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Evaluation
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: Evaluation for document question answering requires a significant amount of
    postprocessing. To avoid taking up too much of your time, this guide skips the
    evaluation step. The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    still calculates the evaluation loss during training so youâ€™re not completely
    in the dark about your modelâ€™s performance. Extractive question answering is typically
    evaluated using F1/exact match. If youâ€™d like to implement it yourself, check
    out the [Question Answering chapter](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)
    of the Hugging Face course for inspiration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£é—®é¢˜å›ç­”çš„è¯„ä¼°éœ€è¦å¤§é‡çš„åå¤„ç†ã€‚ä¸ºäº†é¿å…å ç”¨å¤ªå¤šæ—¶é—´ï¼Œæœ¬æŒ‡å—è·³è¿‡äº†è¯„ä¼°æ­¥éª¤ã€‚[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ä¼šè®¡ç®—è¯„ä¼°æŸå¤±ï¼Œå› æ­¤æ‚¨ä¸ä¼šå®Œå…¨ä¸äº†è§£æ¨¡å‹çš„æ€§èƒ½ã€‚æå–å¼é—®ç­”é€šå¸¸ä½¿ç”¨F1/å®Œå…¨åŒ¹é…è¿›è¡Œè¯„ä¼°ã€‚å¦‚æœæ‚¨æƒ³è‡ªå·±å®ç°ï¼Œè¯·æŸ¥çœ‹Hugging
    Faceè¯¾ç¨‹çš„[é—®ç­”ç« èŠ‚](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)è·å–çµæ„Ÿã€‚
- en: Train
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'Congratulations! Youâ€™ve successfully navigated the toughest part of this guide
    and now you are ready to train your own model. Training involves the following
    steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼æ‚¨å·²æˆåŠŸå®Œæˆæœ¬æŒ‡å—ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†ï¼Œç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚è®­ç»ƒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š
- en: Load the model with [AutoModelForDocumentQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForDocumentQuestionAnswering)
    using the same checkpoint as in the preprocessing.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸é¢„å¤„ç†ç›¸åŒçš„æ£€æŸ¥ç‚¹åŠ è½½[AutoModelForDocumentQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForDocumentQuestionAnswering)æ¨¡å‹ã€‚
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚
- en: Define a function to batch examples together, here the [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)
    will do just fine
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªå°†ç¤ºä¾‹æ‰¹å¤„ç†åœ¨ä¸€èµ·çš„å‡½æ•°ï¼Œè¿™é‡Œ[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)å°†åšå¾—å¾ˆå¥½
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, and data collator.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œä»¥åŠæ¨¡å‹ã€æ•°æ®é›†å’Œæ•°æ®æ”¶é›†å™¨ã€‚
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE21]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    use `output_dir` to specify where to save your model, and configure hyperparameters
    as you see fit. If you wish to share your model with the community, set `push_to_hub`
    to `True` (you must be signed in to Hugging Face to upload your model). In this
    case the `output_dir` will also be the name of the repo where your model checkpoint
    will be pushed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­ä½¿ç”¨`output_dir`æŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ï¼Œå¹¶æ ¹æ®éœ€è¦é…ç½®è¶…å‚æ•°ã€‚å¦‚æœå¸Œæœ›ä¸ç¤¾åŒºåˆ†äº«æ¨¡å‹ï¼Œè¯·å°†`push_to_hub`è®¾ç½®ä¸º`True`ï¼ˆæ‚¨å¿…é¡»ç™»å½•Hugging
    Faceæ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`output_dir`ä¹Ÿå°†æ˜¯å°†æ¨é€æ¨¡å‹æ£€æŸ¥ç‚¹çš„å­˜å‚¨åº“çš„åç§°ã€‚
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Define a simple data collator to batch examples together.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªç®€å•çš„æ•°æ®æ”¶é›†å™¨æ¥å°†ç¤ºä¾‹æ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, bring everything together, and call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå°†æ‰€æœ‰å†…å®¹æ±‡æ€»ï¼Œå¹¶è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)ï¼š
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To add the final model to ğŸ¤— Hub, create a model card and call `push_to_hub`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†æœ€ç»ˆæ¨¡å‹æ·»åŠ åˆ°ğŸ¤— Hubï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡å‹å¡å¹¶è°ƒç”¨`push_to_hub`ï¼š
- en: '[PRE25]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Inference
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Now that you have finetuned a LayoutLMv2 model, and uploaded it to the ğŸ¤— Hub,
    you can use it for inference. The simplest way to try out your finetuned model
    for inference is to use it in a [Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªLayoutLMv2æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ°ğŸ¤— Hubï¼Œæ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ã€‚å°è¯•ä½¿ç”¨å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)ä¸­ä½¿ç”¨å®ƒã€‚
- en: 'Letâ€™s take an example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­ï¼š
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, instantiate a pipeline for document question answering with your model,
    and pass the image + question combination to it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ‚¨çš„æ¨¡å‹ä¸ºæ–‡æ¡£é—®é¢˜å›ç­”å®ä¾‹åŒ–ä¸€ä¸ªæµæ°´çº¿ï¼Œå¹¶å°†å›¾åƒ+é—®é¢˜ç»„åˆä¼ é€’ç»™å®ƒã€‚
- en: '[PRE27]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can also manually replicate the results of the pipeline if youâ€™d like:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶æµæ°´çº¿çš„ç»“æœï¼š
- en: Take an image and a question, prepare them for the model using the processor
    from your model.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ä¸€å¼ å›¾ç‰‡å’Œä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨æ¨¡å‹çš„å¤„ç†å™¨ä¸ºå…¶å‡†å¤‡å¥½ã€‚
- en: Forward the result or preprocessing through the model.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç»“æœæˆ–é¢„å¤„ç†é€šè¿‡æ¨¡å‹å‰å‘ä¼ é€’ã€‚
- en: The model returns `start_logits` and `end_logits`, which indicate which token
    is at the start of the answer and which token is at the end of the answer. Both
    have shape (batch_size, sequence_length).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¿”å›`start_logits`å’Œ`end_logits`ï¼ŒæŒ‡ç¤ºç­”æ¡ˆèµ·å§‹å¤„å’Œç­”æ¡ˆç»“æŸå¤„çš„æ ‡è®°ã€‚ä¸¤è€…çš„å½¢çŠ¶éƒ½æ˜¯(batch_size, sequence_length)ã€‚
- en: Take an argmax on the last dimension of both the `start_logits` and `end_logits`
    to get the predicted `start_idx` and `end_idx`.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹`start_logits`å’Œ`end_logits`çš„æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œargmaxæ“ä½œï¼Œä»¥è·å–é¢„æµ‹çš„`start_idx`å’Œ`end_idx`ã€‚
- en: Decode the answer with the tokenizer.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†è¯å™¨è§£ç ç­”æ¡ˆã€‚
- en: '[PRE28]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
