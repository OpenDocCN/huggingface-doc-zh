- en: Attention Processor
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›å¤„ç†å™¨
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/attnprocessor](https://huggingface.co/docs/diffusers/api/attnprocessor)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/api/attnprocessor](https://huggingface.co/docs/diffusers/api/attnprocessor)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: An attention processor is a class for applying different types of attention
    mechanisms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›å¤„ç†å™¨æ˜¯ä¸€ä¸ªåº”ç”¨ä¸åŒç±»å‹æ³¨æ„åŠ›æœºåˆ¶çš„ç±»ã€‚
- en: AttnProcessor
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnProcessor
- en: '### `class diffusers.models.attention_processor.AttnProcessor`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.AttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)'
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Default processor for performing attention-related computations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰§è¡Œä¸æ³¨æ„åŠ›ç›¸å…³è®¡ç®—çš„é»˜è®¤å¤„ç†å™¨ã€‚
- en: AttnProcessor2_0
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.AttnProcessor2_0`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`class diffusers.models.attention_processor.AttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Processor for implementing scaled dot-product attention (enabled by default
    if youâ€™re using PyTorch 2.0).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å¤„ç†å™¨ï¼ˆå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯PyTorch 2.0ï¼Œåˆ™é»˜è®¤å¯ç”¨ï¼‰ã€‚
- en: FusedAttnProcessor2_0
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FusedAttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.FusedAttnProcessor2_0`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.FusedAttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Processor for implementing scaled dot-product attention (enabled by default
    if youâ€™re using PyTorch 2.0). It uses fused projection layers. For self-attention
    modules, all projection matrices (i.e., query, key, value) are fused. For cross-attention
    modules, key and value projection matrices are fused.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å¤„ç†å™¨ï¼ˆå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯PyTorch 2.0ï¼Œåˆ™é»˜è®¤å¯ç”¨ï¼‰ã€‚å®ƒä½¿ç”¨èåˆçš„æŠ•å½±å±‚ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚
- en: This API is currently ğŸ§ª experimental in nature and can change in future.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤APIç›®å‰å¤„äºğŸ§ªå®éªŒæ€§è´¨ï¼Œå¯èƒ½ä¼šåœ¨æœªæ¥æ›´æ”¹ã€‚
- en: LoRAAttnProcessor
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAAttnProcessor
- en: '### `class diffusers.models.attention_processor.LoRAAttnProcessor`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`, *optional*) â€” The hidden size of the attention layer.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚'
- en: '`cross_attention_dim` (`int`, *optional*) â€” The number of channels in the `encoder_hidden_states`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚'
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º4ï¼‰â€” LoRAæ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚'
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç­‰åŒäº`alpha`ï¼Œä½†å…¶ç”¨æ³•ç‰¹å®šäºKohyaï¼ˆA1111ï¼‰é£æ ¼çš„LoRAsã€‚'
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ`dict`ï¼‰â€” ä¼ é€’ç»™`LoRALinearLayer`å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Processor for implementing the LoRA attention mechanism.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå®ç°LoRAæ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ã€‚
- en: LoRAAttnProcessor2_0
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAAttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.LoRAAttnProcessor2_0`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAAttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`) â€” The hidden size of the attention layer.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`ï¼ˆ`int`ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚'
- en: '`cross_attention_dim` (`int`, *optional*) â€” The number of channels in the `encoder_hidden_states`.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚'
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º4ï¼‰â€” LoRAæ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚'
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç­‰åŒäº`alpha`ï¼Œä½†å…¶ç”¨æ³•ç‰¹å®šäºKohyaï¼ˆA1111ï¼‰é£æ ¼çš„LoRAsã€‚'
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ`dict`ï¼‰â€” ä¼ é€’ç»™`LoRALinearLayer`å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Processor for implementing the LoRA attention mechanism using PyTorch 2.0â€™s
    memory-efficient scaled dot-product attention.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºä½¿ç”¨PyTorch 2.0çš„å†…å­˜é«˜æ•ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å®ç°LoRAæ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ã€‚
- en: CustomDiffusionAttnProcessor
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CustomDiffusionAttnProcessor
- en: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`train_kv` (`bool`, defaults to `True`) â€” Whether to newly train the key and
    value matrices corresponding to the text features.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_kv`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ–‡æœ¬ç‰¹å¾å¯¹åº”çš„é”®å’Œå€¼çŸ©é˜µã€‚'
- en: '`train_q_out` (`bool`, defaults to `True`) â€” Whether to newly train query matrices
    corresponding to the latent image features.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_q_out`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ½œåœ¨å›¾åƒç‰¹å¾å¯¹åº”çš„æŸ¥è¯¢çŸ©é˜µã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚'
- en: '`out_bias` (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_bias`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨`train_q_out`ä¸­åŒ…å«åç½®å‚æ•°ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” è¦ä½¿ç”¨çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: Processor for implementing attention for the Custom Diffusion method.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå®ç°è‡ªå®šä¹‰æ‰©æ•£æ–¹æ³•çš„æ³¨æ„åŠ›å¤„ç†å™¨ã€‚
- en: CustomDiffusionAttnProcessor2_0
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CustomDiffusionAttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)'
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`train_kv` (`bool`, defaults to `True`) â€” Whether to newly train the key and
    value matrices corresponding to the text features.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_kv` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æ–°è®­ç»ƒå¯¹åº”äºæ–‡æœ¬ç‰¹å¾çš„é”®å’Œå€¼çŸ©é˜µã€‚'
- en: '`train_q_out` (`bool`, defaults to `True`) â€” Whether to newly train query matrices
    corresponding to the latent image features.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_q_out` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æ–°è®­ç»ƒå¯¹åº”äºæ½œåœ¨å›¾åƒç‰¹å¾çš„æŸ¥è¯¢çŸ©é˜µã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” `encoder_hidden_states` ä¸­çš„é€šé“æ•°ã€‚'
- en: '`out_bias` (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_bias` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨ `train_q_out` ä¸­åŒ…å«åç½®å‚æ•°ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” è¦ä½¿ç”¨çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: Processor for implementing attention for the Custom Diffusion method using PyTorch
    2.0â€™s memory-efficient scaled dot-product attention.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºä½¿ç”¨ PyTorch 2.0 çš„å†…å­˜é«˜æ•ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å®ç°è‡ªå®šä¹‰æ‰©æ•£æ–¹æ³•çš„å¤„ç†å™¨ã€‚
- en: AttnAddedKVProcessor
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnAddedKVProcessor
- en: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)'
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Processor for performing attention-related computations with extra learnable
    key and value matrices for the text encoder.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºæ‰§è¡Œä¸æ–‡æœ¬ç¼–ç å™¨çš„é¢å¤–å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µç›¸å…³çš„æ³¨æ„åŠ›è®¡ç®—çš„å¤„ç†å™¨ã€‚
- en: AttnAddedKVProcessor2_0
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnAddedKVProcessor2_0
- en: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor2_0`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)'
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Processor for performing scaled dot-product attention (enabled by default if
    youâ€™re using PyTorch 2.0), with extra learnable key and value matrices for the
    text encoder.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºæ‰§è¡Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆå¦‚æœä½¿ç”¨ PyTorch 2.0ï¼Œåˆ™é»˜è®¤å¯ç”¨ï¼‰ï¼Œå…·æœ‰é¢å¤–çš„å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µï¼Œç”¨äºæ–‡æœ¬ç¼–ç å™¨ã€‚
- en: LoRAAttnAddedKVProcessor
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAAttnAddedKVProcessor
- en: '### `class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)'
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`, *optional*) â€” The hidden size of the attention layer.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *å¯é€‰*) â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” `encoder_hidden_states` ä¸­çš„é€šé“æ•°ã€‚'
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank` (`int`, é»˜è®¤ä¸º 4) â€” LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚'
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha` (`int`, *å¯é€‰*) â€” ç­‰åŒäº `alpha`ï¼Œä½†å…¶ä½¿ç”¨æ–¹å¼ç‰¹å®šäº Kohyaï¼ˆA1111ï¼‰é£æ ¼çš„ LoRAsã€‚'
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`dict`) â€” ä¼ é€’ç»™ `LoRALinearLayer` å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Processor for implementing the LoRA attention mechanism with extra learnable
    key and value matrices for the text encoder.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå®ç° LoRA æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ï¼Œå…·æœ‰é¢å¤–çš„å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µï¼Œç”¨äºæ–‡æœ¬ç¼–ç å™¨ã€‚
- en: XFormersAttnProcessor
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XFormersAttnProcessor
- en: '### `class diffusers.models.attention_processor.XFormersAttnProcessor`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.XFormersAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)'
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op` (`Callable`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” ä½œä¸ºæ³¨æ„åŠ›æ“ä½œç¬¦ä½¿ç”¨çš„åŸºç¡€[æ“ä½œç¬¦](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)ã€‚å»ºè®®è®¾ç½®ä¸º
    `None`ï¼Œå¹¶å…è®¸ xFormers é€‰æ‹©æœ€ä½³æ“ä½œç¬¦ã€‚'
- en: Processor for implementing memory efficient attention using xFormers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºä½¿ç”¨ xFormers å®ç°å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚
- en: LoRAXFormersAttnProcessor
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAXFormersAttnProcessor
- en: '### `class diffusers.models.attention_processor.LoRAXFormersAttnProcessor`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAXFormersAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[< æºä»£ç  >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)'
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`, *optional*) â€” The hidden size of the attention layer.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *å¯é€‰*) â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚'
- en: '`cross_attention_dim` (`int`, *optional*) â€” The number of channels in the `encoder_hidden_states`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim` (`int`, *å¯é€‰*) â€” `encoder_hidden_states` ä¸­çš„é€šé“æ•°ã€‚'
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank` (`int`, é»˜è®¤ä¸º 4) â€” LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦ã€‚'
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op` (`Callable`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” ä½œä¸ºæ³¨æ„åŠ›æ“ä½œç¬¦ä½¿ç”¨çš„åŸºç¡€[æ“ä½œç¬¦](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)ã€‚å»ºè®®è®¾ç½®ä¸º
    `None`ï¼Œå¹¶å…è®¸ xFormers é€‰æ‹©æœ€ä½³æ“ä½œç¬¦ã€‚'
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha` (`int`, *å¯é€‰*) â€” ç­‰åŒäº `alpha`ï¼Œä½†å…¶ä½¿ç”¨æ–¹å¼ç‰¹å®šäº Kohyaï¼ˆA1111ï¼‰é£æ ¼çš„ LoRAsã€‚'
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ`dict`ï¼‰â€” ä¼ é€’ç»™`LoRALinearLayer`å±‚çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Processor for implementing the LoRA attention mechanism with memory efficient
    attention using xFormers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°LoRAæ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å™¨ï¼Œä½¿ç”¨xFormerså®ç°å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›ã€‚
- en: CustomDiffusionXFormersAttnProcessor
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CustomDiffusionXFormersAttnProcessor
- en: '### `class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)'
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`train_kv` (`bool`, defaults to `True`) â€” Whether to newly train the key and
    value matrices corresponding to the text features.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_kv`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ–‡æœ¬ç‰¹å¾å¯¹åº”çš„é”®å’Œå€¼çŸ©é˜µã€‚'
- en: '`train_q_out` (`bool`, defaults to `True`) â€” Whether to newly train query matrices
    corresponding to the latent image features.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_q_out`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ–°è®­ç»ƒä¸æ½œåœ¨å›¾åƒç‰¹å¾å¯¹åº”çš„æŸ¥è¯¢çŸ©é˜µã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” æ³¨æ„åŠ›å±‚çš„éšè—å¤§å°ã€‚'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” `encoder_hidden_states`ä¸­çš„é€šé“æ•°ã€‚'
- en: '`out_bias` (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_bias`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨`train_q_out`ä¸­åŒ…å«åç½®å‚æ•°ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” è¦ä½¿ç”¨çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰â€” ç”¨ä½œæ³¨æ„åŠ›æ“ä½œå™¨çš„åŸºç¡€[operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)ã€‚å»ºè®®è®¾ç½®ä¸º`None`ï¼Œå¹¶å…è®¸xFormersé€‰æ‹©æœ€ä½³æ“ä½œå™¨ã€‚'
- en: Processor for implementing memory efficient attention using xFormers for the
    Custom Diffusion method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ä½¿ç”¨xFormersè¿›è¡Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›çš„å¤„ç†å™¨ï¼Œç”¨äºè‡ªå®šä¹‰æ‰©æ•£æ–¹æ³•ã€‚
- en: SlicedAttnProcessor
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SlicedAttnProcessor
- en: '### `class diffusers.models.attention_processor.SlicedAttnProcessor`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.SlicedAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)'
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`slice_size` (`int`, *optional*) â€” The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è®¡ç®—æ³¨æ„åŠ›çš„æ­¥æ•°ã€‚ä½¿ç”¨`attention_head_dim // slice_size`ä¸ªåˆ‡ç‰‡ï¼Œ`attention_head_dim`å¿…é¡»æ˜¯`slice_size`çš„å€æ•°ã€‚'
- en: Processor for implementing sliced attention.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°åˆ‡ç‰‡æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚
- en: SlicedAttnAddedKVProcessor
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SlicedAttnAddedKVProcessor
- en: '### `class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)'
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`slice_size` (`int`, *optional*) â€” The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è®¡ç®—æ³¨æ„åŠ›çš„æ­¥æ•°ã€‚ä½¿ç”¨`attention_head_dim // slice_size`ä¸ªåˆ‡ç‰‡ï¼Œ`attention_head_dim`å¿…é¡»æ˜¯`slice_size`çš„å€æ•°ã€‚'
- en: Processor for implementing sliced attention with extra learnable key and value
    matrices for the text encoder.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°å¸¦æœ‰é¢å¤–å¯å­¦ä¹ é”®å’Œå€¼çŸ©é˜µçš„åˆ‡ç‰‡æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚
