- en: Attention Processor
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力处理器
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/attnprocessor](https://huggingface.co/docs/diffusers/api/attnprocessor)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/attnprocessor](https://huggingface.co/docs/diffusers/api/attnprocessor)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: An attention processor is a class for applying different types of attention
    mechanisms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力处理器是一个应用不同类型注意力机制的类。
- en: AttnProcessor
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnProcessor
- en: '### `class diffusers.models.attention_processor.AttnProcessor`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.AttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)'
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Default processor for performing attention-related computations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 执行与注意力相关计算的默认处理器。
- en: AttnProcessor2_0
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.AttnProcessor2_0`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`class diffusers.models.attention_processor.AttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Processor for implementing scaled dot-product attention (enabled by default
    if you’re using PyTorch 2.0).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实现缩放点积注意力的处理器（如果您使用的是PyTorch 2.0，则默认启用）。
- en: FusedAttnProcessor2_0
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FusedAttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.FusedAttnProcessor2_0`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.FusedAttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Processor for implementing scaled dot-product attention (enabled by default
    if you’re using PyTorch 2.0). It uses fused projection layers. For self-attention
    modules, all projection matrices (i.e., query, key, value) are fused. For cross-attention
    modules, key and value projection matrices are fused.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实现缩放点积注意力的处理器（如果您使用的是PyTorch 2.0，则默认启用）。它使用融合的投影层。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。
- en: This API is currently 🧪 experimental in nature and can change in future.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此API目前处于🧪实验性质，可能会在未来更改。
- en: LoRAAttnProcessor
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAAttnProcessor
- en: '### `class diffusers.models.attention_processor.LoRAAttnProcessor`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*) — The hidden size of the attention layer.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*）— 注意力层的隐藏大小。'
- en: '`cross_attention_dim` (`int`, *optional*) — The number of channels in the `encoder_hidden_states`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`（`int`，*可选*）— `encoder_hidden_states`中的通道数。'
- en: '`rank` (`int`, defaults to 4) — The dimension of the LoRA update matrices.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`（`int`，默认为4）— LoRA更新矩阵的维度。'
- en: '`network_alpha` (`int`, *optional*) — Equivalent to `alpha` but it’s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha`（`int`，*可选*）— 等同于`alpha`，但其用法特定于Kohya（A1111）风格的LoRAs。'
- en: '`kwargs` (`dict`) — Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`dict`）— 传递给`LoRALinearLayer`层的额外关键字参数。'
- en: Processor for implementing the LoRA attention mechanism.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实现LoRA注意力机制的处理器。
- en: LoRAAttnProcessor2_0
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAAttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.LoRAAttnProcessor2_0`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAAttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`) — The hidden size of the attention layer.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`）— 注意力层的隐藏大小。'
- en: '`cross_attention_dim` (`int`, *optional*) — The number of channels in the `encoder_hidden_states`.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`（`int`，*可选*）— `encoder_hidden_states`中的通道数。'
- en: '`rank` (`int`, defaults to 4) — The dimension of the LoRA update matrices.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`（`int`，默认为4）— LoRA更新矩阵的维度。'
- en: '`network_alpha` (`int`, *optional*) — Equivalent to `alpha` but it’s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha`（`int`，*可选*）— 等同于`alpha`，但其用法特定于Kohya（A1111）风格的LoRAs。'
- en: '`kwargs` (`dict`) — Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`dict`）— 传递给`LoRALinearLayer`层的额外关键字参数。'
- en: Processor for implementing the LoRA attention mechanism using PyTorch 2.0’s
    memory-efficient scaled dot-product attention.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用PyTorch 2.0的内存高效缩放点积注意力实现LoRA注意力机制的处理器。
- en: CustomDiffusionAttnProcessor
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CustomDiffusionAttnProcessor
- en: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`train_kv` (`bool`, defaults to `True`) — Whether to newly train the key and
    value matrices corresponding to the text features.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_kv`（`bool`，默认为`True`）— 是否新训练与文本特征对应的键和值矩阵。'
- en: '`train_q_out` (`bool`, defaults to `True`) — Whether to newly train query matrices
    corresponding to the latent image features.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_q_out`（`bool`，默认为`True`）— 是否新训练与潜在图像特征对应的查询矩阵。'
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) — The hidden size of
    the attention layer.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*，默认为`None`）— 注意力层的隐藏大小。'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) — The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`（`int`，*可选*，默认为`None`）— `encoder_hidden_states`中的通道数。'
- en: '`out_bias` (`bool`, defaults to `True`) — Whether to include the bias parameter
    in `train_q_out`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_bias`（`bool`，默认为`True`）— 是否在`train_q_out`中包含偏置参数。'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    to use.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`（`float`，*可选*，默认为0.0）— 要使用的丢弃概率。'
- en: Processor for implementing attention for the Custom Diffusion method.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实现自定义扩散方法的注意力处理器。
- en: CustomDiffusionAttnProcessor2_0
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CustomDiffusionAttnProcessor2_0
- en: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)'
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`train_kv` (`bool`, defaults to `True`) — Whether to newly train the key and
    value matrices corresponding to the text features.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_kv` (`bool`, 默认为 `True`) — 是否新训练对应于文本特征的键和值矩阵。'
- en: '`train_q_out` (`bool`, defaults to `True`) — Whether to newly train query matrices
    corresponding to the latent image features.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_q_out` (`bool`, 默认为 `True`) — 是否新训练对应于潜在图像特征的查询矩阵。'
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) — The hidden size of
    the attention layer.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为 `None`) — 注意力层的隐藏大小。'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) — The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim` (`int`, *可选*, 默认为 `None`) — `encoder_hidden_states` 中的通道数。'
- en: '`out_bias` (`bool`, defaults to `True`) — Whether to include the bias parameter
    in `train_q_out`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_bias` (`bool`, 默认为 `True`) — 是否在 `train_q_out` 中包含偏置参数。'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    to use.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *可选*, 默认为 0.0) — 要使用的丢弃概率。'
- en: Processor for implementing attention for the Custom Diffusion method using PyTorch
    2.0’s memory-efficient scaled dot-product attention.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用 PyTorch 2.0 的内存高效缩放点积注意力实现自定义扩散方法的处理器。
- en: AttnAddedKVProcessor
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnAddedKVProcessor
- en: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)'
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Processor for performing attention-related computations with extra learnable
    key and value matrices for the text encoder.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行与文本编码器的额外可学习键和值矩阵相关的注意力计算的处理器。
- en: AttnAddedKVProcessor2_0
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AttnAddedKVProcessor2_0
- en: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor2_0`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor2_0`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)'
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Processor for performing scaled dot-product attention (enabled by default if
    you’re using PyTorch 2.0), with extra learnable key and value matrices for the
    text encoder.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行缩放点积注意力（如果使用 PyTorch 2.0，则默认启用），具有额外的可学习键和值矩阵，用于文本编码器。
- en: LoRAAttnAddedKVProcessor
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAAttnAddedKVProcessor
- en: '### `class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)'
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*) — The hidden size of the attention layer.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*) — 注意力层的隐藏大小。'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) — The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim` (`int`, *可选*, 默认为 `None`) — `encoder_hidden_states` 中的通道数。'
- en: '`rank` (`int`, defaults to 4) — The dimension of the LoRA update matrices.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank` (`int`, 默认为 4) — LoRA 更新矩阵的维度。'
- en: '`network_alpha` (`int`, *optional*) — Equivalent to `alpha` but it’s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha` (`int`, *可选*) — 等同于 `alpha`，但其使用方式特定于 Kohya（A1111）风格的 LoRAs。'
- en: '`kwargs` (`dict`) — Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`dict`) — 传递给 `LoRALinearLayer` 层的额外关键字参数。'
- en: Processor for implementing the LoRA attention mechanism with extra learnable
    key and value matrices for the text encoder.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实现 LoRA 注意力机制的处理器，具有额外的可学习键和值矩阵，用于文本编码器。
- en: XFormersAttnProcessor
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XFormersAttnProcessor
- en: '### `class diffusers.models.attention_processor.XFormersAttnProcessor`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.XFormersAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)'
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) — The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op` (`Callable`, *可选*, 默认为 `None`) — 作为注意力操作符使用的基础[操作符](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)。建议设置为
    `None`，并允许 xFormers 选择最佳操作符。'
- en: Processor for implementing memory efficient attention using xFormers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用 xFormers 实现内存高效注意力的处理器。
- en: LoRAXFormersAttnProcessor
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRAXFormersAttnProcessor
- en: '### `class diffusers.models.attention_processor.LoRAXFormersAttnProcessor`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.LoRAXFormersAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)'
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*) — The hidden size of the attention layer.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*) — 注意力层的隐藏大小。'
- en: '`cross_attention_dim` (`int`, *optional*) — The number of channels in the `encoder_hidden_states`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim` (`int`, *可选*) — `encoder_hidden_states` 中的通道数。'
- en: '`rank` (`int`, defaults to 4) — The dimension of the LoRA update matrices.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank` (`int`, 默认为 4) — LoRA 更新矩阵的维度。'
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) — The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op` (`Callable`, *可选*, 默认为 `None`) — 作为注意力操作符使用的基础[操作符](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)。建议设置为
    `None`，并允许 xFormers 选择最佳操作符。'
- en: '`network_alpha` (`int`, *optional*) — Equivalent to `alpha` but it’s usage
    is specific to Kohya (A1111) style LoRAs.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_alpha` (`int`, *可选*) — 等同于 `alpha`，但其使用方式特定于 Kohya（A1111）风格的 LoRAs。'
- en: '`kwargs` (`dict`) — Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`dict`）— 传递给`LoRALinearLayer`层的额外关键字参数。'
- en: Processor for implementing the LoRA attention mechanism with memory efficient
    attention using xFormers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 实现LoRA注意力机制的处理器，使用xFormers实现内存高效的注意力。
- en: CustomDiffusionXFormersAttnProcessor
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CustomDiffusionXFormersAttnProcessor
- en: '### `class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)'
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`train_kv` (`bool`, defaults to `True`) — Whether to newly train the key and
    value matrices corresponding to the text features.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_kv`（`bool`，默认为`True`）— 是否新训练与文本特征对应的键和值矩阵。'
- en: '`train_q_out` (`bool`, defaults to `True`) — Whether to newly train query matrices
    corresponding to the latent image features.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_q_out`（`bool`，默认为`True`）— 是否新训练与潜在图像特征对应的查询矩阵。'
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) — The hidden size of
    the attention layer.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*，默认为`None`）— 注意力层的隐藏大小。'
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) — The number
    of channels in the `encoder_hidden_states`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_dim`（`int`，*可选*，默认为`None`）— `encoder_hidden_states`中的通道数。'
- en: '`out_bias` (`bool`, defaults to `True`) — Whether to include the bias parameter
    in `train_q_out`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_bias`（`bool`，默认为`True`）— 是否在`train_q_out`中包含偏置参数。'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    to use.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`（`float`，*可选*，默认为0.0）— 要使用的丢弃概率。'
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) — The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op`（`Callable`，*可选*，默认为`None`）— 用作注意力操作器的基础[operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)。建议设置为`None`，并允许xFormers选择最佳操作器。'
- en: Processor for implementing memory efficient attention using xFormers for the
    Custom Diffusion method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 实现使用xFormers进行内存高效注意力的处理器，用于自定义扩散方法。
- en: SlicedAttnProcessor
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SlicedAttnProcessor
- en: '### `class diffusers.models.attention_processor.SlicedAttnProcessor`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.SlicedAttnProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)'
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`slice_size` (`int`, *optional*) — The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_size`（`int`，*可选*）— 计算注意力的步数。使用`attention_head_dim // slice_size`个切片，`attention_head_dim`必须是`slice_size`的倍数。'
- en: Processor for implementing sliced attention.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实现切片注意力的处理器。
- en: SlicedAttnAddedKVProcessor
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SlicedAttnAddedKVProcessor
- en: '### `class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)'
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`slice_size` (`int`, *optional*) — The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_size`（`int`，*可选*）— 计算注意力的步数。使用`attention_head_dim // slice_size`个切片，`attention_head_dim`必须是`slice_size`的倍数。'
- en: Processor for implementing sliced attention with extra learnable key and value
    matrices for the text encoder.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实现带有额外可学习键和值矩阵的切片注意力的处理器。
