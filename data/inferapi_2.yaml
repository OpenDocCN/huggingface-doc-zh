- en: 🤗 Hosted Inference API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗 托管推理API
- en: 'Original text: [https://huggingface.co/docs/api-inference/index](https://huggingface.co/docs/api-inference/index)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/api-inference/index](https://huggingface.co/docs/api-inference/index)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Test and evaluate, for free, over 150,000 publicly accessible machine learning
    models, or your own private models, via simple HTTP requests, with fast inference
    hosted on Hugging Face shared infrastructure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 免费测试和评估超过150,000个公开可访问的机器学习模型，或您自己的私有模型，通过简单的HTTP请求，快速推理托管在Hugging Face共享基础设施上。
- en: The Inference API is free to use, and rate limited. If you need an inference
    solution for production, check out our [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)
    service. With Inference Endpoints, you can easily deploy any machine learning
    model on dedicated and fully managed infrastructure. Select the cloud, region,
    compute instance, autoscaling range and security level to match your model, latency,
    throughput, and compliance needs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 推理API可免费使用，并受速率限制。如果您需要用于生产的推理解决方案，请查看我们的[推理端点](https://huggingface.co/docs/inference-endpoints/index)服务。通过推理端点，您可以轻松部署任何机器学习模型在专用和完全托管的基础设施上。选择云、区域、计算实例、自动缩放范围和安全级别以匹配您的模型、延迟、吞吐量和合规需求。
- en: 'Main features:'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要特点：
- en: Get predictions from **150,000+ Transformers, Diffusers, or Timm models** (T5,
    Blenderbot, Bart, GPT-2, Pegasus...)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从**150,000+个Transformers、Diffusers或Timm模型**（T5、Blenderbot、Bart、GPT-2、Pegasus...）获取预测
- en: Use built-in integrations with **over 20 Open-Source libraries** (spaCy, SpeechBrain,
    Keras, etc).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置集成与**超过20个开源库**（spaCy、SpeechBrain、Keras等）。
- en: Switch from one model to the next by just switching the model ID
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过仅切换模型ID来切换到下一个模型
- en: Upload, manage and serve your **own models privately**
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传、管理和私密地提供您的**自己的模型**
- en: Run Classification, Image Segmentation, Automatic Speech Recognition, NER, Conversational,
    Summarization, Translation, Question-Answering, Embeddings Extraction tasks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行分类、图像分割、自动语音识别、NER、对话、摘要、翻译、问答、嵌入提取任务
- en: Out of the box accelerated inference on **CPU** powered by Intel Xeon Ice Lake
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Intel Xeon Ice Lake提供动力的**CPU**上的即插即用加速推理
- en: 'Third-party library models:'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三方库模型：
- en: The [Hub](https://huggingface.co) supports many new libraries, such as SpaCy,
    Timm, Keras, fastai, and more. You can read the full list [here](https://hf.co/docs/hub/libraries).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hub](https://huggingface.co)支持许多新库，如SpaCy、Timm、Keras、fastai等。您可以在[这里](https://hf.co/docs/hub/libraries)阅读完整列表。'
- en: Those models are enabled on the API thanks to some docker integration [api-inference-community](https://github.com/huggingface/api-inference-community/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型得以在API上启用，得益于一些docker集成[api-inference-community](https://github.com/huggingface/api-inference-community/)。
- en: 'Please note however, that these models will not allow you ([tracking issue](https://github.com/huggingface/huggingface_hub/issues/85)):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，这些模型将不允许您（[跟踪问题](https://github.com/huggingface/huggingface_hub/issues/85)）：
- en: To get full optimization
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获得完整的优化
- en: To run private models
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行私有模型
- en: To get access to GPU inference
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获得GPU推理访问权限
- en: If you are looking for custom support from the Hugging Face team
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果您正在寻找来自Hugging Face团队的定制支持
- en: '[![HuggingFace Expert Acceleration Program](../Images/4e3f8848a914f36cc180b9e654070ef1.png)](https://huggingface.co/support)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[![HuggingFace专家加速计划](../Images/4e3f8848a914f36cc180b9e654070ef1.png)](https://huggingface.co/support)'
- en: Hugging Face is trusted in production by over 10,000 companies
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face在生产中受到超过10,000家公司的信任
- en: '![](../Images/6ae7702af262288012014841128c4a68.png) ![](../Images/224eaf3c23f0fc3f8f157c596cd4b70c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ae7702af262288012014841128c4a68.png) ![](../Images/224eaf3c23f0fc3f8f157c596cd4b70c.png)'
