- en: ğŸ¤— Hosted Inference API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤— æ‰˜ç®¡æ¨ç†API
- en: 'Original text: [https://huggingface.co/docs/api-inference/index](https://huggingface.co/docs/api-inference/index)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/api-inference/index](https://huggingface.co/docs/api-inference/index)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Test and evaluate, for free, over 150,000 publicly accessible machine learning
    models, or your own private models, via simple HTTP requests, with fast inference
    hosted on Hugging Face shared infrastructure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å…è´¹æµ‹è¯•å’Œè¯„ä¼°è¶…è¿‡150,000ä¸ªå…¬å¼€å¯è®¿é—®çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæˆ–æ‚¨è‡ªå·±çš„ç§æœ‰æ¨¡å‹ï¼Œé€šè¿‡ç®€å•çš„HTTPè¯·æ±‚ï¼Œå¿«é€Ÿæ¨ç†æ‰˜ç®¡åœ¨Hugging Faceå…±äº«åŸºç¡€è®¾æ–½ä¸Šã€‚
- en: The Inference API is free to use, and rate limited. If you need an inference
    solution for production, check out our [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)
    service. With Inference Endpoints, you can easily deploy any machine learning
    model on dedicated and fully managed infrastructure. Select the cloud, region,
    compute instance, autoscaling range and security level to match your model, latency,
    throughput, and compliance needs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†APIå¯å…è´¹ä½¿ç”¨ï¼Œå¹¶å—é€Ÿç‡é™åˆ¶ã€‚å¦‚æœæ‚¨éœ€è¦ç”¨äºç”Ÿäº§çš„æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„[æ¨ç†ç«¯ç‚¹](https://huggingface.co/docs/inference-endpoints/index)æœåŠ¡ã€‚é€šè¿‡æ¨ç†ç«¯ç‚¹ï¼Œæ‚¨å¯ä»¥è½»æ¾éƒ¨ç½²ä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ä¸“ç”¨å’Œå®Œå…¨æ‰˜ç®¡çš„åŸºç¡€è®¾æ–½ä¸Šã€‚é€‰æ‹©äº‘ã€åŒºåŸŸã€è®¡ç®—å®ä¾‹ã€è‡ªåŠ¨ç¼©æ”¾èŒƒå›´å’Œå®‰å…¨çº§åˆ«ä»¥åŒ¹é…æ‚¨çš„æ¨¡å‹ã€å»¶è¿Ÿã€ååé‡å’Œåˆè§„éœ€æ±‚ã€‚
- en: 'Main features:'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»è¦ç‰¹ç‚¹ï¼š
- en: Get predictions from **150,000+ Transformers, Diffusers, or Timm models** (T5,
    Blenderbot, Bart, GPT-2, Pegasus...)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»**150,000+ä¸ªTransformersã€Diffusersæˆ–Timmæ¨¡å‹**ï¼ˆT5ã€Blenderbotã€Bartã€GPT-2ã€Pegasus...ï¼‰è·å–é¢„æµ‹
- en: Use built-in integrations with **over 20 Open-Source libraries** (spaCy, SpeechBrain,
    Keras, etc).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å†…ç½®é›†æˆä¸**è¶…è¿‡20ä¸ªå¼€æºåº“**ï¼ˆspaCyã€SpeechBrainã€Kerasç­‰ï¼‰ã€‚
- en: Switch from one model to the next by just switching the model ID
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä»…åˆ‡æ¢æ¨¡å‹IDæ¥åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
- en: Upload, manage and serve your **own models privately**
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šä¼ ã€ç®¡ç†å’Œç§å¯†åœ°æä¾›æ‚¨çš„**è‡ªå·±çš„æ¨¡å‹**
- en: Run Classification, Image Segmentation, Automatic Speech Recognition, NER, Conversational,
    Summarization, Translation, Question-Answering, Embeddings Extraction tasks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œåˆ†ç±»ã€å›¾åƒåˆ†å‰²ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€NERã€å¯¹è¯ã€æ‘˜è¦ã€ç¿»è¯‘ã€é—®ç­”ã€åµŒå…¥æå–ä»»åŠ¡
- en: Out of the box accelerated inference on **CPU** powered by Intel Xeon Ice Lake
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±Intel Xeon Ice Lakeæä¾›åŠ¨åŠ›çš„**CPU**ä¸Šçš„å³æ’å³ç”¨åŠ é€Ÿæ¨ç†
- en: 'Third-party library models:'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ–¹åº“æ¨¡å‹ï¼š
- en: The [Hub](https://huggingface.co) supports many new libraries, such as SpaCy,
    Timm, Keras, fastai, and more. You can read the full list [here](https://hf.co/docs/hub/libraries).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hub](https://huggingface.co)æ”¯æŒè®¸å¤šæ–°åº“ï¼Œå¦‚SpaCyã€Timmã€Kerasã€fastaiç­‰ã€‚æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://hf.co/docs/hub/libraries)é˜…è¯»å®Œæ•´åˆ—è¡¨ã€‚'
- en: Those models are enabled on the API thanks to some docker integration [api-inference-community](https://github.com/huggingface/api-inference-community/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹å¾—ä»¥åœ¨APIä¸Šå¯ç”¨ï¼Œå¾—ç›Šäºä¸€äº›dockeré›†æˆ[api-inference-community](https://github.com/huggingface/api-inference-community/)ã€‚
- en: 'Please note however, that these models will not allow you ([tracking issue](https://github.com/huggingface/huggingface_hub/issues/85)):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹å°†ä¸å…è®¸æ‚¨ï¼ˆ[è·Ÿè¸ªé—®é¢˜](https://github.com/huggingface/huggingface_hub/issues/85)ï¼‰ï¼š
- en: To get full optimization
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦è·å¾—å®Œæ•´çš„ä¼˜åŒ–
- en: To run private models
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œç§æœ‰æ¨¡å‹
- en: To get access to GPU inference
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦è·å¾—GPUæ¨ç†è®¿é—®æƒé™
- en: If you are looking for custom support from the Hugging Face team
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨å¯»æ‰¾æ¥è‡ªHugging Faceå›¢é˜Ÿçš„å®šåˆ¶æ”¯æŒ
- en: '[![HuggingFace Expert Acceleration Program](../Images/4e3f8848a914f36cc180b9e654070ef1.png)](https://huggingface.co/support)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[![HuggingFaceä¸“å®¶åŠ é€Ÿè®¡åˆ’](../Images/4e3f8848a914f36cc180b9e654070ef1.png)](https://huggingface.co/support)'
- en: Hugging Face is trusted in production by over 10,000 companies
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Faceåœ¨ç”Ÿäº§ä¸­å—åˆ°è¶…è¿‡10,000å®¶å…¬å¸çš„ä¿¡ä»»
- en: '![](../Images/6ae7702af262288012014841128c4a68.png) ![](../Images/224eaf3c23f0fc3f8f157c596cd4b70c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ae7702af262288012014841128c4a68.png) ![](../Images/224eaf3c23f0fc3f8f157c596cd4b70c.png)'
