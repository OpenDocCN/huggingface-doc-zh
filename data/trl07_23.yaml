- en: Examples of using peft with trl to finetune 8-bit models with Low Rank Adaption
    (LoRA)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用peft与trl示例，对使用低秩适应（LoRA）微调8位模型的示例。
- en: 'Original text: [https://huggingface.co/docs/trl/lora_tuning_peft](https://huggingface.co/docs/trl/lora_tuning_peft)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/trl/lora_tuning_peft](https://huggingface.co/docs/trl/lora_tuning_peft)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The notebooks and scripts in this examples show how to use Low Rank Adaptation
    (LoRA) to fine-tune models in a memory efficient manner. Most of PEFT methods
    supported in peft library but note that some PEFT methods such as Prompt tuning
    are not supported. For more information on LoRA, see the [original paper](https://arxiv.org/abs/2106.09685).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例中的笔记本和脚本展示了如何使用低秩适应（LoRA）以内存高效的方式微调模型。peft库支持大多数PEFT方法，但请注意，某些PEFT方法，如提示调整，不受支持。有关LoRA的更多信息，请参阅[原始论文](https://arxiv.org/abs/2106.09685)。
- en: 'Here’s an overview of the `peft`-enabled notebooks and scripts in the [trl
    repository](https://github.com/huggingface/trl/tree/main/examples):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是[trl存储库](https://github.com/huggingface/trl/tree/main/examples)中启用了`peft`的笔记本和脚本的概述：
- en: '| File | Task | Description |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 文件 | 任务 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [`stack_llama/rl_training.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/rl_training.py)
    | RLHF | Distributed fine-tuning of the 7b parameter LLaMA models with a learned
    reward model and `peft`. |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [`stack_llama/rl_training.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/rl_training.py)
    | RLHF | 使用学习奖励模型和`peft`对7b参数LLaMA模型进行分布式微调。 |'
- en: '| [`stack_llama/reward_modeling.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/reward_modeling.py)
    | Reward Modeling | Distributed training of the 7b parameter LLaMA reward model
    with `peft`. |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| [`stack_llama/reward_modeling.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/reward_modeling.py)
    | 奖励建模 | 使用`peft`对7b参数LLaMA奖励模型进行分布式训练。 |'
- en: '| [`stack_llama/supervised_finetuning.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/supervised_finetuning.py)
    | SFT | Distributed instruction/supervised fine-tuning of the 7b parameter LLaMA
    model with `peft`. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| [`stack_llama/supervised_finetuning.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/supervised_finetuning.py)
    | SFT | 使用`peft`对7b参数LLaMA模型进行分布式指导/监督微调。 |'
- en: Installation
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'Note: peft is in active development, so we install directly from their Github
    page. Peft also relies on the latest version of transformers.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：peft正在积极开发中，因此我们直接从他们的Github页面安装。Peft还依赖于transformers的最新版本。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: if you don’t want to log with `wandb` remove `log_with="wandb"` in the
    scripts/notebooks. You can also replace it with your favourite experiment tracker
    that’s [supported by `accelerate`](https://huggingface.co/docs/accelerate/usage_guides/tracking).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您不想使用`wandb`记录日志，请在脚本/笔记本中删除`log_with="wandb"`。您也可以用您喜欢的实验追踪器替换它，该追踪器由`accelerate`支持。
- en: How to use it?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用？
- en: Simply declare a `PeftConfig` object in your script and pass it through `.from_pretrained`
    to load the TRL+PEFT model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在脚本中声明一个`PeftConfig`对象，并通过`.from_pretrained`传递它以加载TRL+PEFT模型。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And if you want to load your model in 8bit precision:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想以8位精度加载模型：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '… or in 4bit precision:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: … 或者使用4位精度：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Launch scripts
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动脚本
- en: 'The `trl` library is powered by `accelerate`. As such it is best to configure
    and launch trainings with the following commands:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`trl`库由`accelerate`提供支持。因此，最好使用以下命令配置和启动训练：'
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using trl + peft and Data Parallelism
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用trl + peft和数据并行性
- en: 'You can scale up to as many GPUs as you want, as long as you are able to fit
    the training process in a single device. The only tweak you need to apply is to
    load the model as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 只要您能够将训练过程适配到单个设备中，您可以扩展到尽可能多的GPU。您需要应用的唯一调整是按以下方式加载模型：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And if you want to load your model in 8bit precision:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想以8位精度加载模型：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '… or in 4bit precision:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: … 或者使用4位精度：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, make sure that the rewards are computed on correct device as well,
    for that you can use `ppo_trainer.model.current_device`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保奖励也在正确的设备上计算，为此，您可以使用`ppo_trainer.model.current_device`。
- en: Naive pipeline parallelism (NPP) for large models (>60B models)
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型模型（>60B模型）的天真管道并行性（NPP）
- en: The `trl` library also supports naive pipeline parallelism (NPP) for large models
    (>60B models). This is a simple way to parallelize the model across multiple GPUs.
    This paradigm, termed as “Naive Pipeline Parallelism” (NPP) is a simple way to
    parallelize the model across multiple GPUs. We load the model and the adapters
    across multiple GPUs and the activations and gradients will be naively communicated
    across the GPUs. This supports `int8` models as well as other `dtype` models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`trl`库还支持大型模型（>60B模型）的天真管道并行性（NPP）。这是一种简单的方式，可以在多个GPU上并行化模型。这种范式被称为“天真管道并行性”（NPP），是一种简单的方式，可以在多个GPU上并行化模型。我们将模型和适配器加载到多个GPU上，激活和梯度将在GPU之间天真地通信。这支持`int8`模型以及其他`dtype`模型。'
- en: '![](../Images/3cc560cbbb52fcacc592d467e360f3f4.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cc560cbbb52fcacc592d467e360f3f4.png)'
- en: How to use NPP?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用NPP？
- en: Simply load your model with a custom `device_map` argument on the `from_pretrained`
    to split your model across multiple devices. Check out this [nice tutorial](https://github.com/huggingface/blog/blob/main/accelerate-large-models.md)
    on how to properly create a `device_map` for your model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在`from_pretrained`上使用自定义的`device_map`参数加载您的模型，以将模型分割到多个设备上。查看这篇[不错的教程](https://github.com/huggingface/blog/blob/main/accelerate-large-models.md)，了解如何为您的模型正确创建`device_map`。
- en: 'Also make sure to have the `lm_head` module on the first GPU device as it may
    throw an error if it is not on the first device. As this time of writing, you
    need to install the `main` branch of `accelerate`: `pip install git+https://github.com/huggingface/accelerate.git@main`
    and `peft`: `pip install git+https://github.com/huggingface/peft.git@main`.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 还要确保将`lm_head`模块放在第一个GPU设备上，否则可能会出错。在撰写本文时，您需要安装`accelerate`的`main`分支：`pip install
    git+https://github.com/huggingface/accelerate.git@main`和`peft`：`pip install git+https://github.com/huggingface/peft.git@main`。
- en: Launch scripts
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动脚本
- en: Although `trl` library is powered by `accelerate`, you should run your training
    script in a single process. Note that we do not support Data Parallelism together
    with NPP yet.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`trl`库由`accelerate`提供支持，但您应该在单个进程中运行训练脚本。请注意，我们目前不支持数据并行处理与NPP一起使用。
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Fine-tuning Llama-2 model
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调Llama-2模型
- en: 'You can easily fine-tune Llama2 model using `SFTTrainer` and the official script!
    For example to fine-tune llama2-7b on the Guanaco dataset, run (tested on a single
    NVIDIA T4-16GB):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`SFTTrainer`和官方脚本轻松地对Llama2模型进行微调！例如，要在Guanaco数据集上对llama2-7b进行微调，请运行（在单个NVIDIA
    T4-16GB上测试过）：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
