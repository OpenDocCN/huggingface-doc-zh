- en: Image classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/tjAIM7BOYhw](https://www.youtube-nocookie.com/embed/tjAIM7BOYhw)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/tjAIM7BOYhw](https://www.youtube-nocookie.com/embed/tjAIM7BOYhw)'
- en: Image classification assigns a label or class to an image. Unlike text or audio
    classification, the inputs are the pixel values that comprise an image. There
    are many applications for image classification, such as detecting damage after
    a natural disaster, monitoring crop health, or helping screen medical images for
    signs of disease.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类为图像分配一个标签或类别。与文本或音频分类不同，输入是组成图像的像素值。图像分类有许多应用，例如在自然灾害后检测损坏、监测作物健康或帮助筛查医学图像中的疾病迹象。
- en: 'This guide illustrates how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南说明了如何：
- en: Fine-tune [ViT](model_doc/vit) on the [Food-101](https://huggingface.co/datasets/food101)
    dataset to classify a food item in an image.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[Food-101](https://huggingface.co/datasets/food101)数据集上对[ViT](model_doc/vit)进行微调，以对图像中的食物项目进行分类。
- en: Use your fine-tuned model for inference.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您微调的模型进行推断。
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中所示的任务由以下模型架构支持：
- en: '[BEiT](../model_doc/beit), [BiT](../model_doc/bit), [ConvNeXT](../model_doc/convnext),
    [ConvNeXTV2](../model_doc/convnextv2), [CvT](../model_doc/cvt), [Data2VecVision](../model_doc/data2vec-vision),
    [DeiT](../model_doc/deit), [DiNAT](../model_doc/dinat), [DINOv2](../model_doc/dinov2),
    [EfficientFormer](../model_doc/efficientformer), [EfficientNet](../model_doc/efficientnet),
    [FocalNet](../model_doc/focalnet), [ImageGPT](../model_doc/imagegpt), [LeViT](../model_doc/levit),
    [MobileNetV1](../model_doc/mobilenet_v1), [MobileNetV2](../model_doc/mobilenet_v2),
    [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2),
    [NAT](../model_doc/nat), [Perceiver](../model_doc/perceiver), [PoolFormer](../model_doc/poolformer),
    [PVT](../model_doc/pvt), [RegNet](../model_doc/regnet), [ResNet](../model_doc/resnet),
    [SegFormer](../model_doc/segformer), [SwiftFormer](../model_doc/swiftformer),
    [Swin Transformer](../model_doc/swin), [Swin Transformer V2](../model_doc/swinv2),
    [VAN](../model_doc/van), [ViT](../model_doc/vit), [ViT Hybrid](../model_doc/vit_hybrid),
    [ViTMSN](../model_doc/vit_msn)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[BEiT](../model_doc/beit)、[BiT](../model_doc/bit)、[ConvNeXT](../model_doc/convnext)、[ConvNeXTV2](../model_doc/convnextv2)、[CvT](../model_doc/cvt)、[Data2VecVision](../model_doc/data2vec-vision)、[DeiT](../model_doc/deit)、[DiNAT](../model_doc/dinat)、[DINOv2](../model_doc/dinov2)、[EfficientFormer](../model_doc/efficientformer)、[EfficientNet](../model_doc/efficientnet)、[FocalNet](../model_doc/focalnet)、[ImageGPT](../model_doc/imagegpt)、[LeViT](../model_doc/levit)、[MobileNetV1](../model_doc/mobilenet_v1)、[MobileNetV2](../model_doc/mobilenet_v2)、[MobileViT](../model_doc/mobilevit)、[MobileViTV2](../model_doc/mobilevitv2)、[NAT](../model_doc/nat)、[Perceiver](../model_doc/perceiver)、[PoolFormer](../model_doc/poolformer)、[PVT](../model_doc/pvt)、[RegNet](../model_doc/regnet)、[ResNet](../model_doc/resnet)、[SegFormer](../model_doc/segformer)、[SwiftFormer](../model_doc/swiftformer)、[Swin
    Transformer](../model_doc/swin)、[Swin Transformer V2](../model_doc/swinv2)、[VAN](../model_doc/van)、[ViT](../model_doc/vit)、[ViT
    Hybrid](../model_doc/vit_hybrid)、[ViTMSN](../model_doc/vit_msn)'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保您已安装所有必要的库：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to log in to your Hugging Face account to upload and share
    your model with the community. When prompted, enter your token to log in:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您登录您的Hugging Face帐户，以便上传和与社区分享您的模型。在提示时，输入您的令牌以登录：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load Food-101 dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载Food-101数据集
- en: Start by loading a smaller subset of the Food-101 dataset from the 🤗 Datasets
    library. This will give you a chance to experiment and make sure everything works
    before spending more time training on the full dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从🤗数据集库中加载Food-101数据集的一个较小子集。这将让您有机会进行实验，并确保一切正常，然后再花更多时间在完整数据集上进行训练。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Split the dataset’s `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)方法将数据集的`train`拆分为训练集和测试集：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then take a look at an example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后看一个示例：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Each example in the dataset has two fields:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每个示例都有两个字段：
- en: '`image`: a PIL image of the food item'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`：食物项目的PIL图像'
- en: '`label`: the label class of the food item'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`：食物项目的标签类别'
- en: 'To make it easier for the model to get the label name from the label id, create
    a dictionary that maps the label name to an integer and vice versa:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型更容易从标签ID获取标签名称，创建一个将标签名称映射到整数及反之的字典：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now you can convert the label id to a label name:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将标签ID转换为标签名称：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Preprocess
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理
- en: 'The next step is to load a ViT image processor to process the image into a
    tensor:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载一个ViT图像处理器，将图像处理为张量：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: PytorchHide Pytorch content
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: Apply some image transformations to the images to make the model more robust
    against overfitting. Here you’ll use torchvision’s [`transforms`](https://pytorch.org/vision/stable/transforms.html)
    module, but you can also use any image library you like.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像应用一些图像转换，使模型更具抗过拟合能力。在这里，您将使用torchvision的[`transforms`](https://pytorch.org/vision/stable/transforms.html)模块，但您也可以使用您喜欢的任何图像库。
- en: 'Crop a random part of the image, resize it, and normalize it with the image
    mean and standard deviation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪图像的随机部分，调整大小，并使用图像的均值和标准差进行归一化：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then create a preprocessing function to apply the transforms and return the
    `pixel_values` - the inputs to the model - of the image:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建一个预处理函数来应用转换并返回`pixel_values` - 图像的模型输入：
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To apply the preprocessing function over the entire dataset, use 🤗 Datasets
    [with_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.with_transform)
    method. The transforms are applied on the fly when you load an element of the
    dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要在整个数据集上应用预处理函数，请使用🤗数据集的[with_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.with_transform)方法。当加载数据集的元素时，转换会即时应用：
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now create a batch of examples using [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator).
    Unlike other data collators in 🤗 Transformers, the `DefaultDataCollator` does
    not apply additional preprocessing such as padding.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)创建一批示例。与🤗
    Transformers中的其他数据整理器不同，`DefaultDataCollator`不会应用额外的预处理，如填充。
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: TensorFlowHide TensorFlow content
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: To avoid overfitting and to make the model more robust, add some data augmentation
    to the training part of the dataset. Here we use Keras preprocessing layers to
    define the transformations for the training data (includes data augmentation),
    and transformations for the validation data (only center cropping, resizing and
    normalizing). You can use `tf.image`or any other library you prefer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合并使模型更加健壮，在数据集的训练部分添加一些数据增强。在这里，我们使用Keras预处理层来定义训练数据（包括数据增强）的转换，以及验证数据（仅中心裁剪、调整大小和归一化）的转换。您可以使用`tf.image`或您喜欢的任何其他库。
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, create functions to apply appropriate transformations to a batch of images,
    instead of one image at a time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建函数将适当的转换应用于一批图像，而不是一次一个图像。
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Use 🤗 Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    to apply the transformations on the fly:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用🤗 Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)在运行时应用转换：
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As a final preprocessing step, create a batch of examples using `DefaultDataCollator`.
    Unlike other data collators in 🤗 Transformers, the `DefaultDataCollator` does
    not apply additional preprocessing, such as padding.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的预处理步骤，使用`DefaultDataCollator`创建一批示例。与🤗 Transformers中的其他数据整理器不同，`DefaultDataCollator`不会应用额外的预处理，如填充。
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Evaluate
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: 'Including a metric during training is often helpful for evaluating your model’s
    performance. You can quickly load an evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)
    metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中包含一个度量通常有助于评估模型的性能。您可以使用🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)库快速加载评估方法。对于此任务，加载[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)度量（查看🤗
    Evaluate [快速导览](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算度量）：
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then create a function that passes your predictions and labels to `compute`
    to calculate the accuracy:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建一个函数，将您的预测和标签传递给`compute`以计算准确性：
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Your `compute_metrics` function is ready to go now, and you’ll return to it
    when you set up your training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您的`compute_metrics`函数现在已经准备就绪，当您设置训练时会返回到它。
- en: Train
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: PytorchHide Pytorch content
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: If you aren’t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)对模型进行微调，请查看基本教程[这里](../training#train-with-pytorch-trainer)！
- en: 'You’re ready to start training your model now! Load ViT with [AutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForImageClassification).
    Specify the number of labels along with the number of expected labels, and the
    label mappings:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以开始训练您的模型了！使用[AutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForImageClassification)加载ViT。指定标签数量以及预期标签数量和标签映射：
- en: '[PRE18]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At this point, only three steps remain:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，只剩下三个步骤：
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    It is important you don’t remove unused columns because that’ll drop the `image`
    column. Without the `image` column, you can’t create `pixel_values`. Set `remove_unused_columns=False`
    to prevent this behavior! The only other required parameter is `output_dir` which
    specifies where to save your model. You’ll push this model to the Hub by setting
    `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).
    At the end of each epoch, the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the accuracy and save the training checkpoint.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中定义您的训练超参数。重要的是不要删除未使用的列，因为那会删除`image`列。没有`image`列，您就无法创建`pixel_values`。设置`remove_unused_columns=False`以防止这种行为！唯一的其他必需参数是`output_dir`，指定保存模型的位置。通过设置`push_to_hub=True`将此模型推送到Hub（您需要登录Hugging
    Face才能上传您的模型）。在每个epoch结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将评估准确性并保存训练检查点。
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练参数传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，以及模型、数据集、分词器、数据整理器和`compute_metrics`函数。
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)来微调您的模型。
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将您的模型共享到Hub，这样每个人都可以使用您的模型：
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TensorFlowHide TensorFlow content
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: If you are unfamiliar with fine-tuning a model with Keras, check out the [basic
    tutorial](./training#train-a-tensorflow-model-with-keras) first!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉使用Keras微调模型，请先查看[基本教程](./training#train-a-tensorflow-model-with-keras)！
- en: 'To fine-tune a model in TensorFlow, follow these steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要在TensorFlow中微调模型，请按照以下步骤进行：
- en: Define the training hyperparameters, and set up an optimizer and a learning
    rate schedule.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练超参数，并设置优化器和学习率调度。
- en: Instantiate a pre-trained model.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个预训练模型。
- en: Convert a 🤗 Dataset to a `tf.data.Dataset`.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将🤗数据集转换为`tf.data.Dataset`。
- en: Compile your model.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译您的模型。
- en: Add callbacks and use the `fit()` method to run the training.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加回调并使用`fit()`方法运行训练。
- en: Upload your model to 🤗 Hub to share with the community.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的模型上传到🤗 Hub以与社区共享。
- en: 'Start by defining the hyperparameters, optimizer and learning rate schedule:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义超参数、优化器和学习率调度：
- en: '[PRE21]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, load ViT with [TFAutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForImageClassification)
    along with the label mappings:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用[TFAutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForImageClassification)加载ViT以及标签映射：
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Convert your datasets to the `tf.data.Dataset` format using the [to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
    and your `data_collator`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)和您的`data_collator`将数据集转换为`tf.data.Dataset`格式：
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Configure the model for training with `compile()`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`compile()`配置模型进行训练：
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To compute the accuracy from the predictions and push your model to the 🤗 Hub,
    use [Keras callbacks](../main_classes/keras_callbacks). Pass your `compute_metrics`
    function to [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback),
    and use the [PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback)
    to upload the model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要从预测中计算准确性并将模型推送到🤗 Hub，请使用[Keras回调](../main_classes/keras_callbacks)。将您的`compute_metrics`函数传递给[KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback)，并使用[PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback)上传模型：
- en: '[PRE25]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, you are ready to train your model! Call `fit()` with your training
    and validation datasets, the number of epochs, and your callbacks to fine-tune
    the model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您已经准备好训练您的模型了！使用您的训练和验证数据集、时代数和回调来微调模型调用`fit()`：
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Congratulations! You have fine-tuned your model and shared it on the 🤗 Hub.
    You can now use it for inference!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经对模型进行了微调，并在🤗 Hub上共享。现在您可以用它进行推理！
- en: For a more in-depth example of how to finetune a model for image classification,
    take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何为图像分类微调模型的更深入示例，请查看相应的[PyTorch笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)。
- en: Inference
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: Great, now that you’ve fine-tuned a model, you can use it for inference!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，现在您已经对模型进行了微调，可以用于推理！
- en: 'Load an image you’d like to run inference on:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 加载要运行推理的图像：
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![image of beignets](../Images/c05aba9fec7ae7783c6fb3a48b107e6b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![beignets的图像](../Images/c05aba9fec7ae7783c6fb3a48b107e6b.png)'
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for image classification with your model, and pass your
    image to it:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用您微调的模型进行推理的最简单方法是在[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中使用它。使用您的模型实例化一个用于图像分类的`pipeline`，并将图像传递给它：
- en: '[PRE28]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can also manually replicate the results of the `pipeline` if you’d like:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果愿意，您也可以手动复制`pipeline`的结果：
- en: PytorchHide Pytorch content
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: 'Load an image processor to preprocess the image and return the `input` as PyTorch
    tensors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 加载图像处理器以预处理图像并将`input`返回为PyTorch张量：
- en: '[PRE29]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Pass your inputs to the model and return the logits:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入传递给模型并返回logits：
- en: '[PRE30]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Get the predicted label with the highest probability, and use the model’s `id2label`
    mapping to convert it to a label:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 获取具有最高概率的预测标签，并使用模型的`id2label`映射将其转换为标签：
- en: '[PRE31]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: TensorFlowHide TensorFlow content
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: 'Load an image processor to preprocess the image and return the `input` as TensorFlow
    tensors:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 加载图像处理器以预处理图像并将`input`返回为TensorFlow张量：
- en: '[PRE32]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Pass your inputs to the model and return the logits:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入传递给模型并返回logits：
- en: '[PRE33]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Get the predicted label with the highest probability, and use the model’s `id2label`
    mapping to convert it to a label:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 获取具有最高概率的预测标签，并使用模型的`id2label`映射将其转换为标签：
- en: '[PRE34]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
