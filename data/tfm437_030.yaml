- en: Image classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_classification)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/tjAIM7BOYhw](https://www.youtube-nocookie.com/embed/tjAIM7BOYhw)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/tjAIM7BOYhw](https://www.youtube-nocookie.com/embed/tjAIM7BOYhw)'
- en: Image classification assigns a label or class to an image. Unlike text or audio
    classification, the inputs are the pixel values that comprise an image. There
    are many applications for image classification, such as detecting damage after
    a natural disaster, monitoring crop health, or helping screen medical images for
    signs of disease.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»ä¸ºå›¾åƒåˆ†é…ä¸€ä¸ªæ ‡ç­¾æˆ–ç±»åˆ«ã€‚ä¸æ–‡æœ¬æˆ–éŸ³é¢‘åˆ†ç±»ä¸åŒï¼Œè¾“å…¥æ˜¯ç»„æˆå›¾åƒçš„åƒç´ å€¼ã€‚å›¾åƒåˆ†ç±»æœ‰è®¸å¤šåº”ç”¨ï¼Œä¾‹å¦‚åœ¨è‡ªç„¶ç¾å®³åæ£€æµ‹æŸåã€ç›‘æµ‹ä½œç‰©å¥åº·æˆ–å¸®åŠ©ç­›æŸ¥åŒ»å­¦å›¾åƒä¸­çš„ç–¾ç—…è¿¹è±¡ã€‚
- en: 'This guide illustrates how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—è¯´æ˜äº†å¦‚ä½•ï¼š
- en: Fine-tune [ViT](model_doc/vit) on the [Food-101](https://huggingface.co/datasets/food101)
    dataset to classify a food item in an image.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[Food-101](https://huggingface.co/datasets/food101)æ•°æ®é›†ä¸Šå¯¹[ViT](model_doc/vit)è¿›è¡Œå¾®è°ƒï¼Œä»¥å¯¹å›¾åƒä¸­çš„é£Ÿç‰©é¡¹ç›®è¿›è¡Œåˆ†ç±»ã€‚
- en: Use your fine-tuned model for inference.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
- en: '[BEiT](../model_doc/beit), [BiT](../model_doc/bit), [ConvNeXT](../model_doc/convnext),
    [ConvNeXTV2](../model_doc/convnextv2), [CvT](../model_doc/cvt), [Data2VecVision](../model_doc/data2vec-vision),
    [DeiT](../model_doc/deit), [DiNAT](../model_doc/dinat), [DINOv2](../model_doc/dinov2),
    [EfficientFormer](../model_doc/efficientformer), [EfficientNet](../model_doc/efficientnet),
    [FocalNet](../model_doc/focalnet), [ImageGPT](../model_doc/imagegpt), [LeViT](../model_doc/levit),
    [MobileNetV1](../model_doc/mobilenet_v1), [MobileNetV2](../model_doc/mobilenet_v2),
    [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2),
    [NAT](../model_doc/nat), [Perceiver](../model_doc/perceiver), [PoolFormer](../model_doc/poolformer),
    [PVT](../model_doc/pvt), [RegNet](../model_doc/regnet), [ResNet](../model_doc/resnet),
    [SegFormer](../model_doc/segformer), [SwiftFormer](../model_doc/swiftformer),
    [Swin Transformer](../model_doc/swin), [Swin Transformer V2](../model_doc/swinv2),
    [VAN](../model_doc/van), [ViT](../model_doc/vit), [ViT Hybrid](../model_doc/vit_hybrid),
    [ViTMSN](../model_doc/vit_msn)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[BEiT](../model_doc/beit)ã€[BiT](../model_doc/bit)ã€[ConvNeXT](../model_doc/convnext)ã€[ConvNeXTV2](../model_doc/convnextv2)ã€[CvT](../model_doc/cvt)ã€[Data2VecVision](../model_doc/data2vec-vision)ã€[DeiT](../model_doc/deit)ã€[DiNAT](../model_doc/dinat)ã€[DINOv2](../model_doc/dinov2)ã€[EfficientFormer](../model_doc/efficientformer)ã€[EfficientNet](../model_doc/efficientnet)ã€[FocalNet](../model_doc/focalnet)ã€[ImageGPT](../model_doc/imagegpt)ã€[LeViT](../model_doc/levit)ã€[MobileNetV1](../model_doc/mobilenet_v1)ã€[MobileNetV2](../model_doc/mobilenet_v2)ã€[MobileViT](../model_doc/mobilevit)ã€[MobileViTV2](../model_doc/mobilevitv2)ã€[NAT](../model_doc/nat)ã€[Perceiver](../model_doc/perceiver)ã€[PoolFormer](../model_doc/poolformer)ã€[PVT](../model_doc/pvt)ã€[RegNet](../model_doc/regnet)ã€[ResNet](../model_doc/resnet)ã€[SegFormer](../model_doc/segformer)ã€[SwiftFormer](../model_doc/swiftformer)ã€[Swin
    Transformer](../model_doc/swin)ã€[Swin Transformer V2](../model_doc/swinv2)ã€[VAN](../model_doc/van)ã€[ViT](../model_doc/vit)ã€[ViT
    Hybrid](../model_doc/vit_hybrid)ã€[ViTMSN](../model_doc/vit_msn)'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to log in to your Hugging Face account to upload and share
    your model with the community. When prompted, enter your token to log in:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œä»¥ä¾¿ä¸Šä¼ å’Œä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load Food-101 dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½Food-101æ•°æ®é›†
- en: Start by loading a smaller subset of the Food-101 dataset from the ğŸ¤— Datasets
    library. This will give you a chance to experiment and make sure everything works
    before spending more time training on the full dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆä»ğŸ¤—æ•°æ®é›†åº“ä¸­åŠ è½½Food-101æ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Split the datasetâ€™s `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)æ–¹æ³•å°†æ•°æ®é›†çš„`train`æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then take a look at an example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åçœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Each example in the dataset has two fields:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½æœ‰ä¸¤ä¸ªå­—æ®µï¼š
- en: '`image`: a PIL image of the food item'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼šé£Ÿç‰©é¡¹ç›®çš„PILå›¾åƒ'
- en: '`label`: the label class of the food item'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`ï¼šé£Ÿç‰©é¡¹ç›®çš„æ ‡ç­¾ç±»åˆ«'
- en: 'To make it easier for the model to get the label name from the label id, create
    a dictionary that maps the label name to an integer and vice versa:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æ¨¡å‹æ›´å®¹æ˜“ä»æ ‡ç­¾IDè·å–æ ‡ç­¾åç§°ï¼Œåˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾åç§°æ˜ å°„åˆ°æ•´æ•°åŠåä¹‹çš„å­—å…¸ï¼š
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now you can convert the label id to a label name:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å°†æ ‡ç­¾IDè½¬æ¢ä¸ºæ ‡ç­¾åç§°ï¼š
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Preprocess
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: 'The next step is to load a ViT image processor to process the image into a
    tensor:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ªViTå›¾åƒå¤„ç†å™¨ï¼Œå°†å›¾åƒå¤„ç†ä¸ºå¼ é‡ï¼š
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: PytorchHide Pytorch content
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: Apply some image transformations to the images to make the model more robust
    against overfitting. Here youâ€™ll use torchvisionâ€™s [`transforms`](https://pytorch.org/vision/stable/transforms.html)
    module, but you can also use any image library you like.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å›¾åƒåº”ç”¨ä¸€äº›å›¾åƒè½¬æ¢ï¼Œä½¿æ¨¡å‹æ›´å…·æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å°†ä½¿ç”¨torchvisionçš„[`transforms`](https://pytorch.org/vision/stable/transforms.html)æ¨¡å—ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾åƒåº“ã€‚
- en: 'Crop a random part of the image, resize it, and normalize it with the image
    mean and standard deviation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è£å‰ªå›¾åƒçš„éšæœºéƒ¨åˆ†ï¼Œè°ƒæ•´å¤§å°ï¼Œå¹¶ä½¿ç”¨å›¾åƒçš„å‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œå½’ä¸€åŒ–ï¼š
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then create a preprocessing function to apply the transforms and return the
    `pixel_values` - the inputs to the model - of the image:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°æ¥åº”ç”¨è½¬æ¢å¹¶è¿”å›`pixel_values` - å›¾åƒçš„æ¨¡å‹è¾“å…¥ï¼š
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets
    [with_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.with_transform)
    method. The transforms are applied on the fly when you load an element of the
    dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œè¯·ä½¿ç”¨ğŸ¤—æ•°æ®é›†çš„[with_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.with_transform)æ–¹æ³•ã€‚å½“åŠ è½½æ•°æ®é›†çš„å…ƒç´ æ—¶ï¼Œè½¬æ¢ä¼šå³æ—¶åº”ç”¨ï¼š
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now create a batch of examples using [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator).
    Unlike other data collators in ğŸ¤— Transformers, the `DefaultDataCollator` does
    not apply additional preprocessing such as padding.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚ä¸ğŸ¤—
    Transformersä¸­çš„å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œ`DefaultDataCollator`ä¸ä¼šåº”ç”¨é¢å¤–çš„é¢„å¤„ç†ï¼Œå¦‚å¡«å……ã€‚
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: TensorFlowHide TensorFlow content
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: To avoid overfitting and to make the model more robust, add some data augmentation
    to the training part of the dataset. Here we use Keras preprocessing layers to
    define the transformations for the training data (includes data augmentation),
    and transformations for the validation data (only center cropping, resizing and
    normalizing). You can use `tf.image`or any other library you prefer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆå¹¶ä½¿æ¨¡å‹æ›´åŠ å¥å£®ï¼Œåœ¨æ•°æ®é›†çš„è®­ç»ƒéƒ¨åˆ†æ·»åŠ ä¸€äº›æ•°æ®å¢å¼ºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨Kerasé¢„å¤„ç†å±‚æ¥å®šä¹‰è®­ç»ƒæ•°æ®ï¼ˆåŒ…æ‹¬æ•°æ®å¢å¼ºï¼‰çš„è½¬æ¢ï¼Œä»¥åŠéªŒè¯æ•°æ®ï¼ˆä»…ä¸­å¿ƒè£å‰ªã€è°ƒæ•´å¤§å°å’Œå½’ä¸€åŒ–ï¼‰çš„è½¬æ¢ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`tf.image`æˆ–æ‚¨å–œæ¬¢çš„ä»»ä½•å…¶ä»–åº“ã€‚
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, create functions to apply appropriate transformations to a batch of images,
    instead of one image at a time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåˆ›å»ºå‡½æ•°å°†é€‚å½“çš„è½¬æ¢åº”ç”¨äºä¸€æ‰¹å›¾åƒï¼Œè€Œä¸æ˜¯ä¸€æ¬¡ä¸€ä¸ªå›¾åƒã€‚
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Use ğŸ¤— Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    to apply the transformations on the fly:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)åœ¨è¿è¡Œæ—¶åº”ç”¨è½¬æ¢ï¼š
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As a final preprocessing step, create a batch of examples using `DefaultDataCollator`.
    Unlike other data collators in ğŸ¤— Transformers, the `DefaultDataCollator` does
    not apply additional preprocessing, such as padding.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæœ€åçš„é¢„å¤„ç†æ­¥éª¤ï¼Œä½¿ç”¨`DefaultDataCollator`åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚ä¸ğŸ¤— Transformersä¸­çš„å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œ`DefaultDataCollator`ä¸ä¼šåº”ç”¨é¢å¤–çš„é¢„å¤„ç†ï¼Œå¦‚å¡«å……ã€‚
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Evaluate
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: 'Including a metric during training is often helpful for evaluating your modelâ€™s
    performance. You can quickly load an evaluation method with the ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)
    metric (see the ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªåº¦é‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)åº¦é‡ï¼ˆæŸ¥çœ‹ğŸ¤—
    Evaluate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡ï¼‰ï¼š
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then create a function that passes your predictions and labels to `compute`
    to calculate the accuracy:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™`compute`ä»¥è®¡ç®—å‡†ç¡®æ€§ï¼š
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Your `compute_metrics` function is ready to go now, and youâ€™ll return to it
    when you set up your training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„`compute_metrics`å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶ä¼šè¿”å›åˆ°å®ƒã€‚
- en: Train
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: PytorchHide Pytorch content
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—Pytorchå†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[è¿™é‡Œ](../training#train-with-pytorch-trainer)ï¼
- en: 'Youâ€™re ready to start training your model now! Load ViT with [AutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForImageClassification).
    Specify the number of labels along with the number of expected labels, and the
    label mappings:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[AutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForImageClassification)åŠ è½½ViTã€‚æŒ‡å®šæ ‡ç­¾æ•°é‡ä»¥åŠé¢„æœŸæ ‡ç­¾æ•°é‡å’Œæ ‡ç­¾æ˜ å°„ï¼š
- en: '[PRE18]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At this point, only three steps remain:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    It is important you donâ€™t remove unused columns because thatâ€™ll drop the `image`
    column. Without the `image` column, you canâ€™t create `pixel_values`. Set `remove_unused_columns=False`
    to prevent this behavior! The only other required parameter is `output_dir` which
    specifies where to save your model. Youâ€™ll push this model to the Hub by setting
    `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).
    At the end of each epoch, the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the accuracy and save the training checkpoint.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚é‡è¦çš„æ˜¯ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºé‚£ä¼šåˆ é™¤`image`åˆ—ã€‚æ²¡æœ‰`image`åˆ—ï¼Œæ‚¨å°±æ— æ³•åˆ›å»º`pixel_values`ã€‚è®¾ç½®`remove_unused_columns=False`ä»¥é˜²æ­¢è¿™ç§è¡Œä¸ºï¼å”¯ä¸€çš„å…¶ä»–å¿…éœ€å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆæ‚¨éœ€è¦ç™»å½•Hugging
    Faceæ‰èƒ½ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªepochç»“æŸæ—¶ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è¯„ä¼°å‡†ç¡®æ€§å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œä»¥åŠæ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TensorFlowHide TensorFlow content
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: If you are unfamiliar with fine-tuning a model with Keras, check out the [basic
    tutorial](./training#train-a-tensorflow-model-with-keras) first!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨Keraså¾®è°ƒæ¨¡å‹ï¼Œè¯·å…ˆæŸ¥çœ‹[åŸºæœ¬æ•™ç¨‹](./training#train-a-tensorflow-model-with-keras)ï¼
- en: 'To fine-tune a model in TensorFlow, follow these steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
- en: Define the training hyperparameters, and set up an optimizer and a learning
    rate schedule.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰è®­ç»ƒè¶…å‚æ•°ï¼Œå¹¶è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ã€‚
- en: Instantiate a pre-trained model.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: Convert a ğŸ¤— Dataset to a `tf.data.Dataset`.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ğŸ¤—æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`ã€‚
- en: Compile your model.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¼–è¯‘æ‚¨çš„æ¨¡å‹ã€‚
- en: Add callbacks and use the `fit()` method to run the training.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ·»åŠ å›è°ƒå¹¶ä½¿ç”¨`fit()`æ–¹æ³•è¿è¡Œè®­ç»ƒã€‚
- en: Upload your model to ğŸ¤— Hub to share with the community.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„æ¨¡å‹ä¸Šä¼ åˆ°ğŸ¤— Hubä»¥ä¸ç¤¾åŒºå…±äº«ã€‚
- en: 'Start by defining the hyperparameters, optimizer and learning rate schedule:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆå®šä¹‰è¶…å‚æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ï¼š
- en: '[PRE21]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, load ViT with [TFAutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForImageClassification)
    along with the label mappings:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½¿ç”¨[TFAutoModelForImageClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForImageClassification)åŠ è½½ViTä»¥åŠæ ‡ç­¾æ˜ å°„ï¼š
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Convert your datasets to the `tf.data.Dataset` format using the [to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
    and your `data_collator`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)å’Œæ‚¨çš„`data_collator`å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Configure the model for training with `compile()`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`compile()`é…ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼š
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To compute the accuracy from the predictions and push your model to the ğŸ¤— Hub,
    use [Keras callbacks](../main_classes/keras_callbacks). Pass your `compute_metrics`
    function to [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback),
    and use the [PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback)
    to upload the model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä»é¢„æµ‹ä¸­è®¡ç®—å‡†ç¡®æ€§å¹¶å°†æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hubï¼Œè¯·ä½¿ç”¨[Keraså›è°ƒ](../main_classes/keras_callbacks)ã€‚å°†æ‚¨çš„`compute_metrics`å‡½æ•°ä¼ é€’ç»™[KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback)ï¼Œå¹¶ä½¿ç”¨[PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback)ä¸Šä¼ æ¨¡å‹ï¼š
- en: '[PRE25]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, you are ready to train your model! Call `fit()` with your training
    and validation datasets, the number of epochs, and your callbacks to fine-tune
    the model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€æ—¶ä»£æ•°å’Œå›è°ƒæ¥å¾®è°ƒæ¨¡å‹è°ƒç”¨`fit()`ï¼š
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Congratulations! You have fine-tuned your model and shared it on the ğŸ¤— Hub.
    You can now use it for inference!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨ğŸ¤— Hubä¸Šå…±äº«ã€‚ç°åœ¨æ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼
- en: For a more in-depth example of how to finetune a model for image classification,
    take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£å¦‚ä½•ä¸ºå›¾åƒåˆ†ç±»å¾®è°ƒæ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„[PyTorchç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)ã€‚
- en: Inference
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Great, now that youâ€™ve fine-tuned a model, you can use it for inference!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªæ£’äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥ç”¨äºæ¨ç†ï¼
- en: 'Load an image youâ€™d like to run inference on:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½è¦è¿è¡Œæ¨ç†çš„å›¾åƒï¼š
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![image of beignets](../Images/c05aba9fec7ae7783c6fb3a48b107e6b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![beignetsçš„å›¾åƒ](../Images/c05aba9fec7ae7783c6fb3a48b107e6b.png)'
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for image classification with your model, and pass your
    image to it:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºå›¾åƒåˆ†ç±»çš„`pipeline`ï¼Œå¹¶å°†å›¾åƒä¼ é€’ç»™å®ƒï¼š
- en: '[PRE28]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can also manually replicate the results of the `pipeline` if youâ€™d like:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼š
- en: PytorchHide Pytorch content
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: 'Load an image processor to preprocess the image and return the `input` as PyTorch
    tensors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½å›¾åƒå¤„ç†å™¨ä»¥é¢„å¤„ç†å›¾åƒå¹¶å°†`input`è¿”å›ä¸ºPyTorchå¼ é‡ï¼š
- en: '[PRE29]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Pass your inputs to the model and return the logits:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›logitsï¼š
- en: '[PRE30]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Get the predicted label with the highest probability, and use the modelâ€™s `id2label`
    mapping to convert it to a label:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„é¢„æµ‹æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„`id2label`æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š
- en: '[PRE31]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: TensorFlowHide TensorFlow content
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlowå†…å®¹
- en: 'Load an image processor to preprocess the image and return the `input` as TensorFlow
    tensors:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½å›¾åƒå¤„ç†å™¨ä»¥é¢„å¤„ç†å›¾åƒå¹¶å°†`input`è¿”å›ä¸ºTensorFlowå¼ é‡ï¼š
- en: '[PRE32]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Pass your inputs to the model and return the logits:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›logitsï¼š
- en: '[PRE33]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Get the predicted label with the highest probability, and use the modelâ€™s `id2label`
    mapping to convert it to a label:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„é¢„æµ‹æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„`id2label`æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š
- en: '[PRE34]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
