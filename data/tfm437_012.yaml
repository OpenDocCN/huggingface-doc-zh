- en: Load adapters with 🤗 PEFT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用🤗 PEFT加载适配器
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/peft](https://huggingface.co/docs/transformers/v4.37.2/en/peft)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/peft](https://huggingface.co/docs/transformers/v4.37.2/en/peft)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft)
    methods freeze the pretrained model parameters during fine-tuning and add a small
    number of trainable parameters (the adapters) on top of it. The adapters are trained
    to learn task-specific information. This approach has been shown to be very memory-efficient
    with lower compute usage while producing results comparable to a fully fine-tuned
    model.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[参数高效微调（PEFT）](https://huggingface.co/blog/peft)方法在微调期间冻结预训练模型参数，并在其上添加少量可训练参数（适配器）。适配器被训练以学习特定任务的信息。这种方法已被证明在使用更低的计算资源的同时产生与完全微调模型相媲美的结果时非常节省内存。'
- en: Adapters trained with PEFT are also usually an order of magnitude smaller than
    the full model, making it convenient to share, store, and load them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PEFT训练的适配器通常比完整模型小一个数量级，这样方便分享、存储和加载。
- en: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
- en: The adapter weights for a OPTForCausalLM model stored on the Hub are only ~6MB
    compared to the full size of the model weights, which can be ~700MB.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在Hub上的OPTForCausalLM模型的适配器权重仅约为6MB，而模型权重的完整大小可能约为700MB。
- en: If you’re interested in learning more about the 🤗 PEFT library, check out the
    [documentation](https://huggingface.co/docs/peft/index).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于🤗 PEFT库的信息，请查看[文档](https://huggingface.co/docs/peft/index)。
- en: Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: 'Get started by installing 🤗 PEFT:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过安装🤗 PEFT来开始：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to try out the brand new features, you might be interested in installing
    the library from source:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想尝试全新的功能，您可能会对从源代码安装库感兴趣：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Supported PEFT models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的PEFT模型
- en: '🤗 Transformers natively supports some PEFT methods, meaning you can load adapter
    weights stored locally or on the Hub and easily run or train them with a few lines
    of code. The following methods are supported:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers原生支持一些PEFT方法，这意味着您可以加载本地或Hub上存储的适配器权重，并使用几行代码轻松运行或训练它们。支持以下方法：
- en: '[Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[低秩适配器](https://huggingface.co/docs/peft/conceptual_guides/lora)'
- en: '[IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)'
- en: '[AdaLoRA](https://arxiv.org/abs/2303.10512)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AdaLoRA](https://arxiv.org/abs/2303.10512)'
- en: If you want to use other PEFT methods, such as prompt learning or prompt tuning,
    or about the 🤗 PEFT library in general, please refer to the [documentation](https://huggingface.co/docs/peft/index).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用其他PEFT方法，如提示学习或提示调整，或者了解🤗 PEFT库的一般信息，请参考[文档](https://huggingface.co/docs/peft/index)。
- en: Load a PEFT adapter
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载PEFT适配器
- en: 'To load and use a PEFT adapter model from 🤗 Transformers, make sure the Hub
    repository or local directory contains an `adapter_config.json` file and the adapter
    weights, as shown in the example image above. Then you can load the PEFT adapter
    model using the `AutoModelFor` class. For example, to load a PEFT adapter model
    for causal language modeling:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 加载和使用🤗 Transformers中的PEFT适配器模型时，请确保Hub存储库或本地目录包含一个`adapter_config.json`文件和适配器权重，如上面的示例图所示。然后，您可以使用`AutoModelFor`类加载PEFT适配器模型。例如，要加载用于因果语言建模的PEFT适配器模型：
- en: specify the PEFT model id
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定PEFT模型ID
- en: pass it to the [AutoModelForCausalLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM)
    class
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其传递给[AutoModelForCausalLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM)类
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can load a PEFT adapter with either an `AutoModelFor` class or the base
    model class like `OPTForCausalLM` or `LlamaForCausalLM`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`AutoModelFor`类或基本模型类（如`OPTForCausalLM`或`LlamaForCausalLM`）加载PEFT适配器。
- en: 'You can also load a PEFT adapter by calling the `load_adapter` method:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过调用`load_adapter`方法加载PEFT适配器：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Load in 8bit or 4bit
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以8位或4位加载
- en: 'The `bitsandbytes` integration supports 8bit and 4bit precision data types,
    which are useful for loading large models because it saves memory (see the `bitsandbytes`
    integration [guide](./quantization#bitsandbytes-integration) to learn more). Add
    the `load_in_8bit` or `load_in_4bit` parameters to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    and set `device_map="auto"` to effectively distribute the model to your hardware:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`bitsandbytes`集成支持8位和4位精度数据类型，对于加载大型模型很有用，因为它节省内存（请参阅`bitsandbytes`集成[指南](./quantization#bitsandbytes-integration)以了解更多）。将`load_in_8bit`或`load_in_4bit`参数添加到[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)中，并设置`device_map="auto"`以有效地将模型分配到您的硬件：'
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Add a new adapter
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加一个新适配器
- en: 'You can use `~peft.PeftModel.add_adapter` to add a new adapter to a model with
    an existing adapter as long as the new adapter is the same type as the current
    one. For example, if you have an existing LoRA adapter attached to a model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`~peft.PeftModel.add_adapter`将一个新适配器添加到具有现有适配器的模型中，只要新适配器与当前适配器的类型相同。例如，如果您有一个已经连接到模型的现有LoRA适配器：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To add a new adapter:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个新适配器：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now you can use `~peft.PeftModel.set_adapter` to set which adapter to use:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用`~peft.PeftModel.set_adapter`来设置要使用的适配器：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable and disable adapters
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用和禁用适配器
- en: 'Once you’ve added an adapter to a model, you can enable or disable the adapter
    module. To enable the adapter module:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您向模型添加了适配器，您可以启用或禁用适配器模块。要启用适配器模块：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To disable the adapter module:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用适配器模块：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Train a PEFT adapter
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个PEFT适配器
- en: 'PEFT adapters are supported by the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class so that you can train an adapter for your specific use case. It only requires
    adding a few more lines of code. For example, to train a LoRA adapter:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT适配器受[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类支持，因此您可以为特定用例训练一个适配器。只需要添加几行代码。例如，要训练一个LoRA适配器：
- en: If you aren’t familiar with fine-tuning a model with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the [Fine-tune a pretrained model](training) tutorial.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)微调模型，请查看[微调预训练模型](training)教程。
- en: Define your adapter configuration with the task type and hyperparameters (see
    `~peft.LoraConfig` for more details about what the hyperparameters do).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用任务类型和超参数定义您的适配器配置（有关超参数的详细信息，请参阅`~peft.LoraConfig`）。
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Add adapter to the model.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将适配器添加到模型中。
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now you can pass the model to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)!
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您可以将模型传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)！
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To save your trained adapter and load it back:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 保存您训练过的适配器并加载回来：
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Add additional trainable layers to a PEFT adapter
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向PEFT适配器添加额外的可训练层
- en: 'You can also fine-tune additional trainable adapters on top of a model that
    has adapters attached by passing `modules_to_save` in your PEFT config. For example,
    if you want to also fine-tune the lm_head on top of a model with a LoRA adapter:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过在PEFT配置中传递`modules_to_save`来在已附加适配器的模型顶部微调额外的可训练适配器。例如，如果您想在具有LoRA适配器的模型顶部也微调lm_head：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
