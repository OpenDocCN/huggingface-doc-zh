- en: Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trainer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is a complete training and evaluation loop for PyTorch models implemented in the
    Transformers library. You only need to pass it the necessary pieces for training
    (model, tokenizer, dataset, evaluation function, training hyperparameters, etc.),
    and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class takes care of the rest. This makes it easier to start training faster without
    manually writing your own training loop. But at the same time, [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is very customizable and offers a ton of training options so you can tailor it
    to your exact training needs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)是在Transformers库中实现的PyTorch模型的完整训练和评估循环。您只需要传递训练所需的必要部分（模型、分词器、数据集、评估函数、训练超参数等），[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类会处理其余部分。这使得更容易开始训练，而无需手动编写自己的训练循环。但同时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)非常可定制，并提供大量的训练选项，因此您可以根据自己的训练需求进行定制。'
- en: In addition to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class, Transformers also provides a [Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    class for sequence-to-sequence tasks like translation or summarization. There
    is also the [SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    class from the [TRL](https://hf.co/docs/trl) library which wraps the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class and is optimized for training language models like Llama-2 and Mistral with
    autoregressive techniques. [SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    also supports features like sequence packing, LoRA, quantization, and DeepSpeed
    for efficiently scaling to any model size.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 除了[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类外，Transformers还提供了一个[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)类，用于序列到序列任务，如翻译或摘要。还有来自[TRL](https://hf.co/docs/trl)库的[SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)类，它包装了[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类，并针对使用自回归技术的Llama-2和Mistral等语言模型进行了优化。[SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)还支持序列打包、LoRA、量化和DeepSpeed等功能，以有效地扩展到任何模型大小。
- en: Feel free to check out the [API reference](./main_classes/trainer) for these
    other [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)-type
    classes to learn more about when to use which one. In general, [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is the most versatile option and is appropriate for a broad spectrum of tasks.
    [Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    is designed for sequence-to-sequence tasks and [SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    is designed for training language models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随时查看[API参考](./main_classes/trainer)以了解其他[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类型类的更多信息，以便了解何时使用哪种。一般来说，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)是最通用的选择，适用于广泛的任务。[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)专为序列到序列任务设计，而[SFTTrainer](https://huggingface.co/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)专为训练语言模型设计。
- en: Before you start, make sure [Accelerate](https://hf.co/docs/accelerate) - a
    library for enabling and running PyTorch training across distributed environments
    - is installed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保已安装[Accelerate](https://hf.co/docs/accelerate) - 一个用于在分布式环境中启用和运行PyTorch训练的库。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This guide provides an overview of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指南提供了[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类的概述。
- en: Basic usage
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本用法
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    includes all the code you’ll find in a basic training loop:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)包含在基本训练循环中找到的所有代码：'
- en: perform a training step to calculate the loss
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行训练步骤来计算损失
- en: calculate the gradients with the [backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)
    method
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)方法计算梯度
- en: update the weights based on the gradients
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据梯度更新权重
- en: repeat this process until you’ve reached a predetermined number of epochs
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这个过程，直到达到预定的epoch数量
- en: The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class abstracts all of this code away so you don’t have to worry about manually
    writing a training loop every time or if you’re just getting started with PyTorch
    and training. You only need to provide the essential components required for training,
    such as a model and a dataset, and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class handles everything else.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类将所有这些代码抽象化，因此您无需每次手动编写训练循环，或者如果您刚开始使用PyTorch和训练时担心。您只需要提供训练所需的基本组件，如模型和数据集，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类会处理其他一切。'
- en: If you want to specify any training options or hyperparameters, you can find
    them in the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    class. For example, let’s define where to save the model in `output_dir` and push
    the model to the Hub after training with `push_to_hub=True`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要指定任何训练选项或超参数，您可以在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)类中找到它们。例如，让我们定义在`output_dir`中保存模型的位置，并在训练后使用`push_to_hub=True`将模型推送到Hub。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Pass `training_args` to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with a model, dataset, something to preprocess the dataset with (depending
    on your data type it could be a tokenizer, feature extractor or image processor),
    a data collator, and a function to compute the metrics you want to track during
    training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将`training_args`传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，以及一个模型、数据集、用于预处理数据集的内容（根据数据类型可能是令牌化器、特征提取器或图像处理器）、数据整理器和一个函数来计算您想要在训练过程中跟踪的指标。
- en: Finally, call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to start training!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)开始训练！
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Checkpoints
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查点
- en: The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class saves your model checkpoints to the directory specified in the `output_dir`
    parameter of [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    You’ll find the checkpoints saved in a `checkpoint-000` subfolder where the numbers
    at the end correspond to the training step. Saving checkpoints are useful for
    resuming training later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类将您的模型检查点保存到[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中指定的目录中的`output_dir`参数。您将在`checkpoint-000`子文件夹中找到保存的检查点，其中末尾的数字对应训练步骤。保存检查点对于稍后恢复训练很有用。'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can save your checkpoints (the optimizer state is not saved by default)
    to the Hub by setting `push_to_hub=True` in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    to commit and push them. Other options for deciding how your checkpoints are saved
    are set up in the [`hub_strategy`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.hub_strategy)
    parameter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中设置`push_to_hub=True`将检查点保存到Hub以提交和推送它们（默认情况下不保存优化器状态）。设置如何保存检查点的其他选项在[`hub_strategy`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.hub_strategy)参数中设置：
- en: '`hub_strategy="checkpoint"` pushes the latest checkpoint to a subfolder named
    “last-checkpoint” from which you can resume training'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub_strategy="checkpoint"` 将最新的检查点推送到名为“last-checkpoint”的子文件夹，您可以从中恢复训练'
- en: '`hug_strategy="all_checkpoints"` pushes all checkpoints to the directory defined
    in `output_dir` (you’ll see one checkpoint per folder in your model repository)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hug_strategy="all_checkpoints"` 将所有检查点推送到`output_dir`中定义的目录（您将在模型存储库中看到每个文件夹中的一个检查点）'
- en: When you resume training from a checkpoint, the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    tries to keep the Python, NumPy, and PyTorch RNG states the same as they were
    when the checkpoint was saved. But because PyTorch has various non-deterministic
    default settings, the RNG states aren’t guaranteed to be the same. If you want
    to enable full determinism, take a look at the [Controlling sources of randomness](https://pytorch.org/docs/stable/notes/randomness#controlling-sources-of-randomness)
    guide to learn what you can enable to make your training fully deterministic.
    Keep in mind though that by making certain settings deterministic, training may
    be slower.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当您从检查点恢复训练时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)会尝试保持Python、NumPy和PyTorch
    RNG状态与保存检查点时相同。但由于PyTorch具有各种非确定性的默认设置，RNG状态不能保证相同。如果要启用完全确定性，请查看[控制随机性源](https://pytorch.org/docs/stable/notes/randomness#controlling-sources-of-randomness)指南，了解您可以启用哪些内容以使您的训练完全确定性。请记住，通过使某些设置确定性，训练可能会变慢。
- en: Customize the Trainer
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义Trainer
- en: 'While the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class is designed to be accessible and easy-to-use, it also offers a lot of customizability
    for more adventurous users. Many of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)’s
    method can be subclassed and overridden to support the functionality you want,
    without having to rewrite the entire training loop from scratch to accommodate
    it. These methods include:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类旨在易于访问和使用，但也为更有冒险精神的用户提供了许多可定制性。许多[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的方法可以被子类化和重写，以支持您想要的功能，而无需重写整个训练循环以适应它。这些方法包括：
- en: '[get_train_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_train_dataloader)
    creates a training DataLoader'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[get_train_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_train_dataloader)
    创建一个训练DataLoader'
- en: '[get_eval_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_eval_dataloader)
    creates an evaluation DataLoader'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[get_eval_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_eval_dataloader)
    创建一个评估DataLoader'
- en: '[get_test_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_test_dataloader)
    creates a test DataLoader'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[get_test_dataloader()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.get_test_dataloader)
    创建一个测试DataLoader'
- en: '[log()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log)
    logs information on the various objects that watch training'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[log()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log)
    记录监视训练的各种对象的信息'
- en: '[create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler)
    creates an optimizer and learning rate scheduler if they weren’t passed in the
    `__init__`; these can also be separately customized with [create_optimizer()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer)
    and [create_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_scheduler)
    respectively'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler)
    在`__init__`中没有传入优化器和学习率调度器时创建它们；也可以使用[create_optimizer()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer)和[create_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_scheduler)进行单独定制'
- en: '[compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)
    computes the loss on a batch of training inputs'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)
    计算一批训练输入的损失'
- en: '[training_step()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.training_step)
    performs the training step'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[training_step()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.training_step)
    执行训练步骤'
- en: '[prediction_step()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.prediction_step)
    performs the prediction and test step'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[prediction_step()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.prediction_step)
    执行预测和测试步骤'
- en: '[evaluate()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.evaluate)
    evaluates the model and returns the evaluation metrics'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[evaluate()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.evaluate)
    评估模型并返回评估指标'
- en: '[predict()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.predict)
    makes predictions (with metrics if labels are available) on the test set'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[predict()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.predict)
    在测试集上进行预测（如果有标签，则带有指标）'
- en: For example, if you want to customize the [compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)
    method to use a weighted loss instead.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想要自定义[compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)方法以使用加权损失。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Callbacks
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回调
- en: Another option for customizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is to use [callbacks](callbacks). Callbacks *don’t change* anything in the training
    loop. They inspect the training loop state and then execute some action (early
    stopping, logging results, etc.) depending on the state. In other words, a callback
    can’t be used to implement something like a custom loss function and you’ll need
    to subclass and override the [compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)
    method for that.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的另一个选项是使用[callbacks](callbacks)。回调*不会改变*训练循环中的任何内容。它们检查训练循环状态，然后根据状态执行某些操作（提前停止、记录结果等）。换句话说，回调不能用于实现类似自定义损失函数的内容，您需要子类化并重写[compute_loss()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.compute_loss)方法。
- en: For example, if you want to add an early stopping callback to the training loop
    after 10 steps.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想在训练循环中的10步后添加一个提前停止回调。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then pass it to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)’s
    `callback` parameter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的`callback`参数。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Logging
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志
- en: Check out the [logging](./main_classes/logging) API reference for more information
    about the different logging levels.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[logging](./main_classes/logging) API参考以获取有关不同日志级别的更多信息。
- en: The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is set to `logging.INFO` by default which reports errors, warnings, and other
    basic information. A [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    replica - in distributed environments - is set to `logging.WARNING` which only
    reports errors and warnings. You can change the logging level with the [`log_level`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.log_level)
    and [`log_level_replica`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.log_level_replica)
    parameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)设置为`logging.INFO`，报告错误、警告和其他基本信息。在分布式环境中，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的副本设置为`logging.WARNING`，仅报告错误和警告。您可以使用[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中的`log_level`和`log_level_replica`参数更改日志级别。
- en: To configure the log level setting for each node, use the [`log_on_each_node`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.log_on_each_node)
    parameter to determine whether to use the log level on each node or only on the
    main node.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要为每个节点配置日志级别设置，请使用[`log_on_each_node`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.log_on_each_node)参数来确定是在每个节点上使用日志级别还是仅在主节点上使用。
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    sets the log level separately for each node in the `Trainer.__init__()` method,
    so you may want to consider setting this sooner if you’re using other Transformers
    functionalities before creating the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    object.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    在`Trainer.__init__()`方法中为每个节点单独设置日志级别，因此如果您在创建[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)对象之前使用其他Transformers功能，可能需要考虑更早设置这个。'
- en: 'For example, to set your main code and modules to use the same log level according
    to each node:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要根据每个节点设置主代码和模块使用相同的日志级别：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Use different combinations of `log_level` and `log_level_replica` to configure
    what gets logged on each of the nodes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同组合的`log_level`和`log_level_replica`来配置每个节点上记录什么。
- en: single nodemulti-node
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点多节点
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: NEFTune
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NEFTune
- en: '[NEFTune](https://hf.co/papers/2310.05914) is a technique that can improve
    performance by adding noise to the embedding vectors during training. To enable
    it in [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    set the `neftune_noise_alpha` parameter in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    to control how much noise is added.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[NEFTune](https://hf.co/papers/2310.05914)是一种通过在训练过程中向嵌入向量添加噪音来提高性能的技术。要在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)中启用它，设置[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中的`neftune_noise_alpha`参数来控制添加多少噪音。'
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: NEFTune is disabled after training to restore the original embedding layer to
    avoid any unexpected behavior.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后禁用NEFTune，以恢复原始嵌入层，避免任何意外行为。
- en: Accelerate and Trainer
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速和Trainer
- en: The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class is powered by [Accelerate](https://hf.co/docs/accelerate), a library for
    easily training PyTorch models in distributed environments with support for integrations
    such as [FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)
    and [DeepSpeed](https://www.deepspeed.ai/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类由[Accelerate](https://hf.co/docs/accelerate)支持，这是一个用于在分布式环境中轻松训练PyTorch模型的库，支持集成如[FullyShardedDataParallel
    (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)和[DeepSpeed](https://www.deepspeed.ai/)。'
- en: Learn more about FSDP sharding strategies, CPU offloading, and more with the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    in the [Fully Sharded Data Parallel](fsdp) guide.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的[Fully
    Sharded Data Parallel](fsdp)指南中了解更多关于FSDP分片策略、CPU卸载等内容。
- en: 'To use Accelerate with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    run the [`accelerate.config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)
    command to set up training for your training environment. This command creates
    a `config_file.yaml` that’ll be used when you launch your training script. For
    example, some example configurations you can setup are:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)与Accelerate，运行[`accelerate.config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)命令来设置您的训练环境。这个命令会创建一个`config_file.yaml`，在您启动训练脚本时会用到。例如，您可以设置一些示例配置：
- en: DistributedDataParallelFSDPDeepSpeedDeepSpeed with Accelerate plugin
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: DistributedDataParallelFSDPDeepSpeedDeepSpeed与Accelerate插件
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The [`accelerate_launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)
    command is the recommended way to launch your training script on a distributed
    system with Accelerate and [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    with the parameters specified in `config_file.yaml`. This file is saved to the
    Accelerate cache folder and automatically loaded when you run `accelerate_launch`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[`accelerate_launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)命令是在分布式系统上启动您的训练脚本的推荐方式，使用Accelerate和[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)中在`config_file.yaml`中指定的参数。这个文件保存在Accelerate缓存文件夹中，并在运行`accelerate_launch`时自动加载。'
- en: 'For example, to run the [run_glue.py](https://github.com/huggingface/transformers/blob/f4db565b695582891e43a5e042e5d318e28f20b8/examples/pytorch/text-classification/run_glue.py#L4)
    training script with the FSDP configuration:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用FSDP配置运行[run_glue.py](https://github.com/huggingface/transformers/blob/f4db565b695582891e43a5e042e5d318e28f20b8/examples/pytorch/text-classification/run_glue.py#L4)训练脚本：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You could also specify the parameters from the `config_file.yaml` file directly
    in the command line:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以直接在命令行中指定来自`config_file.yaml`文件的参数：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Check out the [Launching your Accelerate scripts](https://huggingface.co/docs/accelerate/basic_tutorials/launch)
    tutorial to learn more about `accelerate_launch` and custom configurations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[启动您的Accelerate脚本](https://huggingface.co/docs/accelerate/basic_tutorials/launch)教程，了解更多关于`accelerate_launch`和自定义配置的内容。
