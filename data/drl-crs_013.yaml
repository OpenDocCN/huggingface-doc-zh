- en: Summary
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/summary](https://huggingface.co/learn/deep-rl-course/unit1/summary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/learn/deep-rl-course/unit1/summary](https://huggingface.co/learn/deep-rl-course/unit1/summary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'That was a lot of information! Let’s summarize:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是很多信息！让我们总结一下：
- en: Reinforcement Learning is a computational approach of learning from actions.
    We build an agent that learns from the environment **by interacting with it through
    trial and error** and receiving rewards (negative or positive) as feedback.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习是一种从行动中学习的计算方法。我们构建一个代理，通过**通过试错与环境互动**并接收奖励（负面或正面）作为反馈来学习。
- en: The goal of any RL agent is to maximize its expected cumulative reward (also
    called expected return) because RL is based on the **reward hypothesis**, which
    is that **all goals can be described as the maximization of the expected cumulative
    reward.**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何强化学习代理的目标都是最大化其预期累积奖励（也称为预期回报），因为强化学习基于**奖励假设**，即**所有目标都可以描述为最大化预期累积奖励。**
- en: The RL process is a loop that outputs a sequence of **state, action, reward
    and next state.**
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习过程是一个循环，输出一系列**状态、行动、奖励和下一个状态。**
- en: 'To calculate the expected cumulative reward (expected return), we discount
    the rewards: the rewards that come sooner (at the beginning of the game) **are
    more probable to happen since they are more predictable than the long term future
    reward.**'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了计算预期累积奖励（预期回报），我们对奖励进行折现：较早出现的奖励（在游戏开始时）**更有可能发生，因为它们比长期未来奖励更可预测。**
- en: To solve an RL problem, you want to **find an optimal policy**. The policy is
    the “brain” of your agent, which will tell us **what action to take given a state.**
    The optimal policy is the one which **gives you the actions that maximize the
    expected return.**
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要解决强化学习问题，您希望**找到一个最优策略**。策略是您代理的“大脑”，它将告诉我们**在给定状态下采取什么行动。**最优策略是那个**给出最大化预期回报的行动。**
- en: 'There are two ways to find your optimal policy:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有两种方法可以找到您的最优策略：
- en: 'By training your policy directly: **policy-based methods.**'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过直接训练您的策略：**基于策略的方法。**
- en: 'By training a value function that tells us the expected return the agent will
    get at each state and use this function to define our policy: **value-based methods.**'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过训练一个告诉我们代理在每个状态将获得的预期回报的价值函数，并使用这个函数来定义我们的策略：**基于价值的方法。**
- en: Finally, we speak about Deep RL because we introduce **deep neural networks
    to estimate the action to take (policy-based) or to estimate the value of a state
    (value-based)** hence the name “deep”.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们谈到了深度强化学习，因为我们引入了**深度神经网络来估计要采取的行动（基于策略）或估计状态的价值（基于价值）**，因此得名“深度”。
