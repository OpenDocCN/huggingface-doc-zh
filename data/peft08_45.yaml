- en: P-tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P-tuning
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/p_tuning](https://huggingface.co/docs/peft/package_reference/p_tuning)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/peft/package_reference/p_tuning](https://huggingface.co/docs/peft/package_reference/p_tuning)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[P-tuning](https://hf.co/papers/2103.10385) adds trainable prompt embeddings
    to the input that is optimized by a prompt encoder to find a better prompt, eliminating
    the need to manually design prompts. The prompt tokens can be added anywhere in
    the input sequence, and p-tuning also introduces anchor tokens for improving performance.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[P-tuning](https://hf.co/papers/2103.10385)将可训练的提示嵌入添加到输入中，由提示编码器优化以找到更好的提示，消除了手动设计提示的需要。提示标记可以添加到输入序列的任何位置，P-tuning还引入了用于提高性能的锚定标记。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文的摘要是：
- en: '*While GPTs with traditional fine-tuning fail to achieve strong results on
    natural language understanding (NLU), we show that GPTs can be better than or
    comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning —
    which employs trainable continuous prompt embeddings. On the knowledge probing
    (LAMA) benchmark, the best GPT recovers 64\% (P@1) of world knowledge without
    any additional text provided during test time, which substantially improves the
    previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve
    comparable and sometimes better performance to similar-sized BERTs in supervised
    learning. Importantly, we find that P-tuning also improves BERTs’ performance
    in both few-shot and supervised settings while largely reducing the need for prompt
    engineering. Consequently, P-tuning outperforms the state-of-the-art approaches
    on the few-shot SuperGlue benchmark.*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管传统微调的GPT在自然语言理解（NLU）方面表现不佳，但我们表明，使用一种新颖的方法P-tuning，即使用可训练的连续提示嵌入，GPT在NLU任务上可以优于或与类似大小的BERT相媲美。在知识探测（LAMA）基准测试中，最佳GPT在测试时没有提供任何额外文本的情况下，恢复了64\%（P@1）的世界知识，这大大提高了之前最佳结果超过20个百分点。在SuperGlue基准测试中，GPT在监督学习中实现了与类似大小的BERT相媲美甚至更好的性能。重要的是，我们发现P-tuning还改善了BERT在少样本和监督设置中的性能，同时大大减少了提示工程的需求。因此，P-tuning在少样本SuperGlue基准测试中胜过了最先进的方法。*。'
- en: PromptEncoderConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PromptEncoderConfig
- en: '### `class peft.PromptEncoderConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PromptEncoderConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/config.py#L28)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/config.py#L28)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_reparameterization_type` (Union[`PromptEncoderReparameterizationType`,
    `str`]) — The type of reparameterization to use.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_reparameterization_type`（Union[`PromptEncoderReparameterizationType`，`str`]）—
    要使用的重新参数化类型。'
- en: '`encoder_hidden_size` (`int`) — The hidden size of the prompt encoder.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_size`（`int`）— 提示编码器的隐藏大小。'
- en: '`encoder_num_layers` (`int`) — The number of layers of the prompt encoder.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_num_layers`（`int`）— 提示编码器的层数。'
- en: '`encoder_dropout` (`float`) — The dropout probability of the prompt encoder.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_dropout`（`float`）— 提示编码器的丢失概率。'
- en: This is the configuration class to store the configuration of a [PromptEncoder](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoder).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[PromptEncoder](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoder)的配置。
- en: PromptEncoder
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PromptEncoder
- en: '### `class peft.PromptEncoder`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PromptEncoder`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/model.py#L24)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/model.py#L24)'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PromptEncoderConfig](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoderConfig))
    — The configuration of the prompt encoder.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PromptEncoderConfig](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoderConfig)）—
    提示编码器的配置。'
- en: The prompt encoder network that is used to generate the virtual token embeddings
    for p-tuning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成P-tuning的虚拟标记嵌入的提示编码器网络。
- en: 'Example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Attributes**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性**：'
- en: '`embedding` (`torch.nn.Embedding`) — The embedding layer of the prompt encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding`（`torch.nn.Embedding`）— 提示编码器的嵌入层。'
- en: '`mlp_head` (`torch.nn.Sequential`) — The MLP head of the prompt encoder if
    `inference_mode=False`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_head`（`torch.nn.Sequential`）— 如果`inference_mode=False`，则为提示编码器的MLP头。'
- en: '`lstm_head` (`torch.nn.LSTM`) — The LSTM head of the prompt encoder if `inference_mode=False`
    and `encoder_reparameterization_type="LSTM"`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lstm_head`（`torch.nn.LSTM`）— 如果`inference_mode=False`和`encoder_reparameterization_type="LSTM"`，则为提示编码器的LSTM头。'
- en: '`token_dim` (`int`) — The hidden embedding dimension of the base transformer
    model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_dim`（`int`）— 基础变换器模型的隐藏嵌入维度。'
- en: '`input_size` (`int`) — The input size of the prompt encoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size`（`int`）— 提示编码器的输入大小。'
- en: '`output_size` (`int`) — The output size of the prompt encoder.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_size`（`int`）— 提示编码器的输出大小。'
- en: '`hidden_size` (`int`) — The hidden size of the prompt encoder.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`）— 提示编码器的隐藏大小。'
- en: '`total_virtual_tokens` (`int`): The total number of virtual tokens of the prompt
    encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_virtual_tokens`（`int`）：提示编码器的虚拟标记总数。'
- en: '`encoder_type` (Union[`PromptEncoderReparameterizationType`, `str`]): The encoder
    type of the prompt encoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_type`（Union[`PromptEncoderReparameterizationType`，`str`]）：提示编码器的编码器类型。'
- en: 'Input shape: (`batch_size`, `total_virtual_tokens`)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输入形状：（`batch_size`，`total_virtual_tokens`）
- en: 'Output shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状：（`batch_size`，`total_virtual_tokens`，`token_dim`）
