- en: UniSpeech-SAT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UniSpeech-SAT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech-sat](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech-sat)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech-sat](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech-sat)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The UniSpeech-SAT model was proposed in [UniSpeech-SAT: Universal Speech Representation
    Learning with Speaker Aware Pre-Training](https://arxiv.org/abs/2110.05752) by
    Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian
    Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu .'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'UniSpeech-SAT模型是由Sanyuan Chen、Yu Wu、Chengyi Wang、Zhengyang Chen、Zhuo Chen、Shujie
    Liu、Jian Wu、Yao Qian、Furu Wei、Jinyu Li、Xiangzhan Yu在[UniSpeech-SAT: Universal
    Speech Representation Learning with Speaker Aware Pre-Training](https://arxiv.org/abs/2110.05752)中提出的。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Self-supervised learning (SSL) is a long-standing goal for speech processing,
    since it utilizes large-scale unlabeled data and avoids extensive human labeling.
    Recent years witness great successes in applying self-supervised learning in speech
    recognition, while limited exploration was attempted in applying SSL for modeling
    speaker characteristics. In this paper, we aim to improve the existing SSL framework
    for speaker representation learning. Two methods are introduced for enhancing
    the unsupervised speaker information extraction. First, we apply the multi-task
    learning to the current SSL framework, where we integrate the utterance-wise contrastive
    loss with the SSL objective function. Second, for better speaker discrimination,
    we propose an utterance mixing strategy for data augmentation, where additional
    overlapped utterances are created unsupervisely and incorporate during training.
    We integrate the proposed methods into the HuBERT framework. Experiment results
    on SUPERB benchmark show that the proposed system achieves state-of-the-art performance
    in universal representation learning, especially for speaker identification oriented
    tasks. An ablation study is performed verifying the efficacy of each proposed
    method. Finally, we scale up training dataset to 94 thousand hours public audio
    data and achieve further performance improvement in all SUPERB tasks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*自监督学习（SSL）是语音处理的一个长期目标，因为它利用大规模未标记数据，避免了大量的人工标注。近年来，在语音识别中应用自监督学习取得了巨大成功，但在建模说话者特征方面的尝试有限。在本文中，我们旨在改进现有的SSL框架，用于说话者表示学习。我们介绍了两种方法来增强无监督说话者信息提取。首先，我们将多任务学习应用于当前的SSL框架，将话语级对比损失与SSL目标函数整合在一起。其次，为了更好地区分说话者，我们提出了一种话语混合策略用于数据增强，其中额外的重叠话语是无监督创建的，并在训练过程中整合。我们将这些方法整合到HuBERT框架中。在SUPERB基准上的实验结果表明，所提出的系统在通用表示学习方面取得了最先进的性能，特别是针对说话者识别导向的任务。进行了消融研究，验证了每种提出方法的有效性。最后，我们将训练数据集扩大到94,000小时的公共音频数据，并在所有SUPERB任务中实现了进一步的性能提升。*'
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The Authors’ code can be found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech-SAT).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献的。作者的代码可以在[这里](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech-SAT)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: UniSpeechSat is a speech model that accepts a float array corresponding to the
    raw waveform of the speech signal. Please use [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    for the feature extraction.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UniSpeechSat是一个语音模型，接受与语音信号的原始波形对应的浮点数组。请使用[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)进行特征提取。
- en: UniSpeechSat model can be fine-tuned using connectionist temporal classification
    (CTC) so the model output has to be decoded using [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UniSpeechSat模型可以使用连接主义时间分类（CTC）进行微调，因此模型输出必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)进行解码。
- en: UniSpeechSat performs especially well on speaker verification, speaker identification,
    and speaker diarization tasks.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UniSpeechSat在说话者验证、说话者识别和说话者日程任务中表现特别好。
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Audio classification task guide](../tasks/audio_classification)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[音频分类任务指南](../tasks/audio_classification)'
- en: '[Automatic speech recognition task guide](../tasks/asr)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动语音识别任务指南](../tasks/asr)'
- en: UniSpeechSatConfig
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSatConfig
- en: '### `class transformers.UniSpeechSatConfig`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechSatConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/configuration_unispeech_sat.py#L34)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/configuration_unispeech_sat.py#L34)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 32) — Vocabulary size of the UniSpeechSat
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling [UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel).
    Vocabulary size of the model. Defines the different tokens that can be represented
    by the *inputs_ids* passed to the forward method of [UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为32）— UniSpeechSat模型的词汇大小。定义了在调用[UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)时可以表示的不同令牌的数量。模型的词汇大小。定义了可以由传递给[UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)的*inputs_ids*表示的不同令牌。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*，默认为768）— 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers`（`int`，*可选*，默认为12）— Transformer编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`（`int`，*可选*，默认为12）— Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入、编码器和池化器中所有完全连接层的dropout概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 完全连接层内激活的dropout比率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for output of the feature encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 特征编码器输出的dropout概率。'
- en: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for the output of the feature encoder that’s used by the quantizer.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — 用于量化器使用的特征编码器输出的dropout概率。'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the final projection layer of [UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_dropout` (`float`, *optional*, defaults to 0.1) — [UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC)的最终投影层的dropout概率。'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop probability.
    See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.1) — LayerDrop概率。更多详情请参阅[LayerDrop论文](see
    [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。'
- en: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — The norm to
    be applied to 1D convolutional layers in feature encoder. One of `"group"` for
    group normalization of only the first 1D convolutional layer or `"layer"` for
    layer normalization of all 1D convolutional layers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — 应用于特征编码器中的1D卷积层的规范化方式。可以选择`"group"`表示仅对第一个1D卷积层进行组归一化，或者选择`"layer"`表示对所有1D卷积层进行层归一化。'
- en: '`feat_extract_activation` (`str, *optional*, defaults to` “gelu”`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` “gelu”`,` “relu”`,` “selu”`and`“gelu_new”` are
    supported.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_activation` (`str, *optional*, defaults to` “gelu”`) -- 特征提取器中1D卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持`“gelu”`、`“relu”`、`“selu”`和`“gelu_new”`。'
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — A tuple of integers defining the number of input
    and output channels of each 1D convolutional layer in the feature encoder. The
    length of *conv_dim* defines the number of 1D convolutional layers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — 定义特征编码器中每个1D卷积层的输入和输出通道数的整数元组。*conv_dim*的长度定义了1D卷积层的数量。'
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the feature encoder. The length of *conv_stride* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — 定义特征编码器中每个1D卷积层的步幅的整数元组。*conv_stride*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 2, 2)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the feature encoder. The length of *conv_kernel* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 2, 2)`) — 定义特征编码器中每个1D卷积层的卷积核大小的整数元组。*conv_kernel*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) — Whether the 1D convolutional
    layers have a bias.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_bias` (`bool`, *optional*, defaults to `False`) — 1D卷积层是否带有偏置。'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了1D卷积位置嵌入层的卷积核大小。'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D卷积位置嵌入层的组数。'
- en: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — Whether
    to apply *stable* layer norm architecture of the Transformer encoder. `do_stable_layer_norm
    is True` corresponds to applying layer norm before the attention layer, whereas
    `do_stable_layer_norm is False` corresponds to applying layer norm after the attention
    layer.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — 是否应用Transformer编码器的*稳定*层归一化架构。`do_stable_layer_norm
    is True`表示在注意力层之前应用层归一化，而`do_stable_layer_norm is False`表示在注意力层之后应用层归一化。'
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — Whether to
    apply *SpecAugment* data augmentation to the outputs of the feature encoder. For
    reference see [SpecAugment: A Simple Data Augmentation Method for Automatic Speech
    Recognition](https://arxiv.org/abs/1904.08779).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_spec_augment` (`bool`, *optional*, 默认为`True`) — 是否对特征编码器的输出应用*SpecAugment*数据增强。有关详细信息，请参阅[SpecAugment：用于自动语音识别的简单数据增强方法](https://arxiv.org/abs/1904.08779)。'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates ”mask_time_prob*len(time_axis)/mask_time_length” independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if` apply_spec_augment is
    True`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *optional*, 默认为0.05) — 沿时间轴遮蔽的所有特征向量的百分比（介于0和1之间）。遮蔽过程在该轴上生成“mask_time_prob*len(time_axis)/mask_time_length”个独立的遮罩。如果从每个特征向量被选择为要遮蔽的向量跨度的起点的概率推理，*mask_time_prob*应为`prob_vector_start*mask_time_length`。请注意，重叠可能会降低实际遮蔽向量的百分比。仅在`apply_spec_augment`为True时相关。'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) — Length of vector span
    along the time axis.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *optional*, 默认为10) — 沿时间轴的向量跨度长度。'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2) — The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *optional*, 默认为2) — 沿时间轴生成的长度为`mask_feature_length`的最小遮罩数量，每个时间步，与`mask_feature_prob`无关。仅在“mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”时相关。'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates ”mask_feature_prob*len(feature_axis)/mask_time_length”
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *optional*, 默认为0.0) — 沿特征轴遮蔽的所有特征向量的百分比（介于0和1之间）。遮蔽过程在该轴上生成“mask_feature_prob*len(feature_axis)/mask_time_length”个独立的遮罩。如果从每个特征向量被选择为要遮蔽的向量跨度的起点的概率推理，*mask_feature_prob*应为`prob_vector_start*mask_feature_length`。请注意，重叠可能会降低实际遮蔽向量的百分比。仅在`apply_spec_augment`为True时相关。'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) — Length of vector
    span along the feature axis.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *optional*, 默认为10) — 沿特征轴的向量跨度长度。'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0) — The minimum number
    of masks of length `mask_feature_length` generated along the feature axis, each
    time step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *optional*, 默认为0) — 沿特征轴生成的长度为`mask_feature_length`的最小遮罩数量，每个时间步，与`mask_feature_prob`无关。仅在“mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”时相关。'
- en: '`num_codevectors_per_group` (`int`, *optional*, defaults to 320) — Number of
    entries in each quantization codebook (group).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevectors_per_group` (`int`, *optional*, 默认为320) — 每个量化码书（组）中的条目数。'
- en: '`num_codevector_groups` (`int`, *optional*, defaults to 2) — Number of codevector
    groups for product codevector quantization.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevector_groups` (`int`, *optional*, 默认为2) — 产品码矢量量化的码矢量组数。'
- en: '`contrastive_logits_temperature` (`float`, *optional*, defaults to 0.1) — The
    temperature *kappa* in the contrastive loss.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_logits_temperature` (`float`, *optional*, 默认为0.1) — 对比损失中的温度*kappa*。'
- en: '`num_negatives` (`int`, *optional*, defaults to 100) — Number of negative samples
    for the contrastive loss.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_negatives` (`int`, *optional*, 默认为100) — 对比损失的负样本数量。'
- en: '`codevector_dim` (`int`, *optional*, defaults to 256) — Dimensionality of the
    quantized feature vectors.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codevector_dim` (`int`, *optional*, 默认为256) — 量化特征向量的维度。'
- en: '`proj_codevector_dim` (`int`, *optional*, defaults to 256) — Dimensionality
    of the final projection of both the quantized and the transformer features.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proj_codevector_dim` (`int`, *optional*, 默认为256) — 量化和变换特征的最终投影的维度。'
- en: '`diversity_loss_weight` (`int`, *optional*, defaults to 0.1) — The weight of
    the codebook diversity loss component.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss_weight` (`int`, *optional*, 默认为0.1) — 码书多样性损失组件的权重。'
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"mean"`) — Specifies
    the reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when
    training an instance of [UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_loss_reduction` (`str`, *optional*, 默认为`"mean"`) — 指定应用于`torch.nn.CTCLoss`输出的减少方式。仅在训练[UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC)实例时相关。'
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_zero_infinity` (`bool`, *optional*, 默认为`False`) — 是否将`torch.nn.CTCLoss`的无限损失和相关梯度置零。当输入太短无法与目标对齐时，主要会出现无限损失。仅在训练[UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC)实例时相关。'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [UniSpeechSatForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`，*可选*，默认为`False`）— 是否使用带有学习权重的层输出的加权平均值。仅在使用[UniSpeechSatForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification)的实例时相关。'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) — Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`，*可选*，默认为256）— 分类前的投影维度均值池化。'
- en: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) — A tuple of integers defining the number of output channels
    of each 1D convolutional layer in the *TDNN* module of the *XVector* model. The
    length of *tdnn_dim* defines the number of *TDNN* layers.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dim` (`Tuple[int]`或`List[int]`，*可选*，默认为`(512, 512, 512, 512, 1500)`）—
    一个整数元组，定义*XVector*模型的*TDNN*模块中每个1D卷积层的输出通道数。 *tdnn_dim*的长度定义了*TDNN*层的数量。'
- en: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the *TDNN* module of the *XVector* model. The length of *tdnn_kernel*
    has to match the length of *tdnn_dim*.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_kernel` (`Tuple[int]`或`List[int]`，*可选*，默认为`(5, 3, 3, 1, 1)`）— 一个整数元组，定义*XVector*模型的*TDNN*模块中每个1D卷积层的内核大小。
    *tdnn_kernel*的长度必须与*tdnn_dim*的长度相匹配。'
- en: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) — A tuple of integers defining the dilation factor of each 1D convolutional
    layer in *TDNN* module of the *XVector* model. The length of *tdnn_dilation* has
    to match the length of *tdnn_dim*.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dilation` (`Tuple[int]`或`List[int]`，*可选*，默认为`(1, 2, 3, 1, 1)`）— 一个整数元组，定义*XVector*模型的*TDNN*模块中每个1D卷积层的扩张因子。
    *tdnn_dilation*的长度必须与*tdnn_dim*的长度相匹配。'
- en: '`xvector_output_dim` (`int`, *optional*, defaults to 512) — Dimensionality
    of the *XVector* embedding vectors.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xvector_output_dim` (`int`，*可选*，默认为512）— *XVector*嵌入向量的维度。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The id of the padding token.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`，*可选*，默认为0）— 填充标记的id。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 1) — The id of the “beginning-of-sequence”
    token.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`，*可选*，默认为1）— “序列开始”标记的id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — The id of the “end-of-sequence”
    token.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`，*可选*，默认为2）— “序列结束”标记的id。'
- en: '`num_clusters` (`int`, *optional*, defaults to 504) — Number of clusters for
    weak labeling. Only relevant when using an instance of [UniSpeechSatForPreTraining](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_clusters` (`int`，*可选*，默认为504）— 弱标记的簇数。仅在使用[UniSpeechSatForPreTraining](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining)的实例时相关。'
- en: This is the configuration class to store the configuration of a [UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel).
    It is used to instantiate an UniSpeechSat model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the UniSpeechSat [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)
    architecture.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)配置的配置类。根据指定的参数实例化UniSpeechSat模型，定义模型架构。使用默认值实例化配置将产生类似于UniSpeechSat
    [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: UniSpeechSat specific outputs
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSat特定输出
- en: '### `class transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L79)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L79)'
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (*optional*, returned when model is in train mode, `torch.FloatTensor`
    of shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（*可选*，当模型处于训练模式时返回，形状为`(1,)`的`torch.FloatTensor`）— 总损失，作为对比损失（L_m）和多样性损失（L_d）的和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。
    （分类）损失。'
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`）—
    模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码的投影量化状态。'
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    config.proj_codevector_dim)`）— 量化提取的特征向量投影到*config.proj_codevector_dim*，表示对比损失的正目标向量。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of `UniSpeechSatForPreTrainingOutput`, with potential hidden states
    and attentions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`UniSpeechSatForPreTrainingOutput`的输出类型，具有潜在的隐藏状态和注意力。'
- en: UniSpeechSatModel
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSatModel
- en: '### `class transformers.UniSpeechSatModel`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechSatModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1100)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1100)'
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）-
    模型的配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: 'The bare UniSpeechSat Model transformer outputting raw hidden-states without
    any specific head on top. UniSpeechSat was proposed in [wav2vec 2.0: A Framework
    for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的UniSpeechSat模型变压器输出原始隐藏状态，没有特定的头部。UniSpeechSat是由Alexei Baevski、Henry Zhou、Abdelrahman
    Mohamed、Michael Auli在[《wav2vec 2.0:自监督学习语音表示的框架》](https://arxiv.org/abs/2006.11477)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以了解所有与一般用法和行为相关的事项。
- en: '#### `forward`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1168)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1168)'
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库（`pip
    install soundfile`）。要准备好数组为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如[microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)，在进行批量推断时应`不`传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    and inputs.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）—
    模型最后一层输出的隐藏状态序列。'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features`（形状为`(batch_size, sequence_length, conv_dim[-1])`的`torch.FloatTensor`）—
    模型最后一个卷积层提取的特征向量序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)
    forward method, overrides the `__call__` special method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[UniSpeechSatModel](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: UniSpeechSatForCTC
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSatForCTC
- en: '### `class transformers.UniSpeechSatForCTC`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechSatForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1362)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1362)'
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — 具有模型所有参数的模型配置类。 使用配置文件初始化不会加载与模型关联的权重，只会加载配置。 请查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: '`target_lang` (`str`, *optional*) — Language id of adapter weights. Adapter
    weights are stored in the format adapter.<lang>.safetensors or adapter.<lang>.bin.
    Only relevant when using an instance of [UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC)
    with adapters. Uses ‘eng’ by default.</lang></lang>'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_lang` (`str`, *optional*) — 适配器权重的语言 id。 适配器权重存储在格式为 adapter.<lang>.safetensors
    或 adapter.<lang>.bin 的文件中。 仅在使用带有适配器的 [UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC)
    实例时相关。 默认情况下使用 ‘eng’。</lang></lang>'
- en: 'UniSpeechSat Model with a `language modeling` head on top for Connectionist
    Temporal Classification (CTC). UniSpeechSat was proposed in [wav2vec 2.0: A Framework
    for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '带有顶部 `语言建模` 的 UniSpeechSat 模型，用于 Connectionist Temporal Classification (CTC)。
    UniSpeechSat 是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec
    2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。
    请查看超类文档，了解库为其所有模型实现的通用方法（例如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。 将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1445)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1445)'
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 输入原始语音波形的浮点值。 值可以通过将 `.flac` 或 `.wav` 音频文件加载到 `List[float]` 类型的数组或 `numpy.ndarray`
    中获得，例如通过 soundfile 库 (`pip install soundfile`)。 要将数组准备成 `input_values`，应使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    进行填充和转换为 `torch.FloatTensor` 类型的张量。 有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。 选择的掩码值在 `[0, 1]` 中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表 `未被掩码` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表 `被掩码` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在相应的处理器具有 `config.return_attention_mask == True` 时才应传递 `attention_mask`。 对于所有处理器具有
    `config.return_attention_mask == False` 的模型，例如 [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)，在进行批量推断时应
    `不` 传递 `attention_mask` 以避免性能下降。 对于这些模型，`input_values` 应简单地填充为 0 并在不传递 `attention_mask`
    的情况下传递。 请注意，这些模型的结果也会因 `input_values` 是否填充而略有不同。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。 有关更多详细信息，请参阅返回张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。 有关更多详细信息，请参阅返回张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    — Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, target_length)`，*optional*) —
    用于连接主义时间分类的标签。注意`target_length`必须小于或等于输出logits的序列长度。索引在`[-100, 0, ..., config.vocab_size
    - 1]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0, ..., config.vocab_size - 1]`中的标签。'
- en: Returns
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`torch.FloatTensor`元组'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    and inputs.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[UniSpeechSatForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: UniSpeechSatForSequenceClassification
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSatForSequenceClassification
- en: '### `class transformers.UniSpeechSatForSequenceClassification`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechSatForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1525)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1525)'
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: UniSpeechSat Model with a sequence classification head on top (a linear layer
    over the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: UniSpeechSat模型在顶部具有一个序列分类头（在池化输出上的线性层），用于类似SUPERB关键词检测的任务。
- en: 'UniSpeechSat was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: UniSpeechSat是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[《wav2vec
    2.0:自监督学习语音表示的框架》](https://arxiv.org/abs/2006.11477)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1580)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1580)'
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库(`pip
    install soundfile`)。要准备好数组以获得`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参见[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充令牌索引上执行卷积和注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被掩盖的令牌，
- en: 0 for tokens that are `masked`.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被掩盖的令牌。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如[microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)，在进行批量推断时，应`不`传递`attention_mask`以避免性能下降。对于这样的模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    and inputs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包含根据配置([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))和输入而异的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果config.num_labels==1则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出一个，+
    每个层的输出一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中使用注意力softmax后的注意力权重，用于计算加权平均值。
- en: The [UniSpeechSatForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[UniSpeechSatForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification)前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传播的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: UniSpeechSatForAudioFrameClassification
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSatForAudioFrameClassification
- en: '### `class transformers.UniSpeechSatForAudioFrameClassification`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechSatForAudioFrameClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1650)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1650)'
- en: '[PRE12]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig）
    - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: UniSpeech-SAT Model with a frame classification head on top for tasks like Speaker
    Diarization.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: UniSpeech-SAT模型顶部带有用于说话人分离等任务的帧分类头。
- en: 'UniSpeechSat was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: UniSpeechSat是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[wav2vec
    2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1701)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1701)'
- en: '[PRE13]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`） - 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得。通过[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)将数组准备为`input_values`，并将其转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）
    - 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如[microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)，在进行批量推断时，应`不`传递`attention_mask`以避免性能下降。对于这样的模型，`input_values`应该简单地用0填充并在不带`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充而产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    and inputs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）—
    分类分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，+
    每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [UniSpeechSatForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[UniSpeechSatForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: UniSpeechSatForXVector
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSatForXVector
- en: '### `class transformers.UniSpeechSatForXVector`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechSatForXVector`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1814)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1814)'
- en: '[PRE15]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: UniSpeech-SAT Model with an XVector feature extraction head on top for tasks
    like Speaker Verification.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: UniSpeech-SAT模型在顶部使用XVector特征提取头，用于说话者验证等任务。
- en: 'UniSpeechSat was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'UniSpeechSat是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[wav2vec
    2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1883)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1883)'
- en: '[PRE16]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`） — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，*例如*通过soundfile库（`pip
    install soundfile`）。要准备好数组为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）
    — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`屏蔽`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如[microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)，在进行批量推断时应`不`传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应简单地填充为0并在不传递`attention_mask`的情况下传递。请注意，这些模型的结果也会因`input_values`是否填充而略有不同。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*） — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    and inputs.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    — Classification hidden states before AMSoftmax.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.xvector_output_dim)`)
    — AMSoftmax之前的分类隐藏状态。'
- en: '`embeddings` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    — Utterance embeddings used for vector similarity-based retrieval.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embeddings` (`torch.FloatTensor`，形状为`(batch_size, config.xvector_output_dim)`)
    — 用于基于向量相似性检索的话语嵌入。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [UniSpeechSatForXVector](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector)
    forward method, overrides the `__call__` special method.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[UniSpeechSatForXVector](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: UniSpeechSatForPreTraining
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechSatForPreTraining
- en: '### `class transformers.UniSpeechSatForPreTraining`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechSatForPreTraining`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1224)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1224)'
- en: '[PRE18]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: 'UniSpeechSat Model with a quantizer and `VQ` head on top. UniSpeechSat was
    proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: UniSpeechSat模型具有量化器和顶部的`VQ`头。UniSpeechSat是由Alexei Baevski、Henry Zhou、Abdelrahman
    Mohamed、Michael Auli在[《wav2vec 2.0:自监督学习语音表示的框架》](https://arxiv.org/abs/2006.11477)中提出的。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1293)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py#L1293)'
- en: '[PRE19]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得。通过声音文件库（`pip
    install soundfile`）等方式。要准备好数组为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表未被掩盖的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表被掩盖的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如 [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft)，在进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`
    应该简单地用 0 填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: Returns
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig))
    and inputs.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput)
    或一个 `torch.FloatTensor` 元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（[UniSpeechSatConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig)）和输入。
- en: '`loss` (*optional*, returned when model is in train mode, `torch.FloatTensor`
    of shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*optional*，在模型处于训练模式时返回，形状为`(1,)`的`torch.FloatTensor`) — 总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。
    （分类）损失。'
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`)
    — 模型的隐藏状态投影到 *config.proj_codevector_dim*，可用于预测掩码投影量化状态。'
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    config.proj_codevector_dim)`) — 投影到 *config.proj_codevector_dim* 的量化提取特征向量，表示对比损失的正目标向量。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选的*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [UniSpeechSatForPreTraining](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[UniSpeechSatForPreTraining](/docs/transformers/v4.37.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个函数，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
