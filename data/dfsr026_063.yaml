- en: T2I-Adapter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: T2I-Adapter
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/t2i_adapters](https://huggingface.co/docs/diffusers/training/t2i_adapters)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/training/t2i_adapters](https://huggingface.co/docs/diffusers/training/t2i_adapters)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[T2I-Adapter](https://hf.co/papers/2302.08453) is a lightweight adapter model
    that provides an additional conditioning input image (line art, canny, sketch,
    depth, pose) to better control image generation. It is similar to a ControlNet,
    but it is a lot smaller (~77M parameters and ~300MB file size) because its only
    inserts weights into the UNet instead of copying and training it.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[T2I-Adapter](https://hf.co/papers/2302.08453) 是一个轻量级的适配器模型，提供额外的条件输入图像（线条艺术、canny、素描、深度、姿势）以更好地控制图像生成。它类似于
    ControlNet，但更小（~77M 参数和 ~300MB 文件大小），因为它只向 UNet 插入权重，而不是复制和训练它。'
- en: The T2I-Adapter is only available for training with the Stable Diffusion XL
    (SDXL) model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: T2I-Adapter 仅适用于使用 Stable Diffusion XL (SDXL) 模型进行训练。
- en: This guide will explore the [train_t2i_adapter_sdxl.py](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py)
    training script to help you become familiar with it, and how you can adapt it
    for your own use-case.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将探讨 [train_t2i_adapter_sdxl.py](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py)
    训练脚本，帮助您熟悉它，以及如何为自己的用例进行适应。
- en: 'Before running the script, make sure you install the library from source:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行脚本之前，请确保从源代码安装库：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then navigate to the example folder containing the training script and install
    the required dependencies for the script you’re using:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后导航到包含训练脚本的示例文件夹，并安装您正在使用的脚本所需的依赖项：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. It’ll automatically configure your training setup based on your
    hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Accelerate 是一个帮助您在多个 GPU/TPU 或混合精度下训练的库。它会根据您的硬件和环境自动配置您的训练设置。查看 🤗 Accelerate
    [快速入门](https://huggingface.co/docs/accelerate/quicktour) 以了解更多。
- en: 'Initialize an 🤗 Accelerate environment:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个 🤗 Accelerate 环境：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To setup a default 🤗 Accelerate environment without choosing any configurations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置一个默认的 🤗 Accelerate 环境而不选择任何配置：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or if your environment doesn’t support an interactive shell, like a notebook,
    you can use:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 或者如果您的环境不支持交互式 shell，比如笔记本，您可以使用：
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您想在自己的数据集上训练模型，请查看 [创建用于训练的数据集](create_dataset) 指南，了解如何创建适用于训练脚本的数据集。
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesn’t cover every aspect of the script
    in detail. If you’re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py)
    and let us know if you have any questions or concerns.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分突出显示了训练脚本中重要的部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读 [脚本](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py)，并告诉我们您是否有任何问题或疑虑。
- en: Script parameters
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚本参数
- en: The training script provides many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L233)
    function. It provides default values for each parameter, such as the training
    batch size and learning rate, but you can also set your own values in the training
    command if you’d like.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本提供了许多参数，帮助您自定义训练运行。所有参数及其描述都可以在 [`parse_args()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L233)
    函数中找到。它为每个参数提供了默认值，如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。
- en: 'For example, to activate gradient accumulation, add the `--gradient_accumulation_steps`
    parameter to the training command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要激活梯度累积，将 `--gradient_accumulation_steps` 参数添加到训练命令中：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Many of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters)
    training guide, so this guide just focuses on the relevant T2I-Adapter parameters:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基本和重要的参数在 [Text-to-image](text2image#script-parameters) 训练指南中有描述，因此本指南只关注相关的
    T2I-Adapter 参数：
- en: '`--pretrained_vae_model_name_or_path`: path to a pretrained VAE; the SDXL VAE
    is known to suffer from numerical instability, so this parameter allows you to
    specify a better [VAE](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--pretrained_vae_model_name_or_path`：预训练 VAE 的路径；SDXL VAE 已知存在数值不稳定性问题，因此此参数允许您指定更好的
    [VAE](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)'
- en: '`--crops_coords_top_left_h` and `--crops_coords_top_left_w`: height and width
    coordinates to include in SDXL’s crop coordinate embeddings'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--crops_coords_top_left_h` 和 `--crops_coords_top_left_w`：要包含在 SDXL 的裁剪坐标嵌入中的高度和宽度坐标'
- en: '`--conditioning_image_column`: the column of the conditioning images in the
    dataset'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--conditioning_image_column`：数据集中条件图像的列'
- en: '`--proportion_empty_prompts`: the proportion of image prompts to replace with
    empty strings'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--proportion_empty_prompts`：要替换为空字符串的图像提示的比例'
- en: Training script
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练脚本
- en: As with the script parameters, a walkthrough of the training script is provided
    in the [Text-to-image](text2image#training-script) training guide. Instead, this
    guide takes a look at the T2I-Adapter relevant parts of the script.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与脚本参数一样，[Text-to-image](text2image#training-script) 训练指南中提供了训练脚本的详细说明。相反，本指南将查看脚本中与
    T2I-Adapter 相关的部分。
- en: The training script begins by preparing the dataset. This incudes [tokenizing](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L674)
    the prompt and [applying transforms](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L714)
    to the images and conditioning images.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本从准备数据集开始。这包括[tokenizing](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L674)提示和[应用变换](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L714)到图像和调节图像。
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Within the [`main()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L770)
    function, the T2I-Adapter is either loaded from a pretrained adapter or it is
    randomly initialized:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在[`main()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L770)函数中，T2I-Adapter要么从预训练的适配器加载，要么随机初始化：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The [optimizer](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L952)
    is initialized for the T2I-Adapter parameters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[optimizer](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L952)已初始化为T2I-Adapter参数：'
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, in the [training loop](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L1086),
    the adapter conditioning image and the text embeddings are passed to the UNet
    to predict the noise residual:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[training loop](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/t2i_adapter/train_t2i_adapter_sdxl.py#L1086)中，适配器调节图像和文本嵌入被传递给UNet以预测噪声残差：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you want to learn more about how the training loop works, check out the [Understanding
    pipelines, models and schedulers](../using-diffusers/write_own_pipeline) tutorial
    which breaks down the basic pattern of the denoising process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解训练循环的工作原理，请查看[Understanding pipelines, models and schedulers](../using-diffusers/write_own_pipeline)教程，该教程解析了去噪过程的基本模式。
- en: Launch the script
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动脚本
- en: Now you’re ready to launch the training script! 🚀
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好启动训练脚本了！🚀
- en: For this example training, you’ll use the [fusing/fill50k](https://huggingface.co/datasets/fusing/fill50k)
    dataset. You can also create and use your own dataset if you want (see the [Create
    a dataset for training](https://moon-ci-docs.huggingface.co/docs/diffusers/pr_5512/en/training/create_dataset)
    guide).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例训练，您将使用[fusing/fill50k](https://huggingface.co/datasets/fusing/fill50k)数据集。如果您愿意，您也可以创建并使用自己的数据集（请参阅[Create
    a dataset for training](https://moon-ci-docs.huggingface.co/docs/diffusers/pr_5512/en/training/create_dataset)指南）。
- en: Set the environment variable `MODEL_DIR` to a model id on the Hub or a path
    to a local model and `OUTPUT_DIR` to where you want to save the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将环境变量`MODEL_DIR`设置为Hub上的模型ID或本地模型的路径，将`OUTPUT_DIR`设置为您想要保存模型的位置。
- en: 'Download the following images to condition your training with:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下载以下图像以调节您的训练：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To monitor training progress with Weights & Biases, add the `--report_to=wandb`
    parameter to the training command. You’ll also need to add the `--validation_image`,
    `--validation_prompt`, and `--validation_steps` to the training command to keep
    track of results. This can be really useful for debugging the model and viewing
    intermediate results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Weights & Biases监控训练进度，将`--report_to=wandb`参数添加到训练命令中。您还需要将`--validation_image`，`--validation_prompt`和`--validation_steps`添加到训练命令中以跟踪结果。这对于调试模型和查看中间结果非常有用。
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once training is complete, you can use your T2I-Adapter for inference:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，您可以使用您的T2I-Adapter进行推断。
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next steps
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'Congratulations on training a T2I-Adapter model! 🎉 To learn more:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您训练了一个T2I-Adapter模型！🎉 要了解更多：
- en: Read the [Efficient Controllable Generation for SDXL with T2I-Adapters](https://huggingface.co/blog/t2i-sdxl-adapters)
    blog post to learn more details about the experimental results from the T2I-Adapter
    team.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读[Efficient Controllable Generation for SDXL with T2I-Adapters](https://huggingface.co/blog/t2i-sdxl-adapters)博文，了解T2I-Adapter团队的实验结果的更多细节。
