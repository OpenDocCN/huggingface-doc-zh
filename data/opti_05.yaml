- en: ğŸ¤— Optimum notebooks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimum ç¬”è®°æœ¬
- en: 'Original text: [https://huggingface.co/docs/optimum/notebooks](https://huggingface.co/docs/optimum/notebooks)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/optimum/notebooks](https://huggingface.co/docs/optimum/notebooks)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: You can find here a list of the notebooks associated with each accelerator in
    ğŸ¤— Optimum.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ğŸ¤— Optimumä¸­æ‰¾åˆ°ä¸æ¯ä¸ªåŠ é€Ÿå™¨ç›¸å…³çš„ç¬”è®°æœ¬åˆ—è¡¨ã€‚
- en: Optimum Habana
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optimum Habana
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| ç¬”è®°æœ¬ | æè¿° | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to use DeepSpeed to train models with billions of parameters on Habana
    Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL
    for causal language modeling on Habana Gaudi. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨DeepSpeedåœ¨Habana Gaudiä¸Šè®­ç»ƒæ‹¥æœ‰æ•°åäº¿å‚æ•°çš„æ¨¡å‹](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | å±•ç¤ºå¦‚ä½•ä½¿ç”¨DeepSpeedåœ¨Habana Gaudiä¸Šé¢„è®­ç»ƒ/å¾®è°ƒ1.6Bå‚æ•°çš„GPT2-XLï¼Œç”¨äºå› æœè¯­è¨€å»ºæ¨¡ | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    |'
- en: Optimum Intel
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optimum Intel
- en: OpenVINO
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenVINO
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| ç¬”è®°æœ¬ | æè¿° | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | Explains how to export your model to OpenVINO and run inference with OpenVINO
    Runtime on various tasks | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨OpenVINOè¿›è¡Œæ¨ç†](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | è§£é‡Šå¦‚ä½•å°†æ¨¡å‹å¯¼å‡ºåˆ°OpenVINOï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šä½¿ç”¨OpenVINO Runtimeè¿›è¡Œæ¨ç† | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    |'
- en: '| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | Show how to apply post-training quantization on a question answering model using
    [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with
    OpenVINO | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨NNCFå¯¹é—®ç­”æ¨¡å‹è¿›è¡Œé‡åŒ–](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | å±•ç¤ºå¦‚ä½•ä½¿ç”¨[NNCF](https://github.com/openvinotoolkit/nncf)å¯¹é—®ç­”æ¨¡å‹è¿›è¡Œè®­ç»ƒåé‡åŒ–ï¼Œå¹¶ä½¿ç”¨OpenVINOåŠ é€Ÿæ¨ç†
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    |'
- en: '| [Compare outputs of a quantized Stable Diffusion model with its full-precision
    counterpart](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | Show how to load and compare outputs from two Stable Diffusion models with different
    precision | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨é‡åŒ–çš„ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è¾“å‡ºä¸å…¶å…¨ç²¾åº¦å¯¹åº”ç‰©è¿›è¡Œæ¯”è¾ƒ](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | å±•ç¤ºå¦‚ä½•åŠ è½½å’Œæ¯”è¾ƒä¸¤ä¸ªå…·æœ‰ä¸åŒç²¾åº¦çš„ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è¾“å‡º | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    |'
- en: Neural Compressor
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¥ç»å‹ç¼©å™¨
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| ç¬”è®°æœ¬ | æè¿° | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | Show how to apply quantization while training your model using Intel [Neural
    Compressor](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open
    in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨Intel Neural Compressorå¯¹æ–‡æœ¬åˆ†ç±»æ¨¡å‹è¿›è¡Œé‡åŒ–](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | å±•ç¤ºå¦‚ä½•åœ¨ä»»ä½•GLUEä»»åŠ¡ä¸­ä½¿ç”¨Intel [Neural Compressor](https://github.com/intel/neural-compressor)åœ¨è®­ç»ƒæ¨¡å‹æ—¶åº”ç”¨é‡åŒ–ã€‚
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    |'
- en: Optimum ONNX Runtime
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€ä½³ONNX Runtime
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ç¬”è®°æœ¬ | æè¿° | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime)
    for any GLUE task. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨ONNX Runtimeå¯¹æ–‡æœ¬åˆ†ç±»æ¨¡å‹è¿›è¡Œé‡åŒ–](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | å±•ç¤ºå¦‚ä½•åœ¨ä»»ä½•GLUEä»»åŠ¡ä¸­ä½¿ç”¨[ONNX Runtime](https://github.com/microsoft/onnxruntime)å¯¹æ¨¡å‹åº”ç”¨é™æ€å’ŒåŠ¨æ€é‡åŒ–ã€‚
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    |'
- en: '| [How to fine-tune a model for text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | Show how to DistilBERT model on GLUE tasks using [ONNX Runtime](https://github.com/microsoft/onnxruntime).
    | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨ONNX Runtimeå¯¹æ–‡æœ¬åˆ†ç±»æ¨¡å‹è¿›è¡Œå¾®è°ƒ](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | å±•ç¤ºå¦‚ä½•åœ¨GLUEä»»åŠ¡ä¸­ä½¿ç”¨[ONNX Runtime](https://github.com/microsoft/onnxruntime)å¾®è°ƒDistilBERTæ¨¡å‹ã€‚
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    |'
- en: '| [How to fine-tune a model for summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | Show how to fine-tune a T5 model on the BBC news corpus. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨ONNX Runtimeå¯¹æ‘˜è¦æ¨¡å‹è¿›è¡Œå¾®è°ƒ](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | å±•ç¤ºå¦‚ä½•åœ¨BBCæ–°é—»è¯­æ–™åº“ä¸Šå¾®è°ƒT5æ¨¡å‹ã€‚ | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    |'
- en: '| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | Show how to fine-tune a DeBERTa model on the squad. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [å¦‚ä½•ä½¿ç”¨ONNX Runtimeå¯¹DeBERTaè¿›è¡Œé—®ç­”å¾®è°ƒ](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | å±•ç¤ºå¦‚ä½•åœ¨squadä¸Šå¾®è°ƒDeBERTaæ¨¡å‹ã€‚ | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | [![åœ¨AWS Studioä¸­æ‰“å¼€](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    |'
