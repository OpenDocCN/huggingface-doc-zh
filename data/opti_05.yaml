- en: 🤗 Optimum notebooks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗 Optimum 笔记本
- en: 'Original text: [https://huggingface.co/docs/optimum/notebooks](https://huggingface.co/docs/optimum/notebooks)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/optimum/notebooks](https://huggingface.co/docs/optimum/notebooks)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: You can find here a list of the notebooks associated with each accelerator in
    🤗 Optimum.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在🤗 Optimum中找到与每个加速器相关的笔记本列表。
- en: Optimum Habana
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optimum Habana
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 笔记本 | 描述 | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to use DeepSpeed to train models with billions of parameters on Habana
    Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL
    for causal language modeling on Habana Gaudi. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用DeepSpeed在Habana Gaudi上训练拥有数十亿参数的模型](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | 展示如何使用DeepSpeed在Habana Gaudi上预训练/微调1.6B参数的GPT2-XL，用于因果语言建模 | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    |'
- en: Optimum Intel
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optimum Intel
- en: OpenVINO
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenVINO
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 笔记本 | 描述 | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | Explains how to export your model to OpenVINO and run inference with OpenVINO
    Runtime on various tasks | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用OpenVINO进行推理](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | 解释如何将模型导出到OpenVINO，并在各种任务上使用OpenVINO Runtime进行推理 | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    |'
- en: '| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | Show how to apply post-training quantization on a question answering model using
    [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with
    OpenVINO | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用NNCF对问答模型进行量化](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | 展示如何使用[NNCF](https://github.com/openvinotoolkit/nncf)对问答模型进行训练后量化，并使用OpenVINO加速推理
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    |'
- en: '| [Compare outputs of a quantized Stable Diffusion model with its full-precision
    counterpart](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | Show how to load and compare outputs from two Stable Diffusion models with different
    precision | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [使用量化的稳定扩散模型的输出与其全精度对应物进行比较](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | 展示如何加载和比较两个具有不同精度的稳定扩散模型的输出 | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    |'
- en: Neural Compressor
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经压缩器
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 笔记本 | 描述 | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | Show how to apply quantization while training your model using Intel [Neural
    Compressor](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open
    in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用Intel Neural Compressor对文本分类模型进行量化](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | 展示如何在任何GLUE任务中使用Intel [Neural Compressor](https://github.com/intel/neural-compressor)在训练模型时应用量化。
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    |'
- en: Optimum ONNX Runtime
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳ONNX Runtime
- en: '| Notebook | Description | Colab | Studio Lab |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 笔记本 | 描述 | Colab | Studio Lab |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime)
    for any GLUE task. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用ONNX Runtime对文本分类模型进行量化](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | 展示如何在任何GLUE任务中使用[ONNX Runtime](https://github.com/microsoft/onnxruntime)对模型应用静态和动态量化。
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    |'
- en: '| [How to fine-tune a model for text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | Show how to DistilBERT model on GLUE tasks using [ONNX Runtime](https://github.com/microsoft/onnxruntime).
    | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用ONNX Runtime对文本分类模型进行微调](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | 展示如何在GLUE任务中使用[ONNX Runtime](https://github.com/microsoft/onnxruntime)微调DistilBERT模型。
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    |'
- en: '| [How to fine-tune a model for summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | Show how to fine-tune a T5 model on the BBC news corpus. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用ONNX Runtime对摘要模型进行微调](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | 展示如何在BBC新闻语料库上微调T5模型。 | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    |'
- en: '| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | Show how to fine-tune a DeBERTa model on the squad. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [如何使用ONNX Runtime对DeBERTa进行问答微调](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | 展示如何在squad上微调DeBERTa模型。 | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | [![在AWS Studio中打开](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    |'
