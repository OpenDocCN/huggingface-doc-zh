- en: TRL - Transformer Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRL - Transformer Reinforcement Learning
- en: 'Original text: [https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)
- en: '![](../Images/4b294f1850c2a0865587ccc36b23d404.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/4b294f1850c2a0865587ccc36b23d404.png)'
- en: TRL is a full stack library where we provide a set of tools to train transformer
    language models with Reinforcement Learning, from the Supervised Fine-tuning step
    (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.
    The library is integrated with ğŸ¤— [transformers](https://github.com/huggingface/transformers).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TRLæ˜¯ä¸€ä¸ªå…¨æ ˆåº“ï¼Œæˆ‘ä»¬æä¾›ä¸€å¥—å·¥å…·æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå˜å‹å™¨è¯­è¨€æ¨¡å‹ï¼Œä»ç›‘ç£å¾®è°ƒæ­¥éª¤ï¼ˆSFTï¼‰ã€å¥–åŠ±å»ºæ¨¡æ­¥éª¤ï¼ˆRMï¼‰åˆ°è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ­¥éª¤ã€‚è¯¥åº“ä¸ğŸ¤—
    [transformers](https://github.com/huggingface/transformers)é›†æˆã€‚
- en: '![](../Images/6bff4454a0be1455f1b3b09f74640ec7.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bff4454a0be1455f1b3b09f74640ec7.png)'
- en: 'Check the appropriate sections of the documentation depending on your needs:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ‚¨çš„éœ€æ±‚æŸ¥çœ‹æ–‡æ¡£ä¸­çš„é€‚å½“éƒ¨åˆ†ï¼š
- en: API documentation
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: APIæ–‡æ¡£
- en: '[Model Classes](models): *A brief overview of what each public model class
    does.*'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ¨¡å‹ç±»](models): *æ¯ä¸ªå…¬å…±æ¨¡å‹ç±»çš„ç®€è¦æ¦‚è¿°*'
- en: '[`SFTTrainer`](sft_trainer): *Supervise Fine-tune your model easily with `SFTTrainer`*'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`SFTTrainer`](sft_trainer): *ä½¿ç”¨`SFTTrainer`è½»æ¾ç›‘ç£å¾®è°ƒæ‚¨çš„æ¨¡å‹*'
- en: '[`RewardTrainer`](reward_trainer): *Train easily your reward model using `RewardTrainer`.*'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`RewardTrainer`](reward_trainer): *ä½¿ç”¨`RewardTrainer`è½»æ¾è®­ç»ƒæ‚¨çš„å¥–åŠ±æ¨¡å‹*'
- en: '[`PPOTrainer`](ppo_trainer): *Further fine-tune the supervised fine-tuned model
    using PPO algorithm*'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`PPOTrainer`](ppo_trainer): *ä½¿ç”¨PPOç®—æ³•è¿›ä¸€æ­¥å¾®è°ƒç›‘ç£å¾®è°ƒçš„æ¨¡å‹*'
- en: '[Best-of-N Sampling](best-of-n): *Use best of n sampling as an alternative
    way to sample predictions from your active model*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æœ€ä½³Né‡‡æ ·](best-of-n): *ä½¿ç”¨æœ€ä½³Né‡‡æ ·ä½œä¸ºä»æ´»è·ƒæ¨¡å‹ä¸­é‡‡æ ·é¢„æµ‹çš„æ›¿ä»£æ–¹å¼*'
- en: '[`DPOTrainer`](dpo_trainer): *Direct Preference Optimization training using
    `DPOTrainer`.*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`DPOTrainer`](dpo_trainer): *ä½¿ç”¨`DPOTrainer`è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–è®­ç»ƒ*'
- en: '[`TextEnvironment`](text_environment): *Text environment to train your model
    using tools with RL.*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`TextEnvironment`](text_environment): *ä½¿ç”¨RLå·¥å…·è®­ç»ƒæ‚¨çš„æ¨¡å‹çš„æ–‡æœ¬ç¯å¢ƒ*'
- en: Examples
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹
- en: '[Sentiment Tuning](sentiment_tuning): *Fine tune your model to generate positive
    movie contents*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æƒ…æ„Ÿå¾®è°ƒ](sentiment_tuning): *å¾®è°ƒæ‚¨çš„æ¨¡å‹ä»¥ç”Ÿæˆç§¯æçš„ç”µå½±å†…å®¹*'
- en: '[Training with PEFT](lora_tuning_peft): *Memory efficient RLHF training using
    adapters with PEFT*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨PEFTè¿›è¡Œè®­ç»ƒ](lora_tuning_peft): *ä½¿ç”¨PEFTå’Œé€‚é…å™¨è¿›è¡Œå†…å­˜é«˜æ•ˆçš„RLHFè®­ç»ƒ*'
- en: '[Detoxifying LLMs](detoxifying_a_lm): *Detoxify your language model through
    RLHF*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å»æ¯’åŒ–LLMs](detoxifying_a_lm): *é€šè¿‡RLHFå»æ¯’åŒ–æ‚¨çš„è¯­è¨€æ¨¡å‹*'
- en: '[StackLlama](using_llama_models): *End-to-end RLHF training of a Llama model
    on Stack exchange dataset*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨Llamaæ¨¡å‹](using_llama_models): *åœ¨Stackäº¤æ¢æ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„RLHFè®­ç»ƒ*'
- en: '[Learning with Tools](learning_tools): *Walkthrough of using `TextEnvironments`*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨å·¥å…·è¿›è¡Œå­¦ä¹ ](learning_tools): *ä½¿ç”¨`TextEnvironments`çš„æ¼”ç»ƒ*'
- en: '[Multi-Adapter Training](multi_adapter_rl): *Use a single base model and multiple
    adapters for memory efficient end-to-end training*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¤šé€‚é…å™¨è®­ç»ƒ](multi_adapter_rl): *ä½¿ç”¨å•ä¸ªåŸºç¡€æ¨¡å‹å’Œå¤šä¸ªé€‚é…å™¨è¿›è¡Œå†…å­˜é«˜æ•ˆçš„ç«¯åˆ°ç«¯è®­ç»ƒ*'
- en: Blog posts
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åšå®¢æ–‡ç« 
- en: '[![thumbnail](../Images/6a5d903a4cb2b18948ab6f3db912185c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![ç¼©ç•¥å›¾](../Images/6a5d903a4cb2b18948ab6f3db912185c.png)'
- en: Illustrating Reinforcement Learning from Human Feedback](https://huggingface.co/blog/rlhf)
    [![thumbnail](../Images/22aa00200f429b7d9ce649b675e3f3b1.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä»äººç±»åé¦ˆä¸­é˜é‡Šå¼ºåŒ–å­¦ä¹ ](https://huggingface.co/blog/rlhf) [![ç¼©ç•¥å›¾](../Images/22aa00200f429b7d9ce649b675e3f3b1.png)
- en: Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://huggingface.co/blog/trl-peft)
    [![thumbnail](../Images/c83a44cfb05cf6785db903a8231e5d59.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨24GBæ¶ˆè´¹çº§GPUä¸Šä½¿ç”¨RLHFå¯¹20B LLMè¿›è¡Œå¾®è°ƒ](https://huggingface.co/blog/trl-peft) [![ç¼©ç•¥å›¾](../Images/c83a44cfb05cf6785db903a8231e5d59.png)
- en: 'StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama)
    [![thumbnail](../Images/aca25811c595ffdf3eabce54f97a87d5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: StackLLaMAï¼šä½¿ç”¨RLHFè¿›è¡ŒLLaMAçš„å®è·µæŒ‡å—](https://huggingface.co/blog/stackllama) [![ç¼©ç•¥å›¾](../Images/aca25811c595ffdf3eabce54f97a87d5.png)
- en: Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl) [![thumbnail](../Images/3aa727b1aa5caba1090533892ec9e378.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DPOå¯¹Llama 2è¿›è¡Œå¾®è°ƒ](https://huggingface.co/blog/dpo-trl) [![ç¼©ç•¥å›¾](../Images/3aa727b1aa5caba1090533892ec9e378.png)
- en: Finetune Stable Diffusion Models with DDPO via TRL](https://huggingface.co/blog/trl-ddpo)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TRLé€šè¿‡DDPOå¾®è°ƒç¨³å®šæ‰©æ•£æ¨¡å‹](https://huggingface.co/blog/trl-ddpo)
