- en: Metal Performance Shaders (MPS)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡‘å±æ€§èƒ½ç€è‰²å™¨ï¼ˆMPSï¼‰
- en: 'Original text: [https://huggingface.co/docs/diffusers/optimization/mps](https://huggingface.co/docs/diffusers/optimization/mps)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/optimization/mps](https://huggingface.co/docs/diffusers/optimization/mps)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'ğŸ¤— Diffusers is compatible with Apple silicon (M1/M2 chips) using the PyTorch
    [`mps`](https://pytorch.org/docs/stable/notes/mps.html) device, which uses the
    Metal framework to leverage the GPU on MacOS devices. Youâ€™ll need to have:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Diffuserså…¼å®¹ä½¿ç”¨PyTorch [`mps`](https://pytorch.org/docs/stable/notes/mps.html)è®¾å¤‡çš„è‹¹æœç¡…ï¼ˆM1/M2èŠ¯ç‰‡ï¼‰ï¼Œè¯¥è®¾å¤‡ä½¿ç”¨Metalæ¡†æ¶æ¥åˆ©ç”¨MacOSè®¾å¤‡ä¸Šçš„GPUã€‚æ‚¨éœ€è¦ï¼š
- en: macOS computer with Apple silicon (M1/M2) hardware
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…·æœ‰è‹¹æœç¡…ï¼ˆM1/M2ï¼‰ç¡¬ä»¶çš„macOSè®¡ç®—æœº
- en: macOS 12.6 or later (13.0 or later recommended)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: macOS 12.6æˆ–æ›´é«˜ç‰ˆæœ¬ï¼ˆå»ºè®®ä½¿ç”¨13.0æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰
- en: arm64 version of Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pythonçš„arm64ç‰ˆæœ¬
- en: '[PyTorch 2.0](https://pytorch.org/get-started/locally/) (recommended) or 1.13
    (minimum version supported for `mps`)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 2.0](https://pytorch.org/get-started/locally/)ï¼ˆå»ºè®®ï¼‰æˆ–1.13ï¼ˆ`mps`æ”¯æŒçš„æœ€ä½ç‰ˆæœ¬ï¼‰'
- en: 'The `mps` backend uses PyTorchâ€™s `.to()` interface to move the Stable Diffusion
    pipeline on to your M1 or M2 device:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`mps`åç«¯ä½¿ç”¨PyTorchçš„`.to()`æ¥å£å°†ç¨³å®šæ‰©æ•£ç®¡é“ç§»åŠ¨åˆ°æ‚¨çš„M1æˆ–M2è®¾å¤‡ä¸Šï¼š'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generating multiple prompts in a batch can [crash](https://github.com/huggingface/diffusers/issues/363)
    or fail to work reliably. We believe this is related to the [`mps`](https://github.com/pytorch/pytorch/issues/84039)
    backend in PyTorch. While this is being investigated, you should iterate instead
    of batching.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰¹å¤„ç†ä¸­ç”Ÿæˆå¤šä¸ªæç¤ºå¯èƒ½ä¼š[å´©æºƒ](https://github.com/huggingface/diffusers/issues/363)æˆ–æ— æ³•å¯é å·¥ä½œã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸PyTorchä¸­çš„[`mps`](https://github.com/pytorch/pytorch/issues/84039)åç«¯æœ‰å…³ã€‚åœ¨è°ƒæŸ¥æ­¤é—®é¢˜æ—¶ï¼Œæ‚¨åº”è¯¥è¿­ä»£è€Œä¸æ˜¯æ‰¹å¤„ç†ã€‚
- en: If youâ€™re using **PyTorch 1.13**, you need to â€œprimeâ€ the pipeline with an additional
    one-time pass through it. This is a temporary workaround for an issue where the
    first inference pass produces slightly different results than subsequent ones.
    You only need to do this pass once, and after just one inference step you can
    discard the result.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨**PyTorch 1.13**ï¼Œæ‚¨éœ€è¦é€šè¿‡é¢å¤–çš„ä¸€æ¬¡é€šè¿‡â€œprimeâ€ç®¡é“ã€‚è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶è§£å†³æ–¹æ³•ï¼Œç”¨äºè§£å†³ç¬¬ä¸€æ¬¡æ¨ç†é€šè¿‡äº§ç”Ÿä¸åç»­æ¨ç†é€šè¿‡ç•¥æœ‰ä¸åŒç»“æœçš„é—®é¢˜ã€‚æ‚¨åªéœ€è¦æ‰§è¡Œæ­¤æ­¥éª¤ä¸€æ¬¡ï¼Œä»…ç»è¿‡ä¸€æ¬¡æ¨ç†æ­¥éª¤åï¼Œæ‚¨å¯ä»¥ä¸¢å¼ƒç»“æœã€‚
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Troubleshoot
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•…éšœæ’é™¤
- en: M1/M2 performance is very sensitive to memory pressure. When this occurs, the
    system automatically swaps if it needs to which significantly degrades performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: M1/M2çš„æ€§èƒ½å¯¹å†…å­˜å‹åŠ›éå¸¸æ•æ„Ÿã€‚å½“å‘ç”Ÿè¿™ç§æƒ…å†µæ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¿›è¡Œäº¤æ¢ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼Œè¿™ä¼šæ˜¾è‘—é™ä½æ€§èƒ½ã€‚
- en: 'To prevent this from happening, we recommend *attention slicing* to reduce
    memory pressure during inference and prevent swapping. This is especially relevant
    if your computer has less than 64GB of system RAM, or if you generate images at
    non-standard resolutions larger than 512Ã—512 pixels. Call the [enable_attention_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_attention_slicing)
    function on your pipeline:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºé˜²æ­¢è¿™ç§æƒ…å†µå‘ç”Ÿï¼Œæˆ‘ä»¬å»ºè®®*æ³¨æ„åŠ›åˆ‡ç‰‡*ä»¥å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜å‹åŠ›å¹¶é˜²æ­¢äº¤æ¢ã€‚å¦‚æœæ‚¨çš„è®¡ç®—æœºç³»ç»ŸRAMå°‘äº64GBï¼Œæˆ–è€…ç”Ÿæˆçš„å›¾åƒåˆ†è¾¨ç‡å¤§äº512Ã—512åƒç´ çš„éæ ‡å‡†åˆ†è¾¨ç‡ï¼Œåˆ™è¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚åœ¨æ‚¨çš„ç®¡é“ä¸Šè°ƒç”¨[enable_attention_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_attention_slicing)å‡½æ•°ï¼š
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Attention slicing performs the costly attention operation in multiple steps
    instead of all at once. It usually improves performance by ~20% in computers without
    universal memory, but weâ€™ve observed *better performance* in most Apple silicon
    computers unless you have 64GB of RAM or more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›åˆ‡ç‰‡å°†æ˜‚è´µçš„æ³¨æ„åŠ›æ“ä½œåˆ†ä¸ºå¤šä¸ªæ­¥éª¤ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§å®Œæˆã€‚åœ¨æ²¡æœ‰é€šç”¨å†…å­˜çš„è®¡ç®—æœºä¸Šï¼Œè¿™é€šå¸¸å¯ä»¥æé«˜çº¦20%çš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨å¤§å¤šæ•°è‹¹æœç¡…è®¡ç®—æœºä¸Šæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œé™¤éæ‚¨æœ‰64GBæˆ–æ›´å¤šçš„RAMã€‚
