- en: Quicktour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速导览
- en: 'Original text: [https://huggingface.co/docs/peft/quicktour](https://huggingface.co/docs/peft/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/quicktour](https://huggingface.co/docs/peft/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: PEFT offers parameter-efficient methods for finetuning large pretrained models.
    The traditional paradigm is to finetune all of a model’s parameters for each downstream
    task, but this is becoming exceedingly costly and impractical because of the enormous
    number of parameters in models today. Instead, it is more efficient to train a
    smaller number of prompt parameters or use a reparametrization method like low-rank
    adaptation (LoRA) to reduce the number of trainable parameters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT提供了用于微调大型预训练模型的参数高效方法。传统范式是为每个下游任务微调模型的所有参数，但由于当今模型中参数数量巨大，这种方法变得极其昂贵和不切实际。相反，更有效的方法是训练较少数量的提示参数或使用低秩适应（LoRA）等重新参数化方法来减少可训练参数的数量。
- en: This quicktour will show you PEFT’s main features and how you can train or run
    inference on large models that would typically be inaccessible on consumer devices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这个快速导览将展示PEFT的主要特点，以及您如何训练或在通常无法在消费者设备上访问的大型模型上运行推理。
- en: Train
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Each PEFT method is defined by a [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
    class that stores all the important parameters for building a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
    For example, to train with LoRA, load and create a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    class and specify the following parameters:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 每个PEFT方法都由一个[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)类定义，该类存储构建[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)所需的所有重要参数。例如，要使用LoRA进行训练，请加载并创建一个[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)类，并指定以下参数：
- en: '`task_type`: the task to train for (sequence-to-sequence language modeling
    in this case)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_type`：要进行训练的任务（在本例中为序列到序列语言建模）'
- en: '`inference_mode`: whether you’re using the model for inference or not'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inference_mode`：您是否在使用模型进行推理'
- en: '`r`: the dimension of the low-rank matrices'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r`：低秩矩阵的维度'
- en: '`lora_alpha`: the scaling factor for the low-rank matrices'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_alpha`：低秩矩阵的缩放因子'
- en: '`lora_dropout`: the dropout probability of the LoRA layers'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_dropout`：LoRA层的dropout概率'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: See the [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    reference for more details about other parameters you can adjust, such as the
    modules to target or the bias type.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其他可调整的参数的更多详细信息，请参阅[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)参考。
- en: Once the [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    is setup, create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    with the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function. It takes a base model - which you can load from the Transformers library
    - and the [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    containing the parameters for how to configure a model for training with LoRA.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)设置好，使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。它需要一个基础模型
    - 您可以从Transformers库中加载 - 和包含如何配置模型以使用LoRA进行训练的[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)。
- en: Load the base model you want to finetune.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 加载您想要微调的基础模型。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Wrap the base model and `peft_config` with the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
    To get a sense of the number of trainable parameters in your model, use the `print_trainable_parameters`
    method.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数将基础模型和`peft_config`封装起来，创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。使用`print_trainable_parameters`方法来了解模型中可训练参数的数量。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Out of [bigscience/mt0-large’s](https://huggingface.co/bigscience/mt0-large)
    1.2B parameters, you’re only training 0.19% of them!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)的12亿参数中，您只训练了其中的0.19%！
- en: That is it 🎉! Now you can train the model with the Transformers [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    Accelerate, or any custom PyTorch training loop.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样🎉！现在您可以使用Transformers的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)、Accelerate或任何自定义的PyTorch训练循环来训练模型。
- en: For example, to train with the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class, setup a [TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    class with some training hyperparameters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要使用[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类进行训练，请设置一个带有一些训练超参数的[TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)类。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Pass the model, training arguments, dataset, tokenizer, and any other necessary
    component to the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    and call [train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to start training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型、训练参数、数据集、分词器和其他必要组件传递给[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，并调用[train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)开始训练。
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Save model
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存模型
- en: After your model is finished training, you can save your model to a directory
    using the [save_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练完成后，您可以使用[save_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)函数将模型保存到目录中。
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can also save your model to the Hub (make sure you’re logged in to your
    Hugging Face account first) with the [push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
    function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)函数将模型保存到Hub（请确保您首先登录到您的Hugging
    Face帐户）。
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Both methods only save the extra PEFT weights that were trained, meaning it
    is super efficient to store, transfer, and load. For example, this [facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora)
    model trained with LoRA only contains two files: `adapter_config.json` and `adapter_model.safetensors`.
    The `adapter_model.safetensors` file is just 6.3MB!'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法只保存经过训练的额外PEFT权重，这意味着存储、传输和加载非常高效。例如，使用LoRA训练的这个[facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora)模型只包含两个文件：`adapter_config.json`和`adapter_model.safetensors`。`adapter_model.safetensors`文件只有6.3MB！
- en: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
- en: The adapter weights for a opt-350m model stored on the Hub are only ~6MB compared
    to the full size of the model weights, which can be ~700MB.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型权重的完整大小相比，存储在Hub上的opt-350m模型的适配器权重仅为约6MB，后者可能为约700MB。
- en: Inference
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: Take a look at the [AutoPeftModel](package_reference/auto_class) API reference
    for a complete list of available `AutoPeftModel` classes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[AutoPeftModel](package_reference/auto_class) API参考，了解可用的`AutoPeftModel`类的完整列表。
- en: 'Easily load any PEFT-trained model for inference with the [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    class and the [from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类和[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法轻松加载任何经过PEFT训练的模型进行推理：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For other tasks that aren’t explicitly supported with an `AutoPeftModelFor`
    class - such as automatic speech recognition - you can still use the base [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    class to load a model for the task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有明确支持的任务，如自动语音识别等，您仍然可以使用基本的[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类加载用于该任务的模型。
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next steps
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'Now that you’ve seen how to train a model with one of the PEFT methods, we
    encourage you to try out some of the other methods like prompt tuning. The steps
    are very similar to the ones shown in the quicktour:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到如何使用PEFT方法之一训练模型，我们鼓励您尝试一些其他方法，如提示调整。这些步骤与快速入门中显示的步骤非常相似：
- en: prepare a [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
    for a PEFT method
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为PEFT方法准备一个[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
- en: use the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    method to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    from the configuration and base model
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)方法从配置和基本模型创建[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
- en: Then you can train it however you like! To load a PEFT model for inference,
    you can use the [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    class.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以按照自己的喜好进行训练！要加载用于推理的PEFT模型，您可以使用[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类。
- en: Feel free to also take a look at the task guides if you’re interested in training
    a model with another PEFT method for a specific task such as semantic segmentation,
    multilingual automatic speech recognition, DreamBooth, token classification, and
    more.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣使用其他PEFT方法训练模型，例如语义分割、多语言自动语音识别、DreamBooth、标记分类等，请随时查看任务指南。
