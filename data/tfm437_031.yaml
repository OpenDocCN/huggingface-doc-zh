- en: Image Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分割
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/dKE8SIt9C-w](https://www.youtube-nocookie.com/embed/dKE8SIt9C-w)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/dKE8SIt9C-w](https://www.youtube-nocookie.com/embed/dKE8SIt9C-w)'
- en: 'Image segmentation models separate areas corresponding to different areas of
    interest in an image. These models work by assigning a label to each pixel. There
    are several types of segmentation: semantic segmentation, instance segmentation,
    and panoptic segmentation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割模型将图像中对应不同感兴趣区域的区域分开。这些模型通过为每个像素分配一个标签来工作。有几种类型的分割：语义分割、实例分割和全景分割。
- en: 'In this guide, we will:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我们将：
- en: '[Take a look at different types of segmentation](#types-of-segmentation).'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[查看不同类型的分割](#types-of-segmentation)。'
- en: '[Have an end-to-end fine-tuning example for semantic segmentation](#fine-tuning-a-model-for-segmentation).'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[有一个用于语义分割的端到端微调示例](#fine-tuning-a-model-for-segmentation)。'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保已安装所有必要的库：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to log in to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to log in:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您登录您的Hugging Face帐户，这样您就可以上传和与社区分享您的模型。在提示时，输入您的令牌以登录：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Types of Segmentation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割类型
- en: Semantic segmentation assigns a label or class to every single pixel in an image.
    Let’s take a look at a semantic segmentation model output. It will assign the
    same class to every instance of an object it comes across in an image, for example,
    all cats will be labeled as “cat” instead of “cat-1”, “cat-2”. We can use transformers’
    image segmentation pipeline to quickly infer a semantic segmentation model. Let’s
    take a look at the example image.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割为图像中的每个像素分配一个标签或类。让我们看一下语义分割模型的输出。它将为图像中遇到的每个对象实例分配相同的类，例如，所有猫都将被标记为“cat”而不是“cat-1”、“cat-2”。我们可以使用transformers的图像分割管道快速推断一个语义分割模型。让我们看一下示例图像。
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Segmentation Input](../Images/64569586219c10ac0e1d0e7dcb95de24.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![分割输入](../Images/64569586219c10ac0e1d0e7dcb95de24.png)'
- en: We will use [nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024)。
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The segmentation pipeline output includes a mask for every predicted class.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分割管道输出包括每个预测类的掩码。
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Taking a look at the mask for the car class, we can see every car is classified
    with the same mask.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 查看汽车类的掩码，我们可以看到每辆汽车都被分类为相同的掩码。
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Semantic Segmentation Output](../Images/b6bfa966a8f6ae7291a1b6e1dd4d6e5a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![语义分割输出](../Images/b6bfa966a8f6ae7291a1b6e1dd4d6e5a.png)'
- en: In instance segmentation, the goal is not to classify every pixel, but to predict
    a mask for **every instance of an object** in a given image. It works very similar
    to object detection, where there is a bounding box for every instance, there’s
    a segmentation mask instead. We will use [facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance)
    for this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例分割中，目标不是对每个像素进行分类，而是为给定图像中的**每个对象实例**预测一个掩码。它的工作方式与目标检测非常相似，其中每个实例都有一个边界框，而这里有一个分割掩码。我们将使用[facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance)。
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see below, there are multiple cars classified, and there’s no classification
    for pixels other than pixels that belong to car and person instances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，有多辆汽车被分类，除了属于汽车和人实例的像素之外，没有对其他像素进行分类。
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Checking out one of the car masks below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 查看下面的一辆汽车掩码。
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Semantic Segmentation Output](../Images/b99b68749dae50fc56726918431cadc3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![语义分割输出](../Images/b99b68749dae50fc56726918431cadc3.png)'
- en: Panoptic segmentation combines semantic segmentation and instance segmentation,
    where every pixel is classified into a class and an instance of that class, and
    there are multiple masks for each instance of a class. We can use [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic)
    for this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 全景分割结合了语义分割和实例分割，其中每个像素被分类为一个类和该类的一个实例，并且每个类的每个实例有多个掩码。我们可以使用[facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic)。
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see below, we have more classes. We will later illustrate to see
    that every pixel is classified into one of the classes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，我们有更多的类。稍后我们将说明，每个像素都被分类为其中的一个类。
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s have a side by side comparison for all types of segmentation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对所有类型的分割进行一次并排比较。
- en: '![Segmentation Maps Compared](../Images/3c09fa36de09454c16c19c1b23a22865.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![分割地图比较](../Images/3c09fa36de09454c16c19c1b23a22865.png)'
- en: Seeing all types of segmentation, let’s have a deep dive on fine-tuning a model
    for semantic segmentation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 看到所有类型的分割，让我们深入研究为语义分割微调模型。
- en: Common real-world applications of semantic segmentation include training self-driving
    cars to identify pedestrians and important traffic information, identifying cells
    and abnormalities in medical imagery, and monitoring environmental changes from
    satellite imagery.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割的常见实际应用包括训练自动驾驶汽车识别行人和重要的交通信息，识别医学图像中的细胞和异常，以及监测卫星图像中的环境变化。
- en: Fine-tuning a Model for Segmentation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为分割微调模型
- en: 'We will now:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将：
- en: Finetune [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)
    on the [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[SceneParse150](https://huggingface.co/datasets/scene_parse_150)数据集上对[SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)进行微调。
- en: Use your fine-tuned model for inference.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您微调的模型进行推断。
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中演示的任务由以下模型架构支持：
- en: '[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision),
    [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit),
    [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer),
    [UPerNet](../model_doc/upernet)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision),
    [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit),
    [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer),
    [UPerNet](../model_doc/upernet)'
- en: Load SceneParse150 dataset
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载 SceneParse150 数据集
- en: Start by loading a smaller subset of the SceneParse150 dataset from the 🤗 Datasets
    library. This’ll give you a chance to experiment and make sure everything works
    before spending more time training on the full dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从 🤗 数据集库中加载 SceneParse150 数据集的一个较小子集。这将让您有机会进行实验，并确保一切正常，然后再花更多时间在完整数据集上进行训练。
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Split the dataset’s `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    方法将数据集的 `train` 分割为训练集和测试集：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then take a look at an example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后看一个例子：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`image`: a PIL image of the scene.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`：场景的PIL图像。'
- en: '`annotation`: a PIL image of the segmentation map, which is also the model’s
    target.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`annotation`：分割地图的 PIL 图像，也是模型的目标。'
- en: '`scene_category`: a category id that describes the image scene like “kitchen”
    or “office”. In this guide, you’ll only need `image` and `annotation`, both of
    which are PIL images.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scene_category`：描述图像场景的类别 id，如“厨房”或“办公室”。在本指南中，您只需要 `image` 和 `annotation`，两者都是
    PIL 图像。'
- en: 'You’ll also want to create a dictionary that maps a label id to a label class
    which will be useful when you set up the model later. Download the mappings from
    the Hub and create the `id2label` and `label2id` dictionaries:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要创建一个将标签 id 映射到标签类的字典，这在稍后设置模型时会很有用。从 Hub 下载映射并创建 `id2label` 和 `label2id`
    字典：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Custom dataset
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自定义数据集
- en: 'You could also create and use your own dataset if you prefer to train with
    the [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py)
    script instead of a notebook instance. The script requires:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更喜欢使用 [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py)
    脚本而不是笔记本实例进行训练，您也可以创建并使用自己的数据集。该脚本需要：
- en: a [DatasetDict](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.DatasetDict)
    with two [Image](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Image)
    columns, “image” and “label”
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个包含两个 [Image](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Image)
    列“image”和“label”的 [DatasetDict](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.DatasetDict)。
- en: '[PRE15]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: an id2label dictionary mapping the class integers to their class names
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个 id2label 字典，将类整数映射到它们的类名
- en: '[PRE16]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As an example, take a look at this [example dataset](https://huggingface.co/datasets/nielsr/ade20k-demo)
    which was created with the steps shown above.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查看这个[示例数据集](https://huggingface.co/datasets/nielsr/ade20k-demo)，该数据集是使用上述步骤创建的。
- en: Preprocess
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: 'The next step is to load a SegFormer image processor to prepare the images
    and annotations for the model. Some datasets, like this one, use the zero-index
    as the background class. However, the background class isn’t actually included
    in the 150 classes, so you’ll need to set `reduce_labels=True` to subtract one
    from all the labels. The zero-index is replaced by `255` so it’s ignored by SegFormer’s
    loss function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '下一步是加载一个 SegFormer 图像处理器，准备图像和注释以供模型使用。某些数据集，如此类数据集，使用零索引作为背景类。但是，背景类实际上不包括在
    150 个类中，因此您需要设置 `reduce_labels=True`，从所有标签中减去一个。零索引被替换为 `255`，因此 SegFormer 的损失函数会忽略它： '
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: PytorchHide Pytorch content
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch 内容
- en: It is common to apply some data augmentations to an image dataset to make a
    model more robust against overfitting. In this guide, you’ll use the [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)
    function from [torchvision](https://pytorch.org/vision/stable/index.html) to randomly
    change the color properties of an image, but you can also use any image library
    you like.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会对图像数据集应用一些数据增强，以使模型更具抗过拟合能力。在本指南中，您将使用 [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)
    函数从 [torchvision](https://pytorch.org/vision/stable/index.html) 随机更改图像的颜色属性，但您也可以使用任何您喜欢的图像库。
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now create two preprocessing functions to prepare the images and annotations
    for the model. These functions convert the images into `pixel_values` and annotations
    to `labels`. For the training set, `jitter` is applied before providing the images
    to the image processor. For the test set, the image processor crops and normalizes
    the `images`, and only crops the `labels` because no data augmentation is applied
    during testing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建两个预处理函数，准备图像和注释以供模型使用。这些函数将图像转换为 `pixel_values`，将注释转换为 `labels`。对于训练集，在将图像提供给图像处理器之前应用
    `jitter`。对于测试集，图像处理器裁剪和规范化 `images`，仅裁剪 `labels`，因为在测试期间不应用数据增强。
- en: '[PRE19]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To apply the `jitter` over the entire dataset, use the 🤗 Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    function. The transform is applied on the fly which is faster and consumes less
    disk space:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要在整个数据集上应用 `jitter`，请使用 🤗 数据集 [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    函数。变换是实时应用的，速度更快，占用的磁盘空间更少：
- en: '[PRE20]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TensorFlowHide TensorFlow content
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow 内容
- en: 'It is common to apply some data augmentations to an image dataset to make a
    model more robust against overfitting. In this guide, you’ll use [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)
    to randomly change the color properties of an image, but you can also use any
    image library you like. Define two separate transformation functions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像数据集应用一些数据增强是常见的，可以使模型更具抗过拟合能力。在本指南中，您将使用[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)来随机更改图像的颜色属性，但您也可以使用任何您喜欢的图像库。定义两个单独的转换函数：
- en: training data transformations that include image augmentation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括图像增强的训练数据转换
- en: validation data transformations that only transpose the images, since computer
    vision models in 🤗 Transformers expect channels-first layout
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据转换仅转置图像，因为🤗 Transformers中的计算机视觉模型期望通道优先布局
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Next, create two preprocessing functions to prepare batches of images and annotations
    for the model. These functions apply the image transformations and use the earlier
    loaded `image_processor` to convert the images into `pixel_values` and annotations
    to `labels`. `ImageProcessor` also takes care of resizing and normalizing the
    images.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建两个预处理函数，用于为模型准备图像和注释的批处理。这些函数应用图像转换，并使用之前加载的`image_processor`将图像转换为`pixel_values`，将注释转换为`labels`。`ImageProcessor`还负责调整大小和规范化图像。
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To apply the preprocessing transformations over the entire dataset, use the
    🤗 Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    function. The transform is applied on the fly which is faster and consumes less
    disk space:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要在整个数据集上应用预处理转换，使用🤗 Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)函数。转换是实时应用的，速度更快，占用的磁盘空间更少：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Evaluate
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: 'Including a metric during training is often helpful for evaluating your model’s
    performance. You can quickly load an evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy)
    (IoU) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中包含一个度量标准通常有助于评估模型的性能。您可以使用🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)库快速加载一个评估方法。对于这个任务，加载[mean
    Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy)
    (IoU)度量标准（查看🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算度量标准）：
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then create a function to `compute` the metrics. Your predictions need to be
    converted to logits first, and then reshaped to match the size of the labels before
    you can call `compute`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建一个函数来`compute`度量标准。您的预测需要首先转换为logits，然后重新调整形状以匹配标签的大小，然后才能调用`compute`：
- en: PytorchHide Pytorch content
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: TensorFlowHide TensorFlow content
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: '[PRE26]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Your `compute_metrics` function is ready to go now, and you’ll return to it
    when you setup your training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您的`compute_metrics`函数现在已经准备就绪，当您设置训练时会再次用到它。
- en: Train
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: PytorchHide Pytorch content
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: If you aren’t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#finetune-with-trainer)!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉如何使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)对模型进行微调，请查看这里的基本教程[../training#finetune-with-trainer]！
- en: 'You’re ready to start training your model now! Load SegFormer with [AutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation),
    and pass the model the mapping between label ids and label classes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经准备好开始训练您的模型了！使用[AutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation)加载SegFormer，并将模型传递给标签id和标签类之间的映射：
- en: '[PRE27]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'At this point, only three steps remain:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 目前只剩下三个步骤：
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    It is important you don’t remove unused columns because this’ll drop the `image`
    column. Without the `image` column, you can’t create `pixel_values`. Set `remove_unused_columns=False`
    to prevent this behavior! The only other required parameter is `output_dir` which
    specifies where to save your model. You’ll push this model to the Hub by setting
    `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).
    At the end of each epoch, the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the IoU metric and save the training checkpoint.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中定义您的训练超参数。重要的是不要删除未使用的列，因为这会删除`image`列。没有`image`列，您就无法创建`pixel_values`。设置`remove_unused_columns=False`以防止这种行为！另一个必需的参数是`output_dir`，指定保存模型的位置。通过设置`push_to_hub=True`将此模型推送到Hub（您需要登录Hugging
    Face才能上传您的模型）。在每个epoch结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将评估IoU度量标准并保存训练检查点。
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练参数传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，同时还需要传递模型、数据集、分词器、数据整理器和`compute_metrics`函数。
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)来微调您的模型。
- en: '[PRE28]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将您的模型共享到Hub，这样每个人都可以使用您的模型：
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: TensorFlowHide TensorFlow content
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: If you are unfamiliar with fine-tuning a model with Keras, check out the [basic
    tutorial](./training#train-a-tensorflow-model-with-keras) first!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉使用Keras进行模型微调，请先查看[基本教程](./training#train-a-tensorflow-model-with-keras)！
- en: 'To fine-tune a model in TensorFlow, follow these steps:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要在TensorFlow中微调模型，请按照以下步骤进行：
- en: Define the training hyperparameters, and set up an optimizer and a learning
    rate schedule.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练超参数，并设置优化器和学习率调度。
- en: Instantiate a pretrained model.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个预训练模型。
- en: Convert a 🤗 Dataset to a `tf.data.Dataset`.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个🤗数据集转换为`tf.data.Dataset`。
- en: Compile your model.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译您的模型。
- en: Add callbacks to calculate metrics and upload your model to 🤗 Hub
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加回调以计算指标并将您的模型上传到🤗 Hub
- en: Use the `fit()` method to run the training.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法运行训练。
- en: 'Start by defining the hyperparameters, optimizer and learning rate schedule:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义超参数、优化器和学习率调度：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, load SegFormer with [TFAutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation)
    along with the label mappings, and compile it with the optimizer. Note that Transformers
    models all have a default task-relevant loss function, so you don’t need to specify
    one unless you want to:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用[TFAutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation)加载SegFormer以及标签映射，并使用优化器对其进行编译。请注意，Transformers模型都有一个默认的与任务相关的损失函数，因此除非您想要指定一个，否则不需要指定：
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Convert your datasets to the `tf.data.Dataset` format using the [to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
    and the [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)和[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)将您的数据集转换为`tf.data.Dataset`格式：
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To compute the accuracy from the predictions and push your model to the 🤗 Hub,
    use [Keras callbacks](../main_classes/keras_callbacks). Pass your `compute_metrics`
    function to [KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback),
    and use the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)
    to upload the model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要从预测中计算准确率并将您的模型推送到🤗 Hub，请使用[Keras回调](../main_classes/keras_callbacks)。将您的`compute_metrics`函数传递给[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)，并使用[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)来上传模型：
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, you are ready to train your model! Call `fit()` with your training
    and validation datasets, the number of epochs, and your callbacks to fine-tune
    the model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您已经准备好训练您的模型了！使用您的训练和验证数据集、时代数量和回调来调用`fit()`来微调模型：
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Congratulations! You have fine-tuned your model and shared it on the 🤗 Hub.
    You can now use it for inference!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经对模型进行了微调并在🤗 Hub上分享了它。现在您可以用它进行推理！
- en: Inference
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理
- en: Great, now that you’ve finetuned a model, you can use it for inference!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在您已经对模型进行了微调，可以用它进行推理！
- en: 'Load an image for inference:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一张图片进行推理：
- en: '[PRE35]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Image of bedroom](../Images/1f1abe71d12160da7bd59d35ef05323c.png)PytorchHide
    Pytorch content'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![卧室图像](../Images/1f1abe71d12160da7bd59d35ef05323c.png)Pytorch隐藏 Pytorch内容'
- en: 'We will now see how to infer without a pipeline. Process the image with an
    image processor and place the `pixel_values` on a GPU:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到如何在没有管道的情况下进行推理。使用图像处理器处理图像，并将`pixel_values`放在GPU上：
- en: '[PRE36]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Pass your input to the model and return the `logits`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入传递给模型并返回`logits`：
- en: '[PRE37]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, rescale the logits to the original image size:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将logits重新缩放到原始图像大小：
- en: '[PRE38]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: TensorFlowHide TensorFlow content
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow内容
- en: 'Load an image processor to preprocess the image and return the input as TensorFlow
    tensors:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一个图像处理器来预处理图像并将输入返回为TensorFlow张量：
- en: '[PRE39]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Pass your input to the model and return the `logits`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入传递给模型并返回`logits`：
- en: '[PRE40]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, rescale the logits to the original image size and apply argmax on the
    class dimension:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将logits重新缩放到原始图像大小，并在类维度上应用argmax：
- en: '[PRE41]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To visualize the results, load the [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)
    as `ade_palette()` that maps each class to their RGB values. Then you can combine
    and plot your image and the predicted segmentation map:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化结果，加载[数据集颜色调色板](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)作为`ade_palette()`，将每个类映射到它们的RGB值。然后您可以组合并绘制您的图像和预测的分割地图：
- en: '[PRE42]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Image of bedroom overlaid with segmentation map](../Images/2bc12ae66cec2ea434945aaaa1e5c6bf.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![覆盖有分割地图的卧室图像](../Images/2bc12ae66cec2ea434945aaaa1e5c6bf.png)'
