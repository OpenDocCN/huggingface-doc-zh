- en: Image Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†å‰²
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/dKE8SIt9C-w](https://www.youtube-nocookie.com/embed/dKE8SIt9C-w)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/dKE8SIt9C-w](https://www.youtube-nocookie.com/embed/dKE8SIt9C-w)'
- en: 'Image segmentation models separate areas corresponding to different areas of
    interest in an image. These models work by assigning a label to each pixel. There
    are several types of segmentation: semantic segmentation, instance segmentation,
    and panoptic segmentation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†å‰²æ¨¡å‹å°†å›¾åƒä¸­å¯¹åº”ä¸åŒæ„Ÿå…´è¶£åŒºåŸŸçš„åŒºåŸŸåˆ†å¼€ã€‚è¿™äº›æ¨¡å‹é€šè¿‡ä¸ºæ¯ä¸ªåƒç´ åˆ†é…ä¸€ä¸ªæ ‡ç­¾æ¥å·¥ä½œã€‚æœ‰å‡ ç§ç±»å‹çš„åˆ†å‰²ï¼šè¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ã€‚
- en: 'In this guide, we will:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- en: '[Take a look at different types of segmentation](#types-of-segmentation).'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹ä¸åŒç±»å‹çš„åˆ†å‰²](#types-of-segmentation)ã€‚'
- en: '[Have an end-to-end fine-tuning example for semantic segmentation](#fine-tuning-a-model-for-segmentation).'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æœ‰ä¸€ä¸ªç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç«¯åˆ°ç«¯å¾®è°ƒç¤ºä¾‹](#fine-tuning-a-model-for-segmentation)ã€‚'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to log in to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to log in:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Types of Segmentation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†å‰²ç±»å‹
- en: Semantic segmentation assigns a label or class to every single pixel in an image.
    Letâ€™s take a look at a semantic segmentation model output. It will assign the
    same class to every instance of an object it comes across in an image, for example,
    all cats will be labeled as â€œcatâ€ instead of â€œcat-1â€, â€œcat-2â€. We can use transformersâ€™
    image segmentation pipeline to quickly infer a semantic segmentation model. Letâ€™s
    take a look at the example image.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åˆ†å‰²ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ä¸€ä¸ªæ ‡ç­¾æˆ–ç±»ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„è¾“å‡ºã€‚å®ƒå°†ä¸ºå›¾åƒä¸­é‡åˆ°çš„æ¯ä¸ªå¯¹è±¡å®ä¾‹åˆ†é…ç›¸åŒçš„ç±»ï¼Œä¾‹å¦‚ï¼Œæ‰€æœ‰çŒ«éƒ½å°†è¢«æ ‡è®°ä¸ºâ€œcatâ€è€Œä¸æ˜¯â€œcat-1â€ã€â€œcat-2â€ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨transformersçš„å›¾åƒåˆ†å‰²ç®¡é“å¿«é€Ÿæ¨æ–­ä¸€ä¸ªè¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹å›¾åƒã€‚
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Segmentation Input](../Images/64569586219c10ac0e1d0e7dcb95de24.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ†å‰²è¾“å…¥](../Images/64569586219c10ac0e1d0e7dcb95de24.png)'
- en: We will use [nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨[nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024)ã€‚
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The segmentation pipeline output includes a mask for every predicted class.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å‰²ç®¡é“è¾“å‡ºåŒ…æ‹¬æ¯ä¸ªé¢„æµ‹ç±»çš„æ©ç ã€‚
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Taking a look at the mask for the car class, we can see every car is classified
    with the same mask.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æ±½è½¦ç±»çš„æ©ç ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯è¾†æ±½è½¦éƒ½è¢«åˆ†ç±»ä¸ºç›¸åŒçš„æ©ç ã€‚
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Semantic Segmentation Output](../Images/b6bfa966a8f6ae7291a1b6e1dd4d6e5a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![è¯­ä¹‰åˆ†å‰²è¾“å‡º](../Images/b6bfa966a8f6ae7291a1b6e1dd4d6e5a.png)'
- en: In instance segmentation, the goal is not to classify every pixel, but to predict
    a mask for **every instance of an object** in a given image. It works very similar
    to object detection, where there is a bounding box for every instance, thereâ€™s
    a segmentation mask instead. We will use [facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance)
    for this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®ä¾‹åˆ†å‰²ä¸­ï¼Œç›®æ ‡ä¸æ˜¯å¯¹æ¯ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»ï¼Œè€Œæ˜¯ä¸ºç»™å®šå›¾åƒä¸­çš„**æ¯ä¸ªå¯¹è±¡å®ä¾‹**é¢„æµ‹ä¸€ä¸ªæ©ç ã€‚å®ƒçš„å·¥ä½œæ–¹å¼ä¸ç›®æ ‡æ£€æµ‹éå¸¸ç›¸ä¼¼ï¼Œå…¶ä¸­æ¯ä¸ªå®ä¾‹éƒ½æœ‰ä¸€ä¸ªè¾¹ç•Œæ¡†ï¼Œè€Œè¿™é‡Œæœ‰ä¸€ä¸ªåˆ†å‰²æ©ç ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance)ã€‚
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see below, there are multiple cars classified, and thereâ€™s no classification
    for pixels other than pixels that belong to car and person instances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹æ‰€ç¤ºï¼Œæœ‰å¤šè¾†æ±½è½¦è¢«åˆ†ç±»ï¼Œé™¤äº†å±äºæ±½è½¦å’Œäººå®ä¾‹çš„åƒç´ ä¹‹å¤–ï¼Œæ²¡æœ‰å¯¹å…¶ä»–åƒç´ è¿›è¡Œåˆ†ç±»ã€‚
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Checking out one of the car masks below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ä¸‹é¢çš„ä¸€è¾†æ±½è½¦æ©ç ã€‚
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Semantic Segmentation Output](../Images/b99b68749dae50fc56726918431cadc3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![è¯­ä¹‰åˆ†å‰²è¾“å‡º](../Images/b99b68749dae50fc56726918431cadc3.png)'
- en: Panoptic segmentation combines semantic segmentation and instance segmentation,
    where every pixel is classified into a class and an instance of that class, and
    there are multiple masks for each instance of a class. We can use [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic)
    for this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨æ™¯åˆ†å‰²ç»“åˆäº†è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¢«åˆ†ç±»ä¸ºä¸€ä¸ªç±»å’Œè¯¥ç±»çš„ä¸€ä¸ªå®ä¾‹ï¼Œå¹¶ä¸”æ¯ä¸ªç±»çš„æ¯ä¸ªå®ä¾‹æœ‰å¤šä¸ªæ©ç ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic)ã€‚
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see below, we have more classes. We will later illustrate to see
    that every pixel is classified into one of the classes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹æ‰€ç¤ºï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçš„ç±»ã€‚ç¨åæˆ‘ä»¬å°†è¯´æ˜ï¼Œæ¯ä¸ªåƒç´ éƒ½è¢«åˆ†ç±»ä¸ºå…¶ä¸­çš„ä¸€ä¸ªç±»ã€‚
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Letâ€™s have a side by side comparison for all types of segmentation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯¹æ‰€æœ‰ç±»å‹çš„åˆ†å‰²è¿›è¡Œä¸€æ¬¡å¹¶æ’æ¯”è¾ƒã€‚
- en: '![Segmentation Maps Compared](../Images/3c09fa36de09454c16c19c1b23a22865.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ†å‰²åœ°å›¾æ¯”è¾ƒ](../Images/3c09fa36de09454c16c19c1b23a22865.png)'
- en: Seeing all types of segmentation, letâ€™s have a deep dive on fine-tuning a model
    for semantic segmentation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹åˆ°æ‰€æœ‰ç±»å‹çš„åˆ†å‰²ï¼Œè®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä¸ºè¯­ä¹‰åˆ†å‰²å¾®è°ƒæ¨¡å‹ã€‚
- en: Common real-world applications of semantic segmentation include training self-driving
    cars to identify pedestrians and important traffic information, identifying cells
    and abnormalities in medical imagery, and monitoring environmental changes from
    satellite imagery.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åˆ†å‰²çš„å¸¸è§å®é™…åº”ç”¨åŒ…æ‹¬è®­ç»ƒè‡ªåŠ¨é©¾é©¶æ±½è½¦è¯†åˆ«è¡Œäººå’Œé‡è¦çš„äº¤é€šä¿¡æ¯ï¼Œè¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„ç»†èƒå’Œå¼‚å¸¸ï¼Œä»¥åŠç›‘æµ‹å«æ˜Ÿå›¾åƒä¸­çš„ç¯å¢ƒå˜åŒ–ã€‚
- en: Fine-tuning a Model for Segmentation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºåˆ†å‰²å¾®è°ƒæ¨¡å‹
- en: 'We will now:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†ï¼š
- en: Finetune [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)
    on the [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[SceneParse150](https://huggingface.co/datasets/scene_parse_150)æ•°æ®é›†ä¸Šå¯¹[SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)è¿›è¡Œå¾®è°ƒã€‚
- en: Use your fine-tuned model for inference.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­æ¼”ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
- en: '[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision),
    [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit),
    [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer),
    [UPerNet](../model_doc/upernet)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision),
    [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit),
    [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer),
    [UPerNet](../model_doc/upernet)'
- en: Load SceneParse150 dataset
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ è½½ SceneParse150 æ•°æ®é›†
- en: Start by loading a smaller subset of the SceneParse150 dataset from the ğŸ¤— Datasets
    library. Thisâ€™ll give you a chance to experiment and make sure everything works
    before spending more time training on the full dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆä» ğŸ¤— æ•°æ®é›†åº“ä¸­åŠ è½½ SceneParse150 æ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Split the datasetâ€™s `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    æ–¹æ³•å°†æ•°æ®é›†çš„ `train` åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then take a look at an example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åçœ‹ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`image`: a PIL image of the scene.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼šåœºæ™¯çš„PILå›¾åƒã€‚'
- en: '`annotation`: a PIL image of the segmentation map, which is also the modelâ€™s
    target.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`annotation`ï¼šåˆ†å‰²åœ°å›¾çš„ PIL å›¾åƒï¼Œä¹Ÿæ˜¯æ¨¡å‹çš„ç›®æ ‡ã€‚'
- en: '`scene_category`: a category id that describes the image scene like â€œkitchenâ€
    or â€œofficeâ€. In this guide, youâ€™ll only need `image` and `annotation`, both of
    which are PIL images.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scene_category`ï¼šæè¿°å›¾åƒåœºæ™¯çš„ç±»åˆ« idï¼Œå¦‚â€œå¨æˆ¿â€æˆ–â€œåŠå…¬å®¤â€ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨åªéœ€è¦ `image` å’Œ `annotation`ï¼Œä¸¤è€…éƒ½æ˜¯
    PIL å›¾åƒã€‚'
- en: 'Youâ€™ll also want to create a dictionary that maps a label id to a label class
    which will be useful when you set up the model later. Download the mappings from
    the Hub and create the `id2label` and `label2id` dictionaries:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾ id æ˜ å°„åˆ°æ ‡ç­¾ç±»çš„å­—å…¸ï¼Œè¿™åœ¨ç¨åè®¾ç½®æ¨¡å‹æ—¶ä¼šå¾ˆæœ‰ç”¨ã€‚ä» Hub ä¸‹è½½æ˜ å°„å¹¶åˆ›å»º `id2label` å’Œ `label2id`
    å­—å…¸ï¼š
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Custom dataset
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰æ•°æ®é›†
- en: 'You could also create and use your own dataset if you prefer to train with
    the [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py)
    script instead of a notebook instance. The script requires:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ›´å–œæ¬¢ä½¿ç”¨ [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py)
    è„šæœ¬è€Œä¸æ˜¯ç¬”è®°æœ¬å®ä¾‹è¿›è¡Œè®­ç»ƒï¼Œæ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºå¹¶ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ã€‚è¯¥è„šæœ¬éœ€è¦ï¼š
- en: a [DatasetDict](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.DatasetDict)
    with two [Image](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Image)
    columns, â€œimageâ€ and â€œlabelâ€
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«ä¸¤ä¸ª [Image](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Image)
    åˆ—â€œimageâ€å’Œâ€œlabelâ€çš„ [DatasetDict](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.DatasetDict)ã€‚
- en: '[PRE15]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: an id2label dictionary mapping the class integers to their class names
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª id2label å­—å…¸ï¼Œå°†ç±»æ•´æ•°æ˜ å°„åˆ°å®ƒä»¬çš„ç±»å
- en: '[PRE16]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As an example, take a look at this [example dataset](https://huggingface.co/datasets/nielsr/ade20k-demo)
    which was created with the steps shown above.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼ŒæŸ¥çœ‹è¿™ä¸ª[ç¤ºä¾‹æ•°æ®é›†](https://huggingface.co/datasets/nielsr/ade20k-demo)ï¼Œè¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨ä¸Šè¿°æ­¥éª¤åˆ›å»ºçš„ã€‚
- en: Preprocess
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: 'The next step is to load a SegFormer image processor to prepare the images
    and annotations for the model. Some datasets, like this one, use the zero-index
    as the background class. However, the background class isnâ€™t actually included
    in the 150 classes, so youâ€™ll need to set `reduce_labels=True` to subtract one
    from all the labels. The zero-index is replaced by `255` so itâ€™s ignored by SegFormerâ€™s
    loss function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ª SegFormer å›¾åƒå¤„ç†å™¨ï¼Œå‡†å¤‡å›¾åƒå’Œæ³¨é‡Šä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚æŸäº›æ•°æ®é›†ï¼Œå¦‚æ­¤ç±»æ•°æ®é›†ï¼Œä½¿ç”¨é›¶ç´¢å¼•ä½œä¸ºèƒŒæ™¯ç±»ã€‚ä½†æ˜¯ï¼ŒèƒŒæ™¯ç±»å®é™…ä¸Šä¸åŒ…æ‹¬åœ¨
    150 ä¸ªç±»ä¸­ï¼Œå› æ­¤æ‚¨éœ€è¦è®¾ç½® `reduce_labels=True`ï¼Œä»æ‰€æœ‰æ ‡ç­¾ä¸­å‡å»ä¸€ä¸ªã€‚é›¶ç´¢å¼•è¢«æ›¿æ¢ä¸º `255`ï¼Œå› æ­¤ SegFormer çš„æŸå¤±å‡½æ•°ä¼šå¿½ç•¥å®ƒï¼š '
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: PytorchHide Pytorch content
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè— Pytorch å†…å®¹
- en: It is common to apply some data augmentations to an image dataset to make a
    model more robust against overfitting. In this guide, youâ€™ll use the [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)
    function from [torchvision](https://pytorch.org/vision/stable/index.html) to randomly
    change the color properties of an image, but you can also use any image library
    you like.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ä¼šå¯¹å›¾åƒæ•°æ®é›†åº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºï¼Œä»¥ä½¿æ¨¡å‹æ›´å…·æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)
    å‡½æ•°ä» [torchvision](https://pytorch.org/vision/stable/index.html) éšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒåº“ã€‚
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now create two preprocessing functions to prepare the images and annotations
    for the model. These functions convert the images into `pixel_values` and annotations
    to `labels`. For the training set, `jitter` is applied before providing the images
    to the image processor. For the test set, the image processor crops and normalizes
    the `images`, and only crops the `labels` because no data augmentation is applied
    during testing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œå‡†å¤‡å›¾åƒå’Œæ³¨é‡Šä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›å‡½æ•°å°†å›¾åƒè½¬æ¢ä¸º `pixel_values`ï¼Œå°†æ³¨é‡Šè½¬æ¢ä¸º `labels`ã€‚å¯¹äºè®­ç»ƒé›†ï¼Œåœ¨å°†å›¾åƒæä¾›ç»™å›¾åƒå¤„ç†å™¨ä¹‹å‰åº”ç”¨
    `jitter`ã€‚å¯¹äºæµ‹è¯•é›†ï¼Œå›¾åƒå¤„ç†å™¨è£å‰ªå’Œè§„èŒƒåŒ– `images`ï¼Œä»…è£å‰ª `labels`ï¼Œå› ä¸ºåœ¨æµ‹è¯•æœŸé—´ä¸åº”ç”¨æ•°æ®å¢å¼ºã€‚
- en: '[PRE19]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To apply the `jitter` over the entire dataset, use the ğŸ¤— Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    function. The transform is applied on the fly which is faster and consumes less
    disk space:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨ `jitter`ï¼Œè¯·ä½¿ç”¨ ğŸ¤— æ•°æ®é›† [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    å‡½æ•°ã€‚å˜æ¢æ˜¯å®æ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š
- en: '[PRE20]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TensorFlowHide TensorFlow content
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè— TensorFlow å†…å®¹
- en: 'It is common to apply some data augmentations to an image dataset to make a
    model more robust against overfitting. In this guide, youâ€™ll use [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)
    to randomly change the color properties of an image, but you can also use any
    image library you like. Define two separate transformation functions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å›¾åƒæ•°æ®é›†åº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºæ˜¯å¸¸è§çš„ï¼Œå¯ä»¥ä½¿æ¨¡å‹æ›´å…·æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)æ¥éšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒåº“ã€‚å®šä¹‰ä¸¤ä¸ªå•ç‹¬çš„è½¬æ¢å‡½æ•°ï¼š
- en: training data transformations that include image augmentation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…æ‹¬å›¾åƒå¢å¼ºçš„è®­ç»ƒæ•°æ®è½¬æ¢
- en: validation data transformations that only transpose the images, since computer
    vision models in ğŸ¤— Transformers expect channels-first layout
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éªŒè¯æ•°æ®è½¬æ¢ä»…è½¬ç½®å›¾åƒï¼Œå› ä¸ºğŸ¤— Transformersä¸­çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹æœŸæœ›é€šé“ä¼˜å…ˆå¸ƒå±€
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Next, create two preprocessing functions to prepare batches of images and annotations
    for the model. These functions apply the image transformations and use the earlier
    loaded `image_processor` to convert the images into `pixel_values` and annotations
    to `labels`. `ImageProcessor` also takes care of resizing and normalizing the
    images.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šçš„æ‰¹å¤„ç†ã€‚è¿™äº›å‡½æ•°åº”ç”¨å›¾åƒè½¬æ¢ï¼Œå¹¶ä½¿ç”¨ä¹‹å‰åŠ è½½çš„`image_processor`å°†å›¾åƒè½¬æ¢ä¸º`pixel_values`ï¼Œå°†æ³¨é‡Šè½¬æ¢ä¸º`labels`ã€‚`ImageProcessor`è¿˜è´Ÿè´£è°ƒæ•´å¤§å°å’Œè§„èŒƒåŒ–å›¾åƒã€‚
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To apply the preprocessing transformations over the entire dataset, use the
    ğŸ¤— Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    function. The transform is applied on the fly which is faster and consumes less
    disk space:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†è½¬æ¢ï¼Œä½¿ç”¨ğŸ¤— Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)å‡½æ•°ã€‚è½¬æ¢æ˜¯å®æ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Evaluate
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: 'Including a metric during training is often helpful for evaluating your modelâ€™s
    performance. You can quickly load an evaluation method with the ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy)
    (IoU) metric (see the ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªåº¦é‡æ ‡å‡†é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼ŒåŠ è½½[mean
    Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy)
    (IoU)åº¦é‡æ ‡å‡†ï¼ˆæŸ¥çœ‹ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æ ‡å‡†ï¼‰ï¼š
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then create a function to `compute` the metrics. Your predictions need to be
    converted to logits first, and then reshaped to match the size of the labels before
    you can call `compute`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥`compute`åº¦é‡æ ‡å‡†ã€‚æ‚¨çš„é¢„æµ‹éœ€è¦é¦–å…ˆè½¬æ¢ä¸ºlogitsï¼Œç„¶åé‡æ–°è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…æ ‡ç­¾çš„å¤§å°ï¼Œç„¶åæ‰èƒ½è°ƒç”¨`compute`ï¼š
- en: PytorchHide Pytorch content
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: TensorFlowHide TensorFlow content
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlowå†…å®¹
- en: '[PRE26]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Your `compute_metrics` function is ready to go now, and youâ€™ll return to it
    when you setup your training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„`compute_metrics`å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶ä¼šå†æ¬¡ç”¨åˆ°å®ƒã€‚
- en: Train
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: PytorchHide Pytorch content
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#finetune-with-trainer)!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[../training#finetune-with-trainer]ï¼
- en: 'Youâ€™re ready to start training your model now! Load SegFormer with [AutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation),
    and pass the model the mapping between label ids and label classes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ç°åœ¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[AutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation)åŠ è½½SegFormerï¼Œå¹¶å°†æ¨¡å‹ä¼ é€’ç»™æ ‡ç­¾idå’Œæ ‡ç­¾ç±»ä¹‹é—´çš„æ˜ å°„ï¼š
- en: '[PRE27]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'At this point, only three steps remain:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰åªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    It is important you donâ€™t remove unused columns because thisâ€™ll drop the `image`
    column. Without the `image` column, you canâ€™t create `pixel_values`. Set `remove_unused_columns=False`
    to prevent this behavior! The only other required parameter is `output_dir` which
    specifies where to save your model. Youâ€™ll push this model to the Hub by setting
    `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).
    At the end of each epoch, the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the IoU metric and save the training checkpoint.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚é‡è¦çš„æ˜¯ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºè¿™ä¼šåˆ é™¤`image`åˆ—ã€‚æ²¡æœ‰`image`åˆ—ï¼Œæ‚¨å°±æ— æ³•åˆ›å»º`pixel_values`ã€‚è®¾ç½®`remove_unused_columns=False`ä»¥é˜²æ­¢è¿™ç§è¡Œä¸ºï¼å¦ä¸€ä¸ªå¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆæ‚¨éœ€è¦ç™»å½•Hugging
    Faceæ‰èƒ½ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªepochç»“æŸæ—¶ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è¯„ä¼°IoUåº¦é‡æ ‡å‡†å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼ŒåŒæ—¶è¿˜éœ€è¦ä¼ é€’æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE28]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: TensorFlowHide TensorFlow content
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlowå†…å®¹
- en: If you are unfamiliar with fine-tuning a model with Keras, check out the [basic
    tutorial](./training#train-a-tensorflow-model-with-keras) first!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨Kerasè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè¯·å…ˆæŸ¥çœ‹[åŸºæœ¬æ•™ç¨‹](./training#train-a-tensorflow-model-with-keras)ï¼
- en: 'To fine-tune a model in TensorFlow, follow these steps:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
- en: Define the training hyperparameters, and set up an optimizer and a learning
    rate schedule.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰è®­ç»ƒè¶…å‚æ•°ï¼Œå¹¶è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ã€‚
- en: Instantiate a pretrained model.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: Convert a ğŸ¤— Dataset to a `tf.data.Dataset`.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ä¸€ä¸ªğŸ¤—æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`ã€‚
- en: Compile your model.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¼–è¯‘æ‚¨çš„æ¨¡å‹ã€‚
- en: Add callbacks to calculate metrics and upload your model to ğŸ¤— Hub
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ·»åŠ å›è°ƒä»¥è®¡ç®—æŒ‡æ ‡å¹¶å°†æ‚¨çš„æ¨¡å‹ä¸Šä¼ åˆ°ğŸ¤— Hub
- en: Use the `fit()` method to run the training.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`fit()`æ–¹æ³•è¿è¡Œè®­ç»ƒã€‚
- en: 'Start by defining the hyperparameters, optimizer and learning rate schedule:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆå®šä¹‰è¶…å‚æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ï¼š
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, load SegFormer with [TFAutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation)
    along with the label mappings, and compile it with the optimizer. Note that Transformers
    models all have a default task-relevant loss function, so you donâ€™t need to specify
    one unless you want to:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½¿ç”¨[TFAutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation)åŠ è½½SegFormerä»¥åŠæ ‡ç­¾æ˜ å°„ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–å™¨å¯¹å…¶è¿›è¡Œç¼–è¯‘ã€‚è¯·æ³¨æ„ï¼ŒTransformersæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šï¼š
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Convert your datasets to the `tf.data.Dataset` format using the [to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
    and the [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)å’Œ[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To compute the accuracy from the predictions and push your model to the ğŸ¤— Hub,
    use [Keras callbacks](../main_classes/keras_callbacks). Pass your `compute_metrics`
    function to [KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback),
    and use the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)
    to upload the model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä»é¢„æµ‹ä¸­è®¡ç®—å‡†ç¡®ç‡å¹¶å°†æ‚¨çš„æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hubï¼Œè¯·ä½¿ç”¨[Keraså›è°ƒ](../main_classes/keras_callbacks)ã€‚å°†æ‚¨çš„`compute_metrics`å‡½æ•°ä¼ é€’ç»™[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)ï¼Œå¹¶ä½¿ç”¨[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)æ¥ä¸Šä¼ æ¨¡å‹ï¼š
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, you are ready to train your model! Call `fit()` with your training
    and validation datasets, the number of epochs, and your callbacks to fine-tune
    the model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€æ—¶ä»£æ•°é‡å’Œå›è°ƒæ¥è°ƒç”¨`fit()`æ¥å¾®è°ƒæ¨¡å‹ï¼š
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Congratulations! You have fine-tuned your model and shared it on the ğŸ¤— Hub.
    You can now use it for inference!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒå¹¶åœ¨ğŸ¤— Hubä¸Šåˆ†äº«äº†å®ƒã€‚ç°åœ¨æ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼
- en: Inference
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Great, now that youâ€™ve finetuned a model, you can use it for inference!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼
- en: 'Load an image for inference:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†ï¼š
- en: '[PRE35]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Image of bedroom](../Images/1f1abe71d12160da7bd59d35ef05323c.png)PytorchHide
    Pytorch content'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![å§å®¤å›¾åƒ](../Images/1f1abe71d12160da7bd59d35ef05323c.png)Pytorchéšè— Pytorchå†…å®¹'
- en: 'We will now see how to infer without a pipeline. Process the image with an
    image processor and place the `pixel_values` on a GPU:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åœ¨æ²¡æœ‰ç®¡é“çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†ã€‚ä½¿ç”¨å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒï¼Œå¹¶å°†`pixel_values`æ”¾åœ¨GPUä¸Šï¼š
- en: '[PRE36]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Pass your input to the model and return the `logits`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š
- en: '[PRE37]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, rescale the logits to the original image size:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå°†logitsé‡æ–°ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå¤§å°ï¼š
- en: '[PRE38]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: TensorFlowHide TensorFlow content
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè— TensorFlowå†…å®¹
- en: 'Load an image processor to preprocess the image and return the input as TensorFlow
    tensors:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€ä¸ªå›¾åƒå¤„ç†å™¨æ¥é¢„å¤„ç†å›¾åƒå¹¶å°†è¾“å…¥è¿”å›ä¸ºTensorFlowå¼ é‡ï¼š
- en: '[PRE39]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Pass your input to the model and return the `logits`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š
- en: '[PRE40]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, rescale the logits to the original image size and apply argmax on the
    class dimension:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå°†logitsé‡æ–°ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå¤§å°ï¼Œå¹¶åœ¨ç±»ç»´åº¦ä¸Šåº”ç”¨argmaxï¼š
- en: '[PRE41]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To visualize the results, load the [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)
    as `ade_palette()` that maps each class to their RGB values. Then you can combine
    and plot your image and the predicted segmentation map:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯è§†åŒ–ç»“æœï¼ŒåŠ è½½[æ•°æ®é›†é¢œè‰²è°ƒè‰²æ¿](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)ä½œä¸º`ade_palette()`ï¼Œå°†æ¯ä¸ªç±»æ˜ å°„åˆ°å®ƒä»¬çš„RGBå€¼ã€‚ç„¶åæ‚¨å¯ä»¥ç»„åˆå¹¶ç»˜åˆ¶æ‚¨çš„å›¾åƒå’Œé¢„æµ‹çš„åˆ†å‰²åœ°å›¾ï¼š
- en: '[PRE42]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Image of bedroom overlaid with segmentation map](../Images/2bc12ae66cec2ea434945aaaa1e5c6bf.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![è¦†ç›–æœ‰åˆ†å‰²åœ°å›¾çš„å§å®¤å›¾åƒ](../Images/2bc12ae66cec2ea434945aaaa1e5c6bf.png)'
