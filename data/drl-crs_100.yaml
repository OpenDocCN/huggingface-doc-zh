- en: Hands-on
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit8/hands-on-cleanrl](https://huggingface.co/learn/deep-rl-course/unit8/hands-on-cleanrl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/learn/deep-rl-course/unit8/hands-on-cleanrl](https://huggingface.co/learn/deep-rl-course/unit8/hands-on-cleanrl)
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![提问](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb)'
- en: Now that we studied the theory behind PPO, the best way to understand how it
    works **is to implement it from scratch.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了PPO背后的理论，理解它的最佳方法**是从头开始实现它**。
- en: Implementing an architecture from scratch is the best way to understand it,
    and it’s a good habit. We have already done it for a value-based method with Q-Learning
    and a Policy-based method with Reinforce.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始实现架构是理解它的最佳方法，也是一个好习惯。我们已经为基于价值的Q学习方法和基于策略的Reinforce方法做过了。
- en: 'So, to be able to code it, we’re going to use two resources:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了能够编写代码，我们将使用两个资源：
- en: A tutorial made by [Costa Huang](https://github.com/vwxyzjn). Costa is behind
    [CleanRL](https://github.com/vwxyzjn/cleanrl), a Deep Reinforcement Learning library
    that provides high-quality single-file implementation with research-friendly features.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由[Costa Huang](https://github.com/vwxyzjn)制作的教程。Costa是[CleanRL](https://github.com/vwxyzjn/cleanrl)背后的人，这是一个提供高质量单文件实现和研究友好功能的深度强化学习库。
- en: In addition to the tutorial, to go deeper, you can read the 13 core implementation
    details: [https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了教程外，为了更深入地了解，您可以阅读13个核心实现细节：[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
- en: 'Then, to test its robustness, we’re going to train it in:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了测试其稳健性，我们将在以下进行训练：
- en: '[LunarLander-v2](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LunarLander-v2](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)'
- en: <assets/63_deep_rl_intro/lunarlander.mp4>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <assets/63_deep_rl_intro/lunarlander.mp4>
- en: And finally, we will push the trained model to the Hub to evaluate and visualize
    your agent playing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把训练好的模型推送到Hub，以评估和可视化您的代理游戏。
- en: LunarLander-v2 is the first environment you used when you started this course.
    At that time, you didn’t know how it worked, and now you can code it from scratch
    and train it. **How incredible is that 🤩.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LunarLander-v2是您开始本课程时使用的第一个环境。那时，您不知道它是如何工作的，现在您可以从头开始编写代码并进行训练。**这是多么令人难以置信的
    🤩。**
- en: '[https://giphy.com/embed/pynZagVcYxVUk](https://giphy.com/embed/pynZagVcYxVUk)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://giphy.com/embed/pynZagVcYxVUk](https://giphy.com/embed/pynZagVcYxVUk)'
- en: '[via GIPHY](https://giphy.com/gifs/the-office-michael-heartbreak-pynZagVcYxVUk)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[通过GIPHY](https://giphy.com/gifs/the-office-michael-heartbreak-pynZagVcYxVUk)'
- en: Let’s get started! 🚀
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！🚀
- en: 'The colab notebook:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: colab笔记本：
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit8/unit8_part1.ipynb)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit8/unit8_part1.ipynb)'
- en: 'Unit 8: Proximal Policy Gradient (PPO) with PyTorch 🤖'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Unit 8: 使用PyTorch进行Proximal Policy Gradient（PPO）🤖'
- en: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Unit 8](../Images/99ae9849fcb07d6d32b6cef4d05623c4.png)'
- en: In this notebook, you’ll learn to **code your PPO agent from scratch with PyTorch
    using CleanRL implementation as model**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本笔记本中，您将学习**使用PyTorch从头开始编写您的PPO代理，使用CleanRL实现作为模型**。
- en: 'To test its robustness, we’re going to train it in:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试其稳健性，我们将在以下进行训练：
- en: '[LunarLander-v2 🚀](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LunarLander-v2 🚀](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)'
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断努力改进我们的教程，所以**如果您在本笔记本中发现问题**，请在GitHub Repo上[提出问题](https://github.com/huggingface/deep-rl-class/issues)。
- en: Objectives of this notebook 🏆
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本笔记本的目标 🏆
- en: 'At the end of the notebook, you will:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的末尾，您将：
- en: Be able to **code your PPO agent from scratch using PyTorch**.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**使用PyTorch从头开始编写您的PPO代理**。
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score 🔥.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**将您训练的代理和代码推送到Hub**，并附带一个漂亮的视频回放和评估分数🔥。
- en: Prerequisites 🏗️
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件 🏗️
- en: 'Before diving into the notebook, you need to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究笔记本之前，您需要：
- en: 🔲 📚 Study [PPO by reading Unit 8](https://huggingface.co/deep-rl-course/unit8/introduction)
    🤗
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 🔲 📚 学习[PPO，阅读第8单元](https://huggingface.co/deep-rl-course/unit8/introduction)
    🤗
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push one model, we don’t ask for a minimal result but we **advise
    you to try different hyperparameters settings to get better results**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这个实践过程的[认证过程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)，您需要推送一个模型，我们不要求最小结果，但我们**建议您尝试不同的超参数设置以获得更好的结果**。
- en: If you don’t find your model, **go to the bottom of the page and click on the
    refresh button**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找不到您的模型，**请转到页面底部并单击刷新按钮**
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有关认证过程的更多信息，请查看此部分👉[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: Set the GPU 💪
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置GPU 💪
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了**加速代理的训练，我们将使用GPU**。要做到这一点，请转到`运行时 > 更改运行时类型`
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
- en: '`Hardware Accelerator > GPU`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`硬件加速器 > GPU`'
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![GPU步骤2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
- en: Create a virtual display 🔽
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个虚拟显示 🔽
- en: During the notebook, we’ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们需要生成一个重播视频。为此，在colab中，**我们需要有一个虚拟屏幕来渲染环境**（从而记录帧）。
- en: Hence the following cell will install the librairies and create and run a virtual
    screen 🖥
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下单元格将安装库并创建并运行一个虚拟屏幕 🖥
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install dependencies 🔽
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装依赖 🔽
- en: For this exercise, we use `gym==0.21` because the video was recorded with Gym.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们使用`gym==0.21`，因为视频是用Gym录制的。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s code PPO from scratch with Costa Huang’s tutorial
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们从头开始用Costa Huang的教程编写PPO
- en: For the core implementation of PPO we’re going to use the excellent [Costa Huang](https://costa.sh/)
    tutorial.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于PPO的核心实现，我们将使用优秀的[Costa Huang](https://costa.sh/)教程。
- en: 'In addition to the tutorial, to go deeper you can read the 37 core implementation
    details: [https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了教程，要深入了解，您可以阅读37个核心实现细节：[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
- en: '👉 The video tutorial: [https://youtu.be/MEt6rrxH8W4](https://youtu.be/MEt6rrxH8W4)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 视频教程：[https://youtu.be/MEt6rrxH8W4](https://youtu.be/MEt6rrxH8W4)
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Add the Hugging Face Integration 🤗
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加Hugging Face集成 🤗
- en: In order to push our model to the Hub, we need to define a function `package_to_hub`
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将我们的模型推送到Hub，我们需要定义一个`package_to_hub`函数
- en: Add dependencies we need to push our model to the Hub
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加我们需要将模型推送到Hub的依赖项
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Add new argument in `parse_args()` function to define the repo-id where we want
    to push the model.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`parse_args()`函数中添加新参数，以定义我们要推送模型的repo-id。
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we add the methods needed to push the model to the Hub
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们添加推送模型到Hub所需的方法
- en: 'These methods will:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法将：
- en: '`_evalutate_agent()`: evaluate the agent.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_evalutate_agent()`: 评估代理。'
- en: '`_generate_model_card()`: generate the model card of your agent.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_generate_model_card()`: 生成您的代理的模型卡片。'
- en: '`_record_video()`: record a video of your agent.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_record_video()`: 记录您的代理的视频。'
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we call this function at the end of the PPO training
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在PPO训练结束时调用这个函数
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here’s what the final ppo.py file looks like:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是最终ppo.py文件的样子：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够与社区分享您的模型，还有三个步骤要遵循：
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ (如果尚未完成) 创建一个HF帐户 ➡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2️⃣ Sign in and get your authentication token from the Hugging Face website.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 登录并从Hugging Face网站获取您的身份验证令牌。
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的令牌([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **具有写入权限**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![创建HF令牌](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: Copy the token
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制令牌
- en: Run the cell below and paste the token
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行下面的单元格并粘贴令牌
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you don’t want to use Google Colab or a Jupyter Notebook, you need to use
    this command instead: `huggingface-cli login`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用Google Colab或Jupyter Notebook，您需要使用这个命令：`huggingface-cli login`
- en: Let’s start the training 🔥
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们开始训练 🔥
- en: ⚠️ ⚠️ ⚠️ Don’t use **the same repo id with the one you used for the Unit 1**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ ⚠️ ⚠️ 不要使用**与您在第1单元中使用的相同的repo id**
- en: Now that you’ve coded PPO from scratch and added the Hugging Face Integration,
    we’re ready to start the training 🔥
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在您已经从头开始编写了PPO并添加了Hugging Face集成，我们准备开始训练 🔥
- en: First, you need to copy all your code to a file you create called `ppo.py`
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，您需要将所有代码复制到一个名为`ppo.py`的文件中
- en: '![PPO](../Images/c6a57155d0c4da38fd607c740b13277e.png) ![PPO](../Images/2fd5c967c9514b78a4b2bbbefa476afd.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![PPO](../Images/c6a57155d0c4da38fd607c740b13277e.png) ![PPO](../Images/2fd5c967c9514b78a4b2bbbefa476afd.png)'
- en: Now we just need to run this python script using `python <name-of-python-script>.py`
    with the additional parameters we defined using `argparse`
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们只需要使用`python <name-of-python-script>.py`运行这个python脚本，使用我们定义的额外参数`argparse`
- en: You should modify more hyperparameters otherwise the training will not be super
    stable.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该修改更多超参数，否则训练将不会很稳定。
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Some additional challenges 🏆
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些额外的挑战 🏆
- en: The best way to learn **is to try things on your own**! Why not try another
    environment? Or why not trying to modify the implementation to work with Gymnasium?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的最佳方式**是自己尝试**！为什么不尝试另一个环境？或者为什么不尝试修改实现以适用于体育馆？
- en: See you in Unit 8, part 2 where we’re going to train agents to play Doom 🔥
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第8单元的第2部分见，那里我们将训练代理玩毁灭 🔥
- en: Keep learning, stay awesome 🤗
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续学习，保持出色 🤗
