- en: Text-to-image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒ
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/text2image](https://huggingface.co/docs/diffusers/training/text2image)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/training/text2image](https://huggingface.co/docs/diffusers/training/text2image)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The text-to-image script is experimental, and itâ€™s easy to overfit and run into
    issues like catastrophic forgetting. Try exploring different hyperparameters to
    get the best results on your dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒè„šæœ¬æ˜¯å®éªŒæ€§çš„ï¼Œå¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆå¹¶é‡åˆ°ç¾éš¾æ€§é—å¿˜ç­‰é—®é¢˜ã€‚å°è¯•æ¢ç´¢ä¸åŒçš„è¶…å‚æ•°ï¼Œä»¥è·å¾—æ•°æ®é›†çš„æœ€ä½³ç»“æœã€‚
- en: Text-to-image models like Stable Diffusion are conditioned to generate images
    given a text prompt.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¦‚Stable Diffusionæ˜¯æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒçš„ã€‚
- en: Training a model can be taxing on your hardware, but if you enable `gradient_checkpointing`
    and `mixed_precision`, it is possible to train a model on a single 24GB GPU. If
    youâ€™re training with larger batch sizes or want to train faster, itâ€™s better to
    use GPUs with more than 30GB of memory. You can reduce your memory footprint by
    enabling memory-efficient attention with [xFormers](../optimization/xformers).
    JAX/Flax training is also supported for efficient training on TPUs and GPUs, but
    it doesnâ€™t support gradient checkpointing, gradient accumulation or xFormers.
    A GPU with at least 30GB of memory or a TPU v3 is recommended for training with
    Flax.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å¯èƒ½ä¼šå¯¹æ‚¨çš„ç¡¬ä»¶é€ æˆè´Ÿæ‹…ï¼Œä½†å¦‚æœå¯ç”¨`gradient_checkpointing`å’Œ`mixed_precision`ï¼Œå¯ä»¥åœ¨å•ä¸ª24GB GPUä¸Šè®­ç»ƒæ¨¡å‹ã€‚å¦‚æœæ‚¨ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°è¿›è¡Œè®­ç»ƒæˆ–å¸Œæœ›è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œæœ€å¥½ä½¿ç”¨å…·æœ‰è¶…è¿‡30GBå†…å­˜çš„GPUã€‚æ‚¨å¯ä»¥é€šè¿‡å¯ç”¨[xFormers](../optimization/xformers)çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æ¥å‡å°‘å†…å­˜å ç”¨ã€‚JAX/Flaxè®­ç»ƒä¹Ÿæ”¯æŒåœ¨TPUå’ŒGPUä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œä½†ä¸æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ¢¯åº¦ç´¯ç§¯æˆ–xFormersã€‚å»ºè®®ä½¿ç”¨è‡³å°‘30GBå†…å­˜çš„GPUæˆ–TPU
    v3è¿›è¡ŒFlaxè®­ç»ƒã€‚
- en: This guide will explore the [train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)
    training script to help you become familiar with it, and how you can adapt it
    for your own use-case.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†æ¢ç´¢[train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)è®­ç»ƒè„šæœ¬ï¼Œä»¥å¸®åŠ©æ‚¨ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•ä¸ºè‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚
- en: 'Before running the script, make sure you install the library from source:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨ä»æºä»£ç å®‰è£…åº“ï¼š
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then navigate to the example folder containing the training script and install
    the required dependencies for the script youâ€™re using:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè½¬åˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…æ‚¨æ­£åœ¨ä½¿ç”¨çš„è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š
- en: PyTorchFlax
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFlax
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ğŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. Itâ€™ll automatically configure your training setup based on your
    hardware and environment. Take a look at the ğŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateæ˜¯ä¸€ä¸ªå¸®åŠ©æ‚¨åœ¨å¤šä¸ªGPU/TPUä¸Šè®­ç»ƒæˆ–ä½¿ç”¨æ··åˆç²¾åº¦çš„åº“ã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®æ‚¨çš„è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ğŸ¤— Accelerate
    [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour)ä»¥äº†è§£æ›´å¤šã€‚
- en: 'Initialize an ğŸ¤— Accelerate environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ğŸ¤— Accelerateç¯å¢ƒï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To setup a default ğŸ¤— Accelerate environment without choosing any configurations:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¾ç½®é»˜è®¤çš„ğŸ¤— Accelerateç¯å¢ƒè€Œä¸é€‰æ‹©ä»»ä½•é…ç½®ï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or if your environment doesnâ€™t support an interactive shell, like a notebook,
    you can use:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼shellï¼Œæ¯”å¦‚ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºé€‚ç”¨äºè®­ç»ƒè„šæœ¬çš„æ•°æ®é›†ã€‚
- en: Script parameters
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è„šæœ¬å‚æ•°
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesnâ€™t cover every aspect of the script
    in detail. If youâ€™re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)
    and let us know if you have any questions or concerns.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹éƒ¨åˆ†çªå‡ºæ˜¾ç¤ºäº†è®­ç»ƒè„šæœ¬çš„ä¸€äº›é‡è¦éƒ¨åˆ†ï¼Œä»¥å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¿®æ”¹å®ƒï¼Œä½†å¹¶æœªè¯¦ç»†æ¶µç›–è„šæœ¬çš„æ¯ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·éšæ—¶é˜…è¯»[è„šæœ¬](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬æ‚¨æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ã€‚
- en: The training script provides many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L193)
    function. This function provides default values for each parameter, such as the
    training batch size and learning rate, but you can also set your own values in
    the training command if youâ€™d like.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬æä¾›äº†è®¸å¤šå‚æ•°ï¼Œå¸®åŠ©æ‚¨è‡ªå®šä¹‰è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰å‚æ•°åŠå…¶æè¿°éƒ½å¯ä»¥åœ¨[`parse_args()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L193)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚è¯¥å‡½æ•°ä¸ºæ¯ä¸ªå‚æ•°æä¾›é»˜è®¤å€¼ï¼Œä¾‹å¦‚è®­ç»ƒæ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚
- en: 'For example, to speedup training with mixed precision using the fp16 format,
    add the `--mixed_precision` parameter to the training command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨fp16æ ¼å¼åŠ å¿«æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¯·åœ¨è®­ç»ƒå‘½ä»¤ä¸­æ·»åŠ `--mixed_precision`å‚æ•°ï¼š
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Some basic and important parameters include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›åŸºæœ¬ä¸”é‡è¦çš„å‚æ•°åŒ…æ‹¬ï¼š
- en: '`--pretrained_model_name_or_path`: the name of the model on the Hub or a local
    path to the pretrained model'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--pretrained_model_name_or_path`ï¼šHubä¸Šæ¨¡å‹çš„åç§°æˆ–æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„'
- en: '`--dataset_name`: the name of the dataset on the Hub or a local path to the
    dataset to train on'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--dataset_name`ï¼šHubä¸Šæ•°æ®é›†çš„åç§°æˆ–è¦è®­ç»ƒçš„æ•°æ®é›†çš„æœ¬åœ°è·¯å¾„'
- en: '`--image_column`: the name of the image column in the dataset to train on'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--image_column`ï¼šæ•°æ®é›†ä¸­è¦è®­ç»ƒçš„å›¾åƒåˆ—çš„åç§°'
- en: '`--caption_column`: the name of the text column in the dataset to train on'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--caption_column`ï¼šæ•°æ®é›†ä¸­è¦è®­ç»ƒçš„æ–‡æœ¬åˆ—çš„åç§°'
- en: '`--output_dir`: where to save the trained model'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--output_dir`ï¼šä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹çš„ä½ç½®'
- en: '`--push_to_hub`: whether to push the trained model to the Hub'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--push_to_hub`ï¼šæ˜¯å¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ°Hub'
- en: '`--checkpointing_steps`: frequency of saving a checkpoint as the model trains;
    this is useful if for some reason training is interrupted, you can continue training
    from that checkpoint by adding `--resume_from_checkpoint` to your training command'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--checkpointing_steps`ï¼šä¿å­˜æ£€æŸ¥ç‚¹çš„é¢‘ç‡ï¼Œå½“è®­ç»ƒä¸­æ–­æ—¶ï¼Œå¯ä»¥é€šè¿‡åœ¨è®­ç»ƒå‘½ä»¤ä¸­æ·»åŠ  `--resume_from_checkpoint`
    ä»è¯¥æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ'
- en: Min-SNR weighting
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€å°-SNR åŠ æƒ
- en: The [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can
    help with training by rebalancing the loss to achieve faster convergence. The
    training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR
    is compatible with both prediction types. This weighting strategy is only supported
    by PyTorch and is unavailable in the Flax training script.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[æœ€å°-SNR](https://huggingface.co/papers/2303.09556) åŠ æƒç­–ç•¥å¯ä»¥é€šè¿‡é‡æ–°å¹³è¡¡æŸå¤±æ¥å¸®åŠ©è®­ç»ƒï¼Œä»¥å®ç°æ›´å¿«çš„æ”¶æ•›ã€‚è®­ç»ƒè„šæœ¬æ”¯æŒé¢„æµ‹
    `epsilon`ï¼ˆå™ªå£°ï¼‰æˆ– `v_prediction`ï¼Œä½†æœ€å°-SNR ä¸ä¸¤ç§é¢„æµ‹ç±»å‹å…¼å®¹ã€‚è¿™ç§åŠ æƒç­–ç•¥ä»…å— PyTorch æ”¯æŒï¼Œåœ¨ Flax è®­ç»ƒè„šæœ¬ä¸­ä¸å¯ç”¨ã€‚'
- en: 'Add the `--snr_gamma` parameter and set it to the recommended value of 5.0:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ  `--snr_gamma` å‚æ•°ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºæ¨èå€¼ 5.0ï¼š
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can compare the loss surfaces for different `snr_gamma` values in this [Weights
    and Biases](https://wandb.ai/sayakpaul/text2image-finetune-minsnr) report. For
    smaller datasets, the effects of Min-SNR may not be as obvious compared to larger
    datasets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨è¿™ä¸ª [Weights and Biases](https://wandb.ai/sayakpaul/text2image-finetune-minsnr)
    æŠ¥å‘Šä¸­æ¯”è¾ƒä¸åŒ `snr_gamma` å€¼çš„æŸå¤±æ›²é¢ã€‚å¯¹äºè¾ƒå°çš„æ•°æ®é›†ï¼Œä¸è¾ƒå¤§æ•°æ®é›†ç›¸æ¯”ï¼Œæœ€å°-SNR çš„å½±å“å¯èƒ½ä¸é‚£ä¹ˆæ˜æ˜¾ã€‚
- en: Training script
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬
- en: The dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L490)
    function. If you need to adapt the training script, this is where youâ€™ll need
    to make your changes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯åœ¨ [`main()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L490)
    å‡½æ•°ä¸­ã€‚å¦‚æœéœ€è¦è°ƒæ•´è®­ç»ƒè„šæœ¬ï¼Œè¿™å°±æ˜¯æ‚¨éœ€è¦è¿›è¡Œæ›´æ”¹çš„åœ°æ–¹ã€‚
- en: 'The `train_text_to_image` script starts by [loading a scheduler](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L543)
    and tokenizer. You can choose to use a different scheduler here if you want:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_text_to_image` è„šæœ¬é¦–å…ˆ[åŠ è½½è°ƒåº¦å™¨](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L543)å’Œåˆ†è¯å™¨ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œä½¿ç”¨ä¸åŒçš„è°ƒåº¦å™¨ï¼š'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then the script [loads the UNet](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L619)
    model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè„šæœ¬[åŠ è½½ UNet](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L619)
    æ¨¡å‹ï¼š
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, the text and image columns of the dataset need to be preprocessed. The
    [`tokenize_captions`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L724)
    function handles tokenizing the inputs, and the [`train_transforms`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L742)
    function specifies the type of transforms to apply to the image. Both of these
    functions are bundled into `preprocess_train`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œéœ€è¦å¯¹æ•°æ®é›†çš„æ–‡æœ¬å’Œå›¾åƒåˆ—è¿›è¡Œé¢„å¤„ç†ã€‚[`tokenize_captions`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L724)
    å‡½æ•°å¤„ç†è¾“å…¥çš„åˆ†è¯ï¼Œ[`train_transforms`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L742)
    å‡½æ•°æŒ‡å®šè¦åº”ç”¨äºå›¾åƒçš„è½¬æ¢ç±»å‹ã€‚è¿™ä¸¤ä¸ªå‡½æ•°éƒ½æ‰“åŒ…åˆ° `preprocess_train` ä¸­ï¼š
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Lastly, the [training loop](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L878)
    handles everything else. It encodes images into latent space, adds noise to the
    latents, computes the text embeddings to condition on, updates the model parameters,
    and saves and pushes the model to the Hub. If you want to learn more about how
    the training loop works, check out the [Understanding pipelines, models and schedulers](../using-diffusers/write_own_pipeline)
    tutorial which breaks down the basic pattern of the denoising process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L878)
    å¤„ç†å…¶ä»–æ‰€æœ‰äº‹åŠ¡ã€‚å®ƒå°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨ç©ºé—´ï¼Œå‘æ½œåœ¨ç©ºé—´æ·»åŠ å™ªå£°ï¼Œè®¡ç®—æ–‡æœ¬åµŒå…¥ä»¥è¿›è¡Œæ¡ä»¶åŒ–ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¹¶å°†æ¨¡å‹ä¿å­˜å¹¶æ¨é€åˆ° Hubã€‚å¦‚æœæ‚¨æƒ³äº†è§£è®­ç»ƒå¾ªç¯çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹
    [äº†è§£ç®¡é“ã€æ¨¡å‹å’Œè°ƒåº¦å™¨](../using-diffusers/write_own_pipeline) æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹è¯¦ç»†ä»‹ç»äº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚
- en: Launch the script
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯åŠ¨è„šæœ¬
- en: Once youâ€™ve made all your changes or youâ€™re okay with the default configuration,
    youâ€™re ready to launch the training script! ğŸš€
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å®Œæˆæ‰€æœ‰æ›´æ”¹æˆ–å¯¹é»˜è®¤é…ç½®æ»¡æ„ï¼Œæ‚¨å°±å¯ä»¥å¯åŠ¨è®­ç»ƒè„šæœ¬äº†ï¼ğŸš€
- en: PyTorchFlax
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFlax
- en: Letâ€™s train on the [PokÃ©mon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
    dataset to generate your own PokÃ©mon. Set the environment variables `MODEL_NAME`
    and `dataset_name` to the model and the dataset (either from the Hub or a local
    path). If youâ€™re training on more than one GPU, add the `--multi_gpu` parameter
    to the `accelerate launch` command.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ [PokÃ©mon BLIP æ ‡é¢˜](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
    æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œç”Ÿæˆæ‚¨è‡ªå·±çš„ PokÃ©monã€‚å°†ç¯å¢ƒå˜é‡ `MODEL_NAME` å’Œ `dataset_name` è®¾ç½®ä¸ºæ¨¡å‹å’Œæ•°æ®é›†ï¼ˆæ¥è‡ª Hub æˆ–æœ¬åœ°è·¯å¾„ï¼‰ã€‚å¦‚æœæ‚¨è¦åœ¨å¤šä¸ª
    GPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯·åœ¨ `accelerate launch` å‘½ä»¤ä¸­æ·»åŠ  `--multi_gpu` å‚æ•°ã€‚
- en: To train on a local dataset, set the `TRAIN_DIR` and `OUTPUT_DIR` environment
    variables to the path of the dataset and where to save the model to.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æœ¬åœ°æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¯·å°† `TRAIN_DIR` å’Œ `OUTPUT_DIR` ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºæ•°æ®é›†çš„è·¯å¾„å’Œè¦ä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once training is complete, you can use your newly trained model for inference:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š
- en: PyTorchFlax
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFlax
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next steps
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'Congratulations on training your own text-to-image model! To learn more about
    how to use your new model, the following guides may be helpful:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥è´ºæ‚¨è®­ç»ƒæˆåŠŸè‡ªå·±çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼è¦äº†è§£å¦‚ä½•ä½¿ç”¨æ‚¨çš„æ–°æ¨¡å‹ï¼Œä»¥ä¸‹æŒ‡å—å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼š
- en: Learn how to [load LoRA weights](../using-diffusers/loading_adapters#LoRA) for
    inference if you trained your model with LoRA.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•åœ¨æ¨ç†æ—¶[åŠ è½½ LoRA æƒé‡](../using-diffusers/loading_adapters#LoRA)ï¼Œå¦‚æœæ‚¨ä½¿ç”¨ LoRA è®­ç»ƒäº†æ‚¨çš„æ¨¡å‹ã€‚
- en: Learn more about how certain parameters like guidance scale or techniques such
    as prompt weighting can help you control inference in the [Text-to-image](../using-diffusers/conditional_image_generation)
    task guide.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº†è§£æ›´å¤šå…³äºå¦‚ä½•é€šè¿‡æŒ‡å¯¼å°ºåº¦ç­‰å‚æ•°æˆ–æŠ€æœ¯ï¼Œå¦‚æç¤ºåŠ æƒï¼Œæ¥å¸®åŠ©æ‚¨æ§åˆ¶åœ¨[æ–‡æœ¬åˆ°å›¾åƒ](../using-diffusers/conditional_image_generation)ä»»åŠ¡æŒ‡å—ä¸­çš„æ¨ç†ã€‚
