- en: Text-to-image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到图像
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/text2image](https://huggingface.co/docs/diffusers/training/text2image)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/training/text2image](https://huggingface.co/docs/diffusers/training/text2image)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The text-to-image script is experimental, and it’s easy to overfit and run into
    issues like catastrophic forgetting. Try exploring different hyperparameters to
    get the best results on your dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像脚本是实验性的，很容易过拟合并遇到灾难性遗忘等问题。尝试探索不同的超参数，以获得数据集的最佳结果。
- en: Text-to-image models like Stable Diffusion are conditioned to generate images
    given a text prompt.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像模型如Stable Diffusion是根据文本提示生成图像的。
- en: Training a model can be taxing on your hardware, but if you enable `gradient_checkpointing`
    and `mixed_precision`, it is possible to train a model on a single 24GB GPU. If
    you’re training with larger batch sizes or want to train faster, it’s better to
    use GPUs with more than 30GB of memory. You can reduce your memory footprint by
    enabling memory-efficient attention with [xFormers](../optimization/xformers).
    JAX/Flax training is also supported for efficient training on TPUs and GPUs, but
    it doesn’t support gradient checkpointing, gradient accumulation or xFormers.
    A GPU with at least 30GB of memory or a TPU v3 is recommended for training with
    Flax.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型可能会对您的硬件造成负担，但如果启用`gradient_checkpointing`和`mixed_precision`，可以在单个24GB GPU上训练模型。如果您使用更大的批量大小进行训练或希望训练速度更快，最好使用具有超过30GB内存的GPU。您可以通过启用[xFormers](../optimization/xformers)的内存高效注意力来减少内存占用。JAX/Flax训练也支持在TPU和GPU上进行高效训练，但不支持梯度检查点、梯度累积或xFormers。建议使用至少30GB内存的GPU或TPU
    v3进行Flax训练。
- en: This guide will explore the [train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)
    training script to help you become familiar with it, and how you can adapt it
    for your own use-case.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将探索[train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)训练脚本，以帮助您熟悉它，以及如何为自己的用例进行调整。
- en: 'Before running the script, make sure you install the library from source:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行脚本之前，请确保您从源代码安装库：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then navigate to the example folder containing the training script and install
    the required dependencies for the script you’re using:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后转到包含训练脚本的示例文件夹，并安装您正在使用的脚本所需的依赖项：
- en: PyTorchFlax
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFlax
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. It’ll automatically configure your training setup based on your
    hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Accelerate是一个帮助您在多个GPU/TPU上训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate
    [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多。
- en: 'Initialize an 🤗 Accelerate environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化🤗 Accelerate环境：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To setup a default 🤗 Accelerate environment without choosing any configurations:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置默认的🤗 Accelerate环境而不选择任何配置：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or if your environment doesn’t support an interactive shell, like a notebook,
    you can use:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您的环境不支持交互式shell，比如笔记本，您可以使用：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您想在自己的数据集上训练模型，请查看[创建用于训练的数据集](create_dataset)指南，了解如何创建适用于训练脚本的数据集。
- en: Script parameters
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚本参数
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesn’t cover every aspect of the script
    in detail. If you’re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)
    and let us know if you have any questions or concerns.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分突出显示了训练脚本的一些重要部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)，并告诉我们您是否有任何问题或疑虑。
- en: The training script provides many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L193)
    function. This function provides default values for each parameter, such as the
    training batch size and learning rate, but you can also set your own values in
    the training command if you’d like.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本提供了许多参数，帮助您自定义训练运行。所有参数及其描述都可以在[`parse_args()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L193)函数中找到。该函数为每个参数提供默认值，例如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。
- en: 'For example, to speedup training with mixed precision using the fp16 format,
    add the `--mixed_precision` parameter to the training command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要使用fp16格式加快混合精度训练，请在训练命令中添加`--mixed_precision`参数：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Some basic and important parameters include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基本且重要的参数包括：
- en: '`--pretrained_model_name_or_path`: the name of the model on the Hub or a local
    path to the pretrained model'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--pretrained_model_name_or_path`：Hub上模型的名称或本地预训练模型的路径'
- en: '`--dataset_name`: the name of the dataset on the Hub or a local path to the
    dataset to train on'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--dataset_name`：Hub上数据集的名称或要训练的数据集的本地路径'
- en: '`--image_column`: the name of the image column in the dataset to train on'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--image_column`：数据集中要训练的图像列的名称'
- en: '`--caption_column`: the name of the text column in the dataset to train on'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--caption_column`：数据集中要训练的文本列的名称'
- en: '`--output_dir`: where to save the trained model'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--output_dir`：保存训练好的模型的位置'
- en: '`--push_to_hub`: whether to push the trained model to the Hub'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--push_to_hub`：是否将训练好的模型推送到Hub'
- en: '`--checkpointing_steps`: frequency of saving a checkpoint as the model trains;
    this is useful if for some reason training is interrupted, you can continue training
    from that checkpoint by adding `--resume_from_checkpoint` to your training command'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--checkpointing_steps`：保存检查点的频率，当训练中断时，可以通过在训练命令中添加 `--resume_from_checkpoint`
    从该检查点继续训练'
- en: Min-SNR weighting
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小-SNR 加权
- en: The [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can
    help with training by rebalancing the loss to achieve faster convergence. The
    training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR
    is compatible with both prediction types. This weighting strategy is only supported
    by PyTorch and is unavailable in the Flax training script.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[最小-SNR](https://huggingface.co/papers/2303.09556) 加权策略可以通过重新平衡损失来帮助训练，以实现更快的收敛。训练脚本支持预测
    `epsilon`（噪声）或 `v_prediction`，但最小-SNR 与两种预测类型兼容。这种加权策略仅受 PyTorch 支持，在 Flax 训练脚本中不可用。'
- en: 'Add the `--snr_gamma` parameter and set it to the recommended value of 5.0:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 添加 `--snr_gamma` 参数，并将其设置为推荐值 5.0：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can compare the loss surfaces for different `snr_gamma` values in this [Weights
    and Biases](https://wandb.ai/sayakpaul/text2image-finetune-minsnr) report. For
    smaller datasets, the effects of Min-SNR may not be as obvious compared to larger
    datasets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这个 [Weights and Biases](https://wandb.ai/sayakpaul/text2image-finetune-minsnr)
    报告中比较不同 `snr_gamma` 值的损失曲面。对于较小的数据集，与较大数据集相比，最小-SNR 的影响可能不那么明显。
- en: Training script
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练脚本
- en: The dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L490)
    function. If you need to adapt the training script, this is where you’ll need
    to make your changes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集预处理代码和训练循环在 [`main()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L490)
    函数中。如果需要调整训练脚本，这就是您需要进行更改的地方。
- en: 'The `train_text_to_image` script starts by [loading a scheduler](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L543)
    and tokenizer. You can choose to use a different scheduler here if you want:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_text_to_image` 脚本首先[加载调度器](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L543)和分词器。如果需要，您可以选择在这里使用不同的调度器：'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then the script [loads the UNet](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L619)
    model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后脚本[加载 UNet](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L619)
    模型：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, the text and image columns of the dataset need to be preprocessed. The
    [`tokenize_captions`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L724)
    function handles tokenizing the inputs, and the [`train_transforms`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L742)
    function specifies the type of transforms to apply to the image. Both of these
    functions are bundled into `preprocess_train`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，需要对数据集的文本和图像列进行预处理。[`tokenize_captions`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L724)
    函数处理输入的分词，[`train_transforms`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L742)
    函数指定要应用于图像的转换类型。这两个函数都打包到 `preprocess_train` 中：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Lastly, the [training loop](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L878)
    handles everything else. It encodes images into latent space, adds noise to the
    latents, computes the text embeddings to condition on, updates the model parameters,
    and saves and pushes the model to the Hub. If you want to learn more about how
    the training loop works, check out the [Understanding pipelines, models and schedulers](../using-diffusers/write_own_pipeline)
    tutorial which breaks down the basic pattern of the denoising process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，[训练循环](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L878)
    处理其他所有事务。它将图像编码为潜在空间，向潜在空间添加噪声，计算文本嵌入以进行条件化，更新模型参数，并将模型保存并推送到 Hub。如果您想了解训练循环的工作原理，请查看
    [了解管道、模型和调度器](../using-diffusers/write_own_pipeline) 教程，该教程详细介绍了去噪过程的基本模式。
- en: Launch the script
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动脚本
- en: Once you’ve made all your changes or you’re okay with the default configuration,
    you’re ready to launch the training script! 🚀
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成所有更改或对默认配置满意，您就可以启动训练脚本了！🚀
- en: PyTorchFlax
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFlax
- en: Let’s train on the [Pokémon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
    dataset to generate your own Pokémon. Set the environment variables `MODEL_NAME`
    and `dataset_name` to the model and the dataset (either from the Hub or a local
    path). If you’re training on more than one GPU, add the `--multi_gpu` parameter
    to the `accelerate launch` command.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 [Pokémon BLIP 标题](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
    数据集上训练，生成您自己的 Pokémon。将环境变量 `MODEL_NAME` 和 `dataset_name` 设置为模型和数据集（来自 Hub 或本地路径）。如果您要在多个
    GPU 上进行训练，请在 `accelerate launch` 命令中添加 `--multi_gpu` 参数。
- en: To train on a local dataset, set the `TRAIN_DIR` and `OUTPUT_DIR` environment
    variables to the path of the dataset and where to save the model to.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地数据集上训练，请将 `TRAIN_DIR` 和 `OUTPUT_DIR` 环境变量设置为数据集的路径和要保存模型的位置。
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once training is complete, you can use your newly trained model for inference:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，您可以使用新训练的模型进行推理：
- en: PyTorchFlax
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFlax
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next steps
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'Congratulations on training your own text-to-image model! To learn more about
    how to use your new model, the following guides may be helpful:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 祝贺您训练成功自己的文本到图像模型！要了解如何使用您的新模型，以下指南可能会有所帮助：
- en: Learn how to [load LoRA weights](../using-diffusers/loading_adapters#LoRA) for
    inference if you trained your model with LoRA.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何在推理时[加载 LoRA 权重](../using-diffusers/loading_adapters#LoRA)，如果您使用 LoRA 训练了您的模型。
- en: Learn more about how certain parameters like guidance scale or techniques such
    as prompt weighting can help you control inference in the [Text-to-image](../using-diffusers/conditional_image_generation)
    task guide.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解更多关于如何通过指导尺度等参数或技术，如提示加权，来帮助您控制在[文本到图像](../using-diffusers/conditional_image_generation)任务指南中的推理。
