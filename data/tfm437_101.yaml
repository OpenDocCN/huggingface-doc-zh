- en: BERTology
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERTology
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/bertology](https://huggingface.co/docs/transformers/v4.37.2/en/bertology)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/bertology](https://huggingface.co/docs/transformers/v4.37.2/en/bertology)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a growing field of study concerned with investigating the inner working
    of large-scale transformers like BERT (that some call “BERTology”). Some good
    examples of this field are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个不断增长的研究领域，关注调查大规模变压器（如BERT）的内部工作（有些人称之为“BERTology”）。 这个领域的一些很好的例子是：
- en: 'BERT Rediscovers the Classical NLP Pipeline by Ian Tenney, Dipanjan Das, Ellie
    Pavlick: [https://arxiv.org/abs/1905.05950](https://arxiv.org/abs/1905.05950)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT重新发现了经典的NLP流程 作者：Ian Tenney，Dipanjan Das，Ellie Pavlick：[https://arxiv.org/abs/1905.05950](https://arxiv.org/abs/1905.05950)
- en: 'Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, Graham
    Neubig: [https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 十六个头真的比一个好吗？ 作者：Paul Michel，Omer Levy，Graham Neubig：[https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)
- en: 'What Does BERT Look At? An Analysis of BERT’s Attention by Kevin Clark, Urvashi
    Khandelwal, Omer Levy, Christopher D. Manning: [https://arxiv.org/abs/1906.04341](https://arxiv.org/abs/1906.04341)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT看什么？ 作者：Kevin Clark，Urvashi Khandelwal，Omer Levy，Christopher D. Manning：[https://arxiv.org/abs/1906.04341](https://arxiv.org/abs/1906.04341)
- en: 'CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for
    Programming Language Attend Code Structure: [https://arxiv.org/abs/2210.04633](https://arxiv.org/abs/2210.04633)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CAT探测：一种基于度量的方法，解释预训练模型如何关注代码结构：[https://arxiv.org/abs/2210.04633](https://arxiv.org/abs/2210.04633)
- en: 'In order to help this new field develop, we have included a few additional
    features in the BERT/GPT/GPT-2 models to help people access the inner representations,
    mainly adapted from the great work of Paul Michel ([https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助这个新领域发展，我们在BERT/GPT/GPT-2模型中添加了一些额外功能，以帮助人们访问内部表示，主要是从Paul Michel的伟大工作中改编的（[https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)）：
- en: accessing all the hidden-states of BERT/GPT/GPT-2,
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问BERT/GPT/GPT-2的所有隐藏状态，
- en: accessing all the attention weights for each head of BERT/GPT/GPT-2,
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问BERT/GPT/GPT-2每个头的所有注意权重，
- en: retrieving heads output values and gradients to be able to compute head importance
    score and prune head as explained in [https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索头输出值和梯度，以便计算头重要性分数并修剪头，如[https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)中所解释的。
- en: 'To help you understand and use these features, we have added a specific example
    script: [bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)
    while extract information and prune a model pre-trained on GLUE.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您理解和使用这些功能，我们添加了一个特定的示例脚本：[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)，同时提取在GLUE上预训练的模型的信息并修剪。
