- en: Quiz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ‹éªŒ
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/quiz](https://huggingface.co/learn/deep-rl-course/unit4/quiz)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/learn/deep-rl-course/unit4/quiz](https://huggingface.co/learn/deep-rl-course/unit4/quiz)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)
    **is to test yourself.** This will help you to find **where you need to reinforce
    your knowledge**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ å’Œ[é¿å…èƒ½åŠ›é”™è§‰](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)çš„æœ€ä½³æ–¹æ³•**æ˜¯æµ‹è¯•è‡ªå·±**ã€‚è¿™å°†å¸®åŠ©æ‚¨æ‰¾åˆ°**éœ€è¦åŠ å¼ºçŸ¥è¯†çš„åœ°æ–¹**ã€‚
- en: 'Q1: What are the advantages of policy-gradient over value-based methods? (Check
    all that apply)'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q1: æ”¿ç­–æ¢¯åº¦ç›¸å¯¹äºåŸºäºä»·å€¼çš„æ–¹æ³•çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆé€‰æ‹©æ‰€æœ‰é€‚ç”¨çš„é€‰é¡¹ï¼‰'
- en: 'Q2: What is the Policy Gradient Theorem?'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q2: ä»€ä¹ˆæ˜¯æ”¿ç­–æ¢¯åº¦å®šç†ï¼Ÿ'
- en: <details data-svelte-h="svelte-nhcd05"><summary>Solution</summary>
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-nhcd05"><summary>è§£å†³æ–¹æ¡ˆ</summary>
- en: '*The Policy Gradient Theorem* is a formula that will help us to reformulate
    the objective function into a differentiable function that does not involve the
    differentiation of the state distribution.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ”¿ç­–æ¢¯åº¦å®šç†*æ˜¯ä¸€ä¸ªå…¬å¼ï¼Œå°†å¸®åŠ©æˆ‘ä»¬å°†ç›®æ ‡å‡½æ•°é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªä¸æ¶‰åŠçŠ¶æ€åˆ†å¸ƒå¾®åˆ†çš„å¯å¾®å‡½æ•°ã€‚'
- en: '![Policy Gradient](../Images/5a9b6c1a3ee9cf5b0e888fb819446af5.png)</details>'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![æ”¿ç­–æ¢¯åº¦](../Images/5a9b6c1a3ee9cf5b0e888fb819446af5.png)</details>'
- en: 'Q3: Whatâ€™s the difference between policy-based methods and policy-gradient
    methods? (Check all that apply)'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q3: æ”¿ç­–åŸºç¡€æ–¹æ³•å’Œæ”¿ç­–æ¢¯åº¦æ–¹æ³•ä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿï¼ˆé€‰æ‹©æ‰€æœ‰é€‚ç”¨çš„é€‰é¡¹ï¼‰'
- en: 'Q4: Why do we use gradient ascent instead of gradient descent to optimize J(Î¸)?'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q4: ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸Šå‡è€Œä¸æ˜¯æ¢¯åº¦ä¸‹é™æ¥ä¼˜åŒ– J(Î¸)ï¼Ÿ'
- en: Congrats on finishing this Quiz ğŸ¥³, if you missed some elements, take time to
    read the chapter again to reinforce (ğŸ˜) your knowledge.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œæ‚¨å®Œæˆäº†è¿™ä¸ªæµ‹éªŒğŸ¥³ï¼Œå¦‚æœæ‚¨é”™è¿‡äº†ä¸€äº›å…ƒç´ ï¼Œè¯·èŠ±æ—¶é—´å†æ¬¡é˜…è¯»ç« èŠ‚ä»¥åŠ å¼ºï¼ˆğŸ˜ï¼‰æ‚¨çš„çŸ¥è¯†ã€‚
