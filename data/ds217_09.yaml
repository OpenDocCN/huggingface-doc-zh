- en: Preprocess
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: 'Original text: [https://huggingface.co/docs/datasets/use_dataset](https://huggingface.co/docs/datasets/use_dataset)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/use_dataset](https://huggingface.co/docs/datasets/use_dataset)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In addition to loading datasets, ğŸ¤— Datasets other main goal is to offer a diverse
    set of preprocessing functions to get a dataset into an appropriate format for
    training with your machine learning framework.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†åŠ è½½æ•°æ®é›†å¤–ï¼ŒğŸ¤— æ•°æ®é›†çš„å¦ä¸€ä¸ªä¸»è¦ç›®æ ‡æ˜¯æä¾›å¤šæ ·åŒ–çš„é¢„å¤„ç†å‡½æ•°ï¼Œä»¥ä½¿æ•°æ®é›†ä»¥é€‚åˆæ‚¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒçš„æ ¼å¼ã€‚
- en: 'There are many possible ways to preprocess a dataset, and it all depends on
    your specific dataset. Sometimes you may need to rename a column, and other times
    you might need to unflatten nested fields. ğŸ¤— Datasets provides a way to do most
    of these things. But in nearly all preprocessing cases, depending on your dataset
    modality, youâ€™ll need to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤šå¯èƒ½çš„æ•°æ®é›†é¢„å¤„ç†æ–¹å¼ï¼Œè¿™å®Œå…¨å–å†³äºæ‚¨çš„å…·ä½“æ•°æ®é›†ã€‚æœ‰æ—¶æ‚¨å¯èƒ½éœ€è¦é‡å‘½ååˆ—ï¼Œè€Œå…¶ä»–æ—¶å€™æ‚¨å¯èƒ½éœ€è¦å±•å¼€åµŒå¥—å­—æ®µã€‚ğŸ¤— æ•°æ®é›†æä¾›äº†æ‰§è¡Œè¿™äº›å¤§éƒ¨åˆ†æ“ä½œçš„æ–¹æ³•ã€‚ä½†åœ¨å‡ ä¹æ‰€æœ‰é¢„å¤„ç†æƒ…å†µä¸‹ï¼Œæ ¹æ®æ‚¨çš„æ•°æ®é›†æ¨¡æ€æ€§ï¼Œæ‚¨éœ€è¦ï¼š
- en: Tokenize a text dataset.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬æ•°æ®é›†è¿›è¡Œæ ‡è®°åŒ–ã€‚
- en: Resample an audio dataset.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡æ–°é‡‡æ ·éŸ³é¢‘æ•°æ®é›†ã€‚
- en: Apply transforms to an image dataset.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹å›¾åƒæ•°æ®é›†åº”ç”¨è½¬æ¢ã€‚
- en: The last preprocessing step is usually setting your dataset format to be compatible
    with your machine learning frameworkâ€™s expected input format.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åçš„é¢„å¤„ç†æ­¥éª¤é€šå¸¸æ˜¯å°†æ•°æ®é›†æ ¼å¼è®¾ç½®ä¸ºä¸æ‚¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶æœŸæœ›çš„è¾“å…¥æ ¼å¼å…¼å®¹ã€‚
- en: 'In this tutorial, youâ€™ll also need to install the ğŸ¤— Transformers library:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨è¿˜éœ€è¦å®‰è£…ğŸ¤— Transformersåº“ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Grab a dataset of your choice and follow along!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªæ•°æ®é›†å¹¶è·Ÿéšæ“ä½œï¼
- en: Tokenize text
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ–‡æœ¬
- en: Models cannot process raw text, so youâ€™ll need to convert the text into numbers.
    Tokenization provides a way to do this by dividing text into individual words
    called *tokens*. Tokens are finally converted to numbers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ— æ³•å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå› æ­¤æ‚¨éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ã€‚é€šè¿‡å°†æ–‡æœ¬åˆ†æˆç§°ä¸º*æ ‡è®°*çš„å•è¯ï¼Œåˆ†è¯æä¾›äº†ä¸€ç§æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æœ€ç»ˆå°†æ ‡è®°è½¬æ¢ä¸ºæ•°å­—ã€‚
- en: Check out the [Tokenizers](https://huggingface.co/course/chapter2/4?fw=pt) section
    in Chapter 2 of the Hugging Face course to learn more about tokenization and different
    tokenization algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹Hugging Faceè¯¾ç¨‹ç¬¬2ç« ä¸­çš„[Tokenizers](https://huggingface.co/course/chapter2/4?fw=pt)éƒ¨åˆ†ï¼Œäº†è§£æœ‰å…³æ ‡è®°åŒ–å’Œä¸åŒæ ‡è®°åŒ–ç®—æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '**1**. Start by loading the [rotten_tomatoes](https://huggingface.co/datasets/rotten_tomatoes)
    dataset and the tokenizer corresponding to a pretrained [BERT](https://huggingface.co/bert-base-uncased)
    model. Using the same tokenizer as the pretrained model is important because you
    want to make sure the text is split in the same way.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**1**. é¦–å…ˆåŠ è½½[rotten_tomatoes](https://huggingface.co/datasets/rotten_tomatoes)æ•°æ®é›†å’Œä¸é¢„è®­ç»ƒçš„[BERT](https://huggingface.co/bert-base-uncased)æ¨¡å‹å¯¹åº”çš„åˆ†è¯å™¨ã€‚ä½¿ç”¨ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸åŒçš„åˆ†è¯å™¨å¾ˆé‡è¦ï¼Œå› ä¸ºæ‚¨å¸Œæœ›ç¡®ä¿æ–‡æœ¬ä»¥ç›¸åŒçš„æ–¹å¼åˆ†å‰²ã€‚'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**2**. Call your tokenizer on the first row of `text` in the dataset:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**2**. åœ¨æ•°æ®é›†ä¸­çš„`text`çš„ç¬¬ä¸€è¡Œä¸Šè°ƒç”¨æ‚¨çš„åˆ†è¯å™¨ï¼š'
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The tokenizer returns a dictionary with three items:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé¡¹ç›®çš„å­—å…¸ï¼š
- en: '`input_ids`: the numbers representing the tokens in the text.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`: ä»£è¡¨æ–‡æœ¬ä¸­æ ‡è®°çš„æ•°å­—ã€‚'
- en: '`token_type_ids`: indicates which sequence a token belongs to if there is more
    than one sequence.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`: å¦‚æœæœ‰å¤šä¸ªåºåˆ—ï¼ŒæŒ‡ç¤ºä¸€ä¸ªæ ‡è®°å±äºå“ªä¸ªåºåˆ—ã€‚'
- en: '`attention_mask`: indicates whether a token should be masked or not.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`: æŒ‡ç¤ºæ ‡è®°æ˜¯å¦åº”è¯¥è¢«å±è”½ã€‚'
- en: These values are actually the model inputs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å€¼å®é™…ä¸Šæ˜¯æ¨¡å‹çš„è¾“å…¥ã€‚
- en: '**3**. The fastest way to tokenize your entire dataset is to use the [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    function. This function speeds up tokenization by applying the tokenizer to batches
    of examples instead of individual examples. Set the `batched` parameter to `True`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**3**. å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ ‡è®°åŒ–çš„æœ€å¿«æ–¹æ³•æ˜¯ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°ã€‚è¯¥å‡½æ•°é€šè¿‡å°†åˆ†è¯å™¨åº”ç”¨äºç¤ºä¾‹æ‰¹æ¬¡è€Œä¸æ˜¯å•ä¸ªç¤ºä¾‹æ¥åŠ å¿«æ ‡è®°åŒ–é€Ÿåº¦ã€‚å°†`batched`å‚æ•°è®¾ç½®ä¸º`True`ï¼š'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**4**. Set the format of your dataset to be compatible with your machine learning
    framework:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**4**. å°†æ•°æ®é›†æ ¼å¼è®¾ç½®ä¸ºä¸æ‚¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶å…¼å®¹ï¼š'
- en: PytorchHide Pytorch content
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—Pytorchå†…å®¹
- en: 'Use the [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)
    function to set the dataset format to be compatible with PyTorch:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)å‡½æ•°å°†æ•°æ®é›†æ ¼å¼è®¾ç½®ä¸ºä¸PyTorchå…¼å®¹ï¼š
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: TensorFlowHide TensorFlow content
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: 'Use the [to_tf_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
    function to set the dataset format to be compatible with TensorFlow. Youâ€™ll also
    need to import a [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding)
    from ğŸ¤— Transformers to combine the varying sequence lengths into a single batch
    of equal lengths:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[to_tf_dataset()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)å‡½æ•°å°†æ•°æ®é›†æ ¼å¼è®¾ç½®ä¸ºä¸TensorFlowå…¼å®¹ã€‚æ‚¨è¿˜éœ€è¦ä»ğŸ¤—
    Transformerså¯¼å…¥ä¸€ä¸ª[æ•°æ®æ”¶é›†å™¨](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding)ï¼Œå°†ä¸åŒé•¿åº¦çš„åºåˆ—ç»„åˆæˆä¸€ä¸ªç›¸åŒé•¿åº¦çš„æ‰¹æ¬¡ï¼š
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**5**. The dataset is now ready for training with your machine learning framework!'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**5**. æ•°æ®é›†ç°åœ¨å·²å‡†å¤‡å¥½ä¸æ‚¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼'
- en: Resample audio signals
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡æ–°é‡‡æ ·éŸ³é¢‘ä¿¡å·
- en: Audio inputs like text datasets need to be divided into discrete data points.
    This is known as *sampling*; the sampling rate tells you how much of the speech
    signal is captured per second. It is important to make sure the sampling rate
    of your dataset matches the sampling rate of the data used to pretrain the model
    youâ€™re using. If the sampling rates are different, the pretrained model may perform
    poorly on your dataset because it doesnâ€™t recognize the differences in the sampling
    rate.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åƒæ–‡æœ¬æ•°æ®é›†ä¸€æ ·ï¼ŒéŸ³é¢‘è¾“å…¥éœ€è¦è¢«åˆ’åˆ†ä¸ºç¦»æ•£æ•°æ®ç‚¹ã€‚è¿™è¢«ç§°ä¸º*é‡‡æ ·*ï¼›é‡‡æ ·ç‡å‘Šè¯‰æ‚¨æ¯ç§’æ•è·äº†å¤šå°‘è¯­éŸ³ä¿¡å·ã€‚é‡è¦çš„æ˜¯ç¡®ä¿æ•°æ®é›†çš„é‡‡æ ·ç‡ä¸ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®çš„é‡‡æ ·ç‡åŒ¹é…ã€‚å¦‚æœé‡‡æ ·ç‡ä¸åŒï¼Œé¢„è®­ç»ƒæ¨¡å‹å¯èƒ½åœ¨æ‚¨çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒæ— æ³•è¯†åˆ«é‡‡æ ·ç‡çš„å·®å¼‚ã€‚
- en: '**1**. Start by loading the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
    dataset, the [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature, and the feature extractor corresponding to a pretrained [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)
    model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**1**. é¦–å…ˆåŠ è½½[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ï¼Œ[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)ç‰¹å¾ï¼Œä»¥åŠå¯¹åº”é¢„è®­ç»ƒçš„[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)æ¨¡å‹çš„ç‰¹å¾æå–å™¨ï¼š'
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**2**. Index into the first row of the dataset. When you call the `audio` column
    of the dataset, it is automatically decoded and resampled:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**2**. å¯¹æ•°æ®é›†çš„ç¬¬ä¸€è¡Œè¿›è¡Œç´¢å¼•ã€‚å½“æ‚¨è°ƒç”¨æ•°æ®é›†çš„`audio`åˆ—æ—¶ï¼Œå®ƒä¼šè¢«è‡ªåŠ¨è§£ç å’Œé‡æ–°é‡‡æ ·ï¼š'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**3**. Reading a dataset card is incredibly useful and can give you a lot of
    information about the dataset. A quick look at the MInDS-14 dataset card tells
    you the sampling rate is 8kHz. Likewise, you can get many details about a model
    from its model card. The Wav2Vec2 model card says it was sampled on 16kHz speech
    audio. This means youâ€™ll need to upsample the MInDS-14 dataset to match the sampling
    rate of the model.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**3**. é˜…è¯»æ•°æ®é›†å¡ç‰‡éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›å…³äºæ•°æ®é›†çš„è®¸å¤šä¿¡æ¯ã€‚å¿«é€ŸæŸ¥çœ‹MInDS-14æ•°æ®é›†å¡ç‰‡å‘Šè¯‰æ‚¨é‡‡æ ·ç‡ä¸º8kHzã€‚åŒæ ·ï¼Œæ‚¨å¯ä»¥ä»æ¨¡å‹å¡ç‰‡ä¸­è·å–æœ‰å…³æ¨¡å‹çš„è®¸å¤šç»†èŠ‚ã€‚Wav2Vec2æ¨¡å‹å¡ç‰‡æ˜¾ç¤ºå…¶åœ¨16kHzè¯­éŸ³éŸ³é¢‘ä¸Šé‡‡æ ·ã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦å°†MInDS-14æ•°æ®é›†ä¸Šé‡‡æ ·ä»¥åŒ¹é…æ¨¡å‹çš„é‡‡æ ·ç‡ã€‚'
- en: 'Use the [cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast_column)
    function and set the `sampling_rate` parameter in the [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature to upsample the audio signal. When you call the `audio` column now, it
    is decoded and resampled to 16kHz:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast_column)å‡½æ•°ï¼Œå¹¶åœ¨[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)ç‰¹å¾ä¸­è®¾ç½®`sampling_rate`å‚æ•°æ¥ä¸Šé‡‡æ ·éŸ³é¢‘ä¿¡å·ã€‚å½“æ‚¨ç°åœ¨è°ƒç”¨`audio`åˆ—æ—¶ï¼Œå®ƒä¼šè¢«è§£ç å¹¶é‡æ–°é‡‡æ ·ä¸º16kHzï¼š
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**4**. Use the [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    function to resample the entire dataset to 16kHz. This function speeds up resampling
    by applying the feature extractor to batches of examples instead of individual
    examples. Set the `batched` parameter to `True`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**4**. ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°å°†æ•´ä¸ªæ•°æ®é›†é‡æ–°é‡‡æ ·ä¸º16kHzã€‚æ­¤å‡½æ•°é€šè¿‡å°†ç‰¹å¾æå–å™¨åº”ç”¨äºæ‰¹é‡ç¤ºä¾‹è€Œä¸æ˜¯å•ä¸ªç¤ºä¾‹æ¥åŠ å¿«é‡æ–°é‡‡æ ·é€Ÿåº¦ã€‚å°†`batched`å‚æ•°è®¾ç½®ä¸º`True`ï¼š'
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**5**. The dataset is now ready for training with your machine learning framework!'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**5**. ç°åœ¨æ•°æ®é›†å·²å‡†å¤‡å¥½ä¸æ‚¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼'
- en: Apply data augmentations
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åº”ç”¨æ•°æ®å¢å¼º
- en: The most common preprocessing youâ€™ll do with image datasets is *data augmentation*,
    a process that introduces random variations to an image without changing the meaning
    of the data. This can mean changing the color properties of an image or randomly
    cropping an image. You are free to use any data augmentation library you like,
    and ğŸ¤— Datasets will help you apply your data augmentations to your dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†å¯¹å›¾åƒæ•°æ®é›†è¿›è¡Œçš„æœ€å¸¸è§çš„é¢„å¤„ç†æ˜¯*æ•°æ®å¢å¼º*ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šåœ¨ä¸æ”¹å˜æ•°æ®å«ä¹‰çš„æƒ…å†µä¸‹å¼•å…¥å›¾åƒçš„éšæœºå˜åŒ–ã€‚è¿™å¯èƒ½æ„å‘³ç€æ”¹å˜å›¾åƒçš„é¢œè‰²å±æ€§æˆ–éšæœºè£å‰ªå›¾åƒã€‚æ‚¨å¯ä»¥è‡ªç”±é€‰æ‹©ä»»ä½•æ‚¨å–œæ¬¢çš„æ•°æ®å¢å¼ºåº“ï¼ŒğŸ¤—
    Datasetså°†å¸®åŠ©æ‚¨å°†æ•°æ®å¢å¼ºåº”ç”¨åˆ°æ‚¨çš„æ•°æ®é›†ä¸Šã€‚
- en: '**1**. Start by loading the [Beans](https://huggingface.co/datasets/beans)
    dataset, the `Image` feature, and the feature extractor corresponding to a pretrained
    [ViT](https://huggingface.co/google/vit-base-patch16-224-in21k) model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**1**. é¦–å…ˆåŠ è½½[Beans](https://huggingface.co/datasets/beans)æ•°æ®é›†ï¼Œ`Image`ç‰¹å¾ï¼Œä»¥åŠå¯¹åº”é¢„è®­ç»ƒçš„[ViT](https://huggingface.co/google/vit-base-patch16-224-in21k)æ¨¡å‹çš„ç‰¹å¾æå–å™¨ï¼š'
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**2**. Index into the first row of the dataset. When you call the `image` column
    of the dataset, the underlying PIL object is automatically decoded into an image.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**2**. å¯¹æ•°æ®é›†çš„ç¬¬ä¸€è¡Œè¿›è¡Œç´¢å¼•ã€‚å½“æ‚¨è°ƒç”¨æ•°æ®é›†çš„`image`åˆ—æ—¶ï¼Œåº•å±‚çš„PILå¯¹è±¡ä¼šè‡ªåŠ¨è§£ç ä¸ºå›¾åƒã€‚'
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**3**. Now, you can apply some transforms to the image. Feel free to take a
    look at the [various transforms available](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)
    in torchvision and choose one youâ€™d like to experiment with. This example applies
    a transform that randomly rotates the image:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**3**. ç°åœ¨ï¼Œæ‚¨å¯ä»¥å¯¹å›¾åƒåº”ç”¨ä¸€äº›å˜æ¢ã€‚éšæ„æŸ¥çœ‹torchvisionä¸­æä¾›çš„[å„ç§å¯ç”¨å˜æ¢](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)ï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªæ‚¨æƒ³è¦å°è¯•çš„å˜æ¢ã€‚è¿™ä¸ªç¤ºä¾‹åº”ç”¨äº†ä¸€ä¸ªéšæœºæ—‹è½¬å›¾åƒçš„å˜æ¢ï¼š'
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**4**. Use the [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)
    function to apply the transform on-the-fly. When you index into the image `pixel_values`,
    the transform is applied, and your image gets rotated.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**4**. ä½¿ç”¨[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)å‡½æ•°æ¥å®æ—¶åº”ç”¨å˜æ¢ã€‚å½“æ‚¨ç´¢å¼•åˆ°å›¾åƒçš„`pixel_values`æ—¶ï¼Œå˜æ¢ä¼šè¢«åº”ç”¨ï¼Œæ‚¨çš„å›¾åƒä¼šè¢«æ—‹è½¬ã€‚'
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**5**. The dataset is now ready for training with your machine learning framework!'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**5**. ç°åœ¨æ•°æ®é›†å·²å‡†å¤‡å¥½ä¸æ‚¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼'
