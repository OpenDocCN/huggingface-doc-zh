- en: Training from memory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从内存中训练
- en: 'Original text: [https://huggingface.co/docs/tokenizers/training_from_memory](https://huggingface.co/docs/tokenizers/training_from_memory)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/tokenizers/training_from_memory](https://huggingface.co/docs/tokenizers/training_from_memory)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In the [Quicktour](quicktour), we saw how to build and train a tokenizer using
    text files, but we can actually use any Python Iterator. In this section we’ll
    see a few different ways of training our tokenizer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[快速入门](quicktour)中，我们看到了如何使用文本文件构建和训练分词器，但实际上我们可以使用任何Python迭代器。在本节中，我们将看到几种不同的训练分词器的方法。
- en: 'For all the examples listed below, we’ll use the same [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    and `Trainer`, built as following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下面列出的所有示例，我们将使用相同的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)和`Trainer`，构建如下：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This tokenizer is based on the [Unigram](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Unigram)
    model. It takes care of normalizing the input using the NFKC Unicode normalization
    method, and uses a [ByteLevel](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)
    pre-tokenizer with the corresponding decoder.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器基于[Unigram](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Unigram)模型。它通过NFKC
    Unicode规范化方法对输入进行处理，并使用具有相应解码器的[ByteLevel](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)预分词器。
- en: For more information on the components used here, you can check [here](components).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此处使用的组件的更多信息，您可以在[此处](components)查看。
- en: The most basic way
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最基本的方式
- en: 'As you probably guessed already, the easiest way to train our tokenizer is
    by using a `List`{.interpreted-text role=“obj”}:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经猜到了，训练我们的分词器最简单的方法是使用`List`{.interpreted-text role=“obj”}：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Easy, right? You can use anything working as an iterator here, be it a `List`{.interpreted-text
    role=“obj”}, `Tuple`{.interpreted-text role=“obj”}, or a `np.Array`{.interpreted-text
    role=“obj”}. Anything works as long as it provides strings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简单，对吧？您可以在这里使用任何作为迭代器的东西，无论是`List`{.interpreted-text role=“obj”}、`Tuple`{.interpreted-text
    role=“obj”}还是`np.Array`{.interpreted-text role=“obj”}。只要提供字符串，任何东西都可以使用。
- en: Using the 🤗 Datasets library
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用🤗数据集库
- en: An awesome way to access one of the many datasets that exist out there is by
    using the 🤗 Datasets library. For more information about it, you should check
    [the official documentation here](https://huggingface.co/docs/datasets/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 访问众多现有数据集之一的绝佳方式是使用🤗数据集库。有关更多信息，请查看[此处的官方文档](https://huggingface.co/docs/datasets/)。
- en: 'Let’s start by loading our dataset:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据集开始：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to build an iterator over this dataset. The easiest way to
    do this is probably by using a generator:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建一个在这个数据集上的迭代器。可能最简单的方法是使用生成器：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see here, for improved efficiency we can actually provide a batch
    of examples used to train, instead of iterating over them one by one. By doing
    so, we can expect performances very similar to those we got while training directly
    from files.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这里所看到的，为了提高效率，我们实际上可以提供一批用于训练的示例，而不是逐个迭代它们。通过这样做，我们可以期望获得与直接从文件训练时相似的性能。
- en: 'With our iterator ready, we just need to launch the training. In order to improve
    the look of our progress bars, we can specify the total length of the dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有了准备好的迭代器，我们只需要启动训练。为了改善进度条的外观，我们可以指定数据集的总长度：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And that’s it!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！
- en: Using gzip files
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用gzip文件
- en: 'Since gzip files in Python can be used as iterators, it is extremely simple
    to train on such files:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Python中的gzip文件可以用作迭代器，因此在这些文件上进行训练非常简单：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now if we wanted to train from multiple gzip files, it wouldn’t be much harder:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想要从多个gzip文件进行训练，那也不会更难：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And voilà!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后就完成了！
