- en: Training from memory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»å†…å­˜ä¸­è®­ç»ƒ
- en: 'Original text: [https://huggingface.co/docs/tokenizers/training_from_memory](https://huggingface.co/docs/tokenizers/training_from_memory)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/tokenizers/training_from_memory](https://huggingface.co/docs/tokenizers/training_from_memory)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In the [Quicktour](quicktour), we saw how to build and train a tokenizer using
    text files, but we can actually use any Python Iterator. In this section weâ€™ll
    see a few different ways of training our tokenizer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[å¿«é€Ÿå…¥é—¨](quicktour)ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶æ„å»ºå’Œè®­ç»ƒåˆ†è¯å™¨ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•Pythonè¿­ä»£å™¨ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å‡ ç§ä¸åŒçš„è®­ç»ƒåˆ†è¯å™¨çš„æ–¹æ³•ã€‚
- en: 'For all the examples listed below, weâ€™ll use the same [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
    and `Trainer`, built as following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸‹é¢åˆ—å‡ºçš„æ‰€æœ‰ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)å’Œ`Trainer`ï¼Œæ„å»ºå¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This tokenizer is based on the [Unigram](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Unigram)
    model. It takes care of normalizing the input using the NFKC Unicode normalization
    method, and uses a [ByteLevel](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)
    pre-tokenizer with the corresponding decoder.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ†è¯å™¨åŸºäº[Unigram](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Unigram)æ¨¡å‹ã€‚å®ƒé€šè¿‡NFKC
    Unicodeè§„èŒƒåŒ–æ–¹æ³•å¯¹è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œå¹¶ä½¿ç”¨å…·æœ‰ç›¸åº”è§£ç å™¨çš„[ByteLevel](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)é¢„åˆ†è¯å™¨ã€‚
- en: For more information on the components used here, you can check [here](components).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ­¤å¤„ä½¿ç”¨çš„ç»„ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥åœ¨[æ­¤å¤„](components)æŸ¥çœ‹ã€‚
- en: The most basic way
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€åŸºæœ¬çš„æ–¹å¼
- en: 'As you probably guessed already, the easiest way to train our tokenizer is
    by using a `List`{.interpreted-text role=â€œobjâ€}:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½å·²ç»çŒœåˆ°äº†ï¼Œè®­ç»ƒæˆ‘ä»¬çš„åˆ†è¯å™¨æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨`List`{.interpreted-text role=â€œobjâ€}ï¼š
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Easy, right? You can use anything working as an iterator here, be it a `List`{.interpreted-text
    role=â€œobjâ€}, `Tuple`{.interpreted-text role=â€œobjâ€}, or a `np.Array`{.interpreted-text
    role=â€œobjâ€}. Anything works as long as it provides strings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•ï¼Œå¯¹å§ï¼Ÿæ‚¨å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ä»»ä½•ä½œä¸ºè¿­ä»£å™¨çš„ä¸œè¥¿ï¼Œæ— è®ºæ˜¯`List`{.interpreted-text role=â€œobjâ€}ã€`Tuple`{.interpreted-text
    role=â€œobjâ€}è¿˜æ˜¯`np.Array`{.interpreted-text role=â€œobjâ€}ã€‚åªè¦æä¾›å­—ç¬¦ä¸²ï¼Œä»»ä½•ä¸œè¥¿éƒ½å¯ä»¥ä½¿ç”¨ã€‚
- en: Using the ğŸ¤— Datasets library
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“
- en: An awesome way to access one of the many datasets that exist out there is by
    using the ğŸ¤— Datasets library. For more information about it, you should check
    [the official documentation here](https://huggingface.co/docs/datasets/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è®¿é—®ä¼—å¤šç°æœ‰æ•°æ®é›†ä¹‹ä¸€çš„ç»ä½³æ–¹å¼æ˜¯ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ­¤å¤„çš„å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/datasets/)ã€‚
- en: 'Letâ€™s start by loading our dataset:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»åŠ è½½æ•°æ®é›†å¼€å§‹ï¼š
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to build an iterator over this dataset. The easiest way to
    do this is probably by using a generator:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šçš„è¿­ä»£å™¨ã€‚å¯èƒ½æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨ç”Ÿæˆå™¨ï¼š
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see here, for improved efficiency we can actually provide a batch
    of examples used to train, instead of iterating over them one by one. By doing
    so, we can expect performances very similar to those we got while training directly
    from files.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨åœ¨è¿™é‡Œæ‰€çœ‹åˆ°çš„ï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥æä¾›ä¸€æ‰¹ç”¨äºè®­ç»ƒçš„ç¤ºä¾‹ï¼Œè€Œä¸æ˜¯é€ä¸ªè¿­ä»£å®ƒä»¬ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›è·å¾—ä¸ç›´æ¥ä»æ–‡ä»¶è®­ç»ƒæ—¶ç›¸ä¼¼çš„æ€§èƒ½ã€‚
- en: 'With our iterator ready, we just need to launch the training. In order to improve
    the look of our progress bars, we can specify the total length of the dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†å‡†å¤‡å¥½çš„è¿­ä»£å™¨ï¼Œæˆ‘ä»¬åªéœ€è¦å¯åŠ¨è®­ç»ƒã€‚ä¸ºäº†æ”¹å–„è¿›åº¦æ¡çš„å¤–è§‚ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ•°æ®é›†çš„æ€»é•¿åº¦ï¼š
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And thatâ€™s it!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼
- en: Using gzip files
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨gzipæ–‡ä»¶
- en: 'Since gzip files in Python can be used as iterators, it is extremely simple
    to train on such files:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºPythonä¸­çš„gzipæ–‡ä»¶å¯ä»¥ç”¨ä½œè¿­ä»£å™¨ï¼Œå› æ­¤åœ¨è¿™äº›æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒéå¸¸ç®€å•ï¼š
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now if we wanted to train from multiple gzip files, it wouldnâ€™t be much harder:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ä»å¤šä¸ªgzipæ–‡ä»¶è¿›è¡Œè®­ç»ƒï¼Œé‚£ä¹Ÿä¸ä¼šæ›´éš¾ï¼š
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And voilÃ !
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°±å®Œæˆäº†ï¼
