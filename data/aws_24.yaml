- en: Adding support for new architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为新架构添加支持
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/community/contributing](https://huggingface.co/docs/optimum-neuron/community/contributing)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/optimum-neuron/community/contributing](https://huggingface.co/docs/optimum-neuron/community/contributing)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '***NOTE:*** ❗This section does not apply to the decoder model’s inference with
    autoregressive sampling integrated through `transformers-neuronx`. If you want
    to add support for these models, please open an issue on the Optimum Neuron GitHub
    repo, and ping maintainers for help.'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意:*** ❗这一部分不适用于通过`transformers-neuronx`进行自回归采样的解码器模型的推断。如果您想为这些模型添加支持，请在Optimum
    Neuron GitHub存储库上开启一个问题，并@维护者寻求帮助。'
- en: You want to export and run a new model on AWS Inferentia or Trainium? Check
    the guideline, and submit a pull request to [🤗 Optimum Neuron’s GitHub repo](https://github.com/huggingface/optimum-neuron/)!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您想在AWS Inferentia或Trainium上导出并运行新模型吗？查看指南，并向[🤗 Optimum Neuron的GitHub存储库](https://github.com/huggingface/optimum-neuron/)提交拉取请求！
- en: 'To support a new model architecture in the Optimum Neuron library here are
    some steps to follow:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Optimum Neuron库中支持新的模型架构，以下是一些要遵循的步骤：
- en: Implement a custom Neuron configuration.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现自定义神经元配置。
- en: Export and validate the model.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导出并验证模型。
- en: Contribute to the GitHub repo.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贡献到GitHub存储库。
- en: Implement a custom Neuron configuration
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自定义神经元配置
- en: 'To support the export of a new model to a Neuron compatible format, the first
    thing to do is to define a Neuron configuration, describing how to export the
    PyTorch model by specifying:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持将新模型导出为Neuron兼容格式，首先要做的是定义一个神经元配置，描述如何通过指定来导出PyTorch模型：
- en: The input names.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入名称。
- en: The output names.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出名称。
- en: 'The dummy inputs used to trace the model: the Neuron Compiler records the computational
    graph via tracing and works on the resulting `TorchScript` module.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于跟踪模型的虚拟输入：Neuron编译器通过跟踪记录计算图，并在生成的`TorchScript`模块上工作。
- en: The compilation arguments used to control the trade-off between hardware efficiency
    (latency, throughput) and accuracy.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于控制硬件效率（延迟、吞吐量）和准确性之间权衡的编译参数。
- en: Depending on the choice of model and task, we represent the data above with
    configuration classes. Each configuration class is associated with a specific
    model architecture, and follows the naming convention `ArchitectureNameNeuronConfig`.
    For instance, the configuration that specifies the Neuron export of BERT models
    is `BertNeuronConfig`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型和任务的选择，我们使用配置类表示上面的数据。每个配置类与特定的模型架构相关联，并遵循命名约定`ArchitectureNameNeuronConfig`。例如，指定BERT模型的Neuron导出的配置是`BertNeuronConfig`。
- en: 'Since many architectures share similar properties for their Neuron configuration,
    🤗 Optimum adopts a 3-level class hierarchy:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多架构共享类似的神经元配置属性，🤗 Optimum采用了3级类层次结构：
- en: Abstract and generic base classes. These handle all the fundamental features,
    while being agnostic to the modality (text, image, audio, etc).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抽象和通用的基类。这些处理所有基本特性，同时对模态（文本、图像、音频等）保持不可知。
- en: Middle-end classes. These are aware of the modality. Multiple config classes
    could exist for the same modality, depending on the inputs they support. They
    specify which input generators should be used for generating the dummy inputs,
    but remain model-agnostic.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 中间类。这些了解模态。对于同一模态，可能存在多个配置类，取决于它们支持的输入。它们指定应使用哪些输入生成器来生成虚拟输入，但保持与模型无关。
- en: Model-specific classes like the `BertNeuronConfig` mentioned above. These are
    the ones actually used to export models.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像上面提到的`BertNeuronConfig`这样的特定于模型的类。这些实际上是用于导出模型的类。
- en: 'Example: Adding support for ESM models'
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例：为ESM模型添加支持
- en: Here we take the support of [ESM models](https://huggingface.co/docs/transformers/model_doc/esm#esm)
    as an example. Let’s create an `EsmNeuronConfig` class in the `optimum/exporters/neuron/model_configs.py`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们以[ESM模型](https://huggingface.co/docs/transformers/model_doc/esm#esm)为例。让我们在`optimum/exporters/neuron/model_configs.py`中创建一个`EsmNeuronConfig`类。
- en: When an Esm model interprets as a text encoder, we are able to inherit from
    the middle-end class [`TextEncoderNeuronConfig`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/optimum/exporters/neuron/config.py#L36).
    Since the modeling and configuration of Esm is almost the same as BERT when it
    is interpreted as an encoder, we can use the `NormalizedConfigManager` with `model_type=bert`
    to normalize the configuration to generate dummy inputs for tracing the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个Esm模型被解释为文本编码器时，我们可以从中间类[`TextEncoderNeuronConfig`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/optimum/exporters/neuron/config.py#L36)继承。由于当Esm被解释为编码器时，建模和配置几乎与BERT相同，我们可以使用`NormalizedConfigManager`和`model_type=bert`来规范化配置，以生成用于跟踪模型的虚拟输入。
- en: And one last step, since `optimum-neuron` is an extension of `optimum`, we need
    to register the Neuron config that we create to the [TasksManager](https://huggingface.co/docs/optimum/main/en/exporters/task_manager#optimum.exporters.TasksManager)
    with the `register_in_tasks_manager` decorator by specifying the model type and
    supported tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，由于`optimum-neuron`是`optimum`的扩展，我们需要将创建的神经元配置注册到[TasksManager](https://huggingface.co/docs/optimum/main/en/exporters/task_manager#optimum.exporters.TasksManager)中，通过指定模型类型和支持的任务使用`register_in_tasks_manager`装饰器。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Export and validate the model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出并验证模型
- en: 'With the Neuron configuration class that you implemented, now do a quick test
    if it works as expected:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您实现的神经元配置类，现在快速测试一下是否按预期工作：
- en: Export
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导出
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'During the export [`validate_model_outputs`](https://github.com/huggingface/optimum-neuron/blob/7b18de9ddfa5c664c94051304c651eaf855c3e0b/optimum/exporters/neuron/convert.py#L136)
    will be called to validate the outputs of your exported Neuron model by comparing
    them to the results of PyTorch on the CPU. You could also validate the model manually
    with:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在导出期间，将调用[`validate_model_outputs`](https://github.com/huggingface/optimum-neuron/blob/7b18de9ddfa5c664c94051304c651eaf855c3e0b/optimum/exporters/neuron/convert.py#L136)来验证导出的Neuron模型的输出，通过将其与PyTorch在CPU上的结果进行比较。您也可以手动验证模型。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Inference (optional)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断（可选）
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Contribute to the GitHub repo
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贡献到GitHub仓库
- en: We are almost all set. Now submit a pull request to make your work accessible
    to all community members!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎已经准备就绪。现在提交一个拉取请求，使您的工作对所有社区成员可访问！
- en: Open an issue in the [Optimum Neuron GitHub repo](https://github.com/huggingface/optimum-neuron/issues)
    to describe the new feature and make it visible to Optimum Neuron’s maintainers.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[Optimum Neuron GitHub仓库](https://github.com/huggingface/optimum-neuron/issues)中开启一个问题来描述新功能，并让Optimum
    Neuron的维护者们看到。
- en: Add the model to the exporter test in [`optimum-neuron/tests/exporters/exporters_utils.py`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/tests/exporters/exporters_utils.py)
    and the inference test in [`optimum-neuron/tests/inference/inference_utils.py`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/tests/inference/inference_utils.py).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型添加到[`optimum-neuron/tests/exporters/exporters_utils.py`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/tests/exporters/exporters_utils.py)中的导出器测试和[`optimum-neuron/tests/inference/inference_utils.py`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/tests/inference/inference_utils.py)中的推理测试。
- en: Open a pull request! (Don’t forget to link it to the issue you opened, so that
    the maintainers could better track it and provide help when needed.)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开启一个拉取请求！（不要忘记将其链接到您开启的问题，以便维护者在需要时更好地跟踪并提供帮助。）
- en: We usually test smaller checkpoints to accelerate the CIs, you could find tiny
    models for testing under the [`Hugging Face Internal Testing Organization`](https://huggingface.co/hf-internal-testing).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会测试较小的检查点以加速CI，您可以在[`Hugging Face Internal Testing Organization`](https://huggingface.co/hf-internal-testing)下找到用于测试的微型模型。
- en: You have made a new model accessible on Neuron for the community! Thanks for
    joining us in the endeavor of democratizing good machine learning 🤗.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在Neuron上为社区提供了一个新的模型！感谢您加入我们努力使优秀的机器学习民主化的行动🤗。
