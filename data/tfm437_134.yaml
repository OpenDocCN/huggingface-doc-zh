- en: BARTpho
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BARTpho
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bartpho](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bartpho)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原始文本: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bartpho](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bartpho)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The BARTpho model was proposed in [BARTpho: Pre-trained Sequence-to-Sequence
    Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran,
    Duong Minh Le and Dat Quoc Nguyen.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'BARTpho模型是由Nguyen Luong Tran, Duong Minh Le和Dat Quoc Nguyen在[《BARTpho: Pre-trained
    Sequence-to-Sequence Models for Vietnamese》](https://arxiv.org/abs/2109.09701)中提出的。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We present BARTpho with two versions — BARTpho_word and BARTpho_syllable —
    the first public large-scale monolingual sequence-to-sequence models pre-trained
    for Vietnamese. Our BARTpho uses the “large” architecture and pre-training scheme
    of the sequence-to-sequence denoising model BART, thus especially suitable for
    generative NLP tasks. Experiments on a downstream task of Vietnamese text summarization
    show that in both automatic and human evaluations, our BARTpho outperforms the
    strong baseline mBART and improves the state-of-the-art. We release BARTpho to
    facilitate future research and applications of generative Vietnamese NLP tasks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们提供了两个版本的BARTpho — BARTpho_word和BARTpho_syllable — 这是为越南语预训练的首个公开大规模单语序列到序列模型。我们的BARTpho使用了BART序列到序列去噪模型的“large”架构和预训练方案，因此特别适用于生成式NLP任务。在越南语文本摘要的下游任务上的实验表明，在自动和人工评估中，我们的BARTpho优于强基线mBART，并改进了最新技术。我们发布BARTpho以促进未来的生成式越南语NLP任务的研究和应用。*'
- en: This model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The
    original code can be found [here](https://github.com/VinAIResearch/BARTpho).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[dqnguyen](https://huggingface.co/dqnguyen)贡献的。原始代码可以在[这里](https://github.com/VinAIResearch/BARTpho)找到。
- en: Usage example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法示例
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法提示
- en: 'Following mBART, BARTpho uses the “large” architecture of BART with an additional
    layer-normalization layer on top of both the encoder and decoder. Thus, usage
    examples in the [documentation of BART](bart), when adapting to use with BARTpho,
    should be adjusted by replacing the BART-specialized classes with the mBART-specialized
    counterparts. For example:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与mBART一样，BARTpho使用BART的“large”架构，并在编码器和解码器的顶部增加了一个额外的层归一化层。因此，在[BART文档](bart)中的用法示例，在适应BARTpho时，应通过用mBART专用类替换BART专用类来进行调整。例如：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This implementation is only for tokenization: “monolingual_vocab_file” consists
    of Vietnamese-specialized types extracted from the pre-trained SentencePiece model
    “vocab_file” that is available from the multilingual XLM-RoBERTa. Other languages,
    if employing this pre-trained multilingual SentencePiece model “vocab_file” for
    subword segmentation, can reuse BartphoTokenizer with their own language-specialized
    “monolingual_vocab_file”.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个实现仅用于标记化：“monolingual_vocab_file”包含从多语言XLM-RoBERTa的预训练SentencePiece模型“vocab_file”中提取的越南语专用类型。其他语言，如果使用这个预训练的多语言SentencePiece模型“vocab_file”进行子词分割，可以重用BartphoTokenizer与自己的语言专用“monolingual_vocab_file”。
- en: BartphoTokenizer
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BartphoTokenizer
- en: '### `class transformers.BartphoTokenizer`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BartphoTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L46)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L46)'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file. This vocabulary is the
    pre-trained SentencePiece model available from the multilingual XLM-RoBERTa, also
    used in mBART, consisting of 250K types.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。这个词汇是来自多语言XLM-RoBERTa的预训练SentencePiece模型，也被mBART使用，包含250K种类型。'
- en: '`monolingual_vocab_file` (`str`) — Path to the monolingual vocabulary file.
    This monolingual vocabulary consists of Vietnamese-specialized types extracted
    from the multilingual vocabulary vocab_file of 250K types.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monolingual_vocab_file` (`str`) — 单语词汇文件的路径。这个单语词汇包含从250K种类型的多语言词汇`vocab_file`中提取的越南语专用类型。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在构建序列时使用特殊标记时，这不是用于序列开头的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在构建序列时使用特殊标记时，这不是用于序列结尾的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *可选*, 默认为 `"<s>"`) — 用于进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。在使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为`"<pad>"`）- 用于填充的标记，例如在批处理不同长度的序列时。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`（`str`，*可选*，默认为`"<mask>"`）- 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs`（`dict`，*可选*）- 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`：启用子词正则化。'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`：unigram的抽样参数。对于BPE-Dropout无效。'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`：不执行抽样。'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`：从nbest_size结果中抽样。'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`: 假设nbest_size是无限的，并使用前向过滤和后向抽样算法从所有假设（格子）中抽样。'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：unigram抽样的平滑参数，以及BPE-dropout合并操作的丢失概率。'
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model`（`SentencePieceProcessor`）- 用于每次转换（字符串、标记和ID）的*SentencePiece*处理器。'
- en: Adapted from [XLMRobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer).
    Based on [SentencePiece](https://github.com/google/sentencepiece).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 改编自[XLMRobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer)。基于[SentencePiece](https://github.com/google/sentencepiece)。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此标记器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L191)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L191)'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）- 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）- 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An BARTPho sequence has the
    following format:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记构建用于序列分类任务的序列或序列对的模型输入。BARTPho序列的格式如下：
- en: 'single sequence: `<s> X </s>`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`<s> X </s>`
- en: 'pair of sequences: `<s> A </s></s> B </s>`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`<s> A </s></s> B </s>`
- en: '#### `convert_tokens_to_string`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L293)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L293)'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Converts a sequence of tokens (strings for sub-words) in a single string.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记（子词的字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L245)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L245)'
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）- ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）- 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 零的列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. BARTPho does not make use of token type ids, therefore a list of zeros is
    returned.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。BARTPho不使用标记类型ID，因此返回一个零的列表。
- en: '#### `get_special_tokens_mask`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L217)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L217)'
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）- ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）- 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`（`bool`，*可选*，默认为`False`）- 标记列表是否已经格式化为模型的特殊标记。'
- en: Returns
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用标记器`prepare_for_model`方法添加特殊标记时调用此方法。
