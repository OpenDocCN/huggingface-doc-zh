- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡åŒ–
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/quantization](https://huggingface.co/docs/accelerate/usage_guides/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/usage_guides/quantization](https://huggingface.co/docs/accelerate/usage_guides/quantization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: bitsandbytes Integration
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: bitsandbytes é›†æˆ
- en: ğŸ¤— Accelerate brings `bitsandbytes` quantization to your model. You can now load
    any pytorch model in 8-bit or 4-bit with a few lines of code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerate å°† `bitsandbytes` é‡åŒ–å¼•å…¥åˆ°æ‚¨çš„æ¨¡å‹ä¸­ã€‚æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨å‡ è¡Œä»£ç åœ¨ 8 ä½æˆ– 4 ä½ä¸­åŠ è½½ä»»ä½• pytorch
    æ¨¡å‹ã€‚
- en: If you want to use ğŸ¤— Transformers models with `bitsandbytes`, you should follow
    this [documentation](https://huggingface.co/docs/transformers/main_classes/quantization).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦ä½¿ç”¨å¸¦æœ‰ `bitsandbytes` çš„ ğŸ¤— Transformers æ¨¡å‹ï¼Œæ‚¨åº”è¯¥éµå¾ªè¿™ä¸ª[æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/quantization)ã€‚
- en: To learn more about how the `bitsandbytes` quantization works, check out the
    blog posts on [8-bit quantization](https://huggingface.co/blog/hf-bitsandbytes-integration)
    and [4-bit quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£ `bitsandbytes` é‡åŒ–çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹å…³äº[8 ä½é‡åŒ–](https://huggingface.co/blog/hf-bitsandbytes-integration)å’Œ[4
    ä½é‡åŒ–](https://huggingface.co/blog/4bit-transformers-bitsandbytes)çš„åšå®¢æ–‡ç« ã€‚
- en: Pre-Requisites
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢„å¤‡æ¡ä»¶
- en: 'You will need to install the following requirements:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦å®‰è£…ä»¥ä¸‹è¦æ±‚ï¼š
- en: Install `bitsandbytes` library
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®‰è£… `bitsandbytes` åº“
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Install latest `accelerate` from source
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `accelerate`
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install `minGPT` and `huggingface_hub` to run examples
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®‰è£… `minGPT` å’Œ `huggingface_hub` ä»¥è¿è¡Œç¤ºä¾‹
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å·¥ä½œåŸç†
- en: First, we need to initialize our model. To save memory, we can initialize an
    empty model using the context manager [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)
    åˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹ã€‚
- en: Letâ€™s take the GPT2 model from minGPT library.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥ minGPT åº“ä¸­çš„ GPT2 æ¨¡å‹ä¸ºä¾‹ã€‚
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we need to get the path to the weights of your model. The path can be
    the state_dict file (e.g. â€œpytorch_model.binâ€) or a folder containing the sharded
    checkpoints.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬éœ€è¦è·å–æ‚¨æ¨¡å‹çš„æƒé‡è·¯å¾„ã€‚è·¯å¾„å¯ä»¥æ˜¯ state_dict æ–‡ä»¶ï¼ˆä¾‹å¦‚â€œpytorch_model.binâ€ï¼‰æˆ–åŒ…å«åˆ†ç‰‡æ£€æŸ¥ç‚¹çš„æ–‡ä»¶å¤¹ã€‚
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, you need to set your quantization configuration with [BnbQuantizationConfig](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.BnbQuantizationConfig).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨éœ€è¦ä½¿ç”¨ [BnbQuantizationConfig](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.BnbQuantizationConfig)
    è®¾ç½®æ‚¨çš„é‡åŒ–é…ç½®ã€‚
- en: 'Hereâ€™s an example for 8-bit quantization:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ª 8 ä½é‡åŒ–çš„ä¾‹å­ï¼š
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Hereâ€™s an example for 4-bit quantization:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ª 4 ä½é‡åŒ–çš„ä¾‹å­ï¼š
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To quantize your empty model with the selected configuration, you need to use
    [load_and_quantize_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.load_and_quantize_model).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨æ‰€é€‰é…ç½®é‡åŒ–æ‚¨çš„ç©ºæ¨¡å‹ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ [load_and_quantize_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.load_and_quantize_model)ã€‚
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Saving and loading 8-bit model
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¿å­˜å’ŒåŠ è½½ 8 ä½æ¨¡å‹
- en: You can save your 8-bit model with accelerate using [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    ä½¿ç”¨ accelerate ä¿å­˜æ‚¨çš„ 8 ä½æ¨¡å‹ã€‚
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that 4-bit model serialization is currently not supported.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œç›®å‰ä¸æ”¯æŒ 4 ä½æ¨¡å‹åºåˆ—åŒ–ã€‚
- en: Offload modules to cpu and disk
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ¨¡å—å¸è½½åˆ° CPU å’Œç£ç›˜
- en: You can offload some modules to cpu/disk if you donâ€™t have enough space on the
    GPU to store the entire model on your GPUs. This uses big model inference under
    the hood. Check this [documentation](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)
    for more details.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„ GPU ä¸Šæ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´æ¥å­˜å‚¨æ•´ä¸ªæ¨¡å‹ï¼Œæ‚¨å¯ä»¥å°†ä¸€äº›æ¨¡å—å¸è½½åˆ° CPU/ç£ç›˜ã€‚è¿™åœ¨åº•å±‚ä½¿ç”¨å¤§æ¨¡å‹æ¨æ–­ã€‚æŸ¥çœ‹è¿™ä¸ª[æ–‡æ¡£](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: For 8-bit quantization, the selected modules will be converted to 8-bit precision.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº 8 ä½é‡åŒ–ï¼Œæ‰€é€‰æ¨¡å—å°†è½¬æ¢ä¸º 8 ä½ç²¾åº¦ã€‚
- en: For 4-bit quantization, the selected modules will be kept in `torch_dtype` that
    the user passed in `BnbQuantizationConfig`. We will add support to convert these
    offloaded modules in 4-bit when 4-bit serialization will be possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº 4 ä½é‡åŒ–ï¼Œæ‰€é€‰æ¨¡å—å°†ä¿ç•™åœ¨ç”¨æˆ·åœ¨ `BnbQuantizationConfig` ä¸­ä¼ é€’çš„ `torch_dtype` ä¸­ã€‚å½“ 4 ä½åºåˆ—åŒ–å¯è¡Œæ—¶ï¼Œæˆ‘ä»¬å°†æ·»åŠ æ”¯æŒå°†è¿™äº›å¸è½½çš„æ¨¡å—è½¬æ¢ä¸º
    4 ä½ã€‚
- en: 'You just need to pass a custom `device_map` in order to offload modules on
    cpu/disk. The offload modules will be dispatched on the GPU when needed. Hereâ€™s
    an example :'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åªéœ€è¦ä¼ é€’ä¸€ä¸ªè‡ªå®šä¹‰çš„ `device_map`ï¼Œä»¥ä¾¿å°†æ¨¡å—å¸è½½åˆ° CPU/ç£ç›˜ã€‚å½“éœ€è¦æ—¶ï¼Œå¸è½½çš„æ¨¡å—å°†è¢«åˆ†æ´¾åˆ° GPU ä¸Šã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fine-tune a quantized model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¾®è°ƒä¸€ä¸ªé‡åŒ–æ¨¡å‹
- en: It is not possible to perform pure 8bit or 4bit training on these models. However,
    you can train these models by leveraging parameter efficient fine tuning methods
    (PEFT) and train for example adapters on top of them. Please have a look at [peft](https://github.com/huggingface/peft)
    library for more details.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›æ¨¡å‹ä¸Šæ— æ³•æ‰§è¡Œçº¯ 8 ä½æˆ– 4 ä½è®­ç»ƒã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡åˆ©ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆPEFTï¼‰æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ä¸Šè®­ç»ƒé€‚é…å™¨ç­‰ã€‚è¯·æŸ¥çœ‹ [peft](https://github.com/huggingface/peft)
    åº“ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: Currently, you canâ€™t add adapters on top of any quantized model. However, with
    the official support of adapters with ğŸ¤— Transformers models, you can fine-tune
    quantized models. If you want to finetune a ğŸ¤— Transformers model , follow this
    [documentation](https://huggingface.co/docs/transformers/main_classes/quantization)
    instead. Check out this [demo](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)
    on how to fine-tune a 4-bit ğŸ¤— Transformers model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œæ‚¨æ— æ³•åœ¨ä»»ä½•é‡åŒ–æ¨¡å‹ä¹‹ä¸Šæ·»åŠ é€‚é…å™¨ã€‚ç„¶è€Œï¼Œé€šè¿‡ ğŸ¤— Transformers æ¨¡å‹çš„å®˜æ–¹é€‚é…å™¨æ”¯æŒï¼Œæ‚¨å¯ä»¥å¾®è°ƒé‡åŒ–æ¨¡å‹ã€‚å¦‚æœæ‚¨æƒ³è¦å¾®è°ƒä¸€ä¸ª ğŸ¤— Transformers
    æ¨¡å‹ï¼Œè¯·å‚è€ƒè¿™ä¸ª[æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/quantization)ã€‚æŸ¥çœ‹è¿™ä¸ª[æ¼”ç¤º](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)ï¼Œäº†è§£å¦‚ä½•å¾®è°ƒä¸€ä¸ª
    4 ä½ ğŸ¤— Transformers æ¨¡å‹ã€‚
- en: Note that you donâ€™t need to pass `device_map` when loading the model for training.
    It will automatically load your model on your GPU. Please note that `device_map=auto`
    should be used for inference only.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨åŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ‚¨æ— éœ€ä¼ é€’ `device_map`ã€‚å®ƒå°†è‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ° GPU ä¸Šã€‚è¯·æ³¨æ„ï¼Œ`device_map=auto`åº”ä»…ç”¨äºæ¨æ–­ã€‚
- en: Example demo - running GPT2 1.5b on a Google Colab
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ¼”ç¤º - åœ¨Google Colabä¸Šè¿è¡ŒGPT2 1.5b
- en: Check out the Google Colab [demo](https://colab.research.google.com/drive/1T1pOgewAWVpR9gKpaEWw4orOrzPFb3yM?usp=sharing)
    for running quantized models on a GTP2 model. The GPT2-1.5B model checkpoint is
    in FP32 which uses 6GB of memory. After quantization, it uses 1.6GB with 8-bit
    modules and 1.2GB with 4-bit modules.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹åœ¨Google Colabä¸Šè¿è¡Œé‡åŒ–æ¨¡å‹çš„æ¼”ç¤ºã€‚GPT2-1.5Bæ¨¡å‹æ£€æŸ¥ç‚¹ä½¿ç”¨FP32ï¼Œå ç”¨6GBå†…å­˜ã€‚é‡åŒ–åï¼Œä½¿ç”¨8ä½æ¨¡å—å ç”¨1.6GBï¼Œä½¿ç”¨4ä½æ¨¡å—å ç”¨1.2GBã€‚
