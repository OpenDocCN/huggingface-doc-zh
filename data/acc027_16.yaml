- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/quantization](https://huggingface.co/docs/accelerate/usage_guides/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/accelerate/usage_guides/quantization](https://huggingface.co/docs/accelerate/usage_guides/quantization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: bitsandbytes Integration
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: bitsandbytes 集成
- en: 🤗 Accelerate brings `bitsandbytes` quantization to your model. You can now load
    any pytorch model in 8-bit or 4-bit with a few lines of code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Accelerate 将 `bitsandbytes` 量化引入到您的模型中。您现在可以使用几行代码在 8 位或 4 位中加载任何 pytorch
    模型。
- en: If you want to use 🤗 Transformers models with `bitsandbytes`, you should follow
    this [documentation](https://huggingface.co/docs/transformers/main_classes/quantization).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要使用带有 `bitsandbytes` 的 🤗 Transformers 模型，您应该遵循这个[文档](https://huggingface.co/docs/transformers/main_classes/quantization)。
- en: To learn more about how the `bitsandbytes` quantization works, check out the
    blog posts on [8-bit quantization](https://huggingface.co/blog/hf-bitsandbytes-integration)
    and [4-bit quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 `bitsandbytes` 量化的工作原理，请查看关于[8 位量化](https://huggingface.co/blog/hf-bitsandbytes-integration)和[4
    位量化](https://huggingface.co/blog/4bit-transformers-bitsandbytes)的博客文章。
- en: Pre-Requisites
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预备条件
- en: 'You will need to install the following requirements:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装以下要求：
- en: Install `bitsandbytes` library
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 `bitsandbytes` 库
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Install latest `accelerate` from source
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从源代码安装最新的 `accelerate`
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Install `minGPT` and `huggingface_hub` to run examples
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 `minGPT` 和 `huggingface_hub` 以运行示例
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作原理
- en: First, we need to initialize our model. To save memory, we can initialize an
    empty model using the context manager [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要初始化我们的模型。为了节省内存，我们可以使用上下文管理器 [init_empty_weights()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.init_empty_weights)
    初始化一个空模型。
- en: Let’s take the GPT2 model from minGPT library.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 minGPT 库中的 GPT2 模型为例。
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we need to get the path to the weights of your model. The path can be
    the state_dict file (e.g. “pytorch_model.bin”) or a folder containing the sharded
    checkpoints.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要获取您模型的权重路径。路径可以是 state_dict 文件（例如“pytorch_model.bin”）或包含分片检查点的文件夹。
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, you need to set your quantization configuration with [BnbQuantizationConfig](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.BnbQuantizationConfig).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要使用 [BnbQuantizationConfig](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.BnbQuantizationConfig)
    设置您的量化配置。
- en: 'Here’s an example for 8-bit quantization:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个 8 位量化的例子：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here’s an example for 4-bit quantization:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个 4 位量化的例子：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To quantize your empty model with the selected configuration, you need to use
    [load_and_quantize_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.load_and_quantize_model).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用所选配置量化您的空模型，您需要使用 [load_and_quantize_model()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.load_and_quantize_model)。
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Saving and loading 8-bit model
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存和加载 8 位模型
- en: You can save your 8-bit model with accelerate using [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    使用 accelerate 保存您的 8 位模型。
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that 4-bit model serialization is currently not supported.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目前不支持 4 位模型序列化。
- en: Offload modules to cpu and disk
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将模块卸载到 CPU 和磁盘
- en: You can offload some modules to cpu/disk if you don’t have enough space on the
    GPU to store the entire model on your GPUs. This uses big model inference under
    the hood. Check this [documentation](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)
    for more details.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的 GPU 上没有足够的空间来存储整个模型，您可以将一些模块卸载到 CPU/磁盘。这在底层使用大模型推断。查看这个[文档](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)以获取更多详细信息。
- en: For 8-bit quantization, the selected modules will be converted to 8-bit precision.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 8 位量化，所选模块将转换为 8 位精度。
- en: For 4-bit quantization, the selected modules will be kept in `torch_dtype` that
    the user passed in `BnbQuantizationConfig`. We will add support to convert these
    offloaded modules in 4-bit when 4-bit serialization will be possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 4 位量化，所选模块将保留在用户在 `BnbQuantizationConfig` 中传递的 `torch_dtype` 中。当 4 位序列化可行时，我们将添加支持将这些卸载的模块转换为
    4 位。
- en: 'You just need to pass a custom `device_map` in order to offload modules on
    cpu/disk. The offload modules will be dispatched on the GPU when needed. Here’s
    an example :'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需要传递一个自定义的 `device_map`，以便将模块卸载到 CPU/磁盘。当需要时，卸载的模块将被分派到 GPU 上。这里是一个例子：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fine-tune a quantized model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调一个量化模型
- en: It is not possible to perform pure 8bit or 4bit training on these models. However,
    you can train these models by leveraging parameter efficient fine tuning methods
    (PEFT) and train for example adapters on top of them. Please have a look at [peft](https://github.com/huggingface/peft)
    library for more details.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些模型上无法执行纯 8 位或 4 位训练。但是，您可以通过利用参数高效的微调方法（PEFT）来训练这些模型，并在其上训练适配器等。请查看 [peft](https://github.com/huggingface/peft)
    库以获取更多详细信息。
- en: Currently, you can’t add adapters on top of any quantized model. However, with
    the official support of adapters with 🤗 Transformers models, you can fine-tune
    quantized models. If you want to finetune a 🤗 Transformers model , follow this
    [documentation](https://huggingface.co/docs/transformers/main_classes/quantization)
    instead. Check out this [demo](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)
    on how to fine-tune a 4-bit 🤗 Transformers model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，您无法在任何量化模型之上添加适配器。然而，通过 🤗 Transformers 模型的官方适配器支持，您可以微调量化模型。如果您想要微调一个 🤗 Transformers
    模型，请参考这个[文档](https://huggingface.co/docs/transformers/main_classes/quantization)。查看这个[演示](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)，了解如何微调一个
    4 位 🤗 Transformers 模型。
- en: Note that you don’t need to pass `device_map` when loading the model for training.
    It will automatically load your model on your GPU. Please note that `device_map=auto`
    should be used for inference only.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在加载模型进行训练时，您无需传递 `device_map`。它将自动将您的模型加载到 GPU 上。请注意，`device_map=auto`应仅用于推断。
- en: Example demo - running GPT2 1.5b on a Google Colab
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例演示 - 在Google Colab上运行GPT2 1.5b
- en: Check out the Google Colab [demo](https://colab.research.google.com/drive/1T1pOgewAWVpR9gKpaEWw4orOrzPFb3yM?usp=sharing)
    for running quantized models on a GTP2 model. The GPT2-1.5B model checkpoint is
    in FP32 which uses 6GB of memory. After quantization, it uses 1.6GB with 8-bit
    modules and 1.2GB with 4-bit modules.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 查看在Google Colab上运行量化模型的演示。GPT2-1.5B模型检查点使用FP32，占用6GB内存。量化后，使用8位模块占用1.6GB，使用4位模块占用1.2GB。
