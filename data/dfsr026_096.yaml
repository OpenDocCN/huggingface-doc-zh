- en: Evaluating Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估扩散模型
- en: 'Original text: [https://huggingface.co/docs/diffusers/conceptual/evaluation](https://huggingface.co/docs/diffusers/conceptual/evaluation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/conceptual/evaluation](https://huggingface.co/docs/diffusers/conceptual/evaluation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/evaluation.ipynb)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/evaluation.ipynb)'
- en: Evaluation of generative models like [Stable Diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)
    is subjective in nature. But as practitioners and researchers, we often have to
    make careful choices amongst many different possibilities. So, when working with
    different generative models (like GANs, Diffusion, etc.), how do we choose one
    over the other?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像[Stable Diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)这样的生成模型的评估是主观的。但作为从业者和研究人员，我们经常不得不在许多不同的可能性之间做出谨慎的选择。因此，在使用不同的生成模型（如GAN、Diffusion等）时，我们如何选择其中一个而不是另一个呢？
- en: Qualitative evaluation of such models can be error-prone and might incorrectly
    influence a decision. However, quantitative metrics don’t necessarily correspond
    to image quality. So, usually, a combination of both qualitative and quantitative
    evaluations provides a stronger signal when choosing one model over the other.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些模型的定性评估可能存在错误，并可能错误地影响决策。然而，定量指标不一定对应图像质量。因此，通常在选择一个模型而不是另一个时，定性和定量评估的结合提供了更强的信号。
- en: In this document, we provide a non-exhaustive overview of qualitative and quantitative
    methods to evaluate Diffusion models. For quantitative methods, we specifically
    focus on how to implement them alongside `diffusers`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文档中，我们提供了一个关于定性和定量方法评估扩散模型的非详尽概述。对于定量方法，我们特别关注如何将它们与`diffusers`一起实施。
- en: The methods shown in this document can also be used to evaluate different [noise
    schedulers](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview)
    keeping the underlying generation model fixed.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文档中显示的方法也可用于评估不同的[噪声调度器](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview)，同时保持基础生成模型不变。
- en: Scenarios
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景
- en: 'We cover Diffusion models with the following pipelines:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们涵盖以下管道的扩散模型：
- en: Text-guided image generation (such as the [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img)).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本引导的图像生成（如[`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img)）。
- en: Text-guided image generation, additionally conditioned on an input image (such
    as the [`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img)
    and [`StableDiffusionInstructPix2PixPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix)).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本引导的图像生成，另外还依赖于输入图像（例如[`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img)和[`StableDiffusionInstructPix2PixPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix)）。
- en: Class-conditioned image generation models (such as the [`DiTPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别条件的图像生成模型（如[`DiTPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)）。
- en: Qualitative Evaluation
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定性评估
- en: Qualitative evaluation typically involves human assessment of generated images.
    Quality is measured across aspects such as compositionality, image-text alignment,
    and spatial relations. Common prompts provide a degree of uniformity for subjective
    metrics. DrawBench and PartiPrompts are prompt datasets used for qualitative benchmarking.
    DrawBench and PartiPrompts were introduced by [Imagen](https://imagen.research.google/)
    and [Parti](https://parti.research.google/) respectively.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 定性评估通常涉及对生成的图像进行人类评估。质量通过诸如组成性、图像文本对齐和空间关系等方面来衡量。常见提示为主观指标提供了一定程度的统一性。DrawBench和PartiPrompts是用于定性基准测试的提示数据集。DrawBench和PartiPrompts分别由[Imagen](https://imagen.research.google/)和[Parti](https://parti.research.google/)引入。
- en: 'From the [official Parti website](https://parti.research.google/):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[官方Parti网站](https://parti.research.google/)：
- en: PartiPrompts (P2) is a rich set of over 1600 prompts in English that we release
    as part of this work. P2 can be used to measure model capabilities across various
    categories and challenge aspects.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PartiPrompts（P2）是我们作为这项工作的一部分发布的一组超过1600个英文提示。P2可用于衡量模型在各种类别和挑战方面的能力。
- en: '![parti-prompts](../Images/a33baa9f457f5cb38b2e24aaacf3265c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![parti-prompts](../Images/a33baa9f457f5cb38b2e24aaacf3265c.png)'
- en: 'PartiPrompts has the following columns:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PartiPrompts具有以下列：
- en: Prompt
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示
- en: Category of the prompt (such as “Abstract”, “World Knowledge”, etc.)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示的类别（如“抽象”，“世界知识”等）
- en: Challenge reflecting the difficulty (such as “Basic”, “Complex”, “Writing &
    Symbols”, etc.)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反映难度的挑战（如“基本”，“复杂”，“写作与符号”等）
- en: These benchmarks allow for side-by-side human evaluation of different image
    generation models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基准允许对不同图像生成模型进行并排人类评估。
- en: 'For this, the 🧨 Diffusers team has built **Open Parti Prompts**, which is a
    community-driven qualitative benchmark based on Parti Prompts to compare state-of-the-art
    open-source diffusion models:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，🧨 Diffusers团队构建了**Open Parti Prompts**，这是一个基于Parti Prompts的社区驱动的定性基准，用于比较最先进的开源扩散模型：
- en: '[Open Parti Prompts Game](https://huggingface.co/spaces/OpenGenAI/open-parti-prompts):
    For 10 parti prompts, 4 generated images are shown and the user selects the image
    that suits the prompt best.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[打开Parti Prompts游戏](https://huggingface.co/spaces/OpenGenAI/open-parti-prompts)：展示10个parti提示，用户选择最适合提示的图像。'
- en: '[Open Parti Prompts Leaderboard](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard):
    The leaderboard comparing the currently best open-sourced diffusion models to
    each other.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[打开Parti Prompts排行榜](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard)：将当前最佳的开源扩散模型进行比较。'
- en: To manually compare images, let’s see how we can use `diffusers` on a couple
    of PartiPrompts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了手动比较图像，让我们看看如何在几个PartiPrompts上使用`diffusers`。
- en: 'Below we show some prompts sampled across different challenges: Basic, Complex,
    Linguistic Structures, Imagination, and Writing & Symbols. Here we are using PartiPrompts
    as a [dataset](https://huggingface.co/datasets/nateraw/parti-prompts).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们展示了在不同挑战中抽样的一些提示：基本、复杂、语言结构、想象力和书写与符号。在这里，我们使用PartiPrompts作为一个[数据集](https://huggingface.co/datasets/nateraw/parti-prompts)。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can use these prompts to generate some images using Stable Diffusion
    ([v1-4 checkpoint](https://huggingface.co/CompVis/stable-diffusion-v1-4)):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这些提示来使用Stable Diffusion（[v1-4检查点](https://huggingface.co/CompVis/stable-diffusion-v1-4)）生成一些图像：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![parti-prompts-14](../Images/bee6f1ffad12dea358f2f7f0b461f9a4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![parti-prompts-14](../Images/bee6f1ffad12dea358f2f7f0b461f9a4.png)'
- en: 'We can also set `num_images_per_prompt` accordingly to compare different images
    for the same prompt. Running the same pipeline but with a different checkpoint
    ([v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)), yields:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以相应地设置`num_images_per_prompt`以比较相同提示的不同图像。使用不同检查点（[v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)）运行相同的管道，产生：
- en: '![parti-prompts-15](../Images/8809682b839cee78ffe96b7e472b1f04.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![parti-prompts-15](../Images/8809682b839cee78ffe96b7e472b1f04.png)'
- en: Once several images are generated from all the prompts using multiple models
    (under evaluation), these results are presented to human evaluators for scoring.
    For more details on the DrawBench and PartiPrompts benchmarks, refer to their
    respective papers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用多个模型（正在评估中）从所有提示生成了几幅图像，这些结果将呈现给人类评估者进行评分。有关DrawBench和PartiPrompts基准的更多详细信息，请参考它们各自的论文。
- en: It is useful to look at some inference samples while a model is training to
    measure the training progress. In our [training scripts](https://github.com/huggingface/diffusers/tree/main/examples/),
    we support this utility with additional support for logging to TensorBoard and
    Weights & Biases.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练时查看一些推理样本是有用的，以衡量训练进度。在我们的[训练脚本](https://github.com/huggingface/diffusers/tree/main/examples/)中，我们支持这种实用程序，并额外支持记录到TensorBoard和Weights
    & Biases。
- en: Quantitative Evaluation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定量评估
- en: 'In this section, we will walk you through how to evaluate three different diffusion
    pipelines using:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将指导您如何使用以下内容评估三种不同的扩散管道：
- en: CLIP score
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP分数
- en: CLIP directional similarity
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP方向相似性
- en: FID
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FID
- en: Text-guided image generation
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本引导的图像生成
- en: '[CLIP score](https://arxiv.org/abs/2104.08718) measures the compatibility of
    image-caption pairs. Higher CLIP scores imply higher compatibility 🔼. The CLIP
    score is a quantitative measurement of the qualitative concept “compatibility”.
    Image-caption pair compatibility can also be thought of as the semantic similarity
    between the image and the caption. CLIP score was found to have high correlation
    with human judgement.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIP分数](https://arxiv.org/abs/2104.08718)衡量了图像-标题对的兼容性。更高的CLIP分数意味着更高的兼容性🔼。CLIP分数是对定性概念“兼容性”的定量测量。图像-标题对的兼容性也可以被视为图像和标题之间的语义相似性。发现CLIP分数与人类判断具有很高的相关性。'
- en: 'Let’s first load a [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载一个[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Generate some images with multiple prompts:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个提示生成一些图像：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And then, we calculate the CLIP score.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算CLIP分数。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the above example, we generated one image per prompt. If we generated multiple
    images per prompt, we would have to take the average score from the generated
    images per prompt.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们为每个提示生成了一幅图像。如果我们为每个提示生成多幅图像，我们将不得不从每个提示生成的图像中取平均分数。
- en: 'Now, if we wanted to compare two checkpoints compatible with the [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    we should pass a generator while calling the pipeline. First, we generate images
    with a fixed seed with the [v1-4 Stable Diffusion checkpoint](https://huggingface.co/CompVis/stable-diffusion-v1-4):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想要比较两个与[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)兼容的检查点，我们应该在调用管道时传递一个生成器。首先，我们使用[v1-4
    Stable Diffusion检查点](https://huggingface.co/CompVis/stable-diffusion-v1-4)生成图像：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we load the [v1-5 checkpoint](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    to generate images:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载[v1-5检查点](https://huggingface.co/runwayml/stable-diffusion-v1-5)来生成图像：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And finally, we compare their CLIP scores:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们比较它们的CLIP分数：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It seems like the [v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    checkpoint performs better than its predecessor. Note, however, that the number
    of prompts we used to compute the CLIP scores is quite low. For a more practical
    evaluation, this number should be way higher, and the prompts should be diverse.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎[v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)检查点的性能优于其前身。然而，请注意，我们用于计算CLIP分数的提示数量相当低。为了进行更实际的评估，这个数字应该更高，并且提示应该更加多样化。
- en: By construction, there are some limitations in this score. The captions in the
    training dataset were crawled from the web and extracted from `alt` and similar
    tags associated an image on the internet. They are not necessarily representative
    of what a human being would use to describe an image. Hence we had to “engineer”
    some prompts here.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从构造上看，这个分数存在一些限制。训练数据集中的标题是从网络上爬取的，并从互联网上与图像相关联的`alt`和类似标签中提取的。它们不一定代表人类用来描述图像的方式。因此，我们必须在这里“设计”一些提示。
- en: Image-conditioned text-to-image generation
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像条件的文本到图像生成
- en: In this case, we condition the generation pipeline with an input image as well
    as a text prompt. Let’s take the [StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline),
    as an example. It takes an edit instruction as an input prompt and an input image
    to be edited.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将生成管道与输入图像以及文本提示一起进行条件化。让我们以[StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline)为例。它将编辑指令作为输入提示，并输入要编辑的图像。
- en: 'Here is one example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个例子：
- en: '![edit-instruction](../Images/dd654c9963d1e235d32cfd4a6a4ce36b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![edit-instruction](../Images/dd654c9963d1e235d32cfd4a6a4ce36b.png)'
- en: One strategy to evaluate such a model is to measure the consistency of the change
    between the two images (in [CLIP](https://huggingface.co/docs/transformers/model_doc/clip)
    space) with the change between the two image captions (as shown in [CLIP-Guided
    Domain Adaptation of Image Generators](https://arxiv.org/abs/2108.00946)). This
    is referred to as the ”**CLIP directional similarity**“.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 评估这样一个模型的一种策略是测量两个图像之间的变化的一致性（在[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)空间中）与两个图像标题之间的变化之间的一致性（如[CLIP引导的图像生成领域自适应](https://arxiv.org/abs/2108.00946)所示）。这被称为“**CLIP方向相似性**”。
- en: Caption 1 corresponds to the input image (image 1) that is to be edited.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题1对应于要编辑的输入图像（图像1）。
- en: Caption 2 corresponds to the edited image (image 2). It should reflect the edit
    instruction.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题2对应于编辑后的图像（图像2）。它应该反映编辑指令。
- en: 'Following is a pictorial overview:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图解概述：
- en: '![edit-consistency](../Images/4246d7a1ea8f4bbc62b049bedecedc25.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![edit-consistency](../Images/4246d7a1ea8f4bbc62b049bedecedc25.png)'
- en: We have prepared a mini dataset to implement this metric. Let’s first load the
    dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备了一个小型数据集来实现这个指标。让我们首先加载数据集。
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here we have:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有：
- en: '`input` is a caption corresponding to the `image`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input` 是与 `image` 对应的标题。'
- en: '`edit` denotes the edit instruction.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edit` 表示编辑指令。'
- en: '`output` denotes the modified caption reflecting the `edit` instruction.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output` 表示反映 `edit` 指令的修改后标题。'
- en: Let’s take a look at a sample.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例。
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And here is the image:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是图片：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![edit-dataset](../Images/d57605cd3c5d841cf20ff0af193b5321.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![edit-dataset](../Images/d57605cd3c5d841cf20ff0af193b5321.png)'
- en: We will first edit the images of our dataset with the edit instruction and compute
    the directional similarity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用编辑指令编辑数据集的图像，并计算方向相似性。
- en: 'Let’s first load the [StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载[StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline)：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we perform the edits:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进行编辑：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To measure the directional similarity, we first load CLIP’s image and text
    encoders:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量方向相似性，我们首先加载CLIP的图像和文本编码器：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that we are using a particular CLIP checkpoint, i.e., `openai/clip-vit-large-patch14`.
    This is because the Stable Diffusion pre-training was performed with this CLIP
    variant. For more details, refer to the [documentation](https://huggingface.co/docs/transformers/model_doc/clip).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用特定的CLIP检查点，即 `openai/clip-vit-large-patch14`。这是因为Stable Diffusion的预训练是使用这个CLIP变体进行的。有关更多详细信息，请参阅[文档](https://huggingface.co/docs/transformers/model_doc/clip)。
- en: 'Next, we prepare a PyTorch `nn.Module` to compute directional similarity:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们准备一个PyTorch `nn.Module` 来计算方向相似性：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let’s put `DirectionalSimilarity` to use now.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 `DirectionalSimilarity`。
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Like the CLIP Score, the higher the CLIP directional similarity, the better
    it is.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与CLIP分数类似，CLIP方向相似性越高，效果越好。
- en: It should be noted that the `StableDiffusionInstructPix2PixPipeline` exposes
    two arguments, namely, `image_guidance_scale` and `guidance_scale` that let you
    control the quality of the final edited image. We encourage you to experiment
    with these two arguments and see the impact of that on the directional similarity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是 `StableDiffusionInstructPix2PixPipeline` 公开了两个参数，即 `image_guidance_scale`
    和 `guidance_scale`，让您控制最终编辑图像的质量。我们鼓励您尝试这两个参数，并查看对方向相似性的影响。
- en: We can extend the idea of this metric to measure how similar the original image
    and edited version are. To do that, we can just do `F.cosine_similarity(img_feat_two,
    img_feat_one)`. For these kinds of edits, we would still want the primary semantics
    of the images to be preserved as much as possible, i.e., a high similarity score.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以扩展这个指标的思想，来衡量原始图像和编辑版本有多相似。为此，我们只需执行 `F.cosine_similarity(img_feat_two,
    img_feat_one)`。对于这种类型的编辑，我们仍然希望尽可能保留图像的主要语义，即高相似性分数。
- en: We can use these metrics for similar pipelines such as the [`StableDiffusionPix2PixZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix_zero#diffusers.StableDiffusionPix2PixZeroPipeline).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些指标来评估类似的流水线，比如[`StableDiffusionPix2PixZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix_zero#diffusers.StableDiffusionPix2PixZeroPipeline)。
- en: Both CLIP score and CLIP direction similarity rely on the CLIP model, which
    can make the evaluations biased.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP分数和CLIP方向相似性都依赖于CLIP模型，这可能会使评估产生偏见。
- en: '***Extending metrics like IS, FID (discussed later), or KID can be difficult***
    when the model under evaluation was pre-trained on a large image-captioning dataset
    (such as the [LAION-5B dataset](https://laion.ai/blog/laion-5b/)). This is because
    underlying these metrics is an InceptionNet (pre-trained on the ImageNet-1k dataset)
    used for extracting intermediate image features. The pre-training dataset of Stable
    Diffusion may have limited overlap with the pre-training dataset of InceptionNet,
    so it is not a good candidate here for feature extraction.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '***当评估的模型在大型图像字幕数据集上进行了预训练（例如[LAION-5B数据集](https://laion.ai/blog/laion-5b/)）时，扩展诸如IS、FID（稍后讨论）或KID等指标可能会很困难。***这是因为这些指标的基础是InceptionNet（在ImageNet-1k数据集上进行了预训练）用于提取中间图像特征。Stable
    Diffusion的预训练数据集可能与InceptionNet的预训练数据集有限的重叠，因此在这里不是进行特征提取的好选择。'
- en: '***Using the above metrics helps evaluate models that are class-conditioned.
    For example, [DiT](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit).
    It was pre-trained being conditioned on the ImageNet-1k classes.***'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '***使用上述指标有助于评估类别条件的模型。例如，[DiT](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)。它是在ImageNet-1k类别上进行条件预训练的。***'
- en: Class-conditioned image generation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别条件的图像生成
- en: Class-conditioned generative models are usually pre-trained on a class-labeled
    dataset such as [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k). Popular
    metrics for evaluating these models include Fréchet Inception Distance (FID),
    Kernel Inception Distance (KID), and Inception Score (IS). In this document, we
    focus on FID ([Heusel et al.](https://arxiv.org/abs/1706.08500)). We show how
    to compute it with the [`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit),
    which uses the [DiT model](https://arxiv.org/abs/2212.09748) under the hood.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 类别条件生成模型通常是在类别标记的数据集上预训练的，例如[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)。用于评估这些模型的流行指标包括Fréchet
    Inception Distance（FID）、Kernel Inception Distance（KID）和Inception Score（IS）。在本文档中，我们专注于FID（[Heusel等人](https://arxiv.org/abs/1706.08500)）。我们展示了如何使用[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)来计算它，该工具在底层使用[DiT模型](https://arxiv.org/abs/2212.09748)。
- en: 'FID aims to measure how similar are two datasets of images. As per [this resource](https://mmgeneration.readthedocs.io/en/latest/quick_run.html#fid):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: FID旨在衡量两组图像数据集之间的相似性。根据[这个资源](https://mmgeneration.readthedocs.io/en/latest/quick_run.html#fid)：
- en: Fréchet Inception Distance is a measure of similarity between two datasets of
    images. It was shown to correlate well with the human judgment of visual quality
    and is most often used to evaluate the quality of samples of Generative Adversarial
    Networks. FID is calculated by computing the Fréchet distance between two Gaussians
    fitted to feature representations of the Inception network.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Fréchet Inception Distance是衡量两组图像数据集之间相似性的指标。它被证明与人类对视觉质量的判断相关，并且最常用于评估生成对抗网络样本的质量。FID通过计算适应于Inception网络特征表示的两个高斯分布之间的Fréchet距离来计算。
- en: These two datasets are essentially the dataset of real images and the dataset
    of fake images (generated images in our case). FID is usually calculated with
    two large datasets. However, for this document, we will work with two mini datasets.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个数据集本质上是真实图像数据集和假图像数据集（在我们的情况下生成的图像）。FID通常是使用两个大型数据集来计算的。但是，在本文档中，我们将使用两个小型数据集。
- en: 'Let’s first download a few images from the ImageNet-1k training set:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先从ImageNet-1k训练集中下载一些图像：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'These are 10 images from the following ImageNet-1k classes: “cassette_player”,
    “chain_saw” (x2), “church”, “gas_pump” (x3), “parachute” (x2), and “tench”.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是来自以下ImageNet-1k类别的10张图像：“卡带播放器”、“链锯”（x2）、“教堂”、“加油泵”（x3）、“降落伞”（x2）和“鲑鱼”。
- en: '![real-images](../Images/f9f0dfbeb754ed8d4f547460644fd7bd.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![真实图像](../Images/f9f0dfbeb754ed8d4f547460644fd7bd.png)'
- en: '*Real images.*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*真实图像。*'
- en: Now that the images are loaded, let’s apply some lightweight pre-processing
    on them to use them for FID calculation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图像已加载，让我们对它们进行一些轻量级预处理，以便用于FID计算。
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We now load the [`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)
    to generate images conditioned on the above-mentioned classes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们加载[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)来生成基于上述类别的图像。
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we can compute the FID using [`torchmetrics`](https://torchmetrics.readthedocs.io/).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用[`torchmetrics`](https://torchmetrics.readthedocs.io/)来计算FID。
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The lower the FID, the better it is. Several things can influence FID here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: FID值越低，效果越好。这里有几个因素可能会影响FID：
- en: Number of images (both real and fake)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数量（真实和假的）
- en: Randomness induced in the diffusion process
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散过程中引入的随机性
- en: Number of inference steps in the diffusion process
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散过程中的推理步骤数量
- en: The scheduler being used in the diffusion process
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散过程中使用的调度程序
- en: For the last two points, it is, therefore, a good practice to run the evaluation
    across different seeds and inference steps, and then report an average result.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后两点，因此最好的做法是在不同的种子和推理步骤上运行评估，然后报告平均结果。
- en: 'FID results tend to be fragile as they depend on a lot of factors:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: FID的结果往往很脆弱，因为它取决于许多因素：
- en: The specific Inception model used during computation.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算过程中使用的特定Inception模型。
- en: The implementation accuracy of the computation.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算的实现准确性。
- en: The image format (not the same if we start from PNGs vs JPGs).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像格式（如果我们从PNG开始与从JPG开始不同）。
- en: Keeping that in mind, FID is often most useful when comparing similar runs,
    but it is hard to reproduce paper results unless the authors carefully disclose
    the FID measurement code.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点，FID在比较相似的运行时通常最有用，但要复现论文结果很难，除非作者仔细披露FID测量代码。
- en: These points apply to other related metrics too, such as KID and IS.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些要点也适用于其他相关指标，如KID和IS。
- en: As a final step, let’s visually inspect the `fake_images`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，让我们直观地检查一下`fake_images`。
- en: '![fake-images](../Images/d9d2a296bc3c224017b8ff324b7d9c84.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![假图像](../Images/d9d2a296bc3c224017b8ff324b7d9c84.png)'
- en: '*Fake images.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*假图像。*'
