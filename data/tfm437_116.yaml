- en: Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The base classes [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel),
    and [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)
    implement the common methods for loading/saving a model either from a local file
    or directory, or from a pretrained model configuration provided by the library
    (downloaded from HuggingFaceâ€™s AWS S3 repository).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç±» [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    å’Œ [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)
    å®ç°äº†ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•åŠ è½½/ä¿å­˜æ¨¡å‹çš„å¸¸ç”¨æ–¹æ³•ï¼Œæˆ–ä»åº“æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ï¼ˆä» HuggingFace çš„ AWS S3 å­˜å‚¨åº“ä¸‹è½½ï¼‰åŠ è½½æ¨¡å‹ã€‚
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    also implement a few methods which are common among all the models to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    å’Œ [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    è¿˜å®ç°äº†ä¸€äº›æ‰€æœ‰æ¨¡å‹å…±æœ‰çš„æ–¹æ³•ï¼š'
- en: resize the input token embeddings when new tokens are added to the vocabulary
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æ–°çš„æ ‡è®°æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­æ—¶ï¼Œè°ƒæ•´è¾“å…¥æ ‡è®°åµŒå…¥å¤§å°
- en: prune the attention heads of the model.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿®å‰ªæ¨¡å‹çš„æ³¨æ„åŠ›å¤´ã€‚
- en: The other methods that are common to each model are defined in [ModuleUtilsMixin](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.ModuleUtilsMixin)
    (for the PyTorch models) and `~modeling_tf_utils.TFModuleUtilsMixin` (for the
    TensorFlow models) or for text generation, [GenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin)
    (for the PyTorch models), [TFGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin)
    (for the TensorFlow models) and [FlaxGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin)
    (for the Flax/JAX models).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹å…±æœ‰çš„å…¶ä»–æ–¹æ³•åœ¨ [ModuleUtilsMixin](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.ModuleUtilsMixin)ï¼ˆç”¨äº
    PyTorch æ¨¡å‹ï¼‰å’Œ `~modeling_tf_utils.TFModuleUtilsMixin`ï¼ˆç”¨äº TensorFlow æ¨¡å‹ï¼‰ä¸­å®šä¹‰ï¼Œæˆ–è€…ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„
    [GenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin)ï¼ˆç”¨äº
    PyTorch æ¨¡å‹ï¼‰ã€[TFGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin)ï¼ˆç”¨äº
    TensorFlow æ¨¡å‹ï¼‰å’Œ [FlaxGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin)ï¼ˆç”¨äº
    Flax/JAX æ¨¡å‹ï¼‰ã€‚
- en: PreTrainedModel
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PreTrainedModel
- en: '### `class transformers.PreTrainedModel`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PreTrainedModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1157)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1157)'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Base class for all models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹çš„åŸºç±»ã€‚
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    takes care of storing the configuration of the models and handles methods for
    loading, downloading and saving models as well as a few methods common to all
    models to:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    è´Ÿè´£å­˜å‚¨æ¨¡å‹çš„é…ç½®ï¼Œå¹¶å¤„ç†åŠ è½½ã€ä¸‹è½½å’Œä¿å­˜æ¨¡å‹ä»¥åŠä¸€äº›æ‰€æœ‰æ¨¡å‹å…±æœ‰çš„æ–¹æ³•ï¼š'
- en: resize the input embeddings,
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œ
- en: prune heads in the self-attention heads.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿®å‰ªè‡ªæ³¨æ„åŠ›å¤´ä¸­çš„å¤´ã€‚
- en: 'Class attributes (overridden by derived classes):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å±æ€§ï¼ˆæ´¾ç”Ÿç±»è¦†ç›–ï¼‰ï¼š
- en: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” A subclass of [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    to use as configuration class for this model architecture.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” ç”¨ä½œæ­¤æ¨¡å‹æ¶æ„çš„é…ç½®ç±»çš„ [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„å­ç±»ã€‚'
- en: '`load_tf_weights` (`Callable`) â€” A python *method* for loading a TensorFlow
    checkpoint in a PyTorch model, taking as arguments:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_tf_weights` (`Callable`) â€” ç”¨äºåœ¨ PyTorch æ¨¡å‹ä¸­åŠ è½½ TensorFlow æ£€æŸ¥ç‚¹çš„ Python *æ–¹æ³•*ï¼Œå‚æ•°ä¸ºï¼š'
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    â€” An instance of the model on which to load the TensorFlow checkpoint.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    â€” è¦åŠ è½½ TensorFlow æ£€æŸ¥ç‚¹çš„æ¨¡å‹å®ä¾‹ã€‚'
- en: '`config` (`PreTrainedConfig`) â€” An instance of the configuration associated
    to the model.'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`PreTrainedConfig`) â€” ä¸æ¨¡å‹å…³è”çš„é…ç½®å®ä¾‹ã€‚'
- en: '`path` (`str`) â€” A path to the TensorFlow checkpoint.'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path` (`str`) â€” TensorFlow æ£€æŸ¥ç‚¹çš„è·¯å¾„ã€‚'
- en: '`base_model_prefix` (`str`) â€” A string indicating the attribute associated
    to the base model in derived classes of the same architecture adding modules on
    top of the base model.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base_model_prefix` (`str`) â€” ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒæŒ‡ç¤ºæ´¾ç”Ÿç±»ä¸­åŸºç¡€æ¨¡å‹å…³è”çš„å±æ€§ï¼Œè¯¥å±æ€§åœ¨åŸºç¡€æ¨¡å‹çš„é¡¶éƒ¨æ·»åŠ æ¨¡å—ã€‚'
- en: '`is_parallelizable` (`bool`) â€” A flag indicating whether this model supports
    model parallelization.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_parallelizable` (`bool`) â€” ä¸€ä¸ªæŒ‡ç¤ºæ­¤æ¨¡å‹æ˜¯å¦æ”¯æŒæ¨¡å‹å¹¶è¡ŒåŒ–çš„æ ‡å¿—ã€‚'
- en: '`main_input_name` (`str`) â€” The name of the principal input to the model (often
    `input_ids` for NLP models, `pixel_values` for vision models and `input_values`
    for speech models).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main_input_name` (`str`) â€” æ¨¡å‹çš„ä¸»è¦è¾“å…¥åç§°ï¼ˆé€šå¸¸ä¸º NLP æ¨¡å‹çš„ `input_ids`ï¼Œè§†è§‰æ¨¡å‹çš„ `pixel_values`
    å’Œè¯­éŸ³æ¨¡å‹çš„ `input_values`ï¼‰ã€‚'
- en: '#### `push_to_hub`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`repo_id` (`str`) â€” The name of the repository you want to push your model
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id` (`str`) â€” æ‚¨è¦æ¨é€æ¨¡å‹çš„å­˜å‚¨åº“åç§°ã€‚åœ¨æ¨é€åˆ°ç‰¹å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚'
- en: '`use_temp_dir` (`bool`, *optional*) â€” Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨ä¿å­˜çš„æ–‡ä»¶ï¼Œç„¶åå°†å®ƒä»¬æ¨é€åˆ° Hubã€‚å¦‚æœæ²¡æœ‰åä¸º`repo_id`çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚'
- en: '`commit_message` (`str`, *optional*) â€” Message to commit while pushing. Will
    default to `"Upload model"`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º`"Upload model"`ã€‚'
- en: '`private` (`bool`, *optional*) â€” Whether or not the repository created should
    be private.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦åº”è¯¥åˆ›å»ºç§æœ‰å­˜å‚¨åº“ã€‚'
- en: '`token` (`bool` or `str`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`ï¼ˆ`bool`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ`huggingface-cli
    login`æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ä¸­ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š`repo_url`ï¼Œåˆ™é»˜è®¤ä¸º`True`ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) â€” Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size`ï¼ˆ`int`æˆ–`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"5GB"`ï¼‰â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹å°†åˆ†ç‰‡ï¼Œæ¯ä¸ªåˆ†ç‰‡çš„å¤§å°éƒ½å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚`"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤è®¾ç½®ä¸º`"5GB"`ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åœ¨å…è´¹çš„
    Google Colab å®ä¾‹ä¸Šè½»æ¾åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°ä»»ä½• CPU OOM é—®é¢˜ã€‚'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) â€” Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åˆ›å»ºå¸¦æœ‰ä¸Šä¼ æ–‡ä»¶çš„PRæˆ–ç›´æ¥æäº¤ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å°†æ¨¡å‹æƒé‡è½¬æ¢ä¸ºsafetensorsæ ¼å¼ä»¥è¿›è¡Œæ›´å®‰å…¨çš„åºåˆ—åŒ–ã€‚'
- en: '`revision` (`str`, *optional*) â€” Branch to push the uploaded files to.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” è¦å°†ä¸Šä¼ çš„æ–‡ä»¶æ¨é€åˆ°çš„åˆ†æ”¯ã€‚'
- en: '`commit_description` (`str`, *optional*) â€” The description of the commit that
    will be created'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_description`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” å°†åˆ›å»ºçš„æäº¤çš„æè¿°'
- en: '`tags` (`List[str]`, *optional*) â€” List of tags to push on the Hub.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” è¦æ¨é€åˆ° Hub ä¸Šçš„æ ‡ç­¾åˆ—è¡¨ã€‚'
- en: Upload the model file to the ğŸ¤— Model Hub.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹æ–‡ä»¶ä¸Šä¼ åˆ°ğŸ¤—æ¨¡å‹ä¸­å¿ƒã€‚
- en: 'Examples:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `add_model_tags`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_model_tags`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1270)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1270)'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tags` (`Union[List[str], str]`) â€” The desired tags to inject in the model'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`ï¼ˆ`Union[List[str], str]`ï¼‰â€” è¦æ³¨å…¥æ¨¡å‹ä¸­çš„æ‰€éœ€æ ‡ç­¾ã€‚'
- en: Add custom tags into the model that gets pushed to the Hugging Face Hub. Will
    not overwrite existing tags in the model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è‡ªå®šä¹‰æ ‡ç­¾æ·»åŠ åˆ°æ¨é€åˆ° Hugging Face Hub çš„æ¨¡å‹ä¸­ã€‚ä¸ä¼šè¦†ç›–æ¨¡å‹ä¸­çš„ç°æœ‰æ ‡ç­¾ã€‚
- en: 'Examples:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `can_generate`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `can_generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1438)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1438)'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Returns
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`bool`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`bool`'
- en: Whether this model can generate sequences with `.generate()`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯å¦å¯ä»¥ç”Ÿæˆåºåˆ—ï¼Œä½¿ç”¨`.generate()`ã€‚
- en: Returns whether this model can generate sequences with `.generate()`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›æ­¤æ¨¡å‹æ˜¯å¦å¯ä»¥ç”Ÿæˆåºåˆ—ï¼Œä½¿ç”¨`.generate()`ã€‚
- en: '#### `disable_input_require_grads`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_input_require_grads`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1582)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1582)'
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Removes the `_require_grads_hook`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ é™¤`_require_grads_hook`ã€‚
- en: '#### `enable_input_require_grads`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_input_require_grads`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1571)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1571)'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enables the gradients for the input embeddings. This is useful for fine-tuning
    adapter weights while keeping the model weights fixed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨è¾“å…¥åµŒå…¥çš„æ¢¯åº¦ã€‚è¿™å¯¹äºå¾®è°ƒé€‚é…å™¨æƒé‡å¹¶ä¿æŒæ¨¡å‹æƒé‡å›ºå®šå¾ˆæœ‰ç”¨ã€‚
- en: '#### `from_pretrained`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2617)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2617)'
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`, *optional*) â€” Can
    be either:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path`ï¼ˆ`str`æˆ–`os.PathLike`ï¼Œ*å¯é€‰*ï¼‰â€” å¯ä»¥æ˜¯ï¼š'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ‰˜ç®¡åœ¨huggingface.coæ¨¡å‹å­˜å‚¨åº“ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹çš„*æ¨¡å‹ID*ã€‚æœ‰æ•ˆçš„æ¨¡å‹IDå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹å‘½åç©ºé—´åŒ–ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡å‘åŒ…å«ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)ä¿å­˜çš„æ¨¡å‹æƒé‡çš„*ç›®å½•*çš„è·¯å¾„ï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/`ã€‚
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒ‡å‘*TensorFlowç´¢å¼•æ£€æŸ¥ç‚¹æ–‡ä»¶*çš„è·¯å¾„æˆ–URLï¼ˆä¾‹å¦‚ï¼Œ`./tf_model/model.ckpt.index`ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåº”å°†`from_tf`è®¾ç½®ä¸º`True`ï¼Œå¹¶å°†é…ç½®å¯¹è±¡ä½œä¸º`config`å‚æ•°æä¾›ã€‚ä½¿ç”¨æ­¤åŠ è½½è·¯å¾„æ¯”ä½¿ç”¨æä¾›çš„è½¬æ¢è„šæœ¬å°†TensorFlowæ£€æŸ¥ç‚¹è½¬æ¢ä¸ºPyTorchæ¨¡å‹å¹¶éšååŠ è½½PyTorchæ¨¡å‹è¦æ…¢ã€‚
- en: A path or url to a model folder containing a *flax checkpoint file* in *.msgpack*
    format (e.g, `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax`
    should be set to `True`.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«*flax checkpoint file*çš„æ¨¡å‹æ–‡ä»¶å¤¹çš„è·¯å¾„æˆ–urlï¼Œæ ¼å¼ä¸º*.msgpack*ï¼ˆä¾‹å¦‚ï¼Œ`./flax_model/`åŒ…å«`flax_model.msgpack`ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`from_flax`åº”è®¾ç½®ä¸º`True`ã€‚
- en: '`None` if you are both providing the configuration and state dictionary (resp.
    with keyword arguments `config` and `state_dict`).'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åŒæ—¶æä¾›é…ç½®å’ŒçŠ¶æ€å­—å…¸ï¼ˆåˆ†åˆ«ä½¿ç”¨å…³é”®å­—å‚æ•°`config`å’Œ`state_dict`ï¼‰ï¼Œåˆ™ä¸º`None`ã€‚
- en: '`model_args` (sequence of positional arguments, *optional*) â€” All remaining
    positional arguments will be passed to the underlying modelâ€™s `__init__` method.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`ï¼ˆä½ç½®å‚æ•°åºåˆ—ï¼Œ*å¯é€‰*ï¼‰â€” æ‰€æœ‰å‰©ä½™çš„ä½ç½®å‚æ•°å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹çš„`__init__`æ–¹æ³•ã€‚'
- en: '`config` (`Union[PretrainedConfig, str, os.PathLike]`, *optional*) â€” Can be
    either:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ`Union[PretrainedConfig, str, os.PathLike]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯ä»¥æ˜¯ï¼š'
- en: an instance of a class derived from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)æ´¾ç”Ÿçš„ç±»çš„å®ä¾‹ï¼Œ
- en: a string or path valid as input to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained).
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä½œä¸ºè¾“å…¥æœ‰æ•ˆçš„å­—ç¬¦ä¸²æˆ–è·¯å¾„ï¼Œç”¨äº[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)ã€‚
- en: 'Configuration for the model to use instead of an automatically loaded configuration.
    Configuration can be automatically loaded when:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»£æ›¿è‡ªåŠ¨åŠ è½½çš„é…ç½®ä½¿ç”¨çš„æ¨¡å‹é…ç½®ã€‚å½“ï¼š
- en: The model is a model provided by the library (loaded with the *model id* string
    of a pretrained model).
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯åº“æä¾›çš„æ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„*æ¨¡å‹id*å­—ç¬¦ä¸²åŠ è½½ï¼‰ã€‚
- en: The model was saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    and is reloaded by supplying the save directory.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)ä¿å­˜çš„ï¼Œå¹¶é€šè¿‡æä¾›ä¿å­˜ç›®å½•é‡æ–°åŠ è½½ã€‚
- en: The model is loaded by supplying a local directory as `pretrained_model_name_or_path`
    and a configuration JSON file named *config.json* is found in the directory.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æä¾›æœ¬åœ°ç›®å½•ä½œä¸º`pretrained_model_name_or_path`åŠ è½½æ¨¡å‹ï¼Œå¹¶åœ¨ç›®å½•ä¸­æ‰¾åˆ°åä¸º*config.json*çš„é…ç½®JSONæ–‡ä»¶ã€‚
- en: '`state_dict` (`Dict[str, torch.Tensor]`, *optional*) â€” A state dictionary to
    use instead of a state dictionary loaded from saved weights file.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict`ï¼ˆ`Dict[str, torch.Tensor]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªçŠ¶æ€å­—å…¸ï¼Œç”¨äºæ›¿ä»£ä»ä¿å­˜çš„æƒé‡æ–‡ä»¶åŠ è½½çš„çŠ¶æ€å­—å…¸ã€‚'
- en: This option can be used if you want to create a model from a pretrained configuration
    but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    and [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    is not a simpler option.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³ä»é¢„è®­ç»ƒé…ç½®åˆ›å»ºæ¨¡å‹ä½†åŠ è½½è‡ªå·±çš„æƒé‡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨æ­¤é€‰é¡¹ã€‚ä¸è¿‡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥æ£€æŸ¥æ˜¯å¦ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)å’Œ[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)ä¸æ˜¯æ›´ç®€å•çš„é€‰é¡¹ã€‚
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” Path to a directory in
    which a downloaded pretrained model configuration should be cached if the standard
    cache should not be used.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir`ï¼ˆ`Union[str, os.PathLike]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹é…ç½®åº”ç¼“å­˜åœ¨å…¶ä¸­çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚'
- en: '`from_tf` (`bool`, *optional*, defaults to `False`) â€” Load the model weights
    from a TensorFlow checkpoint save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_tf`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” ä»TensorFlowæ£€æŸ¥ç‚¹ä¿å­˜æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡ï¼ˆè¯·å‚é˜…`pretrained_model_name_or_path`å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`from_flax` (`bool`, *optional*, defaults to `False`) â€” Load the model weights
    from a Flax checkpoint save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_flax`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” ä»Flaxæ£€æŸ¥ç‚¹ä¿å­˜æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡ï¼ˆè¯·å‚é˜…`pretrained_model_name_or_path`å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`ignore_mismatched_sizes` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to raise an error if some of the weights from the checkpoint do not have
    the same size as the weights of the model (if for instance, you are instantiating
    a model with 10 labels from a checkpoint with 3 labels).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_mismatched_sizes`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” å¦‚æœæ£€æŸ¥ç‚¹ä¸­çš„æŸäº›æƒé‡ä¸æ¨¡å‹çš„æƒé‡å¤§å°ä¸åŒï¼Œæ˜¯å¦å¼•å‘é”™è¯¯ï¼ˆä¾‹å¦‚ï¼Œæ‚¨ä»å…·æœ‰3ä¸ªæ ‡ç­¾çš„æ£€æŸ¥ç‚¹å®ä¾‹åŒ–å…·æœ‰10ä¸ªæ ‡ç­¾çš„æ¨¡å‹ï¼‰ã€‚'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åˆ é™¤æ¥æ”¶ä¸å®Œæ•´çš„æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„æ–‡ä»¶ï¼Œå°†å°è¯•æ¢å¤ä¸‹è½½ã€‚'
- en: '`proxies` (`Dict[str, str]`, *optional*) â€” A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies`ï¼ˆ`Dict[str, str]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªä»£ç†æœåŠ¡å™¨å­—å…¸ï¼ŒæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨ï¼Œä¾‹å¦‚ï¼Œ`{''http'': ''foo.bar:3128'',
    ''http://hostname'': ''foo.bar:4012''}`ã€‚ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸Šä½¿ç”¨ã€‚'
- en: '`output_loading_info(bool,` *optional*, defaults to `False`) â€” Whether ot not
    to also return a dictionary containing missing keys, unexpected keys and error
    messages.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_loading_info(bool,` *å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ªåŒ…å«ç¼ºå¤±é”®ã€æ„å¤–é”®å’Œé”™è¯¯æ¶ˆæ¯çš„å­—å…¸ã€‚'
- en: '`local_files_only(bool,` *optional*, defaults to `False`) â€” Whether or not
    to only look at local files (i.e., do not try to download the model).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only(bool,` *å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä»…æŸ¥çœ‹æœ¬åœ°æ–‡ä»¶ï¼ˆå³ï¼Œä¸å°è¯•ä¸‹è½½æ¨¡å‹ï¼‰ã€‚'
- en: '`token` (`str` or `bool`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`ï¼ˆ`str`æˆ–`bool`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`æˆ–æœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è¿è¡Œ`huggingface-cli
    login`æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ä¸­ï¼‰ã€‚'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) â€” The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å®ƒå¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°æˆ–æäº¤ IDï¼Œå› ä¸ºæˆ‘ä»¬åœ¨
    huggingface.co ä¸Šä½¿ç”¨åŸºäº git çš„ç³»ç»Ÿæ¥å­˜å‚¨æ¨¡å‹å’Œå…¶ä»–å·¥ä»¶ï¼Œæ‰€ä»¥ `revision` å¯ä»¥æ˜¯ git å…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚'
- en: To test a pull request you made on the Hub, you can pass `revision=â€œrefs/pr/<pr_number>â€œ.</pr_number>
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¦æµ‹è¯•æ‚¨åœ¨ Hub ä¸Šåˆ›å»ºçš„æ‹‰å–è¯·æ±‚ï¼Œå¯ä»¥ä¼ é€’ `revision=â€œrefs/pr/<pr_number>â€œã€‚</pr_number>
- en: '`mirror` (`str`, *optional*) â€” Mirror source to accelerate downloads in China.
    If you are from China and have an accessibility problem, you can set this option
    to resolve it. Note that we do not guarantee the timeliness or safety. Please
    refer to the mirror site for more information.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mirror` (`str`, *å¯é€‰*) â€” é•œåƒæºä»¥åŠ é€Ÿä¸­å›½çš„ä¸‹è½½ã€‚å¦‚æœæ‚¨æ¥è‡ªä¸­å›½å¹¶ä¸”æœ‰è®¿é—®é—®é¢˜ï¼Œæ‚¨å¯ä»¥è®¾ç½®æ­¤é€‰é¡¹æ¥è§£å†³ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ä¿è¯åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ã€‚è¯·å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: '`_fast_init(bool,` *optional*, defaults to `True`) â€” Whether or not to disable
    fast initialization.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_fast_init(bool,` *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦ç¦ç”¨å¿«é€Ÿåˆå§‹åŒ–ã€‚'
- en: One should only disable *_fast_init* to ensure backwards compatibility with
    `transformers.__version__ < 4.6.0` for seeded model initialization. This argument
    will be removed at the next major version. See [pull request 11471](https://github.com/huggingface/transformers/pull/11471)
    for more information.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®ä¿ä¸ `transformers.__version__ < 4.6.0` çš„ç§å­æ¨¡å‹åˆå§‹åŒ–å‘åå…¼å®¹ï¼Œåº”è¯¥åªç¦ç”¨ *_fast_init*ã€‚æ­¤å‚æ•°å°†åœ¨ä¸‹ä¸€ä¸ªä¸»è¦ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ‹‰å–è¯·æ±‚
    11471](https://github.com/huggingface/transformers/pull/11471)ã€‚
- en: Parameters for big model inference
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹æ¨¡å‹æ¨ç†çš„å‚æ•°
- en: '`low_cpu_mem_usage(bool,` *optional*) â€” Tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. This is an
    experimental feature and a subject to change at any moment.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage(bool,` *å¯é€‰*) â€” å°è¯•åœ¨åŠ è½½æ¨¡å‹æ—¶ä¸ä½¿ç”¨è¶…è¿‡ CPU å†…å­˜ä¸­çš„ 1x æ¨¡å‹å¤§å°ï¼ˆåŒ…æ‹¬å³°å€¼å†…å­˜ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œéšæ—¶å¯èƒ½æ›´æ”¹ã€‚'
- en: '`torch_dtype` (`str` or `torch.dtype`, *optional*) â€” Override the default `torch.dtype`
    and load the model under a specific `dtype`. The different options are:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch_dtype` (`str` æˆ– `torch.dtype`, *å¯é€‰*) â€” è¦†ç›–é»˜è®¤çš„ `torch.dtype` å¹¶åœ¨ç‰¹å®šçš„ `dtype`
    ä¸‹åŠ è½½æ¨¡å‹ã€‚ä¸åŒçš„é€‰é¡¹æœ‰ï¼š'
- en: '`torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified `dtype`,
    ignoring the modelâ€™s `config.torch_dtype` if one exists. If not specified'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.float16` æˆ– `torch.bfloat16` æˆ– `torch.float`: ä»¥æŒ‡å®šçš„ `dtype` åŠ è½½ï¼Œå¿½ç•¥æ¨¡å‹çš„ `config.torch_dtype`ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š'
- en: the model will get loaded in `torch.float` (fp32).
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹å°†ä»¥ `torch.float` (fp32) åŠ è½½ã€‚
- en: '`"auto"` - A `torch_dtype` entry in the `config.json` file of the model will
    be attempted to be used. If this entry isnâ€™t found then next check the `dtype`
    of the first weight in the checkpoint thatâ€™s of a floating point type and use
    that as `dtype`. This will load the model using the `dtype` it was saved in at
    the end of the training. It canâ€™t be used as an indicator of how the model was
    trained. Since it could be trained in one of half precision dtypes, but saved
    in fp32.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"auto"` - å°†å°è¯•ä½¿ç”¨æ¨¡å‹çš„ `config.json` æ–‡ä»¶ä¸­çš„ `torch_dtype` æ¡ç›®ã€‚å¦‚æœæ‰¾ä¸åˆ°æ­¤æ¡ç›®ï¼Œåˆ™ä¸‹ä¸€ä¸ªæ£€æŸ¥æ˜¯æ£€æŸ¥ç‚¹ä¸­ç¬¬ä¸€ä¸ªæµ®ç‚¹ç±»å‹çš„æƒé‡çš„
    `dtype` å¹¶å°†å…¶ç”¨ä½œ `dtype`ã€‚è¿™å°†ä½¿ç”¨æ¨¡å‹åœ¨è®­ç»ƒç»“æŸæ—¶ä¿å­˜çš„ `dtype` åŠ è½½æ¨¡å‹ã€‚å®ƒä¸èƒ½ç”¨ä½œæ¨¡å‹è®­ç»ƒæ–¹å¼çš„æŒ‡ç¤ºå™¨ã€‚å› ä¸ºå®ƒå¯èƒ½æ˜¯åœ¨åŠç²¾åº¦
    `dtype` ä¸­è®­ç»ƒï¼Œä½†ä»¥ fp32 ä¿å­˜ã€‚'
- en: <tip>For some models the `dtype` they were trained in is unknown - you may try
    to check the modelâ€™s paper or reach out to the authors and ask them to add this
    information to the modelâ€™s card and to insert the `torch_dtype` entry in `config.json`
    on the hub.</tip>
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <tip>å¯¹äºä¸€äº›æ¨¡å‹ï¼Œå®ƒä»¬è®­ç»ƒæ—¶ä½¿ç”¨çš„ `dtype` æ˜¯æœªçŸ¥çš„ - æ‚¨å¯ä»¥å°è¯•æŸ¥çœ‹æ¨¡å‹çš„è®ºæ–‡æˆ–è”ç³»ä½œè€…ï¼Œå¹¶è¦æ±‚ä»–ä»¬å°†æ­¤ä¿¡æ¯æ·»åŠ åˆ°æ¨¡å‹çš„å¡ç‰‡ä¸­ï¼Œå¹¶åœ¨
    Hub ä¸Šçš„ `config.json` ä¸­æ’å…¥ `torch_dtype` æ¡ç›®ã€‚</tip>
- en: '`device_map` (`str` or `Dict[str, Union[int, str, torch.device]]` or `int`
    or `torch.device`, *optional*) â€” A map that specifies where each submodule should
    go. It doesnâ€™t need to be refined to each parameter/buffer name, once a given
    module name is inside, every submodule of it will be sent to the same device.
    If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal
    rank like `1`) on which the model will be allocated, the device map will map the
    entire model to this device. Passing `device_map = 0` means put the whole model
    on GPU 0.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map` (`str` æˆ– `Dict[str, Union[int, str, torch.device]]` æˆ– `int` æˆ–
    `torch.device`, *å¯é€‰*) â€” ä¸€ä¸ªæŒ‡å®šæ¯ä¸ªå­æ¨¡å—åº”è¯¥æ”¾åœ¨å“ªé‡Œçš„æ˜ å°„ã€‚å®ƒä¸éœ€è¦ç»†åŒ–åˆ°æ¯ä¸ªå‚æ•°/ç¼“å†²åŒºåç§°ï¼Œä¸€æ—¦ç»™å®šæ¨¡å—åç§°åœ¨å†…ï¼Œå®ƒçš„æ¯ä¸ªå­æ¨¡å—éƒ½å°†è¢«å‘é€åˆ°åŒä¸€è®¾å¤‡ã€‚å¦‚æœæˆ‘ä»¬åªä¼ é€’æ¨¡å‹å°†è¢«åˆ†é…çš„è®¾å¤‡ï¼ˆä¾‹å¦‚ï¼Œ`"cpu"`ã€`"cuda:1"`ã€`"mps"`ï¼Œæˆ–è€…åƒ
    `1` è¿™æ ·çš„ GPU åºæ•°ç­‰ï¼‰ï¼Œè®¾å¤‡æ˜ å°„å°†æŠŠæ•´ä¸ªæ¨¡å‹æ˜ å°„åˆ°è¿™ä¸ªè®¾å¤‡ä¸Šã€‚ä¼ é€’ `device_map = 0` æ„å‘³ç€å°†æ•´ä¸ªæ¨¡å‹æ”¾åœ¨ GPU 0 ä¸Šã€‚'
- en: To have Accelerate compute the most optimized `device_map` automatically, set
    `device_map="auto"`. For more information about each option see [designing a device
    map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®© Accelerate è‡ªåŠ¨è®¡ç®—æœ€ä¼˜åŒ–çš„ `device_map`ï¼Œè¯·è®¾ç½® `device_map="auto"`ã€‚æœ‰å…³æ¯ä¸ªé€‰é¡¹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®¾è®¡è®¾å¤‡æ˜ å°„](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map)ã€‚
- en: '`max_memory` (`Dict`, *optional*) â€” A dictionary device identifier to maximum
    memory. Will default to the maximum memory available for each GPU and the available
    CPU RAM if unset.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_memory` (`Dict`, *å¯é€‰*) â€” è®¾å¤‡æ ‡è¯†ç¬¦åˆ°æœ€å¤§å†…å­˜çš„å­—å…¸ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºæ¯ä¸ª GPU å¯ç”¨çš„æœ€å¤§å†…å­˜å’Œå¯ç”¨çš„ CPU
    RAMã€‚'
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) â€” If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_folder` (`str` æˆ– `os.PathLike`, *å¯é€‰*) â€” å¦‚æœ `device_map` åŒ…å«ä»»ä½•å€¼ `"disk"`ï¼Œåˆ™æˆ‘ä»¬å°†å¸è½½æƒé‡çš„æ–‡ä»¶å¤¹ã€‚'
- en: '`offload_state_dict` (`bool`, *optional*) â€” If `True`, will temporarily offload
    the CPU state dict to the hard drive to avoid getting out of CPU RAM if the weight
    of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults
    to `True` when there is some disk offload.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_state_dict` (`bool`, *å¯é€‰*) â€” å¦‚æœä¸º `True`ï¼Œå°†ä¸´æ—¶å°† CPU çŠ¶æ€å­—å…¸è½¬ç§»åˆ°ç¡¬ç›˜ï¼Œä»¥é¿å… CPU
    RAM ä¸è¶³ï¼Œå¦‚æœ CPU çŠ¶æ€å­—å…¸çš„é‡é‡ + æ£€æŸ¥ç‚¹çš„æœ€å¤§åˆ†ç‰‡ä¸é€‚åˆã€‚å½“å­˜åœ¨ä¸€äº›ç£ç›˜å¸è½½æ—¶ï¼Œé»˜è®¤ä¸º `True`ã€‚'
- en: '`load_in_8bit` (`bool`, *optional*, defaults to `False`) â€” If `True`, will
    convert the loaded model into mixed-8bit quantized model. To use this feature
    please install `bitsandbytes` (`pip install -U bitsandbytes`).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_in_8bit` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º`True`ï¼Œå°†åŠ è½½çš„æ¨¡å‹è½¬æ¢ä¸ºæ··åˆ8ä½é‡åŒ–æ¨¡å‹ã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·å®‰è£…`bitsandbytes`ï¼ˆ`pip
    install -U bitsandbytes`ï¼‰ã€‚'
- en: '`load_in_4bit` (`bool`, *optional*, defaults to `False`) â€” If `True`, will
    convert the loaded model into 4bit precision quantized model. To use this feature
    install the latest version of `bitsandbytes` (`pip install -U bitsandbytes`).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_in_4bit` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º`True`ï¼Œå°†åŠ è½½çš„æ¨¡å‹è½¬æ¢ä¸º4ä½ç²¾åº¦é‡åŒ–æ¨¡å‹ã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`bitsandbytes`ï¼ˆ`pip
    install -U bitsandbytes`ï¼‰ã€‚'
- en: '`quantization_config` (`Union[QuantizationConfigMixin,Dict]`, *optional*) â€”
    A dictionary of configuration parameters or a QuantizationConfigMixin object for
    quantization (e.g bitsandbytes, gptq)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quantization_config` (`Union[QuantizationConfigMixin,Dict]`, *å¯é€‰*) â€” é‡åŒ–çš„é…ç½®å‚æ•°å­—å…¸æˆ–QuantizationConfigMixinå¯¹è±¡ï¼ˆä¾‹å¦‚bitsandbytes,
    gptqï¼‰'
- en: '`subfolder` (`str`, *optional*, defaults to `""`) â€” In case the relevant files
    are located inside a subfolder of the model repo on huggingface.co, you can specify
    the folder name here.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subfolder` (`str`, *optional*, é»˜è®¤ä¸º `""`) â€” å¦‚æœç›¸å…³æ–‡ä»¶ä½äºhuggingface.coæ¨¡å‹ä»“åº“çš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡ŒæŒ‡å®šæ–‡ä»¶å¤¹åç§°ã€‚'
- en: '`variant` (`str`, *optional*) â€” If specified load weights from `variant` filename,
    *e.g.* pytorch_model.<variant>.bin. `variant` is ignored when using `from_tf`
    or `from_flax`.</variant>'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`variant` (`str`, *optional*) â€” å¦‚æœæŒ‡å®šï¼Œå°†ä»`variant`æ–‡ä»¶ååŠ è½½æƒé‡ï¼Œä¾‹å¦‚pytorch_model.<variant>.binã€‚åœ¨ä½¿ç”¨`from_tf`æˆ–`from_flax`æ—¶å¿½ç•¥`variant`ã€‚</variant>'
- en: '`use_safetensors` (`bool`, *optional*, defaults to `None`) â€” Whether or not
    to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
    is not installed, it will be set to `False`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_safetensors` (`bool`, *optional*, é»˜è®¤ä¸º `None`) â€” æ˜¯å¦ä½¿ç”¨`safetensors`æ£€æŸ¥ç‚¹ã€‚é»˜è®¤ä¸º`None`ã€‚å¦‚æœæœªæŒ‡å®šå¹¶ä¸”æœªå®‰è£…`safetensors`ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º`False`ã€‚'
- en: '`kwargs` (remaining dictionary of keyword arguments, *optional*) â€” Can be used
    to update the configuration object (after it being loaded) and initiate the model
    (e.g., `output_attentions=True`). Behaves differently depending on whether a `config`
    is provided or automatically loaded:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆå‰©ä½™çš„å…³é”®å­—å‚æ•°å­—å…¸ï¼Œ*å¯é€‰*ï¼‰ â€” å¯ç”¨äºæ›´æ–°é…ç½®å¯¹è±¡ï¼ˆåŠ è½½åï¼‰å¹¶åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œ`output_attentions=True`ï¼‰ã€‚æ ¹æ®æ˜¯å¦æä¾›äº†`config`æˆ–è‡ªåŠ¨åŠ è½½ï¼Œè¡Œä¸ºä¼šæœ‰æ‰€ä¸åŒï¼š'
- en: If a configuration is provided with `config`, `**kwargs` will be directly passed
    to the underlying modelâ€™s `__init__` method (we assume all relevant updates to
    the configuration have already been done)
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæä¾›äº†é…ç½®`config`ï¼Œ`**kwargs`å°†ç›´æ¥ä¼ é€’ç»™åº•å±‚æ¨¡å‹çš„`__init__`æ–¹æ³•ï¼ˆæˆ‘ä»¬å‡è®¾é…ç½®çš„æ‰€æœ‰ç›¸å…³æ›´æ–°å·²ç»å®Œæˆï¼‰
- en: If a configuration is not provided, `kwargs` will be first passed to the configuration
    class initialization function ([from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)).
    Each key of `kwargs` that corresponds to a configuration attribute will be used
    to override said attribute with the supplied `kwargs` value. Remaining keys that
    do not correspond to any configuration attribute will be passed to the underlying
    modelâ€™s `__init__` function.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæœªæä¾›é…ç½®ï¼Œ`kwargs`å°†é¦–å…ˆä¼ é€’ç»™é…ç½®ç±»åˆå§‹åŒ–å‡½æ•°ï¼ˆ[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)ï¼‰ã€‚ä¸é…ç½®å±æ€§å¯¹åº”çš„`kwargs`çš„æ¯ä¸ªé”®å°†ç”¨æä¾›çš„`kwargs`å€¼è¦†ç›–è¯¥å±æ€§ã€‚ä¸å¯¹åº”ä»»ä½•é…ç½®å±æ€§çš„å‰©ä½™é”®å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹çš„`__init__`å‡½æ•°ã€‚
- en: Instantiate a pretrained pytorch model from a pre-trained model configuration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒæ¨¡å‹é…ç½®å®ä¾‹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒçš„pytorchæ¨¡å‹ã€‚
- en: The model is set in evaluation mode by default using `model.eval()` (Dropout
    modules are deactivated). To train the model, you should first set it back in
    training mode with `model.train()`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥è¯„ä¼°æ¨¡å¼è®¾ç½®ï¼Œä½¿ç”¨`model.eval()`ï¼ˆDropoutæ¨¡å—è¢«åœç”¨ï¼‰ã€‚è¦è®­ç»ƒæ¨¡å‹ï¼Œæ‚¨åº”è¯¥é¦–å…ˆä½¿ç”¨`model.train()`å°†å…¶è®¾ç½®å›è®­ç»ƒæ¨¡å¼ã€‚
- en: The warning *Weights from XXX not initialized from pretrained model* means that
    the weights of XXX do not come pretrained with the rest of the model. It is up
    to you to train those weights with a downstream fine-tuning task.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š*Weights from XXX not initialized from pretrained model*è¡¨ç¤ºXXXçš„æƒé‡ä¸æ˜¯ä¸æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ä¸€èµ·é¢„è®­ç»ƒçš„ã€‚æ‚¨éœ€è¦ä½¿ç”¨ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡æ¥è®­ç»ƒè¿™äº›æƒé‡ã€‚
- en: The warning *Weights from XXX not used in YYY* means that the layer XXX is not
    used by YYY, therefore those weights are discarded.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š*Weights from XXX not used in YYY*è¡¨ç¤ºå±‚XXXæœªè¢«YYYä½¿ç”¨ï¼Œå› æ­¤è¿™äº›æƒé‡å°†è¢«ä¸¢å¼ƒã€‚
- en: Activate the special [â€œoffline-modeâ€](https://huggingface.co/transformers/installation.html#offline-mode)
    to use this method in a firewalled environment.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»ç‰¹æ®Šçš„[â€œç¦»çº¿æ¨¡å¼â€](https://huggingface.co/transformers/installation.html#offline-mode)ä»¥åœ¨å—é˜²ç«å¢™ä¿æŠ¤çš„ç¯å¢ƒä¸­ä½¿ç”¨æ­¤æ–¹æ³•ã€‚
- en: 'Examples:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`low_cpu_mem_usage` algorithm:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage`ç®—æ³•ï¼š'
- en: This is an experimental function that loads the model using ~1x model size CPU
    memory
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨çº¦1å€æ¨¡å‹å¤§å°CPUå†…å­˜åŠ è½½æ¨¡å‹çš„å®éªŒæ€§åŠŸèƒ½
- en: 'Here is how it works:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å®ƒçš„å·¥ä½œåŸç†ï¼š
- en: save which state_dict keys we have
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¿å­˜æˆ‘ä»¬æ‹¥æœ‰çš„state_dicté”®
- en: drop state_dict before the model is created, since the latter takes 1x model
    size CPU memory
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨åˆ›å»ºæ¨¡å‹ä¹‹å‰åˆ é™¤state_dictï¼Œå› ä¸ºåè€…éœ€è¦1å€æ¨¡å‹å¤§å°çš„CPUå†…å­˜
- en: after the model has been instantiated switch to the meta device all params/buffers
    that are going to be replaced from the loaded state_dict
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å®ä¾‹åŒ–æ¨¡å‹åï¼Œåˆ‡æ¢åˆ°å…ƒè®¾å¤‡ï¼Œæ‰€æœ‰å°†ä»åŠ è½½çš„state_dictä¸­æ›¿æ¢çš„å‚æ•°/ç¼“å†²åŒº
- en: load state_dict 2nd time
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ¬¡åŠ è½½state_dict
- en: replace the params/buffers from the state_dict
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»state_dictä¸­æ›¿æ¢å‚æ•°/ç¼“å†²åŒº
- en: Currently, it canâ€™t handle deepspeed ZeRO stage 3 and ignores loading errors
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œå®ƒæ— æ³•å¤„ç†deepspeed ZeROé˜¶æ®µ3å¹¶å¿½ç•¥åŠ è½½é”™è¯¯
- en: '#### `get_input_embeddings`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_input_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1588)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1588)'
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`nn.Module`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Module`'
- en: A torch module mapping vocabulary to hidden states.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯æ±‡æ˜ å°„åˆ°éšè—çŠ¶æ€çš„torchæ¨¡å—ã€‚
- en: Returns the modelâ€™s input embeddings.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›æ¨¡å‹çš„è¾“å…¥åµŒå…¥ã€‚
- en: '#### `get_memory_footprint`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_memory_footprint`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2540)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2540)'
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`return_buffers` (`bool`, *optional*, defaults to `True`) â€” Whether to return
    the size of the buffer tensors in the computation of the memory footprint. Buffers
    are tensors that do not require gradients and not registered as parameters. E.g.
    mean and std in batch norm layers. Please see: [https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2](https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_buffers`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨è®¡ç®—å†…å­˜å ç”¨æ—¶è¿”å›ç¼“å†²å¼ é‡çš„å¤§å°ã€‚ç¼“å†²åŒºæ˜¯ä¸éœ€è¦æ¢¯åº¦ä¸”æœªæ³¨å†Œä¸ºå‚æ•°çš„å¼ é‡ã€‚ä¾‹å¦‚ï¼Œæ‰¹é‡å½’ä¸€åŒ–å±‚ä¸­çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚è¯·å‚è§ï¼š[https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2](https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2)'
- en: 'Get the memory footprint of a model. This will return the memory footprint
    of the current model in bytes. Useful to benchmark the memory footprint of the
    current model and design some tests. Solution inspired from the PyTorch discussions:
    [https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2](https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚è¿™å°†ä»¥å­—èŠ‚ä¸ºå•ä½è¿”å›å½“å‰æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚æœ‰åŠ©äºåŸºå‡†æµ‹è¯•å½“å‰æ¨¡å‹çš„å†…å­˜å ç”¨å¹¶è®¾è®¡ä¸€äº›æµ‹è¯•ã€‚è§£å†³æ–¹æ¡ˆçµæ„Ÿæ¥è‡ªPyTorchè®¨è®ºï¼š[https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2](https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2)
- en: '#### `get_output_embeddings`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_output_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1614)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1614)'
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Returns
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`nn.Module`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Module`'
- en: A torch module mapping hidden states to vocabulary.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨çš„torchæ¨¡å—ã€‚
- en: Returns the modelâ€™s output embeddings.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›æ¨¡å‹çš„è¾“å‡ºåµŒå…¥ã€‚
- en: '#### `gradient_checkpointing_disable`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `gradient_checkpointing_disable`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2166)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2166)'
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Deactivates gradient checkpointing for the current model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºå½“å‰æ¨¡å‹åœç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚
- en: Note that in other frameworks this feature can be referred to as â€œactivation
    checkpointingâ€ or â€œcheckpoint activationsâ€.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨å…¶ä»–æ¡†æ¶ä¸­ï¼Œæ­¤åŠŸèƒ½å¯èƒ½è¢«ç§°ä¸ºâ€œæ¿€æ´»æ£€æŸ¥ç‚¹â€æˆ–â€œæ£€æŸ¥ç‚¹æ¿€æ´»â€ã€‚
- en: '#### `gradient_checkpointing_enable`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `gradient_checkpointing_enable`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2102)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2102)'
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`gradient_checkpointing_kwargs` (dict, *optional*) â€” Additional keyword arguments
    passed along to the `torch.utils.checkpoint.checkpoint` function.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_checkpointing_kwargs`ï¼ˆå­—å…¸ï¼Œ*å¯é€‰*ï¼‰â€” ä¼ é€’ç»™`torch.utils.checkpoint.checkpoint`å‡½æ•°çš„é™„åŠ å…³é”®å­—å‚æ•°ã€‚'
- en: Activates gradient checkpointing for the current model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºå½“å‰æ¨¡å‹æ¿€æ´»æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚
- en: Note that in other frameworks this feature can be referred to as â€œactivation
    checkpointingâ€ or â€œcheckpoint activationsâ€.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨å…¶ä»–æ¡†æ¶ä¸­ï¼Œæ­¤åŠŸèƒ½å¯èƒ½è¢«ç§°ä¸ºâ€œæ¿€æ´»æ£€æŸ¥ç‚¹â€æˆ–â€œæ£€æŸ¥ç‚¹æ¿€æ´»â€ã€‚
- en: We pass the `__call__` method of the modules instead of `forward` because `__call__`
    attaches all the hooks of the module. [https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2](https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¼ é€’æ¨¡å—çš„`__call__`æ–¹æ³•è€Œä¸æ˜¯`forward`ï¼Œå› ä¸º`__call__`ä¼šé™„åŠ æ¨¡å—çš„æ‰€æœ‰é’©å­ã€‚[https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2](https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2)
- en: '#### `init_weights`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `init_weights`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2068)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2068)'
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`,
    you need to implement any initialization logic in `_init_weights`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦ä¿®å‰ªå¹¶å¯èƒ½åˆå§‹åŒ–æƒé‡ã€‚å¦‚æœä½¿ç”¨è‡ªå®šä¹‰`PreTrainedModel`ï¼Œåˆ™éœ€è¦åœ¨`_init_weights`ä¸­å®ç°ä»»ä½•åˆå§‹åŒ–é€»è¾‘ã€‚
- en: '#### `post_init`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_init`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1256)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1256)'
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A method executed at the end of each Transformer model initialization, to execute
    code that needs the modelâ€™s modules properly initialized (such as weight initialization).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªTransformeræ¨¡å‹åˆå§‹åŒ–ç»“æŸæ—¶æ‰§è¡Œçš„æ–¹æ³•ï¼Œç”¨äºæ‰§è¡Œéœ€è¦æ¨¡å‹æ¨¡å—æ­£ç¡®åˆå§‹åŒ–çš„ä»£ç ï¼ˆä¾‹å¦‚æƒé‡åˆå§‹åŒ–ï¼‰ã€‚
- en: '#### `prune_heads`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prune_heads`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2085)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2085)'
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`heads_to_prune` (`Dict[int, List[int]]`) â€” Dictionary with keys being selected
    layer indices (`int`) and associated values being the list of heads to prune in
    said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads
    0 and 2 on layer 1 and heads 2 and 3 on layer 2.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`heads_to_prune`ï¼ˆ`Dict[int, List[int]]`ï¼‰â€” é”®ä¸ºé€‰å®šçš„å±‚ç´¢å¼•ï¼ˆ`int`ï¼‰çš„å­—å…¸ï¼Œç›¸å…³å€¼ä¸ºè¯¥å±‚ä¸­è¦ä¿®å‰ªçš„å¤´éƒ¨åˆ—è¡¨ï¼ˆ`int`çš„åˆ—è¡¨ï¼‰ã€‚ä¾‹å¦‚{1:
    [0, 2]ï¼Œ2: [2, 3]}å°†åœ¨ç¬¬1å±‚ä¿®å‰ªå¤´éƒ¨0å’Œ2ï¼Œåœ¨ç¬¬2å±‚ä¿®å‰ªå¤´éƒ¨2å’Œ3ã€‚'
- en: Prunes heads of the base model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®å‰ªåŸºæœ¬æ¨¡å‹çš„å¤´éƒ¨ã€‚
- en: '#### `register_for_auto_class`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_for_auto_class`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4427)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4427)'
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`auto_class` (`str` or `type`, *optional*, defaults to `"AutoModel"`) â€” The
    auto class to register this new model with.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_class`ï¼ˆ`str`æˆ–`type`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"AutoModel"`ï¼‰â€” è¦æ³¨å†Œæ­¤æ–°æ¨¡å‹çš„è‡ªåŠ¨ç±»ã€‚'
- en: Register this class with a given auto class. This should only be used for custom
    models as the ones in the library are already mapped with an auto class.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤ç±»ä¸ç»™å®šçš„è‡ªåŠ¨ç±»æ³¨å†Œã€‚è¿™ä»…åº”ç”¨äºè‡ªå®šä¹‰æ¨¡å‹ï¼Œå› ä¸ºåº“ä¸­çš„æ¨¡å‹å·²ç»ä¸è‡ªåŠ¨ç±»æ˜ å°„ã€‚
- en: This API is experimental and may have some slight breaking changes in the next
    releases.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤APIæ˜¯å®éªŒæ€§çš„ï¼Œå¯èƒ½åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æœ‰ä¸€äº›è½»å¾®çš„ç ´åæ€§æ›´æ”¹ã€‚
- en: '#### `resize_token_embeddings`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `resize_token_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1786)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1786)'
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`new_num_tokens` (`int`, *optional*) â€” The new number of tokens in the embedding
    matrix. Increasing the size will add newly initialized vectors at the end. Reducing
    the size will remove vectors from the end. If not provided or `None`, just returns
    a pointer to the input tokens `torch.nn.Embedding` module of the model without
    doing anything.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_num_tokens` (`int`, *optional*) â€” åµŒå…¥çŸ©é˜µä¸­çš„æ–°æ ‡è®°æ•°ã€‚å¢åŠ å¤§å°å°†åœ¨æœ«å°¾æ·»åŠ æ–°åˆå§‹åŒ–çš„å‘é‡ã€‚å‡å°å¤§å°å°†ä»æœ«å°¾åˆ é™¤å‘é‡ã€‚å¦‚æœæœªæä¾›æˆ–ä¸º`None`ï¼Œåˆ™åªè¿”å›æŒ‡å‘æ¨¡å‹çš„è¾“å…¥æ ‡è®°`torch.nn.Embedding`æ¨¡å—çš„æŒ‡é’ˆï¼Œè€Œä¸æ‰§è¡Œä»»ä½•æ“ä½œã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the embedding matrix
    to a multiple of the provided value.If `new_num_tokens` is set to `None` will
    just pad the embedding to a multiple of `pad_to_multiple_of`.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åµŒå…¥çŸ©é˜µåˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚å¦‚æœ`new_num_tokens`è®¾ç½®ä¸º`None`ï¼Œåˆ™åªä¼šå°†åµŒå…¥å¡«å……åˆ°`pad_to_multiple_of`çš„å€æ•°ã€‚'
- en: 'This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128\. For more details about this, or help on
    choosing the correct value for resizing, refer to this guide: [https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºå¯ç”¨NVIDIAç¡¬ä»¶ä¸Šçš„Tensor Coresç‰¹åˆ«æœ‰ç”¨ï¼Œè®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰ï¼Œæˆ–è€…å¯¹äºå—ç›Šäºåºåˆ—é•¿åº¦ä¸º128çš„å€æ•°çš„TPUsã€‚æœ‰å…³æ­¤æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œæˆ–è€…æœ‰å…³é€‰æ‹©è°ƒæ•´å¤§å°çš„æ­£ç¡®å€¼çš„å¸®åŠ©ï¼Œè¯·å‚é˜…æ­¤æŒ‡å—ï¼š[https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)
- en: Returns
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.nn.Embedding`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Embedding`'
- en: Pointer to the input tokens Embeddings Module of the model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡å‘æ¨¡å‹çš„è¾“å…¥æ ‡è®°åµŒå…¥æ¨¡å—ã€‚
- en: Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ`new_num_tokens != config.vocab_size`ï¼Œåˆ™è°ƒæ•´æ¨¡å‹çš„è¾“å…¥æ ‡è®°åµŒå…¥çŸ©é˜µçš„å¤§å°ã€‚
- en: Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()`
    method.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹ç±»å…·æœ‰`tie_weights()`æ–¹æ³•æ—¶è´Ÿè´£ç»‘å®šæƒé‡åµŒå…¥ã€‚
- en: '#### `reverse_bettertransformer`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reverse_bettertransformer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4481)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4481)'
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Returns
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)'
- en: The model converted back to the original modeling.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹è½¬æ¢å›åŸå§‹å»ºæ¨¡ã€‚
- en: Reverts the transformation from [to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)
    so that the original modeling is used, for example in order to save the model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: æ’¤æ¶ˆä»[to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)çš„è½¬æ¢ï¼Œä»¥ä¾¿ä½¿ç”¨åŸå§‹å»ºæ¨¡ï¼Œä¾‹å¦‚ä¸ºäº†ä¿å­˜æ¨¡å‹ã€‚
- en: '#### `save_pretrained`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2199)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2199)'
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory to which to save. Will
    be created if it doesnâ€™t exist.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰ â€” è¦ä¿å­˜åˆ°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºè¯¥ç›®å½•ã€‚'
- en: '`is_main_process` (`bool`, *optional*, defaults to `True`) â€” Whether the process
    calling this is the main process or not. Useful when in distributed training like
    TPUs and need to call this function on all processes. In this case, set `is_main_process=True`
    only on the main process to avoid race conditions.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_main_process` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚åœ¨åƒTPUè¿™æ ·çš„åˆ†å¸ƒå¼è®­ç»ƒä¸­å¾ˆæœ‰ç”¨ï¼Œéœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨æ­¤å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè®¾ç½®`is_main_process=True`ï¼Œä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚'
- en: '`state_dict` (nested dictionary of `torch.Tensor`) â€” The state dictionary of
    the model to save. Will default to `self.state_dict()`, but can be used to only
    save parts of the model or if special precautions need to be taken when recovering
    the state dictionary of a model (like when using model parallelism).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict`ï¼ˆ`torch.Tensor`çš„åµŒå¥—å­—å…¸ï¼‰ â€” è¦ä¿å­˜çš„æ¨¡å‹çš„çŠ¶æ€å­—å…¸ã€‚å°†é»˜è®¤ä¸º`self.state_dict()`ï¼Œä½†å¯ä»¥ç”¨äºä»…ä¿å­˜æ¨¡å‹çš„éƒ¨åˆ†æˆ–è€…åœ¨æ¢å¤æ¨¡å‹çš„çŠ¶æ€å­—å…¸æ—¶éœ€è¦é‡‡å–ç‰¹æ®Šé¢„é˜²æªæ–½çš„æƒ…å†µï¼ˆä¾‹å¦‚åœ¨ä½¿ç”¨æ¨¡å‹å¹¶è¡Œæ—¶ï¼‰ã€‚'
- en: '`save_function` (`Callable`) â€” The function to use to save the state dictionary.
    Useful on distributed training like TPUs when one need to replace `torch.save`
    by another method.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_function` (`Callable`) â€” ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ã€‚åœ¨åƒTPUè¿™æ ·çš„åˆ†å¸ƒå¼è®­ç»ƒä¸­å¾ˆæœ‰ç”¨ï¼Œå½“éœ€è¦ç”¨å¦ä¸€ç§æ–¹æ³•æ›¿æ¢`torch.save`æ—¶ã€‚'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨ä¿å­˜åå°†æ¨¡å‹æ¨é€åˆ°Hugging Faceæ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`repo_id`æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„`save_directory`çš„åç§°ï¼‰ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) â€” The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`). We default it to 5GB in order for models to be able
    to run easily on free-tier google colab instances without CPU OOM issues.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int`æˆ–`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`"5GB"`) â€” åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹åˆ†ç‰‡çš„å¤§å°å°†å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚`"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤ä¸º5GBï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿåœ¨å…è´¹çš„Google
    Colabå®ä¾‹ä¸Šè½»æ¾è¿è¡Œï¼Œè€Œä¸ä¼šå‡ºç°CPU OOMé—®é¢˜ã€‚'
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹çš„å•ä¸ªæƒé‡å¤§äº`max_shard_size`ï¼Œåˆ™å®ƒå°†åœ¨è‡ªå·±çš„æ£€æŸ¥ç‚¹åˆ†ç‰‡ä¸­ï¼Œè¯¥åˆ†ç‰‡å°†å¤§äº`max_shard_size`ã€‚
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether to
    save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä½¿ç”¨`safetensors`æˆ–ä¼ ç»Ÿçš„PyTorchæ–¹å¼ï¼ˆä½¿ç”¨`pickle`ï¼‰ä¿å­˜æ¨¡å‹ã€‚'
- en: '`variant` (`str`, *optional*) â€” If specified, weights are saved in the format
    pytorch_model.<variant>.bin.</variant>'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`variant` (`str`, *å¯é€‰*) â€” å¦‚æœæŒ‡å®šï¼Œæƒé‡å°†ä»¥ pytorch_model.<variant>.bin çš„æ ¼å¼ä¿å­˜ã€‚'
- en: '`token` (`str` or `bool`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str` æˆ– `bool`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚'
- en: '`save_peft_format` (`bool`, *optional*, defaults to `True`) â€” For backward
    compatibility with PEFT library, in case adapter weights are attached to the model,
    all keys of the state dict of adapters needs to be pre-pended with `base_model.model`.
    Advanced users can disable this behaviours by setting `save_peft_format` to `False`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_peft_format` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” ä¸ºäº†å‘åå…¼å®¹PEFTåº“ï¼Œå¦‚æœé€‚é…å™¨æƒé‡é™„åŠ åˆ°æ¨¡å‹ä¸Šï¼Œé€‚é…å™¨çŠ¶æ€å­—å…¸çš„æ‰€æœ‰é”®éƒ½éœ€è¦ä»¥
    `base_model.model` ä¸ºå‰ç¼€ã€‚é«˜çº§ç”¨æˆ·å¯ä»¥é€šè¿‡å°† `save_peft_format` è®¾ç½®ä¸º `False` æ¥ç¦ç”¨æ­¤è¡Œä¸ºã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ä¼ é€’ç»™ [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Save a model and its configuration file to a directory, so that it can be re-loaded
    using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    class method.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åŠå…¶é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    ç±»æ–¹æ³•é‡æ–°åŠ è½½ã€‚
- en: '#### `set_input_embeddings`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_input_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1601)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1601)'
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`value` (`nn.Module`) â€” A module mapping vocabulary to hidden states.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value` (`nn.Module`) â€” å°†è¯æ±‡æ˜ å°„åˆ°éšè—çŠ¶æ€çš„æ¨¡å—ã€‚'
- en: Set modelâ€™s input embeddings.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®æ¨¡å‹çš„è¾“å…¥åµŒå…¥ã€‚
- en: '#### `tie_weights`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tie_weights`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1641)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1641)'
- en: '[PRE23]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tie the weights between the input embeddings and the output embeddings.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥åµŒå…¥å’Œè¾“å‡ºåµŒå…¥ä¹‹é—´çš„æƒé‡ç»‘å®šåœ¨ä¸€èµ·ã€‚
- en: If the `torchscript` flag is set in the configuration, canâ€™t handle parameter
    sharing so we are cloning the weights instead.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœé…ç½®ä¸­è®¾ç½®äº† `torchscript` æ ‡å¿—ï¼Œåˆ™æ— æ³•å¤„ç†å‚æ•°å…±äº«ï¼Œå› æ­¤æˆ‘ä»¬ä¼šå…‹éš†æƒé‡ã€‚
- en: '#### `to_bettertransformer`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_bettertransformer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4453)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4453)'
- en: '[PRE24]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Returns
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)'
- en: The model converted to BetterTransformer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬æ¢ä¸ºBetterTransformerçš„æ¨¡å‹ã€‚
- en: Converts the model to use [PyTorchâ€™s native attention implementation](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html),
    integrated to Transformers through [Optimum library](https://huggingface.co/docs/optimum/bettertransformer/overview).
    Only a subset of all Transformers models are supported.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹è½¬æ¢ä¸ºä½¿ç”¨[PyTorchçš„æœ¬æœºæ³¨æ„åŠ›å®ç°](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)ï¼Œé€šè¿‡[Optimumåº“](https://huggingface.co/docs/optimum/bettertransformer/overview)é›†æˆåˆ°Transformersä¸­ã€‚ä»…æ”¯æŒæ‰€æœ‰Transformersæ¨¡å‹çš„å­é›†ã€‚
- en: PyTorchâ€™s attention fastpath allows to speed up inference through kernel fusions
    and the use of [nested tensors](https://pytorch.org/docs/stable/nested.html).
    Detailed benchmarks can be found in [this blog post](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchçš„æ³¨æ„åŠ›å¿«é€Ÿè·¯å¾„å…è®¸é€šè¿‡å†…æ ¸èåˆå’Œä½¿ç”¨[åµŒå¥—å¼ é‡](https://pytorch.org/docs/stable/nested.html)åŠ é€Ÿæ¨ç†ã€‚è¯¦ç»†çš„åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨[è¿™ç¯‡åšæ–‡](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ä¸­æ‰¾åˆ°ã€‚
- en: '#### `warn_if_padding_and_no_attention_mask`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `warn_if_padding_and_no_attention_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4503)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4503)'
- en: '[PRE25]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Shows a one-time warning if the input_ids appear to contain padding and no attention
    mask was given.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¾“å…¥çš„ input_ids çœ‹èµ·æ¥åŒ…å«å¡«å……å¹¶ä¸”æ²¡æœ‰ç»™å‡ºæ³¨æ„åŠ›æ©ç ï¼Œåˆ™æ˜¾ç¤ºä¸€æ¬¡è­¦å‘Šã€‚
- en: Large model loading
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤§å‹æ¨¡å‹åŠ è½½
- en: In Transformers 4.20.0, the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method has been reworked to accommodate large models using [Accelerate](https://huggingface.co/docs/accelerate/big_modeling).
    This requires Accelerate >= 0.9.0 and PyTorch >= 1.9.0\. Instead of creating the
    full model, then loading the pretrained weights inside it (which takes twice the
    size of the model in RAM, one for the randomly initialized model, one for the
    weights), there is an option to create the model as an empty shell, then only
    materialize its parameters when the pretrained weights are loaded.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Transformers 4.20.0ä¸­ï¼Œ[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•å·²ç»é‡æ–°è®¾è®¡ï¼Œä»¥é€‚åº”ä½¿ç”¨[Accelerate](https://huggingface.co/docs/accelerate/big_modeling)çš„å¤§å‹æ¨¡å‹ã€‚è¿™éœ€è¦
    Accelerate >= 0.9.0 å’Œ PyTorch >= 1.9.0ã€‚ä¸å…¶åœ¨å†…å­˜ä¸­åˆ›å»ºå®Œæ•´æ¨¡å‹ï¼Œç„¶ååŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆè¿™éœ€è¦æ¨¡å‹å¤§å°çš„ä¸¤å€çš„å†…å­˜ï¼Œä¸€ä¸ªç”¨äºéšæœºåˆå§‹åŒ–æ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºæƒé‡ï¼‰ï¼Œç°åœ¨æœ‰ä¸€ä¸ªé€‰é¡¹å¯ä»¥åˆ›å»ºæ¨¡å‹ä½œä¸ºç©ºå£³ï¼Œç„¶ååªæœ‰åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶æ‰å®ç°å…¶å‚æ•°ã€‚
- en: This option can be activated with `low_cpu_mem_usage=True`. The model is first
    created on the Meta device (with empty weights) and the state dict is then loaded
    inside it (shard by shard in the case of a sharded checkpoint). This way the maximum
    RAM used is the full size of the model only.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡ `low_cpu_mem_usage=True` æ¿€æ´»æ­¤é€‰é¡¹ã€‚æ¨¡å‹é¦–å…ˆåœ¨ Meta è®¾å¤‡ä¸Šåˆ›å»ºï¼ˆå¸¦æœ‰ç©ºæƒé‡ï¼‰ï¼Œç„¶åçŠ¶æ€å­—å…¸è¢«åŠ è½½åˆ°å…¶ä¸­ï¼ˆåœ¨åˆ†ç‰‡æ£€æŸ¥ç‚¹çš„æƒ…å†µä¸‹é€ä¸ªåˆ†ç‰‡ï¼‰ã€‚è¿™æ ·ï¼Œæœ€å¤§ä½¿ç”¨çš„RAMä»…ä¸ºæ¨¡å‹çš„å®Œæ•´å¤§å°ã€‚
- en: '[PRE26]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Moreover, you can directly place the model on different devices if it doesnâ€™t
    fully fit in RAM (only works for inference for now). With `device_map="auto"`,
    Accelerate will determine where to put each layer to maximize the use of your
    fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive
    if you donâ€™t have enough GPU RAM (or CPU RAM). Even if the model is split across
    several devices, it will run as you would normally expect.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå¦‚æœæ¨¡å‹æ— æ³•å®Œå…¨é€‚åº”RAMï¼ˆç›®å‰ä»…é€‚ç”¨äºæ¨æ–­ï¼‰ï¼Œåˆ™å¯ä»¥ç›´æ¥å°†æ¨¡å‹æ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚ä½¿ç”¨`device_map="auto"`ï¼ŒAccelerateå°†ç¡®å®šå°†æ¯ä¸ªå±‚æ”¾ç½®åœ¨å“ªé‡Œä»¥æœ€å¤§åŒ–æ‚¨æœ€å¿«çš„è®¾å¤‡ï¼ˆGPUï¼‰çš„ä½¿ç”¨ï¼Œå¹¶å°†å…¶ä½™éƒ¨åˆ†å¸è½½åˆ°CPUï¼Œç”šè‡³ç¡¬ç›˜ï¼Œå¦‚æœæ‚¨æ²¡æœ‰è¶³å¤Ÿçš„GPU
    RAMï¼ˆæˆ–CPU RAMï¼‰ã€‚å³ä½¿æ¨¡å‹åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šï¼Œå®ƒä¹Ÿå°†æŒ‰ç…§æ‚¨é€šå¸¸çš„é¢„æœŸè¿è¡Œã€‚
- en: 'When passing a `device_map`, `low_cpu_mem_usage` is automatically set to `True`,
    so you donâ€™t need to specify it:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ é€’`device_map`æ—¶ï¼Œ`low_cpu_mem_usage`ä¼šè‡ªåŠ¨è®¾ç½®ä¸º`True`ï¼Œå› æ­¤æ‚¨æ— éœ€æŒ‡å®šå®ƒï¼š
- en: '[PRE27]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can inspect how the model was split across devices by looking at its `hf_device_map`
    attribute:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æŸ¥çœ‹`hf_device_map`å±æ€§æ¥æŸ¥çœ‹æ¨¡å‹å¦‚ä½•åˆ†å¸ƒåœ¨è®¾å¤‡ä¸Šï¼š
- en: '[PRE28]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You can also write your own device map following the same format (a dictionary
    layer name to device). It should map all parameters of the model to a given device,
    but you donâ€™t have to detail where all the submodules of one layer go if that
    layer is entirely on the same device. For instance, the following device map would
    work properly for T0pp (as long as you have the GPU memory):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥æŒ‰ç…§ç›¸åŒæ ¼å¼ç¼–å†™è‡ªå·±çš„è®¾å¤‡æ˜ å°„ï¼ˆå°†å±‚åç§°æ˜ å°„åˆ°è®¾å¤‡çš„å­—å…¸ï¼‰ã€‚å®ƒåº”è¯¥å°†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°æ˜ å°„åˆ°ç»™å®šè®¾å¤‡ï¼Œä½†å¦‚æœè¯¥å±‚å®Œå…¨ä½äºåŒä¸€è®¾å¤‡ä¸Šï¼Œåˆ™ä¸å¿…è¯¦ç»†è¯´æ˜ä¸€ä¸ªå±‚çš„æ‰€æœ‰å­æ¨¡å—å»å“ªé‡Œã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹è®¾å¤‡æ˜ å°„å¯¹äºT0ppå°†æ­£å¸¸å·¥ä½œï¼ˆåªè¦æ‚¨æœ‰GPUå†…å­˜ï¼‰ï¼š
- en: '[PRE30]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Another way to minimize the memory impact of your model is to instantiate it
    at a lower precision dtype (like `torch.float16`) or use direct quantization techniques
    as described below.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘æ¨¡å‹å†…å­˜å½±å“çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä»¥è¾ƒä½ç²¾åº¦dtypeï¼ˆå¦‚`torch.float16`ï¼‰å®ä¾‹åŒ–æ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨ä¸‹é¢æè¿°çš„ç›´æ¥é‡åŒ–æŠ€æœ¯ã€‚
- en: Model Instantiation dtype
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å‹å®ä¾‹åŒ–dtype
- en: 'Under Pytorch a model normally gets instantiated with `torch.float32` format.
    This can be an issue if one tries to load a model whose weights are in fp16, since
    itâ€™d require twice as much memory. To overcome this limitation, you can either
    explicitly pass the desired `dtype` using `torch_dtype` argument:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Pytorchä¸‹ï¼Œæ¨¡å‹é€šå¸¸ä»¥`torch.float32`æ ¼å¼å®ä¾‹åŒ–ã€‚å¦‚æœå°è¯•åŠ è½½æƒé‡ä¸ºfp16çš„æ¨¡å‹ï¼Œåˆ™å¯èƒ½ä¼šå‡ºç°é—®é¢˜ï¼Œå› ä¸ºå®ƒå°†éœ€è¦ä¸¤å€çš„å†…å­˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`torch_dtype`å‚æ•°æ˜¾å¼ä¼ é€’æ‰€éœ€çš„`dtype`ï¼š
- en: '[PRE31]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'or, if you want the model to always load in the most optimal memory pattern,
    you can use the special value `"auto"`, and then `dtype` will be automatically
    derived from the modelâ€™s weights:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¦‚æœå¸Œæœ›æ¨¡å‹å§‹ç»ˆä»¥æœ€ä½³å†…å­˜æ¨¡å¼åŠ è½½ï¼Œå¯ä»¥ä½¿ç”¨ç‰¹æ®Šå€¼`"auto"`ï¼Œç„¶å`dtype`å°†è‡ªåŠ¨ä»æ¨¡å‹çš„æƒé‡ä¸­æ´¾ç”Ÿï¼š
- en: '[PRE32]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Models instantiated from scratch can also be told which `dtype` to use with:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¤´å¼€å§‹å®ä¾‹åŒ–çš„æ¨¡å‹ä¹Ÿå¯ä»¥æŒ‡å®šä½¿ç”¨çš„`dtype`ï¼š
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Due to Pytorch design, this functionality is only available for floating dtypes.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºPytorchè®¾è®¡ï¼Œæ­¤åŠŸèƒ½ä»…é€‚ç”¨äºæµ®ç‚¹dtypeã€‚
- en: ModuleUtilsMixin
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ModuleUtilsMixin
- en: '### `class transformers.modeling_utils.ModuleUtilsMixin`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_utils.ModuleUtilsMixin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L853)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L853)'
- en: '[PRE34]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: A few utilities for `torch.nn.Modules`, to be used as a mixin.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨ä½œmixinçš„`torch.nn.Modules`çš„ä¸€äº›å®ç”¨ç¨‹åºã€‚
- en: '#### `add_memory_hooks`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_memory_hooks`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L884)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L884)'
- en: '[PRE35]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Add a memory hook before and after each sub-module forward pass to record increase
    in memory consumption.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªå­æ¨¡å—æ­£å‘ä¼ é€’ä¹‹å‰å’Œä¹‹åæ·»åŠ å†…å­˜é’©å­ä»¥è®°å½•å†…å­˜æ¶ˆè€—çš„å¢åŠ ã€‚
- en: Increase in memory consumption is stored in a `mem_rss_diff` attribute for each
    module and can be reset to zero with `model.reset_memory_hooks_state()`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜æ¶ˆè€—çš„å¢åŠ å­˜å‚¨åœ¨æ¯ä¸ªæ¨¡å—çš„`mem_rss_diff`å±æ€§ä¸­ï¼Œå¹¶å¯ä»¥ä½¿ç”¨`model.reset_memory_hooks_state()`å°†å…¶é‡ç½®ä¸ºé›¶ã€‚
- en: '#### `estimate_tokens`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `estimate_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1109)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1109)'
- en: '[PRE36]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`inputs` (`dict`) â€” The model inputs.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`ï¼ˆ`dict`ï¼‰â€” æ¨¡å‹è¾“å…¥ã€‚'
- en: Returns
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: The total number of tokens.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤ç‰Œçš„æ€»æ•°ã€‚
- en: Helper function to estimate the total number of tokens from the model inputs.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºä¼°è®¡æ¨¡å‹è¾“å…¥ä¸­æ€»ä»¤ç‰Œæ•°çš„è¾…åŠ©å‡½æ•°ã€‚
- en: '#### `floating_point_ops`'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `floating_point_ops`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1130)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1130)'
- en: '[PRE37]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_size` (`int`) â€” The batch size for the forward pass.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`ï¼ˆ`int`ï¼‰â€” æ­£å‘ä¼ é€’çš„æ‰¹é‡å¤§å°ã€‚'
- en: '`sequence_length` (`int`) â€” The number of tokens in each line of the batch.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_length`ï¼ˆ`int`ï¼‰â€” æ¯ä¸ªæ‰¹æ¬¡è¡Œä¸­çš„ä»¤ç‰Œæ•°ã€‚'
- en: '`exclude_embeddings` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to count embedding and softmax operations.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_embeddings`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è®¡ç®—åµŒå…¥å’Œsoftmaxæ“ä½œã€‚'
- en: Returns
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: The number of floating-point operations.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: æµ®ç‚¹è¿ç®—çš„æ•°é‡ã€‚
- en: Get number of (optionally, non-embeddings) floating-point operations for the
    forward and backward passes of a batch with this transformer model. Default approximation
    neglects the quadratic dependency on the number of tokens (valid if `12 * d_model
    << sequence_length`) as laid out in [this paper](https://arxiv.org/pdf/2001.08361.pdf)
    section 2.1\. Should be overridden for transformers with parameter re-use e.g.
    Albert or Universal Transformers, or if doing long-range modeling with very high
    sequence lengths.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤è½¬æ¢å™¨æ¨¡å‹çš„æ‰¹å¤„ç†çš„æ­£å‘å’Œåå‘ä¼ é€’çš„æµ®ç‚¹æ“ä½œçš„æ•°é‡ï¼ˆå¯é€‰ï¼ŒéåµŒå…¥ï¼‰ã€‚é»˜è®¤è¿‘ä¼¼å¿½ç•¥å¯¹ä»¤ç‰Œæ•°é‡çš„äºŒæ¬¡ä¾èµ–ï¼ˆå¦‚æœ`12 * d_model << sequence_length`ï¼‰å¦‚[æœ¬æ–‡](https://arxiv.org/pdf/2001.08361.pdf)ç¬¬2.1èŠ‚æ‰€è¿°ã€‚å¯¹äºå…·æœ‰å‚æ•°é‡ç”¨çš„å˜å‹å™¨ï¼ˆä¾‹å¦‚Albertæˆ–é€šç”¨å˜å‹å™¨ï¼‰æˆ–è€…å¦‚æœä½¿ç”¨éå¸¸é«˜çš„åºåˆ—é•¿åº¦è¿›è¡Œé•¿è·ç¦»å»ºæ¨¡ï¼Œåˆ™åº”è¯¥è¿›è¡Œè¦†ç›–ã€‚
- en: '#### `get_extended_attention_mask`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_extended_attention_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L972)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L972)'
- en: '[PRE38]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`attention_mask` (`torch.Tensor`) â€” Mask with ones indicating tokens to attend
    to, zeros for tokens to ignore.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`) â€” ä¸€ä¸ªæ©ç ï¼Œå…¶ä¸­çš„1è¡¨ç¤ºè¦å…³æ³¨çš„æ ‡è®°ï¼Œ0è¡¨ç¤ºè¦å¿½ç•¥çš„æ ‡è®°ã€‚'
- en: '`input_shape` (`Tuple[int]`) â€” The shape of the input to the model.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape` (`Tuple[int]`) â€” æ¨¡å‹çš„è¾“å…¥å½¢çŠ¶ã€‚'
- en: Makes broadcastable attention and causal masks so that future and masked tokens
    are ignored.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿å¯å¹¿æ’­çš„æ³¨æ„åŠ›å’Œå› æœæ©ç ï¼Œä»¥ä¾¿å°†æ¥å’Œæ©ç çš„æ ‡è®°è¢«å¿½ç•¥ã€‚
- en: '#### `get_head_mask`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_head_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1024)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1024)'
- en: '[PRE39]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`head_mask` (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers
    x num_heads]`, *optional*) â€” The mask indicating if we should keep the heads or
    not (1.0 for keep, 0.0 for discard).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`[num_heads]`æˆ–`[num_hidden_layers x num_heads]`ï¼Œ*å¯é€‰*)
    â€” æŒ‡ç¤ºæˆ‘ä»¬æ˜¯å¦åº”ä¿ç•™å¤´éƒ¨çš„æ©ç ï¼ˆä¿ç•™ä¸º1.0ï¼Œä¸¢å¼ƒä¸º0.0ï¼‰ã€‚'
- en: '`num_hidden_layers` (`int`) â€” The number of hidden layers in the model.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`) â€” æ¨¡å‹ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`is_attention_chunked` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the attentions scores are computed by chunks or not.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_attention_chunked` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ³¨æ„åŠ›åˆ†æ•°æ˜¯å¦æŒ‰å—è®¡ç®—ã€‚'
- en: Prepare the head mask if needed.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦ï¼Œå‡†å¤‡å¤´æ©ç ã€‚
- en: '#### `invert_attention_mask`'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `invert_attention_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L920)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L920)'
- en: '[PRE40]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`encoder_attention_mask` (`torch.Tensor`) â€” An attention mask.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_mask` (`torch.Tensor`) â€” ä¸€ä¸ªæ³¨æ„åŠ›æ©ç ã€‚'
- en: Returns
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.Tensor`'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: The inverted attention mask.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: åè½¬çš„æ³¨æ„åŠ›æ©ç ã€‚
- en: Invert an attention mask (e.g., switches 0\. and 1.).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: åè½¬æ³¨æ„åŠ›æ©ç ï¼ˆä¾‹å¦‚ï¼Œåˆ‡æ¢0å’Œ1ï¼‰ã€‚
- en: '#### `num_parameters`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_parameters`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1062)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1062)'
- en: '[PRE41]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`only_trainable` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to return only the number of trainable parameters'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_trainable` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åªè¿”å›å¯è®­ç»ƒå‚æ•°çš„æ•°é‡'
- en: '`exclude_embeddings` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to return only the number of non-embeddings parameters'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_embeddings` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åªè¿”å›éåµŒå…¥å‚æ•°çš„æ•°é‡'
- en: Returns
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: The number of parameters.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°çš„æ•°é‡ã€‚
- en: Get number of (optionally, trainable or non-embeddings) parameters in the module.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–æ¨¡å—ä¸­ï¼ˆå¯é€‰åœ°ï¼Œå¯è®­ç»ƒæˆ–éåµŒå…¥ï¼‰å‚æ•°çš„æ•°é‡ã€‚
- en: '#### `reset_memory_hooks_state`'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reset_memory_hooks_state`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L896)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L896)'
- en: '[PRE42]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Reset the `mem_rss_diff` attribute of each module (see [add_memory_hooks()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks)).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: é‡ç½®æ¯ä¸ªæ¨¡å—çš„`mem_rss_diff`å±æ€§ï¼ˆå‚è§[add_memory_hooks()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks)ï¼‰ã€‚
- en: TFPreTrainedModel
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFPreTrainedModel
- en: '### `class transformers.TFPreTrainedModel`'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFPreTrainedModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1058)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1058)'
- en: '[PRE43]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Base class for all TF models.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰TFæ¨¡å‹çš„åŸºç±»ã€‚
- en: '[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    takes care of storing the configuration of the models and handles methods for
    loading, downloading and saving models as well as a few methods common to all
    models to:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: TFPreTrainedModelç±»è´Ÿè´£å­˜å‚¨æ¨¡å‹çš„é…ç½®ï¼Œå¹¶å¤„ç†åŠ è½½ã€ä¸‹è½½å’Œä¿å­˜æ¨¡å‹çš„æ–¹æ³•ï¼Œä»¥åŠä¸€äº›æ‰€æœ‰æ¨¡å‹é€šç”¨çš„æ–¹æ³•ï¼š
- en: resize the input embeddings,
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒæ•´è¾“å…¥åµŒå…¥ï¼Œ
- en: prune heads in the self-attention heads.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿®å‰ªè‡ªæ³¨æ„åŠ›å¤´ã€‚
- en: 'Class attributes (overridden by derived classes):'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å±æ€§ï¼ˆç”±æ´¾ç”Ÿç±»è¦†ç›–ï¼‰ï¼š
- en: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” A subclass of [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    to use as configuration class for this model architecture.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” ç”¨ä½œæ­¤æ¨¡å‹æ¶æ„çš„é…ç½®ç±»çš„[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„å­ç±»ã€‚'
- en: '`base_model_prefix` (`str`) â€” A string indicating the attribute associated
    to the base model in derived classes of the same architecture adding modules on
    top of the base model.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base_model_prefix` (`str`) â€” ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒæŒ‡ç¤ºæ´¾ç”Ÿç±»ä¸­åŸºç¡€æ¨¡å‹å…³è”çš„å±æ€§ï¼Œåœ¨åŒä¸€æ¶æ„çš„æ´¾ç”Ÿç±»ä¸­æ·»åŠ æ¨¡å—åˆ°åŸºç¡€æ¨¡å‹ä¹‹ä¸Šã€‚'
- en: '`main_input_name` (`str`) â€” The name of the principal input to the model (often
    `input_ids` for NLP models, `pixel_values` for vision models and `input_values`
    for speech models).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main_input_name` (`str`) â€” æ¨¡å‹çš„ä¸»è¦è¾“å…¥çš„åç§°ï¼ˆé€šå¸¸ä¸ºNLPæ¨¡å‹çš„`input_ids`ï¼Œè§†è§‰æ¨¡å‹çš„`pixel_values`å’Œè¯­éŸ³æ¨¡å‹çš„`input_values`ï¼‰ã€‚'
- en: '#### `push_to_hub`'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3067)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3067)'
- en: '[PRE44]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`repo_id` (`str`) â€” The name of the repository you want to push your model
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id` (`str`) â€” æ‚¨è¦å°†æ¨¡å‹æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ã€‚åœ¨æ¨é€åˆ°ç»™å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚'
- en: '`use_temp_dir` (`bool`, *optional*) â€” Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir` (`bool`, *å¯é€‰*) â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨ä¿å­˜çš„æ–‡ä»¶ï¼Œç›´åˆ°å®ƒä»¬è¢«æ¨é€åˆ°Hubã€‚å¦‚æœæ²¡æœ‰åä¸º`repo_id`çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚'
- en: '`commit_message` (`str`, *optional*) â€” Message to commit while pushing. Will
    default to `"Upload model"`.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message` (`str`, *å¯é€‰*) â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º`"Upload model"`ã€‚'
- en: '`private` (`bool`, *optional*) â€” Whether or not the repository created should
    be private.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦åº”åˆ›å»ºç§æœ‰å­˜å‚¨åº“ã€‚'
- en: '`token` (`bool` or `str`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`ï¼ˆ`bool`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ`huggingface-cli
    login`æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ä¸­ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š`repo_url`ï¼Œåˆ™é»˜è®¤ä¸º`True`ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) â€” Only
    applicable for models. The maximum size for a checkpoint before being sharded.
    Checkpoints shard will then be each of size lower than this size. If expressed
    as a string, needs to be digits followed by a unit (like `"5MB"`).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size`ï¼ˆ`int`æˆ–`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"10GB"`ï¼‰â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹åˆ†ç‰‡å°†æ¯ä¸ªå¤§å°å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚`"5MB"`ï¼‰ã€‚'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) â€” Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åˆ›å»ºå¸¦æœ‰ä¸Šä¼ æ–‡ä»¶çš„PRæˆ–ç›´æ¥æäº¤ã€‚'
- en: Upload the model files to the ğŸ¤— Model Hub while synchronizing a local clone
    of the repo in `repo_path_or_name`.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹æ–‡ä»¶ä¸Šä¼ åˆ°ğŸ¤—æ¨¡å‹Hubï¼ŒåŒæ—¶åŒæ­¥å­˜å‚¨åº“çš„æœ¬åœ°å…‹éš†åˆ°`repo_path_or_name`ä¸­ã€‚
- en: 'Examples:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE45]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#### `can_generate`'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `can_generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1301)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1301)'
- en: '[PRE46]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Returns
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`bool`'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '`bool`'
- en: Whether this model can generate sequences with `.generate()`.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯å¦å¯ä»¥ä½¿ç”¨`.generate()`ç”Ÿæˆåºåˆ—ã€‚
- en: Returns whether this model can generate sequences with `.generate()`.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›æ­¤æ¨¡å‹æ˜¯å¦å¯ä»¥ä½¿ç”¨`.generate()`ç”Ÿæˆåºåˆ—ã€‚
- en: '#### `compile`'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compile`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1496)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1496)'
- en: '[PRE47]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This is a thin wrapper that sets the modelâ€™s loss output head as the loss if
    the user does not specify a loss function themselves.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªè–„åŒ…è£…å™¨ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šè‡ªå·±çš„æŸå¤±å‡½æ•°ï¼Œåˆ™å°†æ¨¡å‹çš„æŸå¤±è¾“å‡ºå¤´è®¾ç½®ä¸ºæŸå¤±ã€‚
- en: '#### `create_model_card`'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_model_card`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1791)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1791)'
- en: '[PRE48]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`output_dir` (`str` or `os.PathLike`) â€” The folder in which to create the model
    card.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” åˆ›å»ºæ¨¡å‹å¡ç‰‡çš„æ–‡ä»¶å¤¹ã€‚'
- en: '`model_name` (`str`, *optional*) â€” The name of the model.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹çš„åç§°ã€‚'
- en: '`language` (`str`, *optional*) â€” The language of the model (if applicable)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹çš„è¯­è¨€ï¼ˆå¦‚æœé€‚ç”¨ï¼‰'
- en: '`license` (`str`, *optional*) â€” The license of the model. Will default to the
    license of the pretrained model used, if the original model given to the `Trainer`
    comes from a repo on the Hub.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`license`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹çš„è®¸å¯è¯ã€‚å¦‚æœç»™å®šç»™`Trainer`çš„åŸå§‹æ¨¡å‹æ¥è‡ªHubä¸Šçš„repoï¼Œåˆ™é»˜è®¤ä¸ºä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹çš„è®¸å¯è¯ã€‚'
- en: '`tags` (`str` or `List[str]`, *optional*) â€” Some tags to be included in the
    metadata of the model card.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” è¦åŒ…å«åœ¨æ¨¡å‹å¡ç‰‡çš„å…ƒæ•°æ®ä¸­çš„ä¸€äº›æ ‡ç­¾ã€‚'
- en: '`finetuned_from` (`str`, *optional*) â€” The name of the model used to fine-tune
    this one (if applicable). Will default to the name of the repo of the original
    model given to the `Trainer` (if it comes from the Hub).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`finetuned_from`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¾®è°ƒæ­¤æ¨¡å‹çš„æ¨¡å‹çš„åç§°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ã€‚å¦‚æœæ¥è‡ªHubçš„åŸå§‹æ¨¡å‹çš„`Trainer`ç»™å‡ºçš„repoçš„åç§°ï¼Œåˆ™é»˜è®¤ä¸ºåŸå§‹æ¨¡å‹çš„åç§°ã€‚'
- en: '`tasks` (`str` or `List[str]`, *optional*) â€” One or several task identifiers,
    to be included in the metadata of the model card.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tasks`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ªä»»åŠ¡æ ‡è¯†ç¬¦ï¼Œè¦åŒ…å«åœ¨æ¨¡å‹å¡ç‰‡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: '`dataset_tags` (`str` or `List[str]`, *optional*) â€” One or several dataset
    tags, to be included in the metadata of the model card.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_tags`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†æ ‡ç­¾ï¼Œè¦åŒ…å«åœ¨æ¨¡å‹å¡ç‰‡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: '`dataset` (`str` or `List[str]`, *optional*) â€” One or several dataset identifiers,
    to be included in the metadata of the model card.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†æ ‡è¯†ç¬¦ï¼Œè¦åŒ…å«åœ¨æ¨¡å‹å¡ç‰‡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: '`dataset_args` (`str` or `List[str]`, *optional*) â€” One or several dataset
    arguments, to be included in the metadata of the model card.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_args`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†å‚æ•°ï¼Œè¦åŒ…å«åœ¨æ¨¡å‹å¡ç‰‡çš„å…ƒæ•°æ®ä¸­ã€‚'
- en: Creates a draft of a model card using the information available to the `Trainer`.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`Trainer`å¯ç”¨çš„ä¿¡æ¯åˆ›å»ºæ¨¡å‹å¡ç‰‡çš„è‰ç¨¿ã€‚
- en: '#### `eager_serving`'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `eager_serving`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1213)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1213)'
- en: '[PRE49]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`inputs` (`Dict[str, tf.Tensor]`) â€” The input of the saved model as a dictionary
    of tensors.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`ï¼ˆ`Dict[str, tf.Tensor]`ï¼‰â€” ä¿å­˜æ¨¡å‹çš„è¾“å…¥ï¼Œä½œä¸ºå¼ é‡å­—å…¸ã€‚'
- en: Method used for serving the model. This method is deprecated, and will be removed.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºæä¾›æ¨¡å‹çš„æ–¹æ³•ã€‚æ­¤æ–¹æ³•å·²å¼ƒç”¨ï¼Œå°†è¢«ç§»é™¤ã€‚
- en: '#### `from_pretrained`'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2499)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2499)'
- en: '[PRE50]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path` (`str`, *optional*) â€” Can be either:'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” å¯ä»¥æ˜¯ï¼š'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„*æ¨¡å‹id*ï¼Œæ‰˜ç®¡åœ¨huggingface.coä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ä¸­ã€‚æœ‰æ•ˆçš„æ¨¡å‹idå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹å‘½åç©ºé—´ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª*ç›®å½•*çš„è·¯å¾„ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)ä¿å­˜çš„æ¨¡å‹æƒé‡ï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/`ã€‚
- en: A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`).
    In this case, `from_pt` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the PyTorch model in a TensorFlow model using the provided conversion scripts
    and loading the TensorFlow model afterwards.
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª *PyTorch state_dict ä¿å­˜æ–‡ä»¶* çš„è·¯å¾„æˆ– urlï¼ˆä¾‹å¦‚ï¼Œ`./pt_model/pytorch_model.bin`ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`from_pt`
    åº”è®¾ç½®ä¸º `True`ï¼Œå¹¶ä¸”åº”å°†é…ç½®å¯¹è±¡ä½œä¸º `config` å‚æ•°æä¾›ã€‚ä½¿ç”¨æ­¤åŠ è½½è·¯å¾„æ¯”ä½¿ç”¨æä¾›çš„è½¬æ¢è„šæœ¬å°† PyTorch æ¨¡å‹è½¬æ¢ä¸º TensorFlow
    æ¨¡å‹å¹¶éšååŠ è½½ TensorFlow æ¨¡å‹è¦æ…¢ã€‚
- en: '`None` if you are both providing the configuration and state dictionary (resp.
    with keyword arguments `config` and `state_dict`).'
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åŒæ—¶æä¾›é…ç½®å’ŒçŠ¶æ€å­—å…¸ï¼ˆåˆ†åˆ«ä½¿ç”¨å…³é”®å­—å‚æ•° `config` å’Œ `state_dict`ï¼‰ï¼Œåˆ™ä¸º `None`ã€‚
- en: '`model_args` (sequence of positional arguments, *optional*) â€” All remaining
    positional arguments will be passed to the underlying modelâ€™s `__init__` method.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`ï¼ˆä½ç½®å‚æ•°åºåˆ—ï¼Œ*å¯é€‰*ï¼‰ â€” æ‰€æœ‰å‰©ä½™çš„ä½ç½®å‚æ•°å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹çš„ `__init__` æ–¹æ³•ã€‚'
- en: '`config` (`Union[PretrainedConfig, str]`, *optional*) â€” Can be either:'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`Union[PretrainedConfig, str]`, *å¯é€‰*) â€” å¯ä»¥æ˜¯ï¼š'
- en: an instance of a class derived from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä» [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    æ´¾ç”Ÿçš„ç±»çš„å®ä¾‹ï¼Œ
- en: a string valid as input to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained).
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½œä¸º [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)
    è¾“å…¥æœ‰æ•ˆçš„å­—ç¬¦ä¸²ã€‚
- en: 'Configuration for the model to use instead of an automatically loaded configuration.
    Configuration can be automatically loaded when:'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç”¨äºæ›¿ä»£è‡ªåŠ¨åŠ è½½é…ç½®çš„æ¨¡å‹é…ç½®ã€‚å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶ï¼Œé…ç½®å¯ä»¥è‡ªåŠ¨åŠ è½½ï¼š
- en: The model is a model provided by the library (loaded with the *model id* string
    of a pretrained model).
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯åº“æä¾›çš„æ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ *æ¨¡å‹ id* å­—ç¬¦ä¸²åŠ è½½ï¼‰ã€‚
- en: The model was saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)
    and is reloaded by supplying the save directory.
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯ä½¿ç”¨ [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)
    ä¿å­˜çš„ï¼Œå¹¶é€šè¿‡æä¾›ä¿å­˜ç›®å½•é‡æ–°åŠ è½½ã€‚
- en: The model is loaded by supplying a local directory as `pretrained_model_name_or_path`
    and a configuration JSON file named *config.json* is found in the directory.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æä¾›æœ¬åœ°ç›®å½•ä½œä¸º `pretrained_model_name_or_path` å¹¶åœ¨ç›®å½•ä¸­æ‰¾åˆ°åä¸º *config.json* çš„é…ç½® JSON
    æ–‡ä»¶æ¥åŠ è½½æ¨¡å‹ã€‚
- en: '`from_pt` (`bool`, *optional*, defaults to `False`) â€” Load the model weights
    from a PyTorch state_dict save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_pt` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” ä» PyTorch state_dict ä¿å­˜æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡ï¼ˆè¯·å‚é˜…
    `pretrained_model_name_or_path` å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`ignore_mismatched_sizes` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to raise an error if some of the weights from the checkpoint do not have
    the same size as the weights of the model (if for instance, you are instantiating
    a model with 10 labels from a checkpoint with 3 labels).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_mismatched_sizes` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨æ£€æŸ¥ç‚¹çš„æŸäº›æƒé‡ä¸æ¨¡å‹çš„æƒé‡å¤§å°ä¸åŒæ—¶å¼•å‘é”™è¯¯ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä»å…·æœ‰
    3 ä¸ªæ ‡ç­¾çš„æ£€æŸ¥ç‚¹å®ä¾‹åŒ–å…·æœ‰ 10 ä¸ªæ ‡ç­¾çš„æ¨¡å‹ï¼‰ã€‚'
- en: '`cache_dir` (`str`, *optional*) â€” Path to a directory in which a downloaded
    pretrained model configuration should be cached if the standard cache should not
    be used.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir` (`str`, *å¯é€‰*) â€” ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®åº”ç¼“å­˜åœ¨å…¶ä¸­çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸åº”ä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists. proxies â€” (`Dict[str, str],` optional`): A dictionary of proxy
    servers to use by protocol or endpoint, e.g.,` {â€˜httpâ€™: â€˜foo.bar:3128â€™, â€˜http://hostnameâ€™:
    â€˜foo.bar:4012â€™}`. The proxies are used on each request. output_loading_info(`bool`,
    *optional*, defaults to` False`): Whether ot not to also return a dictionary containing
    missing keys, unexpected keys and error messages.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ é™¤æ¥æ”¶ä¸å®Œæ•´çš„æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„æ–‡ä»¶ï¼Œå°†å°è¯•æ¢å¤ä¸‹è½½ã€‚ä»£ç†
    â€” (`Dict[str, str],` å¯é€‰`): ç”¨äºæ¯ä¸ªè¯·æ±‚çš„åè®®æˆ–ç«¯ç‚¹çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚` {â€˜httpâ€™: â€˜foo.bar:3128â€™, â€˜http://hostnameâ€™:
    â€˜foo.bar:4012â€™}`ã€‚ä»£ç†å°†ç”¨äºæ¯ä¸ªè¯·æ±‚ã€‚output_loading_info(`bool`, *å¯é€‰*, é»˜è®¤ä¸º` False`): æ˜¯å¦è¿”å›åŒ…å«ç¼ºå¤±é”®ã€æ„å¤–é”®å’Œé”™è¯¯æ¶ˆæ¯çš„å­—å…¸ã€‚'
- en: '`local_files_only(bool,` *optional*, defaults to `False`) â€” Whether or not
    to only look at local files (e.g., not try downloading the model).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only(bool,` *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…æŸ¥çœ‹æœ¬åœ°æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œä¸å°è¯•ä¸‹è½½æ¨¡å‹ï¼‰ã€‚'
- en: '`token` (`str` or `bool`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str` æˆ– `bool`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œæˆ–è€…æœªæŒ‡å®šï¼Œåˆ™å°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) â€” The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å®ƒå¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°æˆ–æäº¤ idï¼Œå› ä¸ºæˆ‘ä»¬åœ¨
    huggingface.co ä¸Šä½¿ç”¨åŸºäº git çš„ç³»ç»Ÿå­˜å‚¨æ¨¡å‹å’Œå…¶ä»–å·¥ä»¶ï¼Œæ‰€ä»¥ `revision` å¯ä»¥æ˜¯ git å…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚'
- en: Instantiate a pretrained TF 2.0 model from a pre-trained model configuration.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒæ¨¡å‹é…ç½®å®ä¾‹åŒ–é¢„è®­ç»ƒçš„ TF 2.0 æ¨¡å‹ã€‚
- en: The warning *Weights from XXX not initialized from pretrained model* means that
    the weights of XXX do not come pretrained with the rest of the model. It is up
    to you to train those weights with a downstream fine-tuning task.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š *Weights from XXX not initialized from pretrained model* æ„å‘³ç€ XXX çš„æƒé‡ä¸æ˜¯ä¸æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ä¸€èµ·é¢„è®­ç»ƒçš„ã€‚æ‚¨éœ€è¦ä½¿ç”¨ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡æ¥è®­ç»ƒè¿™äº›æƒé‡ã€‚
- en: The warning *Weights from XXX not used in YYY* means that the layer XXX is not
    used by YYY, therefore those weights are discarded.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š*æ¥è‡ª XXX çš„æƒé‡åœ¨ YYY ä¸­æœªä½¿ç”¨*è¡¨ç¤ºå±‚ XXX æœªè¢« YYY ä½¿ç”¨ï¼Œå› æ­¤è¿™äº›æƒé‡è¢«ä¸¢å¼ƒã€‚
- en: 'Examples:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE51]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `get_bias`'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_bias`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1931)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1931)'
- en: '[PRE52]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Returns
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`tf.Variable`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable`'
- en: The weights representing the bias, None if not an LM model.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºåç½®çš„æƒé‡ï¼Œå¦‚æœä¸æ˜¯ LM æ¨¡å‹åˆ™ä¸º Noneã€‚
- en: Dict of bias attached to an LM head. The key represents the name of the bias
    attribute.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: é™„åŠ åˆ° LM head çš„åç½®çš„å­—å…¸ã€‚é”®è¡¨ç¤ºåç½®å±æ€§çš„åç§°ã€‚
- en: '#### `get_head_mask`'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_head_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1168)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1168)'
- en: '[PRE53]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Parameters
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`head_mask` (`tf.Tensor` with shape `[num_heads]` or `[num_hidden_layers x
    num_heads]`, *optional*) â€” The mask indicating if we should keep the heads or
    not (1.0 for keep, 0.0 for discard).'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `[num_heads]` æˆ– `[num_hidden_layers x num_heads]`ï¼Œ*å¯é€‰*)
    â€” æŒ‡ç¤ºæˆ‘ä»¬æ˜¯å¦åº”ä¿ç•™å¤´éƒ¨çš„æ©ç ï¼ˆä¿ç•™ä¸º 1.0ï¼Œä¸¢å¼ƒä¸º 0.0ï¼‰ã€‚'
- en: '`num_hidden_layers` (`int`) â€” The number of hidden layers in the model.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`) â€” æ¨¡å‹ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: Prepare the head mask if needed.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦ï¼Œå‡†å¤‡å¤´éƒ¨æ©ç ã€‚
- en: '#### `get_input_embeddings`'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_input_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1315)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1315)'
- en: '[PRE54]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Returns
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`tf.Variable`'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable`'
- en: The embeddings layer mapping vocabulary to hidden states.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯æ±‡æ˜ å°„åˆ°éšè—çŠ¶æ€çš„åµŒå…¥å±‚ã€‚
- en: Returns the modelâ€™s input embeddings layer.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›æ¨¡å‹çš„è¾“å…¥åµŒå…¥å±‚ã€‚
- en: '#### `get_lm_head`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_lm_head`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1964)'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1964)'
- en: '[PRE55]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Returns
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`tf.keras.layers.Layer`'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.layers.Layer`'
- en: The LM head layer if the model has one, None if not.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹æœ‰ LM head å±‚ï¼Œåˆ™ä¸º LM head å±‚ï¼Œå¦åˆ™ä¸º Noneã€‚
- en: The LM Head layer. This method must be overwritten by all the models that have
    a lm head.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: LM Head å±‚ã€‚è¯¥æ–¹æ³•å¿…é¡»è¢«æ‰€æœ‰å…·æœ‰ lm head çš„æ¨¡å‹è¦†ç›–ã€‚
- en: '#### `get_output_embeddings`'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_output_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1871)'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1871)'
- en: '[PRE56]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Returns
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`tf.Variable`'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable`'
- en: The new weights mapping vocabulary to hidden states.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯æ±‡æ˜ å°„åˆ°éšè—çŠ¶æ€çš„æ–°æƒé‡ã€‚
- en: Returns the modelâ€™s output embeddings
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›æ¨¡å‹çš„è¾“å‡ºåµŒå…¥
- en: '#### `get_output_layer_with_bias`'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_output_layer_with_bias`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1908)'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1908)'
- en: '[PRE57]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Returns
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`tf.keras.layers.Layer`'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.layers.Layer`'
- en: The layer that handles the bias, None if not an LM model.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†åç½®çš„å±‚ï¼Œå¦‚æœä¸æ˜¯ LM æ¨¡å‹åˆ™ä¸º Noneã€‚
- en: Get the layer that handles a bias attribute in case the model has an LM head
    with weights tied to the embeddings
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å¤„ç†åç½®å±æ€§çš„å±‚ï¼Œå¦‚æœæ¨¡å‹å…·æœ‰å°†æƒé‡ç»‘å®šåˆ°åµŒå…¥çš„ LM head
- en: '#### `get_prefix_bias_name`'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_prefix_bias_name`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1921)'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1921)'
- en: '[PRE58]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Returns
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`str`'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`'
- en: The _prefix name of the bias.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: åç½®çš„è¿æ¥å‰ç¼€åç§°ã€‚
- en: Get the concatenated _prefix name of the bias from the model name to the parent
    layer
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹åç§°åˆ°çˆ¶å±‚çš„åç½®çš„è¿æ¥å‰ç¼€åç§°
- en: '#### `load_repo_checkpoint`'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_repo_checkpoint`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1342)'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1342)'
- en: '[PRE59]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parameters
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`repo_path_or_name` (`str`) â€” Can either be a repository name for your {object}
    in the Hub or a path to a local folder (in which case the repository will have
    the name of that local folder).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_path_or_name` (`str`) â€” å¯ä»¥æ˜¯ Hub ä¸­æ‚¨çš„ {object} çš„å­˜å‚¨åº“åç§°ï¼Œä¹Ÿå¯ä»¥æ˜¯æœ¬åœ°æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå­˜å‚¨åº“å°†ä½¿ç”¨è¯¥æœ¬åœ°æ–‡ä»¶å¤¹çš„åç§°ï¼‰ã€‚'
- en: Returns
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`dict`'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '`dict`'
- en: A dictionary of extra metadata from the checkpoint, most commonly an â€œepochâ€
    count.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªæ£€æŸ¥ç‚¹çš„é¢å¤–å…ƒæ•°æ®å­—å…¸ï¼Œé€šå¸¸æ˜¯â€œæ—¶ä»£â€è®¡æ•°ã€‚
- en: Loads a saved checkpoint (model weights and optimizer state) from a repo. Returns
    the current epoch count when the checkpoint was made.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å­˜å‚¨åº“åŠ è½½å·²ä¿å­˜çš„æ£€æŸ¥ç‚¹ï¼ˆæ¨¡å‹æƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼‰ã€‚è¿”å›æ£€æŸ¥ç‚¹ç”Ÿæˆæ—¶çš„å½“å‰æ—¶ä»£è®¡æ•°ã€‚
- en: '#### `prepare_tf_dataset`'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_tf_dataset`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1391)'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1391)'
- en: '[PRE60]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`dataset` (`Any`) â€” A [~`datasets.Dataset`] to be wrapped as a `tf.data.Dataset`.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset` (`Any`) â€” è¦åŒ…è£…ä¸º `tf.data.Dataset` çš„ [~`datasets.Dataset`]ã€‚'
- en: '`batch_size` (`int`, defaults to 8) â€” The size of batches to return.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`ï¼Œé»˜è®¤ä¸º 8) â€” è¦è¿”å›çš„æ‰¹æ¬¡å¤§å°ã€‚'
- en: '`shuffle` (`bool`, defaults to `True`) â€” Whether to return samples from the
    dataset in random order. Usually `True` for training datasets and `False` for
    validation/test datasets.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle` (`bool`ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦ä»¥éšæœºé¡ºåºè¿”å›æ•°æ®é›†ä¸­çš„æ ·æœ¬ã€‚é€šå¸¸å¯¹äºè®­ç»ƒæ•°æ®é›†ä¸º `True`ï¼Œå¯¹äºéªŒè¯/æµ‹è¯•æ•°æ®é›†ä¸º
    `False`ã€‚'
- en: '`tokenizer` ([PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase),
    *optional*) â€” A `PreTrainedTokenizer` that will be used to pad samples to create
    batches. Has no effect if a specific `collate_fn` is passed instead.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)ï¼Œ*å¯é€‰*ï¼‰
    â€” ç”¨äºå¡«å……æ ·æœ¬ä»¥åˆ›å»ºæ‰¹æ¬¡çš„ `PreTrainedTokenizer`ã€‚å¦‚æœä¼ é€’äº†ç‰¹å®šçš„ `collate_fn`ï¼Œåˆ™ä¸ä¼šäº§ç”Ÿå½±å“ã€‚'
- en: '`collate_fn` (`Callable`, *optional*) â€” A function that collates samples from
    the dataset into a single batch. Defaults to `DefaultDataCollator` if no `tokenizer`
    is supplied or `DataCollatorWithPadding` if a `tokenizer` is passed.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collate_fn` (`Callable`ï¼Œ*å¯é€‰*) â€” ä¸€ä¸ªå°†æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•´ç†æˆå•ä¸ªæ‰¹æ¬¡çš„å‡½æ•°ã€‚å¦‚æœæœªæä¾› `tokenizer`ï¼Œåˆ™é»˜è®¤ä¸º
    `DefaultDataCollator`ï¼Œå¦‚æœä¼ é€’äº† `tokenizer`ï¼Œåˆ™ä¸º `DataCollatorWithPadding`ã€‚'
- en: '`collate_fn_args` (`Dict[str, Any]`, *optional*) â€” A dict of arguments to pass
    to the `collate_fn` alongside the list of samples.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collate_fn_args` (`Dict[str, Any]`, *å¯é€‰*) â€” ä¼ é€’ç»™`collate_fn`çš„å‚æ•°å­—å…¸ï¼Œä»¥åŠæ ·æœ¬åˆ—è¡¨ã€‚'
- en: '`drop_remainder` (`bool`, *optional*) â€” Whether to drop the final batch, if
    the batch_size does not evenly divide the dataset length. Defaults to the same
    setting as `shuffle`.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_remainder` (`bool`, *å¯é€‰*) â€” æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼Œå¦‚æœæ‰¹æ¬¡å¤§å°ä¸èƒ½æ•´é™¤æ•°æ®é›†é•¿åº¦ã€‚é»˜è®¤è®¾ç½®ä¸`shuffle`ç›¸åŒã€‚'
- en: '`prefetch` (`bool`, defaults to `True`) â€” Whether to add prefetching to the
    end of the `tf.data` pipeline. This is almost always beneficial for performance,
    but can be disabled in edge cases.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefetch` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨`tf.data`ç®¡é“çš„æœ«å°¾æ·»åŠ é¢„å–ã€‚è¿™å‡ ä¹æ€»æ˜¯æœ‰åˆ©äºæ€§èƒ½ï¼Œä½†åœ¨è¾¹ç¼˜æƒ…å†µä¸‹å¯ä»¥ç¦ç”¨ã€‚'
- en: Returns
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`Dataset`'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset`'
- en: A `tf.data.Dataset` which is ready to pass to the Keras API.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå‡†å¤‡ä¼ é€’ç»™Keras APIçš„`tf.data.Dataset`ã€‚
- en: Wraps a HuggingFace [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)
    as a `tf.data.Dataset` with collation and batching. This method is designed to
    create a â€œready-to-useâ€ dataset that can be passed directly to Keras methods like
    `fit()` without further modification. The method will drop columns from the dataset
    if they donâ€™t match input names for the model. If you want to specify the column
    names to return rather than using the names that match this model, we recommend
    using `Dataset.to_tf_dataset()` instead.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: å°†HuggingFace [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)åŒ…è£…ä¸ºå¸¦æœ‰æ•´ç†å’Œæ‰¹å¤„ç†çš„`tf.data.Dataset`ã€‚æ­¤æ–¹æ³•æ—¨åœ¨åˆ›å»ºä¸€ä¸ªâ€œå³æ’å³ç”¨â€çš„æ•°æ®é›†ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’ç»™Kerasæ–¹æ³•ï¼Œå¦‚`fit()`ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥ä¿®æ”¹ã€‚å¦‚æœæ•°æ®é›†ä¸­çš„åˆ—ä¸æ¨¡å‹çš„è¾“å…¥åç§°ä¸åŒ¹é…ï¼Œè¯¥æ–¹æ³•å°†åˆ é™¤è¿™äº›åˆ—ã€‚å¦‚æœæ‚¨æƒ³æŒ‡å®šè¦è¿”å›çš„åˆ—åï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¸æ­¤æ¨¡å‹åŒ¹é…çš„åç§°ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`Dataset.to_tf_dataset()`ã€‚
- en: '#### `prune_heads`'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prune_heads`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2311)'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2311)'
- en: '[PRE61]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Parameters
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`heads_to_prune` (`Dict[int, List[int]]`) â€” Dictionary with keys being selected
    layer indices (`int`) and associated values being the list of heads to prune in
    said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads
    0 and 2 on layer 1 and heads 2 and 3 on layer 2.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`heads_to_prune` (`Dict[int, List[int]]`) â€” é”®ä¸ºé€‰å®šçš„å±‚ç´¢å¼•ï¼ˆ`int`ï¼‰çš„å­—å…¸ï¼Œç›¸å…³å€¼ä¸ºè¦åœ¨è¯¥å±‚ä¸­ä¿®å‰ªçš„å¤´éƒ¨åˆ—è¡¨ï¼ˆ`int`åˆ—è¡¨ï¼‰ã€‚ä¾‹å¦‚ï¼Œ{1:
    [0, 2], 2: [2, 3]}å°†åœ¨ç¬¬1å±‚ä¿®å‰ªå¤´éƒ¨0å’Œ2ï¼Œåœ¨ç¬¬2å±‚ä¿®å‰ªå¤´éƒ¨2å’Œ3ã€‚'
- en: Prunes heads of the base model.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®å‰ªåŸºç¡€æ¨¡å‹çš„å¤´éƒ¨ã€‚
- en: '#### `register_for_auto_class`'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_for_auto_class`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3176)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3176)'
- en: '[PRE62]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`auto_class` (`str` or `type`, *optional*, defaults to `"TFAutoModel"`) â€” The
    auto class to register this new model with.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_class` (`str` æˆ– `type`, *å¯é€‰*, é»˜è®¤ä¸º `"TFAutoModel"`) â€” è¦æ³¨å†Œæ­¤æ–°æ¨¡å‹çš„è‡ªåŠ¨ç±»ã€‚'
- en: Register this class with a given auto class. This should only be used for custom
    models as the ones in the library are already mapped with an auto class.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç»™å®šçš„è‡ªåŠ¨ç±»æ³¨å†Œæ­¤ç±»ã€‚è¿™åº”ä»…ç”¨äºè‡ªå®šä¹‰æ¨¡å‹ï¼Œå› ä¸ºåº“ä¸­çš„æ¨¡å‹å·²ç»ä¸è‡ªåŠ¨ç±»æ˜ å°„ã€‚
- en: This API is experimental and may have some slight breaking changes in the next
    releases.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤APIæ˜¯å®éªŒæ€§çš„ï¼Œå¯èƒ½åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æœ‰ä¸€äº›è½»å¾®çš„ç ´åæ€§æ›´æ”¹ã€‚
- en: '#### `resize_token_embeddings`'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `resize_token_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1973)'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1973)'
- en: '[PRE63]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Parameters
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`new_num_tokens` (`int`, *optional*) â€” The number of new tokens in the embedding
    matrix. Increasing the size will add newly initialized vectors at the end. Reducing
    the size will remove vectors from the end. If not provided or `None`, just returns
    a pointer to the input tokens without doing anything.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_num_tokens` (`int`, *å¯é€‰*) â€” åµŒå…¥çŸ©é˜µä¸­çš„æ–°æ ‡è®°æ•°é‡ã€‚å¢åŠ å¤§å°å°†åœ¨æœ«å°¾æ·»åŠ æ–°åˆå§‹åŒ–çš„å‘é‡ã€‚å‡å°å¤§å°å°†ä»æœ«å°¾åˆ é™¤å‘é‡ã€‚å¦‚æœæœªæä¾›æˆ–ä¸º`None`ï¼Œåˆ™åªè¿”å›æŒ‡å‘è¾“å…¥æ ‡è®°çš„æŒ‡é’ˆï¼Œè€Œä¸æ‰§è¡Œä»»ä½•æ“ä½œã€‚'
- en: Returns
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`tf.Variable` or `tf.keras.layers.Embedding`'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable` æˆ– `tf.keras.layers.Embedding`'
- en: Pointer to the input tokens of the model.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„è¾“å…¥æ ‡è®°çš„æŒ‡é’ˆã€‚
- en: Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ`new_num_tokens != config.vocab_size`ï¼Œåˆ™è°ƒæ•´æ¨¡å‹çš„è¾“å…¥æ ‡è®°åµŒå…¥çŸ©é˜µå¤§å°ã€‚
- en: Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()`
    method.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹ç±»å…·æœ‰`tie_weights()`æ–¹æ³•ï¼Œåˆ™åœ¨ä¹‹åå¤„ç†æƒé‡åµŒå…¥ã€‚
- en: '#### `save_pretrained`'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2323)'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2323)'
- en: '[PRE64]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Parameters
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str`) â€” Directory to which to save. Will be created if it
    doesnâ€™t exist.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory` (`str`) â€” è¦ä¿å­˜åˆ°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºè¯¥ç›®å½•ã€‚'
- en: '`saved_model` (`bool`, *optional*, defaults to `False`) â€” If the model has
    to be saved in saved model format as well or not.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`saved_model` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿˜è¦å°†æ¨¡å‹ä¿å­˜ä¸ºsaved modelæ ¼å¼ã€‚'
- en: '`version` (`int`, *optional*, defaults to 1) â€” The version of the saved model.
    A saved model needs to be versioned in order to be properly loaded by TensorFlow
    Serving as detailed in the official documentation [https://www.tensorflow.org/tfx/serving/serving_basic](https://www.tensorflow.org/tfx/serving/serving_basic)'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`version` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” å·²ä¿å­˜æ¨¡å‹çš„ç‰ˆæœ¬ã€‚ä¸ºäº†èƒ½å¤Ÿè¢«TensorFlow Servingæ­£ç¡®åŠ è½½ï¼Œä¿å­˜çš„æ¨¡å‹éœ€è¦è¿›è¡Œç‰ˆæœ¬åŒ–ï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£[https://www.tensorflow.org/tfx/serving/serving_basic](https://www.tensorflow.org/tfx/serving/serving_basic)'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨ä¿å­˜åå°†æ¨¡å‹æ¨é€åˆ°Hugging Faceæ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`repo_id`æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„`save_directory`åç§°ï¼‰ã€‚'
- en: '`signatures` (`dict` or `tf.function`, *optional*) â€” Modelâ€™s signature used
    for serving. This will be passed to the `signatures` argument of model.save().'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`signatures` (`dict` æˆ– `tf.function`, *å¯é€‰*) â€” ç”¨äºservingçš„æ¨¡å‹ç­¾åã€‚è¿™å°†ä¼ é€’ç»™model.save()çš„`signatures`å‚æ•°ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) â€” The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º `"10GB"`) - åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹åˆ†ç‰‡å°†å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚
    `"5MB"`ï¼‰ã€‚'
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹çš„å•ä¸ªæƒé‡å¤§äº `max_shard_size`ï¼Œå®ƒå°†åœ¨è‡ªå·±çš„æ£€æŸ¥ç‚¹åˆ†ç‰‡ä¸­ï¼Œè¯¥åˆ†ç‰‡å°†å¤§äº `max_shard_size`ã€‚
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) â€” Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) - æ˜¯å¦åˆ›å»ºå¸¦æœ‰ä¸Šä¼ æ–‡ä»¶çš„ PR æˆ–ç›´æ¥æäº¤ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) â€” Whether to
    save the model using `safetensors` or the traditional TensorFlow way (that uses
    `h5`).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) - æ˜¯å¦ä½¿ç”¨ `safetensors` æˆ–ä¼ ç»Ÿçš„
    TensorFlow æ–¹å¼ï¼ˆä½¿ç”¨ `h5`ï¼‰ä¿å­˜æ¨¡å‹ã€‚'
- en: '`token` (`str` or `bool`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str` æˆ– `bool`, *å¯é€‰*) - ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *å¯é€‰*) - ä¼ é€’ç»™ [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Save a model and its configuration file to a directory, so that it can be re-loaded
    using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    class method.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åŠå…¶é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    ç±»æ–¹æ³•é‡æ–°åŠ è½½ã€‚
- en: '#### `serving`'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `serving`'
- en: '[PRE65]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`Method` used for serving the model. Does not have a specific signature, but
    will be specialized as concrete â€”'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºæä¾›æ¨¡å‹çš„æ–¹æ³•ã€‚æ²¡æœ‰ç‰¹å®šçš„ç­¾åï¼Œä½†å°†ä½œä¸ºå…·ä½“çš„ä¸“ä¸šåŒ– -
- en: '`functions` when saving with `save_pretrained`. â€” inputs (`Dict[str, tf.Tensor]`):
    The input of the saved model as a dictionary of tensors.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `save_pretrained` ä¿å­˜æ—¶çš„ `functions`ã€‚ - è¾“å…¥ï¼ˆ`Dict[str, tf.Tensor]`ï¼‰ï¼šä¿å­˜æ¨¡å‹çš„è¾“å…¥ï¼Œä½œä¸ºå¼ é‡å­—å…¸ã€‚
- en: '#### `serving_output`'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `serving_output`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1277)'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1277)'
- en: '[PRE66]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Prepare the output of the saved model. Can be overridden if specific serving
    modifications are required.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡ä¿å­˜æ¨¡å‹çš„è¾“å‡ºã€‚å¦‚æœéœ€è¦ç‰¹å®šçš„æœåŠ¡ä¿®æ”¹ï¼Œå¯ä»¥è¿›è¡Œè¦†ç›–ã€‚
- en: '#### `set_bias`'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_bias`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1948)'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1948)'
- en: '[PRE67]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`value` (`Dict[tf.Variable]`) â€” All the new bias attached to an LM head.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value` (`Dict[tf.Variable]`) - é™„åŠ åˆ° LM å¤´éƒ¨çš„æ‰€æœ‰æ–°åç½®ã€‚'
- en: Set all the bias in the LM head.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½® LM å¤´éƒ¨ä¸­çš„æ‰€æœ‰åç½®ã€‚
- en: '#### `set_input_embeddings`'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_input_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1851)'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1851)'
- en: '[PRE68]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Parameters
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`value` (`tf.Variable`) â€” The new weights mapping hidden states to vocabulary.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value` (`tf.Variable`) - å°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨çš„æ–°æƒé‡ã€‚'
- en: Set modelâ€™s input embeddings
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®æ¨¡å‹çš„è¾“å…¥åµŒå…¥
- en: '#### `set_output_embeddings`'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_output_embeddings`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1891)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1891)'
- en: '[PRE69]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Parameters
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`value` (`tf.Variable`) â€” The new weights mapping hidden states to vocabulary.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value` (`tf.Variable`) - å°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨çš„æ–°æƒé‡ã€‚'
- en: Set modelâ€™s output embeddings
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®æ¨¡å‹çš„è¾“å‡ºåµŒå…¥
- en: '#### `test_step`'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `test_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1687)'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1687)'
- en: '[PRE70]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: A modification of Kerasâ€™s default `train_step` that correctly handles matching
    outputs to labels for our models and supports directly training on the loss output
    head. In addition, it ensures input keys are copied to the labels where appropriate.
    It will also copy label keys into the input dict when using the dummy loss, to
    ensure that they are available to the model during the forward pass.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ Keras é»˜è®¤çš„ `train_step` è¿›è¡Œä¿®æ”¹ï¼Œæ­£ç¡®å¤„ç†æ¨¡å‹è¾“å‡ºä¸æ ‡ç­¾çš„åŒ¹é…ï¼Œå¹¶æ”¯æŒç›´æ¥åœ¨æŸå¤±è¾“å‡ºå¤´ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒç¡®ä¿é€‚å½“æ—¶å°†è¾“å…¥é”®å¤åˆ¶åˆ°æ ‡ç­¾ä¸­ã€‚å½“ä½¿ç”¨è™šæ‹ŸæŸå¤±æ—¶ï¼Œå®ƒè¿˜ä¼šå°†æ ‡ç­¾é”®å¤åˆ¶åˆ°è¾“å…¥å­—å…¸ä¸­ï¼Œä»¥ç¡®ä¿å®ƒä»¬åœ¨å‰å‘ä¼ é€’æœŸé—´å¯¹æ¨¡å‹å¯ç”¨ã€‚
- en: '#### `train_step`'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `train_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1579)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1579)'
- en: '[PRE71]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: A modification of Kerasâ€™s default `train_step` that correctly handles matching
    outputs to labels for our models and supports directly training on the loss output
    head. In addition, it ensures input keys are copied to the labels where appropriate.
    It will also copy label keys into the input dict when using the dummy loss, to
    ensure that they are available to the model during the forward pass.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ Keras é»˜è®¤çš„ `train_step` è¿›è¡Œä¿®æ”¹ï¼Œæ­£ç¡®å¤„ç†æ¨¡å‹è¾“å‡ºä¸æ ‡ç­¾çš„åŒ¹é…ï¼Œå¹¶æ”¯æŒç›´æ¥åœ¨æŸå¤±è¾“å‡ºå¤´ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒç¡®ä¿é€‚å½“æ—¶å°†è¾“å…¥é”®å¤åˆ¶åˆ°æ ‡ç­¾ä¸­ã€‚å½“ä½¿ç”¨è™šæ‹ŸæŸå¤±æ—¶ï¼Œå®ƒè¿˜ä¼šå°†æ ‡ç­¾é”®å¤åˆ¶åˆ°è¾“å…¥å­—å…¸ä¸­ï¼Œä»¥ç¡®ä¿å®ƒä»¬åœ¨å‰å‘ä¼ é€’æœŸé—´å¯¹æ¨¡å‹å¯ç”¨ã€‚
- en: TFModelUtilsMixin
  id: totrans-576
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFModelUtilsMixin
- en: '### `class transformers.modeling_tf_utils.TFModelUtilsMixin`'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_tf_utils.TFModelUtilsMixin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L104)'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L104)'
- en: '[PRE72]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: A few utilities for `tf.keras.Model`, to be used as a mixin.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äº `tf.keras.Model` çš„ä¸€äº›å®ç”¨ç¨‹åºï¼Œå¯ç”¨ä½œæ··åˆã€‚
- en: '#### `num_parameters`'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_parameters`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L109)'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L109)'
- en: '[PRE73]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Parameters
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`only_trainable` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to return only the number of trainable parameters'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_trainable` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…è¿”å›å¯è®­ç»ƒå‚æ•°çš„æ•°é‡'
- en: Returns
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: The number of parameters.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°æ•°é‡ã€‚
- en: Get the number of (optionally, trainable) parameters in the model.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–æ¨¡å‹ä¸­çš„ï¼ˆå¯é€‰çš„å¯è®­ç»ƒï¼‰å‚æ•°æ•°é‡ã€‚
- en: FlaxPreTrainedModel
  id: totrans-590
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxPreTrainedModel
- en: '### `class transformers.FlaxPreTrainedModel`'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxPreTrainedModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L166)'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L166)'
- en: '[PRE74]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Base class for all models.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹çš„åŸºç±»ã€‚
- en: '[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)
    takes care of storing the configuration of the models and handles methods for
    loading, downloading and saving models.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)
    è´Ÿè´£å­˜å‚¨æ¨¡å‹çš„é…ç½®ï¼Œå¹¶å¤„ç†åŠ è½½ã€ä¸‹è½½å’Œä¿å­˜æ¨¡å‹çš„æ–¹æ³•ã€‚'
- en: 'Class attributes (overridden by derived classes):'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å±æ€§ï¼ˆç”±æ´¾ç”Ÿç±»è¦†ç›–ï¼‰ï¼š
- en: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” A subclass of [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    to use as configuration class for this model architecture.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    â€” ç”¨ä½œæ­¤æ¨¡å‹æ¶æ„çš„é…ç½®ç±»çš„ [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„å­ç±»ã€‚'
- en: '`base_model_prefix` (`str`) â€” A string indicating the attribute associated
    to the base model in derived classes of the same architecture adding modules on
    top of the base model.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base_model_prefix` (`str`) â€” ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒæŒ‡ç¤ºæ´¾ç”Ÿç±»ä¸­åŸºç¡€æ¨¡å‹å…³è”çš„å±æ€§ï¼Œè¯¥æ´¾ç”Ÿç±»åœ¨åŸºç¡€æ¨¡å‹ä¹‹ä¸Šæ·»åŠ æ¨¡å—ã€‚'
- en: '`main_input_name` (`str`) â€” The name of the principal input to the model (often
    `input_ids` for NLP models, `pixel_values` for vision models and `input_values`
    for speech models).'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main_input_name` (`str`) â€” æ¨¡å‹çš„ä¸»è¦è¾“å…¥çš„åç§°ï¼ˆé€šå¸¸ä¸º NLP æ¨¡å‹çš„ `input_ids`ï¼Œè§†è§‰æ¨¡å‹çš„ `pixel_values`
    å’Œè¯­éŸ³æ¨¡å‹çš„ `input_values`ï¼‰ã€‚'
- en: '#### `push_to_hub`'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
- en: '[PRE75]'
  id: totrans-602
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Parameters
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`repo_id` (`str`) â€” The name of the repository you want to push your model
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id` (`str`) â€” æ‚¨è¦å°†æ¨¡å‹æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ã€‚åœ¨æ¨é€åˆ°ç»™å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚'
- en: '`use_temp_dir` (`bool`, *optional*) â€” Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir` (`bool`, *optional*) â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨åœ¨æ¨é€åˆ° Hub ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰åä¸º `repo_id`
    çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º `True`ï¼Œå¦åˆ™ä¸º `False`ã€‚'
- en: '`commit_message` (`str`, *optional*) â€” Message to commit while pushing. Will
    default to `"Upload model"`.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message` (`str`, *optional*) â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º `"Upload model"`ã€‚'
- en: '`private` (`bool`, *optional*) â€” Whether or not the repository created should
    be private.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private` (`bool`, *optional*) â€” æ˜¯å¦åˆ›å»ºçš„å­˜å‚¨åº“åº”ä¸ºç§æœ‰ã€‚'
- en: '`token` (`bool` or `str`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`bool` æˆ– `str`, *optional*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š `repo_url`ï¼Œåˆ™é»˜è®¤ä¸º `True`ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) â€” Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int` æˆ– `str`, *optional*, é»˜è®¤ä¸º `"5GB"`) â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨è¢«åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åæ£€æŸ¥ç‚¹å°†è¢«åˆ†æˆå°äºæ­¤å¤§å°çš„æ¯ä¸ªéƒ¨åˆ†ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚
    `"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤ä¸º `"5GB"`ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åœ¨å…è´¹çš„ Google Colab å®ä¾‹ä¸Šè½»æ¾åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°ä»»ä½• CPU OOM é—®é¢˜ã€‚'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) â€” Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ä¸Šä¼ æ–‡ä»¶çš„ PR æˆ–ç›´æ¥æäº¤ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†æ¨¡å‹æƒé‡è½¬æ¢ä¸º safetensors
    æ ¼å¼ä»¥è¿›è¡Œæ›´å®‰å…¨çš„åºåˆ—åŒ–ã€‚'
- en: '`revision` (`str`, *optional*) â€” Branch to push the uploaded files to.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *optional*) â€” è¦å°†ä¸Šä¼ çš„æ–‡ä»¶æ¨é€åˆ°çš„åˆ†æ”¯ã€‚'
- en: '`commit_description` (`str`, *optional*) â€” The description of the commit that
    will be created'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_description` (`str`, *optional*) â€” å°†è¦åˆ›å»ºçš„æäº¤çš„æè¿°'
- en: '`tags` (`List[str]`, *optional*) â€” List of tags to push on the Hub.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags` (`List[str]`, *optional*) â€” è¦æ¨é€åˆ° Hub ä¸Šçš„æ ‡ç­¾åˆ—è¡¨ã€‚'
- en: Upload the model checkpoint to the ğŸ¤— Model Hub.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¼ æ¨¡å‹æ£€æŸ¥ç‚¹åˆ° ğŸ¤— Model Hubã€‚
- en: 'Examples:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE76]'
  id: totrans-617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '#### `can_generate`'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `can_generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L506)'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L506)'
- en: '[PRE77]'
  id: totrans-620
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Returns whether this model can generate sequences with `.generate()`. Returns:
    `bool`: Whether this model can generate sequences with `.generate()`.'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿”å›æ­¤æ¨¡å‹æ˜¯å¦å¯ä»¥ä½¿ç”¨ `.generate()` ç”Ÿæˆåºåˆ—ã€‚è¿”å›ï¼š`bool`: æ­¤æ¨¡å‹æ˜¯å¦å¯ä»¥ä½¿ç”¨ `.generate()` ç”Ÿæˆåºåˆ—ã€‚'
- en: '#### `from_pretrained`'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L518)'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L518)'
- en: '[PRE78]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Parameters
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) â€” Can be either:'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path` (`str`æˆ–`os.PathLike`) â€” å¯ä»¥æ˜¯ï¼š'
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-627
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ‰˜ç®¡åœ¨huggingface.coæ¨¡å‹å­˜å‚¨åº“ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹çš„*æ¨¡å‹ID*ã€‚æœ‰æ•ˆçš„æ¨¡å‹IDå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹å‘½åç©ºé—´åŒ–ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  id: totrans-628
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained)ä¿å­˜çš„æ¨¡å‹æƒé‡çš„*ç›®å½•*è·¯å¾„ï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/`ã€‚
- en: A path or url to a *pt index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_pt` should be set to `True`.
  id: totrans-629
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ptç´¢å¼•æ£€æŸ¥ç‚¹æ–‡ä»¶*çš„è·¯å¾„æˆ–URLï¼ˆä¾‹å¦‚ï¼Œ`./tf_model/model.ckpt.index`ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`from_pt`åº”è®¾ç½®ä¸º`True`ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`, *optional*, é»˜è®¤ä¸º `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-631
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚
- en: '`model_args` (sequence of positional arguments, *optional*) â€” All remaining
    positional arguments will be passed to the underlying modelâ€™s `__init__` method.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_args`ï¼ˆä½ç½®å‚æ•°åºåˆ—ï¼Œ*optional*ï¼‰ â€” æ‰€æœ‰å‰©ä½™çš„ä½ç½®å‚æ•°å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹çš„`__init__`æ–¹æ³•ã€‚'
- en: '`config` (`Union[PretrainedConfig, str, os.PathLike]`, *optional*) â€” Can be
    either:'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`Union[PretrainedConfig, str, os.PathLike]`, *optional*) â€” å¯ä»¥æ˜¯ï¼š'
- en: an instance of a class derived from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
  id: totrans-636
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)æ´¾ç”Ÿçš„ç±»çš„å®ä¾‹ï¼Œ
- en: a string or path valid as input to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained).
  id: totrans-637
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä½œä¸º[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)è¾“å…¥æœ‰æ•ˆçš„å­—ç¬¦ä¸²æˆ–è·¯å¾„ã€‚
- en: 'Configuration for the model to use instead of an automatically loaded configuration.
    Configuration can be automatically loaded when:'
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨çš„æ¨¡å‹é…ç½®ï¼Œè€Œä¸æ˜¯è‡ªåŠ¨åŠ è½½çš„é…ç½®ã€‚å½“ä»¥ä¸‹æƒ…å†µè‡ªåŠ¨åŠ è½½é…ç½®æ—¶ï¼š
- en: The model is a model provided by the library (loaded with the *model id* string
    of a pretrained model).
  id: totrans-639
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯åº“æä¾›çš„æ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„*æ¨¡å‹ID*å­—ç¬¦ä¸²åŠ è½½ï¼‰ã€‚
- en: The model was saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    and is reloaded by supplying the save directory.
  id: totrans-640
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)ä¿å­˜çš„ï¼Œå¹¶é€šè¿‡æä¾›ä¿å­˜ç›®å½•é‡æ–°åŠ è½½ã€‚
- en: The model is loaded by supplying a local directory as `pretrained_model_name_or_path`
    and a configuration JSON file named *config.json* is found in the directory.
  id: totrans-641
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æä¾›æœ¬åœ°ç›®å½•ä½œä¸º`pretrained_model_name_or_path`åŠ è½½æ¨¡å‹ï¼Œå¹¶åœ¨ç›®å½•ä¸­æ‰¾åˆ°åä¸º*config.json*çš„é…ç½®JSONæ–‡ä»¶ã€‚
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” Path to a directory in
    which a downloaded pretrained model configuration should be cached if the standard
    cache should not be used.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” å¦‚æœä¸åº”ä½¿ç”¨æ ‡å‡†ç¼“å­˜ï¼Œåˆ™åº”å°†ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ç¼“å­˜åœ¨å…¶ä¸­çš„ç›®å½•è·¯å¾„ã€‚'
- en: '`from_pt` (`bool`, *optional*, defaults to `False`) â€” Load the model weights
    from a PyTorch checkpoint save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_pt` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” ä»PyTorchæ£€æŸ¥ç‚¹ä¿å­˜æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡ï¼ˆè¯·å‚é˜…`pretrained_model_name_or_path`å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`ignore_mismatched_sizes` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to raise an error if some of the weights from the checkpoint do not have
    the same size as the weights of the model (if for instance, you are instantiating
    a model with 10 labels from a checkpoint with 3 labels).'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_mismatched_sizes` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” å¦‚æœæ£€æŸ¥ç‚¹ä¸­çš„æŸäº›æƒé‡ä¸æ¨¡å‹çš„æƒé‡å¤§å°ä¸åŒï¼Œæ˜¯å¦å¼•å‘é”™è¯¯ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä»å…·æœ‰3ä¸ªæ ‡ç­¾çš„æ£€æŸ¥ç‚¹å®ä¾‹åŒ–å…·æœ‰10ä¸ªæ ‡ç­¾çš„æ¨¡å‹ï¼‰ã€‚'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ é™¤æ¥æ”¶ä¸å®Œæ•´çš„æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„æ–‡ä»¶ï¼Œå°†å°è¯•æ¢å¤ä¸‹è½½ã€‚'
- en: '`proxies` (`Dict[str, str]`, *optional*) â€” A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies` (`Dict[str, str]`, *optional*) â€” ä¸€ä¸ªæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚ï¼Œ`{''http'':
    ''foo.bar:3128'', ''http://hostname'': ''foo.bar:4012''}`ã€‚ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸Šä½¿ç”¨ã€‚'
- en: '`local_files_only(bool,` *optional*, defaults to `False`) â€” Whether or not
    to only look at local files (i.e., do not try to download the model).'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only(bool,` *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…æŸ¥çœ‹æœ¬åœ°æ–‡ä»¶ï¼ˆå³ï¼Œä¸å°è¯•ä¸‹è½½æ¨¡å‹ï¼‰ã€‚'
- en: '`token` (`str` or `bool`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str` æˆ– `bool`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œ (å­˜å‚¨åœ¨ `~/.huggingface` ä¸­)ã€‚'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) â€” The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å®ƒå¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°æˆ–æäº¤ IDï¼Œå› ä¸ºæˆ‘ä»¬åœ¨
    huggingface.co ä¸Šä½¿ç”¨åŸºäº git çš„ç³»ç»Ÿæ¥å­˜å‚¨æ¨¡å‹å’Œå…¶ä»–å·¥ä»¶ï¼Œæ‰€ä»¥ `revision` å¯ä»¥æ˜¯ git å…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚'
- en: Instantiate a pretrained flax model from a pre-trained model configuration.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒæ¨¡å‹é…ç½®å®ä¾‹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒçš„ flax æ¨¡å‹ã€‚
- en: The warning *Weights from XXX not initialized from pretrained model* means that
    the weights of XXX do not come pretrained with the rest of the model. It is up
    to you to train those weights with a downstream fine-tuning task.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š *æ¥è‡ª XXX çš„æƒé‡æœªä»é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–* æ„å‘³ç€ XXX çš„æƒé‡ä¸æ˜¯ä¸æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ä¸€èµ·é¢„è®­ç»ƒçš„ã€‚æ‚¨éœ€è¦ä½¿ç”¨ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡æ¥è®­ç»ƒè¿™äº›æƒé‡ã€‚
- en: The warning *Weights from XXX not used in YYY* means that the layer XXX is not
    used by YYY, therefore those weights are discarded.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š *æ¥è‡ª XXX çš„æƒé‡åœ¨ YYY ä¸­æœªä½¿ç”¨* æ„å‘³ç€å±‚ XXX åœ¨ YYY ä¸­æœªè¢«ä½¿ç”¨ï¼Œå› æ­¤è¿™äº›æƒé‡è¢«ä¸¢å¼ƒã€‚
- en: 'Examples:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE79]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '#### `load_flax_sharded_weights`'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_flax_sharded_weights`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L459)'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L459)'
- en: '[PRE80]'
  id: totrans-658
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Parameters
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`shard_files` (`List[str]` â€” The list of shard files to load.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shard_files` (`List[str]` â€” è¦åŠ è½½çš„åˆ†ç‰‡æ–‡ä»¶åˆ—è¡¨ã€‚'
- en: Returns
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`Dict`'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict`'
- en: 'A nested dictionary of the model parameters, in the expected format for flax
    models : `{''model'': {''params'': {''...''}}}`.'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¨¡å‹å‚æ•°çš„åµŒå¥—å­—å…¸ï¼Œç¬¦åˆ flax æ¨¡å‹çš„é¢„æœŸæ ¼å¼ï¼š`{''model'': {''params'': {''...''}}}`ã€‚'
- en: This is the same as `flax.serialization.from_bytes` (https:lax.readthedocs.io/en/latest/_modules/flax/serialization.html#from_bytes)
    but for a sharded checkpoint.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ `flax.serialization.from_bytes` ç›¸åŒ (https:lax.readthedocs.io/en/latest/_modules/flax/serialization.html#from_bytes)ï¼Œä½†é€‚ç”¨äºåˆ†ç‰‡æ£€æŸ¥ç‚¹ã€‚
- en: 'This load is performed efficiently: each checkpoint shard is loaded one by
    one in RAM and deleted after being loaded in the model.'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŠ è½½æ•ˆç‡å¾ˆé«˜ï¼šæ¯ä¸ªæ£€æŸ¥ç‚¹åˆ†ç‰‡éƒ½ä¼šé€ä¸ªåŠ è½½åˆ° RAM ä¸­ï¼Œå¹¶åœ¨åŠ è½½åˆ°æ¨¡å‹ååˆ é™¤ã€‚
- en: '#### `register_for_auto_class`'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_for_auto_class`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1226)'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1226)'
- en: '[PRE81]'
  id: totrans-668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Parameters
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`auto_class` (`str` or `type`, *optional*, defaults to `"FlaxAutoModel"`) â€”
    The auto class to register this new model with.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto_class` (`str` æˆ– `type`, *å¯é€‰*, é»˜è®¤ä¸º `"FlaxAutoModel"`) â€” ç”¨äºæ³¨å†Œè¿™ä¸ªæ–°æ¨¡å‹çš„è‡ªåŠ¨ç±»ã€‚'
- en: Register this class with a given auto class. This should only be used for custom
    models as the ones in the library are already mapped with an auto class.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç»™å®šçš„è‡ªåŠ¨ç±»æ³¨å†Œæ­¤ç±»ã€‚è¿™åº”è¯¥ä»…ç”¨äºè‡ªå®šä¹‰æ¨¡å‹ï¼Œå› ä¸ºåº“ä¸­çš„æ¨¡å‹å·²ç»ä¸è‡ªåŠ¨ç±»æ˜ å°„ã€‚
- en: This API is experimental and may have some slight breaking changes in the next
    releases.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ API æ˜¯å®éªŒæ€§çš„ï¼Œå¯èƒ½åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æœ‰ä¸€äº›è½»å¾®çš„ç ´åæ€§æ›´æ”¹ã€‚
- en: '#### `save_pretrained`'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1088)'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1088)'
- en: '[PRE82]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Parameters
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory to which to save. Will
    be created if it doesnâ€™t exist.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory` (`str` æˆ– `os.PathLike`) â€” è¦ä¿å­˜åˆ°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºã€‚'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” ä¿å­˜æ¨¡å‹åæ˜¯å¦å°†å…¶æ¨é€åˆ° Hugging Face æ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨
    `repo_id` æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ (å°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„ `save_directory` åç§°)ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) â€” The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º `"10GB"`) â€” åœ¨åˆ†ç‰‡ä¹‹å‰æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚æ£€æŸ¥ç‚¹åˆ†ç‰‡å°†å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½
    (å¦‚ `"5MB"`)ã€‚'
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  id: totrans-680
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹çš„å•ä¸ªæƒé‡å¤§äº `max_shard_size`ï¼Œå®ƒå°†åœ¨è‡ªå·±çš„æ£€æŸ¥ç‚¹åˆ†ç‰‡ä¸­ï¼Œè¯¥åˆ†ç‰‡å°†å¤§äº `max_shard_size`ã€‚
- en: '`token` (`str` or `bool`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str` æˆ– `bool`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œ (å­˜å‚¨åœ¨ `~/.huggingface` ä¸­)ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ä¼ é€’ç»™ [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) â€” Whether to
    save the model using `safetensors` or through msgpack.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨ `safetensors` æˆ–é€šè¿‡ msgpack
    ä¿å­˜æ¨¡å‹ã€‚'
- en: Save a model and its configuration file to a directory, so that it can be re-loaded
    using the `[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)`
    class method
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åŠå…¶é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ `[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)`
    ç±»æ–¹æ³•é‡æ–°åŠ è½½ã€‚
- en: '#### `to_bf16`'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_bf16`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L329)'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L329)'
- en: '[PRE83]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Parameters
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`params` (`Union[Dict, FrozenDict]`) â€” A `PyTree` of model parameters.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params` (`Union[Dict, FrozenDict]`) â€” æ¨¡å‹å‚æ•°çš„ `PyTree`ã€‚'
- en: '`mask` (`Union[Dict, FrozenDict]`) â€” A `PyTree` with same structure as the
    `params` tree. The leaves should be booleans, `True` for params you want to cast,
    and should be `False` for those you want to skip.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask` (`Union[Dict, FrozenDict]`) â€” ä¸ `params` æ ‘å…·æœ‰ç›¸åŒç»“æ„çš„ `PyTree`ã€‚å¶å­åº”ä¸ºå¸ƒå°”å€¼ï¼Œå¯¹äºè¦è½¬æ¢çš„å‚æ•°åº”ä¸º
    `True`ï¼Œå¯¹äºè¦è·³è¿‡çš„å‚æ•°åº”ä¸º `False`ã€‚'
- en: Cast the floating-point `params` to `jax.numpy.bfloat16`. This returns a new
    `params` tree and does not cast the `params` in place.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æµ®ç‚¹ `params` è½¬æ¢ä¸º `jax.numpy.bfloat16`ã€‚è¿™å°†è¿”å›ä¸€ä¸ªæ–°çš„ `params` æ ‘ï¼Œä¸ä¼šç›´æ¥åœ¨åŸåœ°è½¬æ¢ `params`ã€‚
- en: This method can be used on TPU to explicitly convert the model parameters to
    bfloat16 precision to do full half-precision training or to save weights in bfloat16
    for inference in order to save memory and improve speed.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å¯åœ¨ TPU ä¸Šä½¿ç”¨ï¼Œæ˜¾å¼å°†æ¨¡å‹å‚æ•°è½¬æ¢ä¸º bfloat16 ç²¾åº¦ï¼Œä»¥è¿›è¡Œå®Œå…¨çš„åŠç²¾åº¦è®­ç»ƒæˆ–ä»¥ bfloat16 ä¿å­˜æƒé‡ä»¥ç”¨äºæ¨ç†ï¼Œä»¥èŠ‚çœå†…å­˜å¹¶æé«˜é€Ÿåº¦ã€‚
- en: 'Examples:'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE84]'
  id: totrans-694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '#### `to_fp16`'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_fp16`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L395)'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L395)'
- en: '[PRE85]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Parameters
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`params` (`Union[Dict, FrozenDict]`) â€” A `PyTree` of model parameters.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params` (`Union[Dict, FrozenDict]`) â€” æ¨¡å‹å‚æ•°çš„ `PyTree`ã€‚'
- en: '`mask` (`Union[Dict, FrozenDict]`) â€” A `PyTree` with same structure as the
    `params` tree. The leaves should be booleans, `True` for params you want to cast,
    and should be `False` for those you want to skip'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask` (`Union[Dict, FrozenDict]`) â€” ä¸ `params` æ ‘å…·æœ‰ç›¸åŒç»“æ„çš„ `PyTree`ã€‚å¶å­åº”ä¸ºå¸ƒå°”å€¼ï¼Œå¯¹äºè¦è½¬æ¢çš„å‚æ•°åº”ä¸º
    `True`ï¼Œå¯¹äºè¦è·³è¿‡çš„å‚æ•°åº”ä¸º `False`ã€‚'
- en: Cast the floating-point `parmas` to `jax.numpy.float16`. This returns a new
    `params` tree and does not cast the `params` in place.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æµ®ç‚¹ `params` è½¬æ¢ä¸º `jax.numpy.float16`ã€‚è¿™å°†è¿”å›ä¸€ä¸ªæ–°çš„ `params` æ ‘ï¼Œä¸ä¼šç›´æ¥åœ¨åŸåœ°è½¬æ¢ `params`ã€‚
- en: This method can be used on GPU to explicitly convert the model parameters to
    float16 precision to do full half-precision training or to save weights in float16
    for inference in order to save memory and improve speed.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å¯åœ¨ GPU ä¸Šä½¿ç”¨ï¼Œæ˜¾å¼å°†æ¨¡å‹å‚æ•°è½¬æ¢ä¸º float16 ç²¾åº¦ï¼Œä»¥è¿›è¡Œå®Œå…¨çš„åŠç²¾åº¦è®­ç»ƒæˆ–ä»¥ float16 ä¿å­˜æƒé‡ä»¥ç”¨äºæ¨ç†ï¼Œä»¥èŠ‚çœå†…å­˜å¹¶æé«˜é€Ÿåº¦ã€‚
- en: 'Examples:'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE86]'
  id: totrans-704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '#### `to_fp32`'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_fp32`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L368)'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L368)'
- en: '[PRE87]'
  id: totrans-707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Parameters
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`params` (`Union[Dict, FrozenDict]`) â€” A `PyTree` of model parameters.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params` (`Union[Dict, FrozenDict]`) â€” æ¨¡å‹å‚æ•°çš„ `PyTree`ã€‚'
- en: '`mask` (`Union[Dict, FrozenDict]`) â€” A `PyTree` with same structure as the
    `params` tree. The leaves should be booleans, `True` for params you want to cast,
    and should be `False` for those you want to skip'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask` (`Union[Dict, FrozenDict]`) â€” ä¸ `params` æ ‘å…·æœ‰ç›¸åŒç»“æ„çš„ `PyTree`ã€‚å¶å­åº”ä¸ºå¸ƒå°”å€¼ï¼Œå¯¹äºè¦è½¬æ¢çš„å‚æ•°åº”ä¸º
    `True`ï¼Œå¯¹äºè¦è·³è¿‡çš„å‚æ•°åº”ä¸º `False`ã€‚'
- en: Cast the floating-point `parmas` to `jax.numpy.float32`. This method can be
    used to explicitly convert the model parameters to fp32 precision. This returns
    a new `params` tree and does not cast the `params` in place.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æµ®ç‚¹ `params` è½¬æ¢ä¸º `jax.numpy.float32`ã€‚æ­¤æ–¹æ³•å¯ç”¨äºæ˜¾å¼å°†æ¨¡å‹å‚æ•°è½¬æ¢ä¸º fp32 ç²¾åº¦ã€‚è¿™å°†è¿”å›ä¸€ä¸ªæ–°çš„ `params`
    æ ‘ï¼Œä¸ä¼šç›´æ¥åœ¨åŸåœ°è½¬æ¢ `params`ã€‚
- en: 'Examples:'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE88]'
  id: totrans-713
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Pushing to the Hub
  id: totrans-714
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨é€åˆ° Hub
- en: '### `class transformers.utils.PushToHubMixin`'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.utils.PushToHubMixin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L639)'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L639)'
- en: '[PRE89]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: A Mixin containing the functionality to push a model or tokenizer to the hub.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«å°†æ¨¡å‹æˆ–åˆ†è¯å™¨æ¨é€åˆ° Hub çš„åŠŸèƒ½çš„ Mixinã€‚
- en: '#### `push_to_hub`'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
- en: '[PRE90]'
  id: totrans-721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Parameters
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`repo_id` (`str`) â€” The name of the repository you want to push your {object}
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id` (`str`) â€” æ‚¨è¦å°† {object} æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ã€‚åœ¨æ¨é€åˆ°ç»™å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚'
- en: '`use_temp_dir` (`bool`, *optional*) â€” Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨åœ¨æ¨é€åˆ° Hub ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰åä¸º `repo_id`
    çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º `True`ï¼Œå¦åˆ™ä¸º `False`ã€‚'
- en: '`commit_message` (`str`, *optional*) â€” Message to commit while pushing. Will
    default to `"Upload {object}"`.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message` (`str`ï¼Œ*å¯é€‰*) â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º `"Upload {object}"`ã€‚'
- en: '`private` (`bool`, *optional*) â€” Whether or not the repository created should
    be private.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private` (`bool`ï¼Œ*å¯é€‰*) â€” åˆ›å»ºçš„å­˜å‚¨åº“æ˜¯å¦åº”ä¸ºç§æœ‰ã€‚'
- en: '`token` (`bool` or `str`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`bool` æˆ– `str`ï¼Œ*å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface`ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š `repo_url`ï¼Œåˆ™é»˜è®¤ä¸º `True`ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) â€” Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int` æˆ– `str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"5GB"`) â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨è¢«åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åæ£€æŸ¥ç‚¹å°†è¢«åˆ†æˆå°äºæ­¤å¤§å°çš„æ¯ä¸ªåˆ†ç‰‡ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚
    `"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤ä¸º `"5GB"`ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åœ¨å…è´¹çš„ Google Colab å®ä¾‹ä¸Šè½»æ¾åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°ä»»ä½• CPU OOM é—®é¢˜ã€‚'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) â€” Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ä¸Šä¼ æ–‡ä»¶çš„ PR æˆ–ç›´æ¥æäº¤ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦å°†æ¨¡å‹æƒé‡è½¬æ¢ä¸ºsafetensorsæ ¼å¼ä»¥è¿›è¡Œæ›´å®‰å…¨çš„åºåˆ—åŒ–ã€‚'
- en: '`revision` (`str`, *optional*) â€” Branch to push the uploaded files to.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *å¯é€‰*) â€” è¦å°†ä¸Šä¼ çš„æ–‡ä»¶æ¨é€åˆ°çš„åˆ†æ”¯ã€‚'
- en: '`commit_description` (`str`, *optional*) â€” The description of the commit that
    will be created'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_description` (`str`, *å¯é€‰*) â€” å°†åˆ›å»ºçš„æäº¤æè¿°'
- en: '`tags` (`List[str]`, *optional*) â€” List of tags to push on the Hub.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags` (`List[str]`, *å¯é€‰*) â€” è¦æ¨é€åˆ°ä¸­å¿ƒçš„æ ‡ç­¾åˆ—è¡¨ã€‚'
- en: Upload the {object_files} to the ğŸ¤— Model Hub.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: å°†{object_files}ä¸Šä¼ åˆ°ğŸ¤—æ¨¡å‹ä¸­å¿ƒã€‚
- en: 'Examples:'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE91]'
  id: totrans-736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Sharded checkpoints
  id: totrans-737
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡æ£€æŸ¥ç‚¹
- en: '#### `transformers.modeling_utils.load_sharded_checkpoint`'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `transformers.modeling_utils.load_sharded_checkpoint`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L415)'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L415)'
- en: '[PRE92]'
  id: totrans-740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Parameters
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model in which to load the checkpoint.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¦åŠ è½½æ£€æŸ¥ç‚¹çš„æ¨¡å‹ã€‚'
- en: '`folder` (`str` or `os.PathLike`) â€” A path to a folder containing the sharded
    checkpoint.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`folder` (`str`æˆ–`os.PathLike`) â€” åŒ…å«åˆ†ç‰‡æ£€æŸ¥ç‚¹çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚'
- en: '`strict` (`bool`, *optional`, defaults to` True`) â€” Whether to strictly enforce
    that the keys in the model state dict match the keys in the sharded checkpoint.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strict` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä¸¥æ ¼æ‰§è¡Œæ¨¡å‹çŠ¶æ€å­—å…¸ä¸­çš„é”®ä¸åˆ†ç‰‡æ£€æŸ¥ç‚¹ä¸­çš„é”®åŒ¹é…ã€‚'
- en: '`prefer_safe` (`bool`, *optional*, defaults to `False`) â€” If both safetensors
    and PyTorch save files are present in checkpoint and `prefer_safe` is True, the
    safetensors files will be loaded. Otherwise, PyTorch files are always loaded when
    possible.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefer_safe` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” å¦‚æœæ£€æŸ¥ç‚¹ä¸­åŒæ—¶å­˜åœ¨safetensorså’ŒPyTorchä¿å­˜æ–‡ä»¶ï¼Œå¹¶ä¸”`prefer_safe`ä¸ºTrueï¼Œåˆ™å°†åŠ è½½safetensorsæ–‡ä»¶ã€‚å¦åˆ™ï¼Œå°½å¯èƒ½åŠ è½½PyTorchæ–‡ä»¶ã€‚'
- en: Returns
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`NamedTuple`'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: '`NamedTuple`'
- en: A named tuple with `missing_keys` and `unexpected_keys` fields
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸¦æœ‰`missing_keys`å’Œ`unexpected_keys`å­—æ®µçš„å‘½åå…ƒç»„
- en: '`missing_keys` is a list of str containing the missing keys'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`missing_keys`æ˜¯ä¸€ä¸ªåŒ…å«ç¼ºå¤±é”®çš„å­—ç¬¦ä¸²åˆ—è¡¨'
- en: '`unexpected_keys` is a list of str containing the unexpected keys'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unexpected_keys`æ˜¯ä¸€ä¸ªåŒ…å«æ„å¤–é”®çš„å­—ç¬¦ä¸²åˆ—è¡¨'
- en: This is the same as [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)
    but for a sharded checkpoint.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸[`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)ç›¸åŒï¼Œä½†é€‚ç”¨äºåˆ†ç‰‡æ£€æŸ¥ç‚¹ã€‚
- en: 'This load is performed efficiently: each checkpoint shard is loaded one by
    one in RAM and deleted after being loaded in the model.'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŠ è½½æ•ˆç‡å¾ˆé«˜ï¼šæ¯ä¸ªæ£€æŸ¥ç‚¹åˆ†ç‰‡éƒ½ä¼šé€ä¸ªåœ¨RAMä¸­åŠ è½½ï¼ŒåŠ è½½åˆ°æ¨¡å‹åä¼šè¢«åˆ é™¤ã€‚
