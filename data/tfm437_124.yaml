- en: Tokenizer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'A tokenizer is in charge of preparing the inputs for a model. The library contains
    tokenizers for all the models. Most of the tokenizers are available in two flavors:
    a full python implementation and a “Fast” implementation based on the Rust library
    [🤗 Tokenizers](https://github.com/huggingface/tokenizers). The “Fast” implementations
    allows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器负责为模型准备输入。该库包含所有模型的分词器。大多数分词器有两种版本：完整的Python实现和基于Rust库的“快速”实现[🤗 Tokenizers](https://github.com/huggingface/tokenizers)。
    “快速”实现允许：
- en: a significant speed-up in particular when doing batched tokenization and
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特别是在进行批量分词时，可以显著加快速度。
- en: additional methods to map between the original string (character and words)
    and the token space (e.g. getting the index of the token comprising a given character
    or the span of characters corresponding to a given token).
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 额外的方法用于在原始字符串（字符和单词）和标记空间之间进行映射（例如，获取包含给定字符的标记的索引或与给定标记对应的字符范围）。
- en: The base classes [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    and [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    implement the common methods for encoding string inputs in model inputs (see below)
    and instantiating/saving python and “Fast” tokenizers either from a local file
    or directory or from a pretrained tokenizer provided by the library (downloaded
    from HuggingFace’s AWS S3 repository). They both rely on [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)
    that contains the common methods, and [SpecialTokensMixin](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.SpecialTokensMixin).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基类[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)和[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)实现了对模型输入中的字符串输入进行编码的常用方法（见下文），并且可以从本地文件或目录或从库提供的预训练分词器（从HuggingFace的AWS
    S3存储库下载）实例化/保存Python和“快速”分词器。它们都依赖于包含常用方法的[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)，以及[SpecialTokensMixin](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.SpecialTokensMixin)。
- en: '[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    and [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    thus implement the main methods for using all the tokenizers:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)和[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)因此实现了使用所有分词器的主要方法：'
- en: Tokenizing (splitting strings in sub-word token strings), converting tokens
    strings to ids and back, and encoding/decoding (i.e., tokenizing and converting
    to integers).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词（将字符串拆分为子词标记字符串），将标记字符串转换为ID并返回，以及编码/解码（即，分词和转换为整数）。
- en: Adding new tokens to the vocabulary in a way that is independent of the underlying
    structure (BPE, SentencePiece…).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以独立于底层结构（BPE，SentencePiece等）的方式向词汇表中添加新标记。
- en: 'Managing special tokens (like mask, beginning-of-sentence, etc.): adding them,
    assigning them to attributes in the tokenizer for easy access and making sure
    they are not split during tokenization.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理特殊标记（如掩码，句子开头等）：添加它们，将它们分配给分词器中的属性以便轻松访问，并确保它们在分词过程中不被拆分。
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    holds the output of the [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)’s
    encoding methods (`__call__`, `encode_plus` and `batch_encode_plus`) and is derived
    from a Python dictionary. When the tokenizer is a pure python tokenizer, this
    class behaves just like a standard python dictionary and holds the various model
    inputs computed by these methods (`input_ids`, `attention_mask`…). When the tokenizer
    is a “Fast” tokenizer (i.e., backed by HuggingFace [tokenizers library](https://github.com/huggingface/tokenizers)),
    this class provides in addition several advanced alignment methods which can be
    used to map between the original string (character and words) and the token space
    (e.g., getting the index of the token comprising a given character or the span
    of characters corresponding to a given token).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)保存了[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)的编码方法（`__call__`，`encode_plus`和`batch_encode_plus`）的输出，并从Python字典派生。当分词器是纯Python分词器时，此类的行为就像标准Python字典一样，并保存这些方法计算的各种模型输入（`input_ids`，`attention_mask`等）。当分词器是“快速”分词器（即由HuggingFace的[tokenizers库](https://github.com/huggingface/tokenizers)支持）时，此类还提供了几种高级对齐方法，可用于在原始字符串（字符和单词）和标记空间之间进行映射（例如，获取包含给定字符的标记的索引或与给定标记对应的字符范围）。'
- en: PreTrainedTokenizer
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PreTrainedTokenizer
- en: '### `class transformers.PreTrainedTokenizer`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PreTrainedTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L335)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L335)'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model_max_length` (`int`, *optional*) — The maximum length (in number of tokens)
    for the inputs to the transformer model. When the tokenizer is loaded with [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained),
    this will be set to the value stored for the associated model in `max_model_input_sizes`
    (see above). If no value is provided, will default to VERY_LARGE_INTEGER (`int(1e30)`).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_max_length`（`int`，*可选*）— 输入到变换器模型的最大长度（以标记数计）。当使用[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)加载分词器时，这将设置为存储在`max_model_input_sizes`中的相关模型的值（见上文）。如果未提供值，将默认为VERY_LARGE_INTEGER（`int(1e30)`）。'
- en: '`padding_side` (`str`, *optional*) — The side on which the model should have
    padding applied. Should be selected between [‘right’, ‘left’]. Default value is
    picked from the class attribute of the same name.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side`（`str`，*可选*）— 模型应该应用填充的一侧。应该在[‘right’，‘left’]之间选择。默认值从同名的类属性中选择。'
- en: '`truncation_side` (`str`, *optional*) — The side on which the model should
    have truncation applied. Should be selected between [‘right’, ‘left’]. Default
    value is picked from the class attribute of the same name.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side`（`str`，*可选*）— 模型应该应用截断的一侧。应该在[‘right’，‘left’]之间选择。默认值从同名的类属性中选择。'
- en: '`chat_template` (`str`, *optional*) — A Jinja template string that will be
    used to format lists of chat messages. See [https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)
    for a full description.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template`（`str`，*可选*）— 用于格式化聊天消息列表的Jinja模板字符串。查看[https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)获取完整描述。'
- en: '`model_input_names` (`List[string]`, *optional*) — The list of inputs accepted
    by the forward pass of the model (like `"token_type_ids"` or `"attention_mask"`).
    Default value is picked from the class attribute of the same name.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names`（`List[string]`，*可选*）— 模型前向传递接受的输入列表（如`"token_type_ids"`或`"attention_mask"`）。默认值从同名的类属性中选择。'
- en: '`bos_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing the beginning of a sentence. Will be associated to `self.bos_token`
    and `self.bos_token_id`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示句子开头的特殊标记。将与`self.bos_token`和`self.bos_token_id`相关联。'
- en: '`eos_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing the end of a sentence. Will be associated to `self.eos_token` and
    `self.eos_token_id`.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示句子结束的特殊标记。将与`self.eos_token`和`self.eos_token_id`相关联。'
- en: '`unk_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing an out-of-vocabulary token. Will be associated to `self.unk_token`
    and `self.unk_token_id`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示词汇外标记的特殊标记。将与`self.unk_token`和`self.unk_token_id`相关联。'
- en: '`sep_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    separating two different sentences in the same input (used by BERT for instance).
    Will be associated to `self.sep_token` and `self.sep_token_id`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`（`str`或`tokenizers.AddedToken`，*可选*）— 在同一输入中分隔两个不同句子的特殊标记（例如BERT使用）。将与`self.sep_token`和`self.sep_token_id`相关联。'
- en: '`pad_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    used to make arrays of tokens the same size for batching purpose. Will then be
    ignored by attention mechanisms or loss computation. Will be associated to `self.pad_token`
    and `self.pad_token_id`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`或`tokenizers.AddedToken`，*可选*）— 用于使标记数组大小相同以进行批处理的特殊标记。然后将被注意机制或损失计算忽略。将与`self.pad_token`和`self.pad_token_id`相关联。'
- en: '`cls_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing the class of the input (used by BERT for instance). Will be associated
    to `self.cls_token` and `self.cls_token_id`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示输入类别的特殊标记（例如BERT使用）。将与`self.cls_token`和`self.cls_token_id`相关联。'
- en: '`mask_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing a masked token (used by masked-language modeling pretraining objectives,
    like BERT). Will be associated to `self.mask_token` and `self.mask_token_id`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示掩码标记的特殊标记（用于掩码语言建模预训练目标，如BERT）。将与`self.mask_token`和`self.mask_token_id`相关联。'
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) — A tuple or a list of additional special tokens. Add them here to
    ensure they are skipped when decoding with `skip_special_tokens` is set to True.
    If they are not part of the vocabulary, they will be added at the end of the vocabulary.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens`（`str`或`tokenizers.AddedToken`的元组或列表，*可选*）— 附加特殊标记的元组或列表。在此添加它们以确保在设置`skip_special_tokens`为True时解码时跳过它们。如果它们不是词汇的一部分，它们将被添加到词汇的末尾。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `True`) — Whether
    or not the model should cleanup the spaces that were added when splitting the
    input text during the tokenization process.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*，默认为`True`）— 模型是否应清除在标记化过程中拆分输入文本时添加的空格。'
- en: '`split_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the special tokens should be split during the tokenization process. The
    default behavior is to not split special tokens. This means that if `<s>` is the
    `bos_token`, then `tokenizer.tokenize("<s>") = [''<s>`]. Otherwise, if `split_special_tokens=True`,
    then `tokenizer.tokenize("<s>")` will be give `[''<'', ''s'', ''>'']`. This argument
    is only supported for `slow` tokenizers for the moment.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_special_tokens` (`bool`, *optional*, defaults to `False`) — 是否在标记化过程中拆分特殊标记。默认行为是不拆分特殊标记。这意味着如果
    `<s>` 是 `bos_token`，那么 `tokenizer.tokenize("<s>") = [''<s>`]`。否则，如果 `split_special_tokens=True`，那么
    `tokenizer.tokenize("<s>")` 将会给出 `[''<'', ''s'', ''>'']`。此参数目前仅支持`slow`类型的分词器。'
- en: Base class for all slow tokenizers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所有慢分词器的基类。
- en: Inherits from [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 继承自[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)。
- en: Handle all the shared methods for tokenization and special tokens as well as
    methods downloading/caching/loading pretrained tokenizers as well as adding tokens
    to the vocabulary.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 处理所有用于标记化和特殊标记的共享方法，以及用于下载/缓存/加载预训练tokenizer以及向词汇表添加标记的方法。
- en: This class also contain the added tokens in a unified way on top of all tokenizers
    so we don’t have to handle the specific vocabulary augmentation methods of the
    various underlying dictionary structures (BPE, sentencepiece…).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类还以统一的方式包含了所有tokenizer的添加标记，因此我们不必处理各种底层字典结构（BPE、sentencepiece等）的特定词汇增强方法。
- en: Class attributes (overridden by derived classes)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 类属性（由派生类覆盖）
- en: '`vocab_files_names` (`Dict[str, str]`) — A dictionary with, as keys, the `__init__`
    keyword name of each vocabulary file required by the model, and as associated
    values, the filename for saving the associated file (string).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_files_names` (`Dict[str, str]`) — 一个字典，键是模型所需的每个词汇文件的`__init__`关键字名称，值是保存相关文件的文件名（字符串）。'
- en: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) — A dictionary of
    dictionaries, with the high-level keys being the `__init__` keyword name of each
    vocabulary file required by the model, the low-level being the `short-cut-names`
    of the pretrained models with, as associated values, the `url` to the associated
    pretrained vocabulary file.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) — 一个字典的字典，高级键是模型所需的每个词汇文件的`__init__`关键字名称，低级键是预训练模型的`short-cut-names`，值是相关预训练词汇文件的`url`。'
- en: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) — A dictionary with, as
    keys, the `short-cut-names` of the pretrained models, and as associated values,
    the maximum length of the sequence inputs of this model, or `None` if the model
    has no maximum input size.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) — 一个字典，键是预训练模型的`short-cut-names`，值是该模型的序列输入的最大长度，如果模型没有最大输入大小，则为`None`。'
- en: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) — A dictionary
    with, as keys, the `short-cut-names` of the pretrained models, and as associated
    values, a dictionary of specific arguments to pass to the `__init__` method of
    the tokenizer class for this pretrained model when loading the tokenizer with
    the [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
    method.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) — 一个字典，键是预训练模型的`short-cut-names`，值是一个字典，包含加载预训练模型时传递给tokenizer类的`__init__`方法的特定参数。'
- en: '`model_input_names` (`List[str]`) — A list of inputs expected in the forward
    pass of the model.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names` (`List[str]`) — 模型前向传递中预期的输入列表。'
- en: '`padding_side` (`str`) — The default value for the side on which the model
    should have padding applied. Should be `''right''` or `''left''`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side` (`str`) — 模型应用填充的默认方向。应为`''right''`或`''left''`。'
- en: '`truncation_side` (`str`) — The default value for the side on which the model
    should have truncation applied. Should be `''right''` or `''left''`.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side` (`str`) — 模型应用截断的默认方向。应为`''right''`或`''left''`。'
- en: '#### `__call__`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`（`bool`，*可选*，默认为 `True`） — 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens`
    函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加 `bos` 或 `eos` 标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为
    `False`） — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则填充到模型可接受的最大输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：不进行填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，`str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为
    `False`） — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`：截断到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则将逐个标记截断，从序列对中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：截断到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`：截断到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*） — 由截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`（`int`，*可选*，默认为 0） — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True`
    时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words`（`bool`，*可选*，默认为 `False`） — 输入是否已经预先分词（例如，已经分成单词）。如果设置为
    `True`，分词器会假定输入已经分成单词（例如，通过空格分割），然后进行分词。这对于命名实体识别或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*） — 如果设置，将填充序列到提供的值的倍数。需要激活 `padding`。这对于在具有计算能力
    `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）
    — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回Numpy `np.ndarray`对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids`（`bool`，*可选*）— 是否返回token类型ID。如果保持默认设置，将根据特定分词器的默认值返回token类型ID，由`return_outputs`属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token类型ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`（`bool`，*可选*）— 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由`return_outputs`属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens`（`bool`，*可选*，默认为`False`）— 是否返回溢出的标记序列。如果提供一对输入id序列（或一批对）并且`truncation_strategy
    = longest_first`或`True`，则会引发错误，而不是返回溢出的标记。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask`（`bool`，*可选*，默认为`False`）— 是否返回特殊标记掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping`（`bool`，*可选*，默认为`False`）— 是否返回每个标记的`(char_start, char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length`（`bool`，*可选*，默认为`False`）— 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`（`bool`，*可选*，默认为`True`）— 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法'
- en: Returns
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要提供给模型的token id列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要提供给模型的token类型id列表（当`return_token_type_ids=True`或*“token_type_ids”*在`self.model_input_names`中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token类型ID？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些标记应由模型关注的索引列表（当`return_attention_mask=True`或*“attention_mask”*在`self.model_input_names`中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出标记序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 截断的标记数（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。'
- en: '`length` — The length of the inputs (when `return_length=True`)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当`return_length=True`时）'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对一个或多个序列或一个或多个序列对进行分词和准备模型的主要方法。
- en: '#### `add_tokens`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`new_tokens` (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`)
    — Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken`
    wraps a string token to let you personalize its behavior: whether this token should
    only match against a single word, whether this token should strip all potential
    whitespaces on the left side, whether this token should strip all potential whitespaces
    on the right side, etc.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_tokens`（`str`，`tokenizers.AddedToken`或*str*列表或`tokenizers.AddedToken`）—
    仅当它们尚未在词汇表中时才添加标记。`tokenizers.AddedToken`包装一个字符串标记，让您个性化其行为：此标记是否仅匹配单个单词，此标记是否应剥离左侧的所有潜在空格，此标记是否应剥离右侧的所有潜在空格等。'
- en: '`special_tokens` (`bool`, *optional*, defaults to `False`) — Can be used to
    specify if the token is a special token. This mostly change the normalization
    behavior (special tokens like CLS or [MASK] are usually not lower-cased for instance).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens`（`bool`，*可选*，默认为`False`）- 可用于指定标记是否为特殊标记。这主要会改变标准化行为（例如，特殊标记如CLS或[MASK]通常不会被小写）。'
- en: See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在HuggingFace分词器库中查看`tokenizers.AddedToken`的详细信息。
- en: Returns
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到词汇表中的标记数量。
- en: Add a list of new tokens to the tokenizer class. If the new tokens are not in
    the vocabulary, they are added to it with indices starting from length of the
    current vocabulary and and will be isolated before the tokenization algorithm
    is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm
    are therefore not treated in the same way.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 向分词器类添加一组新标记。如果新标记不在词汇表中，则它们将被添加到词汇表中，索引从当前词汇表的长度开始，并且在应用分词算法之前将被隔离。因此，添加的标记和分词算法的词汇表中的标记不会以相同的方式处理。
- en: Note, when adding new tokens to the vocabulary, you should make sure to also
    resize the token embedding matrix of the model so that its embedding matrix matches
    the tokenizer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当向词汇表添加新标记时，您应该确保还调整模型的标记嵌入矩阵，使其嵌入矩阵与分词器匹配。
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，请使用[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)方法。
- en: 'Examples:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#### `add_special_tokens`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`special_tokens_dict` (dictionary *str* to *str* or `tokenizers.AddedToken`)
    — Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`,
    `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_dict`（字典*str*到*str*或`tokenizers.AddedToken`）- 键应该在预定义特殊属性列表中：[`bos_token`、`eos_token`、`unk_token`、`sep_token`、`pad_token`、`cls_token`、`mask_token`、`additional_special_tokens`]。'
- en: Tokens are only added if they are not already in the vocabulary (tested by checking
    if the tokenizer assign the index of the `unk_token` to them).
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当标记尚未在词汇表中时才会添加标记（通过检查分词器是否将`unk_token`的索引分配给它们进行测试）。
- en: '`replace_additional_special_tokens` (`bool`, *optional*,, defaults to `True`)
    — If `True`, the existing list of additional special tokens will be replaced by
    the list provided in `special_tokens_dict`. Otherwise, `self._additional_special_tokens`
    is just extended. In the former case, the tokens will NOT be removed from the
    tokenizer’s full vocabulary - they are only being flagged as non-special tokens.
    Remember, this only affects which tokens are skipped during decoding, not the
    `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous
    `additional_special_tokens` are still added tokens, and will not be split by the
    model.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_additional_special_tokens`（`bool`，*可选*，默认为`True`）- 如果为`True`，则现有的额外特殊标记列表将被替换为`special_tokens_dict`中提供的列表。否则，`self._additional_special_tokens`将只是扩展。在前一种情况下，这些标记不会从分词器的完整词汇表中删除-它们只被标记为非特殊标记。请记住，这只影响解码时跳过哪些标记，而不是`added_tokens_encoder`和`added_tokens_decoder`。这意味着以前的`additional_special_tokens`仍然是添加的标记，并且不会被模型拆分。'
- en: Returns
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到词汇表中的标记数量。
- en: Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and
    link them to class attributes. If special tokens are NOT in the vocabulary, they
    are added to it (indexed starting from the last index of the current vocabulary).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 向编码器添加特殊标记字典（eos、pad、cls等）并将它们链接到类属性。如果特殊标记不在词汇表中，则它们将被添加到词汇表中（索引从当前词汇表的最后一个索引开始）。
- en: When adding new tokens to the vocabulary, you should make sure to also resize
    the token embedding matrix of the model so that its embedding matrix matches the
    tokenizer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在向词汇表添加新标记时，您应该确保还调整模型的标记嵌入矩阵，使其嵌入矩阵与分词器匹配。
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，请使用[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)方法。
- en: 'Using `add_special_tokens` will ensure your special tokens can be used in several
    ways:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`add_special_tokens`将确保您的特殊标记可以以多种方式使用：
- en: Special tokens can be skipped when decoding using `skip_special_tokens = True`.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在解码时可以通过`skip_special_tokens = True`跳过特殊标记。
- en: Special tokens are carefully handled by the tokenizer (they are never split),
    similar to `AddedTokens`.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器会仔细处理特殊标记（它们永远不会被拆分），类似于`AddedTokens`。
- en: You can easily refer to special tokens using tokenizer class attributes like
    `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and
    fine-tuning scripts.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用分词器类属性如`tokenizer.cls_token`轻松引用特殊标记。这样可以轻松开发与模型无关的训练和微调脚本。
- en: When possible, special tokens are already registered for provided pretrained
    models (for instance [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token` is already registered to be :obj*’[CLS]’* and XLM’s one is also registered
    to be `'</s>'`).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，特殊标记已经为提供的预训练模型注册（例如[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token`已经注册为:obj*’[CLS]’*，XLM的一个也已经注册为`'</s>'`）。
- en: 'Examples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `apply_chat_template`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `apply_chat_template`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`conversation` (Union[List[Dict[str, str]], “Conversation”]) — A Conversation
    object or list of dicts with “role” and “content” keys, representing the chat
    history so far.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conversation`（Union[List[Dict[str, str]]，“Conversation”）— 一个Conversation对象或具有“role”和“content”键的字典列表，表示到目前为止的聊天历史。'
- en: '`chat_template` (str, *optional*) — A Jinja template to use for this conversion.
    If this is not passed, the model’s default chat template will be used instead.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template`（str，*可选*）— 用于此转换的Jinja模板。如果未传递此参数，则将使用模型的默认聊天模板。'
- en: '`add_generation_prompt` (bool, *optional*) — Whether to end the prompt with
    the token(s) that indicate the start of an assistant message. This is useful when
    you want to generate a response from the model. Note that this argument will be
    passed to the chat template, and so it must be supported in the template for this
    argument to have any effect.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_generation_prompt`（bool，*可选*）— 是否以指示助手消息开始的标记结束提示。当您想从模型生成响应时，这很有用。请注意，此参数将传递给聊天模板，因此模板必须支持此参数才能产生任何效果。'
- en: '`tokenize` (`bool`, defaults to `True`) — Whether to tokenize the output. If
    `False`, the output will be a string.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize`（`bool`，默认为`True`）— 是否对输出进行标记化。如果为`False`，输出将是一个字符串。'
- en: '`padding` (`bool`, defaults to `False`) — Whether to pad sequences to the maximum
    length. Has no effect if tokenize is `False`.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，默认为`False`）— 是否将序列填充到最大长度。如果tokenize为`False`，则不起作用。'
- en: '`truncation` (`bool`, defaults to `False`) — Whether to truncate sequences
    at the maximum length. Has no effect if tokenize is `False`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，默认为`False`）— 是否在最大长度处截断序列。如果tokenize为`False`，则不起作用。'
- en: '`max_length` (`int`, *optional*) — Maximum length (in tokens) to use for padding
    or truncation. Has no effect if tokenize is `False`. If not specified, the tokenizer’s
    `max_length` attribute will be used as a default.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 用于填充或截断的最大长度（以标记为单位）。如果tokenize为`False`，则不起作用。如果未指定，将使用分词器的`max_length`属性作为默认值。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors of a particular framework. Has no effect
    if tokenize is `False`. Acceptable values are:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）—
    如果设置，将返回特定框架的张量。如果tokenize为`False`，则不起作用。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.Tensor` objects.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回TensorFlow `tf.Tensor`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return NumPy `np.ndarray` objects.'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回NumPy `np.ndarray`对象。'
- en: '`''jax''`: Return JAX `jnp.ndarray` objects. **tokenizer_kwargs — Additional
    kwargs to pass to the tokenizer.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''jax''`：返回JAX `jnp.ndarray`对象。**tokenizer_kwargs — 要传递给分词器的其他kwargs。'
- en: Returns
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: A list of token ids representing the tokenized chat so far, including control
    tokens. This output is ready to pass to the model, either directly or via methods
    like `generate()`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表示到目前为止的标记化聊天的标记id列表，包括控制标记。此输出已准备好传递给模型，可以直接传递或通过`generate()`等方法传递。
- en: Converts a Conversation object or a list of dictionaries with `"role"` and `"content"`
    keys to a list of token ids. This method is intended for use with chat models,
    and will read the tokenizer’s chat_template attribute to determine the format
    and control tokens to use when converting. When chat_template is None, it will
    fall back to the default_chat_template specified at the class level.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将Conversation对象或带有“role”和“content”键的字典列表转换为标记id列表。此方法旨在与聊天模型一起使用，并将读取分词器的chat_template属性以确定在转换时要使用的格式和控制标记。当chat_template为None时，将退回到类级别指定的default_chat_template。
- en: '#### `batch_decode`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（`Union[List[int]，List[List[int]]，np.ndarray，torch.Tensor，tf.Tensor]`）—
    标记化输入id的列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否在解码中删除特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*）— 是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。'
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（其他关键字参数，*可选*）— 将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[str]`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of decoded sentences.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的句子列表。
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用解码将标记id的列表列表转换为字符串列表。
- en: '#### `decode`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`（`Union[int，List[int]，np.ndarray，torch.Tensor，tf.Tensor]`）— 标记化输入id的列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否在解码中删除特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*）— 是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。'
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（其他关键字参数，*可选*）— 将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`str`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`'
- en: The decoded sentence.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的句子。
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标记器和词汇表将 id 序列转换为字符串，具有删除特殊标记和清理标记化空格的选项。
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于执行 `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。
- en: '#### `encode`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
- en: '[PRE9]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]` or `List[int]`) — The first sequence to be encoded.
    This can be a string, a list of strings (tokenized string using the `tokenize`
    method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`
    method).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`（`str`，`List[str]` 或 `List[int]`）— 要编码的第一个序列。可以是字符串，字符串列表（使用 `tokenize`
    方法进行标记化的字符串）或整数列表（使用 `convert_tokens_to_ids` 方法进行标记化的字符串 id）。'
- en: '`text_pair` (`str`, `List[str]` or `List[int]`, *optional*) — Optional second
    sequence to be encoded. This can be a string, a list of strings (tokenized string
    using the `tokenize` method) or a list of integers (tokenized string ids using
    the `convert_tokens_to_ids` method).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`（`str`，`List[str]` 或 `List[int]`，*可选*）— 要编码的可选第二个序列。可以是字符串，字符串列表（使用
    `tokenize` 方法进行标记化的字符串）或整数列表（使用 `convert_tokens_to_ids` 方法进行标记化的字符串 id）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`（`bool`，*可选*，默认为 `True`）— 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens`
    函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加 `bos` 或 `eos` 标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为
    `False`）— 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：不进行填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，`str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为
    `False`）— 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`：截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则会逐标记截断，从一对序列中最长的序列中移除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`：截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不进行截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则如果截断/填充参数之一需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`（`int`，*可选*，默认为0）— 如果设置为数字，并且`max_length`一起设置，当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words`（`bool`，*可选*，默认为`False`）— 输入是否已经预分词（例如，已经分成单词）。如果设置为`True`，则分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将填充序列到提供的值的倍数。需要激活`padding`。这对于启用具有计算能力`>=
    7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）—
    如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回Numpy `np.ndarray`对象。'
- en: '**kwargs — Passed along to the `.tokenize()` method.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kwargs — 传递给`.tokenize()`方法。'
- en: Returns
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`，`torch.Tensor`，`tf.Tensor`或`np.ndarray`'
- en: The tokenized ids of the text.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的标记化id。
- en: Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分词器和词汇表将字符串转换为id（整数）序列。
- en: Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与执行`self.convert_tokens_to_ids(self.tokenize(text))`相同。
- en: '#### `push_to_hub`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
- en: '[PRE10]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`repo_id` (`str`) — The name of the repository you want to push your tokenizer
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id`（`str`）— 您要将分词器推送到的存储库名称。在推送到给定组织时，应包含您的组织名称。'
- en: '`use_temp_dir` (`bool`, *optional*) — Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir`（`bool`，*可选*）— 是否使用临时目录存储在推送到Hub之前保存的文件。如果没有名为`repo_id`的目录，则默认为`True`，否则为`False`。'
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload tokenizer"`.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message`（`str`，*可选*）— 推送时要提交的消息。默认为`"Upload tokenizer"`。'
- en: '`private` (`bool`, *optional*) — Whether or not the repository created should
    be private.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private`（`bool`，*可选*）— 创建的存储库是否应为私有。'
- en: '`token` (`bool` or `str`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`（`bool`或`str`，*可选*）— 用作远程文件的HTTP令牌。如果为`True`，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。如果未指定`repo_url`，则默认为`True`。'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) — Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size`（`int`或`str`，*可选*，默认为`"5GB"`）— 仅适用于模型。在分片之前的检查点的最大大小。然后，检查点分片将每个大小低于此大小。如果表示为字符串，需要是数字后跟一个单位（如`"5MB"`）。我们将其默认设置为`"5GB"`，以便用户可以在免费的Google
    Colab实例上轻松加载模型，而不会出现任何CPU OOM问题。'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr`（`bool`，*可选*，默认为`False`）— 是否创建具有上传文件的PR或直接提交。'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`（`bool`，*可选*，默认为`True`）— 是否将模型权重转换为safetensors格式以进行更安全的序列化。'
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision`（`str`，*可选*）— 要将上传的文件推送到的分支。'
- en: '`commit_description` (`str`, *optional*) — The description of the commit that
    will be created'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_description`（`str`，*可选*）— 将要创建的提交的描述'
- en: '`tags` (`List[str]`, *optional*) — List of tags to push on the Hub.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`（`List[str]`，*可选*）— 要推送到Hub上的标签列表。'
- en: Upload the tokenizer files to the 🤗 Model Hub.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 将分词器文件上传到🤗模型Hub。
- en: 'Examples:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `convert_ids_to_tokens`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_ids_to_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L953)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L953)'
- en: '[PRE12]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`ids` (`int` or `List[int]`) — The token id (or token ids) to convert to tokens.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ids`（`int`或`List[int]`）— 要转换为标记的标记id（或标记id）。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否在解码中删除特殊标记。'
- en: Returns
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`str` or `List[str]`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`str` 或 `List[str]`'
- en: The decoded token(s).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的标记。
- en: Converts a single index or a sequence of indices in a token or a sequence of
    tokens, using the vocabulary and added tokens.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词汇表和添加的标记将单个索引或索引序列转换为标记或标记序列。
- en: '#### `convert_tokens_to_ids`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L630)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L630)'
- en: '[PRE13]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokens` (`str` or `List[str]`) — One or several token(s) to convert to token
    id(s).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokens` (`str` 或 `List[str]`) — 要转换为标记 ID 的一个或多个标记。'
- en: Returns
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int` or `List[int]`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`int` 或 `List[int]`'
- en: The token id or list of token ids.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 标记 ID 或标记 ID 列表。
- en: Converts a token string (or a sequence of tokens) in a single integer id (or
    a sequence of ids), using the vocabulary.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 将标记字符串（或标记序列）转换为单个整数 ID（或 ID 序列），使用词汇表。
- en: '#### `get_added_vocab`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_added_vocab`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L415)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L415)'
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Returns
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict[str, int]`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, int]`'
- en: The added tokens.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的标记。
- en: Returns the added tokens in the vocabulary as a dictionary of token to index.
    Results might be different from the fast call because for now we always add the
    tokens even if they are already in the vocabulary. This is something we should
    change.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 将词汇表中的添加标记作为标记到索引的字典返回。结果可能与快速调用不同，因为现在我们总是添加标记，即使它们已经在词汇表中。这是我们应该更改的事情。
- en: '#### `num_special_tokens_to_add`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_special_tokens_to_add`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L518)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L518)'
- en: '[PRE15]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pair` (`bool`, *optional*, defaults to `False`) — Whether the number of added
    tokens should be computed in the case of a sequence pair or a single sequence.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pair` (`bool`, *可选*, 默认为 `False`) — 在序列对或单个序列的情况下是否应计算添加的标记数。'
- en: Returns
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of special tokens added to sequences.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到序列中的特殊标记数。
- en: Returns the number of added tokens when encoding a sequence with special tokens.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 返回使用特殊标记编码序列时添加的标记数。
- en: This encodes a dummy input and checks the number of added tokens, and is therefore
    not efficient. Do not put this inside your training loop.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这会对一个虚拟输入进行编码并检查添加的标记数量，因此效率不高。不要将此放在训练循环内。
- en: '#### `prepare_for_tokenization`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_for_tokenization`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L891)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L891)'
- en: '[PRE16]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`) — The text to prepare.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`) — 要准备的文本。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预先标记化（例如，已分割为单词）。如果设置为
    `True`，分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行标记化。这对于 NER 或标记分类很有用。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Keyword arguments to use for the
    tokenization.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 用于标记化的关键字参数。'
- en: Returns
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Tuple[str, Dict[str, Any]]`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuple[str, Dict[str, Any]]`'
- en: The prepared text and the unused kwargs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好的文本和未使用的 kwargs。
- en: Performs any necessary transformations before tokenization.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化之前执行任何必要的转换。
- en: This method should pop the arguments from kwargs and return the remaining `kwargs`
    as well. We test the `kwargs` at the end of the encoding process to be sure all
    the arguments have been used.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法应该从 kwargs 中弹出参数并返回剩余的 `kwargs`。我们在编码过程结束时测试 `kwargs`，以确保所有参数都已使用。
- en: '#### `tokenize`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tokenize`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L541)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L541)'
- en: '[PRE17]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`) — The sequence to be encoded.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`) — 要编码的序列。'
- en: '*`*kwargs` (additional keyword arguments) — Passed along to the model-specific
    `prepare_for_tokenization` preprocessing method.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*kwargs`（额外的关键字参数） — 传递给特定于模型的 `prepare_for_tokenization` 预处理方法。'
- en: Returns
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[str]`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of tokens.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 标记列表。
- en: Converts a string into a sequence of tokens, using the tokenizer.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分词器将字符串转换为标记序列。
- en: Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies
    (BPE/SentencePieces/WordPieces). Takes care of added tokens.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 按词汇表中的单词或子词（BPE/SentencePieces/WordPieces）拆分。处理添加的标记。
- en: PreTrainedTokenizerFast
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PreTrainedTokenizerFast
- en: The [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    depend on the [tokenizers](https://huggingface.co/docs/tokenizers) library. The
    tokenizers obtained from the 🤗 tokenizers library can be loaded very simply into
    🤗 transformers. Take a look at the [Using tokenizers from 🤗 tokenizers](../fast_tokenizers)
    page to understand how this is done.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    依赖于 [tokenizers](https://huggingface.co/docs/tokenizers) 库。从 🤗 tokenizers 库获取的
    tokenizers 可以非常简单地加载到 🤗 transformers 中。查看 [Using tokenizers from 🤗 tokenizers](../fast_tokenizers)
    页面以了解如何执行此操作。'
- en: '### `class transformers.PreTrainedTokenizerFast`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PreTrainedTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L77)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L77)'
- en: '[PRE18]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model_max_length` (`int`, *optional*) — The maximum length (in number of tokens)
    for the inputs to the transformer model. When the tokenizer is loaded with [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained),
    this will be set to the value stored for the associated model in `max_model_input_sizes`
    (see above). If no value is provided, will default to VERY_LARGE_INTEGER (`int(1e30)`).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_max_length` (`int`, *optional*) — 输入到变换器模型的最大长度（以标记数计）。当使用 [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
    加载分词器时，这将设置为存储在 `max_model_input_sizes` 中关联模型的值（见上文）。如果未提供值，将默认为 VERY_LARGE_INTEGER
    (`int(1e30)`）。'
- en: '`padding_side` (`str`, *optional*) — The side on which the model should have
    padding applied. Should be selected between [‘right’, ‘left’]. Default value is
    picked from the class attribute of the same name.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side` (`str`, *optional*) — 模型应该在哪一侧应用填充。应该在 [''right'', ''left'']
    中选择。默认值从同名类属性中选择。'
- en: '`truncation_side` (`str`, *optional*) — The side on which the model should
    have truncation applied. Should be selected between [‘right’, ‘left’]. Default
    value is picked from the class attribute of the same name.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side` (`str`, *optional*) — 模型应该在哪一侧应用截断。应该在 [''right'', ''left'']
    中选择。默认值从同名类属性中选择。'
- en: '`chat_template` (`str`, *optional*) — A Jinja template string that will be
    used to format lists of chat messages. See [https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)
    for a full description.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template` (`str`, *optional*) — 一个 Jinja 模板字符串，用于格式化聊天消息列表。详细描述请参阅 [https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)。'
- en: '`model_input_names` (`List[string]`, *optional*) — The list of inputs accepted
    by the forward pass of the model (like `"token_type_ids"` or `"attention_mask"`).
    Default value is picked from the class attribute of the same name.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names` (`List[string]`, *optional*) — 模型前向传递接受的输入列表（如 `"token_type_ids"`
    或 `"attention_mask"`）。默认值从同名类属性中选择。'
- en: '`bos_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing the beginning of a sentence. Will be associated to `self.bos_token`
    and `self.bos_token_id`.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示句子开头的特殊标记。将与
    `self.bos_token` 和 `self.bos_token_id` 关联。'
- en: '`eos_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing the end of a sentence. Will be associated to `self.eos_token` and
    `self.eos_token_id`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示句子结尾的特殊标记。将与
    `self.eos_token` 和 `self.eos_token_id` 关联。'
- en: '`unk_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing an out-of-vocabulary token. Will be associated to `self.unk_token`
    and `self.unk_token_id`.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示词汇外标记的特殊标记。将与
    `self.unk_token` 和 `self.unk_token_id` 关联。'
- en: '`sep_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    separating two different sentences in the same input (used by BERT for instance).
    Will be associated to `self.sep_token` and `self.sep_token_id`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 用于在同一输入中分隔两个不同句子的特殊标记（例如
    BERT 使用）。将与 `self.sep_token` 和 `self.sep_token_id` 关联。'
- en: '`pad_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    used to make arrays of tokens the same size for batching purpose. Will then be
    ignored by attention mechanisms or loss computation. Will be associated to `self.pad_token`
    and `self.pad_token_id`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 用于使标记数组大小相同以进行批处理的特殊标记。然后将被注意机制或损失计算忽略。将与
    `self.pad_token` 和 `self.pad_token_id` 关联。'
- en: '`cls_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing the class of the input (used by BERT for instance). Will be associated
    to `self.cls_token` and `self.cls_token_id`.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示输入类别的特殊标记（例如
    BERT 使用）。将与 `self.cls_token` 和 `self.cls_token_id` 关联。'
- en: '`mask_token` (`str` or `tokenizers.AddedToken`, *optional*) — A special token
    representing a masked token (used by masked-language modeling pretraining objectives,
    like BERT). Will be associated to `self.mask_token` and `self.mask_token_id`.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示掩码标记的特殊标记（用于掩码语言建模预训练目标，如
    BERT）。将与 `self.mask_token` 和 `self.mask_token_id` 关联。'
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) — A tuple or a list of additional special tokens. Add them here to
    ensure they are skipped when decoding with `skip_special_tokens` is set to True.
    If they are not part of the vocabulary, they will be added at the end of the vocabulary.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (元组或列表，包含 `str` 或 `tokenizers.AddedToken`, *optional*)
    — 附加特殊标记的元组或列表。在这里添加它们以确保在 `skip_special_tokens` 设置为 True 时解码时跳过它们。如果它们不是词汇的一部分，它们将被添加到词汇的末尾。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `True`) — Whether
    or not the model should cleanup the spaces that were added when splitting the
    input text during the tokenization process.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`, *optional*, 默认为 `True`) — 模型是否应该清除在标记化过程中拆分输入文本时添加的空格。'
- en: '`split_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the special tokens should be split during the tokenization process. The
    default behavior is to not split special tokens. This means that if `<s>` is the
    `bos_token`, then `tokenizer.tokenize("<s>") = [''<s>`]. Otherwise, if `split_special_tokens=True`,
    then `tokenizer.tokenize("<s>")` will be give `[''<'', ''s'', ''>'']`. This argument
    is only supported for `slow` tokenizers for the moment.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_special_tokens` (`bool`, *optional*, 默认为 `False`) — 在标记化过程中是否应该拆分特殊标记。默认行为是不拆分特殊标记。这意味着如果
    `<s>` 是 `bos_token`，那么 `tokenizer.tokenize("<s>") = [''<s>`]。否则，如果 `split_special_tokens=True`，那么
    `tokenizer.tokenize("<s>")` 将会给出 `[''<'', ''s'', ''>'']`。此参数目前仅支持 `slow` tokenizers。'
- en: '`tokenizer_object` (`tokenizers.Tokenizer`) — A `tokenizers.Tokenizer` object
    from 🤗 tokenizers to instantiate from. See [Using tokenizers from 🤗 tokenizers](../fast_tokenizers)
    for more information.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_object` (`tokenizers.Tokenizer`) — 一个来自🤗 tokenizers的`tokenizers.Tokenizer`对象，用于实例化。更多信息请参阅[使用🤗
    tokenizers](../fast_tokenizers)。'
- en: '`tokenizer_file` (`str`) — A path to a local JSON file representing a previously
    serialized `tokenizers.Tokenizer` object from 🤗 tokenizers.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file` (`str`) — 一个指向本地JSON文件的路径，表示以前序列化的`tokenizers.Tokenizer`对象。'
- en: Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 所有快速分词器的基类（包装HuggingFace分词器库）。
- en: Inherits from [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 继承自[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)。
- en: Handles all the shared methods for tokenization and special tokens, as well
    as methods for downloading/caching/loading pretrained tokenizers, as well as adding
    tokens to the vocabulary.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 处理所有分词和特殊标记的共享方法，以及用于下载/缓存/加载预训练分词器的方法，以及向词汇表添加标记。
- en: This class also contains the added tokens in a unified way on top of all tokenizers
    so we don’t have to handle the specific vocabulary augmentation methods of the
    various underlying dictionary structures (BPE, sentencepiece…).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类还以统一的方式包含了所有分词器的添加标记，因此我们不必处理各种底层字典结构（BPE、sentencepiece等）的特定词汇增强方法。
- en: Class attributes (overridden by derived classes)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 类属性（派生类覆盖）
- en: '`vocab_files_names` (`Dict[str, str]`) — A dictionary with, as keys, the `__init__`
    keyword name of each vocabulary file required by the model, and as associated
    values, the filename for saving the associated file (string).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_files_names` (`Dict[str, str]`) — 一个字典，键为模型所需的每个词汇文件的`__init__`关键字名称，相关值为保存关联文件的文件名（字符串）。'
- en: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) — A dictionary of
    dictionaries, with the high-level keys being the `__init__` keyword name of each
    vocabulary file required by the model, the low-level being the `short-cut-names`
    of the pretrained models with, as associated values, the `url` to the associated
    pretrained vocabulary file.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) — 一个字典，高级键是模型所需的每个词汇文件的`__init__`关键字名称，低级键是预训练模型的`short-cut-names`，相关值是关联的预训练词汇文件的`url`。'
- en: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) — A dictionary with, as
    keys, the `short-cut-names` of the pretrained models, and as associated values,
    the maximum length of the sequence inputs of this model, or `None` if the model
    has no maximum input size.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) — 一个字典，键为预训练模型的`short-cut-names`，相关值为该模型的序列输入的最大长度，如果模型没有最大输入大小，则为`None`。'
- en: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) — A dictionary
    with, as keys, the `short-cut-names` of the pretrained models, and as associated
    values, a dictionary of specific arguments to pass to the `__init__` method of
    the tokenizer class for this pretrained model when loading the tokenizer with
    the [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
    method.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) — 一个字典，键为预训练模型的`short-cut-names`，相关值为传递给加载预训练模型时tokenizer类的`__init__`方法的特定参数字典，使用[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)方法。'
- en: '`model_input_names` (`List[str]`) — A list of inputs expected in the forward
    pass of the model.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names` (`List[str]`) — 模型前向传递中期望的输入列表。'
- en: '`padding_side` (`str`) — The default value for the side on which the model
    should have padding applied. Should be `''right''` or `''left''`.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side` (`str`) — 模型应用填充的默认方向。应为`''right''`或`''left''`。'
- en: '`truncation_side` (`str`) — The default value for the side on which the model
    should have truncation applied. Should be `''right''` or `''left''`.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side` (`str`) — 模型应用截断的默认方向。应为`''right''`或`''left''`。'
- en: '#### `__call__`'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE19]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *可选*, 默认为`True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为`False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（或如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：无填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *可选*, 默认为`False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则将逐标记截断，从中删除最长序列中的一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：无截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为`None`，则如果截断/填充参数需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *可选*, 默认为0) — 如果与`max_length`一起设置为一个数字，当`return_overflowing_tokens=True`时返回的溢出标记将包含被截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *可选*, 默认为`False`) — 输入是否已经预分词（例如，已分割为单词）。如果设置为`True`，则分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行分词。这对于命名实体识别或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于在具有计算能力`>=
    7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回Numpy `np.ndarray`对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *可选*) — 是否返回token类型ID。如果保持默认设置，将根据特定分词器的默认设置返回token类型ID，由`return_outputs`属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token类型ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认设置返回注意力掩码，由`return_outputs`属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *可选*, 默认为`False`) — 是否返回溢出的token序列。如果提供一对输入id序列（或一批对）并且`truncation_strategy
    = longest_first`或`True`，则会引发错误，而不是返回溢出的token。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *可选*, 默认为`False`) — 是否返回特殊token掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *可选*, 默认为`False`) — 是否返回每个token的`(char_start,
    char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *可选*, 默认为`False`) — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *可选*, 默认为`True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法'
- en: Returns
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要馈送给模型的token id列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要馈送给模型的token类型id列表（当`return_token_type_ids=True`或*“token_type_ids”*在`self.model_input_names`中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token类型ID？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些token应该被模型关注的索引列表（当`return_attention_mask=True`或*“attention_mask”*在`self.model_input_names`中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出token序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 截断的token数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊token，0指定常规序列token（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。'
- en: '`length` — The length of the inputs (when `return_length=True`)'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当`return_length=True`时）'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 将主要方法标记化并为模型准备一个或多个序列或一个或多个序列对。
- en: '#### `add_tokens`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
- en: '[PRE20]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`new_tokens` (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`)
    — Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken`
    wraps a string token to let you personalize its behavior: whether this token should
    only match against a single word, whether this token should strip all potential
    whitespaces on the left side, whether this token should strip all potential whitespaces
    on the right side, etc.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_tokens` (`str`, `tokenizers.AddedToken`或*str*列表或`tokenizers.AddedToken`)
    — 仅当这些标记尚未在词汇表中时才会添加这些标记。`tokenizers.AddedToken`将字符串标记包装起来，以便您可以个性化其行为：这个标记是否只匹配单个单词，这个标记是否应该去除左侧的所有潜在空格，这个标记是否应该去除右侧的所有潜在空格等。'
- en: '`special_tokens` (`bool`, *optional*, defaults to `False`) — Can be used to
    specify if the token is a special token. This mostly change the normalization
    behavior (special tokens like CLS or [MASK] are usually not lower-cased for instance).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens` (`bool`, *可选*, 默认为 `False`) — 可用于指定该标记是否为特殊标记。这主要会改变标准化行为（例如，特殊标记如CLS或[MASK]通常不会被转换为小写）。'
- en: See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在HuggingFace分词器库中查看`tokenizers.AddedToken`的详细信息。
- en: Returns
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到词汇表中的标记数。
- en: Add a list of new tokens to the tokenizer class. If the new tokens are not in
    the vocabulary, they are added to it with indices starting from length of the
    current vocabulary and and will be isolated before the tokenization algorithm
    is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm
    are therefore not treated in the same way.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 向分词器类添加新标记列表。如果新标记不在词汇表中，则它们将被添加到其中，索引从当前词汇表的长度开始，并且在应用分词算法之前将被隔离。因此，添加的标记和分词算法的词汇表中的标记不会以相同的方式处理。
- en: Note, when adding new tokens to the vocabulary, you should make sure to also
    resize the token embedding matrix of the model so that its embedding matrix matches
    the tokenizer.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当向词汇表添加新标记时，您应确保还调整模型的标记嵌入矩阵大小，以使其嵌入矩阵与分词器匹配。
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，请使用[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)方法。
- en: 'Examples:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE21]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#### `add_special_tokens`'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
- en: '[PRE22]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`special_tokens_dict` (dictionary *str* to *str* or `tokenizers.AddedToken`)
    — Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`,
    `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_dict`（字典*str*到*str*或`tokenizers.AddedToken`） — 键应在预定义特殊属性列表中：[`bos_token`,
    `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,
    `additional_special_tokens`]。'
- en: Tokens are only added if they are not already in the vocabulary (tested by checking
    if the tokenizer assign the index of the `unk_token` to them).
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅当这些标记尚未在词汇表中时才会添加这些标记（通过检查分词器是否将`unk_token`的索引分配给它们进行测试）。
- en: '`replace_additional_special_tokens` (`bool`, *optional*,, defaults to `True`)
    — If `True`, the existing list of additional special tokens will be replaced by
    the list provided in `special_tokens_dict`. Otherwise, `self._additional_special_tokens`
    is just extended. In the former case, the tokens will NOT be removed from the
    tokenizer’s full vocabulary - they are only being flagged as non-special tokens.
    Remember, this only affects which tokens are skipped during decoding, not the
    `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous
    `additional_special_tokens` are still added tokens, and will not be split by the
    model.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_additional_special_tokens` (`bool`, *可选*, 默认为 `True`) — 如果为`True`，则现有的额外特殊标记列表将被`special_tokens_dict`中提供的列表替换。否则，`self._additional_special_tokens`将仅被扩展。在前一种情况下，这些标记将不会从分词器的完整词汇表中删除
    - 它们只被标记为非特殊标记。请记住，这仅影响解码过程中跳过哪些标记，而不影响`added_tokens_encoder`和`added_tokens_decoder`。这意味着以前的`additional_special_tokens`仍然是添加的标记，并且不会被模型拆分。'
- en: Returns
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到词汇表中的标记数。
- en: Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and
    link them to class attributes. If special tokens are NOT in the vocabulary, they
    are added to it (indexed starting from the last index of the current vocabulary).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 向编码器添加特殊标记字典（eos，pad，cls等）并将它们链接到类属性。如果特殊标记不在词汇表中，则它们将被添加到其中（索引从当前词汇表的最后一个索引开始）。
- en: When adding new tokens to the vocabulary, you should make sure to also resize
    the token embedding matrix of the model so that its embedding matrix matches the
    tokenizer.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 当向词汇表添加新标记时，您应确保还调整模型的标记嵌入矩阵大小，以使其嵌入矩阵与分词器匹配。
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，请使用[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)方法。
- en: 'Using `add_special_tokens` will ensure your special tokens can be used in several
    ways:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`add_special_tokens`将确保您的特殊标记可以以多种方式使用：
- en: Special tokens can be skipped when decoding using `skip_special_tokens = True`.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码时可以通过`skip_special_tokens = True`跳过特殊标记。
- en: Special tokens are carefully handled by the tokenizer (they are never split),
    similar to `AddedTokens`.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊标记由分词器仔细处理（它们永远不会被拆分），类似于`AddedTokens`。
- en: You can easily refer to special tokens using tokenizer class attributes like
    `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and
    fine-tuning scripts.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过分词器类属性（如`tokenizer.cls_token`）轻松引用特殊标记。这使得开发与模型无关的训练和微调脚本变得容易。
- en: When possible, special tokens are already registered for provided pretrained
    models (for instance [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token` is already registered to be :obj*’[CLS]’* and XLM’s one is also registered
    to be `'</s>'`).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，已经为提供的预训练模型注册了特殊标记（例如[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token`已经注册为：obj*’[CLS]’*，XLM的一个也已经注册为`'</s>'`）。
- en: 'Examples:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE23]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#### `apply_chat_template`'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `apply_chat_template`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
- en: '[PRE24]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`conversation` (Union[List[Dict[str, str]], “Conversation”]) — A Conversation
    object or list of dicts with “role” and “content” keys, representing the chat
    history so far.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conversation`（Union[List[Dict[str, str]], “Conversation”）— 一个Conversation对象或带有“role”和“content”键的字典列表，表示到目前为止的聊天历史。'
- en: '`chat_template` (str, *optional*) — A Jinja template to use for this conversion.
    If this is not passed, the model’s default chat template will be used instead.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template`（str，*可选*）— 用于此转换的Jinja模板。如果未传递此参数，则将使用模型的默认聊天模板。'
- en: '`add_generation_prompt` (bool, *optional*) — Whether to end the prompt with
    the token(s) that indicate the start of an assistant message. This is useful when
    you want to generate a response from the model. Note that this argument will be
    passed to the chat template, and so it must be supported in the template for this
    argument to have any effect.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_generation_prompt`（bool，*可选*）— 是否以指示助手消息开始的标记结束提示。当您想要从模型生成响应时，这很有用。请注意，此参数将传递给聊天模板，因此模板必须支持此参数才能产生任何效果。'
- en: '`tokenize` (`bool`, defaults to `True`) — Whether to tokenize the output. If
    `False`, the output will be a string.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize`（`bool`，默认为`True`）— 是否对输出进行分词。如果为`False`，输出将是一个字符串。'
- en: '`padding` (`bool`, defaults to `False`) — Whether to pad sequences to the maximum
    length. Has no effect if tokenize is `False`.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，默认为`False`）— 是否将序列填充到最大长度。如果tokenize为`False`，则无效。'
- en: '`truncation` (`bool`, defaults to `False`) — Whether to truncate sequences
    at the maximum length. Has no effect if tokenize is `False`.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，默认为`False`）— 是否在最大长度处截断序列。如果tokenize为`False`，则无效。'
- en: '`max_length` (`int`, *optional*) — Maximum length (in tokens) to use for padding
    or truncation. Has no effect if tokenize is `False`. If not specified, the tokenizer’s
    `max_length` attribute will be used as a default.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 用于填充或截断的最大长度（以标记为单位）。如果tokenize为`False`，则无效。如果未指定，将使用分词器的`max_length`属性作为默认值。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors of a particular framework. Has no effect
    if tokenize is `False`. Acceptable values are:'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）—
    如果设置，将返回特定框架的张量。如果tokenize为`False`，则无效。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.Tensor` objects.'
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回TensorFlow `tf.Tensor`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return NumPy `np.ndarray` objects.'
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回NumPy `np.ndarray`对象。'
- en: '`''jax''`: Return JAX `jnp.ndarray` objects. **tokenizer_kwargs — Additional
    kwargs to pass to the tokenizer.'
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''jax''`：返回JAX `jnp.ndarray`对象。**tokenizer_kwargs — 传递给分词器的其他kwargs。'
- en: Returns
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: A list of token ids representing the tokenized chat so far, including control
    tokens. This output is ready to pass to the model, either directly or via methods
    like `generate()`.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 表示到目前为止标记化聊天的标记id列表，包括控制标记。此输出已准备好传递给模型，可以直接传递，也可以通过`generate()`等方法传递。
- en: Converts a Conversation object or a list of dictionaries with `"role"` and `"content"`
    keys to a list of token ids. This method is intended for use with chat models,
    and will read the tokenizer’s chat_template attribute to determine the format
    and control tokens to use when converting. When chat_template is None, it will
    fall back to the default_chat_template specified at the class level.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 将Conversation对象或带有“role”和“content”键的字典列表转换为标记id列表。此方法旨在与聊天模型一起使用，并将读取分词器的chat_template属性以确定在转换时要使用的格式和控制标记。当chat_template为None时，将退回到类级别指定的default_chat_template。
- en: '#### `batch_decode`'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
- en: '[PRE25]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`）—
    标记化输入id的列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否删除解码中的特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*）— 是否清除分词空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。'
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（其他关键字参数，*可选*）— 将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[str]`'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of decoded sentences.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 解码的句子列表。
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用decode将标记id的列表列表转换为字符串列表。
- en: '#### `decode`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
- en: '[PRE26]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`（`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`）—
    标记化输入id的列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`）—在解码时是否删除特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`（`bool`，*可选*）—是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。'
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（附加关键字参数，*可选*）—将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`str`'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`'
- en: The decoded sentence.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的句子。
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列id转换为字符串，使用标记器和词汇表，可以选择删除特殊标记并清理标记化空格。
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于执行`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。
- en: '#### `encode`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
- en: '[PRE27]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]` or `List[int]`) — The first sequence to be encoded.
    This can be a string, a list of strings (tokenized string using the `tokenize`
    method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`
    method).'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`（`str`，`List[str]`或`List[int]`）—要编码的第一个序列。这可以是一个字符串，一个字符串列表（使用`tokenize`方法进行标记化的字符串）或一个整数列表（使用`convert_tokens_to_ids`方法进行标记化的字符串id）。'
- en: '`text_pair` (`str`, `List[str]` or `List[int]`, *optional*) — Optional second
    sequence to be encoded. This can be a string, a list of strings (tokenized string
    using the `tokenize` method) or a list of integers (tokenized string ids using
    the `convert_tokens_to_ids` method).'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`（`str`，`List[str]`或`List[int]`，*可选*）—要编码的可选第二个序列。这可以是一个字符串，一个字符串列表（使用`tokenize`方法进行标记化的字符串）或一个整数列表（使用`convert_tokens_to_ids`方法进行标记化的字符串id）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`（`bool`，*可选*，默认为`True`）—在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）—激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest''`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到由参数`max_length`指定的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_pad''`（默认）：不填充（即，可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`）—激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest_first''`：截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则将逐标记截断，从一对序列中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`：截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_truncate''`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）—由截断/填充参数之一控制要使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则如果截断/填充参数中需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True`
    时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预先分词（例如，已分成单词）。如果设置为
    `True`，分词器将假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活 `padding`。这对于在具有计算能力
    `>= 7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '**kwargs — Passed along to the `.tokenize()` method.'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kwargs — 传递给 `.tokenize()` 方法。'
- en: Returns
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`, `torch.Tensor`, `tf.Tensor` 或 `np.ndarray`'
- en: The tokenized ids of the text.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的标记化id。
- en: Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分词器和词汇表将字符串转换为id（整数）序列。
- en: Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 相当于执行 `self.convert_tokens_to_ids(self.tokenize(text))`。
- en: '#### `push_to_hub`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
- en: '[PRE28]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`repo_id` (`str`) — The name of the repository you want to push your tokenizer
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id` (`str`) — 要将分词器推送到的存储库名称。在推送到给定组织时，应包含您的组织名称。'
- en: '`use_temp_dir` (`bool`, *optional*) — Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir` (`bool`, *可选*) — 是否使用临时目录存储在推送到Hub之前保存的文件。如果没有名为 `repo_id` 的目录，则默认为
    `True`，否则为 `False`。'
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload tokenizer"`.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message` (`str`, *可选*) — 推送时要提交的消息。默认为 `"Upload tokenizer"`。'
- en: '`private` (`bool`, *optional*) — Whether or not the repository created should
    be private.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private` (`bool`, *可选*) — 是否创建的存储库应为私有。'
- en: '`token` (`bool` or `str`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`bool` 或 `str`, *可选*) — 用作远程文件的HTTP bearer授权的令牌。如果为 `True`，将使用运行 `huggingface-cli
    login` 时生成的令牌（存储在 `~/.huggingface`）。如果未指定 `repo_url`，则默认为 `True`。'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) — Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int` 或 `str`, *可选*, 默认为 `"5GB"`) — 仅适用于模型。在分片之前的检查点的最大大小。然后，检查点将分片，每个分片的大小都小于此大小。如果表示为字符串，需要是数字后跟一个单位（如
    `"5MB"`）。我们将其默认为 `"5GB"`，以便用户可以在免费的Google Colab实例上轻松加载模型，而不会出现CPU OOM问题。'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr` (`bool`, *可选*, 默认为 `False`) — 是否创建一个带有上传文件的PR或直接提交。'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *可选*, 默认为 `True`) — 是否将模型权重转换为safetensors格式以进行更安全的序列化。'
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *可选*) — 要将上传的文件推送到的分支。'
- en: '`commit_description` (`str`, *optional*) — The description of the commit that
    will be created'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_description` (`str`, *可选*) — 将创建的提交的描述'
- en: '`tags` (`List[str]`, *optional*) — List of tags to push on the Hub.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags` (`List[str]`, *可选*) — 要推送到Hub上的标签列表。'
- en: Upload the tokenizer files to the 🤗 Model Hub.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 将分词器文件上传到 🤗 Model Hub。
- en: 'Examples:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE29]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#### `convert_ids_to_tokens`'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_ids_to_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L369)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L369)'
- en: '[PRE30]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`ids` (`int` or `List[int]`) — The token id (or token ids) to convert to tokens.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ids`（`int`或`List[int]`）-要转换为标记的标记id（或标记id）。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`（`bool`，*可选*，默认为`False`）-是否在解码中删除特殊标记。'
- en: Returns
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`str` or `List[str]`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`或`List[str]`'
- en: The decoded token(s).
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的标记。
- en: Converts a single index or a sequence of indices in a token or a sequence of
    tokens, using the vocabulary and added tokens.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 将单个索引或索引序列转换为标记或标记序列，使用词汇表和添加的标记。
- en: '#### `convert_tokens_to_ids`'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L314)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L314)'
- en: '[PRE31]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokens` (`str` or `List[str]`) — One or several token(s) to convert to token
    id(s).'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokens`（`str`或`List[str]`）-要转换为标记id的一个或多个标记。'
- en: Returns
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int` or `List[int]`'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`或`List[int]`'
- en: The token id or list of token ids.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 标记id或标记id列表。
- en: Converts a token string (or a sequence of tokens) in a single integer id (or
    a sequence of ids), using the vocabulary.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 将标记字符串（或标记序列）转换为单个整数id（或id序列），使用词汇表。
- en: '#### `get_added_vocab`'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_added_vocab`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L238)'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L238)'
- en: '[PRE32]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Returns
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict[str, int]`'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, int]`'
- en: The added tokens.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的标记。
- en: Returns the added tokens in the vocabulary as a dictionary of token to index.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 将词汇表中添加的标记作为标记到索引的字典返回。
- en: '#### `num_special_tokens_to_add`'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_special_tokens_to_add`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L348)'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L348)'
- en: '[PRE33]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pair` (`bool`, *optional*, defaults to `False`) — Whether the number of added
    tokens should be computed in the case of a sequence pair or a single sequence.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pair`（`bool`，*可选*，默认为`False`）-在序列对或单个序列的情况下是否应计算添加的标记数。'
- en: Returns
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of special tokens added to sequences.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到序列的特殊标记数。
- en: Returns the number of added tokens when encoding a sequence with special tokens.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用特殊标记对序列进行编码时返回添加的标记数。
- en: This encodes a dummy input and checks the number of added tokens, and is therefore
    not efficient. Do not put this inside your training loop.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这会对虚拟输入进行编码并检查添加的标记数，因此效率不高。不要将其放在训练循环内。
- en: '#### `set_truncation_and_padding`'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_truncation_and_padding`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L398)'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L398)'
- en: '[PRE34]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`padding_strategy` ([PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy))
    — The kind of padding that will be applied to the input'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_strategy`（[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)）-将应用于输入的填充类型'
- en: '`truncation_strategy` ([TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy))
    — The kind of truncation that will be applied to the input'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_strategy`（[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)）-将应用于输入的截断类型'
- en: '`max_length` (`int`) — The maximum size of a sequence.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`）-序列的最大大小。'
- en: '`stride` (`int`) — The stride to use when handling overflow.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`（`int`）-处理溢出时要使用的步幅。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）-如果设置，将序列填充到提供的值的倍数。这对于启用具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上的张量核心特别有用。'
- en: Define the truncation and the padding strategies for fast tokenizers (provided
    by HuggingFace tokenizers library) and restore the tokenizer settings afterwards.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 定义快速标记器的截断和填充策略（由HuggingFace标记器库提供），并在恢复标记器设置后恢复标记器设置。
- en: The provided tokenizer has no padding / truncation strategy before the managed
    section. If your tokenizer set a padding / truncation strategy before, then it
    will be reset to no padding / truncation when exiting the managed section.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的标记器在受管理部分之前没有填充/截断策略。如果您的标记器在之前设置了填充/截断策略，则在退出受管理部分时将重置为无填充/截断。
- en: '#### `train_new_from_iterator`'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `train_new_from_iterator`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L687)'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L687)'
- en: '[PRE35]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_iterator` (generator of `List[str]`) — The training corpus. Should be
    a generator of batches of texts, for instance a list of lists of texts if you
    have everything in memory.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_iterator`（`List[str]`的生成器）-训练语料库。应该是文本批次的生成器，例如，如果您将所有内容存储在内存中，则应该是文本列表的列表。'
- en: '`vocab_size` (`int`) — The size of the vocabulary you want for your tokenizer.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`）-您要为标记器设置的词汇表大小。'
- en: '`length` (`int`, *optional*) — The total number of sequences in the iterator.
    This is used to provide meaningful progress tracking'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length`（`int`，*可选*）-迭代器中序列的总数。这用于提供有意义的进度跟踪'
- en: '`new_special_tokens` (list of `str` or `AddedToken`, *optional*) — A list of
    new special tokens to add to the tokenizer you are training.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_special_tokens`（`str`或`AddedToken`的列表，*可选*）-要添加到正在训练的标记器的新特殊标记列表。'
- en: '`special_tokens_map` (`Dict[str, str]`, *optional*) — If you want to rename
    some of the special tokens this tokenizer uses, pass along a mapping old special
    token name to new special token name in this argument.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_map` (`Dict[str, str]`, *可选*) — 如果您想要重命名此分词器使用的一些特殊标记，请在此参数中传递一个旧特殊标记名称到新特殊标记名称的映射。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the trainer from the 🤗 Tokenizers library.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *可选*) — 从 🤗 Tokenizers 库传递给训练器的额外关键字参数。'
- en: Returns
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)'
- en: A new tokenizer of the same type as the original one, trained on `text_iterator`.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 一个与原始分词器相同类型的新分词器，训练于 `text_iterator`。
- en: Trains a tokenizer on a new corpus with the same defaults (in terms of special
    tokens or tokenization pipeline) as the current one.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与当前相同的默认值（特殊标记或标记化流水线方面）在新语料库上训练一个分词器。
- en: BatchEncoding
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BatchEncoding
- en: '### `class transformers.BatchEncoding`'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BatchEncoding`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L176)'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L176)'
- en: '[PRE36]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`data` (`dict`, *optional*) — Dictionary of lists/arrays/tensors returned by
    the `__call__`/`encode_plus`/`batch_encode_plus` methods (‘input_ids’, ‘attention_mask’,
    etc.).'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data` (`dict`, *可选*) — 由 `__call__`/`encode_plus`/`batch_encode_plus` 方法返回的列表/数组/张量的字典（''input_ids''，''attention_mask''等）。'
- en: '`encoding` (`tokenizers.Encoding` or `Sequence[tokenizers.Encoding]`, *optional*)
    — If the tokenizer is a fast tokenizer which outputs additional information like
    mapping from word/character space to token space the `tokenizers.Encoding` instance
    or list of instance (for batches) hold this information.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding` (`tokenizers.Encoding` 或 `Sequence[tokenizers.Encoding]`, *可选*)
    — 如果分词器是一个快速分词器，输出额外信息如从单词/字符空间到标记空间的映射，则 `tokenizers.Encoding` 实例或实例列表（用于批次）保存此信息。'
- en: '`tensor_type` (`Union[None, str, TensorType]`, *optional*) — You can give a
    tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy
    Tensors at initialization.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor_type` (`Union[None, str, TensorType]`, *可选*) — 您可以在此处提供一个 tensor_type，以在初始化时将整数列表转换为
    PyTorch/TensorFlow/Numpy 张量。'
- en: '`prepend_batch_axis` (`bool`, *optional*, defaults to `False`) — Whether or
    not to add a batch axis when converting to tensors (see `tensor_type` above).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepend_batch_axis` (`bool`, *可选*, 默认为 `False`) — 在转换为张量时是否添加批次轴（参见上面的 `tensor_type`）。'
- en: '`n_sequences` (`Optional[int]`, *optional*) — You can give a tensor_type here
    to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at initialization.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_sequences` (`Optional[int]`, *可选*) — 您可以在此处提供一个 tensor_type，以在初始化时将整数列表转换为
    PyTorch/TensorFlow/Numpy 张量。'
- en: Holds the output of the [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__),
    [encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus)
    and [batch_encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus)
    methods (tokens, attention_masks, etc).
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 保存了 [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__),
    [encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus)
    和 [batch_encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus)
    方法的输出（tokens, attention_masks等）。
- en: This class is derived from a python dictionary and can be used as a dictionary.
    In addition, this class exposes utility methods to map from word/character space
    to token space.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 此类派生自 Python 字典，可用作字典。此外，此类公开了实用方法，用于将单词/字符空间映射到标记空间。
- en: '#### `char_to_token`'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `char_to_token`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L555)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L555)'
- en: '[PRE37]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_or_char_index` (`int`) — Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the word in the sequence'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_char_index` (`int`) — 批次中序列的索引。如果批次仅包含一个序列，则这可以是序列中单词的索引'
- en: '`char_index` (`int`, *optional*) — If a batch index is provided in *batch_or_token_index*,
    this can be the index of the word in the sequence.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`char_index` (`int`, *可选*) — 如果在 *batch_or_token_index* 中提供了批次索引，则这可以是序列中单词的索引。'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) — If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided character index belongs to.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index` (`int`, *可选*, 默认为 0) — 如果批次中编码了一对序列，则可以用于指定提供的字符索引属于一对序列中的哪个序列（0或1）。'
- en: Returns
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Index of the token.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的索引。
- en: Get the index of the token in the encoded output comprising a character in the
    original string for a sequence of the batch.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 获取编码输出中包含原始字符串中字符的序列的标记索引。
- en: 'Can be called as:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用为：
- en: '`self.char_to_token(char_index)` if batch size is 1'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.char_to_token(char_index)` 如果批次大小为 1'
- en: '`self.char_to_token(batch_index, char_index)` if batch size is greater or equal
    to 1'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.char_to_token(batch_index, char_index)` 如果批次大小大于或等于 1'
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e. words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入序列以预分词序列（即用户定义的单词）提供时，此方法特别适用。在这种情况下，它允许轻松将编码的标记与提供的分词单词关联起来。
- en: '#### `char_to_word`'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `char_to_word`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L641)'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L641)'
- en: '[PRE38]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_or_char_index` (`int`) — Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the character in the
    original string.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_char_index` (`int`) — 批次中序列的索引。如果批次仅包含一个序列，则这可以是原始字符串中字符的索引。'
- en: '`char_index` (`int`, *optional*) — If a batch index is provided in *batch_or_token_index*,
    this can be the index of the character in the original string.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`char_index`（`int`，*可选*）— 如果在*batch_or_token_index*中提供了批次索引，则可以是原始字符串中字符的索引。'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) — If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided character index belongs to.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index`（`int`，*可选*，默认为0）— 如果批次中编码了一对序列，则可以用来指定提供的字符索引属于该对序列中的哪个序列（0或1）。'
- en: Returns
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int` or `List[int]`'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`或`List[int]`'
- en: Index or indices of the associated encoded token(s).
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 关联编码标记的索引或索引。
- en: Get the word in the original string corresponding to a character in the original
    string of a sequence of the batch.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 获取批次中序列的原始字符串中与标记的字符对应的单词。
- en: 'Can be called as:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用为：
- en: '`self.char_to_word(char_index)` if batch size is 1'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小为1，则为`self.char_to_word(char_index)`
- en: '`self.char_to_word(batch_index, char_index)` if batch size is greater than
    1'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小大于1，则为`self.char_to_word(batch_index, char_index)`
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e. words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入序列以预分词序列（即用户定义的单词）提供时，此方法特别适用。在这种情况下，它允许轻松将编码的标记与提供的分词单词关联起来。
- en: '#### `convert_to_tensors`'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_to_tensors`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L680)'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L680)'
- en: '[PRE39]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tensor_type` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — The type of tensors to use. If `str`, should be one of the values
    of the enum [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType).
    If `None`, no modification is done.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor_type`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）—
    要使用的张量类型。如果是`str`，应该是枚举[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)值之一。如果为`None`，则不进行修改。'
- en: '`prepend_batch_axis` (`int`, *optional*, defaults to `False`) — Whether or
    not to add the batch dimension during the conversion.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepend_batch_axis`（`int`，*可选*，默认为`False`）— 在转换过程中是否添加批次维度。'
- en: Convert the inner content to tensors.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 将内部内容转换为张量。
- en: '#### `sequence_ids`'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `sequence_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L319)'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L319)'
- en: '[PRE40]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_index` (`int`, *optional*, defaults to 0) — The index to access in the
    batch.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`（`int`，*可选*，默认为0）— 要访问的批次中的索引。'
- en: Returns
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Optional[int]]`'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Optional[int]]`'
- en: A list indicating the sequence id corresponding to each token. Special tokens
    added by the tokenizer are mapped to `None` and other tokens are mapped to the
    index of their corresponding sequence.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 一个指示每个标记对应的序列id的列表。由分词器添加的特殊标记映射到`None`，其他标记映射到其对应序列的索引。
- en: 'Return a list mapping the tokens to the id of their original sentences:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 返回将标记映射到其原始句子的id的列表：
- en: '`None` for special tokens added around or between sequences,'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于添加在序列周围或之间的特殊标记，为`None`，
- en: '`0` for tokens corresponding to words in the first sequence,'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`表示对应于第一个序列中的单词的标记，'
- en: '`1` for tokens corresponding to words in the second sequence when a pair of
    sequences was jointly encoded.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一对序列被联合编码时，对于第二个序列中的单词对应的标记，为`1`。
- en: '#### `to`'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L773)'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L773)'
- en: '[PRE41]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`device` (`str` or `torch.device`) — The device to put the tensors on.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`（`str`或`torch.device`）— 要放置张量的设备。'
- en: Returns
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: The same instance after modification.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的相同实例。
- en: Send all values to device by calling `v.to(device)` (PyTorch only).
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`v.to(device)`将所有值发送到设备（仅适用于PyTorch）。
- en: '#### `token_to_chars`'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `token_to_chars`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L516)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L516)'
- en: '[PRE42]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_or_token_index` (`int`) — Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the token in the sequence.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_token_index`（`int`）— 批次中序列的索引。如果批次只包含一个序列，则可以是序列中标记的索引。'
- en: '`token_index` (`int`, *optional*) — If a batch index is provided in *batch_or_token_index*,
    this can be the index of the token or tokens in the sequence.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_index`（`int`，*可选*）— 如果在*batch_or_token_index*中提供了批次索引，则可以是序列中标记或标记的索引。'
- en: Returns
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '[CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)'
- en: Span of characters in the original string, or None, if the token (e.g. ~~,~~
    ) doesn’t correspond to any chars in the origin string.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 原始字符串中字符的范围，如果标记（例如~~,~~）不对应于原始字符串中的任何字符，则为None。
- en: Get the character span corresponding to an encoded token in a sequence of the
    batch.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 获取批次中序列中编码标记对应的字符跨度。
- en: 'Character spans are returned as a [CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)
    with:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 字符跨度以[CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)形式返回，具有：
- en: '`start` — Index of the first character in the original string associated to
    the token.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start`— 与标记关联的原始字符串中第一个字符的索引。'
- en: '`end` — Index of the character following the last character in the original
    string associated to the token.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end`— 跟随与标记关联的原始字符串中最后一个字符的索引。'
- en: 'Can be called as:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用为：
- en: '`self.token_to_chars(token_index)` if batch size is 1'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小为1，则为`self.token_to_chars(token_index)`
- en: '`self.token_to_chars(batch_index, token_index)` if batch size is greater or
    equal to 1'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小大于或等于1，则为`self.token_to_chars(batch_index, token_index)`
- en: '#### `token_to_sequence`'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `token_to_sequence`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L386)'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L386)'
- en: '[PRE43]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Parameters
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_or_token_index` (`int`) — Index of the sequence in the batch. If the
    batch only comprises one sequence, this can be the index of the token in the sequence.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_token_index`（`int`）—批次中序列的索引。如果批次只包含一个序列，这可以是序列中标记的索引。'
- en: '`token_index` (`int`, *optional*) — If a batch index is provided in *batch_or_token_index*,
    this can be the index of the token in the sequence.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_index`（`int`，*可选*）—如果在*batch_or_token_index*中提供了批次索引，则这可以是序列中标记的索引。'
- en: Returns
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Index of the word in the input sequence.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列中的单词索引。
- en: Get the index of the sequence represented by the given token. In the general
    use case, this method returns `0` for a single sequence or the first sequence
    of a pair, and `1` for the second sequence of a pair
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 获取给定标记表示的序列的索引。在一般用例中，此方法对于单个序列或一对序列的第一个序列返回`0`，对于一对序列的第二个序列返回`1`
- en: 'Can be called as:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用为：
- en: '`self.token_to_sequence(token_index)` if batch size is 1'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小为1，则为`self.token_to_sequence(token_index)`
- en: '`self.token_to_sequence(batch_index, token_index)` if batch size is greater
    than 1'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小大于1，则为`self.token_to_sequence(batch_index, token_index)`
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e., words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入序列以预标记序列（即，单词由用户定义）提供时，此方法特别适用。在这种情况下，它允许轻松将编码标记与提供的标记化单词关联起来。
- en: '#### `token_to_word`'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `token_to_word`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L425)'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L425)'
- en: '[PRE44]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_or_token_index` (`int`) — Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the token in the sequence.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_token_index`（`int`）—批次中序列的索引。如果批次只包含一个序列，这可以是序列中标记的索引。'
- en: '`token_index` (`int`, *optional*) — If a batch index is provided in *batch_or_token_index*,
    this can be the index of the token in the sequence.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_index`（`int`，*可选*）—如果在*batch_or_token_index*中提供了批次索引，则这可以是序列中标记的索引。'
- en: Returns
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Index of the word in the input sequence.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列中的单词索引。
- en: Get the index of the word corresponding (i.e. comprising) to an encoded token
    in a sequence of the batch.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 获取与批次序列中编码标记对应的单词的索引。
- en: 'Can be called as:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用为：
- en: '`self.token_to_word(token_index)` if batch size is 1'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小为1，则为`self.token_to_word(token_index)`
- en: '`self.token_to_word(batch_index, token_index)` if batch size is greater than
    1'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批次大小大于1，则为`self.token_to_word(batch_index, token_index)`
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e., words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入序列以预标记序列（即，单词由用户定义）提供时，此方法特别适用。在这种情况下，它允许轻松将编码标记与提供的标记化单词关联起来。
- en: '#### `tokens`'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L301)'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L301)'
- en: '[PRE45]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_index` (`int`, *optional*, defaults to 0) — The index to access in the
    batch.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`（`int`，*可选*，默认为0）—要访问的批次索引。'
- en: Returns
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[str]`'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of tokens at that index.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 该索引处的标记列表。
- en: Return the list of tokens (sub-parts of the input strings after word/subword
    splitting and before conversion to integer indices) at a given batch index (only
    works for the output of a fast tokenizer).
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 返回给定批次索引处的标记列表（在单词/子词拆分后和转换为整数索引之前的输入字符串的子部分）（仅适用于快速标记器的输出）。
- en: '#### `word_ids`'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `word_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L367)'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L367)'
- en: '[PRE46]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_index` (`int`, *optional*, defaults to 0) — The index to access in the
    batch.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`（`int`，*可选*，默认为0）—要访问的批次索引。'
- en: Returns
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Optional[int]]`'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Optional[int]]`'
- en: A list indicating the word corresponding to each token. Special tokens added
    by the tokenizer are mapped to `None` and other tokens are mapped to the index
    of their corresponding word (several tokens will be mapped to the same word index
    if they are parts of that word).
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 一个列表，指示每个标记对应的单词。标记器添加的特殊标记映射到`None`，其他标记映射到其对应单词的索引（如果它们是该单词的一部分，则几个标记将映射到相同的单词索引）。
- en: Return a list mapping the tokens to their actual word in the initial sentence
    for a fast tokenizer.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个将标记映射到初始句子中实际单词的列表，用于快速标记器。
- en: '#### `word_to_chars`'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `word_to_chars`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L596)'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L596)'
- en: '[PRE47]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_or_word_index` (`int`) — Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the word in the sequence'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_word_index`（`int`）—批次中序列的索引。如果批次只包含一个序列，这可以是序列中单词的索引'
- en: '`word_index` (`int`, *optional*) — If a batch index is provided in *batch_or_token_index*,
    this can be the index of the word in the sequence.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_index`（`int`，*可选*）—如果在*batch_or_token_index*中提供了批次索引，则这可以是序列中单词的索引。'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) — If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided word index belongs to.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index`（`int`，*可选*，默认为0）—如果批次中编码了一对序列，则可以用于指定提供的单词索引属于该对中的哪个序列（0或1）。'
- en: Returns
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`CharSpan` or `List[CharSpan]`'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '`CharSpan`或`List[CharSpan]`'
- en: 'Span(s) of the associated character or characters in the string. CharSpan are
    NamedTuple with:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 与字符串中相关字符或字符的范围。CharSpan是NamedTuple，具有：
- en: 'start: index of the first character associated to the token in the original
    string'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'start: 原始字符串中与标记关联的第一个字符的索引'
- en: 'end: index of the character following the last character associated to the
    token in the original string'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'end: 原始字符串中与标记关联的最后一个字符后面的字符的索引'
- en: Get the character span in the original string corresponding to given word in
    a sequence of the batch.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 获取批处理序列中给定单词对应的原始字符串中的字符范围。
- en: 'Character spans are returned as a CharSpan NamedTuple with:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 字符范围以CharSpan NamedTuple形式返回：
- en: 'start: index of the first character in the original string'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'start: 原始字符串中的第一个字符的索引'
- en: 'end: index of the character following the last character in the original string'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'end: 原始字符串中最后一个字符后面的字符的索引'
- en: 'Can be called as:'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用为：
- en: '`self.word_to_chars(word_index)` if batch size is 1'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批处理大小为1，则为`self.word_to_chars(word_index)`
- en: '`self.word_to_chars(batch_index, word_index)` if batch size is greater or equal
    to 1'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果批处理大小大于或等于1，则为`self.word_to_chars(batch_index, word_index)`
- en: '#### `word_to_tokens`'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `word_to_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L463)'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L463)'
- en: '[PRE48]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_or_word_index` (`int`) — Index of the sequence in the batch. If the
    batch only comprises one sequence, this can be the index of the word in the sequence.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_word_index`（`int`）— 批处理中序列的索引。如果批处理仅包括一个序列，则可以是序列中单词的索引。'
- en: '`word_index` (`int`, *optional*) — If a batch index is provided in *batch_or_token_index*,
    this can be the index of the word in the sequence.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_index`（`int`，*可选*）— 如果在*batch_or_token_index*中提供了批处理索引，则可以是序列中单词的索引。'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) — If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided word index belongs to.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index`（`int`，*可选*，默认为0）— 如果批处理中编码了一对序列，则可以用于指定提供的单词索引属于一对序列中的哪个序列（0或1）。'
- en: Returns
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: ([TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan),
    *optional*)
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: ([TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan)，*可选*)
- en: Span of tokens in the encoded sequence. Returns `None` if no tokens correspond
    to the word. This can happen especially when the token is a special token that
    has been used to format the tokenization. For example when we add a class token
    at the very beginning of the tokenization.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 编码序列中的标记范围。如果没有标记与该单词对应，则返回`None`。这可能会发生，特别是当标记是用于格式化标记化的特殊标记时。例如，当我们在标记化的开头添加一个类标记时。
- en: Get the encoded token span corresponding to a word in a sequence of the batch.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 获取与批处理序列中的单词对应的编码标记范围。
- en: 'Token spans are returned as a [TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan)
    with:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 标记范围以[TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan)形式返回：
- en: '`start` — Index of the first token.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start` — 第一个标记的索引。'
- en: '`end` — Index of the token following the last token.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end` — 最后一个标记后面的标记的索引。'
- en: 'Can be called as:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用为：
- en: '`self.word_to_tokens(word_index, sequence_index: int = 0)` if batch size is
    1'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果批处理大小为1，则为`self.word_to_tokens(word_index, sequence_index: int = 0)`'
- en: '`self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)` if
    batch size is greater or equal to 1'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果批处理大小大于或等于1，则为`self.word_to_tokens(batch_index, word_index, sequence_index:
    int = 0)`'
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e. words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入序列以预分词序列（即用户定义的单词）提供时，此方法特别适用。在这种情况下，它允许轻松将编码标记与提供的分词单词关联起来。
- en: '#### `words`'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `words`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L343)'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L343)'
- en: '[PRE49]'
  id: totrans-730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_index` (`int`, *optional*, defaults to 0) — The index to access in the
    batch.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`（`int`，*可选*，默认为0）— 要访问的批处理中的索引。'
- en: Returns
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Optional[int]]`'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Optional[int]]`'
- en: A list indicating the word corresponding to each token. Special tokens added
    by the tokenizer are mapped to `None` and other tokens are mapped to the index
    of their corresponding word (several tokens will be mapped to the same word index
    if they are parts of that word).
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 指示每个标记对应的单词的列表。标记器添加的特殊标记映射到`None`，其他标记映射到其对应单词的索引（如果它们是该单词的一部分，则几个标记将映射到相同的单词索引）。
- en: Return a list mapping the tokens to their actual word in the initial sentence
    for a fast tokenizer.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个列表，将标记映射到初始句子中的实际单词，以便快速标记化器使用。
