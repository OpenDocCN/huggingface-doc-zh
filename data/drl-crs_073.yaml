- en: (Optional) What is Curiosity in Deep Reinforcement Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: （可选）深度强化学习中的好奇心是什么？
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit5/curiosity](https://huggingface.co/learn/deep-rl-course/unit5/curiosity)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/learn/deep-rl-course/unit5/curiosity](https://huggingface.co/learn/deep-rl-course/unit5/curiosity)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an (optional) introduction to Curiosity. If you want to learn more,
    you can read two additional articles where we dive into the mathematical details:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于好奇心的（可选）介绍。如果您想了解更多信息，可以阅读两篇更深入探讨数学细节的文章：
- en: '[Curiosity-Driven Learning through Next State Prediction](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过下一个状态预测进行好奇心驱动学习](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa)'
- en: '[Random Network Distillation: a new take on Curiosity-Driven Learning](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机网络蒸馏：对好奇心驱动学习的新看法](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)'
- en: Two Major Problems in Modern RL
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代强化学习中的两个主要问题
- en: 'To understand what Curiosity is, we first need to understand the two major
    problems with RL:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解好奇心是什么，我们首先需要了解强化学习中的两个主要问题：
- en: First, the *sparse rewards problem:* that is, **most rewards do not contain
    information, and hence are set to zero**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是“稀疏奖励问题”：即大多数奖励不包含信息，因此被设为零。
- en: Remember that RL is based on the *reward hypothesis*, which is the idea that
    each goal can be described as the maximization of the rewards. Therefore, rewards
    act as feedback for RL agents; **if they don’t receive any, their knowledge of
    which action is appropriate (or not) cannot change**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，强化学习基于“奖励假设”，即每个目标都可以描述为奖励的最大化。因此，奖励作为强化学习代理的反馈；如果它们没有收到任何奖励，它们对哪种行为是适当的（或不适当）的知识就无法改变。
- en: '![Curiosity](../Images/542512c1c81fdf970fe9cb37254df345.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![好奇心](../Images/542512c1c81fdf970fe9cb37254df345.png)'
- en: 'Source: Thanks to the reward, our agent knows that this action at that state
    was good'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：感谢奖励，我们的代理知道这个状态下的这个动作是好的
- en: For instance, in [Vizdoom](https://vizdoom.cs.put.edu.pl/), a set of environments
    based on the game Doom “DoomMyWayHome,” your agent is only rewarded **if it finds
    the vest**. However, the vest is far away from your starting point, so most of
    your rewards will be zero. Therefore, if our agent does not receive useful feedback
    (dense rewards), it will take much longer to learn an optimal policy, and **it
    can spend time turning around without finding the goal**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[Vizdoom](https://vizdoom.cs.put.edu.pl/)中，基于游戏Doom“DoomMyWayHome”的一组环境中，只有在代理找到护甲时才会受到奖励。然而，护甲离您的起点很远，因此大部分奖励将为零。因此，如果我们的代理没有收到有用的反馈（密集奖励），它将需要更长时间才能学习到最佳策略，并且可能会花费时间在找不到目标的情况下转圈。
- en: '![Curiosity](../Images/e3753d61bd4ca091ba22d9ba1b36cd3f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![好奇心](../Images/e3753d61bd4ca091ba22d9ba1b36cd3f.png)'
- en: The second big problem is that **the extrinsic reward function is handmade;
    in each environment, a human has to implement a reward function**. But how we
    can scale that in big and complex environments?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个大问题是外部奖励函数是手工制作的；在每个环境中，人类必须实现一个奖励函数。但是我们如何在大型和复杂的环境中扩展它呢？
- en: So what is Curiosity?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么好奇心是什么？
- en: A solution to these problems is **to develop a reward function intrinsic to
    the agent, i.e., generated by the agent itself**. The agent will act as a self-learner
    since it will be the student and its own feedback master.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的方法是开发一个内在于代理的奖励函数，即由代理自己生成。代理将充当自学习者，因为它将是学生和自己的反馈大师。
- en: '**This intrinsic reward mechanism is known as Curiosity** because this reward
    pushes the agent to explore states that are novel/unfamiliar. To achieve that,
    our agent will receive a high reward when exploring new trajectories.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种内在奖励机制被称为好奇心，因为这种奖励推动代理探索新颖/陌生的状态。为了实现这一点，我们的代理将在探索新轨迹时获得高奖励。
- en: This reward is inspired by how humans act. **We naturally have an intrinsic
    desire to explore environments and discover new things**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种奖励受到人类行为的启发。我们自然地有探索环境和发现新事物的内在欲望。
- en: There are different ways to calculate this intrinsic reward. The classical approach
    (Curiosity through next-state prediction) is to calculate Curiosity **as the error
    of our agent in predicting the next state, given the current state and action
    taken**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来计算这种内在奖励。经典方法（通过下一个状态预测的好奇心）是计算好奇心为我们的代理在给定当前状态和采取的行动的情况下预测下一个状态的错误。
- en: '![Curiosity](../Images/6b030efa6065b6f29760b37ae29749d3.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![好奇心](../Images/6b030efa6065b6f29760b37ae29749d3.png)'
- en: Because the idea of Curiosity is to **encourage our agent to perform actions
    that reduce the uncertainty in the agent’s ability to predict the consequences
    of its actions** (uncertainty will be higher in areas where the agent has spent
    less time or in areas with complex dynamics).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因为好奇心的想法是鼓励我们的代理执行减少其行为后果预测不确定性的行动（在代理花费较少时间或在具有复杂动态的区域中，不确定性会更高）。
- en: If the agent spends a lot of time on these states, it will be good at predicting
    the next state (low Curiosity). On the other hand, if it’s in a new, unexplored
    state, it will be hard to predict the following state (high Curiosity).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理在这些状态上花费了很多时间，它将擅长预测下一个状态（低好奇心）。另一方面，如果它处于一个新的、未探索的状态，它将很难预测接下来的状态（高好奇心）。
- en: '![Curiosity](../Images/513912902e6673ccaf455689bc8325ab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![好奇心](../Images/513912902e6673ccaf455689bc8325ab.png)'
- en: Using Curiosity will push our agent to favor transitions with high prediction
    error (which will be higher in areas where the agent has spent less time, or in
    areas with complex dynamics) and **consequently better explore our environment**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用好奇心将推动我们的代理倾向于具有高预测误差的转换（这将在代理在那些花费较少时间的区域或具有复杂动态的区域中更高），因此更好地探索我们的环境。
- en: There’s also **other curiosity calculation methods**. ML-Agents uses a more
    advanced one called Curiosity through random network distillation. This is out
    of the scope of the tutorial but if you’re interested [I wrote an article explaining
    it in detail](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他的好奇心计算方法。ML-Agents使用一种更先进的方法，称为通过随机网络蒸馏实现好奇心。这超出了本教程的范围，但如果你感兴趣，我写了一篇详细解释的文章。
