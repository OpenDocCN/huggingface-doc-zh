- en: LayoutXLM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LayoutXLM
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'LayoutXLM was proposed in [LayoutXLM: Multimodal Pre-training for Multilingual
    Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng
    Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang,
    Furu Wei. It’s a multilingual extension of the [LayoutLMv2 model](https://arxiv.org/abs/2012.14740)
    trained on 53 languages.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'LayoutXLM是由Yiheng Xu、Tengchao Lv、Lei Cui、Guoxin Wang、Yijuan Lu、Dinei Florencio、Cha
    Zhang、Furu Wei提出的[LayoutXLM: 多模态预训练用于多语言视觉丰富文档理解](https://arxiv.org/abs/2104.08836)。它是在53种语言上训练的[LayoutLMv2模型](https://arxiv.org/abs/2012.14740)的多语言扩展。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Multimodal pre-training with text, layout, and image has achieved SOTA performance
    for visually-rich document understanding tasks recently, which demonstrates the
    great potential for joint learning across different modalities. In this paper,
    we present LayoutXLM, a multimodal pre-trained model for multilingual document
    understanding, which aims to bridge the language barriers for visually-rich document
    understanding. To accurately evaluate LayoutXLM, we also introduce a multilingual
    form understanding benchmark dataset named XFUN, which includes form understanding
    samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese),
    and key-value pairs are manually labeled for each language. Experiment results
    show that the LayoutXLM model has significantly outperformed the existing SOTA
    cross-lingual pre-trained models on the XFUN dataset.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近，使用文本、布局和图像进行多模态预训练已经在视觉丰富文档理解任务中取得了SOTA性能，这表明跨不同模态的联合学习具有巨大潜力。在本文中，我们提出了LayoutXLM，这是一个用于多语言文档理解的多模态预训练模型，旨在消除视觉丰富文档理解的语言障碍。为了准确评估LayoutXLM，我们还介绍了一个名为XFUN的多语言表单理解基准数据集，其中包括7种语言（中文、日文、西班牙文、法文、意大利文、德文、葡萄牙文）的表单理解样本，并为每种语言手动标记了键值对。实验结果表明，LayoutXLM模型在XFUN数据集上明显优于现有的SOTA跨语言预训练模型。*'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/microsoft/unilm).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/microsoft/unilm)找到。
- en: Usage tips and examples
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示和示例
- en: 'One can directly plug in the weights of LayoutXLM into a LayoutLMv2 model,
    like so:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接将LayoutXLM的权重插入到LayoutLMv2模型中，如下所示：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that LayoutXLM has its own tokenizer, based on [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast).
    You can initialize it as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LayoutXLM有自己的分词器，基于[LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)。您可以按以下方式初始化它：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Similar to LayoutLMv2, you can use [LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)
    (which internally applies [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    and [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)
    in sequence) to prepare all data for the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与LayoutLMv2类似，您可以使用[LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)（内部应用[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)和[LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)）来为模型准备所有数据。
- en: As LayoutXLM’s architecture is equivalent to that of LayoutLMv2, one can refer
    to [LayoutLMv2’s documentation page](layoutlmv2) for all tips, code examples and
    notebooks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LayoutXLM的架构等同于LayoutLMv2，可以参考[LayoutLMv2的文档页面](layoutlmv2)获取所有提示、代码示例和笔记本。
- en: LayoutXLMTokenizer
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutXLMTokenizer
- en: '### `class transformers.LayoutXLMTokenizer`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutXLMTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L149)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L149)'
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件路径。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于序列分类（整个序列的分类，而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为这个标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [CLS] token.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[CLS]标记的边界框。'
- en: '`sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`)
    — The bounding box to use for the special [SEP] token.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`)
    — 用于特殊[SEP]标记的边界框。'
- en: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [PAD] token.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[PAD]标记的边界框。'
- en: '`pad_token_label` (`int`, *optional*, defaults to -100) — The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorch’s
    CrossEntropyLoss.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_label` (`int`, *optional*, defaults to -100) — 用于填充标记的标签。默认为-100，这是PyTorch的CrossEntropyLoss的`ignore_index`。'
- en: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) — Whether
    or not to only label the first subword, in case word labels are provided.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) — 是否仅标记第一个子词，如果提供了单词标签。'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs` (`dict`, *optional*) — 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可以用于设置：'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`：启用子词正则化。'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`：unigram的采样参数。对于BPE-Dropout无效。'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`：不执行采样。'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`：从nbest_size结果中采样。'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`：假设nbest_size是无限的，并使用前向过滤和后向采样算法从所有假设（格）中采样。'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：用于unigram采样的平滑参数，以及BPE-dropout的合并操作的丢弃概率。'
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和ID）的*SentencePiece*处理器。'
- en: Adapted from [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer).
    Based on [SentencePiece](https://github.com/google/sentencepiece).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 改编自[RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)和[XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)。基于[SentencePiece](https://github.com/google/sentencepiece)。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应该参考这个超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L442)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L442)'
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题）或一个字符串列表的列表（单词批次）。'
- en: '`text_pair` (`List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列应该是一个字符串列表（预分词的字符串）。'
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) — Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) — 单词级边界框。每个边界框应该被归一化为0-1000的比例。'
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) — Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) — 单词级整数标签（用于标记分类任务，如FUNSD、CORD）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — 是否对序列进行编码，相对于其模型使用特殊标记。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, 默认为 `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度，使用参数`max_length`指定，或者填充到模型的最大可接受输入长度，如果未提供该参数。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：不填充（即，可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, 默认为 `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`：截断到指定的最大长度，使用参数`max_length`指定，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批对序列），则逐个截断token，从一对序列中最长的序列中删除一个token。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：截断到指定的最大长度，使用参数`max_length`指定，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批对序列），则仅截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`：截断到指定的最大长度，使用参数`max_length`指定，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批对序列），则仅截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不截断（即，可以输出序列长度大于模型最大可接受输入大小的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) — 控制截断/填充参数使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, 默认为 0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出token将包含截断序列末尾的一些token，以提供截断和溢出序列之间的一些重叠。该参数的值定义重叠token的数量。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) — 如果设置，将序列填充到提供的值的倍数。这对于启用具有计算能力`>=
    7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回Numpy `np.ndarray`对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *optional*) — 是否返回token类型ID。如果保持默认设置，将根据特定分词器的默认设置返回token类型ID，由`return_outputs`属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token类型ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认设置返回注意力掩码，由`return_outputs`属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *optional*, 默认为 `False`) — 是否返回溢出的标记序列。如果提供了一对输入ID序列（或一批对），并且
    `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出的标记。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *optional*, 默认为 `False`) — 是否返回特殊标记掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *optional*, 默认为 `False`) — 是否返回每个标记的 `(char_start,
    char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅在继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    的快速标记器上可用，如果使用Python的标记器，此方法将引发 `NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *optional*, 默认为 `False`) — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *optional*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给 `self.tokenize()`
    方法'
- en: Returns
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要提供给模型的标记ID列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`bbox` — List of bounding boxes to be fed to a model.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` — 要提供给模型的边界框列表。'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要提供给模型的标记类型ID列表（当 `return_token_type_ids=True` 或 *“token_type_ids”*
    在 `self.model_input_names` 中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些标记应该被模型关注的索引列表（当 `return_attention_mask=True` 或 *“attention_mask”*
    在 `self.model_input_names` 中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`labels` — List of labels to be fed to a model. (when `word_labels` is specified).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` — 要提供给模型的标签列表（当指定 `word_labels` 时）。'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出的标记序列列表（当指定 `max_length` 并且 `return_overflowing_tokens=True`
    时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 被截断的标记数（当指定 `max_length` 并且 `return_overflowing_tokens=True`
    时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当 `add_special_tokens=True`
    和 `return_special_tokens_mask=True` 时）。'
- en: '`length` — The length of the inputs (when `return_length=True`).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当 `return_length=True` 时）。'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化和准备模型的一个或多个序列或一个或多个序列对的主要方法，具有单词级归一化的边界框和可选标签。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L314)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L314)'
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 要添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 可选的第二个序列对的ID列表。'
- en: Returns
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLM-RoBERTa sequence has
    the following format:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。XLM-RoBERTa序列的格式如下：
- en: 'single sequence: `<s> X </s>`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`<s> X </s>`
- en: 'pair of sequences: `<s> A </s></s> B </s>`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`<s> A </s></s> B </s>`
- en: '#### `get_special_tokens_mask`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L340)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L340)'
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 可选的第二个序列对的ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *optional*, 默认为 `False`) — 标记列表是否已经使用特殊标记格式化为模型。'
- en: Returns
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用分词器的`prepare_for_model`方法添加特殊标记时调用此方法。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L368)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L368)'
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 零的列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. XLM-RoBERTa does not make use of token type ids, therefore a list of zeros
    is returned.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建用于序列对分类任务的掩码。XLM-RoBERTa不使用标记类型ID，因此返回零的列表。
- en: '#### `save_vocabulary`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L425)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L425)'
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: LayoutXLMTokenizerFast
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutXLMTokenizerFast
- en: '### `class transformers.LayoutXLMTokenizerFast`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutXLMTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L152)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L152)'
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件路径。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，在构建来自多个序列的序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于序列分类时使用的分类器标记（对整个序列进行分类，而不是对每个标记进行分类）。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [CLS] token.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[CLS]标记的边界框。'
- en: '`sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`)
    — The bounding box to use for the special [SEP] token.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`)
    — 用于特殊[SEP]标记的边界框。'
- en: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [PAD] token.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[PAD]标记的边界框。'
- en: '`pad_token_label` (`int`, *optional*, defaults to -100) — The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorch’s
    CrossEntropyLoss.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_label` (`int`, *optional*, defaults to -100) — 用于填充标记的标签。默认为-100，这是PyTorch的CrossEntropyLoss的`ignore_index`。'
- en: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) — Whether
    or not to only label the first subword, in case word labels are provided.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) — 是否仅标记第一个子词，如果提供了单词标签。'
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `["<s>NOTUSED",
    "</s>NOTUSED"]`) — Additional special tokens used by the tokenizer.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `["<s>NOTUSED",
    "</s>NOTUSED"]`) — 分词器使用的额外特殊标记。'
- en: Construct a “fast” LayoutXLM tokenizer (backed by HuggingFace’s *tokenizers*
    library). Adapted from [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer).
    Based on [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速” LayoutXLM 分词器（由 HuggingFace 的 *tokenizers* 库支持）。改编自 [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    和 [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)。基于
    [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models)。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L272)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L272)'
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题），或一个字符串列表的列表（单词批次）。'
- en: '`text_pair` (`List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列应该是一个字符串列表（预分词的字符串）。'
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) — Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) — 单词级别的边界框。每个边界框应标准化为
    0-1000 的比例。'
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) — Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_labels` (`List[int]`, `List[List[int]]`, *可选*) — 单词级别的整数标签（用于诸如 FUNSD、CORD
    等标记分类任务）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 是否使用相对于其模型的特殊标记对序列进行编码。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到指定的最大长度，该长度由参数 `max_length` 指定，或者填充到模型可接受的最大输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）: 无填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *可选*, 默认为 `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则将逐个标记截断，从该对中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）: 无截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则将使用预定义的模型最大长度，如果截断/填充参数中需要最大长度。如果模型没有特定的最大输入长度（如 XLNet）截断/填充到最大长度将被禁用。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, 默认为 0) — 如果设置为一个数字，并且与 `max_length` 一起使用，当 `return_overflowing_tokens=True`
    时返回的溢出 token 将包含截断序列末尾的一些 token，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠 token 的数量。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力 `>=
    7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *optional*) — 是否返回 token 类型 ID。如果保持默认设置，将根据特定分词器的默认值返回
    token 类型 ID，由 `return_outputs` 属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是 token 类型 ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由
    `return_outputs` 属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *optional*, 默认为 `False`) — 是否返回溢出的 token
    序列。如果提供了一对输入 id 序列（或一批对）并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出的
    token。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *optional*, 默认为 `False`) — 是否返回特殊 token
    掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *optional*, 默认为 `False`) — 是否返回每个 token 的
    `(char_start, char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这仅适用于继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    的快速分词器，如果使用 Python 的分词器，此方法将引发 `NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *optional*, 默认为 `False`) — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *optional*, 默认为 `True`) — 是否打印更多信息和警告。 **kwargs — 传递给 `self.tokenize()`
    方法'
- en: Returns
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含以下字段的 [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要提供给模型的 token id 列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`bbox` — List of bounding boxes to be fed to a model.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` — 要提供给模型的边界框列表。'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要提供给模型的 token 类型 id 列表（当 `return_token_type_ids=True` 或
    *“token_type_ids”* 在 `self.model_input_names` 中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是 token 类型 ID？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些 token 应该被模型关注的索引列表（当 `return_attention_mask=True` 或
    *“attention_mask”* 在 `self.model_input_names` 中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`labels` — List of labels to be fed to a model. (when `word_labels` is specified).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` — 要提供给模型的标签列表（当指定 `word_labels` 时）。'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出的 token 序列列表（当指定 `max_length` 并且 `return_overflowing_tokens=True`
    时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 截断的标记数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`且`return_special_tokens_mask=True`时）。'
- en: '`length` — The length of the inputs (when `return_length=True`).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当`return_length=True`时）。'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 主要方法是对一个或多个序列或一个或多个序列对进行标记化和准备模型，其中包含单词级别的归一化边界框和可选标签。
- en: LayoutXLMProcessor
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LayoutXLMProcessor
- en: '### `class transformers.LayoutXLMProcessor`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LayoutXLMProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L26)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L26)'
- en: '[PRE10]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` (`LayoutLMv2ImageProcessor`, *optional*) — An instance of
    [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor).
    The image processor is a required input.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`（`LayoutLMv2ImageProcessor`，*可选*） — [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)的实例。图像处理器是必需的输入。'
- en: '`tokenizer` (`LayoutXLMTokenizer` or `LayoutXLMTokenizerFast`, *optional*)
    — An instance of [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)
    or [LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast).
    The tokenizer is a required input.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`LayoutXLMTokenizer`或`LayoutXLMTokenizerFast`，*可选*） — [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)或[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)的实例。标记器是必需的输入。'
- en: Constructs a LayoutXLM processor which combines a LayoutXLM image processor
    and a LayoutXLM tokenizer into a single processor.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个LayoutXLM处理器，将LayoutXLM图像处理器和LayoutXLM标记器组合成一个单一处理器。
- en: '[LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)
    offers all the functionalities you need to prepare data for the model.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)提供了准备模型数据所需的所有功能。'
- en: It first uses [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    to resize document images to a fixed size, and optionally applies OCR to get words
    and normalized bounding boxes. These are then provided to [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)
    or [LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast),
    which turns the words and bounding boxes into token-level `input_ids`, `attention_mask`,
    `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which
    are turned into token-level `labels` for token classification tasks (such as FUNSD,
    CORD).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 它首先使用[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)将文档图像调整为固定大小，并可选择应用OCR以获取单词和归一化边界框。然后将它们提供给[LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)或[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)，将单词和边界框转换为标记级别的`input_ids`、`attention_mask`、`token_type_ids`、`bbox`。可选地，可以提供整数`word_labels`，这些标签将转换为用于标记分类任务（如FUNSD、CORD）的标记级别`labels`。
- en: '#### `__call__`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L67)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L67)'
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This method first forwards the `images` argument to `~LayoutLMv2ImagePrpcessor.__call__`.
    In case `LayoutLMv2ImagePrpcessor` was initialized with `apply_ocr` set to `True`,
    it passes the obtained words and bounding boxes along with the additional arguments
    to [**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)
    and returns the output, together with resized `images`. In case `LayoutLMv2ImagePrpcessor`
    was initialized with `apply_ocr` set to `False`, it passes the words (`text`/`text_pair`)
    and `boxes` specified by the user along with the additional arguments to [__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)
    and returns the output, together with resized `images`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法首先将`images`参数转发给`~LayoutLMv2ImagePrpcessor.__call__`。如果`LayoutLMv2ImagePrpcessor`初始化时`apply_ocr`设置为`True`，它将获取的单词和边界框以及其他参数传递给[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)并返回输出，同时返回调整大小后的`images`。如果`LayoutLMv2ImagePrpcessor`初始化时`apply_ocr`设置为`False`，它将用户指定的单词（`text`/`text_pair`）和`boxes`以及其他参数传递给[__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)并返回输出，同时返回调整大小后的`images`。
- en: Please refer to the docstring of the above two methods for more information.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息请参考上述两种方法的文档字符串。
