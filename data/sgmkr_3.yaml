- en: Run training on Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨Amazon SageMakerä¸Šè¿è¡Œè®­ç»ƒ
- en: 'Original text: [https://huggingface.co/docs/sagemaker/train](https://huggingface.co/docs/sagemaker/train)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/sagemaker/train](https://huggingface.co/docs/sagemaker/train)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/embed/ok3hetb42gU](https://www.youtube.com/embed/ok3hetb42gU)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/ok3hetb42gU](https://www.youtube.com/embed/ok3hetb42gU)'
- en: 'This guide will show you how to train a ğŸ¤— Transformers model with the `HuggingFace`
    SageMaker Python SDK. Learn how to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨`HuggingFace` SageMaker Python SDKè®­ç»ƒä¸€ä¸ªğŸ¤— Transformersæ¨¡å‹ã€‚å­¦ä¹ å¦‚ä½•ï¼š
- en: '[Install and setup your training environment](#installation-and-setup).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å®‰è£…å’Œè®¾ç½®æ‚¨çš„è®­ç»ƒç¯å¢ƒ](#installation-and-setup)ã€‚'
- en: '[Prepare a training script](#prepare-a-transformers-fine-tuning-script).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‡†å¤‡ä¸€ä¸ªè®­ç»ƒè„šæœ¬](#prepare-a-transformers-fine-tuning-script)ã€‚'
- en: '[Create a Hugging Face Estimator](#create-a-hugging-face-estimator).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åˆ›å»ºä¸€ä¸ªHugging Face Estimator](#create-a-hugging-face-estimator)ã€‚'
- en: '[Run training with the `fit` method](#execute-training).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨`fit`æ–¹æ³•è¿è¡Œè®­ç»ƒ](#execute-training)ã€‚'
- en: '[Access your trained model](#access-trained-model).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è®¿é—®æ‚¨è®­ç»ƒçš„æ¨¡å‹](#access-trained-model)ã€‚'
- en: '[Perform distributed training](#distributed-training).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ](#distributed-training)ã€‚'
- en: '[Create a spot instance](#spot-instances).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åˆ›å»ºä¸€ä¸ªspotå®ä¾‹](#spot-instances)ã€‚'
- en: '[Load a training script from a GitHub repository](#git-repository).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä»GitHubå­˜å‚¨åº“åŠ è½½è®­ç»ƒè„šæœ¬](#git-repository)ã€‚'
- en: '[Collect training metrics](#sagemaker-metrics).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ”¶é›†è®­ç»ƒæŒ‡æ ‡](#sagemaker-metrics)ã€‚'
- en: Installation and setup
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®‰è£…å’Œè®¾ç½®
- en: Before you can train a ğŸ¤— Transformers model with SageMaker, you need to sign
    up for an AWS account. If you donâ€™t have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨å¯ä»¥ä½¿ç”¨SageMakerè®­ç»ƒğŸ¤— Transformersæ¨¡å‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦æ³¨å†ŒAWSè´¦æˆ·ã€‚å¦‚æœæ‚¨è¿˜æ²¡æœ‰AWSè´¦æˆ·ï¼Œè¯·åœ¨[è¿™é‡Œ](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)äº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: 'Once you have an AWS account, get started using one of the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨æ‹¥æœ‰AWSè´¦æˆ·ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€å¼€å§‹ä½¿ç”¨ï¼š
- en: '[SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)'
- en: '[SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMakerç¬”è®°æœ¬å®ä¾‹](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)'
- en: Local environment
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ¬åœ°ç¯å¢ƒ
- en: To start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æœ¬åœ°å¼€å§‹è®­ç»ƒï¼Œæ‚¨éœ€è¦è®¾ç½®é€‚å½“çš„[IAMè§’è‰²](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)ã€‚
- en: 'Upgrade to the latest `sagemaker` version:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å‡çº§åˆ°æœ€æ–°çš„`sagemaker`ç‰ˆæœ¬ï¼š
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**SageMaker environment**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMakerç¯å¢ƒ**'
- en: 'Setup your SageMaker environment as shown below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§ä¸‹é¢æ‰€ç¤ºè®¾ç½®æ‚¨çš„SageMakerç¯å¢ƒï¼š
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Note: The execution role is only available when running a notebook within
    SageMaker. If you run `get_execution_role` in a notebook not on SageMaker, expect
    a `region` error.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šæ‰§è¡Œè§’è‰²ä»…åœ¨SageMakerå†…è¿è¡Œç¬”è®°æœ¬æ—¶å¯ç”¨ã€‚å¦‚æœåœ¨éSageMakerç¬”è®°æœ¬ä¸­è¿è¡Œ`get_execution_role`ï¼Œåˆ™ä¼šå‡ºç°`region`é”™è¯¯ã€‚*'
- en: '**Local environment**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**æœ¬åœ°ç¯å¢ƒ**'
- en: 'Setup your local environment as shown below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§ä¸‹é¢æ‰€ç¤ºè®¾ç½®æ‚¨çš„æœ¬åœ°ç¯å¢ƒï¼š
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Prepare a ğŸ¤— Transformers fine-tuning script
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡†å¤‡ä¸€ä¸ªğŸ¤— Transformerså¾®è°ƒè„šæœ¬
- en: 'Our training script is very similar to a training script you might run outside
    of SageMaker. However, you can access useful properties about the training environment
    through various environment variables (see [here](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md)
    for a complete list), such as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬ä¸æ‚¨å¯èƒ½åœ¨SageMakerä¹‹å¤–è¿è¡Œçš„è®­ç»ƒè„šæœ¬éå¸¸ç›¸ä¼¼ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡å„ç§ç¯å¢ƒå˜é‡è®¿é—®æœ‰å…³è®­ç»ƒç¯å¢ƒçš„æœ‰ç”¨å±æ€§ï¼ˆè¯·å‚é˜…[è¿™é‡Œ](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md)è·å–å®Œæ•´åˆ—è¡¨ï¼‰ï¼Œä¾‹å¦‚ï¼š
- en: '`SM_MODEL_DIR`: A string representing the path to which the training job writes
    the model artifacts. After training, artifacts in this directory are uploaded
    to S3 for model hosting. `SM_MODEL_DIR` is always set to `/opt/ml/model`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SM_MODEL_DIR`: ä¸€ä¸ªè¡¨ç¤ºè®­ç»ƒä½œä¸šå†™å…¥æ¨¡å‹å·¥ä»¶çš„è·¯å¾„çš„å­—ç¬¦ä¸²ã€‚è®­ç»ƒåï¼Œæ­¤ç›®å½•ä¸­çš„å·¥ä»¶å°†ä¸Šä¼ åˆ°S3ä»¥è¿›è¡Œæ¨¡å‹æ‰˜ç®¡ã€‚`SM_MODEL_DIR`å§‹ç»ˆè®¾ç½®ä¸º`/opt/ml/model`ã€‚'
- en: '`SM_NUM_GPUS`: An integer representing the number of GPUs available to the
    host.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SM_NUM_GPUS`: ä¸€ä¸ªè¡¨ç¤ºä¸»æœºå¯ç”¨GPUæ•°é‡çš„æ•´æ•°ã€‚'
- en: '`SM_CHANNEL_XXXX:` A string representing the path to the directory that contains
    the input data for the specified channel. For example, when you specify `train`
    and `test` in the Hugging Face Estimator `fit` method, the environment variables
    are set to `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SM_CHANNEL_XXXX:` ä¸€ä¸ªè¡¨ç¤ºåŒ…å«æŒ‡å®šé€šé“è¾“å…¥æ•°æ®çš„ç›®å½•è·¯å¾„çš„å­—ç¬¦ä¸²ã€‚ä¾‹å¦‚ï¼Œå½“æ‚¨åœ¨Hugging Face Estimatorçš„`fit`æ–¹æ³•ä¸­æŒ‡å®š`train`å’Œ`test`æ—¶ï¼Œç¯å¢ƒå˜é‡è®¾ç½®ä¸º`SM_CHANNEL_TRAIN`å’Œ`SM_CHANNEL_TEST`ã€‚'
- en: The `hyperparameters` defined in the [Hugging Face Estimator](#create-an-huggingface-estimator)
    are passed as named arguments and processed by `ArgumentParser()`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[Hugging Face Estimator](#create-an-huggingface-estimator)ä¸­å®šä¹‰çš„`hyperparameters`ä½œä¸ºå‘½åå‚æ•°ä¼ é€’ï¼Œå¹¶ç”±`ArgumentParser()`å¤„ç†ã€‚
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Note that SageMaker doesnâ€™t support argparse actions. For example, if you
    want to use a boolean hyperparameter, specify `type` as `bool` in your script
    and provide an explicit `True` or `False` value.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¯·æ³¨æ„ï¼ŒSageMakerä¸æ”¯æŒargparseæ“ä½œã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨å¸ƒå°”è¶…å‚æ•°ï¼Œè¯·åœ¨è„šæœ¬ä¸­å°†`type`æŒ‡å®šä¸º`bool`å¹¶æä¾›æ˜ç¡®çš„`True`æˆ–`False`å€¼ã€‚*'
- en: Look [here](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py)
    for a complete example of a ğŸ¤— Transformers training script.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[è¿™é‡Œ](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py)å®Œæ•´çš„ğŸ¤—
    Transformersè®­ç»ƒè„šæœ¬ç¤ºä¾‹ã€‚
- en: Training Output Management
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¾“å‡ºç®¡ç†
- en: If `output_dir` in the `TrainingArguments` is set to â€˜/opt/ml/modelâ€™ the Trainer
    saves all training artifacts, including logs, checkpoints, and models. Amazon
    SageMaker archives the whole â€˜/opt/ml/modelâ€™ directory as `model.tar.gz` and uploads
    it at the end of the training job to Amazon S3\. Depending on your Hyperparameters
    and `TrainingArguments` this could lead to a large artifact (> 5GB), which can
    slow down deployment for Amazon SageMaker Inference. You can control how checkpoints,
    logs, and artifacts are saved by customization the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments).
    For example by providing `save_total_limit` as `TrainingArgument` you can control
    the limit of the total amount of checkpoints. Deletes the older checkpoints in
    `output_dir` if new ones are saved and the maximum limit is reached.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ `TrainingArguments` ä¸­çš„ `output_dir` è®¾ç½®ä¸º â€˜/opt/ml/modelâ€™ï¼Œåˆ™ Trainer ä¼šä¿å­˜æ‰€æœ‰è®­ç»ƒå·¥ä»¶ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€æ£€æŸ¥ç‚¹å’Œæ¨¡å‹ã€‚Amazon
    SageMaker å°†æ•´ä¸ª â€˜/opt/ml/modelâ€™ ç›®å½•å­˜æ¡£ä¸º `model.tar.gz`ï¼Œå¹¶åœ¨è®­ç»ƒä½œä¸šç»“æŸæ—¶å°†å…¶ä¸Šä¼ åˆ° Amazon S3ã€‚æ ¹æ®æ‚¨çš„è¶…å‚æ•°å’Œ
    `TrainingArguments`ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸€ä¸ªå¤§çš„å·¥ä»¶ï¼ˆ> 5GBï¼‰ï¼Œè¿™å¯èƒ½ä¼šå‡æ…¢ Amazon SageMaker æ¨ç†çš„éƒ¨ç½²é€Ÿåº¦ã€‚æ‚¨å¯ä»¥é€šè¿‡è‡ªå®šä¹‰
    [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)
    æ§åˆ¶æ£€æŸ¥ç‚¹ã€æ—¥å¿—å’Œå·¥ä»¶çš„ä¿å­˜æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡æä¾› `save_total_limit` ä½œä¸º `TrainingArgument`ï¼Œæ‚¨å¯ä»¥æ§åˆ¶æ£€æŸ¥ç‚¹çš„æ€»é‡é™åˆ¶ã€‚å¦‚æœä¿å­˜äº†æ–°çš„æ£€æŸ¥ç‚¹å¹¶ä¸”è¾¾åˆ°äº†æœ€å¤§é™åˆ¶ï¼Œåˆ™åˆ é™¤
    `output_dir` ä¸­çš„æ—§æ£€æŸ¥ç‚¹ã€‚
- en: 'In addition to the options already mentioned above, there is another option
    to save the training artifacts during the training session. Amazon SageMaker supports
    [Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html),
    which allows you to continuously save your artifacts during training to Amazon
    S3 rather than at the end of your training. To enable [Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)
    you need to provide the `checkpoint_s3_uri` parameter pointing to an Amazon S3
    location in the `HuggingFace` estimator and set `output_dir` to `/opt/ml/checkpoints`.
    *Note: If you set `output_dir` to `/opt/ml/checkpoints` make sure to call `trainer.save_model("/opt/ml/model")`
    or model.save_pretrained(â€œ/opt/ml/modelâ€)/`tokenizer.save_pretrained("/opt/ml/model")`
    at the end of your training to be able to deploy your model seamlessly to Amazon
    SageMaker for Inference.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä¸Šé¢å·²ç»æåˆ°çš„é€‰é¡¹ä¹‹å¤–ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªé€‰é¡¹å¯ä»¥åœ¨è®­ç»ƒä¼šè¯æœŸé—´ä¿å­˜è®­ç»ƒå·¥ä»¶ã€‚Amazon SageMaker æ”¯æŒ[Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)ï¼Œå…è®¸æ‚¨åœ¨è®­ç»ƒæœŸé—´æŒç»­å°†æ‚¨çš„å·¥ä»¶ä¿å­˜åˆ°
    Amazon S3ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒç»“æŸæ—¶ä¿å­˜ã€‚è¦å¯ç”¨[Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)ï¼Œæ‚¨éœ€è¦æä¾›æŒ‡å‘
    Amazon S3 ä½ç½®çš„ `checkpoint_s3_uri` å‚æ•°ï¼Œè¯¥å‚æ•°ä½äº `HuggingFace` estimator ä¸­ï¼Œå¹¶å°† `output_dir`
    è®¾ç½®ä¸º `/opt/ml/checkpoints`ã€‚*æ³¨æ„ï¼šå¦‚æœå°† `output_dir` è®¾ç½®ä¸º `/opt/ml/checkpoints`ï¼Œè¯·ç¡®ä¿åœ¨è®­ç»ƒç»“æŸæ—¶è°ƒç”¨
    `trainer.save_model("/opt/ml/model")` æˆ– model.save_pretrained(â€œ/opt/ml/modelâ€)/`tokenizer.save_pretrained("/opt/ml/model")`ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ— ç¼åœ°å°†æ‚¨çš„æ¨¡å‹éƒ¨ç½²åˆ°
    Amazon SageMaker è¿›è¡Œæ¨ç†ã€‚*
- en: Create a Hugging Face Estimator
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ª Hugging Face Estimator
- en: 'Run ğŸ¤— Transformers training scripts on SageMaker by creating a [Hugging Face
    Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator).
    The Estimator handles end-to-end SageMaker training. There are several parameters
    you should define in the Estimator:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åˆ›å»º[Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator)åœ¨
    SageMaker ä¸Šè¿è¡Œ ğŸ¤— Transformers è®­ç»ƒè„šæœ¬ã€‚Estimator å¤„ç†ç«¯åˆ°ç«¯çš„ SageMaker è®­ç»ƒã€‚æ‚¨åº”è¯¥åœ¨ Estimator
    ä¸­å®šä¹‰å‡ ä¸ªå‚æ•°ï¼š
- en: '`entry_point` specifies which fine-tuning script to use.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`entry_point` æŒ‡å®šè¦ä½¿ç”¨çš„å¾®è°ƒè„šæœ¬ã€‚'
- en: '`instance_type` specifies an Amazon instance to launch. Refer [here](https://aws.amazon.com/sagemaker/pricing/)
    for a complete list of instance types.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`instance_type` æŒ‡å®šè¦å¯åŠ¨çš„ Amazon å®ä¾‹ã€‚è¯·å‚è€ƒ[è¿™é‡Œ](https://aws.amazon.com/sagemaker/pricing/)è·å–å®Œæ•´çš„å®ä¾‹ç±»å‹åˆ—è¡¨ã€‚'
- en: '`hyperparameters` specifies training hyperparameters. View additional available
    hyperparameters [here](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`hyperparameters` æŒ‡å®šè®­ç»ƒè¶…å‚æ•°ã€‚æŸ¥çœ‹å…¶ä»–å¯ç”¨çš„è¶…å‚æ•°[è¿™é‡Œ](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py)ã€‚'
- en: 'The following code sample shows how to train with a custom script `train.py`
    with three hyperparameters (`epochs`, `per_device_train_batch_size`, and `model_name_or_path`):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç ç¤ºä¾‹æ˜¾ç¤ºå¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰è„šæœ¬ `train.py` è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªè¶…å‚æ•°ï¼ˆ`epochs`ã€`per_device_train_batch_size`
    å’Œ `model_name_or_path`ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you are running a `TrainingJob` locally, define `instance_type='local'` or
    `instance_type='local_gpu'` for GPU usage. Note that this will not work with SageMaker
    Studio.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨æœ¬åœ°è¿è¡Œ `TrainingJob`ï¼Œè¯·å®šä¹‰ `instance_type='local'` æˆ– `instance_type='local_gpu'`
    ä»¥ä½¿ç”¨ GPUã€‚è¯·æ³¨æ„ï¼Œè¿™åœ¨ SageMaker Studio ä¸­ä¸èµ·ä½œç”¨ã€‚
- en: Execute training
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰§è¡Œè®­ç»ƒ
- en: 'Start your `TrainingJob` by calling `fit` on a Hugging Face Estimator. Specify
    your input training data in `fit`. The input training data can be a:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨ Hugging Face Estimator ä¸Šè°ƒç”¨ `fit` æ¥å¯åŠ¨æ‚¨çš„ `TrainingJob`ã€‚åœ¨ `fit` ä¸­æŒ‡å®šæ‚¨çš„è¾“å…¥è®­ç»ƒæ•°æ®ã€‚è¾“å…¥è®­ç»ƒæ•°æ®å¯ä»¥æ˜¯ï¼š
- en: S3 URI such as `s3://my-bucket/my-training-data`.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3 URIï¼Œä¾‹å¦‚ `s3://my-bucket/my-training-data`ã€‚
- en: '`FileSystemInput` for Amazon Elastic File System or FSx for Lustre. See [here](https://sagemaker.readthedocs.io/en/stable/overview.html?highlight=FileSystemInput#use-file-systems-as-training-inputs)
    for more details about using these file systems as input.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº Amazon å¼¹æ€§æ–‡ä»¶ç³»ç»Ÿæˆ– FSx for Lustreï¼Œè¿è¡Œ `FileSystemInput`ã€‚è¯·å‚è€ƒ[è¿™é‡Œ](https://sagemaker.readthedocs.io/en/stable/overview.html?highlight=FileSystemInput#use-file-systems-as-training-inputs)è·å–æœ‰å…³å°†è¿™äº›æ–‡ä»¶ç³»ç»Ÿç”¨ä½œè¾“å…¥çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: 'Call `fit` to begin training:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ `fit` å¼€å§‹è®­ç»ƒï¼š
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'SageMaker starts and manages all the required EC2 instances and initiates the
    `TrainingJob` by running:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker å¯åŠ¨å¹¶ç®¡ç†æ‰€æœ‰å¿…éœ€çš„ EC2 å®ä¾‹ï¼Œå¹¶é€šè¿‡è¿è¡Œä»¥ä¸‹å†…å®¹æ¥å¯åŠ¨ `TrainingJob`ï¼š
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Access trained model
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¿é—®è®­ç»ƒæ¨¡å‹
- en: Once training is complete, you can access your model through the [AWS console](https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-signin)
    or download it directly from S3.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥é€šè¿‡[AWS æ§åˆ¶å°](https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-signin)è®¿é—®æ‚¨çš„æ¨¡å‹ï¼Œæˆ–ç›´æ¥ä»
    S3 ä¸‹è½½ã€‚
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Distributed training
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒ
- en: 'SageMaker provides two strategies for distributed training: data parallelism
    and model parallelism. Data parallelism splits a training set across several GPUs,
    while model parallelism splits a model across several GPUs.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: SageMakeræä¾›äº†ä¸¤ç§åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼šæ•°æ®å¹¶è¡Œismå’Œæ¨¡å‹å¹¶è¡Œismã€‚æ•°æ®å¹¶è¡Œismå°†è®­ç»ƒé›†åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Šï¼Œè€Œæ¨¡å‹å¹¶è¡Œismå°†æ¨¡å‹åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Šã€‚
- en: Data parallelism
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œism
- en: 'The Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    supports SageMakerâ€™s data parallelism library. If your training script uses the
    Trainer API, you only need to define the distribution parameter in the Hugging
    Face Estimator:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Faceçš„[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)æ”¯æŒSageMakerçš„æ•°æ®å¹¶è¡Œismåº“ã€‚å¦‚æœæ‚¨çš„è®­ç»ƒè„šæœ¬ä½¿ç”¨Trainer
    APIï¼Œæ‚¨åªéœ€è¦åœ¨Hugging Face Estimatorä¸­å®šä¹‰åˆ†å¸ƒå‚æ•°ï¼š
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ğŸ““ Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    for an example of how to run the data parallelism library with TensorFlow.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ““ æ‰“å¼€[notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb)ä»¥æŸ¥çœ‹å¦‚ä½•ä½¿ç”¨TensorFlowè¿è¡Œæ•°æ®å¹¶è¡Œismåº“çš„ç¤ºä¾‹ã€‚
- en: Model parallelism
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œism
- en: 'The Hugging Face [Trainer] also supports SageMakerâ€™s model parallelism library.
    If your training script uses the Trainer API, you only need to define the distribution
    parameter in the Hugging Face Estimator (see [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=modelparallel#required-sagemaker-python-sdk-parameters)
    for more detailed information about using model parallelism):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Faceçš„[Trainer]è¿˜æ”¯æŒSageMakerçš„æ¨¡å‹å¹¶è¡Œismåº“ã€‚å¦‚æœæ‚¨çš„è®­ç»ƒè„šæœ¬ä½¿ç”¨Trainer APIï¼Œæ‚¨åªéœ€è¦åœ¨Hugging
    Face Estimatorä¸­å®šä¹‰åˆ†å¸ƒå‚æ•°ï¼ˆæœ‰å…³ä½¿ç”¨æ¨¡å‹å¹¶è¡Œismçš„æ›´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[æ­¤å¤„](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=modelparallel#required-sagemaker-python-sdk-parameters)ï¼‰ï¼š
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ğŸ““ Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb)
    for an example of how to run the model parallelism library.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ““ æ‰“å¼€[notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb)ä»¥æŸ¥çœ‹å¦‚ä½•è¿è¡Œæ¨¡å‹å¹¶è¡Œismåº“çš„ç¤ºä¾‹ã€‚
- en: Spot instances
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spotå®ä¾‹
- en: The Hugging Face extension for the SageMaker Python SDK means we can benefit
    from [fully-managed EC2 spot instances](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html).
    This can help you save up to 90% of training costs!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Faceæ‰©å±•äº†SageMaker Python SDKï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä»[å®Œå…¨æ‰˜ç®¡çš„EC2 spotå®ä¾‹](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)ä¸­å—ç›Šã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨èŠ‚çœé«˜è¾¾90%çš„è®­ç»ƒæˆæœ¬ï¼
- en: '*Note: Unless your training job completes quickly, we recommend you use [checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)
    with managed spot training. In this case, you need to define the `checkpoint_s3_uri`.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šé™¤éæ‚¨çš„è®­ç»ƒä½œä¸šå®Œæˆå¾—å¾ˆå¿«ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨[checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)ä¸æ‰˜ç®¡çš„spotè®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦å®šä¹‰`checkpoint_s3_uri`ã€‚*'
- en: 'Set `use_spot_instances=True` and define your `max_wait` and `max_run` time
    in the Estimator to use spot instances:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®`use_spot_instances=True`å¹¶åœ¨Estimatorä¸­å®šä¹‰æ‚¨çš„`max_wait`å’Œ`max_run`æ—¶é—´ä»¥ä½¿ç”¨spotå®ä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ğŸ““ Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb)
    for an example of how to use spot instances.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ““ æ‰“å¼€[notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb)ä»¥æŸ¥çœ‹å¦‚ä½•ä½¿ç”¨spotå®ä¾‹çš„ç¤ºä¾‹ã€‚
- en: Git repository
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gitå­˜å‚¨åº“
- en: The Hugging Face Estimator can load a training script [stored in a GitHub repository](https://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repository).
    Provide the relative path to the training script in `entry_point` and the relative
    path to the directory in `source_dir`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Estimatorå¯ä»¥åŠ è½½å­˜å‚¨åœ¨GitHubå­˜å‚¨åº“ä¸­çš„è®­ç»ƒè„šæœ¬ï¼ˆhttps://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repositoryï¼‰ã€‚åœ¨`entry_point`ä¸­æä¾›è®­ç»ƒè„šæœ¬çš„ç›¸å¯¹è·¯å¾„ï¼Œåœ¨`source_dir`ä¸­æä¾›ç›®å½•çš„ç›¸å¯¹è·¯å¾„ã€‚
- en: If you are using `git_config` to run the [ğŸ¤— Transformers example scripts](https://github.com/huggingface/transformers/tree/main/examples),
    you need to configure the correct `'branch'` in `transformers_version` (e.g. if
    you use `transformers_version='4.4.2` you have to use `'branch':'v4.4.2'`).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨`git_config`æ¥è¿è¡Œ[ğŸ¤— Transformersç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)ï¼Œæ‚¨éœ€è¦åœ¨`transformers_version`ä¸­é…ç½®æ­£ç¡®çš„`'branch'`ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨`transformers_version='4.4.2'`ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨`'branch':'v4.4.2'`ï¼‰ã€‚
- en: '*Tip: Save your model to S3 by setting `output_dir=/opt/ml/model` in the hyperparameter
    of your training script.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*æç¤ºï¼šé€šè¿‡åœ¨è®­ç»ƒè„šæœ¬çš„è¶…å‚æ•°ä¸­è®¾ç½®`output_dir=/opt/ml/model`å°†æ‚¨çš„æ¨¡å‹ä¿å­˜åˆ°S3ä¸­ã€‚*'
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: SageMaker metrics
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMakeræŒ‡æ ‡
- en: '[SageMaker metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics)
    automatically parses training job logs for metrics and sends them to CloudWatch.
    If you want SageMaker to parse the logs, you must specify the metricâ€™s name and
    a regular expression for SageMaker to use to find the metric.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[SageMakeræŒ‡æ ‡](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics)è‡ªåŠ¨è§£æè®­ç»ƒä½œä¸šæ—¥å¿—ä»¥è·å–æŒ‡æ ‡å¹¶å°†å…¶å‘é€åˆ°CloudWatchã€‚å¦‚æœæ‚¨å¸Œæœ›SageMakerè§£ææ—¥å¿—ï¼Œæ‚¨å¿…é¡»æŒ‡å®šæŒ‡æ ‡çš„åç§°å’ŒSageMakerç”¨äºæŸ¥æ‰¾æŒ‡æ ‡çš„æ­£åˆ™è¡¨è¾¾å¼ã€‚'
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ğŸ““ Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb)
    for an example of how to capture metrics in SageMaker.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ““ æ‰“å¼€[notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb)ä»¥æŸ¥çœ‹å¦‚ä½•åœ¨SageMakerä¸­æ•è·æŒ‡æ ‡çš„ç¤ºä¾‹ã€‚
