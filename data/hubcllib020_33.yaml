- en: Inference Endpoints
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理终端
- en: 'Original text: [https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints](https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints](https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Inference Endpoints provides a secure production solution to easily deploy models
    on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference
    Endpoint is built from a model from the [Hub](https://huggingface.co/models).
    This page is a reference for `huggingface_hub`’s integration with Inference Endpoints.
    For more information about the Inference Endpoints product, check out its [official
    documentation](https://huggingface.co/docs/inference-endpoints/index).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 推理终端提供了一个安全的生产解决方案，可以轻松部署模型到由Hugging Face管理的专用和自动缩放基础设施上。推理终端是从[Hub](https://huggingface.co/models)中的模型构建的。本页是关于`huggingface_hub`与推理终端集成的参考。有关推理终端产品的更多信息，请查看其[官方文档](https://huggingface.co/docs/inference-endpoints/index)。
- en: Check out the [related guide](../guides/inference_endpoints) to learn how to
    use `huggingface_hub` to manage your Inference Endpoints programmatically.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[相关指南](../guides/inference_endpoints)以了解如何使用`huggingface_hub`以编程方式管理您的推理终端。
- en: Inference Endpoints can be fully managed via API. The endpoints are documented
    with [Swagger](https://api.endpoints.huggingface.cloud/). The [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    class is a simple wrapper built on top on this API.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 推理终端可以通过API完全管理。终端的文档在[Swagger](https://api.endpoints.huggingface.cloud/)中记录。[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)类是在此API之上构建的简单包装器。
- en: Methods
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: 'A subset of the Inference Endpoint features are implemented in [HfApi](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 推理终端的一部分功能在[HfApi](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi)中实现：
- en: '[get_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.get_inference_endpoint)
    and [list_inference_endpoints()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.list_inference_endpoints)
    to get information about your Inference Endpoints'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[get_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.get_inference_endpoint)和[list_inference_endpoints()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.list_inference_endpoints)用于获取有关您的推理终端的信息'
- en: '[create_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint),
    [update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint)
    and [delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint)
    to deploy and manage Inference Endpoints'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[create_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint)、[update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint)和[delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint)用于部署和管理推理终端'
- en: '[pause_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint)
    and [resume_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint)
    to pause and resume an Inference Endpoint'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[pause_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint)和[resume_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint)用于暂停和恢复推理终端'
- en: '[scale_to_zero_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint)
    to manually scale an Endpoint to 0 replicas'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scale_to_zero_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint)用于手动将终端缩放为0个副本'
- en: InferenceEndpoint
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理终端
- en: The main dataclass is [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint).
    It contains information about a deployed `InferenceEndpoint`, including its configuration
    and current state. Once deployed, you can run inference on the Endpoint using
    the [InferenceEndpoint.client](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.client)
    and [InferenceEndpoint.async_client](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.async_client)
    properties that respectively return an [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    and an [AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)
    object.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的数据类是[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)。它包含有关部署的`InferenceEndpoint`的信息，包括其配置和当前状态。部署后，您可以使用[InferenceEndpoint.client](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.client)和[InferenceEndpoint.async_client](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.async_client)属性在终端上运行推理，分别返回一个[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)和一个[AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)对象。
- en: '### `class huggingface_hub.InferenceEndpoint`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class huggingface_hub.InferenceEndpoint`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L44)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L44)'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`name` (`str`) — The unique name of the Inference Endpoint.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str`) — 推理终端的唯一名称。'
- en: '`namespace` (`str`) — The namespace where the Inference Endpoint is located.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespace` (`str`) — 推理终端所在的命名空间。'
- en: '`repository` (`str`) — The name of the model repository deployed on this Inference
    Endpoint.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repository` (`str`) — 部署在此推理终端上的模型库的名称。'
- en: '`status` ([InferenceEndpointStatus](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointStatus))
    — The current status of the Inference Endpoint.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`status` ([InferenceEndpointStatus](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointStatus))
    — 推理端点的当前状态。'
- en: '`url` (`str`, *optional*) — The URL of the Inference Endpoint, if available.
    Only a deployed Inference Endpoint will have a URL.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`url` (`str`, *可选*) — 推理端点的URL，如果可用。只有已部署的推理端点才会有URL。'
- en: '`framework` (`str`) — The machine learning framework used for the model.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`) — 用于模型的机器学习框架。'
- en: '`revision` (`str`) — The specific model revision deployed on the Inference
    Endpoint.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`) — 部署在推理端点上的特定模型修订版。'
- en: '`task` (`str`) — The task associated with the deployed model.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`) — 与部署模型相关联的任务。'
- en: '`created_at` (`datetime.datetime`) — The timestamp when the Inference Endpoint
    was created.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`created_at` (`datetime.datetime`) — 推理端点创建时的时间戳。'
- en: '`updated_at` (`datetime.datetime`) — The timestamp of the last update of the
    Inference Endpoint.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`updated_at` (`datetime.datetime`) — 推理端点的最后更新时间戳。'
- en: '`type` ([InferenceEndpointType](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointType))
    — The type of the Inference Endpoint (public, protected, private).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type` ([InferenceEndpointType](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointType))
    — 推理端点的类型（公共、受保护、私有）。'
- en: '`raw` (`Dict`) — The raw dictionary data returned from the API.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw` (`Dict`) — 从API返回的原始字典数据。'
- en: '`token` (`str`, *optional*) — Authentication token for the Inference Endpoint,
    if set when requesting the API.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str`, *可选*) — 推理端点的身份验证令牌，如果在请求API时设置。'
- en: Contains information about a deployed Inference Endpoint.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 包含有关部署的推理端点的信息。
- en: 'Example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `from_raw`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_raw`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L126)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L126)'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Initialize object from raw dictionary.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始字典初始化对象。
- en: '#### `client`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `client`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L145)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L145)'
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Returns
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceClient](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_client#huggingface_hub.InferenceClient)'
- en: an inference client pointing to the deployed endpoint.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 指向已部署端点的推理客户端。
- en: Raises
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 引发
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)'
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)
    — If the Inference Endpoint is not yet deployed.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)
    — 如果推理端点尚未部署。'
- en: Returns a client to make predictions on this Inference Endpoint.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个客户端，用于对此推理端点进行预测。
- en: '#### `async_client`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `async_client`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L162)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L162)'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Returns
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)'
- en: an asyncio-compatible inference client pointing to the deployed endpoint.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 指向已部署端点的与asyncio兼容的推理客户端。
- en: Raises
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 引发
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)'
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)
    — If the Inference Endpoint is not yet deployed.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)
    — 如果推理端点尚未部署。'
- en: Returns a client to make predictions on this Inference Endpoint.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个客户端，用于对此推理端点进行预测。
- en: '#### `delete`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `delete`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L346)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L346)'
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Delete the Inference Endpoint.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 删除推理端点。
- en: This operation is not reversible. If you don’t want to be charged for an Inference
    Endpoint, it is preferable to pause it with [InferenceEndpoint.pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause)
    or scale it to zero with [InferenceEndpoint.scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作不可逆。如果您不想为推理端点付费，最好使用[InferenceEndpoint.pause()](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause)暂停它，或者使用[InferenceEndpoint.scale_to_zero()](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero)将其缩减为零。
- en: This is an alias for [HfApi.delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[HfApi.delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/zh/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint)的别名。
- en: '#### `fetch`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `fetch`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L217)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L217)'
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Returns
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/zh/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
- en: the same Inference Endpoint, mutated in place with the latest data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的推理端点，在原地变异为最新数据。
- en: Fetch latest information about the Inference Endpoint.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 获取有关推理端点的最新信息。
- en: '#### `pause`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pause`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L296)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L296)'
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Returns
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
- en: the same Inference Endpoint, mutated in place with the latest data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的推理端点，在原地改变以获取最新数据。
- en: Pause the Inference Endpoint.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 暂停推理端点。
- en: A paused Inference Endpoint will not be charged. It can be resumed at any time
    using [InferenceEndpoint.resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume).
    This is different than scaling the Inference Endpoint to zero with [InferenceEndpoint.scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero),
    which would be automatically restarted when a request is made to it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 暂停的推理端点不会收费。可以随时使用[InferenceEndpoint.resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume)来恢复。这与使用[InferenceEndpoint.scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero)将推理端点缩减到零不同，后者在请求到达时会自动重新启动。
- en: This is an alias for [HfApi.pause_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[HfApi.pause_inference_endpoint()]的别名(/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint)。当前对象会被就地改变，以获取来自服务器的最新数据。
- en: '#### `resume`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `resume`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L314)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L314)'
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Returns
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
- en: the same Inference Endpoint, mutated in place with the latest data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的推理端点，在原地改变以获取最新数据。
- en: Resume the Inference Endpoint.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复推理端点。
- en: This is an alias for [HfApi.resume_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[HfApi.resume_inference_endpoint()]的别名(/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint)。当前对象会被就地改变，以获取来自服务器的最新数据。
- en: '#### `scale_to_zero`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `scale_to_zero`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L328)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L328)'
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Returns
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
- en: the same Inference Endpoint, mutated in place with the latest data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的推理端点，在原地改变以获取最新数据。
- en: Scale Inference Endpoint to zero.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将推理端点缩减到零。
- en: An Inference Endpoint scaled to zero will not be charged. It will be resume
    on the next request to it, with a cold start delay. This is different than pausing
    the Inference Endpoint with [InferenceEndpoint.pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause),
    which would require a manual resume with [InferenceEndpoint.resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减到零的推理端点不会收费。在下一次请求时会恢复，但会有冷启动延迟。这与使用[InferenceEndpoint.pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause)暂停推理端点不同，后者需要使用[InferenceEndpoint.resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume)手动恢复。
- en: This is an alias for [HfApi.scale_to_zero_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[HfApi.scale_to_zero_inference_endpoint()]的别名(/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint)。当前对象会被就地改变，以获取来自服务器的最新数据。
- en: '#### `update`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L228)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L228)'
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`accelerator` (`str`, *optional*) — The hardware accelerator to be used for
    inference (e.g. `"cpu"`).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accelerator` (`str`, *optional*) — 用于推理的硬件加速器（例如 `"cpu"`）。'
- en: '`instance_size` (`str`, *optional*) — The size or type of the instance to be
    used for hosting the model (e.g. `"large"`).'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_size` (`str`, *optional*) — 用于托管模型的实例的大小或类型（例如 `"large"`）。'
- en: '`instance_type` (`str`, *optional*) — The cloud instance type where the Inference
    Endpoint will be deployed (e.g. `"c6i"`).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_type` (`str`, *optional*) — 推理端点将部署的云实例类型（例如 `"c6i"`）。'
- en: '`min_replica` (`int`, *optional*) — The minimum number of replicas (instances)
    to keep running for the Inference Endpoint.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_replica` (`int`, *optional*) — 推理端点保持运行的最小副本（实例）数量。'
- en: '`max_replica` (`int`, *optional*) — The maximum number of replicas (instances)
    to scale to for the Inference Endpoint.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_replica` (`int`, *optional*) — 推理端点扩展到的最大副本（实例）数量。'
- en: '`repository` (`str`, *optional*) — The name of the model repository associated
    with the Inference Endpoint (e.g. `"gpt2"`).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repository` (`str`, *optional*) — 与推理端点关联的模型存储库的名称（例如 `"gpt2"`）。'
- en: '`framework` (`str`, *optional*) — The machine learning framework used for the
    model (e.g. `"custom"`).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework` (`str`, *optional*) — 用于模型的机器学习框架（例如 `"custom"`）。'
- en: '`revision` (`str`, *optional*) — The specific model revision to deploy on the
    Inference Endpoint (e.g. `"6c0e6080953db56375760c0471a8c5f2929baf11"`).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *optional*) — 部署在推理端点上的特定模型修订版（例如 `"6c0e6080953db56375760c0471a8c5f2929baf11"`）。'
- en: '`task` (`str`, *optional*) — The task on which to deploy the model (e.g. `"text-classification"`).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, *optional*) — 部署模型的任务（例如 `"text-classification"`）。'
- en: Returns
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
- en: the same Inference Endpoint, mutated in place with the latest data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的推理端点，就地修改为最新数据。
- en: Update the Inference Endpoint.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 更新推理端点。
- en: This method allows the update of either the compute configuration, the deployed
    model, or both. All arguments are optional but at least one must be provided.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法允许更新计算配置、部署的模型或两者。所有参数都是可选的，但至少必须提供一个。
- en: This is an alias for [HfApi.update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 [HfApi.update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint)
    的别名。当前对象将被就地修改为来自服务器的最新数据。
- en: '#### `wait`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `wait`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L179)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L179)'
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`timeout` (`int`, *optional*) — The maximum time to wait for the Inference
    Endpoint to be deployed, in seconds. If `None`, will wait indefinitely.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout` (`int`, *optional*) — 等待推理端点部署的最长时间，单位为秒。如果为 `None`，将无限期等待。'
- en: '`refresh_every` (`int`, *optional*) — The time to wait between each fetch of
    the Inference Endpoint status, in seconds. Defaults to 5s.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`refresh_every` (`int`, *optional*) — 每次获取推理端点状态之间等待的时间，单位为秒。默认为5秒。'
- en: Returns
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
- en: the same Inference Endpoint, mutated in place with the latest data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的推理端点，就地修改为最新数据。
- en: Wait for the Inference Endpoint to be deployed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 等待推理端点部署。
- en: Information from the server will be fetched every 1s. If the Inference Endpoint
    is not deployed after `timeout` seconds, a `InferenceEndpointTimeoutError` will
    be raised. The [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    will be mutated in place with the latest data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从服务器获取的信息将每1秒刷新一次。如果在 `timeout` 秒后推理端点未部署，将引发 `InferenceEndpointTimeoutError`。[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    将被就地修改为最新数据。
- en: InferenceEndpointStatus
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InferenceEndpointStatus
- en: '### `class huggingface_hub.InferenceEndpointStatus`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class huggingface_hub.InferenceEndpointStatus`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L27)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L27)'
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: An enumeration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: InferenceEndpointType
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InferenceEndpointType
- en: '### `class huggingface_hub.InferenceEndpointType`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class huggingface_hub.InferenceEndpointType`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L38)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L38)'
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: An enumeration.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: InferenceEndpointError
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InferenceEndpointError
- en: '### `class huggingface_hub.InferenceEndpointError`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class huggingface_hub.InferenceEndpointError`'
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L19)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L19)'
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Generic exception when dealing with Inference Endpoints.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 处理推理端点时的通用异常。
