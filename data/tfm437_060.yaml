- en: Benchmarks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/benchmarks](https://huggingface.co/docs/transformers/v4.37.2/en/benchmarks)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/benchmarks](https://huggingface.co/docs/transformers/v4.37.2/en/benchmarks)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Faceâ€™s Benchmarking tools are deprecated and it is advised to use external
    Benchmarking libraries to measure the speed and memory complexity of Transformer
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Faceçš„åŸºå‡†æµ‹è¯•å·¥å…·å·²è¢«å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨å¤–éƒ¨åŸºå‡†æµ‹è¯•åº“æ¥è¡¡é‡Transformeræ¨¡å‹çš„é€Ÿåº¦å’Œå†…å­˜å¤æ‚æ€§ã€‚
- en: Letâ€™s take a look at how ğŸ¤— Transformers models can be benchmarked, best practices,
    and already available benchmarks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæœ€ä½³å®è·µä»¥åŠå·²æœ‰çš„åŸºå‡†æµ‹è¯•ã€‚
- en: A notebook explaining in more detail how to benchmark ğŸ¤— Transformers models
    can be found [here](https://github.com/huggingface/notebooks/tree/main/examples/benchmark.ipynb).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/huggingface/notebooks/tree/main/examples/benchmark.ipynb)æ‰¾åˆ°æ›´è¯¦ç»†è§£é‡Šå¦‚ä½•å¯¹ğŸ¤—
    Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„ç¬”è®°æœ¬ã€‚
- en: How to benchmark ğŸ¤— Transformers models
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•å¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•
- en: The classes `PyTorchBenchmark` and `TensorFlowBenchmark` allow to flexibly benchmark
    ğŸ¤— Transformers models. The benchmark classes allow us to measure the *peak memory
    usage* and *required time* for both *inference* and *training*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorchBenchmark`å’Œ`TensorFlowBenchmark`ç±»å…è®¸çµæ´»åœ°å¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚åŸºå‡†ç±»å…è®¸æˆ‘ä»¬æµ‹é‡*å³°å€¼å†…å­˜ä½¿ç”¨é‡*å’Œ*æ‰€éœ€æ—¶é—´*ï¼Œç”¨äº*æ¨ç†*å’Œ*è®­ç»ƒ*ã€‚'
- en: Hereby, *inference* is defined by a single forward pass, and *training* is defined
    by a single forward pass and backward pass.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ï¼Œ*æ¨ç†*ç”±å•ä¸ªå‰å‘ä¼ é€’å®šä¹‰ï¼Œ*è®­ç»ƒ*ç”±å•ä¸ªå‰å‘ä¼ é€’å’Œåå‘ä¼ é€’å®šä¹‰ã€‚
- en: The benchmark classes `PyTorchBenchmark` and `TensorFlowBenchmark` expect an
    object of type `PyTorchBenchmarkArguments` and `TensorFlowBenchmarkArguments`,
    respectively, for instantiation. `PyTorchBenchmarkArguments` and `TensorFlowBenchmarkArguments`
    are data classes and contain all relevant configurations for their corresponding
    benchmark class. In the following example, it is shown how a BERT model of type
    *bert-base-cased* can be benchmarked.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå‡†ç±»`PyTorchBenchmark`å’Œ`TensorFlowBenchmark`åˆ†åˆ«æœŸæœ›ä½¿ç”¨`PyTorchBenchmarkArguments`å’Œ`TensorFlowBenchmarkArguments`ç±»å‹çš„å¯¹è±¡è¿›è¡Œå®ä¾‹åŒ–ã€‚`PyTorchBenchmarkArguments`å’Œ`TensorFlowBenchmarkArguments`æ˜¯æ•°æ®ç±»ï¼ŒåŒ…å«å…¶å¯¹åº”åŸºå‡†ç±»çš„æ‰€æœ‰ç›¸å…³é…ç½®ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œå±•ç¤ºäº†å¦‚ä½•å¯¹ç±»å‹ä¸º*bert-base-cased*çš„BERTæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚
- en: PytorchHide Pytorch content
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—äº†Pytorchå†…å®¹
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: TensorFlowHide TensorFlow content
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—äº†TensorFlowå†…å®¹
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, three arguments are given to the benchmark argument data classes, namely
    `models`, `batch_sizes`, and `sequence_lengths`. The argument `models` is required
    and expects a `list` of model identifiers from the [model hub](https://huggingface.co/models)
    The `list` arguments `batch_sizes` and `sequence_lengths` define the size of the
    `input_ids` on which the model is benchmarked. There are many more parameters
    that can be configured via the benchmark argument data classes. For more detail
    on these one can either directly consult the files `src/transformers/benchmark/benchmark_args_utils.py`,
    `src/transformers/benchmark/benchmark_args.py` (for PyTorch) and `src/transformers/benchmark/benchmark_args_tf.py`
    (for Tensorflow). Alternatively, running the following shell commands from root
    will print out a descriptive list of all configurable parameters for PyTorch and
    Tensorflow respectively.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼ŒåŸºå‡†å‚æ•°æ•°æ®ç±»ç»™å‡ºäº†ä¸‰ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯`models`ã€`batch_sizes`å’Œ`sequence_lengths`ã€‚å‚æ•°`models`æ˜¯å¿…éœ€çš„ï¼ŒæœŸæœ›ä»[model
    hub](https://huggingface.co/models)ä¸­çš„æ¨¡å‹æ ‡è¯†ç¬¦åˆ—è¡¨ã€‚`list`å‚æ•°`batch_sizes`å’Œ`sequence_lengths`å®šä¹‰äº†å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„`input_ids`çš„å¤§å°ã€‚è¿˜æœ‰è®¸å¤šå…¶ä»–å‚æ•°å¯ä»¥é€šè¿‡åŸºå‡†å‚æ•°æ•°æ®ç±»è¿›è¡Œé…ç½®ã€‚æœ‰å…³è¿™äº›å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥æŸ¥é˜…æ–‡ä»¶`src/transformers/benchmark/benchmark_args_utils.py`ã€`src/transformers/benchmark/benchmark_args.py`ï¼ˆç”¨äºPyTorchï¼‰å’Œ`src/transformers/benchmark/benchmark_args_tf.py`ï¼ˆç”¨äºTensorflowï¼‰ã€‚æˆ–è€…ï¼Œä»æ ¹ç›®å½•è¿è¡Œä»¥ä¸‹shellå‘½ä»¤å°†åˆ†åˆ«æ‰“å°å‡ºPyTorchå’ŒTensorflowçš„æ‰€æœ‰å¯é…ç½®å‚æ•°çš„æè¿°æ€§åˆ—è¡¨ã€‚
- en: PytorchHide Pytorch content
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—äº†Pytorchå†…å®¹
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: An instantiated benchmark object can then simply be run by calling `benchmark.run()`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œé€šè¿‡è°ƒç”¨`benchmark.run()`æ¥ç®€å•è¿è¡Œå®ä¾‹åŒ–çš„åŸºå‡†å¯¹è±¡ã€‚
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TensorFlowHide TensorFlow content
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—äº†TensorFlowå†…å®¹
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: An instantiated benchmark object can then simply be run by calling `benchmark.run()`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œé€šè¿‡è°ƒç”¨`benchmark.run()`æ¥ç®€å•è¿è¡Œå®ä¾‹åŒ–çš„åŸºå‡†å¯¹è±¡ã€‚
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By default, the *time* and the *required memory* for *inference* are benchmarked.
    In the example output above the first two sections show the result corresponding
    to *inference time* and *inference memory*. In addition, all relevant information
    about the computing environment, *e.g.* the GPU type, the system, the library
    versions, etcâ€¦ are printed out in the third section under *ENVIRONMENT INFORMATION*.
    This information can optionally be saved in a *.csv* file when adding the argument
    `save_to_csv=True` to `PyTorchBenchmarkArguments` and `TensorFlowBenchmarkArguments`
    respectively. In this case, every section is saved in a separate *.csv* file.
    The path to each *.csv* file can optionally be defined via the argument data classes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯¹*æ¨ç†*çš„*æ—¶é—´*å’Œ*æ‰€éœ€å†…å­˜*è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹è¾“å‡ºä¸­ï¼Œå‰ä¸¤ä¸ªéƒ¨åˆ†æ˜¾ç¤ºäº†ä¸*æ¨ç†æ—¶é—´*å’Œ*æ¨ç†å†…å­˜*å¯¹åº”çš„ç»“æœã€‚æ­¤å¤–ï¼Œå…³äºè®¡ç®—ç¯å¢ƒçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼Œ*ä¾‹å¦‚*
    GPUç±»å‹ã€ç³»ç»Ÿã€åº“ç‰ˆæœ¬ç­‰ï¼Œéƒ½ä¼šåœ¨ç¬¬ä¸‰éƒ¨åˆ†çš„*ç¯å¢ƒä¿¡æ¯*ä¸‹æ‰“å°å‡ºæ¥ã€‚å½“å‘`PyTorchBenchmarkArguments`å’Œ`TensorFlowBenchmarkArguments`åˆ†åˆ«æ·»åŠ å‚æ•°`save_to_csv=True`æ—¶ï¼Œæ­¤ä¿¡æ¯å¯ä»¥é€‰æ‹©ä¿å­˜åœ¨*.csv*æ–‡ä»¶ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½ä¿å­˜åœ¨å•ç‹¬çš„*.csv*æ–‡ä»¶ä¸­ã€‚å¯ä»¥é€šè¿‡å‚æ•°æ•°æ®ç±»å¯é€‰åœ°å®šä¹‰æ¯ä¸ª*.csv*æ–‡ä»¶çš„è·¯å¾„ã€‚
- en: Instead of benchmarking pre-trained models via their model identifier, *e.g.*
    `bert-base-uncased`, the user can alternatively benchmark an arbitrary configuration
    of any available model class. In this case, a `list` of configurations must be
    inserted with the benchmark args as follows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æˆ·å¯ä»¥é€‰æ‹©é€šè¿‡å…¶æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œ*ä¾‹å¦‚* `bert-base-uncased`ï¼Œå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©å¯¹ä»»ä½•å¯ç”¨æ¨¡å‹ç±»çš„ä»»æ„é…ç½®è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¿…é¡»åœ¨åŸºå‡†å‚æ•°ä¸­æ’å…¥ä¸€ç»„é…ç½®åˆ—è¡¨ã€‚å¦‚ä¸‹æ‰€ç¤ºã€‚
- en: PytorchHide Pytorch content
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—äº†Pytorchå†…å®¹
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: TensorFlowHide TensorFlow content
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—äº†TensorFlowå†…å®¹
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Again, *inference time* and *required memory* for *inference* are measured,
    but this time for customized configurations of the `BertModel` class. This feature
    can especially be helpful when deciding for which configuration the model should
    be trained.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œ*æ¨ç†æ—¶é—´*å’Œ*æ‰€éœ€å†…å­˜*ç”¨äº*æ¨ç†*çš„æµ‹é‡ï¼Œä½†è¿™æ¬¡æ˜¯é’ˆå¯¹`BertModel`ç±»çš„è‡ªå®šä¹‰é…ç½®ã€‚åœ¨å†³å®šåº”è¯¥ä¸ºå“ªç§é…ç½®è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ­¤åŠŸèƒ½å°¤å…¶æœ‰å¸®åŠ©ã€‚
- en: Benchmark best practices
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•æœ€ä½³å®è·µ
- en: This section lists a couple of best practices one should be aware of when benchmarking
    a model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚åˆ—å‡ºäº†åœ¨å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶åº”æ³¨æ„çš„ä¸€äº›æœ€ä½³å®è·µã€‚
- en: Currently, only single device benchmarking is supported. When benchmarking on
    GPU, it is recommended that the user specifies on which device the code should
    be run by setting the `CUDA_VISIBLE_DEVICES` environment variable in the shell,
    *e.g.* `export CUDA_VISIBLE_DEVICES=0` before running the code.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®å‰ä»…æ”¯æŒå•è®¾å¤‡åŸºå‡†æµ‹è¯•ã€‚åœ¨GPUä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œå»ºè®®ç”¨æˆ·é€šè¿‡åœ¨shellä¸­è®¾ç½®`CUDA_VISIBLE_DEVICES`ç¯å¢ƒå˜é‡æ¥æŒ‡å®šä»£ç åº”åœ¨å“ªä¸ªè®¾å¤‡ä¸Šè¿è¡Œï¼Œä¾‹å¦‚åœ¨è¿è¡Œä»£ç ä¹‹å‰è®¾ç½®`export
    CUDA_VISIBLE_DEVICES=0`ã€‚
- en: The option `no_multi_processing` should only be set to `True` for testing and
    debugging. To ensure accurate memory measurement it is recommended to run each
    memory benchmark in a separate process by making sure `no_multi_processing` is
    set to `True`.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰é¡¹`no_multi_processing`åº”ä»…åœ¨æµ‹è¯•å’Œè°ƒè¯•æ—¶è®¾ç½®ä¸º`True`ã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®çš„å†…å­˜æµ‹é‡ï¼Œå»ºè®®é€šè¿‡ç¡®ä¿å°†`no_multi_processing`è®¾ç½®ä¸º`True`æ¥åœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­è¿è¡Œæ¯ä¸ªå†…å­˜åŸºå‡†æµ‹è¯•ã€‚
- en: One should always state the environment information when sharing the results
    of a model benchmark. Results can vary heavily between different GPU devices,
    library versions, etc., so that benchmark results on their own are not very useful
    for the community.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åˆ†äº«æ¨¡å‹åŸºå‡†æµ‹è¯•ç»“æœæ—¶ï¼Œåº”å§‹ç»ˆè¯´æ˜ç¯å¢ƒä¿¡æ¯ã€‚ç”±äºä¸åŒçš„GPUè®¾å¤‡ã€åº“ç‰ˆæœ¬ç­‰ä¹‹é—´çš„ç»“æœå¯èƒ½ä¼šæœ‰å¾ˆå¤§å·®å¼‚ï¼Œå› æ­¤å•ç‹¬çš„åŸºå‡†æµ‹è¯•ç»“æœå¯¹ç¤¾åŒºæ¥è¯´å¹¶ä¸æ˜¯éå¸¸æœ‰ç”¨ã€‚
- en: Sharing your benchmark
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†äº«æ‚¨çš„åŸºå‡†æµ‹è¯•
- en: 'Previously all available core models (10 at the time) have been benchmarked
    for *inference time*, across many different settings: using PyTorch, with and
    without TorchScript, using TensorFlow, with and without XLA. All of those tests
    were done across CPUs (except for TensorFlow XLA) and GPUs.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰ï¼Œæ‰€æœ‰å¯ç”¨çš„æ ¸å¿ƒæ¨¡å‹ï¼ˆå½“æ—¶ä¸º10ä¸ªï¼‰éƒ½å·²é’ˆå¯¹*æ¨ç†æ—¶é—´*è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†è®¸å¤šä¸åŒçš„è®¾ç½®ï¼šä½¿ç”¨PyTorchï¼Œä½¿ç”¨TorchScriptæˆ–ä¸ä½¿ç”¨ï¼Œä½¿ç”¨TensorFlowï¼Œä½¿ç”¨XLAæˆ–ä¸ä½¿ç”¨ã€‚æ‰€æœ‰è¿™äº›æµ‹è¯•éƒ½æ˜¯åœ¨CPUï¼ˆé™¤äº†TensorFlow
    XLAï¼‰å’ŒGPUä¸Šè¿›è¡Œçš„ã€‚
- en: The approach is detailed in the [following blogpost](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2)
    and the results are available [here](https://docs.google.com/spreadsheets/d/1sryqufw2D0XlUH4sq3e9Wnxu5EAQkaohzrJbd5HdQ_w/edit?usp=sharing).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•åœ¨[ä»¥ä¸‹åšæ–‡](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2)ä¸­æœ‰è¯¦ç»†è¯´æ˜ï¼Œç»“æœå¯åœ¨[æ­¤å¤„](https://docs.google.com/spreadsheets/d/1sryqufw2D0XlUH4sq3e9Wnxu5EAQkaohzrJbd5HdQ_w/edit?usp=sharing)æŸ¥çœ‹ã€‚
- en: With the new *benchmark* tools, it is easier than ever to share your benchmark
    results with the community
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†æ–°çš„*åŸºå‡†*å·¥å…·ï¼Œä¸ç¤¾åŒºåˆ†äº«åŸºå‡†æµ‹è¯•ç»“æœæ¯”ä»¥å¾€æ›´å®¹æ˜“
- en: '[PyTorch Benchmarking Results](https://github.com/huggingface/transformers/tree/main/examples/pytorch/benchmarking/README.md).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorchåŸºå‡†æµ‹è¯•ç»“æœ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/benchmarking/README.md)ã€‚'
- en: '[TensorFlow Benchmarking Results](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/benchmarking/README.md).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlowåŸºå‡†æµ‹è¯•ç»“æœ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/benchmarking/README.md)ã€‚'
