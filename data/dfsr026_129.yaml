- en: Text-to-Video Generation with AnimateDiff
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AnimateDiff 进行文本到视频生成
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/animatediff](https://huggingface.co/docs/diffusers/api/pipelines/animatediff)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/diffusers/api/pipelines/animatediff](https://huggingface.co/docs/diffusers/api/pipelines/animatediff)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: '[AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without
    Specific Tuning](https://arxiv.org/abs/2307.04725) by Yuwei Guo, Ceyuan Yang,
    Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[AnimateDiff：无需特定调整即可动画化您的个性化文本到图像扩散模型](https://arxiv.org/abs/2307.04725) 作者：郭宇伟，杨策远，饶安一，王耀辉，乔宇，林大华，戴波。'
- en: 'The abstract of the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding
    personalization techniques such as DreamBooth and LoRA, everyone can manifest
    their imagination into high-quality images at an affordable cost. Subsequently,
    there is a great demand for image animation techniques to further combine generated
    static images with motion dynamics. In this report, we propose a practical framework
    to animate most of the existing personalized text-to-image models once and for
    all, saving efforts in model-specific tuning. At the core of the proposed framework
    is to insert a newly initialized motion modeling module into the frozen text-to-image
    model and train it on video clips to distill reasonable motion priors. Once trained,
    by simply injecting this motion modeling module, all personalized versions derived
    from the same base T2I readily become text-driven models that produce diverse
    and personalized animated images. We conduct our evaluation on several public
    representative personalized text-to-image models across anime pictures and realistic
    photographs, and demonstrate that our proposed framework helps these models generate
    temporally smooth animation clips while preserving the domain and diversity of
    their outputs. Code and pre-trained weights will be publicly available at [this
    https URL](https://animatediff.github.io/).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*随着文本到图像模型（例如 Stable Diffusion）和相应的个性化技术（如 DreamBooth 和 LoRA）的进步，每个人都可以以较低成本将他们的想象力体现为高质量图像。随后，对于进一步将生成的静态图像与运动动态相结合的图像动画技术有着巨大需求。在本报告中，我们提出了一个实用的框架，一劳永逸地为大多数现有的个性化文本到图像模型添加动画，节省了模型特定调整的工作。所提出的框架的核心是将一个新初始化的运动建模模块插入冻结的文本到图像模型中，并在视频剪辑上训练它以提炼合理的运动先验。一旦训练完成，通过简单地注入这个运动建模模块，所有从相同基础
    T2I 派生的个性化版本都会立即成为产生多样化和个性化动画图像的文本驱动模型。我们在几个公共代表性的个性化文本到图像模型上进行评估，跨动漫图片和现实照片，并展示我们提出的框架帮助这些模型生成在保留输出的领域和多样性的同时产生时间上平滑的动画片段。代码和预训练权重将在
    [此链接](https://animatediff.github.io/) 上公开提供。*'
- en: Available Pipelines
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的流水线
- en: '| Pipeline | Tasks | Demo |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 流水线 | 任务 | 演示 |'
- en: '| --- | --- | :-: |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | :-: |'
- en: '| [AnimateDiffPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/animatediff/pipeline_animatediff.py)
    | *Text-to-Video Generation with AnimateDiff* |  |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| [AnimateDiffPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/animatediff/pipeline_animatediff.py)
    | *使用 AnimateDiff 进行文本到视频生成* |  |'
- en: '| [AnimateDiffVideoToVideoPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py)
    | *Video-to-Video Generation with AnimateDiff* |  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| [AnimateDiffVideoToVideoPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py)
    | *使用 AnimateDiff 进行视频到视频生成* |  |'
- en: Available checkpoints
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的检查点
- en: Motion Adapter checkpoints can be found under [guoyww](https://huggingface.co/guoyww/).
    These checkpoints are meant to work with any model based on Stable Diffusion 1.4/1.5.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Motion Adapter 检查点可以在 [guoyww](https://huggingface.co/guoyww/) 下找到。这些检查点旨在与基于
    Stable Diffusion 1.4/1.5 的任何模型一起使用。
- en: Usage example
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法示例
- en: AnimateDiffPipeline
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AnimateDiffPipeline
- en: AnimateDiff works with a MotionAdapter checkpoint and a Stable Diffusion model
    checkpoint. The MotionAdapter is a collection of Motion Modules that are responsible
    for adding coherent motion across image frames. These modules are applied after
    the Resnet and Attention blocks in Stable Diffusion UNet.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AnimateDiff 与 MotionAdapter 检查点和 Stable Diffusion 模型检查点配合使用。MotionAdapter 是一组负责在图像帧之间添加连贯运动的运动模块集合。这些模块在
    Stable Diffusion UNet 中的 Resnet 和 Attention 块之后应用。
- en: The following example demonstrates how to use a *MotionAdapter* checkpoint with
    Diffusers for inference based on StableDiffusion-1.4/1.5.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何使用 *MotionAdapter* 检查点与 Diffusers 进行基于 StableDiffusion-1.4/1.5 的推理。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are some sample outputs:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例输出：
- en: '| masterpiece, bestquality, sunset. ![masterpiece, bestquality, sunset](../Images/54b95c0d916bbffbc1aba35e0b746b31.png)
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 杰作，最佳质量，日落。 ![杰作，最佳质量，日落](../Images/54b95c0d916bbffbc1aba35e0b746b31.png)
    |'
- en: AnimateDiff tends to work better with finetuned Stable Diffusion models. If
    you plan on using a scheduler that can clip samples, make sure to disable it by
    setting `clip_sample=False` in the scheduler as this can also have an adverse
    effect on generated samples. Additionally, the AnimateDiff checkpoints can be
    sensitive to the beta schedule of the scheduler. We recommend setting this to
    `linear`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: AnimateDiff 更适合与微调的 Stable Diffusion 模型一起使用。如果您计划使用可以剪辑样本的调度程序，请确保通过在调度程序中设置
    `clip_sample=False` 来禁用它，因为这也可能对生成的样本产生不利影响。此外，AnimateDiff 的检查点可能对调度程序的 beta 计划敏感。我们建议将其设置为
    `linear`。
- en: AnimateDiffVideoToVideoPipeline
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AnimateDiffVideoToVideoPipeline
- en: AnimateDiff can also be used to generate visually similar videos or enable style/character/background
    or other edits starting from an initial video, allowing you to seamlessly explore
    creative possibilities.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AnimateDiff 还可用于生成视觉上相似的视频，或启用从初始视频开始的风格/角色/背景或其他编辑，使您能够无缝地探索创意可能性。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here are some sample outputs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例输出：
- en: '| Source Video | Output Video |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 源视频 | 输出视频 |'
- en: '| raccoon playing a guitar ![racoon playing a guitar](../Images/8b06f535c08fd5818358e157de31162b.png)
    | panda playing a guitar ![panda playing a guitar](../Images/d8d633fb4e433a768b4a0d3401f7551e.png)
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 熊猫弹吉他 ![熊猫弹吉他](../Images/8b06f535c08fd5818358e157de31162b.png) | 熊猫弹吉他 ![熊猫弹吉他](../Images/d8d633fb4e433a768b4a0d3401f7551e.png)
    |'
- en: '| closeup of margot robbie, fireworks in the background, high quality ![closeup
    of margot robbie, fireworks in the background, high quality](../Images/7944cbba13143eca48cff9f5becb8432.png)
    | closeup of tony stark, robert downey jr, fireworks ![closeup of tony stark,
    robert downey jr, fireworks](../Images/0753d28093b180396b738602acec6fb8.png) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 玛格特·罗比的特写，背景中的烟花，高质量 ![closeup of margot robbie, fireworks in the background,
    high quality](../Images/7944cbba13143eca48cff9f5becb8432.png) | 托尼·斯塔克的特写，小罗伯特·唐尼，烟花
    ![closeup of tony stark, robert downey jr, fireworks](../Images/0753d28093b180396b738602acec6fb8.png)
    |'
- en: Using Motion LoRAs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Motion LoRAs
- en: Motion LoRAs are a collection of LoRAs that work with the `guoyww/animatediff-motion-adapter-v1-5-2`
    checkpoint. These LoRAs are responsible for adding specific types of motion to
    the animations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Motion LoRAs 是一组 LoRA，与 `guoyww/animatediff-motion-adapter-v1-5-2` 检查点一起使用。这些
    LoRA 负责为动画添加特定类型的运动。
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '| masterpiece, bestquality, sunset. ![masterpiece, bestquality, sunset](../Images/29b358e4dc2b586485e19d14fb0af55e.png)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 杰作，最佳质量，日落。 ![masterpiece, bestquality, sunset](../Images/29b358e4dc2b586485e19d14fb0af55e.png)
    |'
- en: Using Motion LoRAs with PEFT
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PEFT 与 Motion LoRAs
- en: You can also leverage the [PEFT](https://github.com/huggingface/peft) backend
    to combine Motion LoRA’s and create more complex animations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以利用 [PEFT](https://github.com/huggingface/peft) 后端来组合 Motion LoRA，并创建更复杂的动画。
- en: First install PEFT with
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先安装 PEFT
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then you can use the following code to combine Motion LoRAs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下代码组合 Motion LoRAs。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '| masterpiece, bestquality, sunset. ![masterpiece, bestquality, sunset](../Images/14236a74edbef3ceafca9ae32e3ab3d4.png)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 杰作，最佳质量，日落。 ![masterpiece, bestquality, sunset](../Images/14236a74edbef3ceafca9ae32e3ab3d4.png)
    |'
- en: Using FreeInit
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 FreeInit
- en: '[FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537)
    by Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537)
    by Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu.'
- en: FreeInit is an effective method that improves temporal consistency and overall
    quality of videos generated using video-diffusion-models without any addition
    training. It can be applied to AnimateDiff, ModelScope, VideoCrafter and various
    other video generation models seamlessly at inference time, and works by iteratively
    refining the latent-initialization noise. More details can be found it the paper.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: FreeInit 是一种有效的方法，可以在不进行任何额外训练的情况下改善使用视频扩散模型生成的视频的时间一致性和整体质量。它可以在推断时无缝应用于 AnimateDiff、ModelScope、VideoCrafter
    和其他各种视频生成模型，并通过迭代地改进潜在初始化噪声来工作。更多细节可以在论文中找到。
- en: The following example demonstrates the usage of FreeInit.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了 FreeInit 的用法。
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: FreeInit is not really free - the improved quality comes at the cost of extra
    computation. It requires sampling a few extra times depending on the `num_iters`
    parameter that is set when enabling it. Setting the `use_fast_sampling` parameter
    to `True` can improve the overall performance (at the cost of lower quality compared
    to when `use_fast_sampling=False` but still better results than vanilla video
    generation models).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: FreeInit 实际上并不是免费的 - 改进的质量是以额外计算为代价的。它需要根据启用时设置的 `num_iters` 参数进行几次额外采样。将 `use_fast_sampling`
    参数设置为 `True` 可以提高整体性能（与 `use_fast_sampling=False` 时相比质量较低，但仍比普通视频生成模型有更好的结果）。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看调度程序指南以了解如何探索调度程序速度和质量之间的权衡，并查看重用管道中的组件部分，以了解如何有效地将相同组件加载到多个管道中。
- en: AnimateDiffPipeline
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AnimateDiffPipeline
- en: '### `class diffusers.AnimateDiffPipeline`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AnimateDiffPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L155)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L155)'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）
    - 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并从潜在表示中解码图像。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`CLIPTextModel`） - 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。'
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`CLIPTokenizer`） - 用于对文本进行标记化的 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    used to create a UNetMotionModel to denoise the encoded video latents.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）
    - 用于创建 UNetMotionModel 以去噪编码视频潜在表示的 [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。'
- en: '`motion_adapter` (`MotionAdapter`) — A `MotionAdapter` to be used in combination
    with `unet` to denoise the encoded video latents.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_adapter`（`MotionAdapter`） - 用于与 `unet` 结合使用以去噪编码视频潜在表示的 `MotionAdapter`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin））—
    与`unet`结合使用以去噪编码图像潜在变量的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: Pipeline for text-to-video generation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到视频生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道还继承了以下加载方法：
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    用于加载文本反演嵌入'
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    用于加载 LoRA 权重'
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    用于保存 LoRA 权重'
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    用于加载 IP 适配器'
- en: '#### `__call__`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L867)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L867)'
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）— 指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated video.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）—
    生成视频的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated video.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）—
    生成视频的像素宽度。'
- en: '`num_frames` (`int`, *optional*, defaults to 16) — The number of video frames
    that are generated. Defaults to 16 frames which at 8 frames per seconds amounts
    to 2 seconds of video.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_frames`（`int`，*可选*，默认为16）— 生成的视频帧数。默认为16帧，每秒8帧，相当于2秒的视频。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality videos at the expense
    of slower inference.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`（`int`，*可选*，默认为50）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的视频，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`（`float`，*可选*，默认为7.5）— 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`（`str`或`List[str]`，*可选*）— 指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale
    < 1`）被忽略。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`（`float`，*可选*，默认为0.0）— 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数
    eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for video generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`. Latents
    should be of shape `(batch_size, num_channel, num_frames, height, width)`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`（`torch.FloatTensor`，*可选*）— 从高斯分布中预先生成的噪声潜在变量，用作视频生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在变量张量，潜在变量应具有形状`(batch_size,
    num_channel, num_frames, height, width)`。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image — (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。ip_adapter_image
    — (`PipelineImageInput`, *可选*): 可选的图像输入以与IP适配器一起使用。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated video. Choose between `torch.FloatTensor`, `PIL.Image` or `np.array`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"pil"`) — 生成视频的输出格式。可选择`torch.FloatTensor`、`PIL.Image`或`np.array`。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    instead of a plain tuple.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)而不是普通元组。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将作为[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`传递的kwargs字典。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的CLIP层数。值为1表示将使用预最终层的输出来计算提示嵌入。'
- en: '`callback_on_step_end` (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Callable`, *可选*) — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List`, *可选*) — 用于`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。'
- en: Returns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    or `tuple`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)或`tuple`'
- en: If `return_dict` is `True`, [TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated frames.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的帧的列表。
- en: The call function to the pipeline for generation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#### `disable_free_init`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_free_init`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L591)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L591)'
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Disables the FreeInit mechanism if enabled.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，则禁用FreeInit机制。
- en: '#### `disable_freeu`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L539)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L539)'
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用，则禁用FreeU机制。
- en: '#### `disable_vae_slicing`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L491)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L491)'
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片的VAE解码。如果先前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `disable_vae_tiling`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L508)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L508)'
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用平铺的VAE解码。如果先前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。
- en: '#### `enable_free_init`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_free_init`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L547)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L547)'
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_iters` (`int`, *optional*, defaults to `3`) — Number of FreeInit noise
    re-initialization iterations.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iters` (`int`, *可选*, 默认为`3`) — FreeInit噪声重新初始化迭代次数。'
- en: '`use_fast_sampling` (`bool`, *optional*, defaults to `False`) — Whether or
    not to speedup sampling procedure at the cost of probably lower quality results.
    Enables the “Coarse-to-Fine Sampling” strategy, as mentioned in the paper, if
    set to `True`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_fast_sampling` (`bool`, *optional*, 默认为 `False`) — 是否加速采样过程以降低可能的质量结果。如果设置为
    `True`，则启用“由粗到细采样”策略，如论文中所述。'
- en: '`method` (`str`, *optional*, defaults to `butterworth`) — Must be one of `butterworth`,
    `ideal` or `gaussian` to use as the filtering method for the FreeInit low pass
    filter.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`method` (`str`, *optional*, 默认为 `butterworth`) — 必须是 `butterworth`、`ideal`
    或 `gaussian` 中的一个，用作FreeInit低通滤波器的滤波方法。'
- en: '`order` (`int`, *optional*, defaults to `4`) — Order of the filter used in
    `butterworth` method. Larger values lead to `ideal` method behaviour whereas lower
    values lead to `gaussian` method behaviour.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`order` (`int`, *optional*, 默认为 `4`) — `butterworth` 方法中使用的滤波器的阶数。较大的值导致 `ideal`
    方法行为，而较小的值导致 `gaussian` 方法行为。'
- en: '`spatial_stop_frequency` (`float`, *optional*, defaults to `0.25`) — Normalized
    stop frequency for spatial dimensions. Must be between 0 to 1\. Referred to as
    `d_s` in the original implementation.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spatial_stop_frequency` (`float`, *optional*, 默认为 `0.25`) — 空间维度的归一化停止频率。必须介于0到1之间。在原始实现中称为
    `d_s`。'
- en: '`temporal_stop_frequency` (`float`, *optional*, defaults to `0.25`) — Normalized
    stop frequency for temporal dimensions. Must be between 0 to 1\. Referred to as
    `d_t` in the original implementation.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temporal_stop_frequency` (`float`, *optional*, 默认为 `0.25`) — 时间维度的归一化停止频率。必须介于0到1之间。在原始实现中称为
    `d_t`。'
- en: '`generator` (`torch.Generator`, *optional*, defaults to `0.25`) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make FreeInit generation deterministic.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`, *optional*, defaults to `0.25`) — 用于使FreeInit生成变得确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: Enables the FreeInit mechanism as in [https://arxiv.org/abs/2312.07537](https://arxiv.org/abs/2312.07537).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 启用FreeInit机制，如[https://arxiv.org/abs/2312.07537](https://arxiv.org/abs/2312.07537)。
- en: This implementation has been adapted from the [official repository](https://github.com/TianxingWu/FreeInit).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现已经从[官方存储库](https://github.com/TianxingWu/FreeInit)进行了调整。
- en: '#### `enable_freeu`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L516)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L516)'
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1` (`float`) — 阶段1的缩放因子，用于减弱跳跃特征的贡献。这是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2` (`float`) — 阶段2的缩放因子，用于减弱跳跃特征的贡献。这是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1` (`float`) — 阶段1的缩放因子，用于放大主干特征。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2` (`float`) — 阶段2的缩放因子，用于放大主干特征的贡献。'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后缀表示它们被应用的阶段。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如Stable Diffusion
    v1、v2和Stable Diffusion XL）的值组合。
- en: '#### `enable_vae_slicing`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L483)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L483)'
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。当启用此选项时，VAE将输入张量分割成多个切片以在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。
- en: '#### `enable_vae_tiling`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L499)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L499)'
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 启用平铺VAE解码。当启用此选项时，VAE将输入张量分割成瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。
- en: '#### `encode_prompt`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L223)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L223)'
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`): torch
    设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 每个提示应生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用分类器自由引导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递
    `negative_prompt_embeds`。在不使用指导时被忽略（即如果 `guidance_scale` 小于 `1`，则被忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从
    `negative_prompt` 输入参数生成负文本嵌入。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *optional*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA
    比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的 CLIP 层数。值为 1 表示将使用预终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: AnimateDiffVideoToVideoPipeline
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AnimateDiffVideoToVideoPipeline
- en: '### `class diffusers.AnimateDiffVideoToVideoPipeline`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AnimateDiffVideoToVideoPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L166)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L166)'
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModel`) — 冻结的文本编码器（clip-vit-large-patch14）。'
- en: '`tokenizer` (`CLIPTokenizer`) — A [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    to tokenize text.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — 用于对文本进行标记化的 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    used to create a UNetMotionModel to denoise the encoded video latents.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 用于创建 UNetMotionModel 以去噪编码视频潜在空间的 [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。'
- en: '`motion_adapter` (`MotionAdapter`) — A `MotionAdapter` to be used in combination
    with `unet` to denoise the encoded video latents.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`motion_adapter` (`MotionAdapter`) — 与 `unet` 结合使用以去噪编码视频潜在空间的 `MotionAdapter`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 与 `unet` 结合使用以去噪编码图像潜在空间的调度器。可以是 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)
    或 [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)
    中的一个。'
- en: Pipeline for video-to-video generation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 视频到视频生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道还继承以下加载方法：
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载文本反演嵌入的 load_textual_inversion()
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载 LoRA 权重的 load_lora_weights()
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存 LoRA 权重的 save_lora_weights()
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载 IP 适配器的 load_ip_adapter()
- en: '#### `__call__`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L726)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L726)'
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`video` (`List[PipelineImageInput]`) — The input video to condition the generation
    on. Must be a list of images/frames of the video.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`video` (`List[PipelineImageInput]`) — 生成条件的输入视频。必须是视频的图像/帧列表。'
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *optional*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated video.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成视频的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated video.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成视频的像素宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality videos at the expense
    of slower inference.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致视频质量更高，但推理速度较慢。'
- en: '`strength` (`float`, *optional*, defaults to 0.8) — Higher strength leads to
    more differences between original video and generated video.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength` (`float`, *optional*, 默认为0.8) — 更高的强度会导致原始视频和生成视频之间的差异更大。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, 默认为7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt`
    密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *optional*) — 用于指导图像生成中不包含的提示或提示。如果未定义，则需要传递
    `negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）忽略。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *optional*, 默认为0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502)
    论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for video generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`. Latents
    should be of shape `(batch_size, num_channel, num_frames, height, width)`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中抽样的预生成嘈杂潜变量，用作视频生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机
    `generator` 进行抽样生成潜变量张量，形状应为 `(batch_size, num_channel, num_frames, height, width)`。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从
    `prompt` 输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image — (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从
    `negative_prompt` 输入参数生成 `negative_prompt_embeds`。ip_adapter_image — (`PipelineImageInput`,
    *optional*): 用于与IP适配器一起使用的可选图像输入。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated video. Choose between `torch.FloatTensor`, `PIL.Image` or `np.array`.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认为`"pil"`) — 生成视频的输出格式。选择 `torch.FloatTensor`、`PIL.Image`
    或 `np.array` 之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `AnimateDiffPipelineOutput` instead of a plain tuple.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回 `AnimateDiffPipelineOutput`
    而不是普通元组。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 [`AttentionProcessor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中定义的 `self.processor`。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从CLIP中跳过的层数。值为1表示将使用预最终层的输出来计算提示嵌入。'
- en: '`callback_on_step_end` (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end` (`Callable`, *可选*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self:
    DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`
    将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。'
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end`
    函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包括在您的管道类的 `._callback_tensor_inputs`
    属性中列出的变量。'
- en: Returns
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`AnimateDiffPipelineOutput` or `tuple`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`AnimateDiffPipelineOutput` 或 `tuple`'
- en: If `return_dict` is `True`, `AnimateDiffPipelineOutput` is returned, otherwise
    a `tuple` is returned where the first element is a list with the generated frames.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，则返回 `AnimateDiffPipelineOutput`，否则返回一个 `tuple`，其中第一个元素是包含生成帧的列表。
- en: The call function to the pipeline for generation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '#### `disable_freeu`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L521)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L521)'
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了FreeU机制，则禁用它。
- en: '#### `disable_vae_slicing`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L473)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L473)'
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片的VAE解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。
- en: '#### `disable_vae_tiling`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L490)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L490)'
- en: '[PRE22]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用平铺的VAE解码。如果之前启用了 `enable_vae_tiling`，则此方法将返回到一步计算解码。
- en: '#### `enable_freeu`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L498)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L498)'
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1` (`float`) — 用于减弱跳跃特征贡献的阶段1的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2` (`float`) — 用于减弱跳跃特征贡献的阶段2的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1` (`float`) — 用于放大主干特征贡献的阶段1的缩放因子。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2` (`float`) — 用于放大主干特征贡献的阶段2的缩放因子。'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后缀表示它们被应用的阶段。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion
    v1、v2 和 Stable Diffusion XL）的值组合。
- en: '#### `enable_vae_slicing`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L465)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L465)'
- en: '[PRE24]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片的VAE解码。当启用此选项时，VAE将将输入张量分割成片段以在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。
- en: '#### `enable_vae_tiling`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L481)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L481)'
- en: '[PRE25]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 启用平铺的VAE解码。当启用此选项时，VAE将将输入张量分割成瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。
- en: '#### `encode_prompt`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L234)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L234)'
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）— 要编码的提示设备 —（`torch.device`）：torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`（`int`）— 应该为每个提示生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance`（`bool`）— 是否使用分类器自由指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`（`str`或`List[str]`，*可选*）— 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负面文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale`（`float`，*可选*）— 将应用于文本编码器的所有LoRA层的LoRA比例，如果加载了LoRA层。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip`（`int`，*可选*）— 在计算提示嵌入时要跳过的CLIP层数。值为1意味着将使用预终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: AnimateDiffPipelineOutput
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AnimateDiffPipelineOutput
- en: '### `class diffusers.pipelines.animatediff.AnimateDiffPipelineOutput`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.pipelines.animatediff.AnimateDiffPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_output.py#L11)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_output.py#L11)'
- en: '[PRE27]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`frames` (`List[List[PIL.Image.Image]]` or `torch.Tensor` or `np.ndarray`)
    — List of PIL Images of length `batch_size` or torch.Tensor or np.ndarray of shape
    `(batch_size, num_frames, height, width, num_channels)`.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frames`（`List[List[PIL.Image.Image]]`或`torch.Tensor`或`np.ndarray`）— 长度为`batch_size`的PIL图像列表或torch.Tensor或np.ndarray，形状为`(batch_size,
    num_frames, height, width, num_channels)`。'
- en: Output class for AnimateDiff pipelines.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: AnimateDiff管道的输出类。
