- en: Autoformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Autoformer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/autoformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/autoformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/autoformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/autoformer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Autoformer model was proposed in [Autoformer: Decomposition Transformers
    with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)
    by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Autoformer模型是由Haixu Wu，Jiehui Xu，Jianmin Wang，Mingsheng Long在[Autoformer: Decomposition
    Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)中提出的。'
- en: This model augments the Transformer as a deep decomposition architecture, which
    can progressively decompose the trend and seasonal components during the forecasting
    process.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将Transformer作为深度分解架构，可以在预测过程中逐步分解趋势和季节性组件。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Extending the forecasting time is a critical demand for real applications,
    such as extreme weather early warning and long-term energy consumption planning.
    This paper studies the long-term forecasting problem of time series. Prior Transformer-based
    models adopt various self-attention mechanisms to discover the long-range dependencies.
    However, intricate temporal patterns of the long-term future prohibit the model
    from finding reliable dependencies. Also, Transformers have to adopt the sparse
    versions of point-wise self-attentions for long series efficiency, resulting in
    the information utilization bottleneck. Going beyond Transformers, we design Autoformer
    as a novel decomposition architecture with an Auto-Correlation mechanism. We break
    with the pre-processing convention of series decomposition and renovate it as
    a basic inner block of deep models. This design empowers Autoformer with progressive
    decomposition capacities for complex time series. Further, inspired by the stochastic
    process theory, we design the Auto-Correlation mechanism based on the series periodicity,
    which conducts the dependencies discovery and representation aggregation at the
    sub-series level. Auto-Correlation outperforms self-attention in both efficiency
    and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy,
    with a 38% relative improvement on six benchmarks, covering five practical applications:
    energy, traffic, economics, weather and disease.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*延长预测时间是真实应用的关键需求，例如极端天气预警和长期能源消耗规划。本文研究了时间序列的长期预测问题。之前基于Transformer的模型采用各种自注意机制来发现长距离依赖关系。然而，长期未来的复杂时间模式阻碍了模型找到可靠的依赖关系。此外，为了长序列的效率，Transformer必须采用稀疏版本的点对点自注意力，导致信息利用瓶颈。超越Transformer，我们设计Autoformer作为一种具有自相关机制的新型分解架构。我们打破了系列分解的预处理惯例，并将其改造为深度模型的基本内部块。这种设计赋予Autoformer对复杂时间序列的渐进分解能力。此外，受随机过程理论启发，我们设计了基于系列周期性的自相关机制，它在子系列级别进行依赖关系发现和表示聚合。自相关在效率和准确性方面优于自注意力。在长期预测中，Autoformer取得了最先进的准确性，在六个基准测试中相对提高了38％，涵盖了五个实际应用：能源，交通，经济，天气和疾病。*'
- en: This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).
    The original code can be found [here](https://github.com/thuml/Autoformer).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[elisim](https://huggingface.co/elisim)和[kashif](https://huggingface.co/kashif)贡献。原始代码可以在[此处](https://github.com/thuml/Autoformer)找到。
- en: Resources
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started. If you’re interested in submitting a resource to be included
    here, please feel free to open a Pull Request and we’ll review it! The resource
    should ideally demonstrate something new instead of duplicating an existing resource.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列官方Hugging Face和社区（由🌎表示）资源，可帮助您入门。如果您有兴趣提交资源以包含在此处，请随时打开Pull Request，我们将进行审查！资源应该理想地展示一些新东西，而不是重复现有资源。
- en: 'Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers
    are Effective for Time Series Forecasting (+ Autoformer)](https://huggingface.co/blog/autoformer)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在HuggingFace博客中查看Autoformer博文：[是的，Transformers对时间序列预测有效（+ Autoformer）](https://huggingface.co/blog/autoformer)
- en: AutoformerConfig
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoformerConfig
- en: '### `class transformers.AutoformerConfig`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AutoformerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/configuration_autoformer.py#L30)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/configuration_autoformer.py#L30)'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prediction_length` (`int`) — The prediction length for the decoder. In other
    words, the prediction horizon of the model.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_length`（`int`）— 解码器的预测长度。换句话说，模型的预测时间范围。'
- en: '`context_length` (`int`, *optional*, defaults to `prediction_length`) — The
    context length for the encoder. If unset, the context length will be the same
    as the `prediction_length`.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_length`（`int`，*可选*，默认为`prediction_length`）— 编码器的上下文长度。如果未设置，上下文长度将与`prediction_length`相同。'
- en: '`distribution_output` (`string`, *optional*, defaults to `"student_t"`) — The
    distribution emission head for the model. Could be either “student_t”, “normal”
    or “negative_binomial”.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distribution_output`（`string`，*可选*，默认为`"student_t"`）— 模型的分布发射头。可以是“student_t”，“normal”或“negative_binomial”。'
- en: '`loss` (`string`, *optional*, defaults to `"nll"`) — The loss function for
    the model corresponding to the `distribution_output` head. For parametric distributions
    it is the negative log likelihood (nll) - which currently is the only supported
    one.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`string`，*可选*，默认为`"nll"`）— 与`distribution_output`头部对应的模型的损失函数。对于参数分布，它是负对数似然（nll）-
    目前是唯一支持的。'
- en: '`input_size` (`int`, *optional*, defaults to 1) — The size of the target variable
    which by default is 1 for univariate targets. Would be > 1 in case of multivariate
    targets.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size`（`int`，*可选*，默认为1）— 目标变量的大小，默认情况下对于单变量目标为1。在多变量目标的情况下会大于1。'
- en: '`lags_sequence` (`list[int]`, *optional*, defaults to `[1, 2, 3, 4, 5, 6, 7]`)
    — The lags of the input time series as covariates often dictated by the frequency.
    Default is `[1, 2, 3, 4, 5, 6, 7]`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lags_sequence` (`list[int]`, *optional*, defaults to `[1, 2, 3, 4, 5, 6, 7]`)
    — 输入时间序列的滞后作为协变量，通常由频率决定。默认为`[1, 2, 3, 4, 5, 6, 7]`。'
- en: '`scaling` (`bool`, *optional* defaults to `True`) — Whether to scale the input
    targets.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling` (`bool`, *optional* defaults to `True`) — 是否对输入目标进行缩放。'
- en: '`num_time_features` (`int`, *optional*, defaults to 0) — The number of time
    features in the input time series.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_time_features` (`int`, *optional*, defaults to 0) — 输入时间序列中的时间特征数量。'
- en: '`num_dynamic_real_features` (`int`, *optional*, defaults to 0) — The number
    of dynamic real valued features.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_dynamic_real_features` (`int`, *optional*, defaults to 0) — 动态实值特征的数量。'
- en: '`num_static_categorical_features` (`int`, *optional*, defaults to 0) — The
    number of static categorical features.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_static_categorical_features` (`int`, *optional*, defaults to 0) — 静态分类特征的数量。'
- en: '`num_static_real_features` (`int`, *optional*, defaults to 0) — The number
    of static real valued features.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_static_real_features` (`int`, *optional*, defaults to 0) — 静态实值特征的数量。'
- en: '`cardinality` (`list[int]`, *optional*) — The cardinality (number of different
    values) for each of the static categorical features. Should be a list of integers,
    having the same length as `num_static_categorical_features`. Cannot be `None`
    if `num_static_categorical_features` is > 0.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cardinality` (`list[int]`, *optional*) — 每个静态分类特征的基数（不同值的数量）。应该是一个整数列表，长度与`num_static_categorical_features`相同。如果`num_static_categorical_features`大于0，则不能为`None`。'
- en: '`embedding_dimension` (`list[int]`, *optional*) — The dimension of the embedding
    for each of the static categorical features. Should be a list of integers, having
    the same length as `num_static_categorical_features`. Cannot be `None` if `num_static_categorical_features`
    is > 0.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dimension` (`list[int]`, *optional*) — 每个静态分类特征的嵌入维度。应该是一个整数列表，长度与`num_static_categorical_features`相同。如果`num_static_categorical_features`大于0，则不能为`None`。'
- en: '`d_model` (`int`, *optional*, defaults to 64) — Dimensionality of the transformer
    layers.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, defaults to 64) — Transformer层的维度。'
- en: '`encoder_layers` (`int`, *optional*, defaults to 2) — Number of encoder layers.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers` (`int`, *optional*, defaults to 2) — 编码器层的数量。'
- en: '`decoder_layers` (`int`, *optional*, defaults to 2) — Number of decoder layers.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers` (`int`, *optional*, defaults to 2) — 解码器层的数量。'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads` (`int`, *optional*, defaults to 2) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, defaults to 2) — Transformer解码器中每个注意力层的注意力头数。'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 32) — Dimension of the “intermediate”
    (often named feed-forward) layer in encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, defaults to 32) — 编码器中“中间”（通常称为前馈）层的维度。'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 32) — Dimension of the “intermediate”
    (often named feed-forward) layer in decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, defaults to 32) — 解码器中“中间”（通常称为前馈）层的维度。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and decoder.
    If string, `"gelu"` and `"relu"` are supported.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — 编码器和解码器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`和`"relu"`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the encoder, and decoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — 编码器和解码器中所有全连接层的丢弃概率。'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention and fully connected layers for each encoder layer.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 每个编码器层的注意力和全连接层的丢弃概率。'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention and fully connected layers for each decoder layer.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 每个解码器层的注意力和全连接层的丢弃概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention probabilities.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢弃概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    used between the two layers of the feed-forward networks.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 在前馈网络的两层之间使用的丢弃概率。'
- en: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — The number of
    samples to generate in parallel for each time step of inference.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — 每个推断时间步生成的并行样本数量。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated normal weight initialization distribution.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) — 截断正态权重初始化分布的标准差。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether to use the past
    key/values attentions (if applicable to the model) to speed up decoding.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 是否使用过去的键/值注意力（如果适用于模型）以加速解码。'
- en: '`label_length` (`int`, *optional*, defaults to 10) — Start token length of
    the Autoformer decoder, which is used for direct multi-step prediction (i.e. non-autoregressive
    generation).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_length` (`int`, *optional*, defaults to 10) — Autoformer解码器的起始标记长度，用于直接多步预测（即非自回归生成）。'
- en: '`moving_average` (`int`, defaults to 25) — The window size of the moving average.
    In practice, it’s the kernel size in AvgPool1d of the Decomposition Layer.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`moving_average` (`int`, defaults to 25) — 移动平均的窗口大小。在实践中，它是分解层的AvgPool1d中的核大小。'
- en: '`autocorrelation_factor` (`int`, defaults to 3) — “Attention” (i.e. AutoCorrelation
    mechanism) factor which is used to find top k autocorrelations delays. It’s recommended
    in the paper to set it to a number between 1 and 5.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`autocorrelation_factor` (`int`, defaults to 3) — “注意力”（即自相关机制）因子，用于找到前k个自相关延迟。建议在论文中将其设置为1到5之间的数字。'
- en: This is the configuration class to store the configuration of an [AutoformerModel](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerModel).
    It is used to instantiate an Autoformer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Autoformer [huggingface/autoformer-tourism-monthly](https://huggingface.co/huggingface/autoformer-tourism-monthly)
    architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[AutoformerModel](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerModel)配置的配置类。它用于根据指定的参数实例化Autoformer模型，定义模型架构。使用默认值实例化配置将产生类似于Autoformer
    [huggingface/autoformer-tourism-monthly](https://huggingface.co/huggingface/autoformer-tourism-monthly)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: AutoformerModel
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoformerModel
- en: '### `class transformers.AutoformerModel`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AutoformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1429)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1429)'
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig)）—
    模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Autoformer Model outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Autoformer模型输出原始隐藏状态，没有特定的头部。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1607)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1607)'
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Past values of the time series, that serve as context in order to predict the
    future. These values may contain lags, i.e. additional values from the past which
    are added in order to serve as “extra context”. The `past_values` is what the
    Transformer encoder gets as input (with optional additional features, such as
    `static_categorical_features`, `static_real_features`, `past_time_features`).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 时间序列的过去值，用作上下文以预测未来。这些值可能包含滞后，即过去的附加值，这些值被添加以充当“额外上下文”。`past_values` 是Transformer编码器接收的输入（还可以包括可选的其他特征，如`static_categorical_features`、`static_real_features`、`past_time_features`）。'
- en: The sequence length here is equal to `context_length` + `max(config.lags_sequence)`.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的序列长度等于`context_length` + `max(config.lags_sequence)`。
- en: Missing values need to be replaced with zeros.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺失值需要用零替换。
- en: '`past_time_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_features)`, *optional*) — Optional time features, which the model internally
    will add to `past_values`. These could be things like “month of year”, “day of
    the month”, etc. encoded as vectors (for instance as Fourier features). These
    could also be so-called “age” features, which basically help the model know “at
    which point in life” a time-series is. Age features have small values for distant
    past time steps and increase monotonically the more we approach the current time
    step.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_time_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_features)`, *可选*) — 可选的时间特征，模型内部将添加到`past_values`中。这些可能是诸如“年份中的月份”、“月份中的日期”等编码为向量（例如作为傅立叶特征）的内容。这些也可以是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生命中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征充当输入的“位置编码”。因此，与BERT这样的模型不同，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。
- en: The Autoformer only learns additional embeddings for `static_categorical_features`.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Autoformer仅为`static_categorical_features`学习额外的嵌入。
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Boolean mask to indicate which `past_values` were observed and which
    were missing. Mask values selected in `[0, 1]`:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 布尔掩码，指示哪些 `past_values` 被观察到，哪些是缺失的。掩码值选在 `[0, 1]` 之间：'
- en: 1 for values that are `observed`,
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示值为 `observed`,
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示值为 `missing`（即被零替换的 NaN）。
- en: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — Optional static categorical features
    for which the model will learn an embedding, which it will add to the values of
    the time series.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — 模型将学习的可选静态分类特征的嵌入，将其添加到时间序列的值中。'
- en: Static categorical features are features which have the same value for all time
    steps (static over time).
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征是所有时间步长具有相同值的特征（随时间保持不变）。
- en: A typical example of a static categorical feature is a time series ID.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征的典型示例是时间序列 ID。
- en: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — Optional static real features which the
    model will add to the values of the time series.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — 可选的静态实数特征，模型将将其添加到时间序列的值中。'
- en: Static real features are features which have the same value for all time steps
    (static over time).
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实数特征是所有时间步长具有相同值的特征（随时间保持不变）。
- en: A typical example of a static real feature is promotion information.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实数特征的典型示例是促销信息。
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`)
    — Future values of the time series, that serve as labels for the model. The `future_values`
    is what the Transformer needs to learn to output, given the `past_values`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`)
    — 时间序列的未来值，作为模型的标签。`future_values` 是 Transformer 需要学习输出的内容，给定 `past_values`。'
- en: See the demo notebook and code snippets for details.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅演示笔记本和代码片段。
- en: Missing values need to be replaced with zeros.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺失值需要用零替换。
- en: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`, *optional*) — Optional time features, which the model internally
    will add to `future_values`. These could be things like “month of year”, “day
    of the month”, etc. encoded as vectors (for instance as Fourier features). These
    could also be so-called “age” features, which basically help the model know “at
    which point in life” a time-series is. Age features have small values for distant
    past time steps and increase monotonically the more we approach the current time
    step.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`, *optional*) — 可选的时间特征，模型将内部添加到 `future_values` 中。这些可能是像“年份的月份”，“月份的日期”等编码为向量（例如傅立叶特征）的内容。这也可能是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生命中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    features.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。与像 BERT 这样的模型不同，BERT 的位置编码是从头开始内部作为模型的参数学习的，时间序列 Transformer
    需要提供额外的特征。
- en: The Autoformer only learns additional embeddings for `static_categorical_features`.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Autoformer 仅为 `static_categorical_features` 学习额外的嵌入。
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在某些标记索引上执行注意力的掩码。掩码值选在 `[0, 1]` 之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示标记未被掩码，
- en: 0 for tokens that are `masked`.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示标记被掩码。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. By
    default, a causal mask will be used, to make sure the model can only look at previous
    inputs in order to predict the future.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 用于避免在某些标记索引上执行注意力的掩码。默认情况下，将使用因果掩码，以确保模型只能查看以前的输入以预测未来。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — 用于在编码器中使注意力模块的特定头部失效的掩码。掩码值选在 `[0, 1]` 之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — 用于在解码器中使注意力模块的特定头部失效的掩码。掩码值选在 `[0, 1]` 之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — 用于使交叉注意力模块的特定头部失效的掩码。掩码值选在 `[0, 1]` 之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括`last_hidden_state`、`hidden_states`
    (*optional*) 和 `attentions` (*optional*)。`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)` (*optional*)，是编码器最后一层的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回
    — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量，以及2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入最后一个`decoder_input_ids`（这些没有给出其过去键值状态的模型）的形状为`(batch_size,
    1)`，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.autoformer.modeling_autoformer.AutoformerModelOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.autoformer.modeling_autoformer.AutoformerModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.autoformer.modeling_autoformer.AutoformerModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig))
    and inputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.autoformer.modeling_autoformer.AutoformerModelOutput`或一个`torch.FloatTensor`的元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`的形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型解码器最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`trend` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Trend tensor for each time series.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trend` (`torch.FloatTensor`的形状为`(batch_size, sequence_length, hidden_size)`)
    — 每个时间序列的趋势张量。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回
    — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量，以及2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*）- 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`loc` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Shift values of each time series’ context window which is used to
    give the model inputs of the same magnitude and then used to shift back to the
    original magnitude.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc`（`torch.FloatTensor`，形状为`(batch_size,)`或`(batch_size, input_size)`，*可选*）-
    每个时间序列上下文窗口的偏移值，用于使模型输入具有相同的量级，然后用于将其偏移回原始量级。'
- en: '`scale` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Scaling values of each time series’ context window which is used
    to give the model inputs of the same magnitude and then used to rescale back to
    the original magnitude.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`（`torch.FloatTensor`，形状为`(batch_size,)`或`(batch_size, input_size)`，*可选*）-
    每个时间序列上下文窗口的缩放值，用于使模型输入具有相同的量级，然后用于将其重新缩放回原始量级。'
- en: '`static_features:` (`torch.FloatTensor` of shape `(batch_size, feature size)`,
    *optional*) — Static features of each time series’ in a batch which are copied
    to the covariates at inference time.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_features:`（`torch.FloatTensor`，形状为`(batch_size, feature size)`，*可选*）-
    批处理中每个时间序列的静态特征，在推断时复制到协变量中。'
- en: The [AutoformerModel](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[AutoformerModel](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: AutoformerForPrediction
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoformerForPrediction
- en: '### `class transformers.AutoformerForPrediction`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AutoformerForPrediction`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1765)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1765)'
- en: '[PRE5]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig)）—
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Autoformer Model with a distribution head on top for time-series forecasting.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer模型在时间序列预测的顶部具有一个分布头。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般使用和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1809)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/autoformer/modeling_autoformer.py#L1809)'
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Past values of the time series, that serve as context in order to predict the
    future. These values may contain lags, i.e. additional values from the past which
    are added in order to serve as “extra context”. The `past_values` is what the
    Transformer encoder gets as input (with optional additional features, such as
    `static_categorical_features`, `static_real_features`, `past_time_features`).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（`torch.FloatTensor`，形状为`(batch_size, sequence_length)`）— 时间序列的过去值，作为上下文以预测未来。这些值可能包含滞后，即过去的其他值，以作为“额外上下文”添加。`past_values`是Transformer编码器的输入（还可以包括可选的其他特征，如`static_categorical_features`、`static_real_features`、`past_time_features`）。'
- en: The sequence length here is equal to `context_length` + `max(config.lags_sequence)`.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的序列长度等于`context_length` + `max(config.lags_sequence)`。
- en: Missing values need to be replaced with zeros.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺失值需要用零替换。
- en: '`past_time_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_features)`, *optional*) — Optional time features, which the model internally
    will add to `past_values`. These could be things like “month of year”, “day of
    the month”, etc. encoded as vectors (for instance as Fourier features). These
    could also be so-called “age” features, which basically help the model know “at
    which point in life” a time-series is. Age features have small values for distant
    past time steps and increase monotonically the more we approach the current time
    step.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_time_features`（`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    num_features)`，*可选*）— 可选的时间特征，模型内部将其添加到`past_values`中。这些可能是诸如“年份中的月份”，“月份中的日期”等编码为向量（例如傅立叶特征）的内容。这些也可以是所谓的“年龄”特征，基本上帮助模型了解时间序列处于“生命中的哪个阶段”。年龄特征对于过去的时间步具有较小的值，并且随着我们接近当前时间步而单调增加。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。因此，与像BERT这样的模型相反，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。
- en: The Autoformer only learns additional embeddings for `static_categorical_features`.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Autoformer仅为`static_categorical_features`学习额外的嵌入。
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Boolean mask to indicate which `past_values` were observed and which
    were missing. Mask values selected in `[0, 1]`:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask`（`torch.BoolTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0, 1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“观察到”的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“缺失”的值（即用零替换的NaN）。
- en: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — Optional static categorical features
    for which the model will learn an embedding, which it will add to the values of
    the time series.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categorical_features`（`torch.LongTensor`，形状为`(batch_size, number of
    static categorical features)`，*可选*）— 模型将学习嵌入的可选静态分类特征，将其添加到时间序列的值中。'
- en: Static categorical features are features which have the same value for all time
    steps (static over time).
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征是所有时间步长具有相同值的特征（随时间保持静态）。
- en: A typical example of a static categorical feature is a time series ID.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征的典型示例是时间序列ID。
- en: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — Optional static real features which the
    model will add to the values of the time series.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_real_features`（`torch.FloatTensor`，形状为`(batch_size, number of static
    real features)`，*可选*）— 模型将添加到时间序列值中的可选静态实特征。'
- en: Static real features are features which have the same value for all time steps
    (static over time).
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实特征是所有时间步长具有相同值的特征（随时间保持静态）。
- en: A typical example of a static real feature is promotion information.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实特征的典型示例是促销信息。
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`)
    — Future values of the time series, that serve as labels for the model. The `future_values`
    is what the Transformer needs to learn to output, given the `past_values`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values` (`torch.FloatTensor`，形状为`(batch_size, prediction_length)`)
    — 时间序列的未来值，作为模型的标签。Transformer需要学习输出`future_values`，给定`past_values`。'
- en: See the demo notebook and code snippets for details.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅演示笔记本和代码片段。
- en: Missing values need to be replaced with zeros.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺失值需要用零替换。
- en: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`, *optional*) — Optional time features, which the model internally
    will add to `future_values`. These could be things like “month of year”, “day
    of the month”, etc. encoded as vectors (for instance as Fourier features). These
    could also be so-called “age” features, which basically help the model know “at
    which point in life” a time-series is. Age features have small values for distant
    past time steps and increase monotonically the more we approach the current time
    step.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_time_features` (`torch.FloatTensor`，形状为`(batch_size, prediction_length,
    num_features)`，*optional*) — 可选的时间特征，模型内部将其添加到`future_values`中。这些可能是诸如“年份的月份”，“月份的日期”等编码为向量（例如作为傅立叶特征）的内容。这些也可以是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生命中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    features.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。与BERT等模型不同，BERT等模型的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的特征。
- en: The Autoformer only learns additional embeddings for `static_categorical_features`.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Autoformer仅为`static_categorical_features`学习额外的嵌入。
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 用于避免在某些标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被掩盖的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被掩盖的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. By
    default, a causal mask will be used, to make sure the model can only look at previous
    inputs in order to predict the future.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.LongTensor`，形状为`(batch_size, target_sequence_length)`，*optional*)
    — 用于避免在某些标记索引上执行注意力的掩码。默认情况下，将使用因果掩码，以确保模型只能查看以前的输入以预测未来。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`，形状为`(encoder_layers, encoder_attention_heads)`，*optional*)
    — 用于使编码器中的注意力模块中的选定头部失效的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被掩盖。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.Tensor`，形状为`(decoder_layers, decoder_attention_heads)`，*optional*)
    — 用于使解码器中的注意力模块中的选定头部失效的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被掩盖。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor`，形状为`(decoder_layers, decoder_attention_heads)`，*optional*)
    — 用于使交叉注意力模块中的选定头部失效的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被掩盖。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`，*optional*) — 元组包括`last_hidden_state`，`hidden_states`（*optional*）和`attentions`（*optional*）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`的隐藏状态序列是编码器最后一层的输出。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去键值状态提供给此模型的）的形状为`(batch_size,
    1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这很有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Seq2SeqTSPredictionOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSPredictionOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqTSPredictionOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSPredictionOutput)或`torch.FloatTensor`元组'
- en: A [transformers.modeling_outputs.Seq2SeqTSPredictionOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSPredictionOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig))
    and inputs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Seq2SeqTSPredictionOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSPredictionOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[AutoformerConfig](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when a `future_values`
    is provided) — Distributional loss.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`future_values`时返回) — 分布损失。'
- en: '`params` (`torch.FloatTensor` of shape `(batch_size, num_samples, num_params)`)
    — Parameters of the chosen distribution.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params` (`torch.FloatTensor`，形状为`(batch_size, num_samples, num_params)`) —
    所选分布的参数。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入层的输出+每层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — 每个层的输出（如果模型有嵌入层，则为嵌入的输出+每个层的输出）的`torch.FloatTensor`元组，形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每个层的输出以及初始嵌入输出的隐藏状态。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每个层一个），形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`loc` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Shift values of each time series’ context window which is used to
    give the model inputs of the same magnitude and then used to shift back to the
    original magnitude.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc` (`torch.FloatTensor`，形状为`(batch_size,)`或`(batch_size, input_size)`，*optional*)
    — 每个时间序列上下文窗口的偏移值，用于使模型输入具有相同的量级，然后用于将其偏移回原始量级。'
- en: '`scale` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Scaling values of each time series’ context window which is used
    to give the model inputs of the same magnitude and then used to rescale back to
    the original magnitude.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale` (`torch.FloatTensor`，形状为`(batch_size,)`或`(batch_size, input_size)`，*optional*)
    — 每个时间序列上下文窗口的缩放值，用于使模型输入具有相同的量级，然后用于将其重新缩放回原始量级。'
- en: '`static_features` (`torch.FloatTensor` of shape `(batch_size, feature size)`,
    *optional*) — Static features of each time series’ in a batch which are copied
    to the covariates at inference time.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_features` (`torch.FloatTensor`，形状为`(batch_size, feature size)`，*optional*)
    — 每个时间序列批次中的静态特征，在推断时复制到协变量中。'
- en: The [AutoformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerForPrediction)
    forward method, overrides the `__call__` special method.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[AutoformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/autoformer#transformers.AutoformerForPrediction)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
