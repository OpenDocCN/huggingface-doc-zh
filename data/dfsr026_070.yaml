- en: Latent Consistency Distillation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ½œåœ¨ä¸€è‡´æ€§è’¸é¦
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/lcm_distill](https://huggingface.co/docs/diffusers/training/lcm_distill)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/training/lcm_distill](https://huggingface.co/docs/diffusers/training/lcm_distill)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Latent Consistency Models (LCMs)](https://hf.co/papers/2310.04378) are able
    to generate high-quality images in just a few steps, representing a big leap forward
    because many pipelines require at least 25+ steps. LCMs are produced by applying
    the latent consistency distillation method to any Stable Diffusion model. This
    method works by applying *one-stage guided distillation* to the latent space,
    and incorporating a *skipping-step* method to consistently skip timesteps to accelerate
    the distillation process (refer to section 4.1, 4.2, and 4.3 of the paper for
    more details).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰](https://hf.co/papers/2310.04378)èƒ½å¤Ÿåœ¨å‡ ä¸ªæ­¥éª¤å†…ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œè¿™æ˜¯ä¸€ä¸ªé‡å¤§è¿›æ­¥ï¼Œå› ä¸ºè®¸å¤šæµç¨‹è‡³å°‘éœ€è¦25æ­¥ä»¥ä¸Šã€‚LCMsæ˜¯é€šè¿‡å°†æ½œåœ¨ä¸€è‡´æ€§è’¸é¦æ–¹æ³•åº”ç”¨äºä»»ä½•ç¨³å®šæ‰©æ•£æ¨¡å‹è€Œäº§ç”Ÿçš„ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†*å•é˜¶æ®µå¼•å¯¼è’¸é¦*åº”ç”¨äºæ½œåœ¨ç©ºé—´ï¼Œå¹¶ç»“åˆ*è·³æ­¥*æ–¹æ³•æ¥ä¸€è‡´åœ°è·³è¿‡æ—¶é—´æ­¥éª¤ä»¥åŠ é€Ÿè’¸é¦è¿‡ç¨‹ï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è®ºæ–‡çš„ç¬¬4.1ã€4.2å’Œ4.3èŠ‚ï¼‰ã€‚'
- en: If youâ€™re training on a GPU with limited vRAM, try enabling `gradient_checkpointing`,
    `gradient_accumulation_steps`, and `mixed_precision` to reduce memory-usage and
    speedup training. You can reduce your memory-usage even more by enabling memory-efficient
    attention with [xFormers](../optimization/xformers) and [bitsandbytesâ€™](https://github.com/TimDettmers/bitsandbytes)
    8-bit optimizer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨å…·æœ‰æœ‰é™vRAMçš„GPUä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯·å°è¯•å¯ç”¨`gradient_checkpointing`ã€`gradient_accumulation_steps`å’Œ`mixed_precision`ä»¥å‡å°‘å†…å­˜ä½¿ç”¨é‡å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡å¯ç”¨[xFormers](../optimization/xformers)å’Œ[bitsandbytes'](https://github.com/TimDettmers/bitsandbytes)
    8ä½ä¼˜åŒ–å™¨çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: This guide will explore the [train_lcm_distill_sd_wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sd_wds.py)
    script to help you become more familiar with it, and how you can adapt it for
    your own use-case.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†æ¢è®¨[train_lcm_distill_sd_wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sd_wds.py)è„šæœ¬ï¼Œå¸®åŠ©æ‚¨æ›´ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•ä¸ºæ‚¨è‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚
- en: 'Before running the script, make sure you install the library from source:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä»æºä»£ç å®‰è£…åº“ï¼š
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then navigate to the example folder containing the training script and install
    the required dependencies for the script youâ€™re using:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯¼èˆªåˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…æ‚¨æ­£åœ¨ä½¿ç”¨çš„è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ğŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. Itâ€™ll automatically configure your training setup based on your
    hardware and environment. Take a look at the ğŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—åŠ é€Ÿæ˜¯ä¸€ä¸ªå¸®åŠ©æ‚¨åœ¨å¤šä¸ªGPU/TPUä¸Šæˆ–ä½¿ç”¨æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒçš„åº“ã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®æ‚¨çš„è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ğŸ¤—åŠ é€Ÿ[å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour)ä»¥äº†è§£æ›´å¤šã€‚
- en: 'Initialize an ğŸ¤— Accelerate environment (try enabling `torch.compile` to significantly
    speedup training):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ğŸ¤—åŠ é€Ÿç¯å¢ƒï¼ˆå°è¯•å¯ç”¨`torch.compile`ä»¥æ˜¾è‘—åŠ å¿«è®­ç»ƒï¼‰ï¼š
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To setup a default ğŸ¤— Accelerate environment without choosing any configurations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®é»˜è®¤çš„ğŸ¤—åŠ é€Ÿç¯å¢ƒï¼Œä¸é€‰æ‹©ä»»ä½•é…ç½®ï¼š
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Or if your environment doesnâ€™t support an interactive shell, like a notebook,
    you can use:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼shellï¼Œæ¯”å¦‚ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºé€‚ç”¨äºè®­ç»ƒè„šæœ¬çš„æ•°æ®é›†ã€‚
- en: Script parameters
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è„šæœ¬å‚æ•°
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesnâ€™t cover every aspect of the script
    in detail. If youâ€™re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sd_wds.py)
    and let us know if you have any questions or concerns.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹éƒ¨åˆ†çªå‡ºæ˜¾ç¤ºäº†è®­ç»ƒè„šæœ¬çš„ä¸€äº›é‡è¦éƒ¨åˆ†ï¼Œä»¥å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¿®æ”¹å®ƒï¼Œä½†å¹¶æœªè¯¦ç»†æ¶µç›–è„šæœ¬çš„æ¯ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·éšæ—¶é˜…è¯»[è„šæœ¬](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sd_wds.py)ï¼Œå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ã€‚
- en: The training script provides many parameters to help you customize your training
    run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L419)
    function. This function provides default values for each parameter, such as the
    training batch size and learning rate, but you can also set your own values in
    the training command if youâ€™d like.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬æä¾›äº†è®¸å¤šå‚æ•°ï¼Œå¸®åŠ©æ‚¨å®šåˆ¶æ‚¨çš„è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰å‚æ•°åŠå…¶æè¿°å‡åœ¨[`parse_args()`](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L419)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚è¯¥å‡½æ•°ä¸ºæ¯ä¸ªå‚æ•°æä¾›äº†é»˜è®¤å€¼ï¼Œå¦‚è®­ç»ƒæ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚
- en: 'For example, to speedup training with mixed precision using the fp16 format,
    add the `--mixed_precision` parameter to the training command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸ºäº†åŠ å¿«ä½¿ç”¨fp16æ ¼å¼çš„æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒï¼Œå°†`--mixed_precision`å‚æ•°æ·»åŠ åˆ°è®­ç»ƒå‘½ä»¤ä¸­ï¼š
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Most of the parameters are identical to the parameters in the [Text-to-image](text2image#script-parameters)
    training guide, so youâ€™ll focus on the parameters that are relevant to latent
    consistency distillation in this guide.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°å‚æ•°ä¸[æ–‡æœ¬åˆ°å›¾åƒ](text2image#script-parameters)è®­ç»ƒæŒ‡å—ä¸­çš„å‚æ•°ç›¸åŒï¼Œå› æ­¤æ‚¨å°†ä¸“æ³¨äºæœ¬æŒ‡å—ä¸­ä¸æ½œåœ¨ä¸€è‡´æ€§è’¸é¦ç›¸å…³çš„å‚æ•°ã€‚
- en: '`--pretrained_teacher_model`: the path to a pretrained latent diffusion model
    to use as the teacher model'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--pretrained_teacher_model`ï¼šé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è·¯å¾„ï¼Œç”¨ä½œæ•™å¸ˆæ¨¡å‹'
- en: '`--pretrained_vae_model_name_or_path`: path to a pretrained VAE; the SDXL VAE
    is known to suffer from numerical instability, so this parameter allows you to
    specify an alternative VAE (like this [VAE]((https://huggingface.co/madebyollin/sdxl-vae-fp16-fix))
    by madebyollin which works in fp16)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--pretrained_vae_model_name_or_path`: é¢„è®­ç»ƒVAEçš„è·¯å¾„ï¼›SDXL VAEå·²çŸ¥å­˜åœ¨æ•°å€¼ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œå› æ­¤æ­¤å‚æ•°å…è®¸æ‚¨æŒ‡å®šå¦ä¸€ä¸ªVAEï¼ˆæ¯”å¦‚è¿™ä¸ª[VAE]((https://huggingface.co/madebyollin/sdxl-vae-fp16-fix))ï¼Œç”±madebyollinåˆ¶ä½œï¼Œå¯ä»¥åœ¨fp16ä¸­è¿è¡Œï¼‰'
- en: '`--w_min` and `--w_max`: the minimum and maximum guidance scale values for
    guidance scale sampling'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--w_min`å’Œ`--w_max`ï¼šæŒ‡å¯¼å°ºåº¦é‡‡æ ·çš„æœ€å°å’Œæœ€å¤§å€¼'
- en: '`--num_ddim_timesteps`: the number of timesteps for DDIM sampling'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--num_ddim_timesteps`: DDIMé‡‡æ ·çš„æ—¶é—´æ­¥æ•°'
- en: '`--loss_type`: the type of loss (L2 or Huber) to calculate for latent consistency
    distillation; Huber loss is generally preferred because itâ€™s more robust to outliers'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--loss_type`ï¼šç”¨äºè®¡ç®—æ½œåœ¨ä¸€è‡´æ€§è’¸é¦çš„æŸå¤±ç±»å‹ï¼ˆL2æˆ–Huberï¼‰ï¼›é€šå¸¸æ›´å–œæ¬¢HuberæŸå¤±ï¼Œå› ä¸ºå®ƒå¯¹å¼‚å¸¸å€¼æ›´åŠ ç¨³å¥'
- en: '`--huber_c`: the Huber loss parameter'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--huber_c`ï¼šHuberæŸå¤±å‚æ•°'
- en: Training script
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬
- en: The training script starts by creating a dataset class - [`Text2ImageDataset`](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L141)
    - for preprocessing the images and creating a training dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®é›†ç±» - [`Text2ImageDataset`](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L141)
    - ç”¨äºé¢„å¤„ç†å›¾åƒå¹¶åˆ›å»ºè®­ç»ƒæ•°æ®é›†ã€‚
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For improved performance on reading and writing large datasets stored in the
    cloud, this script uses the [WebDataset](https://github.com/webdataset/webdataset)
    format to create a preprocessing pipeline to apply transforms and create a dataset
    and dataloader for training. Images are processed and fed to the training loop
    without having to download the full dataset first.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜åœ¨äº‘ä¸­è¯»å–å’Œå†™å…¥å¤§å‹æ•°æ®é›†çš„æ€§èƒ½ï¼Œæ­¤è„šæœ¬ä½¿ç”¨[WebDataset](https://github.com/webdataset/webdataset)æ ¼å¼åˆ›å»ºä¸€ä¸ªé¢„å¤„ç†ç®¡é“ï¼Œåº”ç”¨å˜æ¢å¹¶åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ç”¨äºè®­ç»ƒã€‚å›¾åƒè¢«å¤„ç†å¹¶é¦ˆé€åˆ°è®­ç»ƒå¾ªç¯ä¸­ï¼Œè€Œæ— éœ€å…ˆä¸‹è½½å®Œæ•´æ•°æ®é›†ã€‚
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the [`main()`](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L768)
    function, all the necessary components like the noise scheduler, tokenizers, text
    encoders, and VAE are loaded. The teacher UNet is also loaded here and then you
    can create a student UNet from the teacher UNet. The student UNet is updated by
    the optimizer during training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[`main()`](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L768)å‡½æ•°ä¸­ï¼ŒåŠ è½½äº†æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼Œå¦‚å™ªå£°è°ƒåº¦ç¨‹åºã€æ ‡è®°å™¨ã€æ–‡æœ¬ç¼–ç å™¨å’ŒVAEã€‚æ•™å¸ˆUNetä¹Ÿåœ¨è¿™é‡ŒåŠ è½½ï¼Œç„¶åæ‚¨å¯ä»¥ä»æ•™å¸ˆUNetåˆ›å»ºä¸€ä¸ªå­¦ç”ŸUNetã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼˜åŒ–å™¨ä¼šæ›´æ–°å­¦ç”ŸUNetã€‚
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now you can create the [optimizer](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L979)
    to update the UNet parameters:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥åˆ›å»º[ä¼˜åŒ–å™¨](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L979)æ¥æ›´æ–°UNetå‚æ•°ï¼š
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create the [dataset](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L994):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»º[æ•°æ®é›†](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L994)ï¼š
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, youâ€™re ready to setup the [training loop](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L1049)
    and implement the latent consistency distillation method (see Algorithm 1 in the
    paper for more details). This section of the script takes care of adding noise
    to the latents, sampling and creating a guidance scale embedding, and predicting
    the original image from the noise.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥è®¾ç½®[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L1049)å¹¶å®ç°æ½œåœ¨ä¸€è‡´æ€§è’¸é¦æ–¹æ³•ï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è®ºæ–‡ä¸­çš„ç®—æ³•1ï¼‰ã€‚è„šæœ¬çš„è¿™ä¸€éƒ¨åˆ†è´Ÿè´£å‘æ½œåœ¨å˜é‡æ·»åŠ å™ªå£°ã€é‡‡æ ·å’Œåˆ›å»ºæŒ‡å¯¼å°ºåº¦åµŒå…¥ï¼Œå¹¶ä»å™ªå£°ä¸­é¢„æµ‹åŸå§‹å›¾åƒã€‚
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It gets the [teacher model predictions](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L1172)
    and the [LCM predictions](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L1209)
    next, calculates the loss, and then backpropagates it to the LCM.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè·å–[æ•™å¸ˆæ¨¡å‹é¢„æµ‹](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L1172)å’Œ[LCMé¢„æµ‹](https://github.com/huggingface/diffusers/blob/3b37488fa3280aed6a95de044d7a42ffdcb565ef/examples/consistency_distillation/train_lcm_distill_sd_wds.py#L1209)ï¼Œè®¡ç®—æŸå¤±ï¼Œç„¶åå°†å…¶åå‘ä¼ æ’­åˆ°LCMã€‚
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you want to learn more about how the training loop works, check out the [Understanding
    pipelines, models and schedulers tutorial](../using-diffusers/write_own_pipeline)
    which breaks down the basic pattern of the denoising process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³äº†è§£è®­ç»ƒå¾ªç¯çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹[ç†è§£ç®¡é“ã€æ¨¡å‹å’Œè°ƒåº¦å™¨æ•™ç¨‹](../using-diffusers/write_own_pipeline)ï¼Œè¯¥æ•™ç¨‹è§£æäº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚
- en: Launch the script
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯åŠ¨è„šæœ¬
- en: Now youâ€™re ready to launch the training script and start distilling!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¯åŠ¨è®­ç»ƒè„šæœ¬å¹¶å¼€å§‹è’¸é¦ï¼
- en: For this guide, youâ€™ll use the `--train_shards_path_or_url` to specify the path
    to the [Conceptual Captions 12M](https://github.com/google-research-datasets/conceptual-12m)
    dataset stored on the Hub [here](https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset).
    Set the `MODEL_DIR` environment variable to the name of the teacher model and
    `OUTPUT_DIR` to where you want to save the model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨`--train_shards_path_or_url`æ¥æŒ‡å®šå­˜å‚¨åœ¨Hubä¸Šçš„[Conceptual Captions 12M](https://github.com/google-research-datasets/conceptual-12m)æ•°æ®é›†çš„è·¯å¾„ã€‚å°†`MODEL_DIR`ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºæ•™å¸ˆæ¨¡å‹çš„åç§°ï¼Œå°†`OUTPUT_DIR`è®¾ç½®ä¸ºæ‚¨æƒ³è¦ä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once training is complete, you can use your new LCM for inference.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°çš„LCMè¿›è¡Œæ¨æ–­ã€‚
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: LoRA
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRA
- en: LoRA is a training technique for significantly reducing the number of trainable
    parameters. As a result, training is faster and it is easier to store the resulting
    weights because they are a lot smaller (~100MBs). Use the [train_lcm_distill_lora_sd_wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py)
    or [train_lcm_distill_lora_sdxl.wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sdxl_wds.py)
    script to train with LoRA.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LoRAæ˜¯ä¸€ç§æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡çš„è®­ç»ƒæŠ€æœ¯ã€‚å› æ­¤ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå­˜å‚¨ç»“æœæƒé‡æ›´å®¹æ˜“ï¼Œå› ä¸ºå®ƒä»¬è¦å°å¾—å¤šï¼ˆ~100MBï¼‰ã€‚ä½¿ç”¨[train_lcm_distill_lora_sd_wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py)æˆ–[train_lcm_distill_lora_sdxl.wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sdxl_wds.py)è„šæœ¬æ¥è¿›è¡ŒLoRAè®­ç»ƒã€‚
- en: The LoRA training script is discussed in more detail in the [LoRA training](lora)
    guide.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³LoRAè®­ç»ƒè„šæœ¬çš„è¯¦ç»†è®¨è®ºï¼Œè¯·å‚é˜…[LoRAè®­ç»ƒ](lora)æŒ‡å—ã€‚
- en: Stable Diffusion XL
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£XL
- en: Stable Diffusion XL (SDXL) is a powerful text-to-image model that generates
    high-resolution images, and it adds a second text-encoder to its architecture.
    Use the [train_lcm_distill_sdxl_wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sdxl_wds.py)
    script to train a SDXL model with LoRA.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£XLï¼ˆSDXLï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶åœ¨å…¶æ¶æ„ä¸­æ·»åŠ äº†ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨ã€‚ä½¿ç”¨[train_lcm_distill_sdxl_wds.py](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sdxl_wds.py)è„šæœ¬æ¥è®­ç»ƒä¸€ä¸ªå¸¦æœ‰LoRAçš„SDXLæ¨¡å‹ã€‚
- en: The SDXL training script is discussed in more detail in the [SDXL training](sdxl)
    guide.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³SDXLè®­ç»ƒè„šæœ¬çš„è¯¦ç»†è®¨è®ºï¼Œè¯·å‚é˜…[SDXLè®­ç»ƒ](sdxl)æŒ‡å—ã€‚
- en: Next steps
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'Congratulations on distilling a LCM model! To learn more about LCM, the following
    may be helpful:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œæ‚¨ç²¾ç‚¼LCMæ¨¡å‹ï¼è¦äº†è§£æ›´å¤šå…³äºLCMçš„ä¿¡æ¯ï¼Œä»¥ä¸‹å†…å®¹å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼š
- en: Learn how to use [LCMs for inference](../using-diffusers/lcm) for text-to-image,
    image-to-image, and with LoRA checkpoints.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•ä½¿ç”¨[LCMsè¿›è¡Œæ¨ç†](../using-diffusers/lcm)ç”¨äºæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒä»¥åŠå¸¦æœ‰LoRAæ£€æŸ¥ç‚¹ã€‚
- en: Read the [SDXL in 4 steps with Latent Consistency LoRAs](https://huggingface.co/blog/lcm_lora)
    blog post to learn more about SDXL LCM-LoRAâ€™s for super fast inference, quality
    comparisons, benchmarks, and more.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜…è¯»[SDXLåœ¨4ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨æ½œåœ¨ä¸€è‡´æ€§LoRAs](https://huggingface.co/blog/lcm_lora)åšå®¢æ–‡ç« ï¼Œäº†è§£æ›´å¤šå…³äºSDXL
    LCM-LoRAçš„ä¿¡æ¯ï¼Œç”¨äºè¶…å¿«æ¨ç†ã€è´¨é‡æ¯”è¾ƒã€åŸºå‡†æµ‹è¯•ç­‰ã€‚
