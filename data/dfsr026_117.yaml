- en: VQModel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VQModel
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/vq](https://huggingface.co/docs/diffusers/api/models/vq)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/api/models/vq](https://huggingface.co/docs/diffusers/api/models/vq)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The VQ-VAE model was introduced in [Neural Discrete Representation Learning](https://huggingface.co/papers/1711.00937)
    by Aaron van den Oord, Oriol Vinyals and Koray Kavukcuoglu. The model is used
    in 🤗 Diffusers to decode latent representations into images. Unlike [AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL),
    the [VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel) works
    in a quantized latent space.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-VAE模型是由Aaron van den Oord、Oriol Vinyals和Koray Kavukcuoglu在[神经离散表示学习](https://huggingface.co/papers/1711.00937)中引入的。该模型用于在🤗
    Diffusers中将潜在表示解码为图像。与[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)不同，[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)在量化的潜在空间中工作。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Learning useful representations without supervision remains a key challenge
    in machine learning. In this paper, we propose a simple yet powerful generative
    model that learns such discrete representations. Our model, the Vector Quantised-Variational
    AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs
    discrete, rather than continuous, codes; and the prior is learnt rather than static.
    In order to learn a discrete latent representation, we incorporate ideas from
    vector quantisation (VQ). Using the VQ method allows the model to circumvent issues
    of “posterior collapse” — where the latents are ignored when they are paired with
    a powerful autoregressive decoder — typically observed in the VAE framework. Pairing
    these representations with an autoregressive prior, the model can generate high
    quality images, videos, and speech as well as doing high quality speaker conversion
    and unsupervised learning of phonemes, providing further evidence of the utility
    of the learnt representations.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*在没有监督的情况下学习有用的表示仍然是机器学习中的一个关键挑战。在本文中，我们提出了一个简单而强大的生成模型，用于学习这种离散表示。我们的模型，即矢量量化变分自动编码器（VQ-VAE），与VAE在两个关键方面不同：编码器网络输出离散而不是连续的代码；先验是学习而不是静态的。为了学习离散的潜在表示，我们融入了矢量量化（VQ）的思想。使用VQ方法使模型能够避开“后验坍缩”的问题——即当潜在空间与强大的自回归解码器配对时，通常在VAE框架中观察到的问题。将这些表示与自回归先验配对，模型可以生成高质量的图像、视频和语音，以及进行高质量的说话人转换和无监督学习音素，进一步证明了学习表示的实用性。*'
- en: VQModel
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VQModel
- en: '### `class diffusers.VQModel`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.VQModel`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L40)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L40)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`in_channels` (int, *optional*, defaults to 3) — Number of channels in the
    input image.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels`（int，*可选*，默认为3）— 输入图像中的通道数。'
- en: '`out_channels` (int, *optional*, defaults to 3) — Number of channels in the
    output.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels`（int，*可选*，默认为3）— 输出中的通道数。'
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `("DownEncoderBlock2D",)`)
    — Tuple of downsample block types.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_types`（`Tuple[str]`，*可选*，默认为`("DownEncoderBlock2D",)`）— 下采样块类型的元组。'
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpDecoderBlock2D",)`)
    — Tuple of upsample block types.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_block_types`（`Tuple[str]`，*可选*，默认为`("UpDecoderBlock2D",)`）— 上采样块类型的元组。'
- en: '`block_out_channels` (`Tuple[int]`, *optional*, defaults to `(64,)`) — Tuple
    of block output channels.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_out_channels`（`Tuple[int]`，*可选*，默认为`(64,)`）— 块输出通道的元组。'
- en: '`layers_per_block` (`int`, *optional*, defaults to `1`) — Number of layers
    per block.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_per_block`（`int`，*可选*，默认为`1`）— 每个块的层数。'
- en: '`act_fn` (`str`, *optional*, defaults to `"silu"`) — The activation function
    to use.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn`（`str`，*可选*，默认为`"silu"`）— 要使用的激活函数。'
- en: '`latent_channels` (`int`, *optional*, defaults to `3`) — Number of channels
    in the latent space.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_channels`（`int`，*可选*，默认为`3`）— 潜在空间中的通道数。'
- en: '`sample_size` (`int`, *optional*, defaults to `32`) — Sample input size.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size`（`int`，*可选*，默认为`32`）— 样本输入大小。'
- en: '`num_vq_embeddings` (`int`, *optional*, defaults to `256`) — Number of codebook
    vectors in the VQ-VAE.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_vq_embeddings`（`int`，*可选*，默认为`256`）— VQ-VAE中码书向量的数量。'
- en: '`norm_num_groups` (`int`, *optional*, defaults to `32`) — Number of groups
    for normalization layers.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_num_groups`（`int`，*可选*，默认为`32`）— 规范化层的组数。'
- en: '`vq_embed_dim` (`int`, *optional*) — Hidden dim of codebook vectors in the
    VQ-VAE.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vq_embed_dim`（`int`，*可选*）— VQ-VAE中码书向量的隐藏维度。'
- en: '`scaling_factor` (`float`, *optional*, defaults to `0.18215`) — The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling_factor`（`float`，*可选*，默认为`0.18215`）— 使用训练集的第一批计算出的训练潜在空间的分量标准差。这用于在训练扩散模型时将潜在空间缩放为单位方差。在传递给扩散模型之前，潜在空间会按照公式`z
    = z * scaling_factor`进行缩放。在解码时，潜在空间会按照公式进行缩放回原始比例：`z = 1 / scaling_factor * z`。有关更多详细信息，请参阅[具有潜在扩散模型的高分辨率图像合成](https://arxiv.org/abs/2112.10752)论文的4.3.2和D.1节。'
- en: '`norm_type` (`str`, *optional*, defaults to `"group"`) — Type of normalization
    layer to use. Can be one of `"group"` or `"spatial"`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_type`（`str`，*可选*，默认为`"group"`）— 要使用的规范化层类型。可以是`"group"`或`"spatial"`之一。'
- en: A VQ-VAE model for decoding latent representations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用于解码潜在表示的VQ-VAE模型。
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for it’s generic methods implemented for all
    models (such as downloading or saving).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)。检查超类文档以了解为所有模型实现的通用方法（如下载或保存）。
- en: '#### `forward`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L158)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L158)'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sample` (`torch.FloatTensor`) — Input sample.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`（`torch.FloatTensor`）— 输入样本。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [models.vq_model.VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    instead of a plain tuple.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*，默认为`True`）— 是否返回一个[models.vq_model.VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    or `tuple`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    或 `tuple`'
- en: If return_dict is True, a [VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)
    is returned, otherwise a plain `tuple` is returned.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果return_dict为True，则返回一个[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)，否则返回一个普通的`tuple`。
- en: The [VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel) forward
    method.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)的前向方法。'
- en: VQEncoderOutput
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VQEncoderOutput
- en: '### `class diffusers.models.vq_model.VQEncoderOutput`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.vq_model.VQEncoderOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L27)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L27)'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`latents` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — The encoded output sample from the last layer of the model.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    模型最后一层的编码输出样本。'
- en: Output of VQModel encoding method.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: VQModel编码方法的输出。
