- en: PEFT configurations and models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PEFT配置和模型
- en: 'Original text: [https://huggingface.co/docs/peft/tutorial/peft_model_config](https://huggingface.co/docs/peft/tutorial/peft_model_config)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/peft/tutorial/peft_model_config](https://huggingface.co/docs/peft/tutorial/peft_model_config)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The sheer size of today’s large pretrained models - which commonly have billions
    of parameters - present a significant training challenge because they require
    more storage space and more computational power to crunch all those calculations.
    You’ll need access to powerful GPUs or TPUs to train these large pretrained models
    which is expensive, not widely accessible to everyone, not environmentally friendly,
    and not very practical. PEFT methods address many of these challenges. There are
    several types of PEFT methods (soft prompting, matrix decomposition, adapters),
    but they all focus on the same thing, reduce the number of trainable parameters.
    This makes it more accessible to train and store large models on consumer hardware.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如今的大型预训练模型之所以庞大 - 通常具有数十亿个参数 - 是因为它们需要更多的存储空间和更多的计算能力来处理所有这些计算。您需要访问强大的GPU或TPU来训练这些大型预训练模型，这是昂贵的，不是每个人都能轻易获得的，不环保，也不太实用。PEFT方法解决了许多这些挑战。有几种类型的PEFT方法（软提示、矩阵分解、适配器），但它们都专注于同一点，减少可训练参数的数量。这使得在消费者硬件上训练和存储大型模型更容易。
- en: The PEFT library is designed to help you quickly train large models on free
    or low-cost GPUs, and in this tutorial, you’ll learn how to setup a configuration
    to apply a PEFT method to a pretrained base model for training. Once the PEFT
    configuration is setup, you can use any training framework you like (Transformer’s
    [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class, [Accelerate](https://hf.co/docs/accelerate), a custom PyTorch training
    loop).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT库旨在帮助您快速在免费或低成本的GPU上训练大型模型，在本教程中，您将学习如何设置一个配置来应用PEFT方法到预训练基础模型进行训练。一旦PEFT配置设置完成，您可以使用任何您喜欢的训练框架（Transformer的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类，[Accelerate](https://hf.co/docs/accelerate)，自定义的PyTorch训练循环）。
- en: PEFT configurations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PEFT配置
- en: Learn more about the parameters you can configure for each PEFT method in their
    respective API reference page.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在各自的API参考页面中了解您可以为每种PEFT方法配置的参数。
- en: A configuration stores important parameters that specify how a particular PEFT
    method should be applied.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 配置存储了指定如何应用特定PEFT方法的重要参数。
- en: For example, take a look at the following [`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json)
    for applying LoRA and [`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json)
    for applying p-tuning (these configuration files are already JSON-serialized).
    Whenever you load a PEFT adapter, it is a good idea to check whether it has an
    associated adapter_config.json file which is required.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查看以下用于应用LoRA的[`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json)和用于应用p-tuning的[`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json)（这些配置文件已经是JSON序列化的）。每当加载PEFT适配器时，最好检查是否有一个必需的关联的adapter_config.json文件。
- en: LoraConfigPromptEncoderConfig
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LoraConfigPromptEncoderConfig
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can create your own configuration for training by initializing a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过初始化[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)来为训练创建自己的配置。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PEFT models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PEFT模型
- en: With a PEFT configuration in hand, you can now apply it to any pretrained model
    to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
    Choose from any of the state-of-the-art models from the [Transformers](https://hf.co/docs/transformers)
    library, a custom model, and even new and unsupported transformer architectures.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有了PEFT配置，您现在可以将其应用于任何预训练模型以创建[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。可以选择来自[Transformers](https://hf.co/docs/transformers)库的任何最先进的模型，自定义模型，甚至新的和不受支持的变压器架构。
- en: For this tutorial, load a base [facebook/opt-350m](https://huggingface.co/facebook/opt-350m)
    model to finetune.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，加载一个基础的[facebook/opt-350m](https://huggingface.co/facebook/opt-350m)模型进行微调。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Use the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    from the base facebook/opt-350m model and the `lora_config` you created earlier.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数从基础facebook/opt-350m模型和您之前创建的`lora_config`创建[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now you can train the [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    with your preferred training framework! After training, you can save your model
    locally with [save_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.save_pretrained)
    or upload it to the Hub with the [push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
    method.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用您喜欢的训练框架训练[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)！训练完成后，您可以使用[save_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.save_pretrained)将模型保存到本地，或使用[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)方法上传到Hub。
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To load a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    for inference, you’ll need to provide the [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
    used to create it and the base model it was trained from.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载用于推理的[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)，您需要提供用于创建它的[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)和它训练的基础模型。
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By default, the [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    is set for inference, but if you’d like to train the adapter some more you can
    set `is_trainable=True`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)设置为推理，但如果您想要进一步训练适配器，可以设置`is_trainable=True`。
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The [PeftModel.from_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)
    method is the most flexible way to load a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    because it doesn’t matter what model framework was used (Transformers, timm, a
    generic PyTorch model). Other classes, like [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel),
    are just a convenient wrapper around the base [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel),
    and makes it easier to load PEFT models directly from the Hub or locally where
    the PEFT weights are stored.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[PeftModel.from_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)方法是加载[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)最灵活的方式，因为不管使用了什么模型框架（Transformers、timm、通用的PyTorch模型）都可以。其他类，如[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)，只是基本[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)的一个方便包装器，使得可以更容易地直接从Hub或本地加载PEFT模型，其中存储了PEFT权重。'
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Take a look at the [AutoPeftModel](package_reference/auto_class) API reference
    to learn more about the [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    classes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[AutoPeftModel](package_reference/auto_class) API参考，了解更多关于[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类的信息。
- en: Next steps
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'With the appropriate [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig),
    you can apply it to any pretrained model to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    and train large powerful models faster on freely available GPUs! To learn more
    about PEFT configurations and models, the following guide may be helpful:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当的[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)，您可以将其应用于任何预训练模型，创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)，并在免费提供的GPU上更快地训练大型强大模型！要了解更多关于PEFT配置和模型的信息，以下指南可能会有所帮助：
- en: Learn how to configure a PEFT method for models that aren’t from Transformers
    in the [Working with custom models](../developer_guides/custom_models) guide.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何为不来自Transformers的模型配置PEFT方法，可以查看[使用自定义模型](../developer_guides/custom_models)指南。
