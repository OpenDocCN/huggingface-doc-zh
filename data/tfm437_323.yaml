- en: Wav2Vec2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wav2Vec2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Wav2Vec2模型是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[wav2vec
    2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)中提出的。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We show for the first time that learning powerful representations from speech
    audio alone followed by fine-tuning on transcribed speech can outperform the best
    semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the
    speech input in the latent space and solves a contrastive task defined over a
    quantization of the latent representations which are jointly learned. Experiments
    using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test
    sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms
    the previous state of the art on the 100 hour subset while using 100 times less
    labeled data. Using just ten minutes of labeled data and pre-training on 53k hours
    of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility
    of speech recognition with limited amounts of labeled data.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们首次展示，仅通过从语音音频中学习强大的表示，然后在转录的语音上进行微调，可以胜过最佳的半监督方法，同时在概念上更简单。wav2vec 2.0在潜在空间中屏蔽语音输入，并解决了一个在联合学习的潜在表示的量化上定义的对比任务。使用Librispeech的所有标记数据进行的实验在干净/其他测试集上实现了1.8/3.3的WER。当将标记数据量降低到一小时时，wav2vec
    2.0在100小时子集上胜过了先前的最先进技术，同时使用的标记数据量减少了100倍。仅使用十分钟的标记数据并在53k小时的未标记数据上进行预训练仍然实现了4.8/8.2的WER。这证明了在有限的标记数据量下进行语音识别的可行性。*'
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: Wav2Vec2 is a speech model that accepts a float array corresponding to the raw
    waveform of the speech signal.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wav2Vec2是一个接受与语音信号的原始波形对应的浮点数组的语音模型。
- en: Wav2Vec2 model was trained using connectionist temporal classification (CTC)
    so the model output has to be decoded using [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wav2Vec2模型是使用连接主义时间分类（CTC）进行训练的，因此模型输出必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)进行解码。
- en: Resources
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with Wav2Vec2\. If you’re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and we’ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一份官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用Wav2Vec2。如果您有兴趣提交资源以包含在此处，请随时打开Pull Request，我们将进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。
- en: Audio Classification
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 音频分类
- en: A notebook on how to [leverage a pretrained Wav2Vec2 model for emotion classification](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb).
    🌎
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关于如何[利用预训练的Wav2Vec2模型进行情感分类](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)的笔记本。🌎
- en: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)受到这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)的支持。'
- en: '[Audio classification task guide](../tasks/audio_classification)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[音频分类任务指南](../tasks/audio_classification)'
- en: Automatic Speech Recognition
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别
- en: A blog post on [boosting Wav2Vec2 with n-grams in 🤗 Transformers](https://huggingface.co/blog/wav2vec2-with-ngram).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇关于[在🤗 Transformers中使用n-grams增强Wav2Vec2的博客文章](https://huggingface.co/blog/wav2vec2-with-ngram)。
- en: A blog post on how to [finetune Wav2Vec2 for English ASR with 🤗 Transformers](https://huggingface.co/blog/fine-tune-wav2vec2-english).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇关于如何[使用🤗 Transformers对英语ASR进行微调的博客文章](https://huggingface.co/blog/fine-tune-wav2vec2-english)。
- en: A blog post on [finetuning XLS-R for Multi-Lingual ASR with 🤗 Transformers](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇关于[使用🤗 Transformers对多语言ASR进行微调的博客文章](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)。
- en: A notebook on how to [create YouTube captions from any video by transcribing
    audio with Wav2Vec2](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb).
    🌎
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关于如何[通过使用Wav2Vec2转录音频从任何视频创建YouTube字幕](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)的笔记本。🌎
- en: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    is supported by a notebook on [how to finetune a speech recognition model in English](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb),
    and [how to finetune a speech recognition model in any language](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)受到一篇关于[如何在英语中微调语音识别模型的笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)和[如何在任何语言中微调语音识别模型的笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)的支持。'
- en: '[Automatic speech recognition task guide](../tasks/asr)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动语音识别任务指南](../tasks/asr)'
- en: 🚀 Deploy
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 部署
- en: A blog post on how to deploy Wav2Vec2 for [Automatic Speech Recogntion with
    Hugging Face’s Transformers & Amazon SageMaker](https://www.philschmid.de/automatic-speech-recognition-sagemaker).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何在[Hugging Face的Transformers和Amazon SageMaker中部署Wav2Vec2进行自动语音识别的博文](https://www.philschmid.de/automatic-speech-recognition-sagemaker)。
- en: Wav2Vec2Config
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2Config
- en: '### `class transformers.Wav2Vec2Config`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/configuration_wav2vec2.py#L32)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/configuration_wav2vec2.py#L32)'
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 32) — Vocabulary size of the Wav2Vec2
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    or [TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model).
    Vocabulary size of the model. Defines the different tokens that can be represented
    by the *inputs_ids* passed to the forward method of [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 32) — Wav2Vec2模型的词汇表大小。定义了在调用[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)或[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)时可以表示的不同标记数量。模型的词汇表大小。定义了在调用[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)的forward方法时可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有完全连接层的dropout概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 完全连接层内激活的dropout比率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the final projection layer of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_dropout` (`float`, *optional*, defaults to 0.1) — [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)最终投影层的dropout概率。'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop probability.
    See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.1) — LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](see
    [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。'
- en: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — The norm to
    be applied to 1D convolutional layers in feature encoder. One of `"group"` for
    group normalization of only the first 1D convolutional layer or `"layer"` for
    layer normalization of all 1D convolutional layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — 应用于特征编码器中1D卷积层的规范化。`"group"`表示仅对第一个1D卷积层进行组归一化，`"layer"`表示对所有1D卷积层进行层归一化。'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for output of the feature encoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 特征编码器输出的dropout概率。'
- en: '`feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` “gelu”`,` “relu”`,` “selu”`and`“gelu_new”` are
    supported.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- 特征提取器中1D卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持`“gelu”`、`“relu”`、`“selu”`和`“gelu_new”`。'
- en: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for quantized feature encoder states.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — 量化特征编码器状态的dropout概率。'
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — A tuple of integers defining the number of input
    and output channels of each 1D convolutional layer in the feature encoder. The
    length of *conv_dim* defines the number of 1D convolutional layers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — 一个整数元组，定义特征编码器中每个1D卷积层的输入和输出通道数。*conv_dim*的长度定义了1D卷积层的数量。'
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the feature encoder. The length of *conv_stride* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — 在特征编码器中每个1D卷积层的步幅的整数元组。*conv_stride*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the feature encoder. The length of *conv_kernel* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) — 在特征编码器中每个1D卷积层的卷积核大小的整数元组。*conv_kernel*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) — Whether the 1D convolutional
    layers have a bias.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_bias` (`bool`, *optional*, defaults to `False`) — 1D卷积层是否具有偏置。'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了1D卷积位置嵌入层的卷积核大小。'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D卷积位置嵌入层的组数。'
- en: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — Whether
    to apply *stable* layer norm architecture of the Transformer encoder. `do_stable_layer_norm
    is True` corresponds to applying layer norm before the attention layer, whereas
    `do_stable_layer_norm is False` corresponds to applying layer norm after the attention
    layer.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — 是否应用Transformer编码器的*stable*层归一化架构。`do_stable_layer_norm为True`表示在注意力层之前应用层归一化，而`do_stable_layer_norm为False`表示在注意力层之后应用层归一化。'
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — Whether to
    apply *SpecAugment* data augmentation to the outputs of the feature encoder. For
    reference see [SpecAugment: A Simple Data Augmentation Method for Automatic Speech
    Recognition](https://arxiv.org/abs/1904.08779).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — 是否将*SpecAugment*数据增强应用于特征编码器的输出。有关详细信息，请参阅[SpecAugment:
    A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)。'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates ”mask_time_prob*len(time_axis)/mask_time_length” independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if` apply_spec_augment is
    True`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — 沿时间轴的所有特征向量中将被掩盖的百分比（介于0和1之间）。掩码过程在轴上生成”mask_time_prob*len(time_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为掩盖的向量跨度起始的概率推理，*
    mask_time_prob *应为`prob_vector_start*mask_time_length`。请注意，重叠可能会降低实际掩盖向量的百分比。仅在`apply_spec_augment为True`时相关。'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) — Length of vector span
    along the time axis.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *optional*, defaults to 10) — 沿时间轴的向量跨度长度。'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), — The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), — 沿时间轴生成的长度为`mask_feature_length`的最小掩码数量，每个时间步，与`mask_feature_prob`无关。仅在”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”时相关'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates ”mask_feature_prob*len(feature_axis)/mask_time_length”
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — 沿特征轴的所有特征向量中将被掩盖的百分比（介于0和1之间）。掩码过程在轴上生成”mask_feature_prob*len(feature_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为掩盖的向量跨度起始的概率推理，*
    mask_feature_prob *应为`prob_vector_start*mask_feature_length`。请注意，重叠可能会降低实际掩盖向量的百分比。仅在`apply_spec_augment为True`时相关。'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) — Length of vector
    span along the feature axis.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *optional*, defaults to 10) — 沿特征轴的向量跨度长度。'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), — The minimum
    number of masks of length `mask_feature_length` generated along the feature axis,
    each time step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), — 沿特征轴生成的长度为`mask_feature_length`的最小掩码数量，每个时间步，与`mask_feature_prob`无关。仅在”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”时相关'
- en: '`num_codevectors_per_group` (`int`, *optional*, defaults to 320) — Number of
    entries in each quantization codebook (group).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevectors_per_group` (`int`, *optional*, defaults to 320) — 每个量化码书（组）中的条目数。'
- en: '`num_codevector_groups` (`int`, *optional*, defaults to 2) — Number of codevector
    groups for product codevector quantization.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevector_groups` (`int`, *optional*, defaults to 2) — 产品码矢量量化的码矢量组数。'
- en: '`contrastive_logits_temperature` (`float`, *optional*, defaults to 0.1) — The
    temperature *kappa* in the contrastive loss.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_logits_temperature` (`float`, *可选*, 默认为 0.1) — 对比损失中的温度 *kappa*。'
- en: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for the output of the feature encoder that’s used by the quantizer.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_quantizer_dropout` (`float`, *可选*, 默认为 0.0) — 用于量化器使用的特征编码器输出的丢弃概率。'
- en: '`num_negatives` (`int`, *optional*, defaults to 100) — Number of negative samples
    for the contrastive loss.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_negatives` (`int`, *可选*, 默认为 100) — 对比损失的负样本数量。'
- en: '`codevector_dim` (`int`, *optional*, defaults to 256) — Dimensionality of the
    quantized feature vectors.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codevector_dim` (`int`, *可选*, 默认为 256) — 量化特征向量的维度。'
- en: '`proj_codevector_dim` (`int`, *optional*, defaults to 256) — Dimensionality
    of the final projection of both the quantized and the transformer features.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proj_codevector_dim` (`int`, *可选*, 默认为 256) — 最终投影的维度，包括量化特征和变换器特征。'
- en: '`diversity_loss_weight` (`int`, *optional*, defaults to 0.1) — The weight of
    the codebook diversity loss component.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss_weight` (`int`, *可选*, 默认为 0.1) — 代码本多样性损失组件的权重。'
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) — Specifies the
    reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training
    an instance of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_loss_reduction` (`str`, *可选*, 默认为 `"sum"`) — 指定应用于 `torch.nn.CTCLoss`
    输出的减少方式。仅在训练 [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    实例时相关。'
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_zero_infinity` (`bool`, *可选*, 默认为 `False`) — 是否将 `torch.nn.CTCLoss` 的无限损失和相关梯度置零。当输入太短无法与目标对齐时，主要会出现无限损失。仅在训练
    [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    实例时相关。'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`, *可选*, 默认为 `False`) — 是否使用具有学习权重的层输出的加权平均。仅在使用
    [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)
    实例时相关。'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) — Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *可选*, 默认为 256) — 用于分类的令牌均值池化之前的投影维度。'
- en: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) — A tuple of integers defining the number of output channels
    of each 1D convolutional layer in the *TDNN* module of the *XVector* model. The
    length of *tdnn_dim* defines the number of *TDNN* layers.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dim` (`Tuple[int]` 或 `List[int]`, *可选*, 默认为 `(512, 512, 512, 512, 1500)`)
    — 一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个一维卷积层的输出通道数。*tdnn_dim* 的长度定义了 *TDNN* 层的数量。'
- en: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the *TDNN* module of the *XVector* model. The length of *tdnn_kernel*
    has to match the length of *tdnn_dim*.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_kernel` (`Tuple[int]` 或 `List[int]`, *可选*, 默认为 `(5, 3, 3, 1, 1)`) — 一个整数元组，定义了
    *XVector* 模型中 *TDNN* 模块中每个一维卷积层的内核大小。*tdnn_kernel* 的长度必须与 *tdnn_dim* 的长度相匹配。'
- en: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) — A tuple of integers defining the dilation factor of each 1D convolutional
    layer in *TDNN* module of the *XVector* model. The length of *tdnn_dilation* has
    to match the length of *tdnn_dim*.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dilation` (`Tuple[int]` 或 `List[int]`, *可选*, 默认为 `(1, 2, 3, 1, 1)`) —
    一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个一维卷积层的膨胀因子。*tdnn_dilation* 的长度必须与 *tdnn_dim*
    的长度相匹配。'
- en: '`xvector_output_dim` (`int`, *optional*, defaults to 512) — Dimensionality
    of the *XVector* embedding vectors.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xvector_output_dim` (`int`, *可选*, 默认为 512) — *XVector* 嵌入向量的维度。'
- en: '`add_adapter` (`bool`, *optional*, defaults to `False`) — Whether a convolutional
    network should be stacked on top of the Wav2Vec2 Encoder. Can be very useful for
    warm-starting Wav2Vec2 for SpeechEncoderDecoder models.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_adapter` (`bool`, *可选*, 默认为 `False`) — 是否在 Wav2Vec2 编码器顶部堆叠卷积网络。对于 Warm-starting
    Wav2Vec2 for SpeechEncoderDecoder 模型非常有用。'
- en: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) — Kernel size of the
    convolutional layers in the adapter network. Only relevant if `add_adapter is
    True`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_kernel_size` (`int`, *可选*, 默认为 3) — 适配器网络中卷积层的内核大小。仅在 `add_adapter`
    为 True 时相关。'
- en: '`adapter_stride` (`int`, *optional*, defaults to 2) — Stride of the convolutional
    layers in the adapter network. Only relevant if `add_adapter is True`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_stride` (`int`, *可选*, 默认为 2) — 适配器网络中卷积层的步幅。仅在 `add_adapter` 为 True
    时相关。'
- en: '`num_adapter_layers` (`int`, *optional*, defaults to 3) — Number of convolutional
    layers that should be used in the adapter network. Only relevant if `add_adapter
    is True`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_adapter_layers` (`int`, *可选*, 默认为 3) — 适配器网络中应使用的卷积层数量。仅在 `add_adapter`
    为 True 时相关。'
- en: '`adapter_attn_dim` (`int`, *optional*) — Dimension of the attention adapter
    weights to be used in each attention block. An example of a model using attention
    adapters is [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_attn_dim` (`int`, *可选*) — 每个注意力块中要使用的注意力适配器权重的维度。使用注意力适配器的模型示例是 [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)。'
- en: '`output_hidden_size` (`int`, *optional*) — Dimensionality of the encoder output
    layer. If not defined, this defaults to *hidden-size*. Only relevant if `add_adapter
    is True`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_size` (`int`, *可选*) — 编码器输出层的维度。如果未定义，则默认为 *hidden-size*。仅在
    `add_adapter` 为 True 时相关。'
- en: This is the configuration class to store the configuration of a [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model).
    It is used to instantiate an Wav2Vec2 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Wav2Vec2 [facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h)
    architecture.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    配置的配置类。它用于根据指定的参数实例化 Wav2Vec2 模型，定义模型架构。使用默认值实例化配置将产生类似于 Wav2Vec2 [facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Wav2Vec2CTCTokenizer
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2CTCTokenizer
- en: '### `class transformers.Wav2Vec2CTCTokenizer`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2CTCTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L127)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L127)'
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含词汇表的文件。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sentence
    token.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 句子开头标记。'
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sentence
    token.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 句子结束标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`word_delimiter_token` (`str`, *optional*, defaults to `"|"`) — The token used
    for defining the end of a word.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_delimiter_token` (`str`, *可选*, 默认为 `"|"`) — 用于定义单词结尾的标记。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `False`) — Whether or not
    to accept lowercase input and lowercase the output when decoding.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *可选*, 默认为 `False`) — 是否接受小写输入并在解码时将输出转换为小写。'
- en: '`target_lang` (`str`, *optional*) — A target language the tokenizer should
    set by default. `target_lang` has to be defined for multi-lingual, nested vocabulary
    such as [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_lang` (`str`, *可选*) — 分词器应默认设置的目标语言。对于多语言、嵌套词汇表，如 [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)，必须定义
    `target_lang`。'
- en: '**kwargs — Additional keyword arguments passed along to [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kwargs — 传递给 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    的额外关键字参数'
- en: Constructs a Wav2Vec2CTC tokenizer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 Wav2Vec2CTC 分词器。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains some of the main methods. Users should refer to the superclass
    for more information regarding such methods.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含一些主要方法。用户应参考超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置
    `is_split_into_words=True`（以消除与批处理序列的歧义）。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置
    `is_split_into_words=True`（以消除与批处理序列的歧义）。'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置
    `is_split_into_words=True`（以消除与批处理序列的歧义）。'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批量序列。每个序列可以是一个字符串或一个字符串列表（预先标记化的字符串）。如果序列以字符串列表（预先标记化）的形式提供，则必须设置`is_split_into_words=True`（以消除批量序列的歧义）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, 默认为`True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, 默认为`False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批量中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到由参数`max_length`指定的最大长度，或者填充到模型的最大可接受输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：不填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, 默认为`False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`：截断到由参数`max_length`指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对），则将逐标记截断，从一对序列中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：截断到由参数`max_length`指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`：截断到由参数`max_length`指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数之一需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, 默认为0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *optional*, 默认为`False`) — 输入是否已经预标记化（例如，已分割为单词）。如果设置为`True`，则分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行标记化。这对于命名实体识别或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于在具有计算能力`>=
    7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回Numpy `np.ndarray`对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *可选*) — 是否返回token type IDs。如果保持默认设置，将根据特定标记化器的默认值返回token
    type IDs，由`return_outputs`属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token type IDs？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定标记化器的默认值返回注意力掩码，由`return_outputs`属性定义。 '
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是attention masks？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *可选*, 默认为 `False`) — 是否返回溢出的token序列。如果提供了一对输入id序列（或一批对）并且`truncation_strategy
    = longest_first`或`True`，则会引发错误，而不是返回溢出的tokens。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *可选*, 默认为 `False`) — 是否返回特殊token掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *可选*, 默认为 `False`) — 是否返回每个token的`(char_start,
    char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速标记化器，如果使用Python的标记化器，此方法将引发`NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *可选*, 默认为 `False`) — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *可选*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法'
- en: Returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要提供给模型的token id列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是input IDs？](../glossary#input-ids)'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要提供给模型的token type ids列表（当`return_token_type_ids=True`或*“token_type_ids”*在`self.model_input_names`中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token type IDs？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些token应该被模型关注的索引列表（当`return_attention_mask=True`或*“attention_mask”*在`self.model_input_names`中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是attention masks？](../glossary#attention-mask)'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出的token序列列表（当指定了`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 被截断的token数量（当指定了`max_length`并且`return_overflowing_tokens=True`时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊token，0指定常规序列token（当`add_special_tokens=True`且`return_special_tokens_mask=True`时）。'
- en: '`length` — The length of the inputs (when `return_length=True`)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当`return_length=True`时）'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将主要方法标记化并为模型准备一个或多个序列或一个或多个序列对。
- en: '#### `save_vocabulary`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L646)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L646)'
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `decode`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L541)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L541)'
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — 分词后的输入id列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens` (`bool`，*可选*，默认为`False`) — 是否在解码中删除特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`，*可选*） — 是否清理分词空格。'
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_char_offsets` (`bool`，*可选*，默认为`False`) — 是否输出字符偏移量。字符偏移量可以与采样率和模型下采样率结合使用，计算转录字符的时间戳。'
- en: Please take a look at the example below to better understand how to make use
    of `output_char_offsets`.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看下面的示例，以更好地理解如何使用`output_char_offsets`。
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`，*可选*，默认为`False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，计算转录单词的时间戳。'
- en: Please take a look at the example below to better understand how to make use
    of `output_word_offsets`.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看下面的示例，以更好地理解如何使用`output_word_offsets`。
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（其他关键字参数，*可选*） — 将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`str` or `Wav2Vec2CTCTokenizerOutput`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`或`Wav2Vec2CTCTokenizerOutput`'
- en: The list of decoded sentences. Will be a `Wav2Vec2CTCTokenizerOutput` when `output_char_offsets
    == True` or `output_word_offsets == True`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的句子列表。当`output_char_offsets == True`或`output_word_offsets == True`时，将是`Wav2Vec2CTCTokenizerOutput`。
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列id转换为字符串，使用分词器和词汇表，可以选择删除特殊标记并清理分词空格。
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于执行`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。
- en: 'Example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `batch_decode`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L471)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L471)'
- en: '[PRE7]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    — 分词后的输入id列表。可以使用`__call__`方法获得。'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens` (`bool`，*可选*，默认为`False`) — 是否在解码中删除特殊标记。'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`，*可选*) — 是否清理分词空格。'
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_char_offsets` (`bool`，*可选*，默认为`False`) — 是否输出字符偏移量。字符偏移量可以与采样率和模型下采样率结合使用，计算转录字符的时间戳。'
- en: Please take a look at the Example of [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)
    to better understand how to make use of `output_char_offsets`. [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)
    works the same way with batched output.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)的示例，以更好地理解如何使用`output_char_offsets`。[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)与批量输出的方式相同。
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`，*可选*，默认为`False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，计算转录单词的时间戳。'
- en: Please take a look at the Example of [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)
    to better understand how to make use of `output_word_offsets`. [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)
    works the same way with batched output.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)的示例，以更好地理解如何使用`output_word_offsets`。[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)与批量输出的方式相同。
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（其他关键字参数，*可选*） — 将传递给底层模型特定的解码方法。'
- en: Returns
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[str]` or `Wav2Vec2CTCTokenizerOutput`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`或`Wav2Vec2CTCTokenizerOutput`'
- en: The list of decoded sentences. Will be a `Wav2Vec2CTCTokenizerOutput` when `output_char_offsets
    == True` or `output_word_offsets == True`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 解码后的句子列表。当`output_char_offsets == True`或`output_word_offsets == True`时，将是`Wav2Vec2CTCTokenizerOutput`。
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用解码函数，将一系列token id的列表转换为字符串列表。
- en: '#### `set_target_lang`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_target_lang`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L213)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L213)'
- en: '[PRE8]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Set the target language of a nested multi-lingual dictionary
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 设置嵌套多语言字典的目标语言
- en: Wav2Vec2FeatureExtractor
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2FeatureExtractor
- en: '### `class transformers.Wav2Vec2FeatureExtractor`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2FeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L31)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L31)'
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_size` (`int`, defaults to 1) — The feature dimension of the extracted
    features.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size`（`int`，默认为1）— 提取特征的特征维度。'
- en: '`sampling_rate` (`int`, defaults to 16000) — The sampling rate at which the
    audio files should be digitalized expressed in hertz (Hz).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`（`int`，默认为16000）— 应以赫兹（Hz）表示的音频文件数字化的采样率。'
- en: '`padding_value` (`float`, defaults to 0.0) — The value that is used to fill
    the padding values.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value`（`float`，默认为0.0）— 用于填充值的值。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether or not to
    zero-mean unit-variance normalize the input. Normalizing can help to significantly
    improve the performance for some models, *e.g.*, [wav2vec2-lv60](https://huggingface.co/models?search=lv60).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize`（`bool`，*可选*，默认为`True`）— 是否对输入进行零均值单位方差归一化。归一化可以帮助一些模型显著提高性能，例如
    [wav2vec2-lv60](https://huggingface.co/models?search=lv60)。'
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not [`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    should return `attention_mask`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`（`bool`，*可选*，默认为`False`）— 是否 [`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    应该返回 `attention_mask`。'
- en: Wav2Vec2 models that have set `config.feat_extract_norm == "group"`, such as
    [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), have `not`
    been trained using `attention_mask`. For such models, `input_values` should simply
    be padded with 0 and no `attention_mask` should be passed.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置了 `config.feat_extract_norm == "group"` 的 Wav2Vec2 模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，没有使用
    `attention_mask` 进行训练。对于这样的模型，`input_values` 应该简单地用 0 填充，不应传递 `attention_mask`。
- en: For Wav2Vec2 models that have set `config.feat_extract_norm == "layer"`, such
    as [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self),
    `attention_mask` should be passed for batched inference.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于设置了 `config.feat_extract_norm == "layer"` 的 Wav2Vec2 模型，例如 [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)，应该为批量推断传递
    `attention_mask`。
- en: Constructs a Wav2Vec2 feature extractor.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 Wav2Vec2 特征提取器。
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此特征提取器继承自 [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L102)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L102)'
- en: '[PRE10]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    — The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_speech`（`np.ndarray`，`List[float]`，`List[np.ndarray]`，`List[List[float]]`）—
    要填充的序列或批次序列。每个序列可以是一个 numpy 数组，一个浮点值列表，一个 numpy 数组列表或一个浮点值列表的列表。必须是单声道音频，不是立体声，即每个时间步长一个浮点数。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）—
    选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定参数 `max_length` 的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：无填充（即，可以输出具有不同长度序列的批次）。'
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 返回列表的最大长度和可选填充长度（见上文）。'
- en: '`truncation` (`bool`) — Activates truncation to cut input sequences longer
    than *max_length* to *max_length*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`）— 激活截断，将输入序列截断为比 *max_length* 更长的序列到 *max_length*。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将填充序列到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力 `>= 7.5`（Volta），或者对于受益于序列长度为 128 的倍数的
    TPU。
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractor’s default.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`（`bool`，*可选*）— 是否返回注意力掩码。如果保持默认值，将根据特定 feature_extractor
    的默认值返回注意力掩码。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Wav2Vec2 models that have set `config.feat_extract_norm == "group"`, such as
    [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), have `not`
    been trained using `attention_mask`. For such models, `input_values` should simply
    be padded with 0 and no `attention_mask` should be passed.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于设置了 `config.feat_extract_norm == "group"` 的 Wav2Vec2 模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，没有使用
    `attention_mask` 进行训练。对于这样的模型，`input_values` 应该简单地用 0 填充，不应传递 `attention_mask`。
- en: For Wav2Vec2 models that have set `config.feat_extract_norm == "layer"`, such
    as [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self),
    `attention_mask` should be passed for batched inference.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于设置了 `config.feat_extract_norm == "layer"` 的 Wav2Vec2 模型，例如 [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)，应该为批量推断传递
    `attention_mask`。
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *可选*) — `raw_speech` 输入采样的采样率。强烈建议在前向调用时传递 `sampling_rate`
    以防止静默错误。'
- en: '`padding_value` (`float`, defaults to 0.0) —'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value` (`float`, 默认为 0.0) —'
- en: Main method to featurize and prepare for the model one or several sequence(s).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对一个或多个序列进行特征化和为模型准备的主要方法。
- en: Wav2Vec2Processor
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2Processor
- en: '### `class transformers.Wav2Vec2Processor`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2Processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L26)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[`<来源>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L26)'
- en: '[PRE11]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_extractor` (`Wav2Vec2FeatureExtractor`) — An instance of [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` (`Wav2Vec2FeatureExtractor`) — [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    的一个实例。特征提取器是必需的输入。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — An instance of [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    The tokenizer is a required input.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    的一个实例。分词器是必需的输入。'
- en: Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor and
    a Wav2Vec2 CTC tokenizer into a single processor.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 Wav2Vec2 处理器，将 Wav2Vec2 特征提取器和 Wav2Vec2 CTC 分词器封装成一个单一处理器。
- en: '[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    offers all the functionalities of [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    and [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    and [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode)
    for more information.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    提供了 [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    和 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    的所有功能。查看 [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    和 [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode)
    的文档字符串以获取更多信息。'
- en: '#### `__call__`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L68)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[`<来源>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L68)'
- en: '[PRE12]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to PreTrainedTokenizer’s [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常模式下使用时，此方法将所有参数转发到 Wav2Vec2FeatureExtractor 的 [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    并返回其输出。如果在上下文 `as_target_processor()` 中使用此方法，将所有参数转发到 PreTrainedTokenizer 的 [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。请参考上述两个方法的文档字符串以获取更多信息。
- en: '#### `pad`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pad`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L106)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[`<来源>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L106)'
- en: '[PRE13]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor’s
    [pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to PreTrainedTokenizer’s [pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常模式下使用时，此方法将所有参数转发到Wav2Vec2FeatureExtractor的[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)并返回其输出。如果在上下文`as_target_processor()`中使用，此方法将所有参数转发到PreTrainedTokenizer的[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)。有关更多信息，请参考上述两种方法的文档字符串。
- en: '#### `from_pretrained`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L49)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L49)'
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#### `save_pretrained`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
- en: '[PRE15]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str` or `os.PathLike`) — Directory where the feature extractor
    JSON file and the tokenizer files will be saved (directory will be created if
    it does not exist).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`（`str`或`os.PathLike`）— 特征提取器JSON文件和分词器文件将保存在的目录（如果目录不存在，则将创建目录）。'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub`（`bool`，*可选*，默认为`False`）— 是否在保存后将模型推送到Hugging Face模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`名称）。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）— 传递给[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法的额外关键字参数。'
- en: Saves the attributes of this processor (feature extractor, tokenizer…) in the
    specified directory so that it can be reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)
    method.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将此处理器的属性（特征提取器、分词器等）保存在指定目录中，以便可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)方法重新加载。
- en: This class method is simply calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained).
    Please refer to the docstrings of the methods above for more information.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类方法只是调用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)和[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)。有关更多信息，请参考上述方法的文档字符串。
- en: '#### `batch_decode`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L136)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L136)'
- en: '[PRE16]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将所有参数转发到PreTrainedTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参考此方法的文档字符串。
- en: '#### `decode`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L143)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L143)'
- en: '[PRE17]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将所有参数转发到PreTrainedTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参考此方法的文档字符串。
- en: Wav2Vec2ProcessorWithLM
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ProcessorWithLM
- en: '### `class transformers.Wav2Vec2ProcessorWithLM`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ProcessorWithLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L67)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L67)'
- en: '[PRE18]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_extractor` ([Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor))
    — An instance of [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`（[Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor））—
    [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)的一个实例。特征提取器是必需的输入。'
- en: '`tokenizer` ([Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer))
    — An instance of [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
    The tokenizer is a required input.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer））—
    [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)的一个实例。分词器是必需的输入。'
- en: '`decoder` (`pyctcdecode.BeamSearchDecoderCTC`) — An instance of `pyctcdecode.BeamSearchDecoderCTC`.
    The decoder is a required input.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder`（`pyctcdecode.BeamSearchDecoderCTC`）— `pyctcdecode.BeamSearchDecoderCTC`的一个实例。解码器是必需的输入。'
- en: Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor, a
    Wav2Vec2 CTC tokenizer and a decoder with language model support into a single
    processor for language model boosted speech recognition decoding.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Wav2Vec2处理器，将Wav2Vec2特征提取器、Wav2Vec2 CTC分词器和具有语言模型支持的解码器包装到一个单一的处理器中，用于语言模型增强的语音识别解码。
- en: '#### `__call__`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L215)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L215)'
- en: '[PRE19]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to Wav2Vec2CTCTokenizer’s [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常模式下使用时，此方法将所有参数转发到Wav2Vec2FeatureExtractor的[**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)，并返回其输出。如果在上下文中使用`as_target_processor()`，此方法将所有参数转发到Wav2Vec2CTCTokenizer的[**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。请参考上述两种方法的文档字符串以获取更多信息。
- en: '#### `pad`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pad`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L254)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L254)'
- en: '[PRE20]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor’s
    [pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to Wav2Vec2CTCTokenizer’s [pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常模式下使用时，此方法将所有参数转发到Wav2Vec2FeatureExtractor的[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)，并返回其输出。如果在上下文中使用`as_target_processor()`，此方法将所有参数转发到Wav2Vec2CTCTokenizer的[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)。请参考上述两种方法的文档字符串以获取更多信息。
- en: '#### `from_pretrained`'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L113)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L113)'
- en: '[PRE21]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) — This can be either:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) — 可以是：'
- en: a string, the *model id* of a pretrained feature_extractor hosted inside a model
    repo on huggingface.co. Valid model ids can be located at the root-level, like
    `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练特征提取器的*模型ID*的字符串，托管在huggingface.co上的模型存储库中。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。
- en: a path to a *directory* containing a feature extractor file saved using the
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)方法保存的特征提取器文件的*目录*路径，例如`./my_model_directory/`。
- en: a path or url to a saved feature extractor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.
    **kwargs — Additional keyword arguments passed along to both [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    and [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从预训练的特征提取器JSON文件的路径或URL，例如`./my_model_directory/preprocessor_config.json`。**kwargs
    — 传递给[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)和[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的额外关键字参数
- en: Instantiate a [Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)
    from a pretrained Wav2Vec2 processor.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的Wav2Vec2处理器实例化一个[Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)。
- en: This class method is simply calling Wav2Vec2FeatureExtractor’s [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained),
    Wav2Vec2CTCTokenizer’s [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained),
    and `pyctcdecode.BeamSearchDecoderCTC.load_from_hf_hub`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类方法只是调用了Wav2Vec2FeatureExtractor的[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)，Wav2Vec2CTCTokenizer的[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)，以及`pyctcdecode.BeamSearchDecoderCTC.load_from_hf_hub`。
- en: Please refer to the docstrings of the methods above for more information.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考上述方法的文档字符串以获取更多信息。
- en: '#### `save_pretrained`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L109)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L109)'
- en: '[PRE22]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#### `batch_decode`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L285)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L285)'
- en: '[PRE23]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`logits` (`np.ndarray`) — The logits output vector of the model representing
    the log probabilities for each token.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`np.ndarray`) — 模型输出的logits向量，表示每个标记的对数概率。'
- en: '`pool` (`multiprocessing.Pool`, *optional*) — An optional user-managed pool.
    If not set, one will be automatically created and closed. The pool should be instantiated
    *after* `Wav2Vec2ProcessorWithLM`. Otherwise, the LM won’t be available to the
    pool’s sub-processes.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool` (`multiprocessing.Pool`, *optional*) — 可选的用户管理的池。如果未设置，将自动创建并关闭一个池。池应在`Wav2Vec2ProcessorWithLM`之后实例化。否则，LM将不可用于池的子进程。'
- en: Currently, only pools created with a ‘fork’ context can be used. If a ‘spawn’
    pool is passed, it will be ignored and sequential decoding will be used instead.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前，只有使用“fork”上下文创建的池才能使用。如果传递了“spawn”池，它将被忽略，而将使用顺序解码。
- en: '`num_processes` (`int`, *optional*) — If `pool` is not set, number of processes
    on which the function should be parallelized over. Defaults to the number of available
    CPUs.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_processes` (`int`, *optional*) — 如果未设置`pool`，则应该在哪些进程上并行化函数。默认为可用CPU的数量。'
- en: '`beam_width` (`int`, *optional*) — Maximum number of beams at each step in
    decoding. Defaults to pyctcdecode’s DEFAULT_BEAM_WIDTH.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_width` (`int`, *optional*) — 解码过程中每一步的最大beam数。默认为pyctcdecode的DEFAULT_BEAM_WIDTH。'
- en: '`beam_prune_logp` (`int`, *optional*) — Beams that are much worse than best
    beam will be pruned Defaults to pyctcdecode’s DEFAULT_PRUNE_LOGP.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_prune_logp` (`int`, *optional*) — 比最佳beam差很多的beam将被修剪。默认为pyctcdecode的DEFAULT_PRUNE_LOGP。'
- en: '`token_min_logp` (`int`, *optional*) — Tokens below this logp are skipped unless
    they are argmax of frame Defaults to pyctcdecode’s DEFAULT_MIN_TOKEN_LOGP.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_min_logp` (`int`, *optional*) — 低于此logp的标记将被跳过，除非它们是帧的argmax。默认为pyctcdecode的DEFAULT_MIN_TOKEN_LOGP。'
- en: '`hotwords` (`List[str]`, *optional*) — List of words with extra importance,
    can be OOV for LM'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotwords` (`List[str]`, *optional*) — 具有额外重要性的单词列表，可以是LM的OOV'
- en: '`hotword_weight` (`int`, *optional*) — Weight factor for hotword importance
    Defaults to pyctcdecode’s DEFAULT_HOTWORD_WEIGHT.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotword_weight` (`int`, *optional*) — 热词重要性的权重因子，默认为pyctcdecode的DEFAULT_HOTWORD_WEIGHT。'
- en: '`alpha` (`float`, *optional*) — Weight for language model during shallow fusion'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha` (`float`, *optional*) — 浅融合期间语言模型的权重'
- en: '`beta` (`float`, *optional*) — Weight for length score adjustment of during
    scoring'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta` (`float`, *optional*) — 在评分过程中长度得分调整的权重'
- en: '`unk_score_offset` (`float`, *optional*) — Amount of log score offset for unknown
    tokens'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_score_offset` (`float`, *optional*) — 未知标记的对数分数偏移量'
- en: '`lm_score_boundary` (`bool`, *optional*) — Whether to have kenlm respect boundaries
    when scoring'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm_score_boundary` (`bool`, *optional*) — 在评分时是否让kenlm尊重边界'
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`, *optional*, 默认为`False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，以计算转录单词的时间戳。'
- en: '`n_best` (`int`, *optional*, defaults to `1`) — Number of best hypotheses to
    return. If `n_best` is greater than 1, the returned `text` will be a list of lists
    of strings, `logit_score` will be a list of lists of floats, and `lm_score` will
    be a list of lists of floats, where the length of the outer list will correspond
    to the batch size and the length of the inner list will correspond to the number
    of returned hypotheses . The value should be >= 1.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_best` (`int`, *optional*, 默认为`1`) — 要返回的最佳假设数量。如果`n_best`大于1，则返回的`text`将是一个字符串列表的列表，`logit_score`将是一个浮点数列表的列表，`lm_score`将是一个浮点数列表的列表，外部列表的长度将对应批次大小，内部列表的长度将对应返回的假设数量。该值应
    >= 1。'
- en: Please take a look at the Example of [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)
    to better understand how to make use of `output_word_offsets`. [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)
    works the same way with batched output.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)的示例，以更好地理解如何使用`output_word_offsets`。[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)与批量输出的方式相同。
- en: Batch decode output logits to audio transcription with language model support.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 批量解码输出logits以支持语言模型的音频转录。
- en: This function makes use of Python’s multiprocessing. Currently, multiprocessing
    is available only on Unix systems (see this [issue](https://github.com/kensho-technologies/pyctcdecode/issues/65)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数利用了Python的多进程。目前，多进程仅在Unix系统上可用（请参阅此[问题](https://github.com/kensho-technologies/pyctcdecode/issues/65)）。
- en: If you are decoding multiple batches, consider creating a `Pool` and passing
    it to `batch_decode`. Otherwise, `batch_decode` will be very slow since it will
    create a fresh `Pool` for each call. See usage example below.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在解码多个批次，请考虑创建一个`Pool`并将其传递给`batch_decode`。否则，`batch_decode`将非常慢，因为它将为每次调用创建一个新的`Pool`。请参见下面的用法示例。
- en: 'Example: See [Decoding multiple audios](#decoding-multiple-audios).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：请参见[解码多个音频](#decoding-multiple-audios)。
- en: '#### `decode`'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L470)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L470)'
- en: '[PRE24]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`logits` (`np.ndarray`) — The logits output vector of the model representing
    the log probabilities for each token.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`np.ndarray`) — 代表每个标记的对数概率的模型输出向量。'
- en: '`beam_width` (`int`, *optional*) — Maximum number of beams at each step in
    decoding. Defaults to pyctcdecode’s DEFAULT_BEAM_WIDTH.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_width` (`int`, *optional*) — 解码过程中每一步的最大beam数。默认为pyctcdecode的DEFAULT_BEAM_WIDTH。'
- en: '`beam_prune_logp` (`int`, *optional*) — A threshold to prune beams with log-probs
    less than best_beam_logp + beam_prune_logp. The value should be <= 0\. Defaults
    to pyctcdecode’s DEFAULT_PRUNE_LOGP.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_prune_logp` (`int`, *optional*) — 一个用于修剪log-probs小于best_beam_logp + beam_prune_logp的阈值。该值应
    <= 0。默认为pyctcdecode的DEFAULT_PRUNE_LOGP。'
- en: '`token_min_logp` (`int`, *optional*) — Tokens with log-probs below token_min_logp
    are skipped unless they are have the maximum log-prob for an utterance. Defaults
    to pyctcdecode’s DEFAULT_MIN_TOKEN_LOGP.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_min_logp`（`int`，*optional*） — log-probs低于token_min_logp的标记将被跳过，除非它们是话语的最大log-prob。默认为pyctcdecode的DEFAULT_MIN_TOKEN_LOGP。'
- en: '`hotwords` (`List[str]`, *optional*) — List of words with extra importance
    which can be missing from the LM’s vocabulary, e.g. [“huggingface”]'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotwords`（`List[str]`，*optional*） — 具有额外重要性的单词列表，可能不在LM的词汇表中，例如[“huggingface”]'
- en: '`hotword_weight` (`int`, *optional*) — Weight multiplier that boosts hotword
    scores. Defaults to pyctcdecode’s DEFAULT_HOTWORD_WEIGHT.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotword_weight`（`int`，*optional*） — 增强热词分数的权重乘数。默认为pyctcdecode的DEFAULT_HOTWORD_WEIGHT。'
- en: '`alpha` (`float`, *optional*) — Weight for language model during shallow fusion'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`（`float`，*optional*） — 浅融合期间语言模型的权重'
- en: '`beta` (`float`, *optional*) — Weight for length score adjustment of during
    scoring'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta`（`float`，*optional*） — 在评分过程中长度分数调整的权重'
- en: '`unk_score_offset` (`float`, *optional*) — Amount of log score offset for unknown
    tokens'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_score_offset`（`float`，*optional*） — 未知标记的log分数偏移量'
- en: '`lm_score_boundary` (`bool`, *optional*) — Whether to have kenlm respect boundaries
    when scoring'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm_score_boundary` (`bool`, *optional*) — 是否在评分时让kenlm尊重边界'
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) — Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`, *optional*, 默认为 `False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，计算转录单词的时间戳。'
- en: '`n_best` (`int`, *optional*, defaults to `1`) — Number of best hypotheses to
    return. If `n_best` is greater than 1, the returned `text` will be a list of strings,
    `logit_score` will be a list of floats, and `lm_score` will be a list of floats,
    where the length of these lists will correspond to the number of returned hypotheses.
    The value should be >= 1.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_best`（`int`，*optional*，默认为`1`） — 要返回的最佳假设数量。如果`n_best`大于1，则返回的`text`将是一个字符串列表，`logit_score`将是一个浮点数列表，`lm_score`将是一个浮点数列表，这些列表的长度将对应于返回的假设数量。该值应大于等于1。'
- en: Please take a look at the example below to better understand how to make use
    of `output_word_offsets`.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看下面的示例，以更好地理解如何使用`output_word_offsets`。
- en: Decode output logits to audio transcription with language model support.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语言模型支持将输出逻辑解码为音频转录。
- en: 'Example:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE25]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Decoding multiple audios
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码多个音频
- en: 'If you are planning to decode multiple batches of audios, you should consider
    using [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)
    and passing an instantiated `multiprocessing.Pool`. Otherwise, [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)
    performance will be slower than calling [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)
    for each audio individually, as it internally instantiates a new `Pool` for every
    call. See the example below:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划解码多批音频，应考虑使用[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)并传递一个实例化的`multiprocessing.Pool`。否则，[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)的性能将比为每个音频单独调用[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)慢，因为它在每次调用时内部实例化一个新的`Pool`。请参阅下面的示例：
- en: '[PRE26]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Wav2Vec2 specific outputs
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2特定输出
- en: '### `class transformers.models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L44)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L44)'
- en: '[PRE27]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (list of `str` or `str`) — Decoded logits in text from. Usually the
    speech transcription.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`（`str`列表或`str`） — 文本中的解码逻辑。通常是语音转录。'
- en: '`logit_score` (list of `float` or `float`) — Total logit score of the beams
    associated with produced text.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logit_score`（`float`列表或`float`） — 与生成文本相关的beam的总logit分数。'
- en: '`lm_score` (list of `float`) — Fused lm_score of the beams associated with
    produced text.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm_score`（`float`列表） — 与生成文本相关的beam的融合lm_score。'
- en: '`word_offsets` (list of `List[Dict[str, Union[int, str]]]` or `List[Dict[str,
    Union[int, str]]]`) — Offsets of the decoded words. In combination with sampling
    rate and model downsampling rate word offsets can be used to compute time stamps
    for each word.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_offsets`（`List[Dict[str, Union[int, str]]]`或`List[Dict[str, Union[int,
    str]]]`列表 — 解码单词的偏移量。结合采样率和模型下采样率，单词偏移量可用于计算每个单词的时间戳。'
- en: Output type of `Wav2Vec2DecoderWithLM`, with transcription.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`Wav2Vec2DecoderWithLM`的输出类型，带有转录。'
- en: '### `class transformers.modeling_outputs.Wav2Vec2BaseModelOutput`'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_outputs.Wav2Vec2BaseModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_outputs.py#L1376)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_outputs.py#L1376)'
- en: '[PRE28]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）
    — 模型最后一层的隐藏状态序列。'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features`（形状为`(batch_size, sequence_length, conv_dim[-1])`的`torch.FloatTensor`）
    — 模型最后一个卷积层的提取特征向量序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Base class for models that have been trained with the Wav2Vec2 loss objective.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Wav2Vec2损失目标进行训练的模型的基类。
- en: '### `class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L100)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L100)'
- en: '[PRE29]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor`
    of shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*可选*, 当传递`sample_negative_indices`时返回, `torch.FloatTensor` of shape
    `(1,)`) — 总损失，由对比损失（L_m）和多样性损失（L_d）的和组成，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。
    （分类）损失。'
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — 模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码的投影量化状态。'
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — 量化提取的特征向量序列，投影到*config.proj_codevector_dim*，代表对比损失的正目标向量。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`contrastive_loss` (*optional*, returned when `sample_negative_indices` are
    passed, `torch.FloatTensor` of shape `(1,)`) — The contrastive loss (L_m) as stated
    in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_loss` (*可选*, 当传递`sample_negative_indices`时返回, `torch.FloatTensor`
    of shape `(1,)`) — 对比损失（L_m），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。'
- en: '`diversity_loss` (*optional*, returned when `sample_negative_indices` are passed,
    `torch.FloatTensor` of shape `(1,)`) — The diversity loss (L_d) as stated in the
    [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss` (*可选*, 当传递`sample_negative_indices`时返回, `torch.FloatTensor`
    of shape `(1,)`) — 多样性损失（L_d），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。'
- en: Output type of [Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining),
    with potential hidden states and attentions.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)的输出类型，具有潜在的隐藏状态和注意力。'
- en: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L44)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L44)'
- en: '[PRE30]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`)
    — Sequence of extracted feature vectors of the last convolutional layer of the
    model with `last_conv_dim` being the dimension of the last convolutional layer.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`)
    — 模型最后一个卷积层提取的特征向量序列，其中`last_conv_dim`是最后一个卷积层的维度。'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of `FlaxWav2Vec2BaseModelOutput`, with potential hidden states and
    attentions.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2BaseModelOutput`的输出类型，具有潜在的隐藏状态和注意力。'
- en: '#### `replace`'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `replace`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
- en: '[PRE31]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: “Returns a new object replacing the specified fields with new values.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: “返回一个用新值替换指定字段的新对象。
- en: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L74)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L74)'
- en: '[PRE32]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (*optional*, returned when model is in train mode, `jnp.ndarray` of
    shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the diversity
    loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*可选*, 当模型处于训练模式时返回，形状为`(1,)`的`jnp.ndarray`) — 总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。
    (分类) 损失。'
- en: '`projected_states` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`)
    — Hidden-states of the model projected to *config.proj_codevector_dim* that can
    be used to predict the masked projected quantized states.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`）
    — 模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码投影量化状态。'
- en: '`projected_quantized_states` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length,
    config.proj_codevector_dim)`） — 投影到*config.proj_codevector_dim*的量化提取特征向量，表示对比损失的正目标向量。'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Output type of `FlaxWav2Vec2ForPreTrainingOutput`, with potential hidden states
    and attentions.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2ForPreTrainingOutput`的输出类型，具有潜在的隐藏状态和注意力。'
- en: '#### `replace`'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `replace`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
- en: '[PRE33]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: “Returns a new object replacing the specified fields with new values.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: “返回一个用新值替换指定字段的新对象。
- en: PytorchHide Pytorch content
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: Wav2Vec2Model
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2Model
- en: '### `class transformers.Wav2Vec2Model`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1440)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1440)'
- en: '[PRE34]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: 'The bare Wav2Vec2 Model transformer outputting raw hidden-states without any
    specific head on top. Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '裸的 Wav2Vec2 模型变压器输出原始隐藏状态，没有特定的顶部头。Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman
    Mohamed、Michael Auli 在 [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) 中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1530)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1530)'
- en: '[PRE35]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 输入原始语音波形的浮点值。值可以通过将 `.flac` 或 `.wav` 音频文件加载到 `List[float]` 类型的数组或 `numpy.ndarray`
    中获得，例如通过 soundfile 库（`pip install soundfile`）。要将数组准备成 `input_values`，应使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    进行填充和转换为 `torch.FloatTensor` 类型的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在 `[0, 1]` 之间:'
- en: 1 for tokens that are `not masked`,
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于 `未被掩码` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于 `被掩码` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在相应的处理器具有 `config.return_attention_mask == True` 时才应传递 `attention_mask`。对于所有处理器具有
    `config.return_attention_mask == False` 的模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时应
    `不` 传递 `attention_mask` 以避免性能下降。对于这些模型，`input_values` 应简单地用 0 填充并在不传递 `attention_mask`
    的情况下传递。请注意，这些模型根据 `input_values` 是否填充会产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Returns
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含各种元素，具体取决于配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列输出。'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length,
    conv_dim[-1])`) — 模型最后一个卷积层提取的特征向量序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选的*，当传递 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    的 forward 方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE36]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Wav2Vec2ForCTC
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForCTC
- en: '### `class transformers.Wav2Vec2ForCTC`'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1859)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1859)'
- en: '[PRE37]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法来加载模型权重。'
- en: '`target_lang` (`str`, *optional*) — Language id of adapter weights. Adapter
    weights are stored in the format adapter.<lang>.safetensors or adapter.<lang>.bin.
    Only relevant when using an instance of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    with adapters. Uses ‘eng’ by default.</lang></lang>'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_lang` (`str`，*可选的*) — 适配器权重的语言 id。适配器权重存储在格式为 adapter.<lang>.safetensors
    或 adapter.<lang>.bin 的文件中。仅在使用带有适配器的 [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    实例时相关。默认使用 ‘eng’。</lang></lang>'
- en: 'Wav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC). Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '带有顶部 `语言建模` 的 Wav2Vec2 模型，用于 Connectionist Temporal Classification (CTC)。Wav2Vec2
    是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec 2.0:
    A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1941)'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1941)'
- en: '[PRE38]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`) —
    输入原始语音波形的浮点值。值可以通过将 `.flac` 或 `.wav` 音频文件加载到类型为 `List[float]` 或 `numpy.ndarray`
    的数组中获得，例如通过 soundfile 库 (`pip install soundfile`)。要准备好数组为 `input_values`，应使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    进行填充和转换为 `torch.FloatTensor` 类型的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被遮蔽的标记，为1，
- en: 0 for tokens that are `masked`.
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被遮蔽的标记，为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型还会根据`input_values`是否填充而产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    — Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *可选*)
    — 连接主义时间分类的标签。请注意，`target_length`必须小于或等于输出logits的序列长度。索引选择在`[-100, 0, ..., config.vocab_size
    - 1]`。所有设置为`-100`的标签都被忽略（遮蔽），损失仅计算标签在`[0, ..., config.vocab_size - 1]`中的情况。'
- en: Returns
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当提供`labels`时返回) — 语言建模损失（用于下一个标记的预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层的输出，则为嵌入的输出+每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行前后处理步骤，而后者会默默忽略它们。
- en: 'Example:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE39]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#### `load_adapter`'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_adapter`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1191)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1191)'
- en: '[PRE40]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`target_lang` (`str`) — Has to be a language id of an existing adapter weight.
    Adapter weights are stored in the format adapter.<lang>.safetensors or adapter.<lang>.bin</lang></lang>'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_lang` (`str`) — 必须是现有适配器权重的语言 ID。适配器权重存储在格式 adapter.<lang>.safetensors
    或 adapter.<lang>.bin</lang></lang>。'
- en: '`force_load` (`bool`, defaults to `True`) — Whether the weights shall be loaded
    even if `target_lang` matches `self.target_lang`.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_load` (`bool`, 默认为 `True`) — 即使 `target_lang` 与 `self.target_lang` 匹配，也要加载权重。'
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) — Path to a directory in
    which a downloaded pretrained model configuration should be cached if the standard
    cache should not be used.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir` (`Union[str, os.PathLike]`, *可选*) — 下载的预训练模型配置应该缓存在其中的目录路径，如果不使用标准缓存。'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download` (`bool`, *可选*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download` (`bool`, *可选*, 默认为 `False`) — 是否删除接收不完整的文件。如果存在这样的文件，将尝试恢复下载。'
- en: '`proxies` (`Dict[str, str]`, *optional*) — A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies` (`Dict[str, str]`, *可选*) — 一个按协议或端点使用的代理服务器字典，例如，`{''http'': ''foo.bar:3128'',
    ''http://hostname'': ''foo.bar:4012''}`。每个请求都会使用代理。'
- en: '`local_files_only(bool,` *optional*, defaults to `False`) — Whether or not
    to only look at local files (i.e., do not try to download the model).'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only(bool,` *可选*, 默认为 `False`) — 是否仅查看本地文件（即不尝试下载模型）。'
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str` 或 `bool`, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，或未指定，将使用运行
    `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface` 中）。'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交 ID，因为我们在
    huggingface.co 上使用基于 git 的系统存储模型和其他工件，所以 `revision` 可以是 git 允许的任何标识符。'
- en: To test a pull request you made on the Hub, you can pass `revision=“refs/pr/<pr_number>“.</pr_number>
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要测试您在 Hub 上提交的拉取请求，可以传递 `revision=“refs/pr/<pr_number>“。</pr_number>
- en: '`mirror` (`str`, *optional*) — Mirror source to accelerate downloads in China.
    If you are from China and have an accessibility problem, you can set this option
    to resolve it. Note that we do not guarantee the timeliness or safety. Please
    refer to the mirror site for more information.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mirror` (`str`, *可选*) — 将源镜像到中国以加速下载。如果您来自中国并且有访问问题，可以设置此选项以解决问题。请注意，我们不保证及时性或安全性。请参考镜像站点获取更多信息。'
- en: Load a language adapter model from a pre-trained adapter model.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的适配器模型加载语言适配器模型。
- en: Activate the special [“offline-mode”](https://huggingface.co/transformers/installation.html#offline-mode)
    to use this method in a firewalled environment.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 激活特殊的[“离线模式”](https://huggingface.co/transformers/installation.html#offline-mode)以在防火墙环境中使用此方法。
- en: 'Examples:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE41]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Wav2Vec2ForSequenceClassification
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForSequenceClassification
- en: '### `class transformers.Wav2Vec2ForSequenceClassification`'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2021)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2021)'
- en: '[PRE42]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: Wav2Vec2 Model with a sequence classification head on top (a linear layer over
    the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部添加了一个序列分类头的 Wav2Vec2 模型（一个线性层在池化输出上方）用于 SUPERB 关键词识别等任务。
- en: 'Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski,
    Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec
    2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存等）的信息。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2073)'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2073)'
- en: '[PRE43]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Parameters
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过声音文件库（`pip
    install soundfile`）。要将数组准备为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行卷积和注意力的掩码。选择的掩码值在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩码的标记为`1`，
- en: 0 for tokens that are `masked`.
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩码的标记为`0`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型还会根据`input_values`是否填充而产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（或如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）- 分类（如果`config.num_labels==1`则为回归）分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Example:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE44]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Wav2Vec2ForAudioFrameClassification
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForAudioFrameClassification
- en: '### `class transformers.Wav2Vec2ForAudioFrameClassification`'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForAudioFrameClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2144)'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2144)'
- en: '[PRE45]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）-
    具有模型所有参数的模型配置类。 使用配置文件初始化不会加载与模型关联的权重，只加载配置。 请查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Wav2Vec2 Model with a frame classification head on top for tasks like Speaker
    Diarization.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 带有顶部帧分类头的Wav2Vec2模型，用于Speaker Diarization等任务。
- en: 'Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski,
    Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[wav2vec
    2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。
    检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。
    将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2194)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2194)'
- en: '[PRE46]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 输入原始语音波形的浮点值。
    值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如 通过声音文件库（`pip
    install soundfile`）。 要将数组准备成`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充并转换为`torch.FloatTensor`类型的张量。
    有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    用于避免在填充标记索引上执行卷积和注意力的掩码。 选择的掩码值在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-561
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“未屏蔽”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被 `masked` 的标记为 0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[注意力掩码是什么？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在相应的处理器具有 `config.return_attention_mask == True` 时才应传递 `attention_mask`。对于所有处理器具有
    `config.return_attention_mask == False` 的模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应
    `不` 传递 `attention_mask` 以避免性能下降。对于这些模型，`input_values` 应该简单地用 0 填充并在不传递 `attention_mask`
    的情况下传递。请注意，这些模型根据 `input_values` 是否填充会产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*optional*) — 用于计算序列分类/回归损失的标签。索引应在
    `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果
    `config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为 `(1,)`，*optional*，当提供 `labels` 时返回) — 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, config.num_labels)`)
    — 分类分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层的输出，则为嵌入层的输出 + 每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Wav2Vec2ForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE47]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Wav2Vec2ForXVector
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForXVector
- en: '### `class transformers.Wav2Vec2ForXVector`'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForXVector`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2305)'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2305)'
- en: '[PRE48]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Wav2Vec2 Model with an XVector feature extraction head on top for tasks like
    Speaker Verification.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: Wav2Vec2模型在顶部具有XVector特征提取头，用于Speaker Verification等任务。
- en: 'Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski,
    Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli提出的[wav2vec
    2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2373)'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2373)'
- en: '[PRE49]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，*例如*通过soundfile库（`pip
    install soundfile`）。要准备好数组以获得`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    用于避免在填充标记索引上执行卷积和注意力的掩码。选择在`[0, 1]`范围内的掩码值：'
- en: 1 for tokens that are `not masked`,
  id: totrans-598
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，值为1，
- en: 0 for tokens that are `masked`.
  id: totrans-599
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，值为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应简单地填充为0并在不传递`attention_mask`的情况下传递。请注意，这些模型的结果也会因`input_values`是否填充而略有不同。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或当 `config.return_dict=False`
    时）包含各种元素，具体取决于配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    — Classification hidden states before AMSoftmax.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为 `(batch_size, config.xvector_output_dim)`)
    — AMSoftmax 之前的分类隐藏状态。'
- en: '`embeddings` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    — Utterance embeddings used for vector similarity-based retrieval.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embeddings` (`torch.FloatTensor`，形状为 `(batch_size, config.xvector_output_dim)`)
    — 用于基于向量相似性的检索的话语嵌入。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。
- en: The [Wav2Vec2ForXVector](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector)
    forward method, overrides the `__call__` special method.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForXVector](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE50]'
  id: totrans-619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Wav2Vec2ForPreTraining
  id: totrans-620
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForPreTraining
- en: '### `class transformers.Wav2Vec2ForPreTraining`'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.Wav2Vec2ForPreTraining` 类'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1591)'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1591)'
- en: '[PRE51]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Parameters
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: 'Wav2Vec2 Model with a quantizer and `VQ` head on top. Wav2Vec2 was proposed
    in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2 模型带有量化器和顶部的 `VQ` 头。Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman
    Mohamed、Michael Auli 在 [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) 中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1652)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1652)'
- en: '[PRE52]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，*例如*通过soundfile库（`pip
    install soundfile`）。要准备好数组以获得`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-635
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-636
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被屏蔽`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，比如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应`不`传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`mask_time_indices` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices`（形状为`(batch_size, sequence_length)`的`torch.BoolTensor`，*可选*）—
    用于对比损失中掩盖提取特征的索引。在训练模式下，模型学习在*config.proj_codevector_dim*空间中预测被掩盖的提取特征。'
- en: '`sampled_negative_indices` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_negatives)`, *optional*) — Indices indicating which quantized target vectors
    are used as negative sampled vectors in contrastive loss. Required input for pre-training.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampled_negative_indices`（形状为`(batch_size, sequence_length, num_negatives)`的`torch.BoolTensor`，*可选*）—
    指示哪些量化目标向量在对比损失中用作负采样向量的索引。预训练所需的输入。'
- en: Returns
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入而异的各种元素。
- en: '`loss` (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor`
    of shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（*可选*，当传递`sample_negative_indices`时返回，形状为`(1,)`的`torch.FloatTensor`）—
    总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。
    （分类）损失。'
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states`（形状为`(batch_size, sequence_length, config.proj_codevector_dim)`的`torch.FloatTensor`）—
    模型投影到*config.proj_codevector_dim*的隐藏状态，可用于预测被屏蔽的投影量化状态。'
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states`（形状为`(batch_size, sequence_length, config.proj_codevector_dim)`的`torch.FloatTensor`）
    — 量化提取的特征向量投影到*config.proj_codevector_dim*，表示对比损失的正目标向量。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层输出的模型隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`contrastive_loss` (*optional*, returned when `sample_negative_indices` are
    passed, `torch.FloatTensor` of shape `(1,)`) — The contrastive loss (L_m) as stated
    in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_loss`（*可选*，当传递`sample_negative_indices`时返回，形状为`(1,)`的`torch.FloatTensor`）
    — 对比损失（L_m），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。'
- en: '`diversity_loss` (*optional*, returned when `sample_negative_indices` are passed,
    `torch.FloatTensor` of shape `(1,)`) — The diversity loss (L_d) as stated in the
    [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss`（*可选*，当传递`sample_negative_indices`时返回，形状为`(1,)`的`torch.FloatTensor`）
    — 多样性损失（L_d），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。'
- en: The [Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE53]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: TensorFlowHide TensorFlow content
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: TFWav2Vec2Model
  id: totrans-661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWav2Vec2Model
- en: '### `class transformers.TFWav2Vec2Model`'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWav2Vec2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1509)'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1509)'
- en: '[PRE54]'
  id: totrans-664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare TFWav2Vec2 Model transformer outputing raw hidden-states without any
    specific head on top.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 裸TFWav2Vec2模型变压器输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作”
    - 只需传递您的输入和标签以任何`model.fit()`支持的格式！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_values` only and nothing else: `model(input_values)`'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅包含`input_values`的单个张量，没有其他内容：`model(input_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_values, attention_mask])` or `model([input_values,
    attention_mask, token_type_ids])`'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度不同的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([input_values, attention_mask])`或`model([input_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_values": input_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_values": input_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像将输入传递给任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1519)'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1519)'
- en: '[PRE55]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Parameters
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `({0})`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`({0})`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0,
    1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-686
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-687
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被屏蔽`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-688
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    — Segment token indices to indicate first and second portions of the inputs. Indices
    are selected in `[0, 1]`:'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）— 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0,
    1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-690
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*句子A*标记。
- en: 1 corresponds to a *sentence B* token.
  id: totrans-691
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-692
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*) —
    Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0,
    config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-694
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`np.ndarray`或`tf.Tensor`，*可选*）—
    用于使自注意力模块的选定头部无效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-696
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未`被屏蔽`，
- en: 0 indicates the head is `masked`.
  id: totrans-697
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是`屏蔽`的。
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `({0}, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_values` you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `input_values` indices into associated vectors than the model’s
    internal embedding lookup matrix.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`({0}, hidden_size)`的`np.ndarray`或`tf.Tensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_values`。如果您想要更多控制权来将`input_values`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *可选*，默认为`False“) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))和输入的各种元素。
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`形状为(batch_size, sequence_length, hidden_size)的tf.Tensor`)
    — 模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.FloatTensor)`, *可选*，在传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *可选*，在传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE56]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: TFWav2Vec2ForSequenceClassification
  id: totrans-715
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWav2Vec2ForSequenceClassification
- en: '### `class transformers.TFWav2Vec2ForSequenceClassification`'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWav2Vec2ForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1758)'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1758)'
- en: '[PRE57]'
  id: totrans-718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `call`'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1799)'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1799)'
- en: '[PRE58]'
  id: totrans-721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: TFWav2Vec2ForCTC
  id: totrans-722
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWav2Vec2ForCTC
- en: '### `class transformers.TFWav2Vec2ForCTC`'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWav2Vec2ForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1591)'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1591)'
- en: '[PRE59]'
  id: totrans-725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parameters
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: TFWav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC).
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: TFWav2Vec2模型，在Connectionist Temporal Classification (CTC)顶部具有`语言建模`头。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作”
    - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外（如`fit()`和`predict()`）使用第二种格式，比如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_values` only and nothing else: `model(input_values)`'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅包含`input_values`的单个张量，没有其他内容：`model(input_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_values, attention_mask])` or `model([input_values,
    attention_mask, token_type_ids])`'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个与文档字符串中给定的顺序相对应的输入张量：`model([input_values, attention_mask])`或`model([input_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_values": input_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_values": input_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像将输入传递给任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1625)'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1625)'
- en: '[PRE60]'
  id: totrans-741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `({0})`) — Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`({0})`）—词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-744
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-745
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）—用于避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0,
    1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-747
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“未被掩码”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-748
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“被掩码”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-749
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    — Segment token indices to indicate first and second portions of the inputs. Indices
    are selected in `[0, 1]`:'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）—段标记索引，用于指示输入的第一部分和第二部分。索引选择在`[0,
    1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-751
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-752
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-753
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*) —
    Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）—每个输入序列标记在位置嵌入中的位置索引。在范围`[0,
    config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-755
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`np.ndarray`或`tf.Tensor`，*可选*）—用于使自注意力模块中选择的头部失效的掩码。选择的掩码值为`[0,
    1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-757
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`not masked`。
- en: 0 indicates the head is `masked`.
  id: totrans-758
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `({0}, hidden_size)`,
    *optional*) — Optionally, instead of passing `input_values` you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `input_values` indices into associated vectors than the model’s
    internal embedding lookup matrix.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`({0}, hidden_size)`的`np.ndarray`或`tf.Tensor`，*可选*） — 可选地，可以直接传递嵌入表示，而不是传递`input_values`。如果您想要更多控制如何将`input_values`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*） — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`（`bool`，*可选*，默认为`False`） — 是否在训练模式下使用模型（某些模块，如dropout模块，在训练和评估之间具有不同的行为）。'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the masked language modeling loss. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_values` docstring)
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`np.ndarray`，*可选*）
    — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（参见`input_values`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`范围内的标记。'
- en: Returns
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    或 `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入。
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) — Language modeling loss (for next-token
    prediction).'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(n,)`的`tf.Tensor`，*可选*，其中n是非掩码标签的数量，在提供`labels`时返回） — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-771
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-773
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [TFWav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFWav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForCTC)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE61]'
  id: totrans-777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: JAXHide JAX content
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: JAXHide JAX内容
- en: FlaxWav2Vec2Model
  id: totrans-779
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWav2Vec2Model
- en: '### `class transformers.FlaxWav2Vec2Model`'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWav2Vec2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1051)'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1051)'
- en: '[PRE62]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`）- 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-786
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的`dtype`执行。
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-787
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`请注意，这仅指定计算的dtype，不影响模型参数的dtype。`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-788
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改模型参数的dtype，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。
- en: 'The bare Wav2Vec2 Model transformer outputting raw hidden-states without any
    specific head on top. Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Wav2Vec2模型变压器输出原始隐藏状态，没有特定的头部。Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman
    Mohamed、Michael Auli在[wav2vec 2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)中提出的。
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规Flax模块，并参考Flax文档以获取有关一般用法和行为的所有相关信息。
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此模型支持JAX的固有功能，例如：
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
- en: '[PRE63]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Parameters
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Float
    values of input raw speech waveform. Values can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `jnp.ndarray`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`）- 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库（`pip
    install soundfile`）。要将数组准备成`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`jnp.ndarray`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing convolution and attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）- 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-803
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，值为1，
- en: 0 for tokens that are `masked`.
  id: totrans-804
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，值为0。
- en: '[What are attention masks?](../glossary#attention-mask) .. warning:: `attention_mask`
    should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-805
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask) .. 警告:: 只有当相应的处理器具有`config.return_attention_mask
    == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应`不`传递`attention_mask`以避免性能下降。对于这样的模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。'
- en: '`mask_time_indices` (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）-
    用于对比损失中掩码提取特征的索引。在训练模式下，模型学习在*config.proj_codevector_dim*空间中预测掩码提取特征。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多细节，请查看返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多细节，请查看返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)或者`tuple(torch.FloatTensor)`'
- en: A [transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`)
    and inputs.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者`config.return_dict=False`）包含不同的元素，取决于配置（`<class
    'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`）和输入。
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`）-
    模型最后一层的隐藏状态序列。'
- en: '`extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`)
    — Sequence of extracted feature vectors of the last convolutional layer of the
    model with `last_conv_dim` being the dimension of the last convolutional layer.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features`（形状为`(batch_size, sequence_length, last_conv_dim)`的`jnp.ndarray`）-
    模型最后一个卷积层的提取特征向量序列，其中`last_conv_dim`是最后一个卷积层的维度。'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-818
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The `FlaxWav2Vec2PreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2PreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE64]'
  id: totrans-822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: FlaxWav2Vec2ForCTC
  id: totrans-823
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWav2Vec2ForCTC
- en: '### `class transformers.FlaxWav2Vec2ForCTC`'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWav2Vec2ForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1169)'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1169)'
- en: '[PRE65]'
  id: totrans-826
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`（`jax.numpy.dtype`，*可选*，默认为 `jax.numpy.float32`） — 计算的数据类型。可以是 `jax.numpy.float32`、`jax.numpy.float16`（在
    GPU 上）和 `jax.numpy.bfloat16`（在 TPU 上）之一。'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的 `dtype` 执行。
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`请注意，这仅指定计算的数据类型，不会影响模型参数的数据类型。`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-832
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改模型参数的数据类型，请参阅 [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    和 [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。
- en: 'Wav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC). Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2 模型在顶部带有“语言建模”头部，用于 Connectionist Temporal Classification (CTC)。Wav2Vec2
    是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec 2.0:
    A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    中提出的。'
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是 Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    子类。将其用作常规 Flax 模块，并参考 Flax 文档以了解所有与一般用法和行为相关的事项。
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此模型支持 JAX 的内在特性，例如：
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[即时 (JIT) 编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
- en: '[PRE66]'
  id: totrans-843
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Parameters
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Float
    values of input raw speech waveform. Values can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `jnp.ndarray`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为 `(batch_size, sequence_length)` 的 `jnp.ndarray`） — 输入原始语音波形的浮点值。可以通过将
    `.flac` 或 `.wav` 音频文件加载到类型为 `List[float]` 或 `numpy.ndarray` 的数组中获得。要将数组准备成 `input_values`，应使用
    [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    进行填充和转换为类型为 `jnp.ndarray` 的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing convolution and attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为 `(batch_size, sequence_length)` 的 `jnp.ndarray`，*可选*）
    — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-847
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于“未掩码”标记，
- en: 0 for tokens that are `masked`.
  id: totrans-848
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于“掩码”标记。
- en: '[What are attention masks?](../glossary#attention-mask) .. warning:: `attention_mask`
    should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-849
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask) .. 警告:: 只有当相应的处理器具有 `config.return_attention_mask
    == True` 时才应传递 `attention_mask`。对于所有处理器具有 `config.return_attention_mask == False`
    的模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应该
    `不` 传递 `attention_mask` 以避免性能下降。对于这样的模型，`input_values` 应该简单地用 0 填充并在不传递 `attention_mask`
    的情况下传递。请注意，这些模型根据 `input_values` 是否填充会产生略有不同的结果。'
- en: '`mask_time_indices` (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices` (`jnp.ndarray`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于对比损失掩盖提取特征的索引。在训练模式下，模型学习在 *config.proj_codevector_dim* 空间中预测掩盖的提取特征。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`)
    and inputs.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)或一个
    `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含不同元素，取决于配置（`<class
    'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`）和输入。
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`jnp.ndarray`，形状为 `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax 前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `jnp.ndarray` 元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递 `output_attentions=True` 或 `config.output_attentions=True`
    时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray`
    元组（每一层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The `FlaxWav2Vec2PreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2PreTrainedModel` 的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用 `Module` 实例而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE67]'
  id: totrans-865
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: FlaxWav2Vec2ForPreTraining
  id: totrans-866
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWav2Vec2ForPreTraining
- en: '### `class transformers.FlaxWav2Vec2ForPreTraining`'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWav2Vec2ForPreTraining`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1318)'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1318)'
- en: '[PRE68]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Parameters
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`, *可选*, 默认为`jax.numpy.float32`) — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '这可用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的`dtype`进行。 '
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-874
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`请注意，这仅指定计算的数据类型，不会影响模型参数的数据类型。`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果要更改模型参数的数据类型，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。
- en: 'Wav2Vec2 Model with a quantizer and `VQ` head on top. Wav2Vec2 was proposed
    in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 带有量化器和顶部`VQ`头的Wav2Vec2模型。Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael
    Auli提出的[wav2vec 2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)。
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规Flax模块，并参考Flax文档以获取有关一般用法和行为的所有相关信息。
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此模型支持JAX的固有特性，例如：
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1322)'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1322)'
- en: '[PRE69]'
  id: totrans-886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Parameters
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Float
    values of input raw speech waveform. Values can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `jnp.ndarray`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库（`pip
    install soundfile`）。要将数组准备为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`jnp.ndarray`类型的张量。详细信息请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing convolution and attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *可选*)
    — 避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-890
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1。
- en: 0 for tokens that are `masked`.
  id: totrans-891
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask) .. warning:: `attention_mask`
    should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-892
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask) .. 警告:: 只有当相应的处理器具有`config.return_attention_mask
    == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。'
- en: '`mask_time_indices` (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`，*optional*）
    — 用于对比损失中掩码提取特征的索引。在训练模式下，模型学习在*config.proj_codevector_dim*空间中预测掩码提取特征。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`)
    and inputs.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class
    ''transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config''>`）和输入的不同元素。'
- en: '`loss` (*optional*, returned when model is in train mode, `jnp.ndarray` of
    shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the diversity
    loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*optional*，在训练模式下返回，形状为`(1,)`的`jnp.ndarray`) — 总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。
    （分类）损失。'
- en: '`projected_states` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`)
    — Hidden-states of the model projected to *config.proj_codevector_dim* that can
    be used to predict the masked projected quantized states.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`）
    — 模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码的投影量化状态。'
- en: '`projected_quantized_states` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length,
    config.proj_codevector_dim)`） — 量化提取的特征向量投影到*config.proj_codevector_dim*，表示对比损失的正目标向量。'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-904
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-906
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [FlaxWav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxWav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传播的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Example:'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE70]'
  id: totrans-910
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
