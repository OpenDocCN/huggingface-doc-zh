- en: Wav2Vec2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wav2Vec2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Wav2Vec2æ¨¡å‹æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliåœ¨[wav2vec
    2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ä¸­æå‡ºçš„ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*We show for the first time that learning powerful representations from speech
    audio alone followed by fine-tuning on transcribed speech can outperform the best
    semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the
    speech input in the latent space and solves a contrastive task defined over a
    quantization of the latent representations which are jointly learned. Experiments
    using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test
    sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms
    the previous state of the art on the 100 hour subset while using 100 times less
    labeled data. Using just ten minutes of labeled data and pre-training on 53k hours
    of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility
    of speech recognition with limited amounts of labeled data.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬é¦–æ¬¡å±•ç¤ºï¼Œä»…é€šè¿‡ä»è¯­éŸ³éŸ³é¢‘ä¸­å­¦ä¹ å¼ºå¤§çš„è¡¨ç¤ºï¼Œç„¶ååœ¨è½¬å½•çš„è¯­éŸ³ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥èƒœè¿‡æœ€ä½³çš„åŠç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ¦‚å¿µä¸Šæ›´ç®€å•ã€‚wav2vec 2.0åœ¨æ½œåœ¨ç©ºé—´ä¸­å±è”½è¯­éŸ³è¾“å…¥ï¼Œå¹¶è§£å†³äº†ä¸€ä¸ªåœ¨è”åˆå­¦ä¹ çš„æ½œåœ¨è¡¨ç¤ºçš„é‡åŒ–ä¸Šå®šä¹‰çš„å¯¹æ¯”ä»»åŠ¡ã€‚ä½¿ç”¨Librispeechçš„æ‰€æœ‰æ ‡è®°æ•°æ®è¿›è¡Œçš„å®éªŒåœ¨å¹²å‡€/å…¶ä»–æµ‹è¯•é›†ä¸Šå®ç°äº†1.8/3.3çš„WERã€‚å½“å°†æ ‡è®°æ•°æ®é‡é™ä½åˆ°ä¸€å°æ—¶æ—¶ï¼Œwav2vec
    2.0åœ¨100å°æ—¶å­é›†ä¸Šèƒœè¿‡äº†å…ˆå‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ‡è®°æ•°æ®é‡å‡å°‘äº†100å€ã€‚ä»…ä½¿ç”¨ååˆ†é’Ÿçš„æ ‡è®°æ•°æ®å¹¶åœ¨53kå°æ—¶çš„æœªæ ‡è®°æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»ç„¶å®ç°äº†4.8/8.2çš„WERã€‚è¿™è¯æ˜äº†åœ¨æœ‰é™çš„æ ‡è®°æ•°æ®é‡ä¸‹è¿›è¡Œè¯­éŸ³è¯†åˆ«çš„å¯è¡Œæ€§ã€‚*'
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[patrickvonplaten](https://huggingface.co/patrickvonplaten)è´¡çŒ®ã€‚
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: Wav2Vec2 is a speech model that accepts a float array corresponding to the raw
    waveform of the speech signal.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wav2Vec2æ˜¯ä¸€ä¸ªæ¥å—ä¸è¯­éŸ³ä¿¡å·çš„åŸå§‹æ³¢å½¢å¯¹åº”çš„æµ®ç‚¹æ•°ç»„çš„è¯­éŸ³æ¨¡å‹ã€‚
- en: Wav2Vec2 model was trained using connectionist temporal classification (CTC)
    so the model output has to be decoded using [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wav2Vec2æ¨¡å‹æ˜¯ä½¿ç”¨è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»ï¼ˆCTCï¼‰è¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤æ¨¡å‹è¾“å‡ºå¿…é¡»ä½¿ç”¨[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)è¿›è¡Œè§£ç ã€‚
- en: Resources
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with Wav2Vec2\. If youâ€™re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and weâ€™ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨Wav2Vec2ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: Audio Classification
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘åˆ†ç±»
- en: A notebook on how to [leverage a pretrained Wav2Vec2 model for emotion classification](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb).
    ğŸŒ
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºå¦‚ä½•[åˆ©ç”¨é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- en: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)å—åˆ°è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)çš„æ”¯æŒã€‚'
- en: '[Audio classification task guide](../tasks/audio_classification)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[éŸ³é¢‘åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/audio_classification)'
- en: Automatic Speech Recognition
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨è¯­éŸ³è¯†åˆ«
- en: A blog post on [boosting Wav2Vec2 with n-grams in ğŸ¤— Transformers](https://huggingface.co/blog/wav2vec2-with-ngram).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äº[åœ¨ğŸ¤— Transformersä¸­ä½¿ç”¨n-gramså¢å¼ºWav2Vec2çš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/wav2vec2-with-ngram)ã€‚
- en: A blog post on how to [finetune Wav2Vec2 for English ASR with ğŸ¤— Transformers](https://huggingface.co/blog/fine-tune-wav2vec2-english).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äºå¦‚ä½•[ä½¿ç”¨ğŸ¤— Transformerså¯¹è‹±è¯­ASRè¿›è¡Œå¾®è°ƒçš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/fine-tune-wav2vec2-english)ã€‚
- en: A blog post on [finetuning XLS-R for Multi-Lingual ASR with ğŸ¤— Transformers](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äº[ä½¿ç”¨ğŸ¤— Transformerså¯¹å¤šè¯­è¨€ASRè¿›è¡Œå¾®è°ƒçš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)ã€‚
- en: A notebook on how to [create YouTube captions from any video by transcribing
    audio with Wav2Vec2](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb).
    ğŸŒ
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºå¦‚ä½•[é€šè¿‡ä½¿ç”¨Wav2Vec2è½¬å½•éŸ³é¢‘ä»ä»»ä½•è§†é¢‘åˆ›å»ºYouTubeå­—å¹•](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- en: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    is supported by a notebook on [how to finetune a speech recognition model in English](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb),
    and [how to finetune a speech recognition model in any language](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)å—åˆ°ä¸€ç¯‡å…³äº[å¦‚ä½•åœ¨è‹±è¯­ä¸­å¾®è°ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)å’Œ[å¦‚ä½•åœ¨ä»»ä½•è¯­è¨€ä¸­å¾®è°ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)çš„æ”¯æŒã€‚'
- en: '[Automatic speech recognition task guide](../tasks/asr)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡æŒ‡å—](../tasks/asr)'
- en: ğŸš€ Deploy
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ éƒ¨ç½²
- en: A blog post on how to deploy Wav2Vec2 for [Automatic Speech Recogntion with
    Hugging Faceâ€™s Transformers & Amazon SageMaker](https://www.philschmid.de/automatic-speech-recognition-sagemaker).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•åœ¨[Hugging Faceçš„Transformerså’ŒAmazon SageMakerä¸­éƒ¨ç½²Wav2Vec2è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„åšæ–‡](https://www.philschmid.de/automatic-speech-recognition-sagemaker)ã€‚
- en: Wav2Vec2Config
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2Config
- en: '### `class transformers.Wav2Vec2Config`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/configuration_wav2vec2.py#L32)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/configuration_wav2vec2.py#L32)'
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 32) â€” Vocabulary size of the Wav2Vec2
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    or [TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model).
    Vocabulary size of the model. Defines the different tokens that can be represented
    by the *inputs_ids* passed to the forward method of [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 32) â€” Wav2Vec2æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)æˆ–[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)çš„forwardæ–¹æ³•æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ã€‚'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å®Œå…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) â€” å®Œå…¨è¿æ¥å±‚å†…æ¿€æ´»çš„dropoutæ¯”ç‡ã€‚'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout ratio
    for the attention probabilities.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for the final projection layer of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_dropout` (`float`, *optional*, defaults to 0.1) â€” [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)æœ€ç»ˆæŠ•å½±å±‚çš„dropoutæ¦‚ç‡ã€‚'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) â€” The LayerDrop probability.
    See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.1) â€” LayerDropæ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LayerDropè®ºæ–‡](see
    [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) â€” The norm to
    be applied to 1D convolutional layers in feature encoder. One of `"group"` for
    group normalization of only the first 1D convolutional layer or `"layer"` for
    layer normalization of all 1D convolutional layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) â€” åº”ç”¨äºç‰¹å¾ç¼–ç å™¨ä¸­1Då·ç§¯å±‚çš„è§„èŒƒåŒ–ã€‚`"group"`è¡¨ç¤ºä»…å¯¹ç¬¬ä¸€ä¸ª1Då·ç§¯å±‚è¿›è¡Œç»„å½’ä¸€åŒ–ï¼Œ`"layer"`è¡¨ç¤ºå¯¹æ‰€æœ‰1Då·ç§¯å±‚è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    for output of the feature encoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) â€” ç‰¹å¾ç¼–ç å™¨è¾“å‡ºçš„dropoutæ¦‚ç‡ã€‚'
- en: '`feat_extract_activation` (`str,` optional`, defaults to` â€œgeluâ€`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` â€œgeluâ€`,` â€œreluâ€`,` â€œseluâ€`and`â€œgelu_newâ€` are
    supported.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_activation` (`str,` optional`, defaults to` â€œgeluâ€`) -- ç‰¹å¾æå–å™¨ä¸­1Då·ç§¯å±‚çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`â€œgeluâ€`ã€`â€œreluâ€`ã€`â€œseluâ€`å’Œ`â€œgelu_newâ€`ã€‚'
- en: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probabilitiy for quantized feature encoder states.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) â€” é‡åŒ–ç‰¹å¾ç¼–ç å™¨çŠ¶æ€çš„dropoutæ¦‚ç‡ã€‚'
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) â€” A tuple of integers defining the number of input
    and output channels of each 1D convolutional layer in the feature encoder. The
    length of *conv_dim* defines the number of 1D convolutional layers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°ã€‚*conv_dim*çš„é•¿åº¦å®šä¹‰äº†1Då·ç§¯å±‚çš„æ•°é‡ã€‚'
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) â€” A tuple of integers defining the stride of each 1D convolutional
    layer in the feature encoder. The length of *conv_stride* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) â€” åœ¨ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ­¥å¹…çš„æ•´æ•°å…ƒç»„ã€‚*conv_stride*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚'
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) â€” A tuple of integers defining the kernel size of each 1D convolutional
    layer in the feature encoder. The length of *conv_kernel* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) â€” åœ¨ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„å·ç§¯æ ¸å¤§å°çš„æ•´æ•°å…ƒç»„ã€‚*conv_kernel*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚'
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) â€” Whether the 1D convolutional
    layers have a bias.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_bias` (`bool`, *optional*, defaults to `False`) â€” 1Då·ç§¯å±‚æ˜¯å¦å…·æœ‰åç½®ã€‚'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) â€” Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) â€” å·ç§¯ä½ç½®åµŒå…¥çš„æ•°é‡ã€‚å®šä¹‰äº†1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„å·ç§¯æ ¸å¤§å°ã€‚'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) â€” Number
    of groups of 1D convolutional positional embeddings layer.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) â€” 1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„ç»„æ•°ã€‚'
- en: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) â€” Whether
    to apply *stable* layer norm architecture of the Transformer encoder. `do_stable_layer_norm
    is True` corresponds to applying layer norm before the attention layer, whereas
    `do_stable_layer_norm is False` corresponds to applying layer norm after the attention
    layer.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åº”ç”¨Transformerç¼–ç å™¨çš„*stable*å±‚å½’ä¸€åŒ–æ¶æ„ã€‚`do_stable_layer_normä¸ºTrue`è¡¨ç¤ºåœ¨æ³¨æ„åŠ›å±‚ä¹‹å‰åº”ç”¨å±‚å½’ä¸€åŒ–ï¼Œè€Œ`do_stable_layer_normä¸ºFalse`è¡¨ç¤ºåœ¨æ³¨æ„åŠ›å±‚ä¹‹ååº”ç”¨å±‚å½’ä¸€åŒ–ã€‚'
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) â€” Whether to
    apply *SpecAugment* data augmentation to the outputs of the feature encoder. For
    reference see [SpecAugment: A Simple Data Augmentation Method for Automatic Speech
    Recognition](https://arxiv.org/abs/1904.08779).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦å°†*SpecAugment*æ•°æ®å¢å¼ºåº”ç”¨äºç‰¹å¾ç¼–ç å™¨çš„è¾“å‡ºã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[SpecAugment:
    A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)ã€‚'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates â€mask_time_prob*len(time_axis)/mask_time_lengthâ€ independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if` apply_spec_augment is
    True`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” æ²¿æ—¶é—´è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_time_prob*len(time_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºæ©ç›–çš„å‘é‡è·¨åº¦èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*
    mask_time_prob *åº”ä¸º`prob_vector_start*mask_time_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½å®é™…æ©ç›–å‘é‡çš„ç™¾åˆ†æ¯”ã€‚ä»…åœ¨`apply_spec_augmentä¸ºTrue`æ—¶ç›¸å…³ã€‚'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) â€” Length of vector span
    along the time axis.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *optional*, defaults to 10) â€” æ²¿æ—¶é—´è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if â€mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masksâ€'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” æ²¿æ—¶é—´è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æœ€å°æ©ç æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masksâ€æ—¶ç›¸å…³'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates â€mask_feature_prob*len(feature_axis)/mask_time_lengthâ€
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” æ²¿ç‰¹å¾è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_feature_prob*len(feature_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºæ©ç›–çš„å‘é‡è·¨åº¦èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*
    mask_feature_prob *åº”ä¸º`prob_vector_start*mask_feature_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½å®é™…æ©ç›–å‘é‡çš„ç™¾åˆ†æ¯”ã€‚ä»…åœ¨`apply_spec_augmentä¸ºTrue`æ—¶ç›¸å…³ã€‚'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) â€” Length of vector
    span along the feature axis.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *optional*, defaults to 10) â€” æ²¿ç‰¹å¾è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” The minimum
    number of masks of length `mask_feature_length` generated along the feature axis,
    each time step, irrespectively of `mask_feature_prob`. Only relevant if â€mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masksâ€'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” æ²¿ç‰¹å¾è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æœ€å°æ©ç æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masksâ€æ—¶ç›¸å…³'
- en: '`num_codevectors_per_group` (`int`, *optional*, defaults to 320) â€” Number of
    entries in each quantization codebook (group).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevectors_per_group` (`int`, *optional*, defaults to 320) â€” æ¯ä¸ªé‡åŒ–ç ä¹¦ï¼ˆç»„ï¼‰ä¸­çš„æ¡ç›®æ•°ã€‚'
- en: '`num_codevector_groups` (`int`, *optional*, defaults to 2) â€” Number of codevector
    groups for product codevector quantization.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevector_groups` (`int`, *optional*, defaults to 2) â€” äº§å“ç çŸ¢é‡é‡åŒ–çš„ç çŸ¢é‡ç»„æ•°ã€‚'
- en: '`contrastive_logits_temperature` (`float`, *optional*, defaults to 0.1) â€” The
    temperature *kappa* in the contrastive loss.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_logits_temperature` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” å¯¹æ¯”æŸå¤±ä¸­çš„æ¸©åº¦ *kappa*ã€‚'
- en: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probabilitiy for the output of the feature encoder thatâ€™s used by the quantizer.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_quantizer_dropout` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” ç”¨äºé‡åŒ–å™¨ä½¿ç”¨çš„ç‰¹å¾ç¼–ç å™¨è¾“å‡ºçš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`num_negatives` (`int`, *optional*, defaults to 100) â€” Number of negative samples
    for the contrastive loss.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_negatives` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 100) â€” å¯¹æ¯”æŸå¤±çš„è´Ÿæ ·æœ¬æ•°é‡ã€‚'
- en: '`codevector_dim` (`int`, *optional*, defaults to 256) â€” Dimensionality of the
    quantized feature vectors.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codevector_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” é‡åŒ–ç‰¹å¾å‘é‡çš„ç»´åº¦ã€‚'
- en: '`proj_codevector_dim` (`int`, *optional*, defaults to 256) â€” Dimensionality
    of the final projection of both the quantized and the transformer features.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proj_codevector_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” æœ€ç»ˆæŠ•å½±çš„ç»´åº¦ï¼ŒåŒ…æ‹¬é‡åŒ–ç‰¹å¾å’Œå˜æ¢å™¨ç‰¹å¾ã€‚'
- en: '`diversity_loss_weight` (`int`, *optional*, defaults to 0.1) â€” The weight of
    the codebook diversity loss component.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss_weight` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” ä»£ç æœ¬å¤šæ ·æ€§æŸå¤±ç»„ä»¶çš„æƒé‡ã€‚'
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) â€” Specifies the
    reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training
    an instance of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_loss_reduction` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"sum"`) â€” æŒ‡å®šåº”ç”¨äº `torch.nn.CTCLoss`
    è¾“å‡ºçš„å‡å°‘æ–¹å¼ã€‚ä»…åœ¨è®­ç»ƒ [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) â€” Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_zero_infinity` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å°† `torch.nn.CTCLoss` çš„æ— é™æŸå¤±å’Œç›¸å…³æ¢¯åº¦ç½®é›¶ã€‚å½“è¾“å…¥å¤ªçŸ­æ— æ³•ä¸ç›®æ ‡å¯¹é½æ—¶ï¼Œä¸»è¦ä¼šå‡ºç°æ— é™æŸå¤±ã€‚ä»…åœ¨è®­ç»ƒ
    [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) â€” Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨å…·æœ‰å­¦ä¹ æƒé‡çš„å±‚è¾“å‡ºçš„åŠ æƒå¹³å‡ã€‚ä»…åœ¨ä½¿ç”¨
    [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)
    å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) â€” Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” ç”¨äºåˆ†ç±»çš„ä»¤ç‰Œå‡å€¼æ± åŒ–ä¹‹å‰çš„æŠ•å½±ç»´åº¦ã€‚'
- en: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) â€” A tuple of integers defining the number of output channels
    of each 1D convolutional layer in the *TDNN* module of the *XVector* model. The
    length of *tdnn_dim* defines the number of *TDNN* layers.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dim` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `(512, 512, 512, 512, 1500)`)
    â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº† *XVector* æ¨¡å‹ä¸­ *TDNN* æ¨¡å—ä¸­æ¯ä¸ªä¸€ç»´å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚*tdnn_dim* çš„é•¿åº¦å®šä¹‰äº† *TDNN* å±‚çš„æ•°é‡ã€‚'
- en: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) â€” A tuple of integers defining the kernel size of each 1D convolutional
    layer in the *TDNN* module of the *XVector* model. The length of *tdnn_kernel*
    has to match the length of *tdnn_dim*.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_kernel` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `(5, 3, 3, 1, 1)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº†
    *XVector* æ¨¡å‹ä¸­ *TDNN* æ¨¡å—ä¸­æ¯ä¸ªä¸€ç»´å·ç§¯å±‚çš„å†…æ ¸å¤§å°ã€‚*tdnn_kernel* çš„é•¿åº¦å¿…é¡»ä¸ *tdnn_dim* çš„é•¿åº¦ç›¸åŒ¹é…ã€‚'
- en: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) â€” A tuple of integers defining the dilation factor of each 1D convolutional
    layer in *TDNN* module of the *XVector* model. The length of *tdnn_dilation* has
    to match the length of *tdnn_dim*.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dilation` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `(1, 2, 3, 1, 1)`) â€”
    ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº† *XVector* æ¨¡å‹ä¸­ *TDNN* æ¨¡å—ä¸­æ¯ä¸ªä¸€ç»´å·ç§¯å±‚çš„è†¨èƒ€å› å­ã€‚*tdnn_dilation* çš„é•¿åº¦å¿…é¡»ä¸ *tdnn_dim*
    çš„é•¿åº¦ç›¸åŒ¹é…ã€‚'
- en: '`xvector_output_dim` (`int`, *optional*, defaults to 512) â€” Dimensionality
    of the *XVector* embedding vectors.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xvector_output_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” *XVector* åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚'
- en: '`add_adapter` (`bool`, *optional*, defaults to `False`) â€” Whether a convolutional
    network should be stacked on top of the Wav2Vec2 Encoder. Can be very useful for
    warm-starting Wav2Vec2 for SpeechEncoderDecoder models.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_adapter` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨ Wav2Vec2 ç¼–ç å™¨é¡¶éƒ¨å †å å·ç§¯ç½‘ç»œã€‚å¯¹äº Warm-starting
    Wav2Vec2 for SpeechEncoderDecoder æ¨¡å‹éå¸¸æœ‰ç”¨ã€‚'
- en: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) â€” Kernel size of the
    convolutional layers in the adapter network. Only relevant if `add_adapter is
    True`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_kernel_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„å†…æ ¸å¤§å°ã€‚ä»…åœ¨ `add_adapter`
    ä¸º True æ—¶ç›¸å…³ã€‚'
- en: '`adapter_stride` (`int`, *optional*, defaults to 2) â€” Stride of the convolutional
    layers in the adapter network. Only relevant if `add_adapter is True`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„æ­¥å¹…ã€‚ä»…åœ¨ `add_adapter` ä¸º True
    æ—¶ç›¸å…³ã€‚'
- en: '`num_adapter_layers` (`int`, *optional*, defaults to 3) â€” Number of convolutional
    layers that should be used in the adapter network. Only relevant if `add_adapter
    is True`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_adapter_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3) â€” é€‚é…å™¨ç½‘ç»œä¸­åº”ä½¿ç”¨çš„å·ç§¯å±‚æ•°é‡ã€‚ä»…åœ¨ `add_adapter`
    ä¸º True æ—¶ç›¸å…³ã€‚'
- en: '`adapter_attn_dim` (`int`, *optional*) â€” Dimension of the attention adapter
    weights to be used in each attention block. An example of a model using attention
    adapters is [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_attn_dim` (`int`, *å¯é€‰*) â€” æ¯ä¸ªæ³¨æ„åŠ›å—ä¸­è¦ä½¿ç”¨çš„æ³¨æ„åŠ›é€‚é…å™¨æƒé‡çš„ç»´åº¦ã€‚ä½¿ç”¨æ³¨æ„åŠ›é€‚é…å™¨çš„æ¨¡å‹ç¤ºä¾‹æ˜¯ [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)ã€‚'
- en: '`output_hidden_size` (`int`, *optional*) â€” Dimensionality of the encoder output
    layer. If not defined, this defaults to *hidden-size*. Only relevant if `add_adapter
    is True`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_size` (`int`, *å¯é€‰*) â€” ç¼–ç å™¨è¾“å‡ºå±‚çš„ç»´åº¦ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™é»˜è®¤ä¸º *hidden-size*ã€‚ä»…åœ¨
    `add_adapter` ä¸º True æ—¶ç›¸å…³ã€‚'
- en: This is the configuration class to store the configuration of a [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model).
    It is used to instantiate an Wav2Vec2 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Wav2Vec2 [facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h)
    architecture.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨ [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– Wav2Vec2 æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Wav2Vec2 [facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Wav2Vec2CTCTokenizer
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2CTCTokenizer
- en: '### `class transformers.Wav2Vec2CTCTokenizer`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2CTCTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L127)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L127)'
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” File containing the vocabulary.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) â€” åŒ…å«è¯æ±‡è¡¨çš„æ–‡ä»¶ã€‚'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) â€” The beginning of sentence
    token.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” å¥å­å¼€å¤´æ ‡è®°ã€‚'
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) â€” The end of sentence
    token.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” å¥å­ç»“æŸæ ‡è®°ã€‚'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) â€” The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) â€” The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<pad>"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚'
- en: '`word_delimiter_token` (`str`, *optional*, defaults to `"|"`) â€” The token used
    for defining the end of a word.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_delimiter_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"|"`) â€” ç”¨äºå®šä¹‰å•è¯ç»“å°¾çš„æ ‡è®°ã€‚'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to accept lowercase input and lowercase the output when decoding.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦æ¥å—å°å†™è¾“å…¥å¹¶åœ¨è§£ç æ—¶å°†è¾“å‡ºè½¬æ¢ä¸ºå°å†™ã€‚'
- en: '`target_lang` (`str`, *optional*) â€” A target language the tokenizer should
    set by default. `target_lang` has to be defined for multi-lingual, nested vocabulary
    such as [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_lang` (`str`, *å¯é€‰*) â€” åˆ†è¯å™¨åº”é»˜è®¤è®¾ç½®çš„ç›®æ ‡è¯­è¨€ã€‚å¯¹äºå¤šè¯­è¨€ã€åµŒå¥—è¯æ±‡è¡¨ï¼Œå¦‚ [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)ï¼Œå¿…é¡»å®šä¹‰
    `target_lang`ã€‚'
- en: '**kwargs â€” Additional keyword arguments passed along to [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kwargs â€” ä¼ é€’ç»™ [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    çš„é¢å¤–å…³é”®å­—å‚æ•°'
- en: Constructs a Wav2Vec2CTC tokenizer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª Wav2Vec2CTC åˆ†è¯å™¨ã€‚
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains some of the main methods. Users should refer to the superclass
    for more information regarding such methods.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `__call__`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®
    `is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®
    `is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®
    `is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹é‡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœåºåˆ—ä»¥å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–ï¼‰çš„å½¢å¼æä¾›ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤æ‰¹é‡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„`PretrainedTokenizerBase.build_inputs_with_special_tokens`å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥idçš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ `bos`æˆ–`eos`æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`ï¼šå¡«å……åˆ°æ‰¹é‡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`ï¼šå¡«å……åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰ï¼Œåˆ™å°†é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€éœ€è¦æœ€å¤§é•¿åº¦ï¼‰ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, é»˜è®¤ä¸º0) â€” å¦‚æœä¸`max_length`ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«ä»æˆªæ–­åºåˆ—æœ«å°¾è¿”å›çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„æ ‡è®°åŒ–ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†å‰²ä¸ºå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™åˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†å‰²ä¸ºå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ã€‚è¿™å¯¹äºå‘½åå®ä½“è¯†åˆ«æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´»`padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>=
    7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨Tensor Coresç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: è¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: '`return_token_type_ids` (`bool`, *optional*) â€” Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›token type IDsã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šæ ‡è®°åŒ–å™¨çš„é»˜è®¤å€¼è¿”å›token
    type IDsï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯token type IDsï¼Ÿ](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) â€” Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šæ ‡è®°åŒ–å™¨çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚ '
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯attention masksï¼Ÿ](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„tokenåºåˆ—ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹è¾“å…¥idåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy
    = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„tokensã€‚'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return special tokens mask information.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›ç‰¹æ®Štokenæ©ç ä¿¡æ¯ã€‚'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›æ¯ä¸ªtokençš„`(char_start,
    char_end)`ã€‚'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Pythonâ€™s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿæ ‡è®°åŒ–å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„æ ‡è®°åŒ–å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚
- en: '`return_length` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) â€” Whether or not to print
    more information and warnings. **kwargs â€” passed to the `self.tokenize()` method'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•'
- en: Returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š
- en: '`input_ids` â€” List of token ids to be fed to a model.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„token idåˆ—è¡¨ã€‚'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯input IDsï¼Ÿ](../glossary#input-ids)'
- en: '`token_type_ids` â€” List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *â€œtoken_type_idsâ€* is in `self.model_input_names`).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„token type idsåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*â€œtoken_type_idsâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯token type IDsï¼Ÿ](../glossary#token-type-ids)'
- en: '`attention_mask` â€” List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *â€œattention_maskâ€* is
    in `self.model_input_names`).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` â€” æŒ‡å®šå“ªäº›tokenåº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*â€œattention_maskâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯attention masksï¼Ÿ](../glossary#attention-mask)'
- en: '`overflowing_tokens` â€” List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` â€” æº¢å‡ºçš„tokenåºåˆ—åˆ—è¡¨ï¼ˆå½“æŒ‡å®šäº†`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`num_truncated_tokens` â€” Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` â€” è¢«æˆªæ–­çš„tokenæ•°é‡ï¼ˆå½“æŒ‡å®šäº†`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`special_tokens_mask` â€” List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Štokenï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—tokenï¼ˆå½“`add_special_tokens=True`ä¸”`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚'
- en: '`length` â€” The length of the inputs (when `return_length=True`)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸»è¦æ–¹æ³•æ ‡è®°åŒ–å¹¶ä¸ºæ¨¡å‹å‡†å¤‡ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹ã€‚
- en: '#### `save_vocabulary`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L646)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L646)'
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `decode`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L541)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L541)'
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” åˆ†è¯åçš„è¾“å…¥idåˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚'
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_char_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå­—ç¬¦åç§»é‡ã€‚å­—ç¬¦åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å­—ç¬¦çš„æ—¶é—´æˆ³ã€‚'
- en: Please take a look at the example below to better understand how to make use
    of `output_char_offsets`.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_char_offsets`ã€‚
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚'
- en: Please take a look at the example below to better understand how to make use
    of `output_word_offsets`.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰ â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`str` or `Wav2Vec2CTCTokenizerOutput`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`æˆ–`Wav2Vec2CTCTokenizerOutput`'
- en: The list of decoded sentences. Will be a `Wav2Vec2CTCTokenizerOutput` when `output_char_offsets
    == True` or `output_word_offsets == True`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„å¥å­åˆ—è¡¨ã€‚å½“`output_char_offsets == True`æˆ–`output_word_offsets == True`æ—¶ï¼Œå°†æ˜¯`Wav2Vec2CTCTokenizerOutput`ã€‚
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸€ç³»åˆ—idè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨åˆ†è¯å™¨å’Œè¯æ±‡è¡¨ï¼Œå¯ä»¥é€‰æ‹©åˆ é™¤ç‰¹æ®Šæ ‡è®°å¹¶æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºæ‰§è¡Œ`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚
- en: 'Example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `batch_decode`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L471)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L471)'
- en: '[PRE7]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” åˆ†è¯åçš„è¾“å…¥idåˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚'
- en: '`output_char_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to output character offsets. Character offsets can be used in combination
    with the sampling rate and model downsampling rate to compute the time-stamps
    of transcribed characters.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_char_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå­—ç¬¦åç§»é‡ã€‚å­—ç¬¦åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å­—ç¬¦çš„æ—¶é—´æˆ³ã€‚'
- en: Please take a look at the Example of [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)
    to better understand how to make use of `output_char_offsets`. [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)
    works the same way with batched output.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_char_offsets`ã€‚[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)ä¸æ‰¹é‡è¾“å‡ºçš„æ–¹å¼ç›¸åŒã€‚
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚'
- en: Please take a look at the Example of [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)
    to better understand how to make use of `output_word_offsets`. [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)
    works the same way with batched output.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)ä¸æ‰¹é‡è¾“å‡ºçš„æ–¹å¼ç›¸åŒã€‚
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰ â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[str]` or `Wav2Vec2CTCTokenizerOutput`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`æˆ–`Wav2Vec2CTCTokenizerOutput`'
- en: The list of decoded sentences. Will be a `Wav2Vec2CTCTokenizerOutput` when `output_char_offsets
    == True` or `output_word_offsets == True`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„å¥å­åˆ—è¡¨ã€‚å½“`output_char_offsets == True`æˆ–`output_word_offsets == True`æ—¶ï¼Œå°†æ˜¯`Wav2Vec2CTCTokenizerOutput`ã€‚
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒç”¨è§£ç å‡½æ•°ï¼Œå°†ä¸€ç³»åˆ—token idçš„åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
- en: '#### `set_target_lang`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_target_lang`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L213)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L213)'
- en: '[PRE8]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Set the target language of a nested multi-lingual dictionary
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®åµŒå¥—å¤šè¯­è¨€å­—å…¸çš„ç›®æ ‡è¯­è¨€
- en: Wav2Vec2FeatureExtractor
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2FeatureExtractor
- en: '### `class transformers.Wav2Vec2FeatureExtractor`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2FeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L31)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L31)'
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`feature_size` (`int`, defaults to 1) â€” The feature dimension of the extracted
    features.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º1ï¼‰â€” æå–ç‰¹å¾çš„ç‰¹å¾ç»´åº¦ã€‚'
- en: '`sampling_rate` (`int`, defaults to 16000) â€” The sampling rate at which the
    audio files should be digitalized expressed in hertz (Hz).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º16000ï¼‰â€” åº”ä»¥èµ«å…¹ï¼ˆHzï¼‰è¡¨ç¤ºçš„éŸ³é¢‘æ–‡ä»¶æ•°å­—åŒ–çš„é‡‡æ ·ç‡ã€‚'
- en: '`padding_value` (`float`, defaults to 0.0) â€” The value that is used to fill
    the padding values.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value`ï¼ˆ`float`ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” ç”¨äºå¡«å……å€¼çš„å€¼ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    zero-mean unit-variance normalize the input. Normalizing can help to significantly
    improve the performance for some models, *e.g.*, [wav2vec2-lv60](https://huggingface.co/models?search=lv60).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œé›¶å‡å€¼å•ä½æ–¹å·®å½’ä¸€åŒ–ã€‚å½’ä¸€åŒ–å¯ä»¥å¸®åŠ©ä¸€äº›æ¨¡å‹æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä¾‹å¦‚
    [wav2vec2-lv60](https://huggingface.co/models?search=lv60)ã€‚'
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not [`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    should return `attention_mask`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ [`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    åº”è¯¥è¿”å› `attention_mask`ã€‚'
- en: Wav2Vec2 models that have set `config.feat_extract_norm == "group"`, such as
    [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), have `not`
    been trained using `attention_mask`. For such models, `input_values` should simply
    be padded with 0 and no `attention_mask` should be passed.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ç½®äº† `config.feat_extract_norm == "group"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œæ²¡æœ‰ä½¿ç”¨
    `attention_mask` è¿›è¡Œè®­ç»ƒã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……ï¼Œä¸åº”ä¼ é€’ `attention_mask`ã€‚
- en: For Wav2Vec2 models that have set `config.feat_extract_norm == "layer"`, such
    as [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self),
    `attention_mask` should be passed for batched inference.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºè®¾ç½®äº† `config.feat_extract_norm == "layer"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)ï¼Œåº”è¯¥ä¸ºæ‰¹é‡æ¨æ–­ä¼ é€’
    `attention_mask`ã€‚
- en: Constructs a Wav2Vec2 feature extractor.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª Wav2Vec2 ç‰¹å¾æå–å™¨ã€‚
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç‰¹å¾æå–å™¨ç»§æ‰¿è‡ª [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `__call__`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L102)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L102)'
- en: '[PRE10]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    â€” The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_speech`ï¼ˆ`np.ndarray`ï¼Œ`List[float]`ï¼Œ`List[np.ndarray]`ï¼Œ`List[List[float]]`ï¼‰â€”
    è¦å¡«å……çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ª numpy æ•°ç»„ï¼Œä¸€ä¸ªæµ®ç‚¹å€¼åˆ—è¡¨ï¼Œä¸€ä¸ª numpy æ•°ç»„åˆ—è¡¨æˆ–ä¸€ä¸ªæµ®ç‚¹å€¼åˆ—è¡¨çš„åˆ—è¡¨ã€‚å¿…é¡»æ˜¯å•å£°é“éŸ³é¢‘ï¼Œä¸æ˜¯ç«‹ä½“å£°ï¼Œå³æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Select a strategy to pad the returned sequences
    (according to the modelâ€™s padding side and padding index) among:'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ï¼Œ`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”
    é€‰æ‹©ä¸€ç§ç­–ç•¥æ¥å¡«å……è¿”å›çš„åºåˆ—ï¼ˆæ ¹æ®æ¨¡å‹çš„å¡«å……æ–¹å‘å’Œå¡«å……ç´¢å¼•ï¼‰ï¼ŒåŒ…æ‹¬ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`ï¼šå¡«å……åˆ°æŒ‡å®šå‚æ•° `max_length` çš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Maximum length of the returned list and
    optionally padding length (see above).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è¿”å›åˆ—è¡¨çš„æœ€å¤§é•¿åº¦å’Œå¯é€‰å¡«å……é•¿åº¦ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚'
- en: '`truncation` (`bool`) â€” Activates truncation to cut input sequences longer
    than *max_length* to *max_length*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼‰â€” æ¿€æ´»æˆªæ–­ï¼Œå°†è¾“å…¥åºåˆ—æˆªæ–­ä¸ºæ¯” *max_length* æ›´é•¿çš„åºåˆ—åˆ° *max_length*ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºå¯ç”¨ NVIDIA ç¡¬ä»¶ä¸Šçš„ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ï¼Œå…¶è®¡ç®—èƒ½åŠ› `>= 7.5`ï¼ˆVoltaï¼‰ï¼Œæˆ–è€…å¯¹äºå—ç›Šäºåºåˆ—é•¿åº¦ä¸º 128 çš„å€æ•°çš„
    TPUã€‚
- en: '`return_attention_mask` (`bool`, *optional*) â€” Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractorâ€™s default.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤å€¼ï¼Œå°†æ ¹æ®ç‰¹å®š feature_extractor
    çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›æ©ç ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: Wav2Vec2 models that have set `config.feat_extract_norm == "group"`, such as
    [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), have `not`
    been trained using `attention_mask`. For such models, `input_values` should simply
    be padded with 0 and no `attention_mask` should be passed.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºè®¾ç½®äº† `config.feat_extract_norm == "group"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œæ²¡æœ‰ä½¿ç”¨
    `attention_mask` è¿›è¡Œè®­ç»ƒã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……ï¼Œä¸åº”ä¼ é€’ `attention_mask`ã€‚
- en: For Wav2Vec2 models that have set `config.feat_extract_norm == "layer"`, such
    as [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self),
    `attention_mask` should be passed for batched inference.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºè®¾ç½®äº† `config.feat_extract_norm == "layer"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)ï¼Œåº”è¯¥ä¸ºæ‰¹é‡æ¨æ–­ä¼ é€’
    `attention_mask`ã€‚
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚'
- en: '`sampling_rate` (`int`, *optional*) â€” The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *å¯é€‰*) â€” `raw_speech` è¾“å…¥é‡‡æ ·çš„é‡‡æ ·ç‡ã€‚å¼ºçƒˆå»ºè®®åœ¨å‰å‘è°ƒç”¨æ—¶ä¼ é€’ `sampling_rate`
    ä»¥é˜²æ­¢é™é»˜é”™è¯¯ã€‚'
- en: '`padding_value` (`float`, defaults to 0.0) â€”'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value` (`float`, é»˜è®¤ä¸º 0.0) â€”'
- en: Main method to featurize and prepare for the model one or several sequence(s).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—è¿›è¡Œç‰¹å¾åŒ–å’Œä¸ºæ¨¡å‹å‡†å¤‡çš„ä¸»è¦æ–¹æ³•ã€‚
- en: Wav2Vec2Processor
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2Processor
- en: '### `class transformers.Wav2Vec2Processor`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2Processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L26)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[`<æ¥æº>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L26)'
- en: '[PRE11]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`feature_extractor` (`Wav2Vec2FeatureExtractor`) â€” An instance of [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` (`Wav2Vec2FeatureExtractor`) â€” [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    çš„ä¸€ä¸ªå®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” An instance of [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    The tokenizer is a required input.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    â€” [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor and
    a Wav2Vec2 CTC tokenizer into a single processor.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ª Wav2Vec2 å¤„ç†å™¨ï¼Œå°† Wav2Vec2 ç‰¹å¾æå–å™¨å’Œ Wav2Vec2 CTC åˆ†è¯å™¨å°è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚
- en: '[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    offers all the functionalities of [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    and [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    and [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode)
    for more information.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    æä¾›äº† [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    å’Œ [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    çš„æ‰€æœ‰åŠŸèƒ½ã€‚æŸ¥çœ‹ [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    å’Œ [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode)
    çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: '#### `__call__`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L68)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[`<æ¥æº>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L68)'
- en: '[PRE12]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractorâ€™s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to PreTrainedTokenizerâ€™s [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ° Wav2Vec2FeatureExtractor çš„ [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    å¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡ `as_target_processor()` ä¸­ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œå°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ° PreTrainedTokenizer çš„ [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ä¸ªæ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `pad`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pad`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L106)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[`<æ¥æº>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L106)'
- en: '[PRE13]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractorâ€™s
    [pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to PreTrainedTokenizerâ€™s [pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2FeatureExtractorçš„[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)å¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡`as_target_processor()`ä¸­ä½¿ç”¨ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `from_pretrained`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L49)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L49)'
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#### `save_pretrained`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
- en: '[PRE15]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory where the feature extractor
    JSON file and the tokenizer files will be saved (directory will be created if
    it does not exist).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” ç‰¹å¾æå–å™¨JSONæ–‡ä»¶å’Œåˆ†è¯å™¨æ–‡ä»¶å°†ä¿å­˜åœ¨çš„ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™å°†åˆ›å»ºç›®å½•ï¼‰ã€‚'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨ä¿å­˜åå°†æ¨¡å‹æ¨é€åˆ°Hugging Faceæ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`repo_id`æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„`save_directory`åç§°ï¼‰ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¼ é€’ç»™[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Saves the attributes of this processor (feature extractor, tokenizerâ€¦) in the
    specified directory so that it can be reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)
    method.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å¤„ç†å™¨çš„å±æ€§ï¼ˆç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨ç­‰ï¼‰ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)æ–¹æ³•é‡æ–°åŠ è½½ã€‚
- en: This class method is simply calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained).
    Please refer to the docstrings of the methods above for more information.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)å’Œ[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `batch_decode`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L136)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L136)'
- en: '[PRE16]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This method forwards all its arguments to PreTrainedTokenizerâ€™s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `decode`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L143)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L143)'
- en: '[PRE17]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This method forwards all its arguments to PreTrainedTokenizerâ€™s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: Wav2Vec2ProcessorWithLM
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ProcessorWithLM
- en: '### `class transformers.Wav2Vec2ProcessorWithLM`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ProcessorWithLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L67)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L67)'
- en: '[PRE18]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`feature_extractor` ([Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor))
    â€” An instance of [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`ï¼ˆ[Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractorï¼‰ï¼‰â€”
    [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)çš„ä¸€ä¸ªå®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` ([Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer))
    â€” An instance of [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
    The tokenizer is a required input.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizerï¼‰ï¼‰â€”
    [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`decoder` (`pyctcdecode.BeamSearchDecoderCTC`) â€” An instance of `pyctcdecode.BeamSearchDecoderCTC`.
    The decoder is a required input.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder`ï¼ˆ`pyctcdecode.BeamSearchDecoderCTC`ï¼‰â€” `pyctcdecode.BeamSearchDecoderCTC`çš„ä¸€ä¸ªå®ä¾‹ã€‚è§£ç å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor, a
    Wav2Vec2 CTC tokenizer and a decoder with language model support into a single
    processor for language model boosted speech recognition decoding.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªWav2Vec2å¤„ç†å™¨ï¼Œå°†Wav2Vec2ç‰¹å¾æå–å™¨ã€Wav2Vec2 CTCåˆ†è¯å™¨å’Œå…·æœ‰è¯­è¨€æ¨¡å‹æ”¯æŒçš„è§£ç å™¨åŒ…è£…åˆ°ä¸€ä¸ªå•ä¸€çš„å¤„ç†å™¨ä¸­ï¼Œç”¨äºè¯­è¨€æ¨¡å‹å¢å¼ºçš„è¯­éŸ³è¯†åˆ«è§£ç ã€‚
- en: '#### `__call__`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L215)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L215)'
- en: '[PRE19]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractorâ€™s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to Wav2Vec2CTCTokenizerâ€™s [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2FeatureExtractorçš„[**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)ï¼Œå¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨`as_target_processor()`ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2CTCTokenizerçš„[**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `pad`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pad`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L254)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L254)'
- en: '[PRE20]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractorâ€™s
    [pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to Wav2Vec2CTCTokenizerâ€™s [pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad).
    Please refer to the docstring of the above two methods for more information.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2FeatureExtractorçš„[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)ï¼Œå¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨`as_target_processor()`ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2CTCTokenizerçš„[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `from_pretrained`'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L113)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L113)'
- en: '[PRE21]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) â€” This can be either:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) â€” å¯ä»¥æ˜¯ï¼š'
- en: a string, the *model id* of a pretrained feature_extractor hosted inside a model
    repo on huggingface.co. Valid model ids can be located at the root-level, like
    `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„*æ¨¡å‹ID*çš„å­—ç¬¦ä¸²ï¼Œæ‰˜ç®¡åœ¨huggingface.coä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ä¸­ã€‚æœ‰æ•ˆçš„æ¨¡å‹IDå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹å‘½åç©ºé—´åŒ–ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚
- en: a path to a *directory* containing a feature extractor file saved using the
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)æ–¹æ³•ä¿å­˜çš„ç‰¹å¾æå–å™¨æ–‡ä»¶çš„*ç›®å½•*è·¯å¾„ï¼Œä¾‹å¦‚`./my_model_directory/`ã€‚
- en: a path or url to a saved feature extractor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.
    **kwargs â€” Additional keyword arguments passed along to both [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    and [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨JSONæ–‡ä»¶çš„è·¯å¾„æˆ–URLï¼Œä¾‹å¦‚`./my_model_directory/preprocessor_config.json`ã€‚**kwargs
    â€” ä¼ é€’ç»™[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)å’Œ[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)çš„é¢å¤–å…³é”®å­—å‚æ•°
- en: Instantiate a [Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)
    from a pretrained Wav2Vec2 processor.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒçš„Wav2Vec2å¤„ç†å™¨å®ä¾‹åŒ–ä¸€ä¸ª[Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)ã€‚
- en: This class method is simply calling Wav2Vec2FeatureExtractorâ€™s [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained),
    Wav2Vec2CTCTokenizerâ€™s [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained),
    and `pyctcdecode.BeamSearchDecoderCTC.load_from_hf_hub`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨äº†Wav2Vec2FeatureExtractorçš„[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)ï¼ŒWav2Vec2CTCTokenizerçš„[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)ï¼Œä»¥åŠ`pyctcdecode.BeamSearchDecoderCTC.load_from_hf_hub`ã€‚
- en: Please refer to the docstrings of the methods above for more information.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `save_pretrained`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L109)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L109)'
- en: '[PRE22]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#### `batch_decode`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L285)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L285)'
- en: '[PRE23]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`logits` (`np.ndarray`) â€” The logits output vector of the model representing
    the log probabilities for each token.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`np.ndarray`) â€” æ¨¡å‹è¾“å‡ºçš„logitså‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡ã€‚'
- en: '`pool` (`multiprocessing.Pool`, *optional*) â€” An optional user-managed pool.
    If not set, one will be automatically created and closed. The pool should be instantiated
    *after* `Wav2Vec2ProcessorWithLM`. Otherwise, the LM wonâ€™t be available to the
    poolâ€™s sub-processes.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool` (`multiprocessing.Pool`, *optional*) â€” å¯é€‰çš„ç”¨æˆ·ç®¡ç†çš„æ± ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†è‡ªåŠ¨åˆ›å»ºå¹¶å…³é—­ä¸€ä¸ªæ± ã€‚æ± åº”åœ¨`Wav2Vec2ProcessorWithLM`ä¹‹åå®ä¾‹åŒ–ã€‚å¦åˆ™ï¼ŒLMå°†ä¸å¯ç”¨äºæ± çš„å­è¿›ç¨‹ã€‚'
- en: Currently, only pools created with a â€˜forkâ€™ context can be used. If a â€˜spawnâ€™
    pool is passed, it will be ignored and sequential decoding will be used instead.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œåªæœ‰ä½¿ç”¨â€œforkâ€ä¸Šä¸‹æ–‡åˆ›å»ºçš„æ± æ‰èƒ½ä½¿ç”¨ã€‚å¦‚æœä¼ é€’äº†â€œspawnâ€æ± ï¼Œå®ƒå°†è¢«å¿½ç•¥ï¼Œè€Œå°†ä½¿ç”¨é¡ºåºè§£ç ã€‚
- en: '`num_processes` (`int`, *optional*) â€” If `pool` is not set, number of processes
    on which the function should be parallelized over. Defaults to the number of available
    CPUs.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_processes` (`int`, *optional*) â€” å¦‚æœæœªè®¾ç½®`pool`ï¼Œåˆ™åº”è¯¥åœ¨å“ªäº›è¿›ç¨‹ä¸Šå¹¶è¡ŒåŒ–å‡½æ•°ã€‚é»˜è®¤ä¸ºå¯ç”¨CPUçš„æ•°é‡ã€‚'
- en: '`beam_width` (`int`, *optional*) â€” Maximum number of beams at each step in
    decoding. Defaults to pyctcdecodeâ€™s DEFAULT_BEAM_WIDTH.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_width` (`int`, *optional*) â€” è§£ç è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„æœ€å¤§beamæ•°ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_BEAM_WIDTHã€‚'
- en: '`beam_prune_logp` (`int`, *optional*) â€” Beams that are much worse than best
    beam will be pruned Defaults to pyctcdecodeâ€™s DEFAULT_PRUNE_LOGP.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_prune_logp` (`int`, *optional*) â€” æ¯”æœ€ä½³beamå·®å¾ˆå¤šçš„beamå°†è¢«ä¿®å‰ªã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_PRUNE_LOGPã€‚'
- en: '`token_min_logp` (`int`, *optional*) â€” Tokens below this logp are skipped unless
    they are argmax of frame Defaults to pyctcdecodeâ€™s DEFAULT_MIN_TOKEN_LOGP.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_min_logp` (`int`, *optional*) â€” ä½äºæ­¤logpçš„æ ‡è®°å°†è¢«è·³è¿‡ï¼Œé™¤éå®ƒä»¬æ˜¯å¸§çš„argmaxã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_MIN_TOKEN_LOGPã€‚'
- en: '`hotwords` (`List[str]`, *optional*) â€” List of words with extra importance,
    can be OOV for LM'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotwords` (`List[str]`, *optional*) â€” å…·æœ‰é¢å¤–é‡è¦æ€§çš„å•è¯åˆ—è¡¨ï¼Œå¯ä»¥æ˜¯LMçš„OOV'
- en: '`hotword_weight` (`int`, *optional*) â€” Weight factor for hotword importance
    Defaults to pyctcdecodeâ€™s DEFAULT_HOTWORD_WEIGHT.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotword_weight` (`int`, *optional*) â€” çƒ­è¯é‡è¦æ€§çš„æƒé‡å› å­ï¼Œé»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_HOTWORD_WEIGHTã€‚'
- en: '`alpha` (`float`, *optional*) â€” Weight for language model during shallow fusion'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha` (`float`, *optional*) â€” æµ…èåˆæœŸé—´è¯­è¨€æ¨¡å‹çš„æƒé‡'
- en: '`beta` (`float`, *optional*) â€” Weight for length score adjustment of during
    scoring'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta` (`float`, *optional*) â€” åœ¨è¯„åˆ†è¿‡ç¨‹ä¸­é•¿åº¦å¾—åˆ†è°ƒæ•´çš„æƒé‡'
- en: '`unk_score_offset` (`float`, *optional*) â€” Amount of log score offset for unknown
    tokens'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_score_offset` (`float`, *optional*) â€” æœªçŸ¥æ ‡è®°çš„å¯¹æ•°åˆ†æ•°åç§»é‡'
- en: '`lm_score_boundary` (`bool`, *optional*) â€” Whether to have kenlm respect boundaries
    when scoring'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm_score_boundary` (`bool`, *optional*) â€” åœ¨è¯„åˆ†æ—¶æ˜¯å¦è®©kenlmå°Šé‡è¾¹ç•Œ'
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œä»¥è®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚'
- en: '`n_best` (`int`, *optional*, defaults to `1`) â€” Number of best hypotheses to
    return. If `n_best` is greater than 1, the returned `text` will be a list of lists
    of strings, `logit_score` will be a list of lists of floats, and `lm_score` will
    be a list of lists of floats, where the length of the outer list will correspond
    to the batch size and the length of the inner list will correspond to the number
    of returned hypotheses . The value should be >= 1.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_best` (`int`, *optional*, é»˜è®¤ä¸º`1`) â€” è¦è¿”å›çš„æœ€ä½³å‡è®¾æ•°é‡ã€‚å¦‚æœ`n_best`å¤§äº1ï¼Œåˆ™è¿”å›çš„`text`å°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼Œ`logit_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨çš„åˆ—è¡¨ï¼Œ`lm_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨çš„åˆ—è¡¨ï¼Œå¤–éƒ¨åˆ—è¡¨çš„é•¿åº¦å°†å¯¹åº”æ‰¹æ¬¡å¤§å°ï¼Œå†…éƒ¨åˆ—è¡¨çš„é•¿åº¦å°†å¯¹åº”è¿”å›çš„å‡è®¾æ•°é‡ã€‚è¯¥å€¼åº”
    >= 1ã€‚'
- en: Please take a look at the Example of [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)
    to better understand how to make use of `output_word_offsets`. [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)
    works the same way with batched output.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)ä¸æ‰¹é‡è¾“å‡ºçš„æ–¹å¼ç›¸åŒã€‚
- en: Batch decode output logits to audio transcription with language model support.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡è§£ç è¾“å‡ºlogitsä»¥æ”¯æŒè¯­è¨€æ¨¡å‹çš„éŸ³é¢‘è½¬å½•ã€‚
- en: This function makes use of Pythonâ€™s multiprocessing. Currently, multiprocessing
    is available only on Unix systems (see this [issue](https://github.com/kensho-technologies/pyctcdecode/issues/65)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‡½æ•°åˆ©ç”¨äº†Pythonçš„å¤šè¿›ç¨‹ã€‚ç›®å‰ï¼Œå¤šè¿›ç¨‹ä»…åœ¨Unixç³»ç»Ÿä¸Šå¯ç”¨ï¼ˆè¯·å‚é˜…æ­¤[é—®é¢˜](https://github.com/kensho-technologies/pyctcdecode/issues/65)ï¼‰ã€‚
- en: If you are decoding multiple batches, consider creating a `Pool` and passing
    it to `batch_decode`. Otherwise, `batch_decode` will be very slow since it will
    create a fresh `Pool` for each call. See usage example below.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨è§£ç å¤šä¸ªæ‰¹æ¬¡ï¼Œè¯·è€ƒè™‘åˆ›å»ºä¸€ä¸ª`Pool`å¹¶å°†å…¶ä¼ é€’ç»™`batch_decode`ã€‚å¦åˆ™ï¼Œ`batch_decode`å°†éå¸¸æ…¢ï¼Œå› ä¸ºå®ƒå°†ä¸ºæ¯æ¬¡è°ƒç”¨åˆ›å»ºä¸€ä¸ªæ–°çš„`Pool`ã€‚è¯·å‚è§ä¸‹é¢çš„ç”¨æ³•ç¤ºä¾‹ã€‚
- en: 'Example: See [Decoding multiple audios](#decoding-multiple-audios).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼šè¯·å‚è§[è§£ç å¤šä¸ªéŸ³é¢‘](#decoding-multiple-audios)ã€‚
- en: '#### `decode`'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L470)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L470)'
- en: '[PRE24]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`logits` (`np.ndarray`) â€” The logits output vector of the model representing
    the log probabilities for each token.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`np.ndarray`) â€” ä»£è¡¨æ¯ä¸ªæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡çš„æ¨¡å‹è¾“å‡ºå‘é‡ã€‚'
- en: '`beam_width` (`int`, *optional*) â€” Maximum number of beams at each step in
    decoding. Defaults to pyctcdecodeâ€™s DEFAULT_BEAM_WIDTH.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_width` (`int`, *optional*) â€” è§£ç è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„æœ€å¤§beamæ•°ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_BEAM_WIDTHã€‚'
- en: '`beam_prune_logp` (`int`, *optional*) â€” A threshold to prune beams with log-probs
    less than best_beam_logp + beam_prune_logp. The value should be <= 0\. Defaults
    to pyctcdecodeâ€™s DEFAULT_PRUNE_LOGP.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_prune_logp` (`int`, *optional*) â€” ä¸€ä¸ªç”¨äºä¿®å‰ªlog-probså°äºbest_beam_logp + beam_prune_logpçš„é˜ˆå€¼ã€‚è¯¥å€¼åº”
    <= 0ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_PRUNE_LOGPã€‚'
- en: '`token_min_logp` (`int`, *optional*) â€” Tokens with log-probs below token_min_logp
    are skipped unless they are have the maximum log-prob for an utterance. Defaults
    to pyctcdecodeâ€™s DEFAULT_MIN_TOKEN_LOGP.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_min_logp`ï¼ˆ`int`ï¼Œ*optional*ï¼‰ â€” log-probsä½äºtoken_min_logpçš„æ ‡è®°å°†è¢«è·³è¿‡ï¼Œé™¤éå®ƒä»¬æ˜¯è¯è¯­çš„æœ€å¤§log-probã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_MIN_TOKEN_LOGPã€‚'
- en: '`hotwords` (`List[str]`, *optional*) â€” List of words with extra importance
    which can be missing from the LMâ€™s vocabulary, e.g. [â€œhuggingfaceâ€]'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotwords`ï¼ˆ`List[str]`ï¼Œ*optional*ï¼‰ â€” å…·æœ‰é¢å¤–é‡è¦æ€§çš„å•è¯åˆ—è¡¨ï¼Œå¯èƒ½ä¸åœ¨LMçš„è¯æ±‡è¡¨ä¸­ï¼Œä¾‹å¦‚[â€œhuggingfaceâ€]'
- en: '`hotword_weight` (`int`, *optional*) â€” Weight multiplier that boosts hotword
    scores. Defaults to pyctcdecodeâ€™s DEFAULT_HOTWORD_WEIGHT.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hotword_weight`ï¼ˆ`int`ï¼Œ*optional*ï¼‰ â€” å¢å¼ºçƒ­è¯åˆ†æ•°çš„æƒé‡ä¹˜æ•°ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_HOTWORD_WEIGHTã€‚'
- en: '`alpha` (`float`, *optional*) â€” Weight for language model during shallow fusion'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`ï¼ˆ`float`ï¼Œ*optional*ï¼‰ â€” æµ…èåˆæœŸé—´è¯­è¨€æ¨¡å‹çš„æƒé‡'
- en: '`beta` (`float`, *optional*) â€” Weight for length score adjustment of during
    scoring'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta`ï¼ˆ`float`ï¼Œ*optional*ï¼‰ â€” åœ¨è¯„åˆ†è¿‡ç¨‹ä¸­é•¿åº¦åˆ†æ•°è°ƒæ•´çš„æƒé‡'
- en: '`unk_score_offset` (`float`, *optional*) â€” Amount of log score offset for unknown
    tokens'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_score_offset`ï¼ˆ`float`ï¼Œ*optional*ï¼‰ â€” æœªçŸ¥æ ‡è®°çš„logåˆ†æ•°åç§»é‡'
- en: '`lm_score_boundary` (`bool`, *optional*) â€” Whether to have kenlm respect boundaries
    when scoring'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm_score_boundary` (`bool`, *optional*) â€” æ˜¯å¦åœ¨è¯„åˆ†æ—¶è®©kenlmå°Šé‡è¾¹ç•Œ'
- en: '`output_word_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to output word offsets. Word offsets can be used in combination with the sampling
    rate and model downsampling rate to compute the time-stamps of transcribed words.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_word_offsets` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚'
- en: '`n_best` (`int`, *optional*, defaults to `1`) â€” Number of best hypotheses to
    return. If `n_best` is greater than 1, the returned `text` will be a list of strings,
    `logit_score` will be a list of floats, and `lm_score` will be a list of floats,
    where the length of these lists will correspond to the number of returned hypotheses.
    The value should be >= 1.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_best`ï¼ˆ`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`1`ï¼‰ â€” è¦è¿”å›çš„æœ€ä½³å‡è®¾æ•°é‡ã€‚å¦‚æœ`n_best`å¤§äº1ï¼Œåˆ™è¿”å›çš„`text`å°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œ`logit_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œ`lm_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œè¿™äº›åˆ—è¡¨çš„é•¿åº¦å°†å¯¹åº”äºè¿”å›çš„å‡è®¾æ•°é‡ã€‚è¯¥å€¼åº”å¤§äºç­‰äº1ã€‚'
- en: Please take a look at the example below to better understand how to make use
    of `output_word_offsets`.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚
- en: Decode output logits to audio transcription with language model support.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯­è¨€æ¨¡å‹æ”¯æŒå°†è¾“å‡ºé€»è¾‘è§£ç ä¸ºéŸ³é¢‘è½¬å½•ã€‚
- en: 'Example:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE25]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Decoding multiple audios
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£ç å¤šä¸ªéŸ³é¢‘
- en: 'If you are planning to decode multiple batches of audios, you should consider
    using [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)
    and passing an instantiated `multiprocessing.Pool`. Otherwise, [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)
    performance will be slower than calling [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)
    for each audio individually, as it internally instantiates a new `Pool` for every
    call. See the example below:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è®¡åˆ’è§£ç å¤šæ‰¹éŸ³é¢‘ï¼Œåº”è€ƒè™‘ä½¿ç”¨[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)å¹¶ä¼ é€’ä¸€ä¸ªå®ä¾‹åŒ–çš„`multiprocessing.Pool`ã€‚å¦åˆ™ï¼Œ[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)çš„æ€§èƒ½å°†æ¯”ä¸ºæ¯ä¸ªéŸ³é¢‘å•ç‹¬è°ƒç”¨[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)æ…¢ï¼Œå› ä¸ºå®ƒåœ¨æ¯æ¬¡è°ƒç”¨æ—¶å†…éƒ¨å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„`Pool`ã€‚è¯·å‚é˜…ä¸‹é¢çš„ç¤ºä¾‹ï¼š
- en: '[PRE26]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Wav2Vec2 specific outputs
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ç‰¹å®šè¾“å‡º
- en: '### `class transformers.models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L44)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L44)'
- en: '[PRE27]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (list of `str` or `str`) â€” Decoded logits in text from. Usually the
    speech transcription.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`ï¼ˆ`str`åˆ—è¡¨æˆ–`str`ï¼‰ â€” æ–‡æœ¬ä¸­çš„è§£ç é€»è¾‘ã€‚é€šå¸¸æ˜¯è¯­éŸ³è½¬å½•ã€‚'
- en: '`logit_score` (list of `float` or `float`) â€” Total logit score of the beams
    associated with produced text.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logit_score`ï¼ˆ`float`åˆ—è¡¨æˆ–`float`ï¼‰ â€” ä¸ç”Ÿæˆæ–‡æœ¬ç›¸å…³çš„beamçš„æ€»logitåˆ†æ•°ã€‚'
- en: '`lm_score` (list of `float`) â€” Fused lm_score of the beams associated with
    produced text.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm_score`ï¼ˆ`float`åˆ—è¡¨ï¼‰ â€” ä¸ç”Ÿæˆæ–‡æœ¬ç›¸å…³çš„beamçš„èåˆlm_scoreã€‚'
- en: '`word_offsets` (list of `List[Dict[str, Union[int, str]]]` or `List[Dict[str,
    Union[int, str]]]`) â€” Offsets of the decoded words. In combination with sampling
    rate and model downsampling rate word offsets can be used to compute time stamps
    for each word.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_offsets`ï¼ˆ`List[Dict[str, Union[int, str]]]`æˆ–`List[Dict[str, Union[int,
    str]]]`åˆ—è¡¨ â€” è§£ç å•è¯çš„åç§»é‡ã€‚ç»“åˆé‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ï¼Œå•è¯åç§»é‡å¯ç”¨äºè®¡ç®—æ¯ä¸ªå•è¯çš„æ—¶é—´æˆ³ã€‚'
- en: Output type of `Wav2Vec2DecoderWithLM`, with transcription.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`Wav2Vec2DecoderWithLM`çš„è¾“å‡ºç±»å‹ï¼Œå¸¦æœ‰è½¬å½•ã€‚'
- en: '### `class transformers.modeling_outputs.Wav2Vec2BaseModelOutput`'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.modeling_outputs.Wav2Vec2BaseModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_outputs.py#L1376)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_outputs.py#L1376)'
- en: '[PRE28]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰
    â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) â€” Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, conv_dim[-1])`çš„`torch.FloatTensor`ï¼‰
    â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚çš„æå–ç‰¹å¾å‘é‡åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: Base class for models that have been trained with the Wav2Vec2 loss objective.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Wav2Vec2æŸå¤±ç›®æ ‡è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹çš„åŸºç±»ã€‚
- en: '### `class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L100)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L100)'
- en: '[PRE29]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`loss` (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor`
    of shape `(1,)`) â€” Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*å¯é€‰*, å½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›, `torch.FloatTensor` of shape
    `(1,)`) â€” æ€»æŸå¤±ï¼Œç”±å¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„å’Œç»„æˆï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚
    ï¼ˆåˆ†ç±»ï¼‰æŸå¤±ã€‚'
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” æ¨¡å‹çš„éšè—çŠ¶æ€æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œå¯ç”¨äºé¢„æµ‹æ©ç çš„æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚'
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” é‡åŒ–æå–çš„ç‰¹å¾å‘é‡åºåˆ—ï¼ŒæŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œä»£è¡¨å¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`contrastive_loss` (*optional*, returned when `sample_negative_indices` are
    passed, `torch.FloatTensor` of shape `(1,)`) â€” The contrastive loss (L_m) as stated
    in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_loss` (*å¯é€‰*, å½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›, `torch.FloatTensor`
    of shape `(1,)`) â€” å¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚'
- en: '`diversity_loss` (*optional*, returned when `sample_negative_indices` are passed,
    `torch.FloatTensor` of shape `(1,)`) â€” The diversity loss (L_d) as stated in the
    [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss` (*å¯é€‰*, å½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›, `torch.FloatTensor`
    of shape `(1,)`) â€” å¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚'
- en: Output type of [Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining),
    with potential hidden states and attentions.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)çš„è¾“å‡ºç±»å‹ï¼Œå…·æœ‰æ½œåœ¨çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›ã€‚'
- en: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L44)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L44)'
- en: '[PRE30]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`)
    â€” Sequence of extracted feature vectors of the last convolutional layer of the
    model with `last_conv_dim` being the dimension of the last convolutional layer.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`)
    â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚æå–çš„ç‰¹å¾å‘é‡åºåˆ—ï¼Œå…¶ä¸­`last_conv_dim`æ˜¯æœ€åä¸€ä¸ªå·ç§¯å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: Output type of `FlaxWav2Vec2BaseModelOutput`, with potential hidden states and
    attentions.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2BaseModelOutput`çš„è¾“å‡ºç±»å‹ï¼Œå…·æœ‰æ½œåœ¨çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›ã€‚'
- en: '#### `replace`'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `replace`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
- en: '[PRE31]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: â€œReturns a new object replacing the specified fields with new values.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚
- en: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L74)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L74)'
- en: '[PRE32]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`loss` (*optional*, returned when model is in train mode, `jnp.ndarray` of
    shape `(1,)`) â€” Total loss as the sum of the contrastive loss (L_m) and the diversity
    loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*å¯é€‰*, å½“æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`jnp.ndarray`) â€” æ€»æŸå¤±ï¼Œä½œä¸ºå¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„æ€»å’Œï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚
    (åˆ†ç±») æŸå¤±ã€‚'
- en: '`projected_states` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`)
    â€” Hidden-states of the model projected to *config.proj_codevector_dim* that can
    be used to predict the masked projected quantized states.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`ï¼‰
    â€” æ¨¡å‹çš„éšè—çŠ¶æ€æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œå¯ç”¨äºé¢„æµ‹æ©ç æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚'
- en: '`projected_quantized_states` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    config.proj_codevector_dim)`ï¼‰ â€” æŠ•å½±åˆ°*config.proj_codevector_dim*çš„é‡åŒ–æå–ç‰¹å¾å‘é‡ï¼Œè¡¨ç¤ºå¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: Output type of `FlaxWav2Vec2ForPreTrainingOutput`, with potential hidden states
    and attentions.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2ForPreTrainingOutput`çš„è¾“å‡ºç±»å‹ï¼Œå…·æœ‰æ½œåœ¨çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›ã€‚'
- en: '#### `replace`'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `replace`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
- en: '[PRE33]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: â€œReturns a new object replacing the specified fields with new values.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚
- en: PytorchHide Pytorch content
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: Wav2Vec2Model
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2Model
- en: '### `class transformers.Wav2Vec2Model`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1440)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1440)'
- en: '[PRE34]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: 'The bare Wav2Vec2 Model transformer outputting raw hidden-states without any
    specific head on top. Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 'è£¸çš„ Wav2Vec2 æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman
    Mohamedã€Michael Auli åœ¨ [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1530)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1530)'
- en: '[PRE35]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° `List[float]` ç±»å‹çš„æ•°ç»„æˆ– `numpy.ndarray`
    ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ `input_values`ï¼Œåº”ä½¿ç”¨ [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º `torch.FloatTensor` ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´:'
- en: 1 for tokens that are `not masked`,
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 ç”¨äº `æœªè¢«æ©ç ` çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 ç”¨äº `è¢«æ©ç ` çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == True` æ—¶æ‰åº”ä¼ é€’ `attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰
    `config.return_attention_mask == False` çš„æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶åº”
    `ä¸` ä¼ é€’ `attention_mask` ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values` åº”ç®€å•åœ°ç”¨ 0 å¡«å……å¹¶åœ¨ä¸ä¼ é€’ `attention_mask`
    çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ® `input_values` æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—è¾“å‡ºã€‚'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) â€” Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length,
    conv_dim[-1])`) â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚æå–çš„ç‰¹å¾å‘é‡åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’ `output_hidden_states=True`
    æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰çš„*ï¼Œå½“ä¼ é€’ `output_attentions=True`
    æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length,
    sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)
    çš„ forward æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE36]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Wav2Vec2ForCTC
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForCTC
- en: '### `class transformers.Wav2Vec2ForCTC`'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1859)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1859)'
- en: '[PRE37]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•æ¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`target_lang` (`str`, *optional*) â€” Language id of adapter weights. Adapter
    weights are stored in the format adapter.<lang>.safetensors or adapter.<lang>.bin.
    Only relevant when using an instance of [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    with adapters. Uses â€˜engâ€™ by default.</lang></lang>'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_lang` (`str`ï¼Œ*å¯é€‰çš„*) â€” é€‚é…å™¨æƒé‡çš„è¯­è¨€ idã€‚é€‚é…å™¨æƒé‡å­˜å‚¨åœ¨æ ¼å¼ä¸º adapter.<lang>.safetensors
    æˆ– adapter.<lang>.bin çš„æ–‡ä»¶ä¸­ã€‚ä»…åœ¨ä½¿ç”¨å¸¦æœ‰é€‚é…å™¨çš„ [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    å®ä¾‹æ—¶ç›¸å…³ã€‚é»˜è®¤ä½¿ç”¨ â€˜engâ€™ã€‚</lang></lang>'
- en: 'Wav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC). Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¸¦æœ‰é¡¶éƒ¨ `è¯­è¨€å»ºæ¨¡` çš„ Wav2Vec2 æ¨¡å‹ï¼Œç”¨äº Connectionist Temporal Classification (CTC)ã€‚Wav2Vec2
    æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec 2.0:
    A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1941)'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1941)'
- en: '[PRE38]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€”
    è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º `List[float]` æˆ– `numpy.ndarray`
    çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (`pip install soundfile`)ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä¸º `input_values`ï¼Œåº”ä½¿ç”¨ [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º `torch.FloatTensor` ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«é®è”½çš„æ ‡è®°ï¼Œä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«é®è”½çš„æ ‡è®°ï¼Œä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask
    == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶ï¼Œåº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹è¿˜ä¼šæ ¹æ®`input_values`æ˜¯å¦å¡«å……è€Œäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    â€” Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *å¯é€‰*)
    â€” è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œ`target_length`å¿…é¡»å°äºæˆ–ç­‰äºè¾“å‡ºlogitsçš„åºåˆ—é•¿åº¦ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[-100, 0, ..., config.vocab_size
    - 1]`ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆé®è”½ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0, ..., config.vocab_size - 1]`ä¸­çš„æƒ…å†µã€‚'
- en: Returns
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *å¯é€‰*, å½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°çš„é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE39]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#### `load_adapter`'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_adapter`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1191)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1191)'
- en: '[PRE40]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`target_lang` (`str`) â€” Has to be a language id of an existing adapter weight.
    Adapter weights are stored in the format adapter.<lang>.safetensors or adapter.<lang>.bin</lang></lang>'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_lang` (`str`) â€” å¿…é¡»æ˜¯ç°æœ‰é€‚é…å™¨æƒé‡çš„è¯­è¨€ IDã€‚é€‚é…å™¨æƒé‡å­˜å‚¨åœ¨æ ¼å¼ adapter.<lang>.safetensors
    æˆ– adapter.<lang>.bin</lang></lang>ã€‚'
- en: '`force_load` (`bool`, defaults to `True`) â€” Whether the weights shall be loaded
    even if `target_lang` matches `self.target_lang`.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_load` (`bool`, é»˜è®¤ä¸º `True`) â€” å³ä½¿ `target_lang` ä¸ `self.target_lang` åŒ¹é…ï¼Œä¹Ÿè¦åŠ è½½æƒé‡ã€‚'
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” Path to a directory in
    which a downloaded pretrained model configuration should be cached if the standard
    cache should not be used.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir` (`Union[str, os.PathLike]`, *å¯é€‰*) â€” ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®åº”è¯¥ç¼“å­˜åœ¨å…¶ä¸­çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ é™¤æ¥æ”¶ä¸å®Œæ•´çš„æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„æ–‡ä»¶ï¼Œå°†å°è¯•æ¢å¤ä¸‹è½½ã€‚'
- en: '`proxies` (`Dict[str, str]`, *optional*) â€” A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies` (`Dict[str, str]`, *å¯é€‰*) â€” ä¸€ä¸ªæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚ï¼Œ`{''http'': ''foo.bar:3128'',
    ''http://hostname'': ''foo.bar:4012''}`ã€‚æ¯ä¸ªè¯·æ±‚éƒ½ä¼šä½¿ç”¨ä»£ç†ã€‚'
- en: '`local_files_only(bool,` *optional*, defaults to `False`) â€” Whether or not
    to only look at local files (i.e., do not try to download the model).'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only(bool,` *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…æŸ¥çœ‹æœ¬åœ°æ–‡ä»¶ï¼ˆå³ä¸å°è¯•ä¸‹è½½æ¨¡å‹ï¼‰ã€‚'
- en: '`token` (`str` or `bool`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`str` æˆ– `bool`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è¿è¡Œ
    `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) â€” The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å®ƒå¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°æˆ–æäº¤ IDï¼Œå› ä¸ºæˆ‘ä»¬åœ¨
    huggingface.co ä¸Šä½¿ç”¨åŸºäº git çš„ç³»ç»Ÿå­˜å‚¨æ¨¡å‹å’Œå…¶ä»–å·¥ä»¶ï¼Œæ‰€ä»¥ `revision` å¯ä»¥æ˜¯ git å…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚'
- en: To test a pull request you made on the Hub, you can pass `revision=â€œrefs/pr/<pr_number>â€œ.</pr_number>
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¦æµ‹è¯•æ‚¨åœ¨ Hub ä¸Šæäº¤çš„æ‹‰å–è¯·æ±‚ï¼Œå¯ä»¥ä¼ é€’ `revision=â€œrefs/pr/<pr_number>â€œã€‚</pr_number>
- en: '`mirror` (`str`, *optional*) â€” Mirror source to accelerate downloads in China.
    If you are from China and have an accessibility problem, you can set this option
    to resolve it. Note that we do not guarantee the timeliness or safety. Please
    refer to the mirror site for more information.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mirror` (`str`, *å¯é€‰*) â€” å°†æºé•œåƒåˆ°ä¸­å›½ä»¥åŠ é€Ÿä¸‹è½½ã€‚å¦‚æœæ‚¨æ¥è‡ªä¸­å›½å¹¶ä¸”æœ‰è®¿é—®é—®é¢˜ï¼Œå¯ä»¥è®¾ç½®æ­¤é€‰é¡¹ä»¥è§£å†³é—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ä¿è¯åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ã€‚è¯·å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚'
- en: Load a language adapter model from a pre-trained adapter model.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒçš„é€‚é…å™¨æ¨¡å‹åŠ è½½è¯­è¨€é€‚é…å™¨æ¨¡å‹ã€‚
- en: Activate the special [â€œoffline-modeâ€](https://huggingface.co/transformers/installation.html#offline-mode)
    to use this method in a firewalled environment.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»ç‰¹æ®Šçš„[â€œç¦»çº¿æ¨¡å¼â€](https://huggingface.co/transformers/installation.html#offline-mode)ä»¥åœ¨é˜²ç«å¢™ç¯å¢ƒä¸­ä½¿ç”¨æ­¤æ–¹æ³•ã€‚
- en: 'Examples:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE41]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Wav2Vec2ForSequenceClassification
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForSequenceClassification
- en: '### `class transformers.Wav2Vec2ForSequenceClassification`'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2021)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2021)'
- en: '[PRE42]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Wav2Vec2 Model with a sequence classification head on top (a linear layer over
    the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªåºåˆ—åˆ†ç±»å¤´çš„ Wav2Vec2 æ¨¡å‹ï¼ˆä¸€ä¸ªçº¿æ€§å±‚åœ¨æ± åŒ–è¾“å‡ºä¸Šæ–¹ï¼‰ç”¨äº SUPERB å…³é”®è¯è¯†åˆ«ç­‰ä»»åŠ¡ã€‚
- en: 'Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski,
    Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec
    2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰çš„ä¿¡æ¯ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2073)'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2073)'
- en: '[PRE43]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Parameters
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—å€¼ï¼Œä¾‹å¦‚é€šè¿‡å£°éŸ³æ–‡ä»¶åº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡ä¸º`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«æ©ç çš„æ ‡è®°ä¸º`1`ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«æ©ç çš„æ ‡è®°ä¸º`0`ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶æ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask
    == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶ï¼Œåº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹è¿˜ä¼šæ ¹æ®`input_values`æ˜¯å¦å¡«å……è€Œäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰- åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œ+
    ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE44]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Wav2Vec2ForAudioFrameClassification
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForAudioFrameClassification
- en: '### `class transformers.Wav2Vec2ForAudioFrameClassification`'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForAudioFrameClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2144)'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2144)'
- en: '[PRE45]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰-
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚ è¯·æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Wav2Vec2 Model with a frame classification head on top for tasks like Speaker
    Diarization.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰é¡¶éƒ¨å¸§åˆ†ç±»å¤´çš„Wav2Vec2æ¨¡å‹ï¼Œç”¨äºSpeaker Diarizationç­‰ä»»åŠ¡ã€‚
- en: 'Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski,
    Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliåœ¨[wav2vec
    2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚
    æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚
    å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2194)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2194)'
- en: '[PRE46]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚
    å€¼å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚ é€šè¿‡å£°éŸ³æ–‡ä»¶åº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚ è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚
    æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚ é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-561
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºâ€œæœªå±è”½â€çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢« `masked` çš„æ ‡è®°ä¸º 0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == True` æ—¶æ‰åº”ä¼ é€’ `attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰
    `config.return_attention_mask == False` çš„æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº”
    `ä¸` ä¼ é€’ `attention_mask` ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……å¹¶åœ¨ä¸ä¼ é€’ `attention_mask`
    çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ® `input_values` æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨
    `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ
    `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification loss.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*optional*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    â€” Classification scores (before SoftMax).'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.num_labels)`)
    â€” åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True`
    æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_attentions=True`
    æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length,
    sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Wav2Vec2ForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE47]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Wav2Vec2ForXVector
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForXVector
- en: '### `class transformers.Wav2Vec2ForXVector`'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2ForXVector`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2305)'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2305)'
- en: '[PRE48]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Wav2Vec2 Model with an XVector feature extraction head on top for tasks like
    Speaker Verification.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: Wav2Vec2æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰XVectorç‰¹å¾æå–å¤´ï¼Œç”¨äºSpeaker Verificationç­‰ä»»åŠ¡ã€‚
- en: 'Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski,
    Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliæå‡ºçš„[wav2vec
    2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2373)'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2373)'
- en: '[PRE49]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œ*ä¾‹å¦‚*é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä»¥è·å¾—`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…çš„æ©ç å€¼ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-598
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-599
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶æ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask
    == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶åº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”ç®€å•åœ°å¡«å……ä¸º0å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹çš„ç»“æœä¹Ÿä¼šå› `input_values`æ˜¯å¦å¡«å……è€Œç•¥æœ‰ä¸åŒã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification loss.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›ï¼‰â€” åˆ†ç±»æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    â€” Classification hidden states before AMSoftmax.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.xvector_output_dim)`)
    â€” AMSoftmax ä¹‹å‰çš„åˆ†ç±»éšè—çŠ¶æ€ã€‚'
- en: '`embeddings` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    â€” Utterance embeddings used for vector similarity-based retrieval.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embeddings` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.xvector_output_dim)`)
    â€” ç”¨äºåŸºäºå‘é‡ç›¸ä¼¼æ€§çš„æ£€ç´¢çš„è¯è¯­åµŒå…¥ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True`
    æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True`
    æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length,
    sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Wav2Vec2ForXVector](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector)
    forward method, overrides the `__call__` special method.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForXVector](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE50]'
  id: totrans-619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Wav2Vec2ForPreTraining
  id: totrans-620
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2ForPreTraining
- en: '### `class transformers.Wav2Vec2ForPreTraining`'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.Wav2Vec2ForPreTraining` ç±»'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1591)'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1591)'
- en: '[PRE51]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Parameters
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: 'Wav2Vec2 Model with a quantizer and `VQ` head on top. Wav2Vec2 was proposed
    in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2 æ¨¡å‹å¸¦æœ‰é‡åŒ–å™¨å’Œé¡¶éƒ¨çš„ `VQ` å¤´ã€‚Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman
    Mohamedã€Michael Auli åœ¨ [wav2vec 2.0: A Framework for Self-Supervised Learning
    of Speech Representations](https://arxiv.org/abs/2006.11477) ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1652)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1652)'
- en: '[PRE52]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—å€¼ï¼Œ*ä¾‹å¦‚*é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä»¥è·å¾—`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-635
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº`æœªå±è”½`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-636
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº`è¢«å±è”½`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask
    == False`çš„æ¨¡å‹ï¼Œæ¯”å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº”`ä¸`ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ®`input_values`æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`mask_time_indices` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºå¯¹æ¯”æŸå¤±ä¸­æ©ç›–æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨*config.proj_codevector_dim*ç©ºé—´ä¸­é¢„æµ‹è¢«æ©ç›–çš„æå–ç‰¹å¾ã€‚'
- en: '`sampled_negative_indices` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_negatives)`, *optional*) â€” Indices indicating which quantized target vectors
    are used as negative sampled vectors in contrastive loss. Required input for pre-training.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampled_negative_indices`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, num_negatives)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    æŒ‡ç¤ºå“ªäº›é‡åŒ–ç›®æ ‡å‘é‡åœ¨å¯¹æ¯”æŸå¤±ä¸­ç”¨ä½œè´Ÿé‡‡æ ·å‘é‡çš„ç´¢å¼•ã€‚é¢„è®­ç»ƒæ‰€éœ€çš„è¾“å…¥ã€‚'
- en: Returns
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor`
    of shape `(1,)`) â€” Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰â€”
    æ€»æŸå¤±ï¼Œä½œä¸ºå¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„æ€»å’Œï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚
    ï¼ˆåˆ†ç±»ï¼‰æŸå¤±ã€‚'
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`çš„`torch.FloatTensor`ï¼‰â€”
    æ¨¡å‹æŠ•å½±åˆ°*config.proj_codevector_dim*çš„éšè—çŠ¶æ€ï¼Œå¯ç”¨äºé¢„æµ‹è¢«å±è”½çš„æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚'
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`çš„`torch.FloatTensor`ï¼‰
    â€” é‡åŒ–æå–çš„ç‰¹å¾å‘é‡æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œè¡¨ç¤ºå¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¯å±‚è¾“å‡ºçš„æ¨¡å‹éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`contrastive_loss` (*optional*, returned when `sample_negative_indices` are
    passed, `torch.FloatTensor` of shape `(1,)`) â€” The contrastive loss (L_m) as stated
    in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_loss`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰
    â€” å¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚'
- en: '`diversity_loss` (*optional*, returned when `sample_negative_indices` are passed,
    `torch.FloatTensor` of shape `(1,)`) â€” The diversity loss (L_d) as stated in the
    [official paper](https://arxiv.org/pdf/2006.11477.pdf) .'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰
    â€” å¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚'
- en: The [Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE53]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: TensorFlowHide TensorFlow content
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: TFWav2Vec2Model
  id: totrans-661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWav2Vec2Model
- en: '### `class transformers.TFWav2Vec2Model`'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWav2Vec2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1509)'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1509)'
- en: '[PRE54]'
  id: totrans-664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare TFWav2Vec2 Model transformer outputing raw hidden-states without any
    specific head on top.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸TFWav2Vec2æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€
    - åªéœ€ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ä»¥ä»»ä½•`model.fit()`æ”¯æŒçš„æ ¼å¼ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Keras`Functional`
    APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š
- en: 'a single Tensor with `input_values` only and nothing else: `model(input_values)`'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä»…åŒ…å«`input_values`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_values, attention_mask])` or `model([input_values,
    attention_mask, token_type_ids])`'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦ä¸åŒçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([input_values, attention_mask])`æˆ–`model([input_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_values": input_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_values": input_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1519)'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1519)'
- en: '[PRE55]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Parameters
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `({0})`) â€” Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str,
    np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`({0})`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)å’Œ[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    â€” Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0,
    1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-686
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº`æœªå±è”½`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-687
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº`è¢«å±è”½`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-688
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    â€” Segment token indices to indicate first and second portions of the inputs. Indices
    are selected in `[0, 1]`:'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0,
    1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-690
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 å¯¹åº”äº*å¥å­A*æ ‡è®°ã€‚
- en: 1 corresponds to a *sentence B* token.
  id: totrans-691
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-692
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*) â€”
    Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0,
    config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-694
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-696
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœª`è¢«å±è”½`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-697
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨æ˜¯`å±è”½`çš„ã€‚
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `({0}, hidden_size)`,
    *optional*) â€” Optionally, instead of passing `input_values` you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `input_values` indices into associated vectors than the modelâ€™s
    internal embedding lookup matrix.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`({0}, hidden_size)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_values`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_values`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `Falseâ€œ) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`Falseâ€œ) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: Returns
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)æˆ–`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    â€” Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`å½¢çŠ¶ä¸º(batch_size, sequence_length, hidden_size)çš„tf.Tensor`)
    â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`hidden_states` (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.FloatTensor)`, *å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE56]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: TFWav2Vec2ForSequenceClassification
  id: totrans-715
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWav2Vec2ForSequenceClassification
- en: '### `class transformers.TFWav2Vec2ForSequenceClassification`'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWav2Vec2ForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1758)'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1758)'
- en: '[PRE57]'
  id: totrans-718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `call`'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1799)'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1799)'
- en: '[PRE58]'
  id: totrans-721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: TFWav2Vec2ForCTC
  id: totrans-722
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWav2Vec2ForCTC
- en: '### `class transformers.TFWav2Vec2ForCTC`'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWav2Vec2ForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1591)'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1591)'
- en: '[PRE59]'
  id: totrans-725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parameters
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: TFWav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC).
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: TFWav2Vec2æ¨¡å‹ï¼Œåœ¨Connectionist Temporal Classification (CTC)é¡¶éƒ¨å…·æœ‰`è¯­è¨€å»ºæ¨¡`å¤´ã€‚
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–è€…
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should â€œjust workâ€ for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€
    - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ï¼ˆå¦‚`fit()`å’Œ`predict()`ï¼‰ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨Keras`Functional`
    APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š
- en: 'a single Tensor with `input_values` only and nothing else: `model(input_values)`'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä»…åŒ…å«`input_values`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_values, attention_mask])` or `model([input_values,
    attention_mask, token_type_ids])`'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºç›¸å¯¹åº”çš„è¾“å…¥å¼ é‡ï¼š`model([input_values, attention_mask])`æˆ–`model([input_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_values": input_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_values": input_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you donâ€™t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼
- en: '#### `call`'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1625)'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1625)'
- en: '[PRE60]'
  id: totrans-741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `({0})`) â€” Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str,
    np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`({0})`ï¼‰â€”è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-744
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)å’Œ[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-745
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    â€” Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0,
    1]`ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-747
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºâ€œæœªè¢«æ©ç â€çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-748
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºâ€œè¢«æ©ç â€çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-749
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*)
    â€” Segment token indices to indicate first and second portions of the inputs. Indices
    are selected in `[0, 1]`:'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-751
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-752
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-753
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*) â€”
    Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0,
    config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-755
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0,
    1]`ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-757
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æ˜¯`not masked`ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-758
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `({0}, hidden_size)`,
    *optional*) â€” Optionally, instead of passing `input_values` you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `input_values` indices into associated vectors than the modelâ€™s
    internal embedding lookup matrix.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`({0}, hidden_size)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_values`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_values`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚'
- en: '`training` (`bool`, *optional*, defaults to `Falseâ€œ) â€” Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆæŸäº›æ¨¡å—ï¼Œå¦‚dropoutæ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Labels for computing the masked language modeling loss. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_values` docstring)
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰
    â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size]`èŒƒå›´å†…ï¼ˆå‚è§`input_values`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0,
    ..., config.vocab_size]`èŒƒå›´å†…çš„æ ‡è®°ã€‚'
- en: Returns
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    æˆ– `tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    and inputs.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) â€” Language modeling loss (for next-token
    prediction).'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(n,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå…¶ä¸­næ˜¯éæ©ç æ ‡ç­¾çš„æ•°é‡ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`tf.Tensor`ï¼‰
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-771
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-773
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFWav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFWav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForCTC)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE61]'
  id: totrans-777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: JAXHide JAX content
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: JAXHide JAXå†…å®¹
- en: FlaxWav2Vec2Model
  id: totrans-779
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWav2Vec2Model
- en: '### `class transformers.FlaxWav2Vec2Model`'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWav2Vec2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1051)'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1051)'
- en: '[PRE62]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰-
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`ï¼ˆ`jax.numpy.dtype`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`jax.numpy.float32`ï¼‰- è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-786
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-787
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„dtypeï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„dtypeã€‚`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-788
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„dtypeï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚
- en: 'The bare Wav2Vec2 Model transformer outputting raw hidden-states without any
    specific head on top. Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸Wav2Vec2æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman
    Mohamedã€Michael Auliåœ¨[wav2vec 2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ä¸­æå‡ºçš„ã€‚
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„Flaxæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒJAXçš„å›ºæœ‰åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
- en: '[PRE63]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Parameters
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) â€” Float
    values of input raw speech waveform. Values can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `jnp.ndarray`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼‰- è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`jnp.ndarray`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Mask to avoid performing convolution and attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-803
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-804
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask) .. warning:: `attention_mask`
    should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-805
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask) .. è­¦å‘Š:: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask
    == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº”`ä¸`ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ®`input_values`æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚'
- en: '`mask_time_indices` (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºå¯¹æ¯”æŸå¤±ä¸­æ©ç æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨*config.proj_codevector_dim*ç©ºé—´ä¸­é¢„æµ‹æ©ç æå–ç‰¹å¾ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)æˆ–è€…`tuple(torch.FloatTensor)`'
- en: A [transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`)
    and inputs.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)æˆ–è€…ä¸€ä¸ª`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–è€…`config.return_dict=False`ï¼‰åŒ…å«ä¸åŒçš„å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ`<class
    'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`ï¼‰å’Œè¾“å…¥ã€‚
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`ï¼‰-
    æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`)
    â€” Sequence of extracted feature vectors of the last convolutional layer of the
    model with `last_conv_dim` being the dimension of the last convolutional layer.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, last_conv_dim)`çš„`jnp.ndarray`ï¼‰-
    æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚çš„æå–ç‰¹å¾å‘é‡åºåˆ—ï¼Œå…¶ä¸­`last_conv_dim`æ˜¯æœ€åä¸€ä¸ªå·ç§¯å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–è€…`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–è€…`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-818
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The `FlaxWav2Vec2PreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2PreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE64]'
  id: totrans-822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: FlaxWav2Vec2ForCTC
  id: totrans-823
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWav2Vec2ForCTC
- en: '### `class transformers.FlaxWav2Vec2ForCTC`'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWav2Vec2ForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1169)'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1169)'
- en: '[PRE65]'
  id: totrans-826
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`ï¼ˆ`jax.numpy.dtype`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `jax.numpy.float32`ï¼‰ â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯ `jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨
    GPU ä¸Šï¼‰å’Œ `jax.numpy.bfloat16`ï¼ˆåœ¨ TPU ä¸Šï¼‰ä¹‹ä¸€ã€‚'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„ `dtype` æ‰§è¡Œã€‚
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸ä¼šå½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-832
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜… [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    å’Œ [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚
- en: 'Wav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC). Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wav2Vec2 æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰â€œè¯­è¨€å»ºæ¨¡â€å¤´éƒ¨ï¼Œç”¨äº Connectionist Temporal Classification (CTC)ã€‚Wav2Vec2
    æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec 2.0:
    A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    ä¸­æå‡ºçš„ã€‚'
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯ Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒ JAX çš„å†…åœ¨ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ (JIT) ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)'
- en: '[PRE66]'
  id: totrans-843
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Parameters
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) â€” Float
    values of input raw speech waveform. Values can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `jnp.ndarray`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `jnp.ndarray`ï¼‰ â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†
    `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º `List[float]` æˆ– `numpy.ndarray` çš„æ•°ç»„ä¸­è·å¾—ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ `input_values`ï¼Œåº”ä½¿ç”¨
    [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸ºç±»å‹ä¸º `jnp.ndarray` çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Mask to avoid performing convolution and attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰
    â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]`ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-847
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 ç”¨äºâ€œæœªæ©ç â€æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-848
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 ç”¨äºâ€œæ©ç â€æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask) .. warning:: `attention_mask`
    should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-849
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask) .. è­¦å‘Š:: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask
    == True` æ—¶æ‰åº”ä¼ é€’ `attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == False`
    çš„æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº”è¯¥
    `ä¸` ä¼ é€’ `attention_mask` ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……å¹¶åœ¨ä¸ä¼ é€’ `attention_mask`
    çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ® `input_values` æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚'
- en: '`mask_time_indices` (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” ç”¨äºå¯¹æ¯”æŸå¤±æ©ç›–æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨ *config.proj_codevector_dim* ç©ºé—´ä¸­é¢„æµ‹æ©ç›–çš„æå–ç‰¹å¾ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`)
    and inputs.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)æˆ–ä¸€ä¸ª
    `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«ä¸åŒå…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ`<class
    'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`ï¼‰å’Œè¾“å…¥ã€‚
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’ `output_hidden_states=True`
    æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True`
    æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `jnp.ndarray`
    å…ƒç»„ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The `FlaxWav2Vec2PreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWav2Vec2PreTrainedModel` çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE67]'
  id: totrans-865
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: FlaxWav2Vec2ForPreTraining
  id: totrans-866
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWav2Vec2ForPreTraining
- en: '### `class transformers.FlaxWav2Vec2ForPreTraining`'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWav2Vec2ForPreTraining`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1318)'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1318)'
- en: '[PRE68]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Parameters
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))
    â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`, *å¯é€‰*, é»˜è®¤ä¸º`jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚'
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è¿™å¯ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`è¿›è¡Œã€‚ '
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-874
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸ä¼šå½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`'
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚
- en: 'Wav2Vec2 Model with a quantizer and `VQ` head on top. Wav2Vec2 was proposed
    in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰é‡åŒ–å™¨å’Œé¡¶éƒ¨`VQ`å¤´çš„Wav2Vec2æ¨¡å‹ã€‚Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael
    Auliæå‡ºçš„[wav2vec 2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ã€‚
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„Flaxæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒJAXçš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1322)'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1322)'
- en: '[PRE69]'
  id: totrans-886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Parameters
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) â€” Float
    values of input raw speech waveform. Values can be obtained by loading a `.flac`
    or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `jnp.ndarray`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—å€¼ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡ä¸º`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`jnp.ndarray`ç±»å‹çš„å¼ é‡ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚'
- en: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Mask to avoid performing convolution and attention on padding token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *å¯é€‰*)
    â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-890
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ã€‚
- en: 0 for tokens that are `masked`.
  id: totrans-891
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask) .. warning:: `attention_mask`
    should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, such as [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h),
    `attention_mask` should `not` be passed to avoid degraded performance when doing
    batched inference. For such models `input_values` should simply be padded with
    0 and passed without `attention_mask`. Be aware that these models also yield slightly
    different results depending on whether `input_values` is padded or not.'
  id: totrans-892
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask) .. è­¦å‘Š:: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask
    == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶ï¼Œåº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ®`input_values`æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚'
- en: '`mask_time_indices` (`jnp.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_indices` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*ï¼‰
    â€” ç”¨äºå¯¹æ¯”æŸå¤±ä¸­æ©ç æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨*config.proj_codevector_dim*ç©ºé—´ä¸­é¢„æµ‹æ©ç æå–ç‰¹å¾ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional`) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*ï¼‰ â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`)
    and inputs.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput)
    æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class
    ''transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config''>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚'
- en: '`loss` (*optional*, returned when model is in train mode, `jnp.ndarray` of
    shape `(1,)`) â€” Total loss as the sum of the contrastive loss (L_m) and the diversity
    loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*optional*ï¼Œåœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`jnp.ndarray`) â€” æ€»æŸå¤±ï¼Œä½œä¸ºå¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„æ€»å’Œï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚
    ï¼ˆåˆ†ç±»ï¼‰æŸå¤±ã€‚'
- en: '`projected_states` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`)
    â€” Hidden-states of the model projected to *config.proj_codevector_dim* that can
    be used to predict the masked projected quantized states.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`ï¼‰
    â€” æ¨¡å‹çš„éšè—çŠ¶æ€æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œå¯ç”¨äºé¢„æµ‹æ©ç çš„æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚'
- en: '`projected_quantized_states` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) â€” Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    config.proj_codevector_dim)`ï¼‰ â€” é‡åŒ–æå–çš„ç‰¹å¾å‘é‡æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œè¡¨ç¤ºå¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-904
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-906
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [FlaxWav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxWav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ æ’­çš„é…æ–¹éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE70]'
  id: totrans-910
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
