- en: XLSR-Wav2Vec2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XLSR-Wav2Vec2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlsr_wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlsr_wav2vec2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlsr_wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlsr_wav2vec2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The XLSR-Wav2Vec2 model was proposed in [Unsupervised Cross-Lingual Representation
    Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau,
    Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: XLSR-Wav2Vec2模型是由Alexis Conneau，Alexei Baevski，Ronan Collobert，Abdelrahman Mohamed，Michael
    Auli在[无监督跨语言表示学习语音识别](https://arxiv.org/abs/2006.13979)中提出的。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*This paper presents XLSR which learns cross-lingual speech representations
    by pretraining a single model from the raw waveform of speech in multiple languages.
    We build on wav2vec 2.0 which is trained by solving a contrastive task over masked
    latent speech representations and jointly learns a quantization of the latents
    shared across languages. The resulting model is fine-tuned on labeled data and
    experiments show that cross-lingual pretraining significantly outperforms monolingual
    pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error
    rate reduction of 72% compared to the best known results. On BABEL, our approach
    improves word error rate by 16% relative compared to a comparable system. Our
    approach enables a single multilingual speech recognition model which is competitive
    to strong individual models. Analysis shows that the latent discrete speech representations
    are shared across languages with increased sharing for related languages. We hope
    to catalyze research in low-resource speech understanding by releasing XLSR-53,
    a large model pretrained in 53 languages.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文介绍了XLSR，通过在多种语言的语音的原始波形上预训练单一模型来学习跨语言语音表示。我们构建在wav2vec 2.0的基础上，该模型通过解决对掩码潜在语音表示的对比任务进行训练，并共同学习跨语言共享的潜在量化。结果模型在标记数据上进行微调，实验表明跨语言预训练明显优于单语言预训练。在CommonVoice基准测试中，XLSR相对音素错误率降低了72%，相对于已知最佳结果。在BABEL上，我们的方法相对于类似系统改善了16%的词错误率。我们的方法实现了一个单一的多语言语音识别模型，与强大的个体模型竞争。分析表明，潜在的离散语音表示在不同语言之间共享，对于相关语言的共享增加。我们希望通过发布在53种语言中预训练的大型模型XLSR-53来促进低资源语音理解的研究。*'
- en: The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码可以在[这里](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: XLSR-Wav2Vec2 is a speech model that accepts a float array corresponding to
    the raw waveform of the speech signal.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLSR-Wav2Vec2是一个语音模型，接受与语音信号的原始波形对应的浮点数组。
- en: XLSR-Wav2Vec2 model was trained using connectionist temporal classification
    (CTC) so the model output has to be decoded using [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLSR-Wav2Vec2模型是使用连接主义时间分类（CTC）进行训练的，因此模型输出必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)进行解码。
- en: XLSR-Wav2Vec2’s architecture is based on the Wav2Vec2 model, so one can refer
    to [Wav2Vec2’s documentation page](wav2vec2).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: XLSR-Wav2Vec2的架构基于Wav2Vec2模型，因此可以参考[Wav2Vec2的文档页面](wav2vec2)。
