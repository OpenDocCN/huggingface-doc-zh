- en: Using LLaMA models with TRL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLaMA模型与TRL
- en: 'Original text: [https://huggingface.co/docs/trl/using_llama_models](https://huggingface.co/docs/trl/using_llama_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/trl/using_llama_models](https://huggingface.co/docs/trl/using_llama_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We’ve begun rolling out examples to use Meta’s LLaMA models in `trl` (see [Meta’s
    LLaMA release](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    for the original LLaMA model).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开始推出示例，以在`trl`中使用Meta的LLaMA模型（请参阅[Meta的LLaMA发布](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)以获取原始LLaMA模型）。
- en: Efficient training strategies
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高效的训练策略
- en: 'Even training the smallest LLaMA model requires an enormous amount of memory.
    Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition
    to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer)
    in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB`
    just to fit in memory and would likely need more when you compute intermediate
    values such as attention scores. So you couldn’t train the model even on a single
    80GB A100 like that. You can use some tricks, like more efficient optimizers of
    half-precision training, to squeeze a bit more into memory, but you’ll run out
    sooner or later.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 即使训练最小的LLaMA模型也需要大量内存。一些快速的数学计算：在bf16中，每个参数使用2个字节（在fp32中为4个字节），另外还使用8个字节，例如在Adam优化器中使用的（有关更多信息，请参阅Transformers中的[性能文档](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer)）。因此，一个7B参数模型将使用`(2+8)*7B=70GB`才能完全适应内存，并且在计算注意力分数等中可能需要更多。因此，即使在单个80GB的A100上也无法训练模型。您可以使用一些技巧，例如更高效的优化器或半精度训练，将更多内容压缩到内存中，但迟早会用完。
- en: Another option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques,
    such as the [`peft`](https://github.com/huggingface/peft) library, which can perform
    low-rank adaptation (LoRA) on a model loaded in 8-bit. For more on `peft` + `trl`,
    see the [docs](https://huggingface.co/docs/trl/sentiment_tuning_peft).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是使用参数高效微调（PEFT）技术，例如[`peft`](https://github.com/huggingface/peft)库，它可以在加载为8位的模型上执行低秩适应（LoRA）。有关`peft`
    + `trl`的更多信息，请参阅[文档](https://huggingface.co/docs/trl/sentiment_tuning_peft)。
- en: Loading the model in 8bit reduces the memory footprint drastically since you
    only need one byte per parameter for the weights (e.g. 7B LlaMa is 7GB in memory).
    Instead of training the original weights directly, LoRA adds small adapter layers
    on top of some specific layers (usually the attention layers); thus, the number
    of trainable parameters is drastically reduced.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型加载为8位会显著减少内存占用，因为权重只需要每个参数一个字节（例如，7B LlaMa在内存中占用7GB）。LoRA不是直接训练原始权重，而是在某些特定层（通常是注意力层）的顶部添加小的适配器层，从而大大减少可训练参数的数量。
- en: In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters
    (depending on the batch size and sequence length) to fit the entire fine-tuning
    setup. This enables fine-tuning larger models (up to 50-60B scale models on a
    NVIDIA A100 80GB) at low cost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个经验法则是为每十亿参数分配约1.2-1.4GB的内存（取决于批处理大小和序列长度），以适应整个微调设置。这可以以较低的成本实现对更大模型的微调（在NVIDIA
    A100 80GB上可达50-60B规模的模型）。
- en: 'Now we can fit very large models into a single GPU, but the training might
    still be very slow. The simplest strategy in this scenario is data parallelism:
    we replicate the same training setup into separate GPUs and pass different batches
    to each GPU. With this, you can parallelize the forward/backward passes of the
    model and scale with the number of GPUs.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将非常大的模型适配到单个GPU中，但训练可能仍然非常缓慢。在这种情况下最简单的策略是数据并行处理：我们将相同的训练设置复制到不同的GPU中，并将不同的批次传递给每个GPU。通过这种方式，您可以并行化模型的前向/后向传递，并随着GPU数量的增加而扩展。
- en: '![chapter10_ddp.png](../Images/f281f3d2b69eb8ef7952621def3f8b06.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![chapter10_ddp.png](../Images/f281f3d2b69eb8ef7952621def3f8b06.png)'
- en: We use either the `transformers.Trainer` or `accelerate`, which both support
    data parallelism without any code changes, by simply passing arguments when calling
    the scripts with `torchrun` or `accelerate launch`. The following runs a training
    script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`transformers.Trainer`或`accelerate`，它们都支持数据并行处理，无需任何代码更改，只需在调用带有`torchrun`或`accelerate
    launch`的脚本时传递参数。以下分别在单台机器上使用8个GPU运行训练脚本，使用`accelerate`和`torchrun`。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Supervised fine-tuning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督微调
- en: Before we start training reward models and tuning our model with RL, it helps
    if the model is already good in the domain we are interested in. In our case,
    we want it to answer questions, while for other use cases, we might want it to
    follow instructions, in which case instruction tuning is a great idea. The easiest
    way to achieve this is by continuing to train the language model with the language
    modeling objective on texts from the domain or task. The [StackExchange dataset](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)
    is enormous (over 10 million instructions), so we can easily train the language
    model on a subset of it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练奖励模型并使用RL调整我们的模型之前，如果模型在我们感兴趣的领域已经很好，将会有所帮助。在我们的情况下，我们希望它能回答问题，而对于其他用例，我们可能希望它能遵循指令，这种情况下指令调整是一个很好的主意。实现这一点的最简单方法是继续使用来自该领域或任务的文本进行语言模型的训练。[StackExchange数据集](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)非常庞大（超过1000万条指令），因此我们可以轻松地在其子集上训练语言模型。
- en: 'There is nothing special about fine-tuning the model before doing RLHF - it’s
    just the causal language modeling objective from pretraining that we apply here.
    To use the data efficiently, we use a technique called packing: instead of having
    one text per sample in the batch and then padding to either the longest text or
    the maximal context of the model, we concatenate a lot of texts with a EOS token
    in between and cut chunks of the context size to fill the batch without any padding.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行RLHF之前对模型进行微调没有什么特别的 - 我们只是应用了这里的预训练因果语言建模目标。为了有效使用数据，我们使用一种称为packing的技术：不是每个样本在批处理中有一个文本，然后填充到最长文本或模型的最大上下文，而是在许多文本之间使用EOS标记连接，并切割上下文大小的块以填充批处理而不进行任何填充。
- en: '![chapter10_preprocessing-clm.png](../Images/b6a16749788aead5596b057282494855.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![chapter10_preprocessing-clm.png](../Images/b6a16749788aead5596b057282494855.png)'
- en: With this approach the training is much more efficient as each token that is
    passed through the model is also trained in contrast to padding tokens which are
    usually masked from the loss. If you don’t have much data and are more concerned
    about occasionally cutting off some tokens that are overflowing the context you
    can also use a classical data loader.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，训练效率更高，因为通过模型传递的每个标记也会得到训练，而不像通常从损失中屏蔽的填充标记。如果您没有太多数据，并且更担心偶尔截断一些溢出上下文的标记，您也可以使用经典数据加载器。
- en: The packing is handled by the `ConstantLengthDataset` and we can then use the
    `Trainer` after loading the model with `peft`. First, we load the model in int8,
    prepare it for training, and then add the LoRA adapters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConstantLengthDataset`处理打包，然后在使用`peft`加载模型后，我们可以使用`Trainer`。首先，我们以int8加载模型，为训练做准备，然后添加LoRA适配器。'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We train the model for a few thousand steps with the causal language modeling
    objective and save the model. Since we will tune the model again with different
    objectives, we merge the adapter weights with the original model weights.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用因果语言建模目标对模型进行几千步的训练，并保存模型。由于我们将再次使用不同目标调整模型，我们将适配器权重与原始模型权重合并。
- en: '**Disclaimer:** due to LLaMA’s license, we release only the adapter weights
    for this and the model checkpoints in the following sections. You can apply for
    access to the base model’s weights by filling out Meta AI’s [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)
    and then converting them to the 🤗 Transformers format by running this [script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py).
    Note that you’ll also need to install 🤗 Transformers from source until the `v4.28`
    is released.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明：**由于LLaMA的许可证，我们仅在以下部分发布适配器权重和模型检查点。您可以通过填写Meta AI的[表格](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)申请访问基础模型的权重，然后通过运行此[脚本](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)将其转换为🤗
    Transformers格式。请注意，您还需要安装🤗 Transformers源代码，直到`v4.28`发布为止。'
- en: Now that we have fine-tuned the model for the task, we are ready to train a
    reward model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为任务对模型进行了微调，我们准备训练一个奖励模型。
- en: Reward modeling and human preferences
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励建模和人类偏好
- en: In principle, we could fine-tune the model using RLHF directly with the human
    annotations. However, this would require us to send some samples to humans for
    rating after each optimization iteration. This is expensive and slow due to the
    number of training samples needed for convergence and the inherent latency of
    human reading and annotator speed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，我们可以直接使用人类注释对模型进行RLHF微调。然而，这将要求我们在每次优化迭代后向人类发送一些样本进行评分。由于需要收敛的训练样本数量以及人类阅读和注释者速度的固有延迟，这是昂贵且缓慢的。
- en: 'A trick that works well instead of direct feedback is training a reward model
    on human annotations collected before the RL loop. The goal of the reward model
    is to imitate how a human would rate a text. There are several possible strategies
    to build a reward model: the most straightforward way would be to predict the
    annotation (e.g. a rating score or a binary value for “good”/”bad”). In practice,
    what works better is to predict the ranking of two examples, where the reward
    model is presented with two candidates `(y_k, y_j)` for a given prompt `x` and
    has to predict which one would be rated higher by a human annotator.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的技巧是在RL循环之前收集人类注释并训练一个奖励模型，而不是直接反馈。奖励模型的目标是模仿人类如何评价文本。构建奖励模型有几种可能的策略：最直接的方式是预测注释（例如评分或“好”/“坏”的二进制值）。实际上，更好的做法是预测两个示例的排名，其中奖励模型呈现给定提示`x`的两个候选人`(y_k,
    y_j)`，并且必须预测哪一个会被人类注释者评为更高。
- en: With the StackExchange dataset, we can infer which of the two answers was preferred
    by the users based on the score. With that information and the loss defined above,
    we can then modify the `transformers.Trainer` by adding a custom loss function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过StackExchange数据集，我们可以根据分数推断用户更喜欢两个答案中的哪一个。有了这些信息和上面定义的损失，我们可以通过添加自定义损失函数修改`transformers.Trainer`。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We utilize a subset of a 100,000 pair of candidates and evaluate on a held-out
    set of 50,000\. With a modest training batch size of 4, we train the Llama model
    using the LoRA `peft` adapter for a single epoch using the Adam optimizer with
    BF16 precision. Our LoRA configuration is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了100,000对候选人的子集，并在一个包含50,000个样本的保留集上进行评估。使用较小的训练批次大小为4，我们使用LoRA `peft`适配器在Adam优化器和BF16精度下对Llama模型进行单个时代的训练。我们的LoRA配置是：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As detailed in the next section, the resulting adapter can be merged into the
    frozen model and saved for further downstream use.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如下一节详细介绍的那样，生成的适配器可以合并到冻结模型中并保存以供进一步下游使用。
- en: Reinforcement Learning from Human Feedback
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从人类反馈中进行强化学习
- en: 'With the fine-tuned language model and the reward model at hand, we are now
    ready to run the RL loop. It follows roughly three steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有了经过微调的语言模型和奖励模型，我们现在准备运行RL循环。大致分为三个步骤：
- en: Generate responses from prompts,
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从提示生成响应，
- en: Rate the responses with the reward model,
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用奖励模型对响应进行评分，
- en: Run a reinforcement learning policy-optimization step with the ratings.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用评级运行强化学习策略优化步骤。
- en: 'The Query and Response prompts are templated as follows before being tokenized
    and passed to the model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在将Query和Response提示标记化并传递给模型之前，这些提示的模板如下：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The same template was used for SFT, RM and RLHF stages. Once more, we utilize
    `peft` for memory-efficient training, which offers an extra advantage in the RLHF
    context. Here, the reference model and policy share the same base, the SFT model,
    which we load in 8-bit and freeze during training. We exclusively optimize the
    policy’s LoRA weights using PPO while sharing the base model’s weights.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SFT、RM和RLHF阶段都使用了相同的模板。再次，我们利用`peft`进行内存高效训练，在RLHF环境中提供了额外的优势。在这里，参考模型和策略共享相同的基础，即SFT模型，在训练过程中以8位加载并冻结。我们专门使用PPO优化策略的LoRA权重，同时共享基础模型的权重。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For the rest of the details and evaluation, please refer to our [blog post on
    StackLLaMA](https://huggingface.co/blog/stackllama).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其余细节和评估，请参考我们在StackLLaMA上的博客帖子。
