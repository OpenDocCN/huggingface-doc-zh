- en: Overview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'Original text: [https://huggingface.co/docs/diffusers/optimization/opt_overview](https://huggingface.co/docs/diffusers/optimization/opt_overview)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/optimization/opt_overview](https://huggingface.co/docs/diffusers/optimization/opt_overview)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Generating high-quality outputs is computationally intensive, especially during
    each iterative step where you go from a noisy output to a less noisy output. One
    of 🤗 Diffuser’s goals is to make this technology widely accessible to everyone,
    which includes enabling fast inference on consumer and specialized hardware.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成高质量的输出在计算上是密集的，特别是在每个迭代步骤中，您从嘈杂的输出转换为较少嘈杂的输出。🤗 Diffuser的目标之一是使这项技术普遍可用，这包括在消费者和专用硬件上实现快速推理。
- en: This section will cover tips and tricks - like half-precision weights and sliced
    attention - for optimizing inference speed and reducing memory-consumption. You’ll
    also learn how to speed up your PyTorch code with [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    or [ONNX Runtime](https://onnxruntime.ai/docs/), and enable memory-efficient attention
    with [xFormers](https://facebookresearch.github.io/xformers/). There are also
    guides for running inference on specific hardware like Apple Silicon, and Intel
    or Habana processors.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将涵盖一些技巧和窍门 - 如半精度权重和切片注意力 - 用于优化推理速度和减少内存消耗。您还将学习如何使用[`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)或[ONNX
    Runtime](https://onnxruntime.ai/docs/)加快PyTorch代码的运行速度，并使用[xFormers](https://facebookresearch.github.io/xformers/)实现内存高效的注意力。还有关于在特定硬件上运行推理的指南，如苹果Silicon，以及英特尔或Habana处理器。
