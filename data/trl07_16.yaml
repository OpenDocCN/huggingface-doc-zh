- en: DPO Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DPO训练师
- en: 'Original text: [https://huggingface.co/docs/trl/dpo_trainer](https://huggingface.co/docs/trl/dpo_trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/trl/dpo_trainer](https://huggingface.co/docs/trl/dpo_trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'TRL supports the DPO Trainer for training language models from preference data,
    as described in the paper [Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) by Rafailov et al.,
    2023\. For a full example have a look at [`examples/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TRL支持DPO训练师，用于从偏好数据中训练语言模型，如Rafailov等人在2023年的论文[直接偏好优化：您的语言模型暗中是一个奖励模型](https://arxiv.org/abs/2305.18290)中所述。有关完整示例，请查看[`examples/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py)。
- en: The first step as always is to train your SFT model, to ensure the data we train
    on is in-distribution for the DPO algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先始终是训练您的SFT模型，以确保我们训练的数据对于DPO算法是分布内的。
- en: Expected dataset format
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 期望的数据集格式
- en: 'The DPO trainer expects a very specific format for the dataset. Since the model
    will be trained to directly optimize the preference of which sentence is the most
    relevant, given two sentences. We provide an example from the [`Anthropic/hh-rlhf`](https://huggingface.co/datasets/Anthropic/hh-rlhf)
    dataset below:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: DPO训练师期望数据集具有非常特定的格式。由于模型将被训练直接优化哪个句子在给定两个句子的情况下最相关的偏好，我们在下面提供了一个来自[`Anthropic/hh-rlhf`](https://huggingface.co/datasets/Anthropic/hh-rlhf)数据集的示例：
- en: '![](../Images/108b7079a2ffc651b11289080d0cdc7d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/108b7079a2ffc651b11289080d0cdc7d.png)'
- en: 'Therefore the final dataset object should contain these 3 entries if you use
    the default `DPODataCollatorWithPadding` data collator. The entries should be
    named:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果使用默认的`DPODataCollatorWithPadding`数据整理器，则最终数据集对象应包含这3个条目。这些条目应命名为：
- en: '`prompt`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`'
- en: '`chosen`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chosen`'
- en: '`rejected`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rejected`'
- en: 'for example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where the `prompt` contains the context inputs, `chosen` contains the corresponding
    chosen responses and `rejected` contains the corresponding negative (rejected)
    responses. As can be seen a prompt can have multiple responses and this is reflected
    in the entries being repeated in the dictionary’s value arrays.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`prompt`包含上下文输入，`chosen`包含相应的选择响应，`rejected`包含相应的负面（被拒绝的）响应。可以看到一个提示可以有多个响应，这在字典值数组中的条目重复中得到体现。
- en: Expected model format
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 期望的模型格式
- en: The DPO trainer expects a model of `AutoModelForCausalLM`, compared to PPO that
    expects `AutoModelForCausalLMWithValueHead` for the value function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DPO训练师期望一个`AutoModelForCausalLM`模型，而PPO则期望`AutoModelForCausalLMWithValueHead`用于值函数。
- en: Using the DPOTrainer
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DPOTrainer
- en: For a detailed example have a look at the `examples/scripts/dpo.py` script.
    At a high level we need to initialize the `DPOTrainer` with a `model` we wish
    to train, a reference `ref_model` which we will use to calculate the implicit
    rewards of the preferred and rejected response, the `beta` refers to the hyperparameter
    of the implicit reward, and the dataset contains the 3 entries listed above. Note
    that the `model` and `ref_model` need to have the same architecture (ie decoder
    only or encoder-decoder).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细示例，请查看`examples/scripts/dpo.py`脚本。在高层次上，我们需要使用要训练的`model`初始化`DPOTrainer`，使用`ref_model`来计算首选和被拒绝响应的隐式奖励，`beta`是隐式奖励的超参数，数据集包含上述3个条目。请注意，`model`和`ref_model`需要具有相同的架构（即仅解码器或编码器-解码器）。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After this one can then call:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 之后可以调用：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the `beta` is the temperature parameter for the DPO loss, typically
    something in the range of `0.1` to `0.5`. We ignore the reference model as `beta`
    -> 0.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`beta`是DPO损失的温度参数，通常在`0.1`到`0.5`的范围内。我们忽略参考模型，因为`beta` -> 0。
- en: Loss functions
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: Given the preference data, we can fit a binary classifier according to the Bradley-Terry
    model and in fact the DPO authors propose the sigmoid loss on the normalized likelihood
    via the `logsigmoid` to fit a logistic regression.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据偏好数据，我们可以根据Bradley-Terry模型拟合二元分类器，事实上，DPO作者提出使用通过`logsigmoid`对归一化似然进行sigmoid损失拟合逻辑回归。
- en: The [RSO](https://arxiv.org/abs/2309.06657) authors propose to use a hinge loss
    on the normalized likelihood from the [SLiC](https://arxiv.org/abs/2305.10425)
    paper. The `DPOTrainer` can be switched to this loss via the `loss_type="hinge"`
    argument and the `beta` in this case is the reciprocal of the margin.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[RSO](https://arxiv.org/abs/2309.06657)的作者建议使用来自[SLiC](https://arxiv.org/abs/2305.10425)论文的归一化似然的铰链损失。`DPOTrainer`可以通过`loss_type="hinge"`参数切换到此损失，此时`beta`是边际的倒数。'
- en: The [IPO](https://arxiv.org/abs/2310.12036) authors provide a deeper theoretical
    understanding of the DPO algorithms and identify an issue with overfitting and
    propose an alternative loss which can be used via the `loss_type="ipo"` argument
    to the trainer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[IPO](https://arxiv.org/abs/2310.12036)的作者提供了对DPO算法的更深入的理论理解，并确定了过拟合的问题，并提出了一种替代损失，可以通过`loss_type="ipo"`参数传递给训练师。'
- en: The [cDPO](https://ericmitchell.ai/cdpo.pdf) is a tweak on the DPO loss where
    we assume that the preference labels are noisy with some probability that can
    be passed to the `DPOTrainer` via `label_smoothing` argument (between 0 and 0.5)
    and then a conservative DPO loss is used. Use the `loss_type="cdpo"` argument
    to the trainer to use it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[cDPO](https://ericmitchell.ai/cdpo.pdf)是对DPO损失的调整，其中我们假设偏好标签存在一定概率的噪声，可以通过`label_smoothing`参数（介于0和0.5之间）传递给`DPOTrainer`，然后使用保守的DPO损失。使用`loss_type="cdpo"`参数来使用它。'
- en: The [KTO](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)
    loss is derived to directly maximize the utility of LLM generations instead of
    the log-likelihood of preferences. Thus the dataset are not necessarily preferences
    but rather desirable vs undesirable completions. For paired preference data as
    required by the `DPOTrainer`, use the `loss_type="kto_pair"` argument to the trainer
    to utilize this loss, while for the more general case of desired and undesirable
    data, use the as of yet unimplemented `KTOTrainer`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[KTO](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)损失被推导出来，直接最大化LLM生成的效用，而不是偏好的对数似然。因此，数据集不一定是偏好，而是理想与不理想的完成。对于`DPOTrainer`所需的成对偏好数据，请使用`loss_type="kto_pair"`参数来利用这种损失，而对于期望和不期望数据的更一般情况，请使用尚未实现的`KTOTrainer`。'
- en: Logging
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: 'While training and evaluating we record the following reward metrics:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和评估过程中，我们记录以下奖励指标：
- en: '`rewards/chosen`: the mean difference between the log probabilities of the
    policy model and the reference model for the chosen responses scaled by beta'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/chosen`：策略模型和参考模型对于所选响应的对数概率之间的平均差异，按beta缩放'
- en: '`rewards/rejected`: the mean difference between the log probabilities of the
    policy model and the reference model for the rejected responses scaled by beta'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/rejected`：策略模型和参考模型对于被拒绝响应的对数概率之间的平均差异，按beta缩放'
- en: '`rewards/accuracies`: mean of how often the chosen rewards are > than the corresponding
    rejected rewards'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/accuracies`：所选奖励比相应被拒绝奖励高的平均值'
- en: '`rewards/margins`: the mean difference between the chosen and corresponding
    rejected rewards'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/margins`：所选奖励和相应被拒绝奖励之间的平均差异'
- en: Accelerate DPO fine-tuning using unsloth
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用unsloth加速DPO微调
- en: 'You can further accelerate QLoRA / LoRA (2x faster, 60% less memory) using
    the [`unsloth`](https://github.com/unslothai/unsloth) library that is fully compatible
    with `SFTTrainer`. Currently `unsloth` supports only Llama (Yi, TinyLlama, Qwen,
    Deepseek etc) and Mistral architectures. Some benchmarks for DPO listed below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用与`SFTTrainer`完全兼容的[`unsloth`](https://github.com/unslothai/unsloth)库来进一步加速QLoRA
    / LoRA（速度提高2倍，内存减少60%）。目前，`unsloth`仅支持Llama（Yi，TinyLlama，Qwen，Deepseek等）和Mistral架构。以下是DPO的一些基准测试：
- en: '| GPU | Model | Dataset | 🤗 | 🤗 + Flash Attention 2 | 🦥 Unsloth | 🦥 VRAM saved
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| GPU | Model | Dataset | 🤗 | 🤗 + Flash Attention 2 | 🦥 Unsloth | 🦥 VRAM saved
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| A100 40G | Zephyr 7b | Ultra Chat | 1x | 1.24x | **1.88x** | -11.6% |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| A100 40G | Zephyr 7b | Ultra Chat | 1x | 1.24x | **1.88x** | -11.6% |'
- en: '| Tesla T4 | Zephyr 7b | Ultra Chat | 1x | 1.09x | **1.55x** | -18.6% |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Tesla T4 | Zephyr 7b | Ultra Chat | 1x | 1.09x | **1.55x** | -18.6% |'
- en: 'First install `unsloth` according to the [official documentation](https://github.com/unslothai/unsloth).
    Once installed, you can incorporate unsloth into your workflow in a very simple
    manner; instead of loading `AutoModelForCausalLM`, you just need to load a `FastLanguageModel`
    as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[官方文档](https://github.com/unslothai/unsloth)安装`unsloth`。安装完成后，您可以以非常简单的方式将unsloth整合到您的工作流程中；您只需要加载`FastLanguageModel`，而不是加载`AutoModelForCausalLM`，如下所示：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The saved model is fully compatible with Hugging Face’s transformers library.
    Learn more about unsloth in their [official repository](https://github.com/unslothai/unsloth).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的模型与Hugging Face的transformers库完全兼容。在他们的[官方存储库](https://github.com/unslothai/unsloth)中了解更多关于unsloth的信息。
- en: Reference model considerations with PEFT
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考模型在PEFT中的考虑
- en: You have three main options (plus several variants) for how the reference model
    works when using PEFT, assuming the model that you would like to further enhance
    with DPO was tuned using (Q)LoRA.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PEFT时，您有三个主要选项（以及几个变体）来确定参考模型的工作方式，假设您希望使用DPO进一步增强的模型是使用（Q）LoRA进行调整的。
- en: Simply create two instances of the model, each loading your adapter - works
    fine but is very inefficient.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单地创建两个模型实例，每个加载您的适配器 - 运行良好但效率很低。
- en: Merge the adapter into the base model, create another adapter on top, then leave
    the `model_ref` param null, in which case DPOTrainer will unload the adapter for
    reference inference - efficient, but has potential downsides discussed below.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将适配器合并到基础模型中，顶部创建另一个适配器，然后将`model_ref`参数保留为空，这样DPOTrainer将卸载用于参考推理的适配器 - 高效，但存在下面讨论的潜在缺点。
- en: Load the adapter twice with different names, then use `set_adapter` during training
    to swap between the adapter being DPO’d and the reference adapter - slightly less
    efficient compared to 2 (~adapter size VRAM overhead), but avoids the pitfalls.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两次加载适配器，使用不同的名称，然后在训练过程中使用`set_adapter`来在DPO适配器和参考适配器之间进行切换 - 与2相比稍微不那么高效（适配器大小VRAM开销），但避免了缺点。
- en: Downsides to merging QLoRA before DPO (approach 2)
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在DPO之前合并QLoRA的缺点（方法2）
- en: As suggested by [Tim Dettmers](https://twitter.com/Tim_Dettmers/status/1694654191325573456),
    the best option for merging QLoRA adapters is to first quantize the base model,
    merge the adapter, then convert back to bf16\. Something similar to [this script](https://github.com/jondurbin/qlora/blob/main/qmerge.py)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如[Tim Dettmers](https://twitter.com/Tim_Dettmers/status/1694654191325573456)建议的，合并QLoRA适配器的最佳方法是首先对基础模型进行量化，然后合并适配器，然后转换回bf16。类似于[此脚本](https://github.com/jondurbin/qlora/blob/main/qmerge.py)
- en: You can also just merge the adapters the standard way without quantizing the
    base model, but then you have 1-2% reduced performance (and evidently, more issues
    with empty responses).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以只是以标准方式合并适配器，而不对基础模型进行量化，但这样会导致性能降低1-2%（显然，还会出现更多空响应的问题）。
- en: If you use the recommended approach, which quantizes the model, you’re now in
    a situation where to use QLoRA for DPO, you will need to re-quantize the merged
    model again or use an unquantized merge with lower overall performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用推荐的方法，即对模型进行量化，那么现在您处于这样一种情况：要使用QLoRA进行DPO，您将需要再次对合并的模型进行量化，或者使用性能较低的未量化合并。
- en: Using option 3 - load the adapter twice
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用选项3 - 两次加载适配器
- en: To avoid the downsides with option 2, at the expense of slightly increased VRAM,
    you can load your fine-tuned adapter into the model twice, with different names,
    and set the model/ref adapter names in DPOTrainer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免选项 2 的缺点，可以将微调的适配器加载到模型中两次，使用不同的名称，并在 DPOTrainer 中设置模型/参考适配器的名称，以略微增加 VRAM。
- en: 'For example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: DPOTrainer
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPOTrainer
- en: '### `class trl.DPOTrainer`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.DPOTrainer`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)'
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`transformers.PreTrainedModel`) — The model to train, preferably an
    `AutoModelForSequenceClassification`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`transformers.PreTrainedModel`) — 要训练的模型，最好是 `AutoModelForSequenceClassification`。'
- en: '`ref_model` (`PreTrainedModelWrapper`) — Hugging Face transformer model with
    a casual language modelling head. Used for implicit reward computation and loss.
    If no reference model is provided, the trainer will create a reference model with
    the same architecture as the model to be optimized.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ref_model` (`PreTrainedModelWrapper`) — Hugging Face 转换器模型，带有一个随意语言建模头。用于隐式奖励计算和损失。如果没有提供参考模型，训练器将创建一个与要优化的模型具有相同架构的参考模型。'
- en: '`beta` (`float`, defaults to 0.1) — The beta factor in DPO loss. Higher beta
    means less divergence from the initial policy. For the IPO loss, beta is the regularization
    parameter denoted by tau in the paper.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta` (`float`，默认为 0.1) — DPO 损失中的 beta 因子。较高的 beta 意味着与初始策略的发散较小。对于 IPO 损失，beta
    是论文中表示的正则化参数 tau。'
- en: '`label_smoothing` (`float`, defaults to 0) — The robust DPO label smoothing
    parameter from the [cDPO](https://ericmitchell.ai/cdpo.pdf) report that should
    be between 0 and 0.5.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_smoothing` (`float`，默认为 0) — 来自 [cDPO](https://ericmitchell.ai/cdpo.pdf)
    报告的鲁棒 DPO 标签平滑参数，应该在 0 和 0.5 之间。'
- en: '`loss_type` (`str`, defaults to `"sigmoid"`) — The type of DPO loss to use.
    Either `"sigmoid"` the default DPO loss,`"hinge"` loss from [SLiC](https://arxiv.org/abs/2305.10425)
    paper, `"ipo"` from [IPO](https://arxiv.org/abs/2310.12036) paper, or `"kto"`
    from the HALOs [report](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_type` (`str`，默认为 `"sigmoid"`) — 要使用的 DPO 损失类型。可以是 `"sigmoid"` 默认的 DPO
    损失，来自 [SLiC](https://arxiv.org/abs/2305.10425) 论文的 `"hinge"` 损失，来自 [IPO](https://arxiv.org/abs/2310.12036)
    论文的 `"ipo"`，或来自 HALOs [报告](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)
    的 `"kto"`。'
- en: '`args` (`transformers.TrainingArguments`) — The arguments to use for training.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`transformers.TrainingArguments`) — 用于训练的参数。'
- en: '`data_collator` (`transformers.DataCollator`) — The data collator to use for
    training. If None is specified, the default data collator (`DPODataCollatorWithPadding`)
    will be used which will pad the sequences to the maximum length of the sequences
    in the batch, given a dataset of paired sequences.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_collator` (`transformers.DataCollator`) — 用于训练的数据整理器。如果未指定为 None，则将使用默认数据整理器（`DPODataCollatorWithPadding`），该整理器将将序列填充到批次中序列的最大长度，给定一组成对序列的数据集。'
- en: '`label_pad_token_id` (`int`, defaults to `-100`) — The label pad token id.
    This argument is required if you want to use the default data collator.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_pad_token_id` (`int`，默认为 `-100`) — 标签填充标记 id。如果要使用默认数据整理器，则需要此参数。'
- en: '`padding_value` (`int`, defaults to `0`) — The padding value if it is different
    to the tokenizer’s pad_token_id.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value` (`int`，默认为 `0`) — 如果与标记器的 pad_token_id 不同，则为填充值。'
- en: '`truncation_mode` (`str`, defaults to `keep_end`) — The truncation mode to
    use, either `keep_end` or `keep_start`. This argument is required if you want
    to use the default data collator.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_mode` (`str`，默认为 `keep_end`) — 要使用的截断模式，可以是 `keep_end` 或 `keep_start`。如果要使用默认数据整理器，则需要此参数。'
- en: '`train_dataset` (`datasets.Dataset`) — The dataset to use for training.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset` (`datasets.Dataset`) — 用于训练的数据集。'
- en: '`eval_dataset` (`datasets.Dataset`) — The dataset to use for evaluation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (`datasets.Dataset`) — 用于评估的数据集。'
- en: '`tokenizer` (`transformers.PreTrainedTokenizerBase`) — The tokenizer to use
    for training. This argument is required if you want to use the default data collator.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`transformers.PreTrainedTokenizerBase`) — 用于训练的标记器。如果要使用默认数据整理器，则需要此参数。'
- en: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) — The model initializer
    to use for training. If None is specified, the default model initializer will
    be used.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) — 用于训练的模型初始化器。如果未指定为
    None，则将使用默认模型初始化器。'
- en: '`callbacks` (`List[transformers.TrainerCallback]`) — The callbacks to use for
    training.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callbacks` (`List[transformers.TrainerCallback]`) — 用于训练的回调函数。'
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — The optimizer and scheduler to use for training.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — 用于训练的优化器和调度器。'
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    — The function to use to preprocess the logits before computing the metrics.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    — 在计算指标之前用于预处理对数的函数。'
- en: '`max_length` (`int`, defaults to `None`) — The maximum length of the sequences
    in the batch. This argument is required if you want to use the default data collator.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`，默认为 `None`) — 批次中序列的最大长度。如果要使用默认数据整理器，则需要此参数。'
- en: '`max_prompt_length` (`int`, defaults to `None`) — The maximum length of the
    prompt. This argument is required if you want to use the default data collator.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_prompt_length` (`int`，默认为 `None`) — 提示的最大长度。如果要使用默认数据整理器，则需要此参数。'
- en: '`max_target_length` (`int`, defaults to `None`) — The maximum length of the
    target. This argument is required if you want to use the default data collator
    and your model is an encoder-decoder.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_target_length` (`int`，默认为 `None`) — 目标的最大长度。如果要使用默认数据整理器并且您的模型是编码器-解码器，则需要此参数。'
- en: '`peft_config` (`Dict`, defaults to `None`) — The PEFT configuration to use
    for training. If you pass a PEFT configuration, the model will be wrapped in a
    PEFT model.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config` (`Dict`，默认为 `None`) — 用于训练的 PEFT 配置。如果传递 PEFT 配置，模型将被包装在 PEFT
    模型中。'
- en: '`is_encoder_decoder` (`Optional[bool]`, `optional`, defaults to `None`) — If
    no model is provided, we need to know if the model_init returns an encoder-decoder.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_encoder_decoder` (`Optional[bool]`, `可选`, 默认为 `None`) — 如果没有提供模型，我们需要知道模型初始化是否返回编码器-解码器。'
- en: '`disable_dropout` (`bool`, defaults to `True`) — Whether or not to disable
    dropouts in `model` and `ref_model`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_dropout` (`bool`, 默认为 `True`) — 是否在 `model` 和 `ref_model` 中禁用 dropout。'
- en: '`generate_during_eval` (`bool`, defaults to `False`) — Whether to sample and
    log generations during evaluation step.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_during_eval` (`bool`, 默认为 `False`) — 是否在评估步骤中对生成进行采样和记录。'
- en: '`compute_metrics` (`Callable[[EvalPrediction], Dict]`, *optional*) — The function
    to use to compute the metrics. Must take a `EvalPrediction` and return a dictionary
    string to metric values.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_metrics` (`Callable[[EvalPrediction], Dict]`, *可选*) — 用于计算指标的函数。必须接受一个
    `EvalPrediction` 并返回一个字符串到指标值的字典。'
- en: '`precompute_ref_log_probs` (`bool`, defaults to `False`) — Flag to precompute
    reference model log probabilities and evaluation datasets. This is useful if you
    want to train without the reference model and reduce the total GPU memory needed.
    model_init_kwargs — (`Optional[Dict]`, *optional*): Dict of Optional kwargs to
    pass when instantiating the model from a string ref_model_init_kwargs — (`Optional[Dict]`,
    *optional*): Dict of Optional kwargs to pass when instantiating the ref model
    from a string'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precompute_ref_log_probs` (`bool`, 默认为 `False`) — 用于预计算参考模型对数概率和评估数据集的标志。如果要在没有参考模型的情况下训练并减少所需的总
    GPU 内存，则这很有用。model_init_kwargs — (`Optional[Dict]`, *可选*): 传递给从字符串实例化模型时的可选 kwargs
    字典 ref_model_init_kwargs — (`Optional[Dict]`, *可选*): 传递给从字符串实例化参考模型时的可选 kwargs
    字典'
- en: '`model_adapter_name` (`str`, defaults to `None`) — Name of the train target
    PEFT adapter, when using LoRA with multiple adapters.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_adapter_name` (`str`, 默认为 `None`) — 使用 LoRA 时的训练目标 PEFT 适配器的名称，当有多个适配器时。'
- en: '`ref_adapter_name` (`str`, defaults to `None`) — Name of the reference PEFT
    adapter, when using LoRA with multiple adapters.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ref_adapter_name` (`str`, 默认为 `None`) — 使用 LoRA 时参考 PEFT 适配器的名称，当有多个适配器时。'
- en: Initialize DPOTrainer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 DPOTrainer。
- en: '#### `build_tokenized_answer`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_tokenized_answer`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)'
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`. It does ensure
    `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`. Reference: [https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 分词器确实满足 `enc(a + b) = enc(a) + enc(b)`。它确保 `enc(a + b) = enc(a) + enc(a
    + b)[len(enc(a)):]`。参考：[https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)
- en: '#### `compute_reference_log_probs`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_reference_log_probs`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)'
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Computes log probabilities of the reference model for a single padded batch
    of a DPO specific dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 DPO 特定数据集的单个填充批次的参考模型的对数概率。
- en: '#### `concatenated_forward`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `concatenated_forward`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)'
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Run the given model on the given batch of inputs, concatenating the chosen and
    rejected inputs together.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的输入批次上运行给定模型，将所选和被拒绝的输入连接在一起。
- en: We do this to avoid doing two forward passes, because it’s faster for FSDP.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是为了避免进行两次前向传递，因为对于 FSDP 来说更快。
- en: '#### `concatenated_inputs`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `concatenated_inputs`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)'
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Concatenate the chosen and rejected inputs into a single tensor.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将所选和被拒绝的输入连接成一个张量。
- en: '#### `dpo_loss`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `dpo_loss`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)'
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Returns
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A tuple of three tensors
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含三个张量的元组
- en: (losses, chosen_rewards, rejected_rewards). The losses tensor contains the DPO
    loss for each example in the batch. The chosen_rewards and rejected_rewards tensors
    contain the rewards for the chosen and rejected responses, respectively.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (损失，所选奖励，被拒绝奖励)。损失张量包含批次中每个示例的 DPO 损失。所选奖励和被拒绝奖励张量分别包含所选和被拒绝响应的奖励。
- en: Compute the DPO loss for a batch of policy and reference model log probabilities.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为一批策略和参考模型对数概率计算 DPO 损失。
- en: '#### `evaluation_loop`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluation_loop`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)'
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Overriding built-in evaluation loop to store metrics for each batch. Prediction/evaluation
    loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖内置评估循环以存储每个批次的指标。预测/评估循环，由 `Trainer.evaluate()` 和 `Trainer.predict()` 共享。
- en: Works both with or without labels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否有标签都可以使用。
- en: '#### `get_batch_logps`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_batch_logps`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)'
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Compute the log probabilities of the given labels under the given logits.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 计算给定标签在给定对数下的对数概率。
- en: '#### `get_batch_loss_metrics`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_batch_loss_metrics`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)'
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Compute the DPO loss and other metrics for the given batch of inputs for train
    or test.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为给定的输入批次计算 DPO 损失和其他指标，用于训练或测试。
- en: '#### `get_batch_samples`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_batch_samples`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)'
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Generate samples from the model and reference model for the given batch of inputs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为给定的输入批次从模型和参考模型生成样本。
- en: '#### `get_eval_dataloader`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_eval_dataloader`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)'
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`eval_dataset` (`torch.utils.data.Dataset`, *optional*) — If provided, will
    override `self.eval_dataset`. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement `__len__`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset`（`torch.utils.data.Dataset`，*可选*）- 如果提供，将覆盖`self.eval_dataset`。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将被自动删除。它必须实现`__len__`。'
- en: Returns the evaluation `~torch.utils.data.DataLoader`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 返回评估`~torch.utils.data.DataLoader`。
- en: Subclass of transformers.src.transformers.trainer.get_eval_dataloader to precompute
    `ref_log_probs`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: transformers.src.transformers.trainer.get_eval_dataloader的子类，用于预计算`ref_log_probs`。
- en: '#### `get_train_dataloader`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_train_dataloader`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)'
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Returns the training `~torch.utils.data.DataLoader`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 返回训练`~torch.utils.data.DataLoader`。
- en: Subclass of transformers.src.transformers.trainer.get_train_dataloader to precompute
    `ref_log_probs`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: transformers.src.transformers.trainer.get_train_dataloader的子类，用于预计算`ref_log_probs`。
- en: '#### `log`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `log`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)'
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`logs` (`Dict[str, float]`) — The values to log.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logs`（`Dict[str, float]`）- 要记录的值。'
- en: Log `logs` on the various objects watching training, including stored metrics.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察训练的各种对象上记录`logs`，包括存储的指标。
- en: '#### `null_ref_context`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `null_ref_context`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)'
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Context manager for handling null reference model (that is, peft adapter manipulation).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用于处理空引用模型（即peft适配器操作）的上下文管理器。
- en: '#### `tokenize_row`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tokenize_row`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)'
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Tokenize a single row from a DPO specific dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对来自DPO特定数据集的单行进行标记化。
- en: At this stage, we don’t convert to PyTorch tensors yet; we just handle the truncation
    in case the prompt + chosen or prompt + rejected responses is/are too long. First
    we truncate the prompt; if we’re still too long, we truncate the chosen/rejected.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们还没有将其转换为PyTorch张量；我们只是处理截断，以防提示+所选或提示+拒绝响应太长。首先截断提示；如果仍然太长，我们截断所选/拒绝。
- en: We also create the labels for the chosen/rejected responses, which are of length
    equal to the sum of the length of the prompt and the chosen/rejected response,
    with label_pad_token_id for the prompt tokens.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为所选/拒绝的响应创建标签，其长度等于提示和所选/拒绝响应的长度之和，对于提示标记使用label_pad_token_id。
