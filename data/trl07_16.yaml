- en: DPO Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DPOè®­ç»ƒå¸ˆ
- en: 'Original text: [https://huggingface.co/docs/trl/dpo_trainer](https://huggingface.co/docs/trl/dpo_trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/trl/dpo_trainer](https://huggingface.co/docs/trl/dpo_trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'TRL supports the DPO Trainer for training language models from preference data,
    as described in the paper [Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) by Rafailov et al.,
    2023\. For a full example have a look at [`examples/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TRLæ”¯æŒDPOè®­ç»ƒå¸ˆï¼Œç”¨äºä»åå¥½æ•°æ®ä¸­è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¦‚Rafailovç­‰äººåœ¨2023å¹´çš„è®ºæ–‡[ç›´æ¥åå¥½ä¼˜åŒ–ï¼šæ‚¨çš„è¯­è¨€æ¨¡å‹æš—ä¸­æ˜¯ä¸€ä¸ªå¥–åŠ±æ¨¡å‹](https://arxiv.org/abs/2305.18290)ä¸­æ‰€è¿°ã€‚æœ‰å…³å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[`examples/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py)ã€‚
- en: The first step as always is to train your SFT model, to ensure the data we train
    on is in-distribution for the DPO algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆå§‹ç»ˆæ˜¯è®­ç»ƒæ‚¨çš„SFTæ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬è®­ç»ƒçš„æ•°æ®å¯¹äºDPOç®—æ³•æ˜¯åˆ†å¸ƒå†…çš„ã€‚
- en: Expected dataset format
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœŸæœ›çš„æ•°æ®é›†æ ¼å¼
- en: 'The DPO trainer expects a very specific format for the dataset. Since the model
    will be trained to directly optimize the preference of which sentence is the most
    relevant, given two sentences. We provide an example from the [`Anthropic/hh-rlhf`](https://huggingface.co/datasets/Anthropic/hh-rlhf)
    dataset below:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: DPOè®­ç»ƒå¸ˆæœŸæœ›æ•°æ®é›†å…·æœ‰éå¸¸ç‰¹å®šçš„æ ¼å¼ã€‚ç”±äºæ¨¡å‹å°†è¢«è®­ç»ƒç›´æ¥ä¼˜åŒ–å“ªä¸ªå¥å­åœ¨ç»™å®šä¸¤ä¸ªå¥å­çš„æƒ…å†µä¸‹æœ€ç›¸å…³çš„åå¥½ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢æä¾›äº†ä¸€ä¸ªæ¥è‡ª[`Anthropic/hh-rlhf`](https://huggingface.co/datasets/Anthropic/hh-rlhf)æ•°æ®é›†çš„ç¤ºä¾‹ï¼š
- en: '![](../Images/108b7079a2ffc651b11289080d0cdc7d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/108b7079a2ffc651b11289080d0cdc7d.png)'
- en: 'Therefore the final dataset object should contain these 3 entries if you use
    the default `DPODataCollatorWithPadding` data collator. The entries should be
    named:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœä½¿ç”¨é»˜è®¤çš„`DPODataCollatorWithPadding`æ•°æ®æ•´ç†å™¨ï¼Œåˆ™æœ€ç»ˆæ•°æ®é›†å¯¹è±¡åº”åŒ…å«è¿™3ä¸ªæ¡ç›®ã€‚è¿™äº›æ¡ç›®åº”å‘½åä¸ºï¼š
- en: '`prompt`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`'
- en: '`chosen`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chosen`'
- en: '`rejected`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rejected`'
- en: 'for example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼š
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where the `prompt` contains the context inputs, `chosen` contains the corresponding
    chosen responses and `rejected` contains the corresponding negative (rejected)
    responses. As can be seen a prompt can have multiple responses and this is reflected
    in the entries being repeated in the dictionaryâ€™s value arrays.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­`prompt`åŒ…å«ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œ`chosen`åŒ…å«ç›¸åº”çš„é€‰æ‹©å“åº”ï¼Œ`rejected`åŒ…å«ç›¸åº”çš„è´Ÿé¢ï¼ˆè¢«æ‹’ç»çš„ï¼‰å“åº”ã€‚å¯ä»¥çœ‹åˆ°ä¸€ä¸ªæç¤ºå¯ä»¥æœ‰å¤šä¸ªå“åº”ï¼Œè¿™åœ¨å­—å…¸å€¼æ•°ç»„ä¸­çš„æ¡ç›®é‡å¤ä¸­å¾—åˆ°ä½“ç°ã€‚
- en: Expected model format
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœŸæœ›çš„æ¨¡å‹æ ¼å¼
- en: The DPO trainer expects a model of `AutoModelForCausalLM`, compared to PPO that
    expects `AutoModelForCausalLMWithValueHead` for the value function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DPOè®­ç»ƒå¸ˆæœŸæœ›ä¸€ä¸ª`AutoModelForCausalLM`æ¨¡å‹ï¼Œè€ŒPPOåˆ™æœŸæœ›`AutoModelForCausalLMWithValueHead`ç”¨äºå€¼å‡½æ•°ã€‚
- en: Using the DPOTrainer
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DPOTrainer
- en: For a detailed example have a look at the `examples/scripts/dpo.py` script.
    At a high level we need to initialize the `DPOTrainer` with a `model` we wish
    to train, a reference `ref_model` which we will use to calculate the implicit
    rewards of the preferred and rejected response, the `beta` refers to the hyperparameter
    of the implicit reward, and the dataset contains the 3 entries listed above. Note
    that the `model` and `ref_model` need to have the same architecture (ie decoder
    only or encoder-decoder).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹`examples/scripts/dpo.py`è„šæœ¬ã€‚åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è¦è®­ç»ƒçš„`model`åˆå§‹åŒ–`DPOTrainer`ï¼Œä½¿ç”¨`ref_model`æ¥è®¡ç®—é¦–é€‰å’Œè¢«æ‹’ç»å“åº”çš„éšå¼å¥–åŠ±ï¼Œ`beta`æ˜¯éšå¼å¥–åŠ±çš„è¶…å‚æ•°ï¼Œæ•°æ®é›†åŒ…å«ä¸Šè¿°3ä¸ªæ¡ç›®ã€‚è¯·æ³¨æ„ï¼Œ`model`å’Œ`ref_model`éœ€è¦å…·æœ‰ç›¸åŒçš„æ¶æ„ï¼ˆå³ä»…è§£ç å™¨æˆ–ç¼–ç å™¨-è§£ç å™¨ï¼‰ã€‚
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After this one can then call:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åå¯ä»¥è°ƒç”¨ï¼š
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the `beta` is the temperature parameter for the DPO loss, typically
    something in the range of `0.1` to `0.5`. We ignore the reference model as `beta`
    -> 0.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œ`beta`æ˜¯DPOæŸå¤±çš„æ¸©åº¦å‚æ•°ï¼Œé€šå¸¸åœ¨`0.1`åˆ°`0.5`çš„èŒƒå›´å†…ã€‚æˆ‘ä»¬å¿½ç•¥å‚è€ƒæ¨¡å‹ï¼Œå› ä¸º`beta` -> 0ã€‚
- en: Loss functions
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°
- en: Given the preference data, we can fit a binary classifier according to the Bradley-Terry
    model and in fact the DPO authors propose the sigmoid loss on the normalized likelihood
    via the `logsigmoid` to fit a logistic regression.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®åå¥½æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®Bradley-Terryæ¨¡å‹æ‹ŸåˆäºŒå…ƒåˆ†ç±»å™¨ï¼Œäº‹å®ä¸Šï¼ŒDPOä½œè€…æå‡ºä½¿ç”¨é€šè¿‡`logsigmoid`å¯¹å½’ä¸€åŒ–ä¼¼ç„¶è¿›è¡ŒsigmoidæŸå¤±æ‹Ÿåˆé€»è¾‘å›å½’ã€‚
- en: The [RSO](https://arxiv.org/abs/2309.06657) authors propose to use a hinge loss
    on the normalized likelihood from the [SLiC](https://arxiv.org/abs/2305.10425)
    paper. The `DPOTrainer` can be switched to this loss via the `loss_type="hinge"`
    argument and the `beta` in this case is the reciprocal of the margin.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[RSO](https://arxiv.org/abs/2309.06657)çš„ä½œè€…å»ºè®®ä½¿ç”¨æ¥è‡ª[SLiC](https://arxiv.org/abs/2305.10425)è®ºæ–‡çš„å½’ä¸€åŒ–ä¼¼ç„¶çš„é“°é“¾æŸå¤±ã€‚`DPOTrainer`å¯ä»¥é€šè¿‡`loss_type="hinge"`å‚æ•°åˆ‡æ¢åˆ°æ­¤æŸå¤±ï¼Œæ­¤æ—¶`beta`æ˜¯è¾¹é™…çš„å€’æ•°ã€‚'
- en: The [IPO](https://arxiv.org/abs/2310.12036) authors provide a deeper theoretical
    understanding of the DPO algorithms and identify an issue with overfitting and
    propose an alternative loss which can be used via the `loss_type="ipo"` argument
    to the trainer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[IPO](https://arxiv.org/abs/2310.12036)çš„ä½œè€…æä¾›äº†å¯¹DPOç®—æ³•çš„æ›´æ·±å…¥çš„ç†è®ºç†è§£ï¼Œå¹¶ç¡®å®šäº†è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ›¿ä»£æŸå¤±ï¼Œå¯ä»¥é€šè¿‡`loss_type="ipo"`å‚æ•°ä¼ é€’ç»™è®­ç»ƒå¸ˆã€‚'
- en: The [cDPO](https://ericmitchell.ai/cdpo.pdf) is a tweak on the DPO loss where
    we assume that the preference labels are noisy with some probability that can
    be passed to the `DPOTrainer` via `label_smoothing` argument (between 0 and 0.5)
    and then a conservative DPO loss is used. Use the `loss_type="cdpo"` argument
    to the trainer to use it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[cDPO](https://ericmitchell.ai/cdpo.pdf)æ˜¯å¯¹DPOæŸå¤±çš„è°ƒæ•´ï¼Œå…¶ä¸­æˆ‘ä»¬å‡è®¾åå¥½æ ‡ç­¾å­˜åœ¨ä¸€å®šæ¦‚ç‡çš„å™ªå£°ï¼Œå¯ä»¥é€šè¿‡`label_smoothing`å‚æ•°ï¼ˆä»‹äº0å’Œ0.5ä¹‹é—´ï¼‰ä¼ é€’ç»™`DPOTrainer`ï¼Œç„¶åä½¿ç”¨ä¿å®ˆçš„DPOæŸå¤±ã€‚ä½¿ç”¨`loss_type="cdpo"`å‚æ•°æ¥ä½¿ç”¨å®ƒã€‚'
- en: The [KTO](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)
    loss is derived to directly maximize the utility of LLM generations instead of
    the log-likelihood of preferences. Thus the dataset are not necessarily preferences
    but rather desirable vs undesirable completions. For paired preference data as
    required by the `DPOTrainer`, use the `loss_type="kto_pair"` argument to the trainer
    to utilize this loss, while for the more general case of desired and undesirable
    data, use the as of yet unimplemented `KTOTrainer`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[KTO](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)æŸå¤±è¢«æ¨å¯¼å‡ºæ¥ï¼Œç›´æ¥æœ€å¤§åŒ–LLMç”Ÿæˆçš„æ•ˆç”¨ï¼Œè€Œä¸æ˜¯åå¥½çš„å¯¹æ•°ä¼¼ç„¶ã€‚å› æ­¤ï¼Œæ•°æ®é›†ä¸ä¸€å®šæ˜¯åå¥½ï¼Œè€Œæ˜¯ç†æƒ³ä¸ä¸ç†æƒ³çš„å®Œæˆã€‚å¯¹äº`DPOTrainer`æ‰€éœ€çš„æˆå¯¹åå¥½æ•°æ®ï¼Œè¯·ä½¿ç”¨`loss_type="kto_pair"`å‚æ•°æ¥åˆ©ç”¨è¿™ç§æŸå¤±ï¼Œè€Œå¯¹äºæœŸæœ›å’Œä¸æœŸæœ›æ•°æ®çš„æ›´ä¸€èˆ¬æƒ…å†µï¼Œè¯·ä½¿ç”¨å°šæœªå®ç°çš„`KTOTrainer`ã€‚'
- en: Logging
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ—¥å¿—è®°å½•
- en: 'While training and evaluating we record the following reward metrics:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®°å½•ä»¥ä¸‹å¥–åŠ±æŒ‡æ ‡ï¼š
- en: '`rewards/chosen`: the mean difference between the log probabilities of the
    policy model and the reference model for the chosen responses scaled by beta'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/chosen`ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å¯¹äºæ‰€é€‰å“åº”çš„å¯¹æ•°æ¦‚ç‡ä¹‹é—´çš„å¹³å‡å·®å¼‚ï¼ŒæŒ‰betaç¼©æ”¾'
- en: '`rewards/rejected`: the mean difference between the log probabilities of the
    policy model and the reference model for the rejected responses scaled by beta'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/rejected`ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å¯¹äºè¢«æ‹’ç»å“åº”çš„å¯¹æ•°æ¦‚ç‡ä¹‹é—´çš„å¹³å‡å·®å¼‚ï¼ŒæŒ‰betaç¼©æ”¾'
- en: '`rewards/accuracies`: mean of how often the chosen rewards are > than the corresponding
    rejected rewards'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/accuracies`ï¼šæ‰€é€‰å¥–åŠ±æ¯”ç›¸åº”è¢«æ‹’ç»å¥–åŠ±é«˜çš„å¹³å‡å€¼'
- en: '`rewards/margins`: the mean difference between the chosen and corresponding
    rejected rewards'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards/margins`ï¼šæ‰€é€‰å¥–åŠ±å’Œç›¸åº”è¢«æ‹’ç»å¥–åŠ±ä¹‹é—´çš„å¹³å‡å·®å¼‚'
- en: Accelerate DPO fine-tuning using unsloth
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨unslothåŠ é€ŸDPOå¾®è°ƒ
- en: 'You can further accelerate QLoRA / LoRA (2x faster, 60% less memory) using
    the [`unsloth`](https://github.com/unslothai/unsloth) library that is fully compatible
    with `SFTTrainer`. Currently `unsloth` supports only Llama (Yi, TinyLlama, Qwen,
    Deepseek etc) and Mistral architectures. Some benchmarks for DPO listed below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¸`SFTTrainer`å®Œå…¨å…¼å®¹çš„[`unsloth`](https://github.com/unslothai/unsloth)åº“æ¥è¿›ä¸€æ­¥åŠ é€ŸQLoRA
    / LoRAï¼ˆé€Ÿåº¦æé«˜2å€ï¼Œå†…å­˜å‡å°‘60%ï¼‰ã€‚ç›®å‰ï¼Œ`unsloth`ä»…æ”¯æŒLlamaï¼ˆYiï¼ŒTinyLlamaï¼ŒQwenï¼ŒDeepseekç­‰ï¼‰å’ŒMistralæ¶æ„ã€‚ä»¥ä¸‹æ˜¯DPOçš„ä¸€äº›åŸºå‡†æµ‹è¯•ï¼š
- en: '| GPU | Model | Dataset | ğŸ¤— | ğŸ¤— + Flash Attention 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM saved
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| GPU | Model | Dataset | ğŸ¤— | ğŸ¤— + Flash Attention 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM saved
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| A100 40G | Zephyr 7b | Ultra Chat | 1x | 1.24x | **1.88x** | -11.6% |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| A100 40G | Zephyr 7b | Ultra Chat | 1x | 1.24x | **1.88x** | -11.6% |'
- en: '| Tesla T4 | Zephyr 7b | Ultra Chat | 1x | 1.09x | **1.55x** | -18.6% |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Tesla T4 | Zephyr 7b | Ultra Chat | 1x | 1.09x | **1.55x** | -18.6% |'
- en: 'First install `unsloth` according to the [official documentation](https://github.com/unslothai/unsloth).
    Once installed, you can incorporate unsloth into your workflow in a very simple
    manner; instead of loading `AutoModelForCausalLM`, you just need to load a `FastLanguageModel`
    as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://github.com/unslothai/unsloth)å®‰è£…`unsloth`ã€‚å®‰è£…å®Œæˆåï¼Œæ‚¨å¯ä»¥ä»¥éå¸¸ç®€å•çš„æ–¹å¼å°†unslothæ•´åˆåˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼›æ‚¨åªéœ€è¦åŠ è½½`FastLanguageModel`ï¼Œè€Œä¸æ˜¯åŠ è½½`AutoModelForCausalLM`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The saved model is fully compatible with Hugging Faceâ€™s transformers library.
    Learn more about unsloth in their [official repository](https://github.com/unslothai/unsloth).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜çš„æ¨¡å‹ä¸Hugging Faceçš„transformersåº“å®Œå…¨å…¼å®¹ã€‚åœ¨ä»–ä»¬çš„[å®˜æ–¹å­˜å‚¨åº“](https://github.com/unslothai/unsloth)ä¸­äº†è§£æ›´å¤šå…³äºunslothçš„ä¿¡æ¯ã€‚
- en: Reference model considerations with PEFT
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ¨¡å‹åœ¨PEFTä¸­çš„è€ƒè™‘
- en: You have three main options (plus several variants) for how the reference model
    works when using PEFT, assuming the model that you would like to further enhance
    with DPO was tuned using (Q)LoRA.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨PEFTæ—¶ï¼Œæ‚¨æœ‰ä¸‰ä¸ªä¸»è¦é€‰é¡¹ï¼ˆä»¥åŠå‡ ä¸ªå˜ä½“ï¼‰æ¥ç¡®å®šå‚è€ƒæ¨¡å‹çš„å·¥ä½œæ–¹å¼ï¼Œå‡è®¾æ‚¨å¸Œæœ›ä½¿ç”¨DPOè¿›ä¸€æ­¥å¢å¼ºçš„æ¨¡å‹æ˜¯ä½¿ç”¨ï¼ˆQï¼‰LoRAè¿›è¡Œè°ƒæ•´çš„ã€‚
- en: Simply create two instances of the model, each loading your adapter - works
    fine but is very inefficient.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç®€å•åœ°åˆ›å»ºä¸¤ä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ¯ä¸ªåŠ è½½æ‚¨çš„é€‚é…å™¨ - è¿è¡Œè‰¯å¥½ä½†æ•ˆç‡å¾ˆä½ã€‚
- en: Merge the adapter into the base model, create another adapter on top, then leave
    the `model_ref` param null, in which case DPOTrainer will unload the adapter for
    reference inference - efficient, but has potential downsides discussed below.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†é€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œé¡¶éƒ¨åˆ›å»ºå¦ä¸€ä¸ªé€‚é…å™¨ï¼Œç„¶åå°†`model_ref`å‚æ•°ä¿ç•™ä¸ºç©ºï¼Œè¿™æ ·DPOTrainerå°†å¸è½½ç”¨äºå‚è€ƒæ¨ç†çš„é€‚é…å™¨ - é«˜æ•ˆï¼Œä½†å­˜åœ¨ä¸‹é¢è®¨è®ºçš„æ½œåœ¨ç¼ºç‚¹ã€‚
- en: Load the adapter twice with different names, then use `set_adapter` during training
    to swap between the adapter being DPOâ€™d and the reference adapter - slightly less
    efficient compared to 2 (~adapter size VRAM overhead), but avoids the pitfalls.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸¤æ¬¡åŠ è½½é€‚é…å™¨ï¼Œä½¿ç”¨ä¸åŒçš„åç§°ï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨`set_adapter`æ¥åœ¨DPOé€‚é…å™¨å’Œå‚è€ƒé€‚é…å™¨ä¹‹é—´è¿›è¡Œåˆ‡æ¢ - ä¸2ç›¸æ¯”ç¨å¾®ä¸é‚£ä¹ˆé«˜æ•ˆï¼ˆé€‚é…å™¨å¤§å°VRAMå¼€é”€ï¼‰ï¼Œä½†é¿å…äº†ç¼ºç‚¹ã€‚
- en: Downsides to merging QLoRA before DPO (approach 2)
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨DPOä¹‹å‰åˆå¹¶QLoRAçš„ç¼ºç‚¹ï¼ˆæ–¹æ³•2ï¼‰
- en: As suggested by [Tim Dettmers](https://twitter.com/Tim_Dettmers/status/1694654191325573456),
    the best option for merging QLoRA adapters is to first quantize the base model,
    merge the adapter, then convert back to bf16\. Something similar to [this script](https://github.com/jondurbin/qlora/blob/main/qmerge.py)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[Tim Dettmers](https://twitter.com/Tim_Dettmers/status/1694654191325573456)å»ºè®®çš„ï¼Œåˆå¹¶QLoRAé€‚é…å™¨çš„æœ€ä½³æ–¹æ³•æ˜¯é¦–å…ˆå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œç„¶ååˆå¹¶é€‚é…å™¨ï¼Œç„¶åè½¬æ¢å›bf16ã€‚ç±»ä¼¼äº[æ­¤è„šæœ¬](https://github.com/jondurbin/qlora/blob/main/qmerge.py)
- en: You can also just merge the adapters the standard way without quantizing the
    base model, but then you have 1-2% reduced performance (and evidently, more issues
    with empty responses).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥åªæ˜¯ä»¥æ ‡å‡†æ–¹å¼åˆå¹¶é€‚é…å™¨ï¼Œè€Œä¸å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä½†è¿™æ ·ä¼šå¯¼è‡´æ€§èƒ½é™ä½1-2%ï¼ˆæ˜¾ç„¶ï¼Œè¿˜ä¼šå‡ºç°æ›´å¤šç©ºå“åº”çš„é—®é¢˜ï¼‰ã€‚
- en: If you use the recommended approach, which quantizes the model, youâ€™re now in
    a situation where to use QLoRA for DPO, you will need to re-quantize the merged
    model again or use an unquantized merge with lower overall performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨æ¨èçš„æ–¹æ³•ï¼Œå³å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œé‚£ä¹ˆç°åœ¨æ‚¨å¤„äºè¿™æ ·ä¸€ç§æƒ…å†µï¼šè¦ä½¿ç”¨QLoRAè¿›è¡ŒDPOï¼Œæ‚¨å°†éœ€è¦å†æ¬¡å¯¹åˆå¹¶çš„æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œæˆ–è€…ä½¿ç”¨æ€§èƒ½è¾ƒä½çš„æœªé‡åŒ–åˆå¹¶ã€‚
- en: Using option 3 - load the adapter twice
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é€‰é¡¹3 - ä¸¤æ¬¡åŠ è½½é€‚é…å™¨
- en: To avoid the downsides with option 2, at the expense of slightly increased VRAM,
    you can load your fine-tuned adapter into the model twice, with different names,
    and set the model/ref adapter names in DPOTrainer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…é€‰é¡¹ 2 çš„ç¼ºç‚¹ï¼Œå¯ä»¥å°†å¾®è°ƒçš„é€‚é…å™¨åŠ è½½åˆ°æ¨¡å‹ä¸­ä¸¤æ¬¡ï¼Œä½¿ç”¨ä¸åŒçš„åç§°ï¼Œå¹¶åœ¨ DPOTrainer ä¸­è®¾ç½®æ¨¡å‹/å‚è€ƒé€‚é…å™¨çš„åç§°ï¼Œä»¥ç•¥å¾®å¢åŠ  VRAMã€‚
- en: 'For example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼š
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: DPOTrainer
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPOTrainer
- en: '### `class trl.DPOTrainer`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.DPOTrainer`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)'
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`transformers.PreTrainedModel`) â€” The model to train, preferably an
    `AutoModelForSequenceClassification`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`transformers.PreTrainedModel`) â€” è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œæœ€å¥½æ˜¯ `AutoModelForSequenceClassification`ã€‚'
- en: '`ref_model` (`PreTrainedModelWrapper`) â€” Hugging Face transformer model with
    a casual language modelling head. Used for implicit reward computation and loss.
    If no reference model is provided, the trainer will create a reference model with
    the same architecture as the model to be optimized.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ref_model` (`PreTrainedModelWrapper`) â€” Hugging Face è½¬æ¢å™¨æ¨¡å‹ï¼Œå¸¦æœ‰ä¸€ä¸ªéšæ„è¯­è¨€å»ºæ¨¡å¤´ã€‚ç”¨äºéšå¼å¥–åŠ±è®¡ç®—å’ŒæŸå¤±ã€‚å¦‚æœæ²¡æœ‰æä¾›å‚è€ƒæ¨¡å‹ï¼Œè®­ç»ƒå™¨å°†åˆ›å»ºä¸€ä¸ªä¸è¦ä¼˜åŒ–çš„æ¨¡å‹å…·æœ‰ç›¸åŒæ¶æ„çš„å‚è€ƒæ¨¡å‹ã€‚'
- en: '`beta` (`float`, defaults to 0.1) â€” The beta factor in DPO loss. Higher beta
    means less divergence from the initial policy. For the IPO loss, beta is the regularization
    parameter denoted by tau in the paper.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta` (`float`ï¼Œé»˜è®¤ä¸º 0.1) â€” DPO æŸå¤±ä¸­çš„ beta å› å­ã€‚è¾ƒé«˜çš„ beta æ„å‘³ç€ä¸åˆå§‹ç­–ç•¥çš„å‘æ•£è¾ƒå°ã€‚å¯¹äº IPO æŸå¤±ï¼Œbeta
    æ˜¯è®ºæ–‡ä¸­è¡¨ç¤ºçš„æ­£åˆ™åŒ–å‚æ•° tauã€‚'
- en: '`label_smoothing` (`float`, defaults to 0) â€” The robust DPO label smoothing
    parameter from the [cDPO](https://ericmitchell.ai/cdpo.pdf) report that should
    be between 0 and 0.5.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_smoothing` (`float`ï¼Œé»˜è®¤ä¸º 0) â€” æ¥è‡ª [cDPO](https://ericmitchell.ai/cdpo.pdf)
    æŠ¥å‘Šçš„é²æ£’ DPO æ ‡ç­¾å¹³æ»‘å‚æ•°ï¼Œåº”è¯¥åœ¨ 0 å’Œ 0.5 ä¹‹é—´ã€‚'
- en: '`loss_type` (`str`, defaults to `"sigmoid"`) â€” The type of DPO loss to use.
    Either `"sigmoid"` the default DPO loss,`"hinge"` loss from [SLiC](https://arxiv.org/abs/2305.10425)
    paper, `"ipo"` from [IPO](https://arxiv.org/abs/2310.12036) paper, or `"kto"`
    from the HALOs [report](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_type` (`str`ï¼Œé»˜è®¤ä¸º `"sigmoid"`) â€” è¦ä½¿ç”¨çš„ DPO æŸå¤±ç±»å‹ã€‚å¯ä»¥æ˜¯ `"sigmoid"` é»˜è®¤çš„ DPO
    æŸå¤±ï¼Œæ¥è‡ª [SLiC](https://arxiv.org/abs/2305.10425) è®ºæ–‡çš„ `"hinge"` æŸå¤±ï¼Œæ¥è‡ª [IPO](https://arxiv.org/abs/2310.12036)
    è®ºæ–‡çš„ `"ipo"`ï¼Œæˆ–æ¥è‡ª HALOs [æŠ¥å‘Š](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)
    çš„ `"kto"`ã€‚'
- en: '`args` (`transformers.TrainingArguments`) â€” The arguments to use for training.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` (`transformers.TrainingArguments`) â€” ç”¨äºè®­ç»ƒçš„å‚æ•°ã€‚'
- en: '`data_collator` (`transformers.DataCollator`) â€” The data collator to use for
    training. If None is specified, the default data collator (`DPODataCollatorWithPadding`)
    will be used which will pad the sequences to the maximum length of the sequences
    in the batch, given a dataset of paired sequences.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_collator` (`transformers.DataCollator`) â€” ç”¨äºè®­ç»ƒçš„æ•°æ®æ•´ç†å™¨ã€‚å¦‚æœæœªæŒ‡å®šä¸º Noneï¼Œåˆ™å°†ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼ˆ`DPODataCollatorWithPadding`ï¼‰ï¼Œè¯¥æ•´ç†å™¨å°†å°†åºåˆ—å¡«å……åˆ°æ‰¹æ¬¡ä¸­åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œç»™å®šä¸€ç»„æˆå¯¹åºåˆ—çš„æ•°æ®é›†ã€‚'
- en: '`label_pad_token_id` (`int`, defaults to `-100`) â€” The label pad token id.
    This argument is required if you want to use the default data collator.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_pad_token_id` (`int`ï¼Œé»˜è®¤ä¸º `-100`) â€” æ ‡ç­¾å¡«å……æ ‡è®° idã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚'
- en: '`padding_value` (`int`, defaults to `0`) â€” The padding value if it is different
    to the tokenizerâ€™s pad_token_id.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value` (`int`ï¼Œé»˜è®¤ä¸º `0`) â€” å¦‚æœä¸æ ‡è®°å™¨çš„ pad_token_id ä¸åŒï¼Œåˆ™ä¸ºå¡«å……å€¼ã€‚'
- en: '`truncation_mode` (`str`, defaults to `keep_end`) â€” The truncation mode to
    use, either `keep_end` or `keep_start`. This argument is required if you want
    to use the default data collator.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_mode` (`str`ï¼Œé»˜è®¤ä¸º `keep_end`) â€” è¦ä½¿ç”¨çš„æˆªæ–­æ¨¡å¼ï¼Œå¯ä»¥æ˜¯ `keep_end` æˆ– `keep_start`ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚'
- en: '`train_dataset` (`datasets.Dataset`) â€” The dataset to use for training.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset` (`datasets.Dataset`) â€” ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚'
- en: '`eval_dataset` (`datasets.Dataset`) â€” The dataset to use for evaluation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (`datasets.Dataset`) â€” ç”¨äºè¯„ä¼°çš„æ•°æ®é›†ã€‚'
- en: '`tokenizer` (`transformers.PreTrainedTokenizerBase`) â€” The tokenizer to use
    for training. This argument is required if you want to use the default data collator.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`transformers.PreTrainedTokenizerBase`) â€” ç”¨äºè®­ç»ƒçš„æ ‡è®°å™¨ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚'
- en: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) â€” The model initializer
    to use for training. If None is specified, the default model initializer will
    be used.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) â€” ç”¨äºè®­ç»ƒçš„æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚å¦‚æœæœªæŒ‡å®šä¸º
    Noneï¼Œåˆ™å°†ä½¿ç”¨é»˜è®¤æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚'
- en: '`callbacks` (`List[transformers.TrainerCallback]`) â€” The callbacks to use for
    training.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callbacks` (`List[transformers.TrainerCallback]`) â€” ç”¨äºè®­ç»ƒçš„å›è°ƒå‡½æ•°ã€‚'
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    â€” The optimizer and scheduler to use for training.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    â€” ç”¨äºè®­ç»ƒçš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚'
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    â€” The function to use to preprocess the logits before computing the metrics.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    â€” åœ¨è®¡ç®—æŒ‡æ ‡ä¹‹å‰ç”¨äºé¢„å¤„ç†å¯¹æ•°çš„å‡½æ•°ã€‚'
- en: '`max_length` (`int`, defaults to `None`) â€” The maximum length of the sequences
    in the batch. This argument is required if you want to use the default data collator.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`ï¼Œé»˜è®¤ä¸º `None`) â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚'
- en: '`max_prompt_length` (`int`, defaults to `None`) â€” The maximum length of the
    prompt. This argument is required if you want to use the default data collator.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_prompt_length` (`int`ï¼Œé»˜è®¤ä¸º `None`) â€” æç¤ºçš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚'
- en: '`max_target_length` (`int`, defaults to `None`) â€” The maximum length of the
    target. This argument is required if you want to use the default data collator
    and your model is an encoder-decoder.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_target_length` (`int`ï¼Œé»˜è®¤ä¸º `None`) â€” ç›®æ ‡çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨å¹¶ä¸”æ‚¨çš„æ¨¡å‹æ˜¯ç¼–ç å™¨-è§£ç å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚'
- en: '`peft_config` (`Dict`, defaults to `None`) â€” The PEFT configuration to use
    for training. If you pass a PEFT configuration, the model will be wrapped in a
    PEFT model.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config` (`Dict`ï¼Œé»˜è®¤ä¸º `None`) â€” ç”¨äºè®­ç»ƒçš„ PEFT é…ç½®ã€‚å¦‚æœä¼ é€’ PEFT é…ç½®ï¼Œæ¨¡å‹å°†è¢«åŒ…è£…åœ¨ PEFT
    æ¨¡å‹ä¸­ã€‚'
- en: '`is_encoder_decoder` (`Optional[bool]`, `optional`, defaults to `None`) â€” If
    no model is provided, we need to know if the model_init returns an encoder-decoder.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_encoder_decoder` (`Optional[bool]`, `å¯é€‰`, é»˜è®¤ä¸º `None`) â€” å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æ¨¡å‹åˆå§‹åŒ–æ˜¯å¦è¿”å›ç¼–ç å™¨-è§£ç å™¨ã€‚'
- en: '`disable_dropout` (`bool`, defaults to `True`) â€” Whether or not to disable
    dropouts in `model` and `ref_model`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_dropout` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨ `model` å’Œ `ref_model` ä¸­ç¦ç”¨ dropoutã€‚'
- en: '`generate_during_eval` (`bool`, defaults to `False`) â€” Whether to sample and
    log generations during evaluation step.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_during_eval` (`bool`, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è¯„ä¼°æ­¥éª¤ä¸­å¯¹ç”Ÿæˆè¿›è¡Œé‡‡æ ·å’Œè®°å½•ã€‚'
- en: '`compute_metrics` (`Callable[[EvalPrediction], Dict]`, *optional*) â€” The function
    to use to compute the metrics. Must take a `EvalPrediction` and return a dictionary
    string to metric values.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_metrics` (`Callable[[EvalPrediction], Dict]`, *å¯é€‰*) â€” ç”¨äºè®¡ç®—æŒ‡æ ‡çš„å‡½æ•°ã€‚å¿…é¡»æ¥å—ä¸€ä¸ª
    `EvalPrediction` å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²åˆ°æŒ‡æ ‡å€¼çš„å­—å…¸ã€‚'
- en: '`precompute_ref_log_probs` (`bool`, defaults to `False`) â€” Flag to precompute
    reference model log probabilities and evaluation datasets. This is useful if you
    want to train without the reference model and reduce the total GPU memory needed.
    model_init_kwargs â€” (`Optional[Dict]`, *optional*): Dict of Optional kwargs to
    pass when instantiating the model from a string ref_model_init_kwargs â€” (`Optional[Dict]`,
    *optional*): Dict of Optional kwargs to pass when instantiating the ref model
    from a string'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precompute_ref_log_probs` (`bool`, é»˜è®¤ä¸º `False`) â€” ç”¨äºé¢„è®¡ç®—å‚è€ƒæ¨¡å‹å¯¹æ•°æ¦‚ç‡å’Œè¯„ä¼°æ•°æ®é›†çš„æ ‡å¿—ã€‚å¦‚æœè¦åœ¨æ²¡æœ‰å‚è€ƒæ¨¡å‹çš„æƒ…å†µä¸‹è®­ç»ƒå¹¶å‡å°‘æ‰€éœ€çš„æ€»
    GPU å†…å­˜ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚model_init_kwargs â€” (`Optional[Dict]`, *å¯é€‰*): ä¼ é€’ç»™ä»å­—ç¬¦ä¸²å®ä¾‹åŒ–æ¨¡å‹æ—¶çš„å¯é€‰ kwargs
    å­—å…¸ ref_model_init_kwargs â€” (`Optional[Dict]`, *å¯é€‰*): ä¼ é€’ç»™ä»å­—ç¬¦ä¸²å®ä¾‹åŒ–å‚è€ƒæ¨¡å‹æ—¶çš„å¯é€‰ kwargs
    å­—å…¸'
- en: '`model_adapter_name` (`str`, defaults to `None`) â€” Name of the train target
    PEFT adapter, when using LoRA with multiple adapters.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_adapter_name` (`str`, é»˜è®¤ä¸º `None`) â€” ä½¿ç”¨ LoRA æ—¶çš„è®­ç»ƒç›®æ ‡ PEFT é€‚é…å™¨çš„åç§°ï¼Œå½“æœ‰å¤šä¸ªé€‚é…å™¨æ—¶ã€‚'
- en: '`ref_adapter_name` (`str`, defaults to `None`) â€” Name of the reference PEFT
    adapter, when using LoRA with multiple adapters.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ref_adapter_name` (`str`, é»˜è®¤ä¸º `None`) â€” ä½¿ç”¨ LoRA æ—¶å‚è€ƒ PEFT é€‚é…å™¨çš„åç§°ï¼Œå½“æœ‰å¤šä¸ªé€‚é…å™¨æ—¶ã€‚'
- en: Initialize DPOTrainer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ– DPOTrainerã€‚
- en: '#### `build_tokenized_answer`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_tokenized_answer`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)'
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`. It does ensure
    `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`. Reference: [https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Llama åˆ†è¯å™¨ç¡®å®æ»¡è¶³ `enc(a + b) = enc(a) + enc(b)`ã€‚å®ƒç¡®ä¿ `enc(a + b) = enc(a) + enc(a
    + b)[len(enc(a)):]`ã€‚å‚è€ƒï¼š[https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)
- en: '#### `compute_reference_log_probs`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `compute_reference_log_probs`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)'
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Computes log probabilities of the reference model for a single padded batch
    of a DPO specific dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®— DPO ç‰¹å®šæ•°æ®é›†çš„å•ä¸ªå¡«å……æ‰¹æ¬¡çš„å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡ã€‚
- en: '#### `concatenated_forward`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `concatenated_forward`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)'
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Run the given model on the given batch of inputs, concatenating the chosen and
    rejected inputs together.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»™å®šçš„è¾“å…¥æ‰¹æ¬¡ä¸Šè¿è¡Œç»™å®šæ¨¡å‹ï¼Œå°†æ‰€é€‰å’Œè¢«æ‹’ç»çš„è¾“å…¥è¿æ¥åœ¨ä¸€èµ·ã€‚
- en: We do this to avoid doing two forward passes, because itâ€™s faster for FSDP.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼Œå› ä¸ºå¯¹äº FSDP æ¥è¯´æ›´å¿«ã€‚
- en: '#### `concatenated_inputs`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `concatenated_inputs`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)'
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Concatenate the chosen and rejected inputs into a single tensor.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€é€‰å’Œè¢«æ‹’ç»çš„è¾“å…¥è¿æ¥æˆä¸€ä¸ªå¼ é‡ã€‚
- en: '#### `dpo_loss`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `dpo_loss`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)'
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Returns
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: A tuple of three tensors
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå¼ é‡çš„å…ƒç»„
- en: (losses, chosen_rewards, rejected_rewards). The losses tensor contains the DPO
    loss for each example in the batch. The chosen_rewards and rejected_rewards tensors
    contain the rewards for the chosen and rejected responses, respectively.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (æŸå¤±ï¼Œæ‰€é€‰å¥–åŠ±ï¼Œè¢«æ‹’ç»å¥–åŠ±)ã€‚æŸå¤±å¼ é‡åŒ…å«æ‰¹æ¬¡ä¸­æ¯ä¸ªç¤ºä¾‹çš„ DPO æŸå¤±ã€‚æ‰€é€‰å¥–åŠ±å’Œè¢«æ‹’ç»å¥–åŠ±å¼ é‡åˆ†åˆ«åŒ…å«æ‰€é€‰å’Œè¢«æ‹’ç»å“åº”çš„å¥–åŠ±ã€‚
- en: Compute the DPO loss for a batch of policy and reference model log probabilities.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä¸€æ‰¹ç­–ç•¥å’Œå‚è€ƒæ¨¡å‹å¯¹æ•°æ¦‚ç‡è®¡ç®— DPO æŸå¤±ã€‚
- en: '#### `evaluation_loop`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `evaluation_loop`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)'
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Overriding built-in evaluation loop to store metrics for each batch. Prediction/evaluation
    loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è¦†ç›–å†…ç½®è¯„ä¼°å¾ªç¯ä»¥å­˜å‚¨æ¯ä¸ªæ‰¹æ¬¡çš„æŒ‡æ ‡ã€‚é¢„æµ‹/è¯„ä¼°å¾ªç¯ï¼Œç”± `Trainer.evaluate()` å’Œ `Trainer.predict()` å…±äº«ã€‚
- en: Works both with or without labels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ˜¯å¦æœ‰æ ‡ç­¾éƒ½å¯ä»¥ä½¿ç”¨ã€‚
- en: '#### `get_batch_logps`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_batch_logps`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)'
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Compute the log probabilities of the given labels under the given logits.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ç»™å®šæ ‡ç­¾åœ¨ç»™å®šå¯¹æ•°ä¸‹çš„å¯¹æ•°æ¦‚ç‡ã€‚
- en: '#### `get_batch_loss_metrics`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_batch_loss_metrics`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)'
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Compute the DPO loss and other metrics for the given batch of inputs for train
    or test.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç»™å®šçš„è¾“å…¥æ‰¹æ¬¡è®¡ç®— DPO æŸå¤±å’Œå…¶ä»–æŒ‡æ ‡ï¼Œç”¨äºè®­ç»ƒæˆ–æµ‹è¯•ã€‚
- en: '#### `get_batch_samples`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_batch_samples`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)'
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Generate samples from the model and reference model for the given batch of inputs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç»™å®šçš„è¾“å…¥æ‰¹æ¬¡ä»æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ç”Ÿæˆæ ·æœ¬ã€‚
- en: '#### `get_eval_dataloader`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_eval_dataloader`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)'
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`eval_dataset` (`torch.utils.data.Dataset`, *optional*) â€” If provided, will
    override `self.eval_dataset`. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement `__len__`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset`ï¼ˆ`torch.utils.data.Dataset`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœæä¾›ï¼Œå°†è¦†ç›–`self.eval_dataset`ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ª[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œåˆ™ä¸è¢«`model.forward()`æ–¹æ³•æ¥å—çš„åˆ—å°†è¢«è‡ªåŠ¨åˆ é™¤ã€‚å®ƒå¿…é¡»å®ç°`__len__`ã€‚'
- en: Returns the evaluation `~torch.utils.data.DataLoader`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›è¯„ä¼°`~torch.utils.data.DataLoader`ã€‚
- en: Subclass of transformers.src.transformers.trainer.get_eval_dataloader to precompute
    `ref_log_probs`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: transformers.src.transformers.trainer.get_eval_dataloaderçš„å­ç±»ï¼Œç”¨äºé¢„è®¡ç®—`ref_log_probs`ã€‚
- en: '#### `get_train_dataloader`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_train_dataloader`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)'
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Returns the training `~torch.utils.data.DataLoader`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›è®­ç»ƒ`~torch.utils.data.DataLoader`ã€‚
- en: Subclass of transformers.src.transformers.trainer.get_train_dataloader to precompute
    `ref_log_probs`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: transformers.src.transformers.trainer.get_train_dataloaderçš„å­ç±»ï¼Œç”¨äºé¢„è®¡ç®—`ref_log_probs`ã€‚
- en: '#### `log`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `log`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)'
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`logs` (`Dict[str, float]`) â€” The values to log.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logs`ï¼ˆ`Dict[str, float]`ï¼‰- è¦è®°å½•çš„å€¼ã€‚'
- en: Log `logs` on the various objects watching training, including stored metrics.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è§‚å¯Ÿè®­ç»ƒçš„å„ç§å¯¹è±¡ä¸Šè®°å½•`logs`ï¼ŒåŒ…æ‹¬å­˜å‚¨çš„æŒ‡æ ‡ã€‚
- en: '#### `null_ref_context`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `null_ref_context`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)'
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Context manager for handling null reference model (that is, peft adapter manipulation).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå¤„ç†ç©ºå¼•ç”¨æ¨¡å‹ï¼ˆå³pefté€‚é…å™¨æ“ä½œï¼‰çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚
- en: '#### `tokenize_row`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tokenize_row`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)'
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Tokenize a single row from a DPO specific dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¥è‡ªDPOç‰¹å®šæ•°æ®é›†çš„å•è¡Œè¿›è¡Œæ ‡è®°åŒ–ã€‚
- en: At this stage, we donâ€™t convert to PyTorch tensors yet; we just handle the truncation
    in case the prompt + chosen or prompt + rejected responses is/are too long. First
    we truncate the prompt; if weâ€™re still too long, we truncate the chosen/rejected.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å°†å…¶è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼›æˆ‘ä»¬åªæ˜¯å¤„ç†æˆªæ–­ï¼Œä»¥é˜²æç¤º+æ‰€é€‰æˆ–æç¤º+æ‹’ç»å“åº”å¤ªé•¿ã€‚é¦–å…ˆæˆªæ–­æç¤ºï¼›å¦‚æœä»ç„¶å¤ªé•¿ï¼Œæˆ‘ä»¬æˆªæ–­æ‰€é€‰/æ‹’ç»ã€‚
- en: We also create the labels for the chosen/rejected responses, which are of length
    equal to the sum of the length of the prompt and the chosen/rejected response,
    with label_pad_token_id for the prompt tokens.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜ä¸ºæ‰€é€‰/æ‹’ç»çš„å“åº”åˆ›å»ºæ ‡ç­¾ï¼Œå…¶é•¿åº¦ç­‰äºæç¤ºå’Œæ‰€é€‰/æ‹’ç»å“åº”çš„é•¿åº¦ä¹‹å’Œï¼Œå¯¹äºæç¤ºæ ‡è®°ä½¿ç”¨label_pad_token_idã€‚
