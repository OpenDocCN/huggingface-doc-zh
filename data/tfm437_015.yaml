- en: Generation with LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMsçš„ç”Ÿæˆ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, or Large Language Models, are the key component behind text generation.
    In a nutshell, they consist of large pretrained transformer models trained to
    predict the next word (or, more precisely, token) given some input text. Since
    they predict one token at a time, you need to do something more elaborate to generate
    new sentences other than just calling the model â€” you need to do autoregressive
    generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsï¼Œæˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¯æ–‡æœ¬ç”ŸæˆèƒŒåçš„å…³é”®ç»„ä»¶ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå®ƒä»¬ç”±å¤§å‹é¢„è®­ç»ƒçš„å˜å‹å™¨æ¨¡å‹ç»„æˆï¼Œè®­ç»ƒç”¨äºé¢„æµ‹ç»™å®šä¸€äº›è¾“å…¥æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªå•è¯ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´ï¼Œä»¤ç‰Œï¼‰ã€‚ç”±äºå®ƒä»¬ä¸€æ¬¡é¢„æµ‹ä¸€ä¸ªä»¤ç‰Œï¼Œå› æ­¤æ‚¨éœ€è¦åšä¸€äº›æ›´å¤æ‚çš„äº‹æƒ…æ¥ç”Ÿæˆæ–°çš„å¥å­ï¼Œè€Œä¸ä»…ä»…æ˜¯è°ƒç”¨æ¨¡å‹
    - æ‚¨éœ€è¦è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚
- en: Autoregressive generation is the inference-time procedure of iteratively calling
    a model with its own generated outputs, given a few initial inputs. In ğŸ¤— Transformers,
    this is handled by the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method, which is available to all models with generative capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªå›å½’ç”Ÿæˆæ˜¯åœ¨æ¨ç†æ—¶è¿­ä»£è°ƒç”¨æ¨¡å‹ä»¥ç”Ÿæˆè¾“å‡ºçš„è¿‡ç¨‹ï¼Œç»™å®šä¸€äº›åˆå§‹è¾“å…¥ã€‚åœ¨ğŸ¤— Transformersä¸­ï¼Œè¿™ç”±[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•å¤„ç†ï¼Œé€‚ç”¨äºæ‰€æœ‰å…·æœ‰ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹ã€‚
- en: 'This tutorial will show you how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Generate text with an LLM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LLMç”Ÿæˆæ–‡æœ¬
- en: Avoid common pitfalls
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¿å…å¸¸è§é™·é˜±
- en: Next steps to help you get the most out of your LLM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¸®åŠ©æ‚¨å……åˆ†åˆ©ç”¨LLMçš„ä¸‹ä¸€æ­¥
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generate text
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ–‡æœ¬
- en: A language model trained for [causal language modeling](tasks/language_modeling)
    takes a sequence of text tokens as input and returns the probability distribution
    for the next token.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›è¡Œ[å› æœè¯­è¨€å»ºæ¨¡](tasks/language_modeling)è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†æ–‡æœ¬ä»¤ç‰Œåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„æ¦‚ç‡åˆ†å¸ƒã€‚
- en: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov>
- en: '"Forward pass of an LLM"'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '"LLMçš„å‰å‘ä¼ é€’"'
- en: A critical aspect of autoregressive generation with LLMs is how to select the
    next token from this probability distribution. Anything goes in this step as long
    as you end up with a token for the next iteration. This means it can be as simple
    as selecting the most likely token from the probability distribution or as complex
    as applying a dozen transformations before sampling from the resulting distribution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsè¿›è¡Œè‡ªå›å½’ç”Ÿæˆçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å¦‚ä½•ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚åœ¨è¿™ä¸€æ­¥ä¸­å¯ä»¥é‡‡å–ä»»ä½•æ–¹æ³•ï¼Œåªè¦æœ€ç»ˆå¾—åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£çš„ä»¤ç‰Œå³å¯ã€‚è¿™æ„å‘³ç€å®ƒå¯ä»¥ç®€å•åœ°ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©æœ€å¯èƒ½çš„ä»¤ç‰Œï¼Œä¹Ÿå¯ä»¥åœ¨ä»ç»“æœåˆ†å¸ƒä¸­æŠ½æ ·ä¹‹å‰åº”ç”¨åå‡ ç§è½¬æ¢ã€‚
- en: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov>
- en: '"Autoregressive generation iteratively selects the next token from a probability
    distribution to generate text"'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '"è‡ªå›å½’ç”Ÿæˆé€šè¿‡ä»æ¦‚ç‡åˆ†å¸ƒä¸­è¿­ä»£é€‰æ‹©ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¥ç”Ÿæˆæ–‡æœ¬"'
- en: The process depicted above is repeated iteratively until some stopping condition
    is reached. Ideally, the stopping condition is dictated by the model, which should
    learn when to output an end-of-sequence (`EOS`) token. If this is not the case,
    generation stops when some predefined maximum length is reached.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°è¿‡ç¨‹ä¼šé‡å¤è¿­ä»£ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶ç”±æ¨¡å‹å†³å®šï¼Œè¯¥æ¨¡å‹åº”è¯¥å­¦ä¼šä½•æ—¶è¾“å‡ºä¸€ä¸ªç»ˆæ­¢åºåˆ—ï¼ˆ`EOS`ï¼‰ä»¤ç‰Œã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œå½“è¾¾åˆ°æŸä¸ªé¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶ï¼Œç”Ÿæˆä¼šåœæ­¢ã€‚
- en: Properly setting up the token selection step and the stopping condition is essential
    to make your model behave as youâ€™d expect on your task. That is why we have a
    [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    file associated with each model, which contains a good default generative parameterization
    and is loaded alongside your model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®è®¾ç½®ä»¤ç‰Œé€‰æ‹©æ­¥éª¤å’Œåœæ­¢æ¡ä»¶å¯¹äºä½¿æ‚¨çš„æ¨¡å‹åœ¨ä»»åŠ¡ä¸Šè¡¨ç°å¦‚æ‚¨æœŸæœ›çš„æ–¹å¼è‡³å…³é‡è¦ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹å…³è”ä¸€ä¸ª[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªè‰¯å¥½çš„é»˜è®¤ç”Ÿæˆå‚æ•°è®¾ç½®ï¼Œå¹¶ä¸”ä¸æ‚¨çš„æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚
- en: Letâ€™s talk code!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è°ˆè°ˆä»£ç ï¼
- en: If youâ€™re interested in basic LLM usage, our high-level [`Pipeline`](pipeline_tutorial)
    interface is a great starting point. However, LLMs often require advanced features
    like quantization and fine control of the token selection step, which is best
    done through [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Autoregressive generation with LLMs is also resource-intensive and should be executed
    on a GPU for adequate throughput.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯¹åŸºæœ¬LLMç”¨æ³•æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬çš„é«˜çº§[`Pipeline`](pipeline_tutorial)æ¥å£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ç„¶è€Œï¼ŒLLMsé€šå¸¸éœ€è¦é«˜çº§åŠŸèƒ½ï¼Œå¦‚é‡åŒ–å’Œå¯¹ä»¤ç‰Œé€‰æ‹©æ­¥éª¤çš„ç²¾ç»†æ§åˆ¶ï¼Œæœ€å¥½é€šè¿‡[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ¥å®ç°ã€‚LLMsçš„è‡ªå›å½’ç”Ÿæˆä¹Ÿéœ€è¦å¤§é‡èµ„æºï¼Œå¹¶ä¸”åº”è¯¥åœ¨GPUä¸Šæ‰§è¡Œä»¥è·å¾—è¶³å¤Ÿçš„ååé‡ã€‚
- en: First, you need to load the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ‚¨éœ€è¦åŠ è½½æ¨¡å‹ã€‚
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Youâ€™ll notice two flags in the `from_pretrained` call:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¼šæ³¨æ„åˆ°`from_pretrained`è°ƒç”¨ä¸­æœ‰ä¸¤ä¸ªæ ‡å¿—ï¼š
- en: '`device_map` ensures the model is moved to your GPU(s)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map`ç¡®ä¿æ¨¡å‹è¢«ç§»åŠ¨åˆ°æ‚¨çš„GPUä¸Š'
- en: '`load_in_4bit` applies [4-bit dynamic quantization](main_classes/quantization)
    to massively reduce the resource requirements'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_in_4bit`åº”ç”¨[4ä½åŠ¨æ€é‡åŒ–](main_classes/quantization)ä»¥å¤§å¹…å‡å°‘èµ„æºéœ€æ±‚'
- en: There are other ways to initialize a model, but this is a good baseline to begin
    with an LLM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–åˆå§‹åŒ–æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åŸºå‡†ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨LLMã€‚
- en: Next, you need to preprocess your text input with a [tokenizer](tokenizer_summary).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦ä½¿ç”¨[tokenizer](tokenizer_summary)å¯¹æ–‡æœ¬è¾“å…¥è¿›è¡Œé¢„å¤„ç†ã€‚
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `model_inputs` variable holds the tokenized text input, as well as the attention
    mask. While [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    does its best effort to infer the attention mask when it is not passed, we recommend
    passing it whenever possible for optimal results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_inputs`å˜é‡ä¿å­˜äº†æ ‡è®°åŒ–çš„æ–‡æœ¬è¾“å…¥ï¼Œä»¥åŠæ³¨æ„åŠ›æ©ç ã€‚è™½ç„¶[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)ä¼šå°½åŠ›æ¨æ–­æ³¨æ„åŠ›æ©ç ï¼Œä½†æˆ‘ä»¬å»ºè®®å°½å¯èƒ½åœ¨ç”Ÿæˆæ—¶ä¼ é€’å®ƒä»¥è·å¾—æœ€ä½³ç»“æœã€‚'
- en: After tokenizing the inputs, you can call the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method to returns the generated tokens. The generated tokens then should be converted
    to text before printing.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹è¾“å…¥è¿›è¡Œæ ‡è®°åŒ–åï¼Œæ‚¨å¯ä»¥è°ƒç”¨[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•è¿”å›ç”Ÿæˆçš„æ ‡è®°ã€‚ç„¶ååº”å°†ç”Ÿæˆçš„æ ‡è®°è½¬æ¢ä¸ºæ–‡æœ¬åæ‰“å°ã€‚
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, you donâ€™t need to do it one sequence at a time! You can batch your
    inputs, which will greatly improve the throughput at a small latency and memory
    cost. All you need to do is to make sure you pad your inputs properly (more on
    that below).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨ä¸éœ€è¦ä¸€æ¬¡å¤„ç†ä¸€ä¸ªåºåˆ—ï¼æ‚¨å¯ä»¥å¯¹è¾“å…¥è¿›è¡Œæ‰¹å¤„ç†ï¼Œè¿™å°†å¤§å¤§æé«˜ååé‡ï¼ŒåŒæ—¶å»¶è¿Ÿå’Œå†…å­˜æˆæœ¬å¾ˆå°ã€‚æ‚¨åªéœ€è¦ç¡®ä¿æ­£ç¡®å¡«å……è¾“å…¥å³å¯ï¼ˆä¸‹æ–‡æœ‰æ›´å¤šä¿¡æ¯ï¼‰ã€‚
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And thatâ€™s it! In a few lines of code, you can harness the power of an LLM.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼åœ¨å‡ è¡Œä»£ç ä¸­ï¼Œæ‚¨å°±å¯ä»¥åˆ©ç”¨LLMçš„å¼ºå¤§åŠŸèƒ½ã€‚
- en: Common pitfalls
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¸è§é™·é˜±
- en: There are many [generation strategies](generation_strategies), and sometimes
    the default values may not be appropriate for your use case. If your outputs arenâ€™t
    aligned with what youâ€™re expecting, weâ€™ve created a list of the most common pitfalls
    and how to avoid them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤š[ç”Ÿæˆç­–ç•¥](generation_strategies)ï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚å¦‚æœæ‚¨çš„è¾“å‡ºä¸æ‚¨çš„é¢„æœŸä¸ç¬¦ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªå…³äºæœ€å¸¸è§é™·é˜±ä»¥åŠå¦‚ä½•é¿å…å®ƒä»¬çš„åˆ—è¡¨ã€‚
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Generated output is too short/long
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„è¾“å‡ºè¿‡çŸ­/è¿‡é•¿
- en: If not specified in the [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    file, `generate` returns up to 20 tokens by default. We highly recommend manually
    setting `max_new_tokens` in your `generate` call to control the maximum number
    of new tokens it can return. Keep in mind LLMs (more precisely, [decoder-only
    models](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)) also return
    the input prompt as part of the output.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ä¸­æœªæŒ‡å®šï¼Œ`generate`é»˜è®¤è¿”å›æœ€å¤š20ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨`generate`è°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½®`max_new_tokens`ä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„æœ€å¤§æ–°æ ‡è®°æ•°é‡ã€‚è¯·è®°ä½ï¼ŒLLMsï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œä»…è§£ç å™¨æ¨¡å‹ï¼‰è¿˜ä¼šå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Incorrect generation mode
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ¨¡å¼ä¸æ­£ç¡®
- en: By default, and unless specified in the [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    file, `generate` selects the most likely token at each iteration (greedy decoding).
    Depending on your task, this may be undesirable; creative tasks like chatbots
    or writing an essay benefit from sampling. On the other hand, input-grounded tasks
    like audio transcription or translation benefit from greedy decoding. Enable sampling
    with `do_sample=True`, and you can learn more about this topic in this [blog post](https://huggingface.co/blog/how-to-generate).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ä¸­æŒ‡å®šï¼Œ`generate`åœ¨æ¯æ¬¡è¿­ä»£ä¸­é€‰æ‹©æœ€å¯èƒ½çš„æ ‡è®°ï¼ˆè´ªå©ªè§£ç ï¼‰ã€‚æ ¹æ®æ‚¨çš„ä»»åŠ¡ï¼Œè¿™å¯èƒ½æ˜¯ä¸å¸Œæœ›çš„ï¼›åƒèŠå¤©æœºå™¨äººæˆ–å†™ä½œæ–‡ç« è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡å—ç›ŠäºæŠ½æ ·ã€‚å¦ä¸€æ–¹é¢ï¼ŒåƒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘è¿™æ ·çš„è¾“å…¥é©±åŠ¨ä»»åŠ¡å—ç›Šäºè´ªå©ªè§£ç ã€‚é€šè¿‡`do_sample=True`å¯ç”¨æŠ½æ ·ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤[åšå®¢æ–‡ç« ](https://huggingface.co/blog/how-to-generate)ä¸­äº†è§£æ›´å¤šå…³äºè¿™ä¸ªä¸»é¢˜çš„ä¿¡æ¯ã€‚
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Wrong padding side
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¡«å……æ–¹å‘é”™è¯¯
- en: LLMs are [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)
    architectures, meaning they continue to iterate on your input prompt. If your
    inputs do not have the same length, they need to be padded. Since LLMs are not
    trained to continue from pad tokens, your input needs to be left-padded. Make
    sure you also donâ€™t forget to pass the attention mask to generate!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsæ˜¯ä»…è§£ç å™¨æ¶æ„ï¼Œæ„å‘³ç€å®ƒä»¬ä¼šç»§ç»­è¿­ä»£æ‚¨çš„è¾“å…¥æç¤ºã€‚å¦‚æœæ‚¨çš„è¾“å…¥é•¿åº¦ä¸åŒï¼Œå°±éœ€è¦è¿›è¡Œå¡«å……ã€‚ç”±äºLLMsæ²¡æœ‰ç»è¿‡è®­ç»ƒä»¥ä»å¡«å……æ ‡è®°ç»§ç»­ï¼Œå› æ­¤æ‚¨çš„è¾“å…¥éœ€è¦è¿›è¡Œå·¦å¡«å……ã€‚ç¡®ä¿ä¸è¦å¿˜è®°ä¼ é€’æ³¨æ„åŠ›æ©ç ä»¥ç”Ÿæˆï¼
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Wrong prompt
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æç¤ºé”™è¯¯
- en: 'Some models and tasks expect a certain input prompt format to work properly.
    When this format is not applied, you will get a silent performance degradation:
    the model kinda works, but not as well as if you were following the expected prompt.
    More information about prompting, including which models and tasks need to be
    careful, is available in this [guide](tasks/prompting). Letâ€™s see an example with
    a chat LLM, which makes use of [chat templating](chat_templating):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æ¨¡å‹å’Œä»»åŠ¡æœŸæœ›ç‰¹å®šçš„è¾“å…¥æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚å¦‚æœæœªåº”ç”¨æ­¤æ ¼å¼ï¼Œæ‚¨å°†è·å¾—æ²‰é»˜çš„æ€§èƒ½ä¸‹é™ï¼šæ¨¡å‹å¯èƒ½ä¼šè¿è¡Œï¼Œä½†ä¸å¦‚æŒ‰ç…§é¢„æœŸæç¤ºé‚£æ ·å¥½ã€‚æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦å°å¿ƒï¼Œå¯åœ¨æ­¤[æŒ‡å—](tasks/prompting)ä¸­æ‰¾åˆ°ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªèŠå¤©LLMçš„ç¤ºä¾‹ï¼Œå®ƒä½¿ç”¨èŠå¤©æ¨¡æ¿ï¼š
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Further resources
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æº
- en: 'While the autoregressive generation process is relatively straightforward,
    making the most out of your LLM can be a challenging endeavor because there are
    many moving parts. For your next steps to help you dive deeper into LLM usage
    and understanding:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œä½†å……åˆ†åˆ©ç”¨æ‚¨çš„LLMå¯èƒ½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„åŠªåŠ›ï¼Œå› ä¸ºå…¶ä¸­æœ‰è®¸å¤šè¦ç´ ã€‚ä¸ºäº†å¸®åŠ©æ‚¨æ·±å…¥äº†è§£LLMçš„ä½¿ç”¨å’Œç†è§£ï¼Œè¯·ç»§ç»­ä»¥ä¸‹æ­¥éª¤ï¼š
- en: Advanced generate usage
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é«˜çº§ç”Ÿæˆç”¨æ³•
- en: '[Guide](generation_strategies) on how to control different generation methods,
    how to set up the generation configuration file, and how to stream the output;'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æŒ‡å—](generation_strategies)å…³äºå¦‚ä½•æ§åˆ¶ä¸åŒçš„ç”Ÿæˆæ–¹æ³•ï¼Œå¦‚ä½•è®¾ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼Œä»¥åŠå¦‚ä½•æµå¼ä¼ è¾“è¾“å‡ºï¼›'
- en: '[Guide](chat_templating) on the prompt template for chat LLMs;'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æŒ‡å—](chat_templating)å…³äºèŠå¤©LLMçš„æç¤ºæ¨¡æ¿ï¼›'
- en: '[Guide](tasks/prompting) on to get the most of prompt design;'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æŒ‡å—](tasks/prompting)å¦‚ä½•å……åˆ†åˆ©ç”¨æç¤ºè®¾è®¡ï¼›'
- en: API reference on [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig),
    [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    and [generate-related classes](internal/generation_utils). Most of the classes,
    including the logits processors, have usage examples!
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    ä¸Šçš„ API å‚è€ƒï¼Œ[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)ï¼Œä»¥åŠ
    [generate-related classes](internal/generation_utils)ã€‚å¤§å¤šæ•°ç±»ï¼ŒåŒ…æ‹¬ logits å¤„ç†å™¨ï¼Œéƒ½æœ‰ä½¿ç”¨ç¤ºä¾‹ï¼'
- en: LLM leaderboards
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM æ’è¡Œæ¦œ
- en: '[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    which focuses on the quality of the open-source models;'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ï¼Œä¸“æ³¨äºå¼€æºæ¨¡å‹çš„è´¨é‡ï¼›'
- en: '[Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard),
    which focuses on LLM throughput.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)ï¼Œä¸“æ³¨äº
    LLM ååé‡ã€‚'
- en: Latency, throughput and memory utilization
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å»¶è¿Ÿã€ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡
- en: '[Guide](llm_tutorial_optimization) on how to optimize LLMs for speed and memory;'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Guide](llm_tutorial_optimization) å¦‚ä½•ä¼˜åŒ– LLMs çš„é€Ÿåº¦å’Œå†…å­˜ï¼›'
- en: '[Guide](main_classes/quantization) on quantization such as bitsandbytes and
    autogptq, which shows you how to drastically reduce your memory requirements.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Guide](main_classes/quantization) å…³äºé‡åŒ–ï¼Œå¦‚ bitsandbytes å’Œ autogptqï¼Œå±•ç¤ºäº†å¦‚ä½•å¤§å¹…å‡å°‘å†…å­˜éœ€æ±‚ã€‚'
- en: Related libraries
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç›¸å…³åº“
- en: '[`text-generation-inference`](https://github.com/huggingface/text-generation-inference),
    a production-ready server for LLMs;'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[`text-generation-inference`](https://github.com/huggingface/text-generation-inference)ï¼Œä¸€ä¸ªä¸º
    LLMs å‡†å¤‡çš„ç”Ÿäº§å°±ç»ªæœåŠ¡å™¨ï¼›'
- en: '[`optimum`](https://github.com/huggingface/optimum), an extension of ğŸ¤— Transformers
    that optimizes for specific hardware devices.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[`optimum`](https://github.com/huggingface/optimum)ï¼Œä¸€ä¸ª ğŸ¤— Transformers çš„æ‰©å±•ï¼Œé’ˆå¯¹ç‰¹å®šç¡¬ä»¶è®¾å¤‡è¿›è¡Œä¼˜åŒ–ã€‚'
