- en: Question answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®ç­”
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/question_answering](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/question_answering)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/question_answering](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/question_answering)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/ajPx5LwJD-I](https://www.youtube-nocookie.com/embed/ajPx5LwJD-I)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/ajPx5LwJD-I](https://www.youtube-nocookie.com/embed/ajPx5LwJD-I)'
- en: 'Question answering tasks return an answer given a question. If youâ€™ve ever
    asked a virtual assistant like Alexa, Siri or Google what the weather is, then
    youâ€™ve used a question answering model before. There are two common types of question
    answering tasks:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”ä»»åŠ¡æ ¹æ®é—®é¢˜è¿”å›ç­”æ¡ˆã€‚å¦‚æœæ‚¨æ›¾ç»å‘Alexaã€Siriæˆ–Googleç­‰è™šæ‹ŸåŠ©æ‰‹è¯¢é—®å¤©æ°”æƒ…å†µï¼Œé‚£ä¹ˆæ‚¨ä¹‹å‰ä½¿ç”¨è¿‡é—®ç­”æ¨¡å‹ã€‚é—®ç­”ä»»åŠ¡æœ‰ä¸¤ç§å¸¸è§ç±»å‹ï¼š
- en: 'Extractive: extract the answer from the given context.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æå–å¼ï¼šä»ç»™å®šçš„ä¸Šä¸‹æ–‡ä¸­æå–ç­”æ¡ˆã€‚
- en: 'Abstractive: generate an answer from the context that correctly answers the
    question.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼ï¼šä»ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆä¸€ä¸ªæ­£ç¡®å›ç­”é—®é¢˜çš„ç­”æ¡ˆã€‚
- en: 'This guide will show you how to:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the
    [SQuAD](https://huggingface.co/datasets/squad) dataset for extractive question
    answering.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[SQuAD](https://huggingface.co/datasets/squad)æ•°æ®é›†ä¸Šå¾®è°ƒ[DistilBERT](https://huggingface.co/distilbert-base-uncased)ä»¥è¿›è¡Œæå–å¼é—®ç­”ã€‚
- en: Use your finetuned model for inference.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­æ¼”ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
- en: '[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert),
    [BigBird](../model_doc/big_bird), [BigBird-Pegasus](../model_doc/bigbird_pegasus),
    [BLOOM](../model_doc/bloom), [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine),
    [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text),
    [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert),
    [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ErnieM](../model_doc/ernie_m),
    [Falcon](../model_doc/falcon), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet),
    [Funnel Transformer](../model_doc/funnel), [OpenAI GPT-2](../model_doc/gpt2),
    [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [GPT-J](../model_doc/gptj),
    [I-BERT](../model_doc/ibert), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3),
    [LED](../model_doc/led), [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer),
    [LUKE](../model_doc/luke), [LXMERT](../model_doc/lxmert), [MarkupLM](../model_doc/markuplm),
    [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert),
    [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt),
    [MRA](../model_doc/mra), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha),
    [NystrÃ¶mformer](../model_doc/nystromformer), [OPT](../model_doc/opt), [QDQBert](../model_doc/qdqbert),
    [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta),
    [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert),
    [RoFormer](../model_doc/roformer), [Splinter](../model_doc/splinter), [SqueezeBERT](../model_doc/squeezebert),
    [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta),
    [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod),
    [YOSO](../model_doc/yoso)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert),
    [BigBird](../model_doc/big_bird), [BigBird-Pegasus](../model_doc/bigbird_pegasus),
    [BLOOM](../model_doc/bloom), [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine),
    [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text),
    [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert),
    [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ErnieM](../model_doc/ernie_m),
    [Falcon](../model_doc/falcon), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet),
    [Funnel Transformer](../model_doc/funnel), [OpenAI GPT-2](../model_doc/gpt2),
    [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [GPT-J](../model_doc/gptj),
    [I-BERT](../model_doc/ibert), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3),
    [LED](../model_doc/led), [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer),
    [LUKE](../model_doc/luke), [LXMERT](../model_doc/lxmert), [MarkupLM](../model_doc/markuplm),
    [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert),
    [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt),
    [MRA](../model_doc/mra), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha),
    [NystrÃ¶mformer](../model_doc/nystromformer), [OPT](../model_doc/opt), [QDQBert](../model_doc/qdqbert),
    [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta),
    [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert),
    [RoFormer](../model_doc/roformer), [Splinter](../model_doc/splinter), [SqueezeBERT](../model_doc/squeezebert),
    [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta),
    [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod),
    [YOSO](../model_doc/yoso)'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load SQuAD dataset
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½SQuADæ•°æ®é›†
- en: Start by loading a smaller subset of the SQuAD dataset from the ğŸ¤— Datasets library.
    Thisâ€™ll give you a chance to experiment and make sure everything works before
    spending more time training on the full dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆåŠ è½½æ¥è‡ªğŸ¤—æ•°æ®é›†åº“çš„SQuADæ•°æ®é›†çš„è¾ƒå°å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Split the datasetâ€™s `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)æ–¹æ³•å°†æ•°æ®é›†çš„`train`æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then take a look at an example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åçœ‹ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are several important fields here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰å‡ ä¸ªé‡è¦çš„å­—æ®µï¼š
- en: '`answers`: the starting location of the answer token and the answer text.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answers`ï¼šç­”æ¡ˆæ ‡è®°çš„èµ·å§‹ä½ç½®å’Œç­”æ¡ˆæ–‡æœ¬ã€‚'
- en: '`context`: background information from which the model needs to extract the
    answer.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context`ï¼šæ¨¡å‹éœ€è¦ä»ä¸­æå–ç­”æ¡ˆçš„èƒŒæ™¯ä¿¡æ¯ã€‚'
- en: '`question`: the question a model should answer.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question`ï¼šæ¨¡å‹åº”è¯¥å›ç­”çš„é—®é¢˜ã€‚'
- en: Preprocess
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: '[https://www.youtube-nocookie.com/embed/qgaM0weJHpA](https://www.youtube-nocookie.com/embed/qgaM0weJHpA)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/qgaM0weJHpA](https://www.youtube-nocookie.com/embed/qgaM0weJHpA)'
- en: 'The next step is to load a DistilBERT tokenizer to process the `question` and
    `context` fields:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„æ­¥éª¤æ˜¯åŠ è½½ä¸€ä¸ªDistilBERT tokenizeræ¥å¤„ç†`question`å’Œ`context`å­—æ®µï¼š
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There are a few preprocessing steps particular to question answering tasks
    you should be aware of:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€äº›ç‰¹å®šäºé—®ç­”ä»»åŠ¡çš„é¢„å¤„ç†æ­¥éª¤ï¼Œæ‚¨åº”è¯¥æ³¨æ„ï¼š
- en: Some examples in a dataset may have a very long `context` that exceeds the maximum
    input length of the model. To deal with longer sequences, truncate only the `context`
    by setting `truncation="only_second"`.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¸­çš„ä¸€äº›ç¤ºä¾‹å¯èƒ½å…·æœ‰è¶…å‡ºæ¨¡å‹æœ€å¤§è¾“å…¥é•¿åº¦çš„éå¸¸é•¿çš„`context`ã€‚ä¸ºäº†å¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œåªæˆªæ–­`context`ï¼Œè®¾ç½®`truncation="only_second"`ã€‚
- en: Next, map the start and end positions of the answer to the original `context`
    by setting `return_offset_mapping=True`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œé€šè¿‡è®¾ç½®`return_offset_mapping=True`å°†ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®æ˜ å°„åˆ°åŸå§‹çš„`context`ã€‚
- en: With the mapping in hand, now you can find the start and end tokens of the answer.
    Use the `sequence_ids` method to find which part of the offset corresponds to
    the `question` and which corresponds to the `context`.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ‰äº†æ˜ å°„ï¼Œç°åœ¨æ‚¨å¯ä»¥æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸæ ‡è®°ã€‚ä½¿ç”¨`sequence_ids`æ–¹æ³•æ‰¾åˆ°åç§»çš„å“ªéƒ¨åˆ†å¯¹åº”äº`question`ï¼Œå“ªéƒ¨åˆ†å¯¹åº”äº`context`ã€‚
- en: 'Here is how you can create a function to truncate and map the start and end
    tokens of the `answer` to the `context`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥åˆ›å»ºçš„å‡½æ•°ï¼Œç”¨äºæˆªæ–­å’Œæ˜ å°„`answer`çš„èµ·å§‹å’Œç»“æŸæ ‡è®°åˆ°`context`ï¼š
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    function. You can speed up the `map` function by setting `batched=True` to process
    multiple elements of the dataset at once. Remove any columns you donâ€™t need:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œè¯·ä½¿ç”¨ğŸ¤— Datasets [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®`batched=True`æ¥åŠ é€Ÿ`map`å‡½æ•°ï¼Œä»¥ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ã€‚åˆ é™¤æ‚¨ä¸éœ€è¦çš„ä»»ä½•åˆ—ï¼š
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now create a batch of examples using [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator).
    Unlike other data collators in ğŸ¤— Transformers, the [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)
    does not apply any additional preprocessing such as padding.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚ä¸ğŸ¤—
    Transformersä¸­çš„å…¶ä»–æ•°æ®æ”¶é›†å™¨ä¸åŒï¼Œ[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)ä¸ä¼šåº”ç”¨ä»»ä½•é¢å¤–çš„é¢„å¤„ç†ï¼Œå¦‚å¡«å……ã€‚
- en: PytorchHide Pytorch content
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè— Pytorch å†…å®¹
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: TensorFlowHide TensorFlow content
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè— TensorFlow å†…å®¹
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Train
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: PytorchHide Pytorch content
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè— Pytorch å†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[here](../training#train-with-pytorch-trainer)!
- en: 'Youâ€™re ready to start training your model now! Load DistilBERT with [AutoModelForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForQuestionAnswering):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[AutoModelForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForQuestionAnswering)åŠ è½½DistilBERTï¼š
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'At this point, only three steps remain:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. Youâ€™ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model).
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`ï¼ˆæ‚¨éœ€è¦ç™»å½•åˆ°Hugging
    Faceæ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ï¼Œæ‚¨å°†æŠŠè¿™ä¸ªæ¨¡å‹æ¨é€åˆ°Hubã€‚
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, and data collator.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼ŒåŒæ—¶è¿˜æœ‰æ¨¡å‹ã€æ•°æ®é›†ã€tokenizerå’Œæ•°æ®æ”¶é›†å™¨ã€‚
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: TensorFlowHide TensorFlow content
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè— TensorFlow å†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with Keras, take a look at the
    basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨Keraså¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[here](../training#train-a-tensorflow-model-with-keras)!
- en: 'To finetune a model in TensorFlow, start by setting up an optimizer function,
    learning rate schedule, and some training hyperparameters:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è°ƒåº¦å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then you can load DistilBERT with [TFAutoModelForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForQuestionAnswering):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨[TFAutoModelForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForQuestionAnswering)åŠ è½½DistilBERTï¼š
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Convert your datasets to the `tf.data.Dataset` format with [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ä¸ºè®­ç»ƒé…ç½®æ¨¡å‹ï¼š
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The last thing to setup before you start training is to provide a way to push
    your model to the Hub. This can be done by specifying where to push your model
    and tokenizer in the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰çš„æœ€åä¸€ä»¶äº‹æ˜¯æä¾›ä¸€ç§å°†æ‚¨çš„æ¨¡å‹æ¨é€åˆ°Hubçš„æ–¹æ³•ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)ä¸­æŒ‡å®šæ¨é€æ¨¡å‹å’Œæ ‡è®°å™¨çš„ä½ç½®æ¥å®Œæˆï¼š
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, youâ€™re ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)
    with your training and validation datasets, the number of epochs, and your callback
    to finetune the model:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€æ—¶ä»£æ•°ä»¥åŠå›è°ƒå‡½æ•°è°ƒç”¨[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)æ¥å¾®è°ƒæ¨¡å‹ï¼š
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Once training is completed, your model is automatically uploaded to the Hub
    so everyone can use it!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæ‚¨çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼
- en: For a more in-depth example of how to finetune a model for question answering,
    take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)
    or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£å¦‚ä½•ä¸ºé—®é¢˜å›ç­”å¾®è°ƒæ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„[PyTorchç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)æˆ–[TensorFlowç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ã€‚
- en: Evaluate
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: Evaluation for question answering requires a significant amount of postprocessing.
    To avoid taking up too much of your time, this guide skips the evaluation step.
    The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    still calculates the evaluation loss during training so youâ€™re not completely
    in the dark about your modelâ€™s performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜å›ç­”çš„è¯„ä¼°éœ€è¦å¤§é‡çš„åå¤„ç†ã€‚ä¸ºäº†ä¸å ç”¨å¤ªå¤šæ—¶é—´ï¼Œæœ¬æŒ‡å—è·³è¿‡äº†è¯„ä¼°æ­¥éª¤ã€‚[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä»ç„¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—è¯„ä¼°æŸå¤±ï¼Œå› æ­¤æ‚¨ä¸ä¼šå®Œå…¨ä¸äº†è§£æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: If have more time and youâ€™re interested in how to evaluate your model for question
    answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)
    chapter from the ğŸ¤— Hugging Face Course!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰æ›´å¤šæ—¶é—´ï¼Œå¹¶ä¸”å¯¹å¦‚ä½•è¯„ä¼°é—®é¢˜å›ç­”æ¨¡å‹æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹ğŸ¤— Hugging Faceè¯¾ç¨‹ä¸­çš„[é—®é¢˜å›ç­”](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)ç« èŠ‚ï¼
- en: Inference
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Great, now that youâ€™ve finetuned a model, you can use it for inference!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼
- en: 'Come up with a question and some context youâ€™d like the model to predict:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡ºä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œæ‚¨å¸Œæœ›æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼š
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for question answering with your model, and pass your
    text to it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒè¿‡çš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ä¸­ä½¿ç”¨å®ƒã€‚ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªé—®é¢˜å›ç­”çš„`pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can also manually replicate the results of the `pipeline` if youâ€™d like:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼š
- en: PytorchHide Pytorch content
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—Pytorchå†…å®¹
- en: 'Tokenize the text and return PyTorch tensors:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–å¹¶è¿”å›PyTorchå¼ é‡ï¼š
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Pass your inputs to the model and return the `logits`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š
- en: '[PRE22]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Get the highest probability from the model output for the start and end positions:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹è¾“å‡ºä¸­è·å–å¼€å§‹å’Œç»“æŸä½ç½®çš„æœ€é«˜æ¦‚ç‡ï¼š
- en: '[PRE23]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Decode the predicted tokens to get the answer:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç é¢„æµ‹çš„æ ‡è®°ä»¥è·å–ç­”æ¡ˆï¼š
- en: '[PRE24]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: TensorFlowHide TensorFlow content
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: 'Tokenize the text and return TensorFlow tensors:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–å¹¶è¿”å›TensorFlowå¼ é‡ï¼š
- en: '[PRE25]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Pass your inputs to the model and return the `logits`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š
- en: '[PRE26]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Get the highest probability from the model output for the start and end positions:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹è¾“å‡ºä¸­è·å–å¼€å§‹å’Œç»“æŸä½ç½®çš„æœ€é«˜æ¦‚ç‡ï¼š
- en: '[PRE27]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Decode the predicted tokens to get the answer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç é¢„æµ‹çš„æ ‡è®°ä»¥è·å–ç­”æ¡ˆï¼š
- en: '[PRE28]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
