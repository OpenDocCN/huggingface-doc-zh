- en: Advantage Actor-Critic (A2C)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势演员评论家（A2C）
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/advantage-actor-critic](https://huggingface.co/learn/deep-rl-course/unit6/advantage-actor-critic)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit6/advantage-actor-critic](https://huggingface.co/learn/deep-rl-course/unit6/advantage-actor-critic)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Reducing variance with Actor-Critic methods
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用演员评论家方法减少方差
- en: 'The solution to reducing the variance of the Reinforce algorithm and training
    our agent faster and better is to use a combination of Policy-Based and Value-Based
    methods: *the Actor-Critic method*.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 减少Reinforce算法的方差并更快更好地训练我们的代理的解决方案是使用基于策略和基于价值的方法的组合：*演员评论家方法*。
- en: To understand the Actor-Critic, imagine you’re playing a video game. You can
    play with a friend that will provide you with some feedback. You’re the Actor
    and your friend is the Critic.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解演员评论家，想象一下你在玩一个视频游戏。你可以和一个朋友一起玩，他会给你一些反馈。你是演员，你的朋友是评论家。
- en: '![Actor Critic](../Images/042bd39f9e4b369ec6cfbc9ae67d3829.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![演员评论家](../Images/042bd39f9e4b369ec6cfbc9ae67d3829.png)'
- en: You don’t know how to play at the beginning, **so you try some actions randomly**.
    The Critic observes your action and **provides feedback**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时你不知道如何玩，**所以你随机尝试一些动作**。评论家观察你的动作并**提供反馈**。
- en: Learning from this feedback, **you’ll update your policy and be better at playing
    that game.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个反馈中学习，**你将更新你的策略并在玩游戏时变得更好**。
- en: On the other hand, your friend (Critic) will also update their way to provide
    feedback so it can be better next time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你的朋友（评论家）也会更新他们的方式来提供反馈，以便下次更好。
- en: 'This is the idea behind Actor-Critic. We learn two function approximations:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是演员评论家背后的想法。我们学习两个函数近似：
- en: '*A policy* that **controls how our agent acts**:<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\pi_{\theta}(s)</annotation></semantics></math> πθ​(s)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个控制我们的代理如何行动的策略*：<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\pi_{\theta}(s)</annotation></semantics></math> πθ​(s)'
- en: '*A value function* to assist the policy update by measuring how good the action
    taken is:<math><semantics><mrow><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>w</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow>
    <annotation encoding="application/x-tex">\hat{q}_{w}(s,a)</annotation></semantics></math>
    q^​w​(s,a)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个价值函数*，通过衡量所采取的行动有多好来辅助策略更新：<math><semantics><mrow><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>w</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow>
    <annotation encoding="application/x-tex">\hat{q}_{w}(s,a)</annotation></semantics></math>
    q^​w​(s,a)'
- en: The Actor-Critic Process
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演员评论家过程
- en: Now that we have seen the Actor Critic’s big picture, let’s dive deeper to understand
    how the Actor and Critic improve together during the training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了演员评论家的大局，让我们深入了解在训练过程中演员和评论家如何一起改进。
- en: 'As we saw, with Actor-Critic methods, there are two function approximations
    (two neural networks):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，使用演员评论家方法，有两个函数近似（两个神经网络）：
- en: '*Actor*, a **policy function** parameterized by theta:<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\pi_{\theta}(s)</annotation></semantics></math> πθ​(s)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*演员*，一个由theta参数化的**策略函数**：<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\pi_{\theta}(s)</annotation></semantics></math> πθ​(s)'
- en: '*Critic*, a **value function** parameterized by w:<math><semantics><mrow><msub><mover
    accent="true"><mi>q</mi><mo>^</mo></mover><mi>w</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo
    separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\hat{q}_{w}(s,a)</annotation></semantics></math>
    q^​w​(s,a)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评论家*，一个由w参数化的**价值函数**：<math><semantics><mrow><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>w</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow>
    <annotation encoding="application/x-tex">\hat{q}_{w}(s,a)</annotation></semantics></math>
    q^​w​(s,a)'
- en: 'Let’s see the training process to understand how the Actor and Critic are optimized:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练过程，以了解演员和评论家如何被优化：
- en: At each timestep, t, we get the current state<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">S_t</annotation></semantics></math>St​
    from the environment and **pass it as input through our Actor and Critic**.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个时间步t，我们从环境中得到当前状态<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">S_t</annotation></semantics></math>St​，**通过我们的演员和评论家作为输入传递**。
- en: Our Policy takes the state and **outputs an action** <math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">A_t</annotation></semantics></math> At​.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的策略接受状态并**输出一个动作**<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">A_t</annotation></semantics></math> At​。
- en: '![Step 1 Actor Critic](../Images/5633968bb872e917c968f590412b0f7b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![步骤1 演员评论家](../Images/5633968bb872e917c968f590412b0f7b.png)'
- en: 'The Critic takes that action also as input and, using<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">S_t</annotation></semantics></math>St​
    and<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">A_t</annotation></semantics></math> At​, **computes
    the value of taking that action at that state: the Q-value**.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论家也将该行动作为输入，并使用<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">S_t</annotation></semantics></math>St​和<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">A_t</annotation></semantics></math> At​，**计算在该状态下采取该行动的价值：Q值**。
- en: '![Step 2 Actor Critic](../Images/2240186d96dc80dba4057389b4c239ab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![步骤2 演员评论家](../Images/2240186d96dc80dba4057389b4c239ab.png)'
- en: The action<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">A_t</annotation></semantics></math>At​ performed
    in the environment outputs a new state<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​
    and a reward<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">R_{t+1}</annotation></semantics></math>
    Rt+1​ .
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在环境中执行的动作<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">A_t</annotation></semantics></math>在t+1时刻产生一个新状态<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>和一个奖励<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">R_{t+1}</annotation></semantics></math>。
- en: '![Step 3 Actor Critic](../Images/aca327aae99a5abb4493fc07abd2dd69.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![步骤3 Actor Critic](../Images/aca327aae99a5abb4493fc07abd2dd69.png)'
- en: The Actor updates its policy parameters using the Q value.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员使用Q值更新其策略参数。
- en: '![Step 4 Actor Critic](../Images/cf429723f7f14d366206f1e0c83f52ed.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![步骤4 Actor Critic](../Images/cf429723f7f14d366206f1e0c83f52ed.png)'
- en: Thanks to its updated parameters, the Actor produces the next action to take
    at<math><semantics><mrow><msub><mi>A</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">A_{t+1}</annotation></semantics></math>
    At+1​ given the new state<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>
    St+1​.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于更新的参数，演员产生下一个要在<math><semantics><mrow><msub><mi>A</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">A_{t+1}</annotation></semantics></math>处采取的动作，给定新状态<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>。
- en: The Critic then updates its value parameters.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后评论家更新其价值参数。
- en: '![Step 5 Actor Critic](../Images/ec927ac9ae5bbb27566b494db46568aa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![步骤5 Actor Critic](../Images/ec927ac9ae5bbb27566b494db46568aa.png)'
- en: Adding Advantage in Actor-Critic (A2C)
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Actor-Critic（A2C）中添加优势
- en: We can stabilize learning further by **using the Advantage function as Critic
    instead of the Action value function**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过**使用优势函数作为评论家而不是动作值函数**来进一步稳定学习。
- en: 'The idea is that the Advantage function calculates the relative advantage of
    an action compared to the others possible at a state: **how taking that action
    at a state is better compared to the average value of the state**. It’s subtracting
    the mean value of the state from the state action pair:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，优势函数计算动作相对于在状态可能的其他动作的优势：**在该状态采取该动作相对于该状态的平均值更好的程度**。它从状态动作对中减去了状态的平均值：
- en: '![Advantage Function](../Images/d178c6579c3c840bd90abfd5839b83af.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![优势函数](../Images/d178c6579c3c840bd90abfd5839b83af.png)'
- en: In other words, this function calculates **the extra reward we get if we take
    this action at that state compared to the mean reward we get at that state**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，该函数计算**如果我们在该状态采取此动作与我们在该状态获得的平均奖励相比，我们获得的额外奖励**。
- en: The extra reward is what’s beyond the expected value of that state.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的奖励是超出该状态预期值的部分。
- en: 'If A(s,a) > 0: our gradient is **pushed in that direction**.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果A（s，a）>0：我们的梯度会**被推向那个方向**。
- en: If A(s,a) < 0 (our action does worse than the average value of that state),
    **our gradient is pushed in the opposite direction**.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果A（s，a）<0（我们的动作比该状态的平均值表现更差），**我们的梯度会被推向相反的方向**。
- en: The problem with implementing this advantage function is that it requires two
    value functions — <math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo
    separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">Q(s,a)</annotation></semantics></math>Q(s,a) and
    <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow>
    <annotation encoding="application/x-tex">V(s)</annotation></semantics></math>V(s).
    Fortunately, **we can use the TD error as a good estimator of the advantage function.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这个优势函数的问题在于它需要两个价值函数 — <math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo
    separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">Q(s,a)</annotation></semantics></math>和 <math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">V(s)</annotation></semantics></math>。幸运的是，**我们可以使用TD误差作为优势函数的良好估计器**。
- en: '![Advantage Function](../Images/f524503c61319621c198656df4b56427.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![优势函数](../Images/f524503c61319621c198656df4b56427.png)'
