- en: DeepSpeed Integration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepSpeed é›†æˆ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) implements everything described
    in the [ZeRO paper](https://arxiv.org/abs/1910.02054). Currently it provides full
    support for:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) å®ç°äº† [ZeRO è®ºæ–‡](https://arxiv.org/abs/1910.02054)
    ä¸­æè¿°çš„æ‰€æœ‰å†…å®¹ã€‚ç›®å‰ï¼Œå®ƒå®Œå…¨æ”¯æŒï¼š'
- en: Optimizer state partitioning (ZeRO stage 1)
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†åŒºï¼ˆZeRO é˜¶æ®µ 1ï¼‰
- en: Gradient partitioning (ZeRO stage 2)
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦åˆ†åŒºï¼ˆZeRO é˜¶æ®µ 2ï¼‰
- en: Parameter partitioning (ZeRO stage 3)
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‚æ•°åˆ†åŒºï¼ˆZeRO é˜¶æ®µ 3ï¼‰
- en: Custom mixed precision training handling
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰æ··åˆç²¾åº¦è®­ç»ƒå¤„ç†
- en: A range of fast CUDA-extension-based optimizers
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ç³»åˆ—åŸºäºå¿«é€Ÿ CUDA æ‰©å±•çš„ä¼˜åŒ–å™¨
- en: ZeRO-Offload to CPU and NVMe
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ZeRO-Offload åˆ° CPU å’Œ NVMe
- en: 'ZeRO-Offload has its own dedicated paper: [ZeRO-Offload: Democratizing Billion-Scale
    Model Training](https://arxiv.org/abs/2101.06840). And NVMe-support is described
    in the paper [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep
    Learning](https://arxiv.org/abs/2104.07857).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'ZeRO-Offload æœ‰è‡ªå·±çš„ä¸“ç”¨è®ºæ–‡ï¼š[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)ã€‚NVMe
    æ”¯æŒåœ¨è®ºæ–‡ [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)
    ä¸­æœ‰æè¿°ã€‚'
- en: DeepSpeed ZeRO-2 is primarily used only for training, as its features are of
    no use to inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO-2 ä¸»è¦ä»…ç”¨äºè®­ç»ƒï¼Œå› ä¸ºå…¶ç‰¹æ€§å¯¹æ¨æ–­æ— ç”¨ã€‚
- en: DeepSpeed ZeRO-3 can be used for inference as well, since it allows huge models
    to be loaded on multiple GPUs, which wonâ€™t be possible on a single GPU.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO-3 ä¹Ÿå¯ä»¥ç”¨äºæ¨æ–­ï¼Œå› ä¸ºå®ƒå…è®¸å°†åºå¤§çš„æ¨¡å‹åŠ è½½åˆ°å¤šä¸ª GPU ä¸Šï¼Œè¿™åœ¨å•ä¸ª GPU ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚
- en: 'ğŸ¤— Transformers integrates [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    via 2 options:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformers é€šè¿‡ 2 ä¸ªé€‰é¡¹é›†æˆäº† [DeepSpeed](https://github.com/microsoft/DeepSpeed)ï¼š
- en: Integration of the core DeepSpeed features via [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    This is an everything-done-for-you type of integration - just supply your custom
    config file or use our template and you have nothing else to do. Most of this
    document is focused on this feature.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    é›†æˆæ ¸å¿ƒ DeepSpeed åŠŸèƒ½ã€‚è¿™æ˜¯ä¸€ç§ä¸€åˆ‡éƒ½ä¸ºæ‚¨å®Œæˆçš„é›†æˆæ–¹å¼ - åªéœ€æä¾›æ‚¨çš„è‡ªå®šä¹‰é…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ¿ï¼Œæ‚¨å°±æ— éœ€åšå…¶ä»–äº‹æƒ…ã€‚æœ¬æ–‡æ¡£çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½é›†ä¸­åœ¨è¿™ä¸ªåŠŸèƒ½ä¸Šã€‚
- en: If you donâ€™t use [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and want to use your own Trainer where you integrated DeepSpeed yourself, core
    functionality functions like `from_pretrained` and `from_config` include integration
    of essential parts of DeepSpeed like `zero.Init` for ZeRO stage 3 and higher.
    To tap into this feature read the docs on [non-Trainer DeepSpeed Integration](#nontrainer-deepspeed-integration).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ä½¿ç”¨ [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    å¹¶å¸Œæœ›ä½¿ç”¨è‡ªå·±é›†æˆäº† DeepSpeed çš„ Trainerï¼Œæ ¸å¿ƒåŠŸèƒ½å‡½æ•°å¦‚ `from_pretrained` å’Œ `from_config` åŒ…æ‹¬ DeepSpeed
    çš„å…³é”®éƒ¨åˆ†é›†æˆï¼Œå¦‚ ZeRO é˜¶æ®µ 3 åŠæ›´é«˜ç‰ˆæœ¬çš„ `zero.Init`ã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·é˜…è¯»å…³äº [é Trainer DeepSpeed é›†æˆ](#nontrainer-deepspeed-integration)
    çš„æ–‡æ¡£ã€‚
- en: 'What is integrated:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é›†æˆå†…å®¹ï¼š
- en: 'Training:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒï¼š
- en: DeepSpeed ZeRO training supports the full ZeRO stages 1, 2 and 3 with ZeRO-Infinity
    (CPU and NVME offload).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO è®­ç»ƒæ”¯æŒå®Œæ•´çš„ ZeRO é˜¶æ®µ 1ã€2 å’Œ 3ï¼Œå¸¦æœ‰ ZeRO-Infinityï¼ˆCPU å’Œ NVME å¸è½½ï¼‰ã€‚
- en: 'Inference:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨æ–­ï¼š
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesnâ€™t use an optimizer and a lr scheduler
    and only stage 3 is relevant. For more details see: [zero-inference](#zero-inference).'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO æ¨æ–­æ”¯æŒå¸¦æœ‰ ZeRO-Infinity çš„ ZeRO é˜¶æ®µ 3ã€‚å®ƒä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„ ZeRO åè®®ï¼Œä½†ä¸ä½¿ç”¨ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œåªæœ‰é˜¶æ®µ
    3 ä¸æ¨æ–­ç›¸å…³ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ï¼š[zero-inference](#zero-inference)ã€‚
- en: There is also DeepSpeed Inference - this is a totally different technology which
    uses Tensor Parallelism instead of ZeRO (coming soon).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ DeepSpeed æ¨æ–­ - è¿™æ˜¯ä¸€ç§å®Œå…¨ä¸åŒçš„æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¼ é‡å¹¶è¡Œè€Œä¸æ˜¯ ZeROï¼ˆå³å°†æ¨å‡ºï¼‰ã€‚
- en: Trainer Deepspeed Integration
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Trainer Deepspeed é›†æˆ
- en: Installation
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®‰è£…
- en: 'Install the library via pypi:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ pypi å®‰è£…åº“ï¼š
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'or via `transformers`â€™ `extras`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–é€šè¿‡ `transformers` çš„ `extras`ï¼š
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: or find more details on [the DeepSpeedâ€™s GitHub page](https://github.com/microsoft/deepspeed#installation)
    and [advanced install](https://www.deepspeed.ai/tutorials/advanced-install/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–åœ¨ [DeepSpeed çš„ GitHub é¡µé¢](https://github.com/microsoft/deepspeed#installation)
    å’Œ [é«˜çº§å®‰è£…](https://www.deepspeed.ai/tutorials/advanced-install/) ä¸Šæ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: If youâ€™re still struggling with the build, first make sure to read [CUDA Extension
    Installation Notes](trainer#cuda-extension-installation-notes).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä»åœ¨åŠªåŠ›æ„å»ºï¼Œè¯·é¦–å…ˆç¡®ä¿é˜…è¯» [CUDA æ‰©å±•å®‰è£…è¯´æ˜](trainer#cuda-extension-installation-notes)ã€‚
- en: If you donâ€™t prebuild the extensions and rely on them to be built at run time
    and you tried all of the above solutions to no avail, the next thing to try is
    to pre-build the modules before installing them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ²¡æœ‰é¢„å…ˆæ„å»ºæ‰©å±•å¹¶ä¾èµ–äºè¿è¡Œæ—¶æ„å»ºå®ƒä»¬ï¼Œå¹¶ä¸”å°è¯•äº†ä»¥ä¸Šæ‰€æœ‰è§£å†³æ–¹æ¡ˆä»æ— æ•ˆï¼Œä¸‹ä¸€æ­¥å°è¯•çš„æ˜¯åœ¨å®‰è£…ä¹‹å‰é¢„å…ˆæ„å»ºæ¨¡å—ã€‚
- en: 'To make a local build for DeepSpeed:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¸º DeepSpeed è¿›è¡Œæœ¬åœ°æ„å»ºï¼š
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you intend to use NVMe offload you will also need to include `DS_BUILD_AIO=1`
    in the instructions above (and also install *libaio-dev* system-wide).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ‰“ç®—ä½¿ç”¨ NVMe å¸è½½ï¼Œè¿˜éœ€è¦åœ¨ä¸Šè¿°è¯´æ˜ä¸­åŒ…å« `DS_BUILD_AIO=1`ï¼ˆå¹¶åœ¨ç³»ç»ŸèŒƒå›´å†…å®‰è£… *libaio-dev*ï¼‰ã€‚
- en: 'Edit `TORCH_CUDA_ARCH_LIST` to insert the code for the architectures of the
    GPU cards you intend to use. Assuming all your cards are the same you can get
    the arch via:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–è¾‘ `TORCH_CUDA_ARCH_LIST`ï¼Œæ’å…¥æ‚¨æ‰“ç®—ä½¿ç”¨çš„ GPU æ˜¾å¡çš„æ¶æ„ä»£ç ã€‚å‡è®¾æ‰€æœ‰æ˜¾å¡éƒ½ç›¸åŒï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æ¶æ„ï¼š
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So if you get `8, 6`, then use `TORCH_CUDA_ARCH_LIST="8.6"`. If you have multiple
    different cards, you can list all of them like so `TORCH_CUDA_ARCH_LIST="6.1;8.6"`
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è·å¾—`8, 6`ï¼Œé‚£ä¹ˆè¯·ä½¿ç”¨`TORCH_CUDA_ARCH_LIST="8.6"`ã€‚å¦‚æœæ‚¨æœ‰å¤šå¼ ä¸åŒçš„æ˜¾å¡ï¼Œå¯ä»¥åˆ—å‡ºæ‰€æœ‰æ˜¾å¡ï¼Œä¾‹å¦‚`TORCH_CUDA_ARCH_LIST="6.1;8.6"`ã€‚
- en: 'If you need to use the same setup on multiple machines, make a binary wheel:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨éœ€è¦åœ¨å¤šå°æœºå™¨ä¸Šä½¿ç”¨ç›¸åŒçš„è®¾ç½®ï¼Œè¯·åˆ¶ä½œä¸€ä¸ªäºŒè¿›åˆ¶ wheelï¼š
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: it will generate something like `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`
    which now you can install as `pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`
    locally or on any other machine.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†ç”Ÿæˆç±»ä¼¼äº`dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`çš„å†…å®¹ï¼Œç°åœ¨æ‚¨å¯ä»¥åœ¨æœ¬åœ°æˆ–ä»»ä½•å…¶ä»–æœºå™¨ä¸Šå®‰è£…ä¸º`pip
    install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`ã€‚
- en: Again, remember to ensure to adjust `TORCH_CUDA_ARCH_LIST` to the target architectures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡æé†’ç¡®ä¿è°ƒæ•´`TORCH_CUDA_ARCH_LIST`ä»¥åŒ¹é…ç›®æ ‡æ¶æ„ã€‚
- en: You can find the complete list of NVIDIA GPUs and their corresponding **Compute
    Capabilities** (same as arch in this context) [here](https://developer.nvidia.com/cuda-gpus).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://developer.nvidia.com/cuda-gpus)æ‰¾åˆ°NVIDIA GPUçš„å®Œæ•´åˆ—è¡¨åŠå…¶å¯¹åº”çš„**è®¡ç®—èƒ½åŠ›**ï¼ˆåœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ä¸æ¶æ„ç›¸åŒï¼‰ã€‚
- en: 'You can check the archs pytorch was built with using:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥PyTorchæ„å»ºæ—¶ä½¿ç”¨çš„æ¶æ„ï¼š
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is how to find out the arch for one of the installed GPUs. For example,
    for GPU 0:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å¦‚ä½•æŸ¥æ‰¾å·²å®‰è£…GPUä¹‹ä¸€çš„æ¶æ„ã€‚ä¾‹å¦‚ï¼Œå¯¹äºGPU 0ï¼š
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If the output is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¾“å‡ºæ˜¯ï¼š
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: then you know that this cardâ€™s arch is `8.6`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæ‚¨å°±çŸ¥é“è¿™å¼ å¡çš„æ¶æ„æ˜¯`8.6`ã€‚
- en: You can also leave `TORCH_CUDA_ARCH_LIST` out completely and then the build
    program will automatically query the architecture of the GPUs the build is made
    on. This may or may not match the GPUs on the target machines, thatâ€™s why itâ€™s
    best to specify the desired archs explicitly.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥å®Œå…¨ä¸ä½¿ç”¨`TORCH_CUDA_ARCH_LIST`ï¼Œç„¶åæ„å»ºç¨‹åºå°†è‡ªåŠ¨æŸ¥è¯¢æ„å»ºæ‰€åœ¨çš„GPUçš„æ¶æ„ã€‚è¿™å¯èƒ½ä¸ç›®æ ‡æœºå™¨ä¸Šçš„GPUä¸åŒ¹é…ï¼Œå› æ­¤æœ€å¥½æ˜ç¡®æŒ‡å®šæ‰€éœ€çš„æ¶æ„ã€‚
- en: If after trying everything suggested you still encounter build issues, please,
    proceed with the GitHub Issue of [Deepspeed](https://github.com/microsoft/DeepSpeed/issues),
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°è¯•äº†æ‰€æœ‰å»ºè®®çš„æ–¹æ³•ä»ç„¶é‡åˆ°æ„å»ºé—®é¢˜ï¼Œè¯·ç»§ç»­è¿›è¡Œ[Deepspeed](https://github.com/microsoft/DeepSpeed/issues)çš„GitHubé—®é¢˜å¤„ç†ï¼Œ
- en: Deployment with multiple GPUs
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªGPUè¿›è¡Œéƒ¨ç½²
- en: To deploy the DeepSpeed integration adjust the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments to include a new argument `--deepspeed ds_config.json`,
    where `ds_config.json` is the DeepSpeed configuration file as documented [here](https://www.deepspeed.ai/docs/config-json/).
    The file naming is up to you. Itâ€™s recommended to use DeepSpeedâ€™s `add_config_arguments`
    utility to add the necessary command line arguments to your code. For more information
    please see [DeepSpeedâ€™s Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing)
    doc.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¦éƒ¨ç½²DeepSpeedé›†æˆï¼Œè¯·è°ƒæ•´[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ–°å‚æ•°`--deepspeed
    ds_config.json`ï¼Œå…¶ä¸­`ds_config.json`æ˜¯DeepSpeedé…ç½®æ–‡ä»¶ï¼Œå¦‚[æ­¤å¤„](https://www.deepspeed.ai/docs/config-json/)æ‰€è¿°ã€‚æ–‡ä»¶å‘½åç”±æ‚¨å†³å®šã€‚å»ºè®®ä½¿ç”¨DeepSpeedçš„`add_config_arguments`å®ç”¨ç¨‹åºå‘æ‚¨çš„ä»£ç æ·»åŠ å¿…è¦çš„å‘½ä»¤è¡Œå‚æ•°ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[DeepSpeedçš„å‚æ•°è§£æ](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing)æ–‡æ¡£ã€‚
- en: 'You can use a launcher of your choice here. You can continue using the pytorch
    launcher:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨æ­¤å¤„ä½¿ç”¨æ‚¨é€‰æ‹©çš„å¯åŠ¨å™¨ã€‚æ‚¨å¯ä»¥ç»§ç»­ä½¿ç”¨pytorchå¯åŠ¨å™¨ï¼š
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'or use the launcher provided by `deepspeed`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä½¿ç”¨`deepspeed`æä¾›çš„å¯åŠ¨å™¨ï¼š
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see the arguments arenâ€™t the same, but for most needs either of them
    works. The full details on how to configure various nodes and GPUs can be found
    [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„å‚æ•°ä¸åŒï¼Œä½†å¯¹äºå¤§å¤šæ•°éœ€æ±‚ï¼Œä»»ä½•ä¸€ä¸ªéƒ½å¯ä»¥ã€‚æœ‰å…³å¦‚ä½•é…ç½®å„ä¸ªèŠ‚ç‚¹å’ŒGPUçš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤å¤„](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)ã€‚
- en: When you use the `deepspeed` launcher and you want to use all available gpus
    you can just omit the `--num_gpus` flag.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨ä½¿ç”¨`deepspeed`å¯åŠ¨å™¨å¹¶ä¸”å¸Œæœ›ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„GPUæ—¶ï¼Œæ‚¨å¯ä»¥åªçœç•¥`--num_gpus`æ ‡å¿—ã€‚
- en: 'Here is an example of running `run_translation.py` under DeepSpeed deploying
    all available GPUs:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨DeepSpeedä¸‹éƒ¨ç½²æ‰€æœ‰å¯ç”¨GPUè¿è¡Œ`run_translation.py`çš„ç¤ºä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that in the DeepSpeed documentation you are likely to see `--deepspeed
    --deepspeed_config ds_config.json` - i.e. two DeepSpeed-related arguments, but
    for the sake of simplicity, and since there are already so many arguments to deal
    with, we combined the two into a single argument.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨DeepSpeedæ–‡æ¡£ä¸­ï¼Œæ‚¨å¯èƒ½ä¼šçœ‹åˆ°`--deepspeed --deepspeed_config ds_config.json` - å³ä¸¤ä¸ªä¸DeepSpeedç›¸å…³çš„å‚æ•°ï¼Œä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œå¹¶ä¸”å·²ç»æœ‰å¾ˆå¤šå‚æ•°è¦å¤„ç†ï¼Œæˆ‘ä»¬å°†ä¸¤è€…åˆå¹¶ä¸ºä¸€ä¸ªå‚æ•°ã€‚
- en: For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³ä¸€äº›å®é™…ç”¨ä¾‹ç¤ºä¾‹ï¼Œè¯·å‚é˜…æ­¤[å¸–å­](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400)ã€‚
- en: Deployment with one GPU
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å•ä¸ªGPUè¿›è¡Œéƒ¨ç½²
- en: 'To deploy DeepSpeed with one GPU adjust the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å•ä¸ªGPUéƒ¨ç½²DeepSpeedæ—¶ï¼Œè¯·è°ƒæ•´[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å¦‚ä¸‹ï¼š
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly
    to use just one GPU via `--num_gpus=1`. By default, DeepSpeed deploys all GPUs
    it can see on the given node. If you have only 1 GPU to start with, then you donâ€™t
    need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)
    discusses the launcher options.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸å¤šGPUå‡ ä¹ç›¸åŒï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬æ˜ç¡®å‘Šè¯‰DeepSpeedä»…ä½¿ç”¨ä¸€ä¸ªGPUé€šè¿‡`--num_gpus=1`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeedéƒ¨ç½²ç»™å®šèŠ‚ç‚¹ä¸Šå¯ä»¥çœ‹åˆ°çš„æ‰€æœ‰GPUã€‚å¦‚æœæ‚¨ä¸€å¼€å§‹åªæœ‰1ä¸ªGPUï¼Œåˆ™ä¸éœ€è¦æ­¤å‚æ•°ã€‚ä»¥ä¸‹[æ–‡æ¡£](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)è®¨è®ºäº†å¯åŠ¨å™¨é€‰é¡¹ã€‚
- en: Why would you want to use DeepSpeed with just one GPU?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦ä»…ä½¿ç”¨ä¸€ä¸ªGPUæ¥ä½¿ç”¨DeepSpeedï¼Ÿ
- en: It has a ZeRO-offload feature which can delegate some computations and memory
    to the hostâ€™s CPU and RAM, and thus leave more GPU resources for modelâ€™s needs
    - e.g. larger batch size, or enabling a fitting of a very big model which normally
    wonâ€™t fit.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒå…·æœ‰ZeRO-offloadåŠŸèƒ½ï¼Œå¯ä»¥å°†ä¸€äº›è®¡ç®—å’Œå†…å­˜å§”æ‰˜ç»™ä¸»æœºçš„CPUå’ŒRAMï¼Œä»è€Œä¸ºæ¨¡å‹çš„éœ€æ±‚ç•™ä¸‹æ›´å¤šçš„GPUèµ„æº - ä¾‹å¦‚æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œæˆ–è€…å¯ç”¨ä¸€ä¸ªé€šå¸¸æ— æ³•é€‚åº”çš„éå¸¸å¤§çš„æ¨¡å‹ã€‚
- en: It provides a smart GPU memory management system, that minimizes memory fragmentation,
    which again allows you to fit bigger models and data batches.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒæä¾›äº†ä¸€ä¸ªæ™ºèƒ½çš„GPUå†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œå¯ä»¥æœ€å°åŒ–å†…å­˜ç¢ç‰‡åŒ–ï¼Œè¿™æ ·å¯ä»¥ä½¿æ‚¨é€‚åº”æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®æ‰¹æ¬¡ã€‚
- en: 'While we are going to discuss the configuration in details next, the key to
    getting a huge improvement on a single GPU with DeepSpeed is to have at least
    the following configuration in the configuration file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥è¯¦ç»†è®¨è®ºé…ç½®ï¼Œä½†åœ¨DeepSpeedä¸­è·å¾—å•ä¸ªGPUä¸Šçš„å·¨å¤§æ”¹è¿›çš„å…³é”®æ˜¯è‡³å°‘åœ¨é…ç½®æ–‡ä»¶ä¸­å…·æœ‰ä»¥ä¸‹é…ç½®ï¼š
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: which enables optimizer offload and some other important features. You may experiment
    with the buffer sizes, you will find more details in the discussion below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥å¯ç”¨ä¼˜åŒ–å™¨å¸è½½å’Œä¸€äº›å…¶ä»–é‡è¦åŠŸèƒ½ã€‚æ‚¨å¯ä»¥å°è¯•ä¸åŒçš„ç¼“å†²åŒºå¤§å°ï¼Œåœ¨ä¸‹é¢çš„è®¨è®ºä¸­ä¼šæ‰¾åˆ°æ›´å¤šç»†èŠ‚ã€‚
- en: For a practical usage example of this type of deployment, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ­¤ç±»å‹éƒ¨ç½²çš„å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·å‚è§æ­¤[å¸–å­](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685)ã€‚
- en: You may also try the ZeRO-3 with CPU and NVMe offload as explained further in
    this document.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å°è¯•ä½¿ç”¨CPUå’ŒNVMeå¸è½½çš„ZeRO-3ï¼Œå¦‚æœ¬æ–‡æ¡£ä¸­è¿›ä¸€æ­¥è§£é‡Šçš„é‚£æ ·ã€‚
- en: 'Notes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨ï¼š
- en: 'if you need to run on a specific GPU, which is different from GPU 0, you canâ€™t
    use `CUDA_VISIBLE_DEVICES` to limit the visible scope of available GPUs. Instead,
    you have to use the following syntax:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦åœ¨ç‰¹å®šGPUä¸Šè¿è¡Œï¼Œè€Œä¸æ˜¯GPU 0ï¼Œæ‚¨ä¸èƒ½ä½¿ç”¨`CUDA_VISIBLE_DEVICES`æ¥é™åˆ¶å¯ç”¨GPUçš„å¯è§èŒƒå›´ã€‚ç›¸åï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ä»¥ä¸‹è¯­æ³•ï¼š
- en: '[PRE13]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example, we tell DeepSpeed to use GPU 1 (second gpu).
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å‘Šè¯‰DeepSpeedä½¿ç”¨GPU 1ï¼ˆç¬¬äºŒä¸ªGPUï¼‰ã€‚
- en: Deployment with multiple Nodes
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šèŠ‚ç‚¹éƒ¨ç½²
- en: The information in this section isnâ€™t not specific to the DeepSpeed integration
    and is applicable to any multi-node program. But DeepSpeed provides a `deepspeed`
    launcher that is easier to use than other launchers unless you are in a SLURM
    environment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä¸­çš„ä¿¡æ¯ä¸æ˜¯ç‰¹å®šäºDeepSpeedé›†æˆçš„ï¼Œé€‚ç”¨äºä»»ä½•å¤šèŠ‚ç‚¹ç¨‹åºã€‚ä½†DeepSpeedæä¾›äº†ä¸€ä¸ªæ¯”å…¶ä»–å¯åŠ¨å™¨æ›´å®¹æ˜“ä½¿ç”¨çš„`deepspeed`å¯åŠ¨å™¨ï¼Œé™¤éæ‚¨åœ¨SLURMç¯å¢ƒä¸­ã€‚
- en: For the duration of this section letâ€™s assume that you have 2 nodes with 8 gpus
    each. And you can reach the first node with `ssh hostname1` and second node with
    `ssh hostname2`, and both must be able to reach each other via ssh locally without
    a password. Of course, you will need to rename these host (node) names to the
    actual host names you are working with.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚çš„æŒç»­æ—¶é—´å†…ï¼Œè®©æˆ‘ä»¬å‡è®¾æ‚¨æœ‰2ä¸ªæ¯ä¸ª8ä¸ªGPUçš„èŠ‚ç‚¹ã€‚æ‚¨å¯ä»¥é€šè¿‡`ssh hostname1`åˆ°è¾¾ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé€šè¿‡`ssh hostname2`åˆ°è¾¾ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼Œå¹¶ä¸”ä¸¤ä¸ªèŠ‚ç‚¹å¿…é¡»èƒ½å¤Ÿé€šè¿‡æœ¬åœ°sshæ— å¯†ç åœ°ç›¸äº’åˆ°è¾¾ã€‚å½“ç„¶ï¼Œæ‚¨éœ€è¦å°†è¿™äº›ä¸»æœºï¼ˆèŠ‚ç‚¹ï¼‰åç§°é‡å‘½åä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„å®é™…ä¸»æœºåç§°ã€‚
- en: The torch.distributed.run(torchrun) launcher
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: torch.distributed.run(torchrun)å¯åŠ¨å™¨
- en: 'For example, to use `torch.distributed.run`, you could do:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨`torch.distributed.run`ï¼Œæ‚¨å¯ä»¥è¿™æ ·åšï¼š
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You have to ssh to each node and run this same command on each one of them!
    There is no rush, the launcher will wait until both nodes will synchronize.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¿…é¡»sshåˆ°æ¯ä¸ªèŠ‚ç‚¹å¹¶åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œç›¸åŒçš„å‘½ä»¤ï¼ä¸ç”¨ç€æ€¥ï¼Œå¯åŠ¨å™¨ä¼šç­‰å¾…ç›´åˆ°ä¸¤ä¸ªèŠ‚ç‚¹åŒæ­¥ã€‚
- en: For more information please see [torchrun](https://pytorch.org/docs/stable/elastic/run.html).
    Incidentally, this is also the launcher that replaced `torch.distributed.launch`
    a few pytorch versions back.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[torchrun](https://pytorch.org/docs/stable/elastic/run.html)ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œè¿™ä¹Ÿæ˜¯å‡ ä¸ªpytorchç‰ˆæœ¬å‰æ›¿ä»£äº†`torch.distributed.launch`çš„å¯åŠ¨å™¨ã€‚
- en: The deepspeed launcher
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: deepspeedå¯åŠ¨å™¨
- en: 'To use the `deepspeed` launcher instead, you have to first create a `hostfile`
    file:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨`deepspeed`å¯åŠ¨å™¨ï¼Œæ‚¨é¦–å…ˆéœ€è¦åˆ›å»ºä¸€ä¸ª`hostfile`æ–‡ä»¶ï¼š
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'and then you can launch it as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨ï¼š
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unlike the `torch.distributed.run` launcher, `deepspeed` will automatically
    launch this command on both nodes!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸`torch.distributed.run`å¯åŠ¨å™¨ä¸åŒï¼Œ`deepspeed`å°†è‡ªåŠ¨åœ¨ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šå¯åŠ¨æ­¤å‘½ä»¤ï¼
- en: For more information please see [Resource Configuration (multi-node)](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[èµ„æºé…ç½®ï¼ˆå¤šèŠ‚ç‚¹ï¼‰](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)ã€‚
- en: Launching in a SLURM environment
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åœ¨SLURMç¯å¢ƒä¸­å¯åŠ¨
- en: In the SLURM environment the following approach can be used. The following is
    a slurm script `launch.slurm` which you will need to adapt it to your specific
    SLURM environment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨SLURMç¯å¢ƒä¸­å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªslurmè„šæœ¬`launch.slurm`ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‚¨ç‰¹å®šçš„SLURMç¯å¢ƒè¿›è¡Œè°ƒæ•´ã€‚
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'All is left is to schedule it to run:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä¸‹çš„å°±æ˜¯å®‰æ’å®ƒè¿è¡Œï¼š
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`srun` will take care of launching the program simultaneously on all nodes.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`srun`å°†è´Ÿè´£åŒæ—¶åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šå¯åŠ¨ç¨‹åºã€‚'
- en: Use of Non-shared filesystem
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: éå…±äº«æ–‡ä»¶ç³»ç»Ÿçš„ä½¿ç”¨
- en: 'By default DeepSpeed expects that a multi-node environment uses a shared storage.
    If this is not the case and each node can only see the local filesystem, you need
    to adjust the config file to include a [`checkpoint`_section](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)
    with the following setting:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeedæœŸæœ›å¤šèŠ‚ç‚¹ç¯å¢ƒä½¿ç”¨å…±äº«å­˜å‚¨ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œæ¯ä¸ªèŠ‚ç‚¹åªèƒ½çœ‹åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œæ‚¨éœ€è¦è°ƒæ•´é…ç½®æ–‡ä»¶ä»¥åŒ…å«ä¸€ä¸ª[`checkpoint`_section](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)ï¼Œè®¾ç½®å¦‚ä¸‹ï¼š
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Alternatively, you can also use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)â€™s
    `--save_on_each_node` argument, and the above config will be added automatically
    for you.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)çš„`--save_on_each_node`å‚æ•°ï¼Œä¸Šè¿°é…ç½®å°†è‡ªåŠ¨æ·»åŠ ç»™æ‚¨ã€‚
- en: Deployment in Notebooks
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¬”è®°æœ¬ä¸­çš„éƒ¨ç½²
- en: The problem with running notebook cells as a script is that there is no normal
    `deepspeed` launcher to rely on, so under certain setups we have to emulate it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç¬”è®°æœ¬å•å…ƒæ ¼ä½œä¸ºè„šæœ¬è¿è¡Œçš„é—®é¢˜åœ¨äºæ²¡æœ‰æ­£å¸¸çš„`deepspeed`å¯åŠ¨å™¨å¯ä¾›ä¾èµ–ï¼Œå› æ­¤åœ¨æŸäº›è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬å¿…é¡»æ¨¡æ‹Ÿå®ƒã€‚
- en: If youâ€™re using only 1 GPU, here is how youâ€™d have to adjust your training code
    in the notebook to use DeepSpeed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åªä½¿ç”¨1ä¸ªGPUï¼Œä»¥ä¸‹æ˜¯æ‚¨å¿…é¡»è°ƒæ•´ç¬”è®°æœ¬ä¸­çš„è®­ç»ƒä»£ç ä»¥ä½¿ç”¨DeepSpeedçš„æ–¹å¼ã€‚
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note: `...` stands for the normal arguments that youâ€™d pass to the functions.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼š`...`ä»£è¡¨æ‚¨å°†ä¼ é€’ç»™å‡½æ•°çš„å¸¸è§„å‚æ•°ã€‚
- en: If you want to use more than 1 GPU, you must use a multi-process environment
    for DeepSpeed to work. That is, you have to use the launcher for that purpose
    and this cannot be accomplished by emulating the distributed environment presented
    at the beginning of this section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¦ä½¿ç”¨å¤šä¸ªGPUï¼Œå¿…é¡»ä½¿ç”¨å¤šè¿›ç¨‹ç¯å¢ƒæ‰èƒ½ä½¿DeepSpeedæ­£å¸¸å·¥ä½œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨è¯¥ç›®çš„çš„å¯åŠ¨å™¨ï¼Œè€Œä¸èƒ½é€šè¿‡æ¨¡æ‹Ÿæœ¬èŠ‚å¼€å¤´ä»‹ç»çš„åˆ†å¸ƒå¼ç¯å¢ƒæ¥å®ç°ã€‚
- en: 'If you want to create the config file on the fly in the notebook in the current
    directory, you could have a dedicated cell with:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³åœ¨å½“å‰ç›®å½•çš„ç¬”è®°æœ¬ä¸­å³æ—¶åˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ä¸“ç”¨å•å…ƒæ ¼ï¼š
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If the training script is in a normal file and not in the notebook cells, you
    can launch `deepspeed` normally via shell from a cell. For example, to use `run_translation.py`
    you would launch it with:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè®­ç»ƒè„šæœ¬åœ¨æ™®é€šæ–‡ä»¶ä¸­è€Œä¸æ˜¯åœ¨ç¬”è®°æœ¬å•å…ƒæ ¼ä¸­ï¼Œæ‚¨å¯ä»¥ä»å•å…ƒæ ¼ä¸­æ­£å¸¸å¯åŠ¨`deepspeed`ã€‚ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨`run_translation.py`ï¼Œæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨å®ƒï¼š
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'or with `%%bash` magic, where you can write a multi-line code for the shell
    program to run:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä½¿ç”¨`%%bash`é­”æœ¯ï¼Œæ‚¨å¯ä»¥ç¼–å†™å¤šè¡Œä»£ç ä¾›shellç¨‹åºè¿è¡Œï¼š
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In such case you donâ€™t need any of the code presented at the beginning of this
    section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¸éœ€è¦æœ¬èŠ‚å¼€å¤´å‘ˆç°çš„ä»»ä½•ä»£ç ã€‚
- en: 'Note: While `%%bash` magic is neat, but currently it buffers the output so
    you wonâ€™t see the logs until the process completes.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šè™½ç„¶`%%bash`é­”æœ¯å¾ˆå¥½ï¼Œä½†ç›®å‰å®ƒä¼šç¼“å†²è¾“å‡ºï¼Œå› æ­¤åœ¨è¿›ç¨‹å®Œæˆä¹‹å‰æ‚¨çœ‹ä¸åˆ°æ—¥å¿—ã€‚
- en: Configuration
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é…ç½®
- en: For the complete guide to the DeepSpeed configuration options that can be used
    in its configuration file please refer to the [following documentation](https://www.deepspeed.ai/docs/config-json/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³DeepSpeedé…ç½®æ–‡ä»¶ä¸­å¯ç”¨çš„DeepSpeedé…ç½®é€‰é¡¹çš„å®Œæ•´æŒ‡å—ï¼Œè¯·å‚é˜…[ä»¥ä¸‹æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/)ã€‚
- en: 'You can find dozens of DeepSpeed configuration examples that address various
    practical needs in [the DeepSpeedExamples repo](https://github.com/microsoft/DeepSpeedExamples):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[DeepSpeedExampleså­˜å‚¨åº“](https://github.com/microsoft/DeepSpeedExamples)ä¸­æ‰¾åˆ°æ•°åä¸ªè§£å†³å„ç§å®é™…éœ€æ±‚çš„DeepSpeedé…ç½®ç¤ºä¾‹ï¼š
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Continuing the code from above, letâ€™s say youâ€™re looking to configure the Lamb
    optimizer. So you can search through the example `.json` files with:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç»§ç»­ä¸Šé¢çš„ä»£ç ï¼Œå‡è®¾æ‚¨æƒ³é…ç½®Lambä¼˜åŒ–å™¨ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥æœç´¢ç¤ºä¾‹`.json`æ–‡ä»¶ï¼š '
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Some more examples are to be found in the [main repo](https://github.com/microsoft/DeepSpeed)
    as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ä¸»å­˜å‚¨åº“](https://github.com/microsoft/DeepSpeed)ä¸­è¿˜å¯ä»¥æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚
- en: When using DeepSpeed you always need to supply a DeepSpeed configuration file,
    yet some configuration parameters have to be configured via the command line.
    You will find the nuances in the rest of this guide.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DeepSpeedæ—¶ï¼Œæ‚¨å§‹ç»ˆéœ€è¦æä¾›ä¸€ä¸ªDeepSpeedé…ç½®æ–‡ä»¶ï¼Œä½†æ˜¯æŸäº›é…ç½®å‚æ•°å¿…é¡»é€šè¿‡å‘½ä»¤è¡Œè¿›è¡Œé…ç½®ã€‚æ‚¨å°†åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ä¸­æ‰¾åˆ°ç»†å¾®å·®åˆ«ã€‚
- en: 'To get an idea of what DeepSpeed configuration file looks like, here is one
    that activates ZeRO stage 2 features, including optimizer states cpu offload,
    uses `AdamW` optimizer and `WarmupLR` scheduler and will enable mixed precision
    training if `--fp16` is passed:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£DeepSpeedé…ç½®æ–‡ä»¶çš„å¤–è§‚ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªæ¿€æ´»ZeROé˜¶æ®µ2åŠŸèƒ½çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨çŠ¶æ€cpuå¸è½½ï¼Œä½¿ç”¨`AdamW`ä¼˜åŒ–å™¨å’Œ`WarmupLR`è°ƒåº¦ç¨‹åºï¼Œå¹¶ä¸”å¦‚æœä¼ é€’äº†`--fp16`ï¼Œå°†å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When you execute the program, DeepSpeed will log the configuration it received
    from the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    to the console, so you can see exactly what was the final configuration passed
    to it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‚¨æ‰§è¡Œç¨‹åºæ—¶ï¼ŒDeepSpeedå°†è®°å½•ä»[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ¥æ”¶åˆ°çš„é…ç½®åˆ°æ§åˆ¶å°ï¼Œå› æ­¤æ‚¨å¯ä»¥çœ‹åˆ°æœ€ç»ˆä¼ é€’ç»™å®ƒçš„é…ç½®ã€‚
- en: Passing Configuration
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼ é€’é…ç½®
- en: As discussed in this document normally the DeepSpeed configuration is passed
    as a path to a json file, but if youâ€™re not using the command line interface to
    configure the training, and instead instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    via [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    then for the `deepspeed` argument you can pass a nested `dict`. This allows you
    to create the configuration on the fly and doesnâ€™t require you to write it to
    the file system before passing it to [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ¬æ–‡æ‰€è¿°ï¼Œé€šå¸¸å°†DeepSpeedé…ç½®ä½œä¸ºjsonæ–‡ä»¶çš„è·¯å¾„ä¼ é€’ï¼Œä½†å¦‚æœæ‚¨ä¸ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢é…ç½®è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å®ä¾‹åŒ–[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œé‚£ä¹ˆå¯¹äº`deepspeed`å‚æ•°ï¼Œæ‚¨å¯ä»¥ä¼ é€’ä¸€ä¸ªåµŒå¥—çš„`dict`ã€‚è¿™å…è®¸æ‚¨å³æ—¶åˆ›å»ºé…ç½®ï¼Œè€Œæ— éœ€å°†å…¶å†™å…¥æ–‡ä»¶ç³»ç»Ÿåå†ä¼ é€’ç»™[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ã€‚
- en: 'To summarize you can do:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'or:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼š
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Shared Configuration
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…±äº«é…ç½®
- en: This section is a must-read
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€éƒ¨åˆ†æ˜¯å¿…è¯»çš„
- en: Some configuration values are required by both the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and DeepSpeed to function correctly, therefore, to prevent conflicting definitions,
    which could lead to hard to detect errors, we chose to configure those via the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æŸäº›é…ç½®å€¼å¯¹äº[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å’ŒDeepSpeedçš„æ­£ç¡®è¿è¡Œéƒ½æ˜¯å¿…éœ€çš„ï¼Œå› æ­¤ï¼Œä¸ºäº†é˜²æ­¢å†²çªçš„å®šä¹‰ï¼Œå¯èƒ½å¯¼è‡´éš¾ä»¥æ£€æµ‹çš„é”™è¯¯ï¼Œæˆ‘ä»¬é€‰æ‹©é€šè¿‡[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°è¿›è¡Œé…ç½®ã€‚
- en: Additionally, some configuration values are derived automatically based on the
    modelâ€™s configuration, so instead of remembering to manually adjust multiple values,
    itâ€™s the best to let the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    do the majority of configuration for you.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸€äº›é…ç½®å€¼æ˜¯æ ¹æ®æ¨¡å‹çš„é…ç½®è‡ªåŠ¨æ´¾ç”Ÿçš„ï¼Œå› æ­¤ï¼Œä¸å…¶è®°ä½æ‰‹åŠ¨è°ƒæ•´å¤šä¸ªå€¼ï¼Œä¸å¦‚è®©[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä¸ºæ‚¨å®Œæˆå¤§éƒ¨åˆ†é…ç½®ã€‚
- en: 'Therefore, in the rest of this guide you will find a special configuration
    value: `auto`, which when set will be automatically replaced with the correct
    or most efficient value. Please feel free to choose to ignore this recommendation
    and set the values explicitly, in which case be very careful that your the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    arguments and DeepSpeed configurations agree. For example, are you using the same
    learning rate, or batch size, or gradient accumulation settings? if these mismatch
    the training may fail in very difficult to detect ways. You have been warned.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†æ‰¾åˆ°ä¸€ä¸ªç‰¹æ®Šçš„é…ç½®å€¼ï¼š`auto`ï¼Œè®¾ç½®åå°†è‡ªåŠ¨æ›¿æ¢ä¸ºæ­£ç¡®æˆ–æœ€æœ‰æ•ˆçš„å€¼ã€‚è¯·éšæ„é€‰æ‹©å¿½ç•¥æ­¤å»ºè®®å¹¶æ˜¾å¼è®¾ç½®å€¼ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·éå¸¸å°å¿ƒï¼Œç¡®ä¿æ‚¨çš„[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‚æ•°å’ŒDeepSpeedé…ç½®ä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œæ‚¨æ˜¯å¦ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°æˆ–æ¢¯åº¦ç´¯ç§¯è®¾ç½®ï¼Ÿå¦‚æœè¿™äº›ä¸åŒ¹é…ï¼Œè®­ç»ƒå¯èƒ½ä¼šä»¥éå¸¸éš¾ä»¥æ£€æµ‹çš„æ–¹å¼å¤±è´¥ã€‚æ‚¨å·²ç»è¢«è­¦å‘Šäº†ã€‚
- en: There are multiple other values that are specific to DeepSpeed-only and those
    you will have to set manually to suit your needs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å¤šä¸ªå…¶ä»–å€¼æ˜¯ä¸“é—¨é’ˆå¯¹DeepSpeedçš„ï¼Œæ‚¨å°†éœ€è¦æ‰‹åŠ¨è®¾ç½®ä»¥æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚
- en: 'In your own programs, you can also use the following approach if youâ€™d like
    to modify the DeepSpeed config as a master and configure [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    based on that. The steps are:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨è‡ªå·±çš„ç¨‹åºä¸­ï¼Œå¦‚æœæ‚¨æƒ³è¦ä»¥ä¸»æ§çš„æ–¹å¼ä¿®æ”¹DeepSpeedé…ç½®å¹¶åŸºäºæ­¤é…ç½®[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š
- en: Create or load the DeepSpeed configuration to be used as a master configuration
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæˆ–åŠ è½½è¦ç”¨ä½œä¸»é…ç½®çš„DeepSpeedé…ç½®
- en: Create the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object based on these values
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŸºäºè¿™äº›å€¼åˆ›å»º[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å¯¹è±¡
- en: Do note that some values, such as `scheduler.params.total_num_steps` are calculated
    by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    during `train`, but you can of course do the math yourself.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸€äº›å€¼ï¼Œä¾‹å¦‚`scheduler.params.total_num_steps`æ˜¯ç”±[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)åœ¨`train`æœŸé—´è®¡ç®—çš„ï¼Œä½†æ‚¨å½“ç„¶ä¹Ÿå¯ä»¥è‡ªå·±è¿›è¡Œè®¡ç®—ã€‚
- en: ZeRO
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeRO
- en: '[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/)
    is the workhorse of DeepSpeed. It supports 3 different levels (stages) of optimization.
    The first one is not quite interesting for scalability purposes, therefore this
    document focuses on stages 2 and 3\. Stage 3 is further improved by the latest
    addition of ZeRO-Infinity. You will find more indepth information in the DeepSpeed
    documentation.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/)
    æ˜¯DeepSpeedçš„ä¸»è¦å·¥å…·ã€‚å®ƒæ”¯æŒ3ä¸ªä¸åŒçº§åˆ«ï¼ˆé˜¶æ®µï¼‰çš„ä¼˜åŒ–ã€‚ç¬¬ä¸€ä¸ªå¯¹äºå¯ä¼¸ç¼©æ€§ç›®çš„å¹¶ä¸å¤ªæœ‰è¶£ï¼Œå› æ­¤æœ¬æ–‡æ¡£ä¾§é‡äºé˜¶æ®µ2å’Œ3ã€‚é˜¶æ®µ3é€šè¿‡æœ€æ–°çš„ZeRO-Infinityè¿›ä¸€æ­¥æ”¹è¿›ã€‚æ‚¨å¯ä»¥åœ¨DeepSpeedæ–‡æ¡£ä¸­æ‰¾åˆ°æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚'
- en: The `zero_optimization` section of the configuration file is the most important
    part ([docs](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)),
    since that is where you define which ZeRO stages you want to enable and how to
    configure them. You will find the explanation for each parameter in the DeepSpeed
    docs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®æ–‡ä»¶ä¸­çš„`zero_optimization`éƒ¨åˆ†æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ˆ[æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)ï¼‰ï¼Œå› ä¸ºåœ¨é‚£é‡Œæ‚¨å®šä¹‰äº†è¦å¯ç”¨å“ªäº›ZeROé˜¶æ®µä»¥åŠå¦‚ä½•é…ç½®å®ƒä»¬ã€‚æ‚¨å¯ä»¥åœ¨DeepSpeedæ–‡æ¡£ä¸­æ‰¾åˆ°æ¯ä¸ªå‚æ•°çš„è§£é‡Šã€‚
- en: This section has to be configured exclusively via DeepSpeed configuration -
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    provides no equivalent command line arguments.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤éƒ¨åˆ†å¿…é¡»é€šè¿‡DeepSpeedé…ç½®è¿›è¡Œç‹¬å é…ç½® - [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    ä¸æä¾›ç­‰æ•ˆçš„å‘½ä»¤è¡Œå‚æ•°ã€‚
- en: 'Note: currently DeepSpeed doesnâ€™t validate parameter names, so if you misspell
    any, itâ€™ll use the default setting for the parameter that got misspelled. You
    can watch the DeepSpeed engine start up log messages to see what values it is
    going to use.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šç›®å‰DeepSpeedä¸éªŒè¯å‚æ•°åç§°ï¼Œå› æ­¤å¦‚æœæ‚¨æ‹¼å†™é”™è¯¯ï¼Œå®ƒå°†ä½¿ç”¨æ‹¼å†™é”™è¯¯çš„å‚æ•°çš„é»˜è®¤è®¾ç½®ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹DeepSpeedå¼•æ“å¯åŠ¨æ—¥å¿—æ¶ˆæ¯ï¼Œä»¥æŸ¥çœ‹å®ƒå°†ä½¿ç”¨å“ªäº›å€¼ã€‚
- en: ZeRO-2 Config
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-2é…ç½®
- en: 'The following is an example of configuration for ZeRO stage 2:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ZeROé˜¶æ®µ2çš„é…ç½®ç¤ºä¾‹ï¼š
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Performance tuning:**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€§èƒ½è°ƒä¼˜ï¼š**'
- en: 'enabling `offload_optimizer` should reduce GPU RAM usage (it requires `"stage":
    2`)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¯ç”¨`offload_optimizer`åº”è¯¥å‡å°‘GPU RAMçš„ä½¿ç”¨ï¼ˆéœ€è¦`"stage": 2`ï¼‰'
- en: '`"overlap_comm": true` trades off increased GPU RAM usage to lower all-reduce
    latency. `overlap_comm` uses 4.5x the `allgather_bucket_size` and `reduce_bucket_size`
    values. So if they are set to 5e8, this requires a 9GB footprint (`5e8 x 2Bytes
    x 2 x 4.5`). Therefore, if you have a GPU with 8GB or less RAM, to avoid getting
    OOM-errors you will need to reduce those parameters to about `2e8`, which would
    require 3.6GB. You will want to do the same on larger capacity GPU as well, if
    youâ€™re starting to hit OOM.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"overlap_comm": true` é€šè¿‡å¢åŠ GPU RAMä½¿ç”¨é‡æ¥é™ä½å…¨å±€å½’çº¦å»¶è¿Ÿã€‚`overlap_comm` ä½¿ç”¨4.5å€çš„`allgather_bucket_size`å’Œ`reduce_bucket_size`å€¼ã€‚å› æ­¤ï¼Œå¦‚æœå®ƒä»¬è®¾ç½®ä¸º5e8ï¼Œè¿™å°†éœ€è¦9GBçš„å ç”¨ç©ºé—´ï¼ˆ`5e8
    x 2å­—èŠ‚ x 2 x 4.5`ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨çš„GPUå…·æœ‰8GBæˆ–æ›´å°‘çš„RAMï¼Œä¸ºäº†é¿å…å‡ºç°OOMé”™è¯¯ï¼Œæ‚¨éœ€è¦å°†è¿™äº›å‚æ•°å‡å°‘åˆ°çº¦`2e8`ï¼Œè¿™å°†éœ€è¦3.6GBã€‚å¦‚æœæ‚¨çš„GPUå®¹é‡æ›´å¤§ï¼Œä½†å¼€å§‹å‡ºç°OOMé”™è¯¯ï¼Œæ‚¨ä¹Ÿéœ€è¦åšåŒæ ·çš„æ“ä½œã€‚'
- en: when reducing these buffers youâ€™re trading communication speed to avail more
    GPU RAM. The smaller the buffer size is, the slower the communication gets, and
    the more GPU RAM will be available to other tasks. So if a bigger batch size is
    important, getting a slightly slower training time could be a good trade.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“å‡å°‘è¿™äº›ç¼“å†²åŒºæ—¶ï¼Œæ‚¨æ­£åœ¨äº¤æ¢é€šä¿¡é€Ÿåº¦ä»¥è·å¾—æ›´å¤šçš„GPU RAMã€‚ç¼“å†²åŒºå¤§å°è¶Šå°ï¼Œé€šä¿¡é€Ÿåº¦è¶Šæ…¢ï¼Œå¯ç”¨äºå…¶ä»–ä»»åŠ¡çš„GPU RAMå°±è¶Šå¤šã€‚å› æ­¤ï¼Œå¦‚æœæ›´å¤§çš„æ‰¹é‡å¤§å°å¾ˆé‡è¦ï¼Œç¨å¾®å‡æ…¢è®­ç»ƒæ—¶é—´å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„äº¤æ˜“ã€‚
- en: 'Additionally, `deepspeed==0.4.4` added a new option `round_robin_gradients`
    which you can enable with:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œ`deepspeed==0.4.4`æ·»åŠ äº†ä¸€ä¸ªæ–°é€‰é¡¹`round_robin_gradients`ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ç”¨ï¼š
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is a stage 2 optimization for CPU offloading that parallelizes gradient
    copying to CPU memory among ranks by fine-grained gradient partitioning. Performance
    benefit grows with gradient accumulation steps (more copying between optimizer
    steps) or GPU count (increased parallelism).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºCPUå¸è½½çš„é˜¶æ®µ2ä¼˜åŒ–ï¼Œé€šè¿‡ç»†ç²’åº¦æ¢¯åº¦åˆ†åŒºå°†æ¢¯åº¦å¤åˆ¶åˆ°CPUå†…å­˜ä¸­ï¼Œä»¥åœ¨ç­‰çº§ä¹‹é—´å¹¶è¡ŒåŒ–ã€‚æ€§èƒ½æ”¶ç›Šéšç€æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ï¼ˆåœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹é—´çš„æ›´å¤šå¤åˆ¶ï¼‰æˆ–GPUæ•°é‡ï¼ˆå¢åŠ å¹¶è¡Œæ€§ï¼‰è€Œå¢åŠ ã€‚
- en: ZeRO-3 Config
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-3é…ç½®
- en: 'The following is an example of configuration for ZeRO stage 3:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ZeROé˜¶æ®µ3çš„é…ç½®ç¤ºä¾‹ï¼š
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you are getting OOMs, because your model or activations donâ€™t fit into the
    GPU memory and you have unutilized CPU memory offloading the optimizer states
    and parameters to CPU memory with `"device": "cpu"` may solve this limitation.
    If you donâ€™t want to offload to CPU memory, use `none` instead of `cpu` for the
    `device` entry. Offloading to NVMe is discussed further down.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚æœæ‚¨é‡åˆ°OOMï¼Œå› ä¸ºæ‚¨çš„æ¨¡å‹æˆ–æ¿€æ´»ä¸é€‚åˆGPUå†…å­˜ï¼Œå¹¶ä¸”æ‚¨æœ‰æœªä½¿ç”¨çš„CPUå†…å­˜ï¼Œå°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°å¸è½½åˆ°CPUå†…å­˜å¹¶ä½¿ç”¨`"device": "cpu"`å¯èƒ½è§£å†³æ­¤é™åˆ¶ã€‚å¦‚æœæ‚¨ä¸æƒ³å¸è½½åˆ°CPUå†…å­˜ï¼Œè¯·åœ¨`device`æ¡ç›®ä¸­ä½¿ç”¨`none`è€Œä¸æ˜¯`cpu`ã€‚æœ‰å…³å¸è½½åˆ°NVMeçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸‹æ–‡ã€‚'
- en: Pinned memory is enabled with `pin_memory` set to `true`. This feature can improve
    the throughput at the cost of making less memory available to other processes.
    Pinned memory is set aside to the specific process that requested it and its typically
    accessed much faster than normal CPU memory.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†`pin_memory`è®¾ç½®ä¸º`true`å¯ç”¨äº†å›ºå®šå†…å­˜ã€‚è¿™ä¸ªåŠŸèƒ½å¯ä»¥æé«˜ååé‡ï¼Œä½†ä¼šå‡å°‘å…¶ä»–è¿›ç¨‹å¯ç”¨çš„å†…å­˜ã€‚å›ºå®šå†…å­˜è¢«ä¿ç•™ç»™è¯·æ±‚å®ƒçš„ç‰¹å®šè¿›ç¨‹ï¼Œé€šå¸¸æ¯”æ™®é€šCPUå†…å­˜è®¿é—®é€Ÿåº¦å¿«å¾—å¤šã€‚
- en: '**Performance tuning:**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€§èƒ½è°ƒä¼˜ï¼š**'
- en: '`stage3_max_live_parameters`: `1e9`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_max_live_parameters`ï¼š`1e9`'
- en: '`stage3_max_reuse_distance`: `1e9`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_max_reuse_distance`ï¼š`1e9`'
- en: If hitting OOM reduce `stage3_max_live_parameters` and `stage3_max_reuse_distance`.
    They should have minimal impact on performance unless you are doing activation
    checkpointing. `1e9` would consume ~2GB. The memory is shared by `stage3_max_live_parameters`
    and `stage3_max_reuse_distance`, so itâ€™s not additive, itâ€™s just 2GB total.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœé‡åˆ°OOMï¼Œè¯·å‡å°‘`stage3_max_live_parameters`å’Œ`stage3_max_reuse_distance`ã€‚é™¤éè¿›è¡Œæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œå¦åˆ™å®ƒä»¬å¯¹æ€§èƒ½å½±å“å¾ˆå°ã€‚`1e9`å°†æ¶ˆè€—çº¦2GBã€‚å†…å­˜ç”±`stage3_max_live_parameters`å’Œ`stage3_max_reuse_distance`å…±äº«ï¼Œå› æ­¤ä¸æ˜¯ç´¯åŠ çš„ï¼Œè€Œæ˜¯æ€»å…±2GBã€‚
- en: '`stage3_max_live_parameters` is the upper limit on how many full parameters
    you want to keep on the GPU at any given time. â€œreuse distanceâ€ is a metric we
    are using to figure out when will a parameter be used again in the future, and
    we use the `stage3_max_reuse_distance` to decide whether to throw away the parameter
    or to keep it. If a parameter is going to be used again in near future (less than
    `stage3_max_reuse_distance`) then we keep it to reduce communication overhead.
    This is super helpful when you have activation checkpointing enabled, where we
    do a forward recompute and backward passes a single layer granularity and want
    to keep the parameter in the forward recompute till the backward'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`stage3_max_live_parameters`æ˜¯æ‚¨å¸Œæœ›åœ¨ä»»ä½•ç»™å®šæ—¶é—´ä¿ç•™åœ¨GPUä¸Šçš„å®Œæ•´å‚æ•°çš„ä¸Šé™ã€‚"é‡ç”¨è·ç¦»"æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„åº¦é‡æ ‡å‡†ï¼Œç”¨äºç¡®å®šå‚æ•°åœ¨æœªæ¥ä½•æ—¶å†æ¬¡ä½¿ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨`stage3_max_reuse_distance`æ¥å†³å®šæ˜¯ä¸¢å¼ƒå‚æ•°è¿˜æ˜¯ä¿ç•™å‚æ•°ã€‚å¦‚æœå‚æ•°å°†åœ¨ä¸ä¹…çš„å°†æ¥ï¼ˆå°äº`stage3_max_reuse_distance`ï¼‰å†æ¬¡ä½¿ç”¨ï¼Œåˆ™æˆ‘ä»¬ä¿ç•™å®ƒä»¥å‡å°‘é€šä¿¡å¼€é”€ã€‚å½“å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹æ—¶ï¼Œè¿™éå¸¸æœ‰å¸®åŠ©ï¼Œæˆ‘ä»¬åœ¨å‰å‘é‡è®¡ç®—å’Œåå‘ä¼ é€’ä¸­ä»¥å•å±‚ç²’åº¦æ‰§è¡Œæ“ä½œï¼Œå¹¶å¸Œæœ›åœ¨å‰å‘é‡è®¡ç®—ä¸­ä¿ç•™å‚æ•°ç›´åˆ°åå‘ä¼ é€’ã€‚'
- en: 'The following configuration values depend on the modelâ€™s hidden size:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹é…ç½®å€¼å–å†³äºæ¨¡å‹çš„éšè—å¤§å°ï¼š
- en: '`reduce_bucket_size`: `hidden_size*hidden_size`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_bucket_size`ï¼š`hidden_size*hidden_size`'
- en: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_prefetch_bucket_size`ï¼š`0.9 * hidden_size * hidden_size`'
- en: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_param_persistence_threshold`ï¼š`10 * hidden_size`'
- en: therefore set these values to `auto` and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically assign the recommended values. But, of course, feel free to
    set these explicitly as well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤å°†è¿™äº›å€¼è®¾ç½®ä¸º`auto`ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è‡ªåŠ¨åˆ†é…æ¨èå€¼ã€‚å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™äº›å€¼ã€‚
- en: '`stage3_gather_16bit_weights_on_model_save` enables model fp16 weights consolidation
    when model gets saved. With large models and multiple GPUs this is an expensive
    operation both in terms of memory and speed. Itâ€™s currently required if you plan
    to resume the training. Watch out for future updates that will remove this limitation
    and make things more flexible.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`stage3_gather_16bit_weights_on_model_save`åœ¨æ¨¡å‹ä¿å­˜æ—¶å¯ç”¨æ¨¡å‹fp16æƒé‡åˆå¹¶ã€‚å¯¹äºå¤§å‹æ¨¡å‹å’Œå¤šä¸ªGPUï¼Œè¿™æ˜¯ä¸€é¡¹æ˜‚è´µçš„æ“ä½œï¼Œæ— è®ºæ˜¯åœ¨å†…å­˜è¿˜æ˜¯é€Ÿåº¦æ–¹é¢ã€‚å¦‚æœæ‚¨è®¡åˆ’æ¢å¤è®­ç»ƒï¼Œåˆ™ç›®å‰éœ€è¦è¿™æ ·åšã€‚è¯·æ³¨æ„æœªæ¥çš„æ›´æ–°å°†æ¶ˆé™¤æ­¤é™åˆ¶å¹¶ä½¿äº‹æƒ…æ›´åŠ çµæ´»ã€‚'
- en: If youâ€™re migrating from ZeRO-2 configuration note that `allgather_partitions`,
    `allgather_bucket_size` and `reduce_scatter` configuration parameters are not
    used in ZeRO-3\. If you keep these in the config file they will just be ignored.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä»ZeRO-2é…ç½®è¿ç§»ï¼Œè¯·æ³¨æ„`allgather_partitions`ã€`allgather_bucket_size`å’Œ`reduce_scatter`é…ç½®å‚æ•°åœ¨ZeRO-3ä¸­ä¸ä½¿ç”¨ã€‚å¦‚æœæ‚¨å°†è¿™äº›ä¿ç•™åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼Œå®ƒä»¬å°†è¢«å¿½ç•¥ã€‚
- en: '`sub_group_size`: `1e9`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sub_group_size`ï¼š`1e9`'
- en: '`sub_group_size` controls the granularity in which parameters are updated during
    optimizer steps. Parameters are grouped into buckets of `sub_group_size` and each
    buckets is updated one at a time. When used with NVMe offload in ZeRO-Infinity,
    `sub_group_size` therefore controls the granularity in which model states are
    moved in and out of CPU memory from NVMe during the optimizer step. This prevents
    running out of CPU memory for extremely large models.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`sub_group_size`æ§åˆ¶å‚æ•°åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¸­æ›´æ–°çš„ç²’åº¦ã€‚å‚æ•°è¢«åˆ†ç»„åˆ°`sub_group_size`çš„æ¡¶ä¸­ï¼Œæ¯ä¸ªæ¡¶ä¾æ¬¡æ›´æ–°ã€‚åœ¨ZeRO-Infinityä¸­ä¸NVMeå¸è½½ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œ`sub_group_size`å› æ­¤æ§åˆ¶æ¨¡å‹çŠ¶æ€åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´ä»NVMeç§»å…¥å’Œç§»å‡ºCPUå†…å­˜çš„ç²’åº¦ã€‚è¿™å¯ä»¥é˜²æ­¢æå¤§å‹æ¨¡å‹è€—å°½CPUå†…å­˜ã€‚'
- en: 'You can leave `sub_group_size` to its default value of *1e9* when not using
    NVMe offload. You may want to change its default value in the following cases:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸ä½¿ç”¨NVMeå¸è½½ï¼Œå¯ä»¥å°†`sub_group_size`ä¿ç•™ä¸ºé»˜è®¤å€¼*1e9*ã€‚åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦æ›´æ”¹å…¶é»˜è®¤å€¼ï¼š
- en: 'Running into OOM during optimizer step: Reduce `sub_group_size` to reduce memory
    utilization of temporary buffers'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¸­é‡åˆ°OOMï¼šå‡å°‘`sub_group_size`ä»¥å‡å°‘ä¸´æ—¶ç¼“å†²åŒºçš„å†…å­˜åˆ©ç”¨
- en: 'Optimizer Step is taking a long time: Increase `sub_group_size` to improve
    bandwidth utilization as a result of the increased data buffers.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨æ­¥éª¤èŠ±è´¹å¾ˆé•¿æ—¶é—´ï¼šå¢åŠ `sub_group_size`ä»¥æé«˜å¸¦å®½åˆ©ç”¨ç‡ï¼Œå› ä¸ºæ•°æ®ç¼“å†²åŒºå¢åŠ ã€‚
- en: ZeRO-0 Config
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-0é…ç½®
- en: Note that weâ€™re listing Stage 0 and 1 last since they are rarely used.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†é˜¶æ®µ0å’Œ1åˆ—åœ¨æœ€åï¼Œå› ä¸ºå®ƒä»¬å¾ˆå°‘è¢«ä½¿ç”¨ã€‚
- en: 'Stage 0 is disabling all types of sharding and just using DeepSpeed as DDP.
    You can turn it on with:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: é˜¶æ®µ0æ˜¯ç¦ç”¨æ‰€æœ‰ç±»å‹çš„åˆ†ç‰‡ï¼Œåªä½¿ç”¨DeepSpeedä½œä¸ºDDPã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰“å¼€å®ƒï¼š
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This will essentially disable ZeRO without you needing to change anything else.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åŸºæœ¬ä¸Šç¦ç”¨ZeROï¼Œè€Œæ— éœ€æ›´æ”¹å…¶ä»–ä»»ä½•å†…å®¹ã€‚
- en: ZeRO-1 Config
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-1é…ç½®
- en: 'Stage 1 is Stage 2 minus gradient sharding. You can always try it to speed
    things a tiny bit to only shard the optimizer states with:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: é˜¶æ®µ1æ˜¯é˜¶æ®µ2å‡å»æ¢¯åº¦åˆ†ç‰‡ã€‚æ‚¨å¯ä»¥å°è¯•å°†ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼Œä»¥åŠ å¿«é€Ÿåº¦ï¼š
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: NVMe Support
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NVMeæ”¯æŒ
- en: ZeRO-Infinity allows for training incredibly large models by extending GPU and
    CPU memory with NVMe memory. Thanks to smart partitioning and tiling algorithms
    each GPU needs to send and receive very small amounts of data during offloading
    so modern NVMe proved to be fit to allow for an even larger total memory pool
    available to your training process. ZeRO-Infinity requires ZeRO-3 enabled.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO-Infinityé€šè¿‡ä½¿ç”¨NVMeå†…å­˜æ‰©å±•GPUå’ŒCPUå†…å­˜ï¼Œå…è®¸è®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ã€‚ç”±äºæ™ºèƒ½åˆ†åŒºå’Œå¹³é“ºç®—æ³•ï¼Œæ¯ä¸ªGPUåœ¨å¸è½½æœŸé—´éœ€è¦å‘é€å’Œæ¥æ”¶éå¸¸å°‘é‡çš„æ•°æ®ï¼Œå› æ­¤ç°ä»£NVMeè¢«è¯æ˜é€‚åˆå…è®¸æ›´å¤§çš„æ€»å†…å­˜æ± å¯ç”¨äºæ‚¨çš„è®­ç»ƒè¿‡ç¨‹ã€‚ZeRO-Infinityéœ€è¦å¯ç”¨ZeRO-3ã€‚
- en: 'The following configuration example enables NVMe to offload both optimizer
    states and the params:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹é…ç½®ç¤ºä¾‹å¯ç”¨äº†NVMeä»¥å¸è½½ä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°ï¼š
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can choose to offload both optimizer states and params to NVMe, or just
    one of them or none. For example, if you have copious amounts of CPU memory available,
    by all means offload to CPU memory only as itâ€™d be faster (hint: *â€œdeviceâ€: â€œcpuâ€*).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ‚¨å¯ä»¥é€‰æ‹©å°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°éƒ½å¸è½½åˆ°NVMeï¼Œæˆ–è€…åªå¸è½½å…¶ä¸­ä¸€ä¸ªæˆ–ä¸¤è€…éƒ½ä¸å¸è½½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰å¤§é‡çš„CPUå†…å­˜å¯ç”¨ï¼Œå°½ç®¡å°†å…¶ä»…å¸è½½åˆ°CPUå†…å­˜ï¼Œå› ä¸ºè¿™æ ·ä¼šæ›´å¿«ï¼ˆæç¤ºï¼šâ€œdeviceâ€:
    â€œcpuâ€ï¼‰ã€‚'
- en: Here is the full documentation for offloading [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading)
    and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æœ‰å…³å¸è½½[ä¼˜åŒ–å™¨çŠ¶æ€](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading)å’Œ[å‚æ•°](https://www.deepspeed.ai/docs/config-json/#parameter-offloading)çš„å®Œæ•´æ–‡æ¡£ã€‚
- en: Make sure that your `nvme_path` is actually an NVMe, since it will work with
    the normal hard drive or SSD, but itâ€™ll be much much slower. The fast scalable
    training was designed with modern NVMe transfer speeds in mind (as of this writing
    one can have ~3.5GB/s read, ~3GB/s write peak speeds).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æ‚¨çš„`nvme_path`å®é™…ä¸Šæ˜¯ä¸€ä¸ªNVMeï¼Œå› ä¸ºå®ƒå¯ä»¥ä¸æ™®é€šç¡¬ç›˜æˆ–å›ºæ€ç¡¬ç›˜ä¸€èµ·ä½¿ç”¨ï¼Œä½†é€Ÿåº¦ä¼šæ…¢å¾—å¤šã€‚å¿«é€Ÿå¯æ‰©å±•çš„è®­ç»ƒæ˜¯æ ¹æ®ç°ä»£NVMeä¼ è¾“é€Ÿåº¦è®¾è®¡çš„ï¼ˆæˆªè‡³æœ¬æ–‡æ’°å†™æ—¶ï¼Œè¯»å–é€Ÿåº¦çº¦ä¸º3.5GB/sï¼Œå†™å…¥é€Ÿåº¦çº¦ä¸º3GB/sï¼‰ã€‚
- en: In order to figure out the optimal `aio` configuration block you must run a
    benchmark on your target setup, as [explained here](https://github.com/microsoft/DeepSpeed/issues/998).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¾å‡ºæœ€ä½³çš„`aio`é…ç½®å—ï¼Œæ‚¨å¿…é¡»åœ¨ç›®æ ‡è®¾ç½®ä¸Šè¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¦‚[æ­¤å¤„æ‰€è¿°](https://github.com/microsoft/DeepSpeed/issues/998)ã€‚
- en: ZeRO-2 vs ZeRO-3 Performance
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-2ä¸ZeRO-3æ€§èƒ½
- en: ZeRO-3 is likely to be slower than ZeRO-2 if everything else is configured the
    same because the former has to gather model weights in addition to what ZeRO-2
    does. If ZeRO-2 meets your needs and you donâ€™t need to scale beyond a few GPUs
    then you may choose to stick to it. Itâ€™s important to understand that ZeRO-3 enables
    a much higher scalability capacity at a cost of speed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€åˆ‡é…ç½®ç›¸åŒï¼ŒZeRO-3å¯èƒ½æ¯”ZeRO-2æ…¢ï¼Œå› ä¸ºå‰è€…éœ€è¦æ”¶é›†æ¨¡å‹æƒé‡ä»¥å¤–çš„å†…å®¹ã€‚å¦‚æœZeRO-2æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œå¹¶ä¸”æ‚¨ä¸éœ€è¦æ‰©å±•åˆ°å‡ ä¸ªGPUä¹‹å¤–ï¼Œé‚£ä¹ˆæ‚¨å¯ä»¥é€‰æ‹©åšæŒä½¿ç”¨å®ƒã€‚é‡è¦çš„æ˜¯è¦äº†è§£ï¼ŒZeRO-3åœ¨é€Ÿåº¦ä¸Šçš„ä»£ä»·æ˜¯å®ç°æ›´é«˜çš„å¯ä¼¸ç¼©æ€§å®¹é‡ã€‚
- en: 'Itâ€™s possible to adjust ZeRO-3 configuration to make it perform closer to ZeRO-2:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒæ•´ZeRO-3é…ç½®ï¼Œä½¿å…¶æ€§èƒ½æ¥è¿‘ZeRO-2ï¼š
- en: set `stage3_param_persistence_threshold` to a very large number - larger than
    the largest parameter, e.g., `6 * hidden_size * hidden_size`. This will keep the
    parameters on the GPUs.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†`stage3_param_persistence_threshold`è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸å¤§çš„æ•°å­— - å¤§äºæœ€å¤§å‚æ•°ï¼Œä¾‹å¦‚`6 * hidden_size
    * hidden_size`ã€‚è¿™å°†ä½¿å‚æ•°ä¿ç•™åœ¨GPUä¸Šã€‚
- en: turn off `offload_params` since ZeRO-2 doesnâ€™t have that option.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³é—­`offload_params`ï¼Œå› ä¸ºZeRO-2æ²¡æœ‰è¯¥é€‰é¡¹ã€‚
- en: The performance will likely improve significantly with just `offload_params`
    turned off, even if you donâ€™t change `stage3_param_persistence_threshold`. Of
    course, these changes will impact the size of the model you can train. So these
    help you to trade scalability for speed depending on your needs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ‚¨ä¸æ›´æ”¹`stage3_param_persistence_threshold`ï¼Œåªè¦å…³é—­`offload_params`ï¼Œæ€§èƒ½å¯èƒ½ä¼šæ˜¾ç€æé«˜ã€‚å½“ç„¶ï¼Œè¿™äº›æ›´æ”¹å°†å½±å“æ‚¨å¯ä»¥è®­ç»ƒçš„æ¨¡å‹å¤§å°ã€‚å› æ­¤ï¼Œè¿™äº›å¸®åŠ©æ‚¨æ ¹æ®éœ€è¦åœ¨å¯ä¼¸ç¼©æ€§å’Œé€Ÿåº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚
- en: ZeRO-2 Example
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-2ç¤ºä¾‹
- en: 'Here is a full ZeRO-2 auto-configuration file `ds_config_zero2.json`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ZeRO-2è‡ªåŠ¨é…ç½®æ–‡ä»¶`ds_config_zero2.json`ï¼š
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here is a full ZeRO-2 all-enabled manually set configuration file. It is here
    mainly for you to see what the typical values look like, but we highly recommend
    using the one with multiple `auto` settings in it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ZeRO-2å…¨å¯ç”¨æ‰‹åŠ¨è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚è¿™é‡Œä¸»è¦æ˜¯è®©æ‚¨çœ‹çœ‹å…¸å‹å€¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨å…¶ä¸­å¸¦æœ‰å¤šä¸ª`auto`è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ZeRO-3 Example
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-3ç¤ºä¾‹
- en: 'Here is a full ZeRO-3 auto-configuration file `ds_config_zero3.json`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ZeRO-3è‡ªåŠ¨é…ç½®æ–‡ä»¶`ds_config_zero3.json`ï¼š
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here is a full ZeRO-3 all-enabled manually set configuration file. It is here
    mainly for you to see what the typical values look like, but we highly recommend
    using the one with multiple `auto` settings in it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ZeRO-3å…¨å¯ç”¨æ‰‹åŠ¨è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚è¿™é‡Œä¸»è¦æ˜¯è®©æ‚¨çœ‹çœ‹å…¸å‹å€¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨å…¶ä¸­å¸¦æœ‰å¤šä¸ª`auto`è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How to Choose Which ZeRO Stage and Offloads To Use For Best Performance
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¦‚ä½•é€‰æ‹©æœ€ä½³æ€§èƒ½çš„ZeROé˜¶æ®µå’Œå¸è½½æ–¹å¼
- en: So now you know there are all these different stages. How to decide which of
    them to use? This section will attempt to address this question.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨çŸ¥é“æœ‰æ‰€æœ‰è¿™äº›ä¸åŒçš„é˜¶æ®µã€‚å¦‚ä½•å†³å®šä½¿ç”¨å…¶ä¸­å“ªä¸€ä¸ªï¼Ÿæœ¬èŠ‚å°†å°è¯•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
- en: 'In general the following applies:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œä»¥ä¸‹å†…å®¹é€‚ç”¨ï¼š
- en: Speed-wise (left is faster than right)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€Ÿåº¦æ–¹é¢ï¼ˆå·¦è¾¹æ¯”å³è¾¹å¿«ï¼‰
- en: Stage 0 (DDP) > Stage 1 > Stage 2 > Stage 2 + offload > Stage 3 > Stage 3 +
    offloads
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: é˜¶æ®µ0ï¼ˆDDPï¼‰> é˜¶æ®µ1 > é˜¶æ®µ2 > é˜¶æ®µ2 + å¸è½½ > é˜¶æ®µ3 > é˜¶æ®µ3 + å¸è½½
- en: GPU Memory usage-wise (right is more GPU memory efficient than left)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUå†…å­˜ä½¿ç”¨æ–¹é¢ï¼ˆå³ä¾§æ¯”å·¦ä¾§æ›´èŠ‚çœGPUå†…å­˜ï¼‰
- en: Stage 0 (DDP) < Stage 1 < Stage 2 < Stage 2 + offload < Stage 3 < Stage 3 +
    offloads
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: é˜¶æ®µ0ï¼ˆDDPï¼‰<é˜¶æ®µ1<é˜¶æ®µ2<é˜¶æ®µ2 +å¸è½½<é˜¶æ®µ3<é˜¶æ®µ3 +å¸è½½
- en: So when you want to get the fastest execution while fitting into minimal number
    of GPUs, here is the process you could follow. We start with the fastest approach
    and if running into GPU OOM we then go to the next slower approach, but which
    will use less GPU memory. And so on and so forth.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½“æ‚¨å¸Œæœ›åœ¨æœ€å°‘æ•°é‡çš„GPUä¸­è·å¾—æœ€å¿«çš„æ‰§è¡Œæ—¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹è¿‡ç¨‹è¿›è¡Œã€‚æˆ‘ä»¬ä»æœ€å¿«çš„æ–¹æ³•å¼€å§‹ï¼Œå¦‚æœé‡åˆ°GPU OOMï¼Œåˆ™è½¬å‘ä¸‹ä¸€ä¸ªè¾ƒæ…¢çš„æ–¹æ³•ï¼Œä½†å°†ä½¿ç”¨æ›´å°‘çš„GPUå†…å­˜ã€‚ä¾æ­¤ç±»æ¨ã€‚
- en: First of all set batch size to 1 (you can always use gradient accumulation for
    any desired effective batch size).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆå°†æ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º1ï¼ˆæ‚¨å§‹ç»ˆå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¥è·å¾—ä»»ä½•æ‰€éœ€çš„æœ‰æ•ˆæ‰¹å¤„ç†å¤§å°ï¼‰ã€‚
- en: Enable `--gradient_checkpointing 1` (HF Trainer) or directly `model.gradient_checkpointing_enable()`
    - if OOM then
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯ç”¨`--gradient_checkpointing 1`ï¼ˆHF Trainerï¼‰æˆ–ç›´æ¥`model.gradient_checkpointing_enable()`-å¦‚æœOOMï¼Œåˆ™
- en: Try ZeRO stage 2 first. if OOM then
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¦–å…ˆå°è¯•ZeROé˜¶æ®µ2ã€‚å¦‚æœOOMï¼Œåˆ™
- en: Try ZeRO stage 2 + `offload_optimizer` - if OOM then
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°è¯•ZeROé˜¶æ®µ2 + `offload_optimizer`-å¦‚æœOOMï¼Œåˆ™
- en: Switch to ZeRO stage 3 - if OOM then
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ‡æ¢åˆ°ZeROé˜¶æ®µ3-å¦‚æœOOMï¼Œåˆ™
- en: Enable `offload_param` to `cpu` - if OOM then
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯ç”¨`offload_param`åˆ°`cpu`-å¦‚æœOOMï¼Œåˆ™
- en: Enable `offload_optimizer` to `cpu` - if OOM then
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯ç”¨`offload_optimizer`åˆ°`cpu`-å¦‚æœOOMï¼Œåˆ™
- en: If you still canâ€™t fit a batch size of 1 first check various default values
    and lower them if you can. For example, if you use `generate` and you donâ€™t use
    a wide search beam make it narrower as itâ€™d take a lot of memory.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä»ç„¶æ— æ³•é€‚åº”æ‰¹å¤„ç†å¤§å°ä¸º1ï¼Œè¯·é¦–å…ˆæ£€æŸ¥å„ç§é»˜è®¤å€¼ï¼Œå¹¶åœ¨å¯èƒ½çš„æƒ…å†µä¸‹å°†å…¶é™ä½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨`generate`ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨å®½æœç´¢å…‰æŸï¼Œè¯·å°†å…¶å˜çª„ï¼Œå› ä¸ºè¿™å°†å ç”¨å¤§é‡å†…å­˜ã€‚
- en: Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher
    GPUs and fp16 on older gpu architectures.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»å¯¹ä½¿ç”¨æ··åˆåŠç²¾åº¦è€Œä¸æ˜¯fp32-å› æ­¤åœ¨AmpereåŠæ›´é«˜GPUä¸Šä½¿ç”¨bf16ï¼Œåœ¨æ—§çš„GPUæ¶æ„ä¸Šä½¿ç”¨fp16ã€‚
- en: If you still OOM you could add more hardware or enable ZeRO-Infinity - that
    is switch offloads `offload_param` and `offload_optimizer` to `nvme`. You need
    to make sure itâ€™s a very fast nvme. As an anecdote I was able to infer BLOOM-176B
    on a tiny GPU using ZeRO-Infinity except it was extremely slow. But it worked!
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä»ç„¶OOMï¼Œæ‚¨å¯ä»¥æ·»åŠ æ›´å¤šç¡¬ä»¶æˆ–å¯ç”¨ZeRO-Infinity-å³åˆ‡æ¢å¸è½½`offload_param`å’Œ`offload_optimizer`åˆ°`nvme`ã€‚æ‚¨éœ€è¦ç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªéå¸¸å¿«é€Ÿçš„nvmeã€‚ä½œä¸ºä¸€ä¸ªè½¶äº‹ï¼Œæˆ‘èƒ½å¤Ÿåœ¨ä¸€ä¸ªå°å‹GPUä¸Šæ¨æ–­BLOOM-176Bï¼Œä½¿ç”¨ZeRO-Infinityï¼Œåªæ˜¯é€Ÿåº¦ææ…¢ã€‚ä½†å®ƒæœ‰æ•ˆï¼
- en: You can, of course, work through these steps in reverse by starting with the
    most GPU memory efficient config and then going backwards. Or try bi-secting it.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»æœ€GPUå†…å­˜é«˜æ•ˆçš„é…ç½®å¼€å§‹ï¼Œç„¶åå‘åè¿›è¡Œï¼Œæˆ–è€…å°è¯•äºŒåˆ†æ³•æ¥é€†å‘æ‰§è¡Œè¿™äº›æ­¥éª¤ã€‚
- en: Once you have your batch size 1 not leading to OOM, measure your effective throughput.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨çš„æ‰¹å¤„ç†å¤§å°ä¸º1ä¸ä¼šå¯¼è‡´OOMï¼Œè¯·æµ‹é‡æ‚¨çš„æœ‰æ•ˆååé‡ã€‚
- en: Next try to increase the batch size to as large as you can, since the higher
    the batch size the more efficient the GPUs are as they perform the best when matrices
    they multiply are huge.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥å°è¯•å°†æ‰¹å¤„ç†å¤§å°å¢åŠ åˆ°å°½å¯èƒ½å¤§ï¼Œå› ä¸ºæ‰¹å¤„ç†å¤§å°è¶Šå¤§ï¼ŒGPUçš„æ•ˆç‡å°±è¶Šé«˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨ä¹˜æ³•çŸ©é˜µå¾ˆå¤§æ—¶è¡¨ç°æœ€ä½³ã€‚
- en: Now the performance optimization game starts. You can turn off some offload
    features or step down in ZeRO stages and increase/decrease batch size and again
    measure your effective throughput. Rinse and repeat until satisfied.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ€§èƒ½ä¼˜åŒ–æ¸¸æˆå¼€å§‹äº†ã€‚æ‚¨å¯ä»¥å…³é—­ä¸€äº›å¸è½½åŠŸèƒ½æˆ–é™ä½ZeROé˜¶æ®µï¼Œå¹¶å¢åŠ /å‡å°‘æ‰¹å¤„ç†å¤§å°ï¼Œç„¶åå†æµ‹é‡æ‚¨çš„æœ‰æ•ˆååé‡ã€‚åå¤è¿›è¡Œï¼Œç›´åˆ°æ»¡æ„ä¸ºæ­¢ã€‚
- en: Donâ€™t spend forever on it, but if youâ€™re about to start a 3 months training
    - do spend a few days on it to find the most effective throughput-wise setup.
    So that your training cost will be the lowest and you will finish training faster.
    In the current crazy-paced ML world, if it takes you an extra month to train something
    you are likely to miss a golden opportunity. Of course, this is only me sharing
    an observation and in no way Iâ€™m trying to rush you. Before beginning to train
    BLOOM-176B I spent 2 days on this process and was able to increase throughput
    from 90 to 150 TFLOPs! This effort saved us more than one month of training time.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦èŠ±å¤ªå¤šæ—¶é—´åœ¨ä¸Šé¢ï¼Œä½†å¦‚æœæ‚¨å³å°†å¼€å§‹ä¸ºæœŸ3ä¸ªæœˆçš„åŸ¹è®­-è¯·èŠ±å‡ å¤©æ—¶é—´æ‰¾åˆ°æœ€æœ‰æ•ˆçš„ååé‡è®¾ç½®ã€‚è¿™æ ·ï¼Œæ‚¨çš„åŸ¹è®­æˆæœ¬å°†æœ€ä½ï¼Œæ‚¨å°†æ›´å¿«åœ°å®ŒæˆåŸ¹è®­ã€‚åœ¨å½“å‰å¿«èŠ‚å¥çš„MLä¸–ç•Œä¸­ï¼Œå¦‚æœæ‚¨éœ€è¦é¢å¤–ä¸€ä¸ªæœˆæ¥è®­ç»ƒæŸäº›å†…å®¹ï¼Œæ‚¨å¾ˆå¯èƒ½ä¼šé”™è¿‡ä¸€ä¸ªç»ä½³çš„æœºä¼šã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯æˆ‘åˆ†äº«çš„ä¸€ä¸ªè§‚å¯Ÿï¼Œæˆ‘ç»ä¸ä¼šå‚¬ä¿ƒæ‚¨ã€‚åœ¨å¼€å§‹è®­ç»ƒBLOOM-176Bä¹‹å‰ï¼Œæˆ‘èŠ±äº†2å¤©æ—¶é—´è¿›è¡Œè¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿå°†ååé‡ä»90æé«˜åˆ°150
    TFLOPsï¼è¿™ä¸€åŠªåŠ›ä¸ºæˆ‘ä»¬èŠ‚çœäº†ä¸€ä¸ªå¤šæœˆçš„åŸ¹è®­æ—¶é—´ã€‚
- en: These notes were written primarily for the training mode, but they should mostly
    apply for inference as well. For example, during inference Gradient Checkpointing
    is a no-op since it is only useful during training. Additionally, we found out
    that if you are doing a multi-GPU inference and not using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/),
    [Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) should
    provide a superior performance.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ³¨æ„äº‹é¡¹ä¸»è¦æ˜¯é’ˆå¯¹è®­ç»ƒæ¨¡å¼ç¼–å†™çš„ï¼Œä½†å®ƒä»¬åœ¨æ¨æ–­æ–¹é¢ä¹Ÿåº”è¯¥å¤§å¤šé€‚ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¨æ–­æœŸé—´ï¼Œæ¢¯åº¦æ£€æŸ¥ç‚¹æ˜¯æ— æ•ˆçš„ï¼Œå› ä¸ºå®ƒåªåœ¨è®­ç»ƒæœŸé—´æœ‰ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå¦‚æœæ‚¨æ­£åœ¨è¿›è¡Œå¤šGPUæ¨æ–­å¹¶ä¸”ä¸ä½¿ç”¨[DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/)ï¼Œ[Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts)åº”è¯¥æä¾›æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚
- en: 'Other quick related performance notes:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–å¿«é€Ÿç›¸å…³çš„æ€§èƒ½æ³¨æ„äº‹é¡¹ï¼š
- en: if you are training something from scratch always try to have tensors with shapes
    that are divisible by 16 (e.g. hidden size). For batch size try divisible by 2
    at least. There are [wave and tile quanitization](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)
    divisibility that is hardware-specific if you want to squeeze even higher performance
    from your GPUs.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä»å¤´å¼€å§‹è®­ç»ƒæŸäº›å†…å®¹ï¼Œè¯·å§‹ç»ˆå°è¯•å…·æœ‰å¯è¢«16æ•´é™¤çš„å¼ é‡å½¢çŠ¶ï¼ˆä¾‹å¦‚éšè—å¤§å°ï¼‰ã€‚å¯¹äºæ‰¹å¤„ç†å¤§å°ï¼Œè¯·è‡³å°‘å°è¯•å¯è¢«2æ•´é™¤ã€‚å¦‚æœæ‚¨æƒ³ä»GPUä¸­æŒ¤å–æ›´é«˜çš„æ€§èƒ½ï¼Œåˆ™æœ‰ç¡¬ä»¶ç‰¹å®šçš„[æ³¢å’Œç“·ç –é‡åŒ–](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)å¯è¢«æ•´é™¤ã€‚
- en: Activation Checkpointing or Gradient Checkpointing
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹æˆ–æ¢¯åº¦æ£€æŸ¥ç‚¹
- en: Activation checkpointing and gradient checkpointing are two distinct terms that
    refer to the same methodology. Itâ€™s very confusing but this is how it is.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æ˜¯æŒ‡åŒä¸€æ–¹æ³•çš„ä¸¤ä¸ªä¸åŒæœ¯è¯­ã€‚è¿™å¾ˆä»¤äººå›°æƒ‘ï¼Œä½†äº‹å®å°±æ˜¯å¦‚æ­¤ã€‚
- en: Gradient checkpointing allows one to trade speed for GPU memory, which either
    allows one to overcome a GPU OOM, or increase their batch size, which often leads
    to a better performance.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æ£€æŸ¥ç‚¹å…è®¸å°†é€Ÿåº¦æ¢æˆ GPU å†…å­˜ï¼Œè¿™æ ·å¯ä»¥å…‹æœ GPU OOMï¼Œæˆ–è€…å¢åŠ æ‰¹é‡å¤§å°ï¼Œé€šå¸¸ä¼šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚
- en: HF Transformers models donâ€™t know anything about DeepSpeedâ€™s activation checkpointing,
    so if you try to enable that feature in the DeepSpeed config file, nothing will
    happen.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: HF Transformers æ¨¡å‹å¯¹ DeepSpeed çš„æ¿€æ´»æ£€æŸ¥ç‚¹ä¸€æ— æ‰€çŸ¥ï¼Œå› æ­¤å¦‚æœæ‚¨å°è¯•åœ¨ DeepSpeed é…ç½®æ–‡ä»¶ä¸­å¯ç”¨è¯¥åŠŸèƒ½ï¼Œå°†ä¸ä¼šå‘ç”Ÿä»»ä½•äº‹æƒ…ã€‚
- en: 'Therefore you have two ways to take advantage of this very beneficial feature:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ‚¨æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åˆ©ç”¨è¿™ä¸ªéå¸¸æœ‰ç›Šçš„åŠŸèƒ½ï¼š
- en: If you want to use a HF Transformers models you can do `model.gradient_checkpointing_enable()`
    or use `--gradient_checkpointing` in the HF Trainer, which will automatically
    enable this for you. `torch.utils.checkpoint` is used there.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³ä½¿ç”¨ HF Transformers æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ `model.gradient_checkpointing_enable()` æˆ–åœ¨ HF
    Trainer ä¸­ä½¿ç”¨ `--gradient_checkpointing`ï¼Œè¿™å°†è‡ªåŠ¨ä¸ºæ‚¨å¯ç”¨æ­¤åŠŸèƒ½ã€‚åœ¨é‚£é‡Œä½¿ç”¨ `torch.utils.checkpoint`ã€‚
- en: If you write your own model and you want to use DeepSpeedâ€™s activation checkpointing
    you can use the [API prescribed there](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html).
    You can also take the HF Transformers modeling code and replace `torch.utils.checkpoint`
    with the DeepSpeedâ€™s API. The latter is more flexible since it allows you to offload
    the forward activations to the CPU memory instead of recalculating them.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ç¼–å†™è‡ªå·±çš„æ¨¡å‹å¹¶å¸Œæœ›ä½¿ç”¨ DeepSpeed çš„æ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥ä½¿ç”¨[é‚£é‡Œè§„å®šçš„ API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html)ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨
    HF Transformers å»ºæ¨¡ä»£ç ï¼Œå¹¶å°† `torch.utils.checkpoint` æ›¿æ¢ä¸º DeepSpeed çš„ APIã€‚åè€…æ›´çµæ´»ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨å°†å‰å‘æ¿€æ´»å¸è½½åˆ°
    CPU å†…å­˜ï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—å®ƒä»¬ã€‚
- en: Optimizer and Scheduler
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
- en: As long as you donâ€™t enable `offload_optimizer` you can mix and match DeepSpeed
    and HuggingFace schedulers and optimizers.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦ä¸å¯ç”¨ `offload_optimizer`ï¼Œæ‚¨å¯ä»¥æ··åˆä½¿ç”¨ DeepSpeed å’Œ HuggingFace è°ƒåº¦å™¨å’Œä¼˜åŒ–å™¨ã€‚
- en: It is possible to use a non-DeepSpeed optimizer when `offload_optimizer` is
    enabled, as long as it has both CPU and GPU implementation (except LAMB).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯ç”¨ `offload_optimizer` æ—¶ï¼Œå¯ä»¥ä½¿ç”¨é DeepSpeed ä¼˜åŒ–å™¨ï¼Œåªè¦å®ƒå…·æœ‰ CPU å’Œ GPU å®ç°ï¼ˆé™¤äº† LAMBï¼‰ã€‚
- en: Optimizer
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨
- en: DeepSpeedâ€™s main optimizers are Adam, AdamW, OneBitAdam, and Lamb. These have
    been thoroughly tested with ZeRO and are thus recommended to be used. It, however,
    can import other optimizers from `torch`. The full documentation is [here](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed çš„ä¸»è¦ä¼˜åŒ–å™¨æ˜¯ Adamã€AdamWã€OneBitAdam å’Œ Lambã€‚è¿™äº›å·²ç»é€šè¿‡ ZeRO è¿›è¡Œäº†å½»åº•æµ‹è¯•ï¼Œå› æ­¤å»ºè®®ä½¿ç”¨å®ƒä»¬ã€‚ä½†æ˜¯ï¼Œå®ƒå¯ä»¥ä»
    `torch` å¯¼å…¥å…¶ä»–ä¼˜åŒ–å™¨ã€‚å®Œæ•´æ–‡æ¡£åœ¨[è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters)ã€‚
- en: 'If you donâ€™t configure the `optimizer` entry in the configuration file, the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to `AdamW` and will use the supplied values or the defaults
    for the following command line arguments: `--learning_rate`, `--adam_beta1`, `--adam_beta2`,
    `--adam_epsilon` and `--weight_decay`.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½® `optimizer` æ¡ç›®ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º `AdamW`ï¼Œå¹¶å°†ä½¿ç”¨æä¾›çš„å€¼æˆ–ä»¥ä¸‹å‘½ä»¤è¡Œå‚æ•°çš„é»˜è®¤å€¼ï¼š`--learning_rate`ã€`--adam_beta1`ã€`--adam_beta2`ã€`--adam_epsilon`
    å’Œ `--weight_decay`ã€‚
- en: 'Here is an example of the auto-configured `optimizer` entry for `AdamW`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ `AdamW` çš„è‡ªåŠ¨é…ç½®çš„ `optimizer` æ¡ç›®çš„ç¤ºä¾‹ï¼š
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that the command line arguments will set the values in the configuration
    file. This is so that there is one definitive source of the values and to avoid
    hard to find errors when for example, the learning rate is set to different values
    in different places. Command line rules. The values that get overridden are:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå‘½ä»¤è¡Œå‚æ•°å°†è®¾ç½®é…ç½®æ–‡ä»¶ä¸­çš„å€¼ã€‚è¿™æ ·ä¸€æ¥ï¼Œå°±æœ‰äº†ä¸€ä¸ªæ˜ç¡®çš„å€¼æ¥æºï¼Œé¿å…äº†ä¾‹å¦‚å­¦ä¹ ç‡åœ¨ä¸åŒåœ°æ–¹è®¾ç½®ä¸ºä¸åŒå€¼æ—¶éš¾ä»¥æ‰¾åˆ°çš„é”™è¯¯ã€‚å‘½ä»¤è¡Œè§„åˆ™ã€‚è¢«è¦†ç›–çš„å€¼åŒ…æ‹¬ï¼š
- en: '`lr` with the value of `--learning_rate`'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr` çš„å€¼ä¸º `--learning_rate`'
- en: '`betas` with the value of `--adam_beta1 --adam_beta2`'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas` çš„å€¼ä¸º `--adam_beta1 --adam_beta2`'
- en: '`eps` with the value of `--adam_epsilon`'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps` çš„å€¼ä¸º `--adam_epsilon`'
- en: '`weight_decay` with the value of `--weight_decay`'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` çš„å€¼ä¸º `--weight_decay`'
- en: Therefore please remember to tune the shared hyperparameters on the command
    line.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¯·è®°ä½åœ¨å‘½ä»¤è¡Œä¸Šè°ƒæ•´å…±äº«çš„è¶…å‚æ•°ã€‚
- en: 'You can also set the values explicitly:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥æ˜¾å¼è®¾ç½®è¿™äº›å€¼ï¼š
- en: '[PRE40]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: But then youâ€™re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªè¡ŒåŒæ­¥[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å’Œ
    DeepSpeed é…ç½®ã€‚
- en: If you want to use another optimizer which is not listed above, you will have
    to add to the top level configuration.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¦ä½¿ç”¨ä¸Šé¢æœªåˆ—å‡ºçš„å…¶ä»–ä¼˜åŒ–å™¨ï¼Œåˆ™å¿…é¡»æ·»åŠ åˆ°é¡¶çº§é…ç½®ã€‚
- en: '[PRE41]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Similarly to `AdamW`, you can configure other officially supported optimizers.
    Just remember that those may have different config values. e.g. for Adam you will
    want `weight_decay` around `0.01`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ `AdamW` ç±»ä¼¼ï¼Œæ‚¨å¯ä»¥é…ç½®å…¶ä»–å®˜æ–¹æ”¯æŒçš„ä¼˜åŒ–å™¨ã€‚åªéœ€è®°ä½ï¼Œè¿™äº›å¯èƒ½å…·æœ‰ä¸åŒçš„é…ç½®å€¼ã€‚ä¾‹å¦‚ï¼Œå¯¹äº Adamï¼Œæ‚¨å°†å¸Œæœ› `weight_decay`
    å¤§çº¦ä¸º `0.01`ã€‚
- en: 'Additionally, offload works the best when itâ€™s used with Deepspeedâ€™s CPU Adam
    optimizer. If you want to use a different optimizer with offload, since `deepspeed==0.8.3`
    you need to also add:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå½“ä¸ Deepspeed çš„ CPU Adam ä¼˜åŒ–å™¨ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œå¸è½½æ•ˆæœæœ€ä½³ã€‚å¦‚æœè¦ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨è¿›è¡Œå¸è½½ï¼Œè‡ª `deepspeed==0.8.3`
    ä»¥æ¥ï¼Œæ‚¨è¿˜éœ€è¦æ·»åŠ ï¼š
- en: '[PRE42]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: to the top level configuration.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°é¡¶çº§é…ç½®ã€‚
- en: Scheduler
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨
- en: DeepSpeed supports `LRRangeTest`, `OneCycle`, `WarmupLR` and `WarmupDecayLR`
    learning rate schedulers. The full documentation is [here](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed æ”¯æŒ `LRRangeTest`ã€`OneCycle`ã€`WarmupLR` å’Œ `WarmupDecayLR` å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚å®Œæ•´æ–‡æ¡£åœ¨[è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters)ã€‚
- en: 'Here is where the schedulers overlap between ğŸ¤— Transformers and DeepSpeed:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ğŸ¤— Transformers å’Œ DeepSpeed ä¹‹é—´è°ƒåº¦å™¨é‡å çš„åœ°æ–¹ï¼š
- en: '`WarmupLR` via `--lr_scheduler_type constant_with_warmup`'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ `--lr_scheduler_type constant_with_warmup` å®ç°çš„ `WarmupLR`
- en: '`WarmupDecayLR` via `--lr_scheduler_type linear`. This is also the default
    value for `--lr_scheduler_type`, therefore, if you donâ€™t configure the scheduler
    this is scheduler that will get configured by default.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡`--lr_scheduler_type linear`é…ç½®`WarmupDecayLR`ã€‚è¿™ä¹Ÿæ˜¯`--lr_scheduler_type`çš„é»˜è®¤å€¼ï¼Œå› æ­¤ï¼Œå¦‚æœæ‚¨æ²¡æœ‰é…ç½®è°ƒåº¦ç¨‹åºï¼Œåˆ™é»˜è®¤å°†é…ç½®æ­¤è°ƒåº¦ç¨‹åºã€‚
- en: If you donâ€™t configure the `scheduler` entry in the configuration file, the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will use the values of `--lr_scheduler_type`, `--learning_rate` and `--warmup_steps`
    or `--warmup_ratio` to configure a ğŸ¤— Transformers version of it.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ²¡æœ‰åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®`scheduler`æ¡ç›®ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†ä½¿ç”¨`--lr_scheduler_type`ã€`--learning_rate`å’Œ`--warmup_steps`æˆ–`--warmup_ratio`çš„å€¼æ¥é…ç½®å…¶ğŸ¤—
    Transformersç‰ˆæœ¬ã€‚
- en: 'Here is an example of the auto-configured `scheduler` entry for `WarmupLR`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯`WarmupLR`çš„è‡ªåŠ¨é…ç½®`scheduler`æ¡ç›®ç¤ºä¾‹ï¼š
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Since *â€œautoâ€* is used the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    arguments will set the correct values in the configuration file. This is so that
    there is one definitive source of the values and to avoid hard to find errors
    when, for example, the learning rate is set to different values in different places.
    Command line rules. The values that get set are:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä½¿ç”¨äº†*â€œautoâ€*ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‚æ•°å°†åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®æ­£ç¡®çš„å€¼ã€‚è¿™æ ·ä¸€æ¥ï¼Œæ•°å€¼å°±æœ‰äº†ä¸€ä¸ªæ˜ç¡®çš„æ¥æºï¼Œé¿å…äº†ä¾‹å¦‚å­¦ä¹ ç‡åœ¨ä¸åŒåœ°æ–¹è®¾ç½®ä¸ºä¸åŒå€¼æ—¶éš¾ä»¥æ‰¾åˆ°çš„é”™è¯¯ã€‚å‘½ä»¤è¡Œè§„åˆ™ã€‚è®¾ç½®çš„å€¼åŒ…æ‹¬ï¼š
- en: '`warmup_min_lr` with the value of `0`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_min_lr`ï¼Œå…¶å€¼ä¸º`0`ã€‚'
- en: '`warmup_max_lr` with the value of `--learning_rate`.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_max_lr`ï¼Œå…¶å€¼ä¸º`--learning_rate`ã€‚'
- en: '`warmup_num_steps` with the value of `--warmup_steps` if provided. Otherwise
    will use `--warmup_ratio` multiplied by the number of training steps and rounded
    up.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæä¾›äº†`--warmup_steps`ï¼Œåˆ™`warmup_num_steps`çš„å€¼ä¸º`--warmup_steps`ã€‚å¦åˆ™ï¼Œå°†ä½¿ç”¨`--warmup_ratio`ä¹˜ä»¥è®­ç»ƒæ­¥æ•°å¹¶å››èˆäº”å…¥ã€‚
- en: '`total_num_steps` with either the value of `--max_steps` or if it is not provided,
    derived automatically at run time based on the environment and the size of the
    dataset and other command line arguments (needed for `WarmupDecayLR`).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_num_steps`ï¼Œå…¶å€¼ä¸º`--max_steps`æˆ–è€…å¦‚æœæœªæä¾›ï¼Œåˆ™åœ¨è¿è¡Œæ—¶æ ¹æ®ç¯å¢ƒå’Œæ•°æ®é›†å¤§å°ä»¥åŠå…¶ä»–å‘½ä»¤è¡Œå‚æ•°è‡ªåŠ¨æ¨å¯¼ï¼ˆå¯¹äº`WarmupDecayLR`æ˜¯å¿…éœ€çš„ï¼‰ã€‚'
- en: 'You can, of course, take over any or all of the configuration values and set
    those yourself:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæ‚¨å¯ä»¥æ¥ç®¡ä»»ä½•æˆ–æ‰€æœ‰é…ç½®å€¼ï¼Œå¹¶è‡ªè¡Œè®¾ç½®ï¼š
- en: '[PRE44]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: But then youâ€™re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªè¡ŒåŒæ­¥[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚
- en: 'For example, for `WarmupDecayLR`, you can use the following entry:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äº`WarmupDecayLR`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ¡ç›®ï¼š
- en: '[PRE45]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: and `total_num_steps`, `warmup_max_lr`, `warmup_num_steps` and `total_num_steps`
    will be set at loading time.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`total_num_steps`ã€`warmup_max_lr`ã€`warmup_num_steps`å’Œ`total_num_steps`å°†åœ¨åŠ è½½æ—¶è®¾ç½®ã€‚'
- en: fp32 Precision
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: fp32ç²¾åº¦
- en: Deepspeed supports the full fp32 and the fp16 mixed precision.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Deepspeedæ”¯æŒå®Œæ•´çš„fp32å’Œfp16æ··åˆç²¾åº¦ã€‚
- en: 'Because of the much reduced memory needs and faster speed one gets with the
    fp16 mixed precision, the only time you will want to not use it is when the model
    youâ€™re using doesnâ€™t behave well under this training mode. Typically this happens
    when the model wasnâ€™t pretrained in the fp16 mixed precision (e.g. often this
    happens with bf16-pretrained models). Such models may overflow or underflow leading
    to `NaN` loss. If this is your case then you will want to use the full fp32 mode,
    by explicitly disabling the otherwise default fp16 mixed precision mode with:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä½¿ç”¨fp16æ··åˆç²¾åº¦å¯ä»¥å¤§å¤§å‡å°‘å†…å­˜éœ€æ±‚å¹¶æé«˜é€Ÿåº¦ï¼Œå”¯ä¸€ä¸ä½¿ç”¨å®ƒçš„æƒ…å†µæ˜¯å½“æ‚¨ä½¿ç”¨çš„æ¨¡å‹åœ¨è¿™ç§è®­ç»ƒæ¨¡å¼ä¸‹è¡¨ç°ä¸ä½³æ—¶ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™ç§æƒ…å†µå‘ç”Ÿåœ¨æ¨¡å‹æ²¡æœ‰åœ¨fp16æ··åˆç²¾åº¦ä¸‹è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼ˆä¾‹å¦‚ï¼Œbf16é¢„è®­ç»ƒæ¨¡å‹ç»å¸¸å‡ºç°è¿™ç§æƒ…å†µï¼‰ã€‚è¿™æ ·çš„æ¨¡å‹å¯èƒ½ä¼šæº¢å‡ºæˆ–ä¸‹æº¢ï¼Œå¯¼è‡´`NaN`æŸå¤±ã€‚å¦‚æœæ‚¨é‡åˆ°è¿™ç§æƒ…å†µï¼Œé‚£ä¹ˆæ‚¨éœ€è¦ä½¿ç”¨å®Œæ•´çš„fp32æ¨¡å¼ï¼Œé€šè¿‡æ˜¾å¼ç¦ç”¨é»˜è®¤çš„fp16æ··åˆç²¾åº¦æ¨¡å¼ï¼š
- en: '[PRE46]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: If youâ€™re using the Ampere-architecture based GPU, pytorch version 1.7 and higher
    will automatically switch to using the much more efficient tf32 format for some
    operations, but the results will still be in fp32\. For details and benchmarks,
    please, see [TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices).
    The document includes instructions on how to disable this automatic conversion
    if for some reason you prefer not to use it.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨åŸºäºAmpereæ¶æ„çš„GPUï¼Œpytorchç‰ˆæœ¬1.7åŠæ›´é«˜ç‰ˆæœ¬å°†è‡ªåŠ¨åˆ‡æ¢åˆ°ä½¿ç”¨æ›´é«˜æ•ˆçš„tf32æ ¼å¼è¿›è¡ŒæŸäº›æ“ä½œï¼Œä½†ç»“æœä»å°†æ˜¯fp32ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯å’ŒåŸºå‡†ï¼Œè¯·å‚é˜…[TensorFloat-32(TF32)
    on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)ã€‚è¯¥æ–‡æ¡£åŒ…æ‹¬å¦‚ä½•åœ¨æŸç§æƒ…å†µä¸‹ç¦ç”¨æ­¤è‡ªåŠ¨è½¬æ¢çš„è¯´æ˜ã€‚
- en: With the ğŸ¤— Trainer you can use `--tf32` to enable it, or disable it with `--tf32
    0` or `--no_tf32`. By default the PyTorch default is used.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Trainerï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`--tf32`æ¥å¯ç”¨å®ƒï¼Œæˆ–è€…ä½¿ç”¨`--tf32 0`æˆ–`--no_tf32`æ¥ç¦ç”¨å®ƒã€‚é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨PyTorché»˜è®¤å€¼ã€‚
- en: Automatic Mixed Precision
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨æ··åˆç²¾åº¦
- en: 'You can use automatic mixed precision with either a pytorch-like AMP way or
    the apex-like way:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ç±»ä¼¼äºpytorchçš„AMPæ–¹å¼æˆ–ç±»ä¼¼äºapexçš„æ–¹å¼è¿›è¡Œè‡ªåŠ¨æ··åˆç²¾åº¦ï¼š
- en: fp16
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: fp16
- en: 'To configure pytorch AMP-like mode with fp16 (float16) set:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é…ç½®å¸¦æœ‰fp16ï¼ˆfloat16ï¼‰çš„pytorch AMPæ¨¡å¼ï¼Œè¯·è®¾ç½®ï¼š
- en: '[PRE47]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically enable or disable it based on the value of `args.fp16_backend`.
    The rest of config values are up to you.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†æ ¹æ®`args.fp16_backend`çš„å€¼è‡ªåŠ¨å¯ç”¨æˆ–ç¦ç”¨å®ƒã€‚å…¶ä½™çš„é…ç½®å€¼ç”±æ‚¨å†³å®šã€‚'
- en: This mode gets enabled when `--fp16 --fp16_backend amp` or `--fp16_full_eval`
    command line args are passed.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¼ é€’`--fp16 --fp16_backend amp`æˆ–`--fp16_full_eval`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œæ­¤æ¨¡å¼å°†è¢«å¯ç”¨ã€‚
- en: 'You can also enable/disable this mode explicitly:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥æ˜¾å¼å¯ç”¨/ç¦ç”¨æ­¤æ¨¡å¼ï¼š
- en: '[PRE48]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: But then youâ€™re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªè¡ŒåŒæ­¥[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚
- en: Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯[æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#fp16-training-options)ã€‚
- en: bf16
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bf16
- en: 'If bf16 (bfloat16) is desired instead of fp16 then the following configuration
    section is to be used:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå¸Œæœ›ä½¿ç”¨bf16ï¼ˆbfloat16ï¼‰è€Œä¸æ˜¯fp16ï¼Œåˆ™åº”ä½¿ç”¨ä»¥ä¸‹é…ç½®éƒ¨åˆ†ï¼š
- en: '[PRE49]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: bf16 has the same dynamic range as fp32 and thus doesnâ€™t require loss scaling.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: bf16å…·æœ‰ä¸fp32ç›¸åŒçš„åŠ¨æ€èŒƒå›´ï¼Œå› æ­¤ä¸éœ€è¦æŸå¤±ç¼©æ”¾ã€‚
- en: This mode gets enabled when `--bf16` or `--bf16_full_eval` command line args
    are passed.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¼ é€’`--bf16`æˆ–`--bf16_full_eval`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œå°†å¯ç”¨æ­¤æ¨¡å¼ã€‚
- en: 'You can also enable/disable this mode explicitly:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼å¯ç”¨/ç¦ç”¨æ­¤æ¨¡å¼ï¼š
- en: '[PRE50]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As of `deepspeed==0.6.0` the bf16 support is new and experimental.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: æˆªè‡³`deepspeed==0.6.0`ï¼Œbf16æ”¯æŒæ˜¯æ–°çš„å®éªŒæ€§åŠŸèƒ½ã€‚
- en: If you use [gradient accumulation](#gradient-accumulation) with bf16-enabled,
    you need to be aware that itâ€™ll accumulate gradients in bf16, which may not be
    what you want due to this formatâ€™s low precision, as it may lead to a lossy accumulation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨å¯ç”¨bf16çš„æƒ…å†µä¸‹ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œæ‚¨éœ€è¦æ³¨æ„å®ƒå°†åœ¨bf16ä¸­ç´¯ç§¯æ¢¯åº¦ï¼Œè¿™å¯èƒ½ä¸æ˜¯æ‚¨æƒ³è¦çš„ï¼Œå› ä¸ºè¿™ç§æ ¼å¼çš„ä½ç²¾åº¦å¯èƒ½å¯¼è‡´æŸå¤±çš„ç´¯ç§¯ã€‚
- en: A work is being done to fix that and provide an option to use a higher precision
    `dtype` (fp16 or fp32).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åœ¨è¿›è¡Œä¿®å¤å·¥ä½œï¼Œå¹¶æä¾›ä½¿ç”¨æ›´é«˜ç²¾åº¦`dtype`ï¼ˆfp16æˆ–fp32ï¼‰çš„é€‰é¡¹ã€‚
- en: NCCL Collectives
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NCCLé›†åˆ
- en: There is the `dtype` of the training regime and there is a separate `dtype`
    that is used for communication collectives like various reduction and gathering/scattering
    operations.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è®­ç»ƒåˆ¶åº¦çš„`dtype`ï¼Œè¿˜æœ‰ä¸€ä¸ªç”¨äºé€šä¿¡é›†åˆçš„`dtype`ã€‚
- en: All gather/scatter ops are performed in the same `dtype` the data is in, so
    if youâ€™re using bf16 training regime it gets gathered in bf16 - gathering is a
    non-lossy operation.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ”¶é›†/æ•£å¸ƒæ“ä½œéƒ½ä»¥ç›¸åŒçš„`dtype`æ‰§è¡Œï¼Œå› æ­¤å¦‚æœæ‚¨ä½¿ç”¨bf16è®­ç»ƒåˆ¶åº¦ï¼Œåˆ™ä¼šä»¥bf16è¿›è¡Œæ”¶é›†-æ”¶é›†æ˜¯ä¸€ä¸ªéæŸå¤±æ“ä½œã€‚
- en: Various reduce operations can be quite lossy, for example when gradients are
    averaged across multiple-gpus, if the communications are done in fp16 or bf16
    the outcome is likely be lossy - since when one ads multiple numbers in low precision
    the result isnâ€™t exact. More so with bf16 as it has a lower precision than fp16\.
    Often fp16 is good enough as the loss is minimal when averaging grads which are
    typically very small. Therefore, by default for half precision training fp16 is
    used as the default for reduction operations. But you have full control over this
    functionality and if you choose you can add a small overhead and ensure that reductions
    will be using fp32 as the accumulation dtype and only when the result is ready
    itâ€™ll get downcast to the half precision `dtype` youâ€™re training in.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: å„ç§å‡å°‘æ“ä½œå¯èƒ½ä¼šå¯¼è‡´å¾ˆå¤§çš„æŸå¤±ï¼Œä¾‹å¦‚å½“æ¢¯åº¦åœ¨å¤šä¸ªGPUä¸Šå¹³å‡æ—¶ï¼Œå¦‚æœé€šä¿¡ä½¿ç”¨fp16æˆ–bf16ï¼Œåˆ™ç»“æœå¯èƒ½ä¼šæœ‰æŸå¤±-å› ä¸ºåœ¨ä½ç²¾åº¦ä¸‹ç›¸åŠ å¤šä¸ªæ•°å­—æ—¶ç»“æœå¹¶ä¸ç²¾ç¡®ã€‚bf16çš„ç²¾åº¦æ¯”fp16ä½ï¼Œå› æ­¤æ›´å®¹æ˜“å‡ºç°è¿™ç§æƒ…å†µã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œfp16è¶³å¤Ÿå¥½ï¼Œå› ä¸ºåœ¨å¹³å‡æ¢¯åº¦æ—¶æŸå¤±å¾ˆå°ã€‚å› æ­¤ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå¯¹äºåŠç²¾åº¦è®­ç»ƒï¼Œå‡å°‘æ“ä½œçš„é»˜è®¤å€¼æ˜¯ä½¿ç”¨fp16ã€‚ä½†æ˜¯æ‚¨å¯ä»¥å®Œå…¨æ§åˆ¶æ­¤åŠŸèƒ½ï¼Œå¦‚æœé€‰æ‹©ï¼Œå¯ä»¥å¢åŠ ä¸€äº›å¼€é”€ï¼Œå¹¶ç¡®ä¿å‡å°‘æ“ä½œå°†ä½¿ç”¨fp32ä½œä¸ºç´¯ç§¯dtypeï¼Œä»…å½“ç»“æœå‡†å¤‡å°±ç»ªæ—¶æ‰ä¼šå°†å…¶é™çº§ä¸ºæ‚¨æ­£åœ¨è®­ç»ƒçš„åŠç²¾åº¦`dtype`ã€‚
- en: 'In order to override the default you simply add a new configuration entry:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¦†ç›–é»˜è®¤è®¾ç½®ï¼Œåªéœ€æ·»åŠ ä¸€ä¸ªæ–°çš„é…ç½®æ¡ç›®ï¼š
- en: '[PRE51]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The valid values as of this writing are â€œfp16â€, â€œbfp16â€, â€œfp32â€.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: æˆªè‡³ç›®å‰ï¼Œæœ‰æ•ˆå€¼ä¸ºâ€œfp16â€ï¼Œâ€œbfp16â€ï¼Œâ€œfp32â€ã€‚
- en: 'note: stage zero 3 had a bug with regards to bf16 comm dtype that was fixed
    in `deepspeed==0.8.1`'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šstage zero 3å­˜åœ¨å…³äºbf16 comm dtypeçš„é”™è¯¯ï¼Œå·²åœ¨`deepspeed==0.8.1`ä¸­ä¿®å¤ã€‚
- en: apex
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: apex
- en: 'To configure apex AMP-like mode set:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é…ç½®apex AMPç±»ä¼¼æ¨¡å¼ï¼Œè¯·è®¾ç½®ï¼š
- en: '[PRE52]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically configure it based on the values of `args.fp16_backend` and
    `args.fp16_opt_level`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå™¨å°†æ ¹æ®`args.fp16_backend`å’Œ`args.fp16_opt_level`çš„å€¼è‡ªåŠ¨é…ç½®ã€‚
- en: This mode gets enabled when `--fp16 --fp16_backend apex --fp16_opt_level 01`
    command line args are passed.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¼ é€’`--fp16 --fp16_backend apex --fp16_opt_level 01`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œå°†å¯ç”¨æ­¤æ¨¡å¼ã€‚
- en: 'You can also configure this mode explicitly:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼é…ç½®æ­¤æ¨¡å¼ï¼š
- en: '[PRE53]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: But then youâ€™re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªè¡ŒåŒæ­¥[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚
- en: Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯[æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options)ã€‚
- en: Batch Size
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°
- en: 'To configure batch size, use:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é…ç½®æ‰¹é‡å¤§å°ï¼Œè¯·ä½¿ç”¨ï¼š
- en: '[PRE54]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set `train_micro_batch_size_per_gpu` to the value of `args.per_device_train_batch_size`
    and `train_batch_size` to `args.world_size * args.per_device_train_batch_size
    * args.gradient_accumulation_steps`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå™¨å°†è‡ªåŠ¨å°†`train_micro_batch_size_per_gpu`è®¾ç½®ä¸º`args.per_device_train_batch_size`çš„å€¼ï¼Œå°†`train_batch_size`è®¾ç½®ä¸º`args.world_size
    * args.per_device_train_batch_size * args.gradient_accumulation_steps`çš„å€¼ã€‚
- en: 'You can also set the values explicitly:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™äº›å€¼ï¼š
- en: '[PRE55]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: But then youâ€™re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªè¡ŒåŒæ­¥[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚
- en: Gradient Accumulation
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯
- en: 'To configure gradient accumulation set:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é…ç½®æ¢¯åº¦ç´¯ç§¯ï¼Œè¯·è®¾ç½®ï¼š
- en: '[PRE56]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to the value of `args.gradient_accumulation_steps`.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå™¨å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º`args.gradient_accumulation_steps`çš„å€¼ã€‚
- en: 'You can also set the value explicitly:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®å€¼ï¼š
- en: '[PRE57]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: But then youâ€™re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªè¡ŒåŒæ­¥[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚
- en: Gradient Clipping
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¢¯åº¦è£å‰ª
- en: 'To configure gradient gradient clipping set:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®æ¢¯åº¦è£å‰ªè®¾ç½®ï¼š
- en: '[PRE58]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to the value of `args.max_grad_norm`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º`args.max_grad_norm`çš„å€¼ã€‚'
- en: 'You can also set the value explicitly:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥æ˜¾å¼è®¾ç½®è¯¥å€¼ï¼š
- en: '[PRE59]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: But then youâ€™re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªè¡ŒåŒæ­¥[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚
- en: Getting The Model Weights Out
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è·å–æ¨¡å‹æƒé‡
- en: As long as you continue training and resuming using DeepSpeed you donâ€™t need
    to worry about anything. DeepSpeed stores fp32 master weights in its custom checkpoint
    optimizer files, which are `global_step*/*optim_states.pt` (this is glob pattern),
    and are saved under the normal checkpoint.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦æ‚¨ç»§ç»­ä½¿ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒå’Œæ¢å¤ï¼Œæ‚¨å°±ä¸å¿…æ‹…å¿ƒä»»ä½•äº‹æƒ…ã€‚DeepSpeedå°†fp32ä¸»æƒé‡å­˜å‚¨åœ¨å…¶è‡ªå®šä¹‰æ£€æŸ¥ç‚¹ä¼˜åŒ–å™¨æ–‡ä»¶ä¸­ï¼Œè¿™äº›æ–‡ä»¶æ˜¯`global_step*/*optim_states.pt`ï¼ˆè¿™æ˜¯é€šé…ç¬¦ï¼‰ï¼Œå¹¶ä¿å­˜åœ¨æ­£å¸¸æ£€æŸ¥ç‚¹ä¸‹ã€‚
- en: '**FP16 Weights:**'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**FP16æƒé‡ï¼š**'
- en: When a model is saved under ZeRO-2, you end up having the normal `pytorch_model.bin`
    file with the model weights, but they are only the fp16 version of the weights.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¨¡å‹ä¿å­˜åœ¨ZeRO-2ä¸‹æ—¶ï¼Œæ‚¨æœ€ç»ˆä¼šå¾—åˆ°å¸¦æœ‰æ¨¡å‹æƒé‡çš„æ­£å¸¸`pytorch_model.bin`æ–‡ä»¶ï¼Œä½†å®ƒä»¬åªæ˜¯æƒé‡çš„fp16ç‰ˆæœ¬ã€‚
- en: 'Under ZeRO-3, things are much more complicated, since the model weights are
    partitioned out over multiple GPUs, therefore `"stage3_gather_16bit_weights_on_model_save":
    true` is required to get the `Trainer` to save the fp16 version of the weights.
    If this setting is `False` `pytorch_model.bin` wonâ€™t be created. This is because
    by default DeepSpeedâ€™s `state_dict` contains a placeholder and not the real weights.
    If we were to save this `state_dict` it wonâ€™t be possible to load it back.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ZeRO-3ä¸‹ï¼Œæƒ…å†µè¦å¤æ‚å¾—å¤šï¼Œå› ä¸ºæ¨¡å‹æƒé‡è¢«åˆ†åŒºåˆ°å¤šä¸ªGPUä¸Šï¼Œå› æ­¤éœ€è¦`"stage3_gather_16bit_weights_on_model_save":
    true`æ¥è®©`Trainer`ä¿å­˜æƒé‡çš„fp16ç‰ˆæœ¬ã€‚å¦‚æœæ­¤è®¾ç½®ä¸º`False`ï¼Œå°†ä¸ä¼šåˆ›å»º`pytorch_model.bin`ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹DeepSpeedçš„`state_dict`åŒ…å«ä¸€ä¸ªå ä½ç¬¦è€Œä¸æ˜¯çœŸæ­£çš„æƒé‡ã€‚å¦‚æœæˆ‘ä»¬ä¿å­˜è¿™ä¸ª`state_dict`ï¼Œå°†æ— æ³•åŠ è½½å›æ¥ã€‚'
- en: '[PRE60]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '**FP32 Weights:**'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '**FP32æƒé‡ï¼š**'
- en: While the fp16 weights are fine for resuming training, if you finished finetuning
    your model and want to upload it to the [models hub](https://huggingface.co/models)
    or pass it to someone else you most likely will want to get the fp32 weights.
    This ideally shouldnâ€™t be done during training since this is a process that requires
    a lot of memory, and therefore best to be performed offline after the training
    is complete. But if desired and you have plenty of free CPU memory it can be done
    in the same training script. The following sections will discuss both approaches.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶fp16æƒé‡é€‚ç”¨äºæ¢å¤è®­ç»ƒï¼Œä½†å¦‚æœæ‚¨å®Œæˆäº†å¾®è°ƒæ¨¡å‹å¹¶å¸Œæœ›å°†å…¶ä¸Šä¼ åˆ°[models hub](https://huggingface.co/models)æˆ–ä¼ é€’ç»™å…¶ä»–äººï¼Œæ‚¨å¾ˆå¯èƒ½å¸Œæœ›è·å–fp32æƒé‡ã€‚æœ€å¥½ä¸è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéœ€è¦å¤§é‡å†…å­˜çš„è¿‡ç¨‹ï¼Œå› æ­¤æœ€å¥½åœ¨è®­ç»ƒå®Œæˆåç¦»çº¿æ‰§è¡Œã€‚ä½†å¦‚æœéœ€è¦å¹¶ä¸”æ‚¨æœ‰è¶³å¤Ÿçš„ç©ºé—²CPUå†…å­˜ï¼Œå¯ä»¥åœ¨ç›¸åŒçš„è®­ç»ƒè„šæœ¬ä¸­æ‰§è¡Œã€‚ä»¥ä¸‹éƒ¨åˆ†å°†è®¨è®ºè¿™ä¸¤ç§æ–¹æ³•ã€‚
- en: '**Live FP32 Weights Recovery:**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨çº¿FP32æƒé‡æ¢å¤ï¼š**'
- en: This approach may not work if you model is large and you have little free CPU
    memory left, at the end of the training.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ¨¡å‹å¾ˆå¤§ä¸”å‰©ä½™çš„CPUå†…å­˜å¾ˆå°‘ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¸èµ·ä½œç”¨ã€‚
- en: 'If you have saved at least one checkpoint, and you want to use the latest one,
    you can do the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è‡³å°‘ä¿å­˜äº†ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå¹¶ä¸”æƒ³è¦ä½¿ç”¨æœ€æ–°çš„æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE61]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'If youâ€™re using the `--load_best_model_at_end` class:*~transformers.TrainingArguments*
    argument (to track the best checkpoint), then you can finish the training by first
    saving the final model explicitly and then do the same as above:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨`--load_best_model_at_end`ç±»ï¼š*~transformers.TrainingArguments*å‚æ•°ï¼ˆç”¨äºè·Ÿè¸ªæœ€ä½³æ£€æŸ¥ç‚¹ï¼‰ï¼Œé‚£ä¹ˆæ‚¨å¯ä»¥é€šè¿‡é¦–å…ˆæ˜¾å¼ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼Œç„¶åæ‰§è¡Œä¸ä¸Šè¿°ç›¸åŒçš„æ“ä½œæ¥å®Œæˆè®­ç»ƒï¼š
- en: '[PRE62]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Note, that once `load_state_dict_from_zero_checkpoint` was run, the `model`
    will no longer be usable in the DeepSpeed context of the same application. i.e.
    you will need to re-initialize the deepspeed engine, since `model.load_state_dict(state_dict)`
    will remove all the DeepSpeed magic from it. So do this only at the very end of
    the training.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸€æ—¦è¿è¡Œäº†`load_state_dict_from_zero_checkpoint`ï¼Œ`model`å°†ä¸å†åœ¨ç›¸åŒåº”ç”¨ç¨‹åºçš„DeepSpeedä¸Šä¸‹æ–‡ä¸­å¯ç”¨ã€‚å³æ‚¨éœ€è¦é‡æ–°åˆå§‹åŒ–deepspeedå¼•æ“ï¼Œå› ä¸º`model.load_state_dict(state_dict)`å°†ä»ä¸­åˆ é™¤æ‰€æœ‰DeepSpeedçš„é­”æ³•ã€‚å› æ­¤ï¼Œåªåœ¨è®­ç»ƒçš„æœ€åé˜¶æ®µæ‰§è¡Œæ­¤æ“ä½œã€‚
- en: Of course, you donâ€™t have to use class:*~transformers.Trainer* and you can adjust
    the examples above to your own trainer.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæ‚¨ä¸å¿…ä½¿ç”¨ç±»ï¼š*~transformers.Trainer*ï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„è®­ç»ƒå™¨è°ƒæ•´ä¸Šé¢çš„ç¤ºä¾‹ã€‚
- en: 'If for some reason you want more refinement, you can also extract the fp32
    `state_dict` of the weights and apply these yourself as is shown in the following
    example:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå‡ºäºæŸç§åŸå› æ‚¨æƒ³è¦æ›´å¤šçš„ç»†åŒ–ï¼Œæ‚¨è¿˜å¯ä»¥æå–æƒé‡çš„fp32`state_dict`å¹¶æŒ‰ç…§ä»¥ä¸‹ç¤ºä¾‹è‡ªè¡Œåº”ç”¨ï¼š
- en: '[PRE63]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '**Offline FP32 Weights Recovery:**'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¦»çº¿FP32æƒé‡æ¢å¤ï¼š**'
- en: DeepSpeed creates a special conversion script `zero_to_fp32.py` which it places
    in the top-level of the checkpoint folder. Using this script you can extract the
    weights at any point. The script is standalone and you no longer need to have
    the configuration file or a `Trainer` to do the extraction.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeedåˆ›å»ºäº†ä¸€ä¸ªç‰¹æ®Šçš„è½¬æ¢è„šæœ¬`zero_to_fp32.py`ï¼Œå¹¶å°†å…¶æ”¾åœ¨æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹çš„é¡¶å±‚ã€‚ä½¿ç”¨æ­¤è„šæœ¬ï¼Œæ‚¨å¯ä»¥åœ¨ä»»ä½•æ—¶å€™æå–æƒé‡ã€‚è¯¥è„šæœ¬æ˜¯ç‹¬ç«‹çš„ï¼Œæ‚¨ä¸å†éœ€è¦é…ç½®æ–‡ä»¶æˆ–`Trainer`æ¥æ‰§è¡Œæå–ã€‚
- en: 'Letâ€™s say your checkpoint folder looks like this:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ‚¨çš„æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE64]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In this example there is just one DeepSpeed checkpoint sub-folder *global_step1*.
    Therefore to reconstruct the fp32 weights just run:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­åªæœ‰ä¸€ä¸ªDeepSpeedæ£€æŸ¥ç‚¹å­æ–‡ä»¶å¤¹*global_step1*ã€‚å› æ­¤ï¼Œè¦é‡å»ºfp32æƒé‡ï¼Œåªéœ€è¿è¡Œï¼š
- en: '[PRE65]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This is it. `pytorch_model.bin` will now contain the full fp32 model weights
    consolidated from multiple GPUs.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ã€‚`pytorch_model.bin`ç°åœ¨å°†åŒ…å«ä»å¤šä¸ªGPUä¸­æ•´åˆçš„å®Œæ•´fp32æ¨¡å‹æƒé‡ã€‚
- en: The script will automatically be able to handle either a ZeRO-2 or ZeRO-3 checkpoint.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬å°†è‡ªåŠ¨å¤„ç†ZeRO-2æˆ–ZeRO-3æ£€æŸ¥ç‚¹ã€‚
- en: '`python zero_to_fp32.py -h` will give you usage details.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`python zero_to_fp32.py -h`å°†ä¸ºæ‚¨æä¾›ä½¿ç”¨è¯¦ç»†ä¿¡æ¯ã€‚'
- en: The script will auto-discover the deepspeed sub-folder using the contents of
    the file `latest`, which in the current example will contain `global_step1`.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬å°†ä½¿ç”¨æ–‡ä»¶`latest`çš„å†…å®¹è‡ªåŠ¨å‘ç°deepspeedå­æ–‡ä»¶å¤¹ï¼Œå½“å‰ç¤ºä¾‹ä¸­å°†åŒ…å«`global_step1`ã€‚
- en: 'Note: currently the script requires 2x general RAM of the final fp32 model
    weights.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå½“å‰è„šæœ¬éœ€è¦æœ€ç»ˆfp32æ¨¡å‹æƒé‡çš„2å€é€šç”¨RAMã€‚
- en: ZeRO-3 and Infinity Nuances
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeRO-3å’ŒInfinityç»†å¾®å·®åˆ«
- en: ZeRO-3 is quite different from ZeRO-2 because of its param sharding feature.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO-3ä¸ZeRO-2éå¸¸ä¸åŒï¼Œå› ä¸ºå®ƒå…·æœ‰å‚æ•°åˆ†ç‰‡åŠŸèƒ½ã€‚
- en: ZeRO-Infinity further extends ZeRO-3 to support NVMe memory and multiple other
    speed and scalability improvements.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO-Infinityè¿›ä¸€æ­¥æ‰©å±•äº†ZeRO-3ï¼Œä»¥æ”¯æŒNVMeå†…å­˜å’Œå¤šé¡¹å…¶ä»–é€Ÿåº¦å’Œå¯ä¼¸ç¼©æ€§æ”¹è¿›ã€‚
- en: While all the efforts were made for things to just work without needing any
    special changes to your models, in certain circumstances you may find the following
    information to be needed.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬å·²ç»å°½åŠ›ä½¿äº‹æƒ…èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼Œè€Œæ— éœ€å¯¹æ‚¨çš„æ¨¡å‹è¿›è¡Œä»»ä½•ç‰¹æ®Šæ›´æ”¹ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°éœ€è¦ä»¥ä¸‹ä¿¡æ¯ã€‚
- en: Constructing Massive Models
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ„å»ºå¤§å‹æ¨¡å‹
- en: 'DeepSpeed/ZeRO-3 can handle models with Trillions of parameters which may not
    fit onto the existing RAM. In such cases, but also if you want the initialization
    to happen much faster, initialize the model using *deepspeed.zero.Init()* context
    manager (which is also a function decorator), like so:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed/ZeRO-3å¯ä»¥å¤„ç†å…·æœ‰æ•°ä¸‡äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè¿™äº›å‚æ•°å¯èƒ½æ— æ³•é€‚åº”ç°æœ‰çš„RAMã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½†ä¹Ÿå¦‚æœæ‚¨å¸Œæœ›åˆå§‹åŒ–é€Ÿåº¦æ›´å¿«ï¼Œè¯·ä½¿ç”¨*deepspeed.zero.Init()*ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆä¹Ÿæ˜¯å‡½æ•°è£…é¥°å™¨ï¼‰åˆå§‹åŒ–æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE66]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: As you can see this gives you a randomly initialized model.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œè¿™ä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„æ¨¡å‹ã€‚
- en: 'If you want to use a pretrained model, `model_class.from_pretrained` will activate
    this feature as long as `is_deepspeed_zero3_enabled()` returns `True`, which currently
    is setup by the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object if the passed DeepSpeed configuration file contains ZeRO-3 config section.
    Thus you must create the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object **before** calling `from_pretrained`. Here is an example of a possible
    sequence:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªè¦`is_deepspeed_zero3_enabled()`è¿”å›`True`ï¼Œ`model_class.from_pretrained`å°†æ¿€æ´»æ­¤åŠŸèƒ½ï¼Œå½“å‰æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ç”±[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å¯¹è±¡è®¾ç½®çš„ï¼Œå¦‚æœä¼ é€’çš„DeepSpeedé…ç½®æ–‡ä»¶åŒ…å«ZeRO-3é…ç½®éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæ‚¨å¿…é¡»åœ¨è°ƒç”¨`from_pretrained`ä¹‹å‰åˆ›å»º[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å¯¹è±¡**ä¹‹å‰**ã€‚ä»¥ä¸‹æ˜¯å¯èƒ½çš„é¡ºåºç¤ºä¾‹ï¼š
- en: '[PRE67]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: If youâ€™re using the official example scripts and your command line arguments
    include `--deepspeed ds_config.json` with ZeRO-3 config enabled, then everything
    is already done for you, since this is how example scripts are written.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹è„šæœ¬ï¼Œå¹¶ä¸”æ‚¨çš„å‘½ä»¤è¡Œå‚æ•°åŒ…æ‹¬`--deepspeed ds_config.json`å¹¶å¯ç”¨äº†ZeRO-3é…ç½®ï¼Œåˆ™ä¸€åˆ‡éƒ½å·²ç»ä¸ºæ‚¨å®Œæˆï¼Œå› ä¸ºç¤ºä¾‹è„šæœ¬æ˜¯è¿™æ ·ç¼–å†™çš„ã€‚
- en: 'Note: If the fp16 weights of the model canâ€™t fit onto the memory of a single
    GPU this feature must be used.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå¦‚æœæ¨¡å‹çš„fp16æƒé‡æ— æ³•é€‚åº”å•ä¸ªGPUçš„å†…å­˜ï¼Œåˆ™å¿…é¡»ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
- en: For full details on this method and other related features please refer to [Constructing
    Massive Models](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ­¤æ–¹æ³•å’Œå…¶ä»–ç›¸å…³åŠŸèƒ½çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ„å»ºå¤§å‹æ¨¡å‹](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models)ã€‚
- en: Also when loading fp16-pretrained models, you will want to tell `from_pretrained`
    to use `torch_dtype=torch.float16`. For details, please, see [from_pretrained-torch-dtype](#from_pretrained-torch-dtype).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå½“åŠ è½½fp16é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨å°†å¸Œæœ›å‘Šè¯‰`from_pretrained`ä½¿ç”¨`torch_dtype=torch.float16`ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[from_pretrained-torch-dtype](#from_pretrained-torch-dtype)ã€‚
- en: Gathering Parameters
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ”¶é›†å‚æ•°
- en: Under ZeRO-3 on multiple GPUs no single GPU has all the parameters unless itâ€™s
    the parameters for the currently executing layer. So if you need to access all
    parameters from all layers at once there is a specific method to do it. Most likely
    you wonâ€™t need it, but if you do please refer to [Gathering Parameters](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šä¸ªGPUä¸Šçš„ZeRO-3ä¸­ï¼Œé™¤äº†å½“å‰æ‰§è¡Œå±‚çš„å‚æ•°å¤–ï¼Œæ²¡æœ‰å•ä¸ªGPUæ‹¥æœ‰æ‰€æœ‰å‚æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨éœ€è¦ä¸€æ¬¡è®¿é—®æ‰€æœ‰å±‚çš„æ‰€æœ‰å‚æ•°ï¼Œæœ‰ä¸€ç§ç‰¹å®šçš„æ–¹æ³•å¯ä»¥åšåˆ°ã€‚æ‚¨å¾ˆå¯èƒ½ä¸éœ€è¦å®ƒï¼Œä½†å¦‚æœéœ€è¦ï¼Œè¯·å‚é˜…[æ”¶é›†å‚æ•°](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)ã€‚
- en: We do however use it internally in several places, one such example is when
    loading pretrained model weights in `from_pretrained`. We load one layer at a
    time and immediately partition it to all participating GPUs, as for very large
    models it wonâ€™t be possible to load it on one GPU and then spread it out to multiple
    GPUs, due to memory limitations.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨å‡ ä¸ªåœ°æ–¹å†…éƒ¨ä½¿ç”¨å®ƒï¼Œä¸€ä¸ªä¾‹å­æ˜¯åœ¨`from_pretrained`ä¸­åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ—¶ã€‚æˆ‘ä»¬ä¸€æ¬¡åŠ è½½ä¸€å±‚ï¼Œç„¶åç«‹å³å°†å…¶åˆ†åŒºåˆ°æ‰€æœ‰å‚ä¸çš„GPUä¸Šï¼Œå› ä¸ºå¯¹äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œå°†å…¶åŠ è½½åˆ°ä¸€ä¸ªGPUä¸Šç„¶ååˆ†æ•£åˆ°å¤šä¸ªGPUä¸Šæ˜¯ä¸å¯èƒ½çš„ï¼Œç”±äºå†…å­˜é™åˆ¶ã€‚
- en: 'Also under ZeRO-3, if you write your own code and run into a model parameter
    weight that looks like:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œåœ¨ZeRO-3ä¸‹ï¼Œå¦‚æœæ‚¨ç¼–å†™è‡ªå·±çš„ä»£ç å¹¶é‡åˆ°çœ‹èµ·æ¥åƒæ¨¡å‹å‚æ•°æƒé‡çš„é—®é¢˜ï¼š
- en: '[PRE68]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: stress on `tensor([1.])`, or if you get an error where it says the parameter
    is of size `1`, instead of some much larger multi-dimensional shape, this means
    that the parameter is partitioned and what you see is a ZeRO-3 placeholder.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºè°ƒ`tensor([1.])`ï¼Œæˆ–è€…å¦‚æœå‡ºç°é”™è¯¯ï¼ŒæŒ‡å‡ºå‚æ•°å¤§å°ä¸º`1`ï¼Œè€Œä¸æ˜¯æŸä¸ªæ›´å¤§çš„å¤šç»´å½¢çŠ¶ï¼Œè¿™æ„å‘³ç€å‚æ•°è¢«åˆ†åŒºï¼Œæ‚¨çœ‹åˆ°çš„æ˜¯ZeRO-3å ä½ç¬¦ã€‚
- en: ZeRO Inference
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeROæ¨ç†
- en: ZeRO Inference uses the same config as ZeRO-3 Training. You just donâ€™t need
    the optimizer and scheduler sections. In fact you can leave these in the config
    file if you want to share the same one with the training. They will just be ignored.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ZeROæ¨ç†ä½¿ç”¨ä¸ZeRO-3è®­ç»ƒç›¸åŒçš„é…ç½®ã€‚æ‚¨åªéœ€è¦ä¸éœ€è¦ä¼˜åŒ–å™¨å’Œè°ƒåº¦ç¨‹åºéƒ¨åˆ†ã€‚å®é™…ä¸Šï¼Œå¦‚æœè¦ä¸è®­ç»ƒå…±äº«ç›¸åŒçš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥å°†è¿™äº›éƒ¨åˆ†ä¿ç•™åœ¨é…ç½®æ–‡ä»¶ä¸­ã€‚å®ƒä»¬å°†è¢«å¿½ç•¥ã€‚
- en: 'Otherwise you just need to pass the usual [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    arguments. For example:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™ï¼Œæ‚¨åªéœ€è¦ä¼ é€’é€šå¸¸çš„[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å‚æ•°ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE69]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The only important thing is that you need to use a ZeRO-3 configuration, since
    ZeRO-2 provides no benefit whatsoever for the inference as only ZeRO-3 performs
    sharding of parameters, whereas ZeRO-1 shards gradients and optimizer states.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€é‡è¦çš„æ˜¯æ‚¨éœ€è¦ä½¿ç”¨ZeRO-3é…ç½®ï¼Œå› ä¸ºZeRO-2å¯¹æ¨ç†æ²¡æœ‰ä»»ä½•å¥½å¤„ï¼Œå› ä¸ºåªæœ‰ZeRO-3æ‰§è¡Œå‚æ•°åˆ†ç‰‡ï¼Œè€ŒZeRO-1æ‰§è¡Œæ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„åˆ†ç‰‡ã€‚
- en: 'Here is an example of running `run_translation.py` under DeepSpeed deploying
    all available GPUs:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPUéƒ¨ç½²DeepSpeedæ—¶è¿è¡Œ`run_translation.py`çš„ç¤ºä¾‹ï¼š
- en: '[PRE70]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Since for inference there is no need for additional large memory used by the
    optimizer states and the gradients you should be able to fit much larger batches
    and/or sequence length onto the same hardware.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåœ¨æ¨ç†ä¸­ä¸éœ€è¦é¢å¤–å¤§å†…å­˜ç”¨äºä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨ç›¸åŒçš„ç¡¬ä»¶ä¸Šé€‚åº”æ›´å¤§çš„æ‰¹æ¬¡å’Œ/æˆ–åºåˆ—é•¿åº¦ã€‚
- en: Additionally DeepSpeed is currently developing a related product called Deepspeed-Inference
    which has no relationship to the ZeRO technology, but instead uses tensor parallelism
    to scale models that canâ€™t fit onto a single GPU. This is a work in progress and
    we will provide the integration once that product is complete.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒDeepSpeedç›®å‰æ­£åœ¨å¼€å‘ä¸€ä¸ªåä¸ºDeepspeed-Inferenceçš„ç›¸å…³äº§å“ï¼Œå®ƒä¸ZeROæŠ€æœ¯æ²¡æœ‰å…³ç³»ï¼Œè€Œæ˜¯ä½¿ç”¨å¼ é‡å¹¶è¡Œæ€§æ¥æ‰©å±•æ— æ³•é€‚åº”å•ä¸ªGPUçš„æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼Œä¸€æ—¦è¯¥äº§å“å®Œæˆï¼Œæˆ‘ä»¬å°†æä¾›é›†æˆã€‚
- en: Memory Requirements
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å†…å­˜è¦æ±‚
- en: Since Deepspeed ZeRO can offload memory to CPU (and NVMe) the framework provides
    utils that allow one to tell how much CPU and GPU memory will be needed depending
    on the number of GPUs being used.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºDeepspeed ZeROå¯ä»¥å°†å†…å­˜å¸è½½åˆ°CPUï¼ˆå’ŒNVMeï¼‰ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€äº›å®ç”¨ç¨‹åºï¼Œå…è®¸æ‚¨æ ¹æ®ä½¿ç”¨çš„GPUæ•°é‡å‘Šè¯‰éœ€è¦å¤šå°‘CPUå’ŒGPUå†…å­˜ã€‚
- en: 'Letâ€™s estimate how much memory is needed to finetune â€œbigscience/T0_3Bâ€ on
    a single GPU:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¼°è®¡åœ¨å•ä¸ªGPUä¸Šå¯¹â€œbigscience/T0_3Bâ€è¿›è¡Œå¾®è°ƒæ‰€éœ€çš„å†…å­˜ï¼š
- en: '[PRE71]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: So you can fit it on a single 80GB GPU and no CPU offload, or a tiny 8GB GPU
    but then need ~60GB of CPU memory. (Remember this is just the memory for params,
    optimizer states and gradients - you will need a bit more memory for cuda kernels,
    activations and temps.)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ‚¨å¯ä»¥å°†å…¶æ”¾åœ¨å•ä¸ª80GB GPUä¸Šï¼Œä¸ä½¿ç”¨CPUå¸è½½ï¼Œæˆ–è€…ä½¿ç”¨ä¸€ä¸ªå°å‹çš„8GB GPUï¼Œä½†æ˜¯éœ€è¦å¤§çº¦60GBçš„CPUå†…å­˜ã€‚è¯·è®°ä½ï¼Œè¿™åªæ˜¯å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦çš„å†…å­˜
    - æ‚¨å°†éœ€è¦æ›´å¤šå†…å­˜ç”¨äºcudaå†…æ ¸ã€æ¿€æ´»å’Œä¸´æ—¶å­˜å‚¨ã€‚
- en: Then itâ€™s a tradeoff of cost vs speed. Itâ€™ll be cheaper to buy/rent a smaller
    GPU (or less GPUs since you can use multiple GPUs with Deepspeed ZeRO. But then
    itâ€™ll be slower, so even if you donâ€™t care about how fast something will be done,
    the slowdown has a direct impact on the duration of using the GPU and thus bigger
    cost. So experiment and compare which works the best.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°±æ˜¯æˆæœ¬ä¸é€Ÿåº¦çš„æƒè¡¡ã€‚è´­ä¹°/ç§Ÿç”¨è¾ƒå°çš„GPUï¼ˆæˆ–è¾ƒå°‘çš„GPUï¼Œå› ä¸ºæ‚¨å¯ä»¥ä½¿ç”¨Deepspeed ZeROæ¥ä½¿ç”¨å¤šä¸ªGPUï¼‰ã€‚ä½†è¿™æ ·ä¼šæ›´æ…¢ï¼Œæ‰€ä»¥å³ä½¿æ‚¨ä¸å…³å¿ƒæŸä»¶äº‹æƒ…ä¼šå¤šå¿«å®Œæˆï¼Œå‡é€Ÿä¹Ÿä¼šç›´æ¥å½±å“ä½¿ç”¨GPUçš„æŒç»­æ—¶é—´ï¼Œä»è€Œå¢åŠ æˆæœ¬ã€‚å› æ­¤ï¼Œè¯·è¿›è¡Œå®éªŒå¹¶æ¯”è¾ƒå“ªç§æ–¹æ³•æœ€å¥½ã€‚
- en: If you have enough GPU memory make sure to disable the CPU/NVMe offload as itâ€™ll
    make everything faster.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼Œè¯·ç¡®ä¿ç¦ç”¨CPU/NVMeå¸è½½ï¼Œå› ä¸ºè¿™å°†ä½¿ä¸€åˆ‡æ›´å¿«ã€‚
- en: 'For example, letâ€™s repeat the same for 2 GPUs:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬é‡å¤ä½¿ç”¨2ä¸ªGPUï¼š
- en: '[PRE72]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: So here youâ€™d want 2x 32GB GPUs or higher without offloading to CPU.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ‚¨å¯èƒ½éœ€è¦2ä¸ª32GBæˆ–æ›´é«˜å†…å­˜çš„GPUï¼Œè€Œä¸éœ€è¦å°†å†…å­˜å¸è½½åˆ°CPUã€‚
- en: For full information please see [memory estimators](https://deepspeed.readthedocs.io/en/latest/memory.html).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å®Œæ•´ä¿¡æ¯ï¼Œè¯·å‚é˜…[memory estimators](https://deepspeed.readthedocs.io/en/latest/memory.html)ã€‚
- en: Filing Issues
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æäº¤é—®é¢˜
- en: Here is how to file an issue so that we could quickly get to the bottom of the
    issue and help you to unblock your work.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å¦‚ä½•æäº¤é—®é¢˜ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¿«é€Ÿæ‰¾åˆ°é—®é¢˜çš„æ ¹æºå¹¶å¸®åŠ©æ‚¨è§£é™¤å·¥ä½œé˜»å¡ã€‚
- en: 'In your report please always include:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„æŠ¥å‘Šä¸­ï¼Œè¯·å§‹ç»ˆåŒ…æ‹¬ï¼š
- en: the full Deepspeed config file in the report
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æŠ¥å‘Šä¸­æä¾›å®Œæ•´çš„Deepspeedé…ç½®æ–‡ä»¶
- en: either the command line arguments if you were using the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    or [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    arguments if you were scripting the Trainer setup yourself. Please do not dump
    the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    as it has dozens of entries that are irrelevant.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œæˆ–è€…å¦‚æœæ‚¨è‡ªå·±ç¼–å†™äº†Trainerè®¾ç½®ï¼Œåˆ™ä½¿ç”¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)å‚æ•°ã€‚è¯·ä¸è¦è½¬å‚¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ï¼Œå› ä¸ºå®ƒæœ‰æ•°åä¸ªä¸é—®é¢˜æ— å…³çš„æ¡ç›®ã€‚
- en: 'Output of:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š
- en: '[PRE73]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: If possible include a link to a Google Colab notebook that we can reproduce
    the problem with. You can use this [notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)
    as a starting point.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœå¯èƒ½çš„è¯ï¼Œè¯·åŒ…å«ä¸€ä¸ªé“¾æ¥åˆ°ä¸€ä¸ªGoogle Colabç¬”è®°æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥é‡ç°é—®é¢˜ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™ä¸ª[notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)ä½œä¸ºèµ·ç‚¹ã€‚
- en: Unless itâ€™s impossible please always use a standard dataset that we can use
    and not something custom.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é™¤éä¸å¯èƒ½ï¼Œè¯·å§‹ç»ˆä½¿ç”¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„æ ‡å‡†æ•°æ®é›†ï¼Œè€Œä¸æ˜¯è‡ªå®šä¹‰æ•°æ®é›†ã€‚
- en: If possible try to use one of the existing [examples](https://github.com/huggingface/transformers/tree/main/examples/pytorch)
    to reproduce the problem with.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœå¯èƒ½ï¼Œè¯·å°è¯•ä½¿ç”¨ç°æœ‰çš„[examples](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ä¹‹ä¸€æ¥é‡ç°é—®é¢˜ã€‚
- en: 'Things to consider:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦è€ƒè™‘çš„äº‹é¡¹ï¼š
- en: Deepspeed is often not the cause of the problem.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deepspeedé€šå¸¸ä¸æ˜¯é—®é¢˜çš„åŸå› ã€‚
- en: Some of the filed issues proved to be Deepspeed-unrelated. That is once Deepspeed
    was removed from the setup, the problem was still there.
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€äº›æäº¤çš„é—®é¢˜è¢«è¯æ˜ä¸Deepspeedæ— å…³ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ—¦ä»è®¾ç½®ä¸­ç§»é™¤äº†Deepspeedï¼Œé—®é¢˜ä»ç„¶å­˜åœ¨ã€‚
- en: Therefore, if itâ€™s not absolutely obvious itâ€™s a DeepSpeed-related problem,
    as in you can see that there is an exception and you can see that DeepSpeed modules
    are involved, first re-test your setup without DeepSpeed in it. And only if the
    problem persists then do mentioned Deepspeed and supply all the required details.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœä¸æ˜¯ç»å¯¹æ˜æ˜¾æ˜¯Deepspeedç›¸å…³çš„é—®é¢˜ï¼Œä¾‹å¦‚æ‚¨å¯ä»¥çœ‹åˆ°æœ‰å¼‚å¸¸å¹¶ä¸”å¯ä»¥çœ‹åˆ°æ¶‰åŠDeepspeedæ¨¡å—ï¼Œé¦–å…ˆåœ¨æ²¡æœ‰Deepspeedçš„è®¾ç½®ä¸­é‡æ–°æµ‹è¯•æ‚¨çš„è®¾ç½®ã€‚åªæœ‰åœ¨é—®é¢˜ä»ç„¶å­˜åœ¨æ—¶æ‰æåˆ°Deepspeedå¹¶æä¾›æ‰€æœ‰å¿…è¦çš„ç»†èŠ‚ã€‚
- en: If itâ€™s clear to you that the issue is in the DeepSpeed core and not the integration
    part, please file the Issue directly with [Deepspeed](https://github.com/microsoft/DeepSpeed/).
    If you arenâ€™t sure, please do not worry, either Issue tracker will do, we will
    figure it out once you posted it and redirect you to another Issue tracker if
    need be.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ˜ç¡®çŸ¥é“é—®é¢˜å‡ºåœ¨DeepSpeedæ ¸å¿ƒè€Œä¸æ˜¯é›†æˆéƒ¨åˆ†ï¼Œè¯·ç›´æ¥å‘[Deepspeed](https://github.com/microsoft/DeepSpeed/)
    æäº¤é—®é¢˜ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼Œä»»ä½•ä¸€ä¸ªé—®é¢˜è·Ÿè¸ªå™¨éƒ½å¯ä»¥ï¼Œæˆ‘ä»¬ä¼šåœ¨æ‚¨å‘å¸ƒåæ‰¾å‡ºé—®é¢˜ï¼Œå¹¶åœ¨éœ€è¦æ—¶å°†æ‚¨é‡å®šå‘åˆ°å¦ä¸€ä¸ªé—®é¢˜è·Ÿè¸ªå™¨ã€‚
- en: Troubleshooting
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•…éšœæ’é™¤
- en: the deepspeed process gets killed at startup without a traceback
  id: totrans-439
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ·±åº¦é€Ÿåº¦è¿›ç¨‹åœ¨å¯åŠ¨æ—¶è¢«ç»ˆæ­¢ï¼Œæ²¡æœ‰å›æº¯
- en: If the `deepspeed` process gets killed at launch time without a traceback, that
    usually means that the program tried to allocate more CPU memory than your system
    has or your process is allowed to allocate and the OS kernel killed that process.
    This is because your configuration file most likely has either `offload_optimizer`
    or `offload_param` or both configured to offload to `cpu`. If you have NVMe, experiment
    with offloading to NVMe if youâ€™re running under ZeRO-3\. Here is how you can [estimate
    how much memory is needed for a specific model](https://deepspeed.readthedocs.io/en/latest/memory.html).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ`deepspeed`è¿›ç¨‹åœ¨å¯åŠ¨æ—¶è¢«ç»ˆæ­¢ï¼Œæ²¡æœ‰å›æº¯ï¼Œé€šå¸¸æ„å‘³ç€ç¨‹åºå°è¯•åˆ†é…æ¯”æ‚¨çš„ç³»ç»Ÿå…·æœ‰çš„CPUå†…å­˜æ›´å¤šçš„å†…å­˜ï¼Œæˆ–è€…æ‚¨çš„è¿›ç¨‹è¢«å…è®¸åˆ†é…çš„å†…å­˜ï¼Œè€Œæ“ä½œç³»ç»Ÿå†…æ ¸ç»ˆæ­¢äº†è¯¥è¿›ç¨‹ã€‚è¿™æ˜¯å› ä¸ºæ‚¨çš„é…ç½®æ–‡ä»¶å¾ˆå¯èƒ½å·²ç»é…ç½®äº†`offload_optimizer`æˆ–`offload_param`æˆ–ä¸¤è€…éƒ½é…ç½®ä¸ºè½¬ç§»åˆ°`cpu`ã€‚å¦‚æœæ‚¨æœ‰NVMeï¼Œå°è¯•å°†å…¶è½¬ç§»åˆ°NVMeï¼Œå¦‚æœæ‚¨æ­£åœ¨è¿è¡ŒZeRO-3ã€‚è¿™æ˜¯å¦‚ä½•[ä¼°ç®—ç‰¹å®šæ¨¡å‹æ‰€éœ€å†…å­˜é‡](https://deepspeed.readthedocs.io/en/latest/memory.html)çš„æ–¹æ³•ã€‚
- en: training and/or eval/predict loss is NaN
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œ/æˆ–è¯„ä¼°/é¢„æµ‹æŸå¤±ä¸ºNaN
- en: This often happens when one takes a model pre-trained in bf16 mixed precision
    mode and tries to use it under fp16 (with or without mixed precision). Most models
    trained on TPU and often the ones released by Google are in this category (e.g.
    almost all t5-based models). Here the solution is to either use fp32 or bf16 if
    your hardware supports it (TPU, Ampere GPUs or newer).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¸€ä¸ªä»¥bf16æ··åˆç²¾åº¦æ¨¡å¼é¢„è®­ç»ƒçš„æ¨¡å‹å°è¯•åœ¨fp16ä¸‹ä½¿ç”¨æ—¶ï¼Œé€šå¸¸ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼ˆæ— è®ºæ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦ï¼‰ã€‚å¤§å¤šæ•°åœ¨TPUä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œé€šå¸¸æ˜¯ç”±Googleå‘å¸ƒçš„æ¨¡å‹éƒ½å±äºè¿™ä¸€ç±»ï¼ˆä¾‹å¦‚ï¼Œå‡ ä¹æ‰€æœ‰åŸºäºt5çš„æ¨¡å‹ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè§£å†³æ–¹æ¡ˆæ˜¯è¦ä¹ˆä½¿ç”¨fp32ï¼Œè¦ä¹ˆä½¿ç”¨bf16ï¼Œå¦‚æœæ‚¨çš„ç¡¬ä»¶æ”¯æŒçš„è¯ï¼ˆTPUã€Ampere
    GPUæˆ–æ›´æ–°ï¼‰ã€‚
- en: 'The other problem may have to do with using fp16\. When you configure this
    section:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé—®é¢˜å¯èƒ½ä¸ä½¿ç”¨fp16æœ‰å…³ã€‚å½“æ‚¨é…ç½®æ­¤éƒ¨åˆ†æ—¶ï¼š
- en: '[PRE74]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'and you see in your log that Deepspeed reports `OVERFLOW!` as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”æ‚¨åœ¨æ—¥å¿—ä¸­çœ‹åˆ°DeepspeedæŠ¥å‘Š`OVERFLOW!`å¦‚ä¸‹ï¼š
- en: '[PRE75]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: that means that the Deepspeed loss scaler canâ€™t figure out a scaling co-efficient
    that overcomes loss overflow.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€DeepspeedæŸå¤±ç¼©æ”¾å™¨æ— æ³•æ‰¾åˆ°ä¸€ä¸ªç¼©æ”¾ç³»æ•°æ¥å…‹æœæŸå¤±æº¢å‡ºã€‚
- en: (the log was massaged to be more readable here.)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæ­¤å¤„çš„æ—¥å¿—å·²ç»è¿‡å¤„ç†ï¼Œä»¥ä¾¿æ›´æ˜“é˜…è¯»ã€‚ï¼‰
- en: 'In this case you usually need to raise the value of `initial_scale_power`.
    Setting it to `"initial_scale_power": 32` will typically resolve the problem.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸éœ€è¦æé«˜`initial_scale_power`çš„å€¼ã€‚å°†å…¶è®¾ç½®ä¸º`"initial_scale_power": 32`é€šå¸¸ä¼šè§£å†³é—®é¢˜ã€‚'
- en: Notes
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ³¨
- en: While DeepSpeed has a pip installable PyPI package, it is highly recommended
    that it gets installed from [source](https://github.com/microsoft/deepspeed#installation)
    to best match your hardware and also if you need to enable certain features, like
    1-bit Adam, which arenâ€™t available in the pypi distribution.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è™½ç„¶DeepSpeedæœ‰ä¸€ä¸ªå¯é€šè¿‡pipå®‰è£…çš„PyPIè½¯ä»¶åŒ…ï¼Œä½†å¼ºçƒˆå»ºè®®ä»[æº](https://github.com/microsoft/deepspeed#installation)å®‰è£…ï¼Œä»¥æœ€å¥½åœ°åŒ¹é…æ‚¨çš„ç¡¬ä»¶ï¼Œå¹¶ä¸”å¦‚æœæ‚¨éœ€è¦å¯ç”¨æŸäº›åŠŸèƒ½ï¼Œæ¯”å¦‚1æ¯”ç‰¹Adamï¼Œåœ¨pypiåˆ†å‘ä¸­æ˜¯ä¸å¯ç”¨çš„ã€‚
- en: You donâ€™t have to use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    to use DeepSpeed with ğŸ¤— Transformers - you can use any model with your own trainer,
    and you will have to adapt the latter according to [the DeepSpeed integration
    instructions](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models).
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨ä¸å¿…ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ¥ä½¿ç”¨ğŸ¤—
    Transformersçš„DeepSpeed - æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ¨¡å‹ä¸æ‚¨è‡ªå·±çš„è®­ç»ƒå™¨ï¼Œå¹¶ä¸”æ‚¨å°†æ ¹æ®[DeepSpeedé›†æˆè¯´æ˜](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models)æ¥è°ƒæ•´åè€…ã€‚
- en: Non-Trainer Deepspeed Integration
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éTrainer Deepspeedé›†æˆ
- en: The [HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    is used to integrate Deepspeed into the ğŸ¤— Transformers core functionality, when
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is not used. The only thing that it does is handling Deepspeed ZeRO-3 param gathering
    and automatically splitting the model onto multiple gpus during `from_pretrained`
    call. Everything else you have to do by yourself.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    ç”¨äºå°†Deepspeedé›†æˆåˆ°ğŸ¤— Transformersæ ¸å¿ƒåŠŸèƒ½ä¸­ï¼Œå½“æœªä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ—¶ã€‚å®ƒå”¯ä¸€è¦åšçš„å°±æ˜¯å¤„ç†Deepspeed
    ZeRO-3å‚æ•°æ”¶é›†ï¼Œå¹¶åœ¨`from_pretrained`è°ƒç”¨æœŸé—´è‡ªåŠ¨å°†æ¨¡å‹åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Šã€‚å…¶ä»–æ‰€æœ‰äº‹æƒ…éƒ½éœ€è¦æ‚¨è‡ªå·±æ¥åšã€‚'
- en: When using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    everything is automatically taken care of.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    æ—¶ï¼Œä¸€åˆ‡éƒ½ä¼šè‡ªåŠ¨å¤„ç†ã€‚
- en: When not using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    to efficiently deploy DeepSpeed ZeRO-3, you must instantiate the [HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    object before instantiating the model and keep that object alive.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¸ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ—¶ï¼Œä¸ºäº†æœ‰æ•ˆéƒ¨ç½²DeepSpeed
    ZeRO-3ï¼Œæ‚¨å¿…é¡»åœ¨å®ä¾‹åŒ–æ¨¡å‹ä¹‹å‰å®ä¾‹åŒ–[HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)å¯¹è±¡ï¼Œå¹¶ä¿æŒè¯¥å¯¹è±¡å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚
- en: If youâ€™re using Deepspeed ZeRO-1 or ZeRO-2 you donâ€™t need to use `HfDeepSpeedConfig`
    at all.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨Deepspeed ZeRO-1æˆ–ZeRO-2ï¼Œåˆ™æ ¹æœ¬ä¸éœ€è¦ä½¿ç”¨`HfDeepSpeedConfig`ã€‚
- en: 'For example for a pretrained model:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºé¢„è®­ç»ƒæ¨¡å‹ï¼š
- en: '[PRE76]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'or for non-pretrained model:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–å¯¹äºéé¢„è®­ç»ƒæ¨¡å‹ï¼š
- en: '[PRE77]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Please note that if youâ€™re not using the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    integration, youâ€™re completely on your own. Basically follow the documentation
    on the [Deepspeed](https://www.deepspeed.ai/) website. Also you have to configure
    explicitly the config file - you canâ€™t use `"auto"` values and you will have to
    put real values instead.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)é›†æˆï¼Œæ‚¨å°†å®Œå…¨ç‹¬ç«‹ã€‚åŸºæœ¬ä¸Šéµå¾ª[Deepspeed](https://www.deepspeed.ai/)ç½‘ç«™ä¸Šçš„æ–‡æ¡£ã€‚æ­¤å¤–ï¼Œæ‚¨å¿…é¡»æ˜ç¡®é…ç½®é…ç½®æ–‡ä»¶
    - ä¸èƒ½ä½¿ç”¨`"auto"`å€¼ï¼Œè€Œå¿…é¡»ä½¿ç”¨å®é™…å€¼ã€‚
- en: HfDeepSpeedConfig
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HfDeepSpeedConfig
- en: '### `class transformers.integrations.HfDeepSpeedConfig`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.HfDeepSpeedConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/deepspeed.py#L56)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/deepspeed.py#L56)'
- en: '[PRE78]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Parameters
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config_file_or_dict` (`Union[str, Dict]`) â€” path to DeepSpeed config file
    or dict.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_file_or_dict`ï¼ˆ`Union[str, Dict]`ï¼‰â€” DeepSpeedé…ç½®æ–‡ä»¶æˆ–å­—å…¸çš„è·¯å¾„ã€‚'
- en: This object contains a DeepSpeed configuration dictionary and can be quickly
    queried for things like zero stage.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¯¹è±¡åŒ…å«ä¸€ä¸ªDeepSpeedé…ç½®å­—å…¸ï¼Œå¯ä»¥å¿«é€ŸæŸ¥è¯¢è¯¸å¦‚é›¶é˜¶æ®µä¹‹ç±»çš„å†…å®¹ã€‚
- en: A `weakref` of this object is stored in the moduleâ€™s globals to be able to access
    the config from areas where things like the Trainer object is not available (e.g.
    `from_pretrained` and `_get_resized_embeddings`). Therefore itâ€™s important that
    this object remains alive while the program is still running.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¯¹è±¡çš„`weakref`å­˜å‚¨åœ¨æ¨¡å—çš„å…¨å±€å˜é‡ä¸­ï¼Œä»¥ä¾¿èƒ½å¤Ÿä»Trainerå¯¹è±¡ä¸å¯ç”¨çš„åŒºåŸŸè®¿é—®é…ç½®ï¼ˆä¾‹å¦‚`from_pretrained`å’Œ`_get_resized_embeddings`ï¼‰ã€‚å› æ­¤ï¼Œåœ¨ç¨‹åºä»åœ¨è¿è¡Œæ—¶ï¼Œè¿™ä¸ªå¯¹è±¡ä¿æŒæ´»åŠ¨æ˜¯å¾ˆé‡è¦çš„ã€‚
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    uses the `HfTrainerDeepSpeedConfig` subclass instead. That subclass has logic
    to sync the configuration with values of [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    by replacing special placeholder values: `"auto"`. Without this special logic
    the DeepSpeed configuration is not modified in any way.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä½¿ç”¨`HfTrainerDeepSpeedConfig`å­ç±»ã€‚è¯¥å­ç±»å…·æœ‰å°†é…ç½®ä¸[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)çš„å€¼åŒæ­¥çš„é€»è¾‘ï¼Œé€šè¿‡æ›¿æ¢ç‰¹æ®Šå ä½ç¬¦å€¼ï¼š`"auto"`ã€‚å¦‚æœæ²¡æœ‰è¿™ç§ç‰¹æ®Šé€»è¾‘ï¼ŒDeepSpeedé…ç½®å°†ä¸ä¼šä»¥ä»»ä½•æ–¹å¼ä¿®æ”¹ã€‚'
- en: Custom DeepSpeed ZeRO Inference
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰DeepSpeed ZeROæ¨ç†
- en: Here is an example of how one could do DeepSpeed ZeRO Inference without using
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    when one canâ€™t fit a model onto a single GPU. The solution includes using additional
    GPUs or/and offloading GPU memory to CPU memory.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•åœ¨æ— æ³•å°†æ¨¡å‹æ”¾å…¥å•ä¸ªGPUçš„æƒ…å†µä¸‹è¿›è¡ŒDeepSpeed ZeROæ¨ç†ï¼Œè€Œä¸ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ã€‚è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä½¿ç”¨é¢å¤–çš„GPUå’Œ/æˆ–å°†GPUå†…å­˜è½¬ç§»åˆ°CPUå†…å­˜ã€‚
- en: The important nuance to understand here is that the way ZeRO is designed you
    can process different inputs on different GPUs in parallel.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œéœ€è¦ç†è§£çš„é‡è¦ç»†å¾®å·®åˆ«æ˜¯ï¼ŒZeROçš„è®¾è®¡æ–¹å¼ä½¿æ‚¨å¯ä»¥å¹¶è¡Œå¤„ç†ä¸åŒGPUä¸Šçš„ä¸åŒè¾“å…¥ã€‚
- en: The example has copious notes and is self-documenting.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æœ‰å¤§é‡æ³¨é‡Šå¹¶ä¸”æ˜¯è‡ªæˆ‘è®°å½•çš„ã€‚
- en: 'Make sure to:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ï¼š
- en: disable CPU offload if you have enough GPU memory (since it slows things down)
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼Œè¯·ç¦ç”¨CPUå¸è½½ï¼ˆå› ä¸ºå®ƒä¼šå‡æ…¢é€Ÿåº¦ï¼‰
- en: enable bf16 if you own an Ampere or a newer GPU to make things faster. If you
    donâ€™t have that hardware you may enable fp16 as long as you donâ€™t use any model
    that was pre-trained in bf16 mixed precision (such as most t5 models). These usually
    overflow in fp16 and you will see garbage as output.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ‹¥æœ‰Ampereæˆ–æ›´æ–°çš„GPUï¼Œè¯·å¯ç”¨bf16ä»¥åŠ å¿«é€Ÿåº¦ã€‚å¦‚æœæ‚¨æ²¡æœ‰è¯¥ç¡¬ä»¶ï¼Œå¯ä»¥å¯ç”¨fp16ï¼Œåªè¦ä¸ä½¿ç”¨åœ¨bf16æ··åˆç²¾åº¦ï¼ˆä¾‹å¦‚å¤§å¤šæ•°t5æ¨¡å‹ï¼‰ä¸­é¢„è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™äº›é€šå¸¸åœ¨fp16ä¸­æº¢å‡ºï¼Œæ‚¨å°†çœ‹åˆ°åƒåœ¾è¾“å‡ºã€‚
- en: '[PRE79]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Letâ€™s save it as `t0.py` and run it:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†å…¶ä¿å­˜ä¸º`t0.py`å¹¶è¿è¡Œï¼š
- en: '[PRE80]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This was a very basic example and you will want to adapt it to your needs.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªéå¸¸åŸºæœ¬çš„ç¤ºä¾‹ï¼Œæ‚¨å°†å¸Œæœ›æ ¹æ®è‡ªå·±çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚
- en: generate nuances
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿæˆç»†å¾®å·®åˆ«
- en: When using multiple GPUs with ZeRO Stage-3, one has to synchronize the GPUs
    by calling `generate(..., synced_gpus=True)`. If this is not done if one GPU finished
    generating before other GPUs the whole system will hang as the rest of the GPUs
    will not be able to received the shard of weights from the GPU that stopped generating.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªGPUä¸ZeRO Stage-3ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œå¿…é¡»é€šè¿‡è°ƒç”¨`generate(..., synced_gpus=True)`æ¥åŒæ­¥GPUã€‚å¦‚æœä¸è¿™æ ·åšï¼Œå¦‚æœä¸€ä¸ªGPUåœ¨å…¶ä»–GPUä¹‹å‰å®Œæˆç”Ÿæˆï¼Œæ•´ä¸ªç³»ç»Ÿå°†æŒ‚èµ·ï¼Œå› ä¸ºå…¶ä»–GPUå°†æ— æ³•æ¥æ”¶åœæ­¢ç”Ÿæˆçš„GPUçš„æƒé‡ç‰‡æ®µã€‚
- en: Starting from `transformers>=4.28`, if `synced_gpus` isnâ€™t explicitly specified,
    itâ€™ll be set to `True` automatically if these conditions are detected. But you
    can still override the value of `synced_gpus` if need to.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ä»`transformers>=4.28`å¼€å§‹ï¼Œå¦‚æœæœªæ˜ç¡®æŒ‡å®š`synced_gpus`ï¼Œåˆ™åœ¨æ£€æµ‹åˆ°è¿™äº›æ¡ä»¶æ—¶ï¼Œå®ƒå°†è‡ªåŠ¨è®¾ç½®ä¸º`True`ã€‚ä½†æ˜¯ï¼Œå¦‚æœéœ€è¦ï¼Œä»ç„¶å¯ä»¥è¦†ç›–`synced_gpus`çš„å€¼ã€‚
- en: Testing Deepspeed Integration
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµ‹è¯•Deepspeedé›†æˆ
- en: If you submit a PR that involves DeepSpeed integration please note our CircleCI
    PR CI setup has no GPUs, so we only run tests requiring gpus on a different CI
    nightly. Therefore if you get a green CI report in your PR it doesnâ€™t mean DeepSpeed
    tests pass.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æäº¤æ¶‰åŠDeepSpeedé›†æˆçš„PRï¼Œè¯·æ³¨æ„æˆ‘ä»¬çš„CircleCI PR CIè®¾ç½®æ²¡æœ‰GPUï¼Œå› æ­¤æˆ‘ä»¬åªåœ¨å¦ä¸€ä¸ªCIæ¯æ™šè¿è¡Œéœ€è¦GPUçš„æµ‹è¯•ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨åœ¨PRä¸­æ”¶åˆ°ç»¿è‰²CIæŠ¥å‘Šï¼Œè¿™å¹¶ä¸æ„å‘³ç€DeepSpeedæµ‹è¯•é€šè¿‡ã€‚
- en: 'To run DeepSpeed tests, please run at least:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿è¡ŒDeepSpeedæµ‹è¯•ï¼Œè¯·è‡³å°‘è¿è¡Œï¼š
- en: '[PRE81]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'If you changed any of the modeling or pytorch examples code, then run the model
    zoo tests as well. The following will run all DeepSpeed tests:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ›´æ”¹äº†å»ºæ¨¡æˆ–pytorchç¤ºä¾‹ä»£ç ä¸­çš„ä»»ä½•å†…å®¹ï¼Œåˆ™è¿˜è¦è¿è¡Œæ¨¡å‹åŠ¨ç‰©å›­æµ‹è¯•ã€‚ä»¥ä¸‹å°†è¿è¡Œæ‰€æœ‰DeepSpeedæµ‹è¯•ï¼š
- en: '[PRE82]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Main DeepSpeed Resources
  id: totrans-492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»DeepSpeedèµ„æº
- en: '[Projectâ€™s github](https://github.com/microsoft/deepspeed)'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é¡¹ç›®çš„github](https://github.com/microsoft/deepspeed)'
- en: '[Usage docs](https://www.deepspeed.ai/getting-started/)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨æ–‡æ¡£](https://www.deepspeed.ai/getting-started/)'
- en: '[API docs](https://deepspeed.readthedocs.io/en/latest/index.html)'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[APIæ–‡æ¡£](https://deepspeed.readthedocs.io/en/latest/index.html)'
- en: '[Blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åšå®¢æ–‡ç« ](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
- en: 'Papers:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ï¼š
- en: '[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeROï¼šé¢å‘è®­ç»ƒä¸‡äº¿å‚æ•°æ¨¡å‹çš„å†…å­˜ä¼˜åŒ–](https://arxiv.org/abs/1910.02054)'
- en: '[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Offload: æ°‘ä¸»åŒ–åäº¿è§„æ¨¡çš„æ¨¡å‹è®­ç»ƒ](https://arxiv.org/abs/2101.06840)'
- en: '[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Infinity: æ‰“ç ´ GPU å†…å­˜å£ï¼Œå®ç°æç«¯è§„æ¨¡çš„æ·±åº¦å­¦ä¹ ](https://arxiv.org/abs/2104.07857)'
- en: Finally, please, remember that, HuggingFace [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    only integrates DeepSpeed, therefore if you have any problems or questions with
    regards to DeepSpeed usage, please, file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè¯·è®°ä½ï¼ŒHuggingFace [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    åªé›†æˆäº† DeepSpeedï¼Œå› æ­¤å¦‚æœæ‚¨åœ¨ä½¿ç”¨ DeepSpeed æ–¹é¢é‡åˆ°ä»»ä½•é—®é¢˜æˆ–ç–‘é—®ï¼Œè¯·åœ¨ [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues)
    ä¸Šæäº¤é—®é¢˜ã€‚
