- en: DeepSpeed Integration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepSpeed 集成
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/deepspeed)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) implements everything described
    in the [ZeRO paper](https://arxiv.org/abs/1910.02054). Currently it provides full
    support for:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) 实现了 [ZeRO 论文](https://arxiv.org/abs/1910.02054)
    中描述的所有内容。目前，它完全支持：'
- en: Optimizer state partitioning (ZeRO stage 1)
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器状态分区（ZeRO 阶段 1）
- en: Gradient partitioning (ZeRO stage 2)
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度分区（ZeRO 阶段 2）
- en: Parameter partitioning (ZeRO stage 3)
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数分区（ZeRO 阶段 3）
- en: Custom mixed precision training handling
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义混合精度训练处理
- en: A range of fast CUDA-extension-based optimizers
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一系列基于快速 CUDA 扩展的优化器
- en: ZeRO-Offload to CPU and NVMe
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ZeRO-Offload 到 CPU 和 NVMe
- en: 'ZeRO-Offload has its own dedicated paper: [ZeRO-Offload: Democratizing Billion-Scale
    Model Training](https://arxiv.org/abs/2101.06840). And NVMe-support is described
    in the paper [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep
    Learning](https://arxiv.org/abs/2104.07857).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'ZeRO-Offload 有自己的专用论文：[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)。NVMe
    支持在论文 [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)
    中有描述。'
- en: DeepSpeed ZeRO-2 is primarily used only for training, as its features are of
    no use to inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO-2 主要仅用于训练，因为其特性对推断无用。
- en: DeepSpeed ZeRO-3 can be used for inference as well, since it allows huge models
    to be loaded on multiple GPUs, which won’t be possible on a single GPU.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO-3 也可以用于推断，因为它允许将庞大的模型加载到多个 GPU 上，这在单个 GPU 上是不可能的。
- en: '🤗 Transformers integrates [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    via 2 options:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Transformers 通过 2 个选项集成了 [DeepSpeed](https://github.com/microsoft/DeepSpeed)：
- en: Integration of the core DeepSpeed features via [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    This is an everything-done-for-you type of integration - just supply your custom
    config file or use our template and you have nothing else to do. Most of this
    document is focused on this feature.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    集成核心 DeepSpeed 功能。这是一种一切都为您完成的集成方式 - 只需提供您的自定义配置文件或使用我们的模板，您就无需做其他事情。本文档的大部分内容都集中在这个功能上。
- en: If you don’t use [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and want to use your own Trainer where you integrated DeepSpeed yourself, core
    functionality functions like `from_pretrained` and `from_config` include integration
    of essential parts of DeepSpeed like `zero.Init` for ZeRO stage 3 and higher.
    To tap into this feature read the docs on [non-Trainer DeepSpeed Integration](#nontrainer-deepspeed-integration).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您不使用 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    并希望使用自己集成了 DeepSpeed 的 Trainer，核心功能函数如 `from_pretrained` 和 `from_config` 包括 DeepSpeed
    的关键部分集成，如 ZeRO 阶段 3 及更高版本的 `zero.Init`。要使用此功能，请阅读关于 [非 Trainer DeepSpeed 集成](#nontrainer-deepspeed-integration)
    的文档。
- en: 'What is integrated:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 集成内容：
- en: 'Training:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练：
- en: DeepSpeed ZeRO training supports the full ZeRO stages 1, 2 and 3 with ZeRO-Infinity
    (CPU and NVME offload).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO 训练支持完整的 ZeRO 阶段 1、2 和 3，带有 ZeRO-Infinity（CPU 和 NVME 卸载）。
- en: 'Inference:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 推断：
- en: 'DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses
    the same ZeRO protocol as training, but it doesn’t use an optimizer and a lr scheduler
    and only stage 3 is relevant. For more details see: [zero-inference](#zero-inference).'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO 推断支持带有 ZeRO-Infinity 的 ZeRO 阶段 3。它使用与训练相同的 ZeRO 协议，但不使用优化器和学习率调度器，只有阶段
    3 与推断相关。有关更多详细信息，请参阅：[zero-inference](#zero-inference)。
- en: There is also DeepSpeed Inference - this is a totally different technology which
    uses Tensor Parallelism instead of ZeRO (coming soon).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 还有 DeepSpeed 推断 - 这是一种完全不同的技术，它使用张量并行而不是 ZeRO（即将推出）。
- en: Trainer Deepspeed Integration
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Trainer Deepspeed 集成
- en: Installation
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'Install the library via pypi:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 pypi 安装库：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'or via `transformers`’ `extras`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 或通过 `transformers` 的 `extras`：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: or find more details on [the DeepSpeed’s GitHub page](https://github.com/microsoft/deepspeed#installation)
    and [advanced install](https://www.deepspeed.ai/tutorials/advanced-install/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或在 [DeepSpeed 的 GitHub 页面](https://github.com/microsoft/deepspeed#installation)
    和 [高级安装](https://www.deepspeed.ai/tutorials/advanced-install/) 上找到更多详细信息。
- en: If you’re still struggling with the build, first make sure to read [CUDA Extension
    Installation Notes](trainer#cuda-extension-installation-notes).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仍在努力构建，请首先确保阅读 [CUDA 扩展安装说明](trainer#cuda-extension-installation-notes)。
- en: If you don’t prebuild the extensions and rely on them to be built at run time
    and you tried all of the above solutions to no avail, the next thing to try is
    to pre-build the modules before installing them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有预先构建扩展并依赖于运行时构建它们，并且尝试了以上所有解决方案仍无效，下一步尝试的是在安装之前预先构建模块。
- en: 'To make a local build for DeepSpeed:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要为 DeepSpeed 进行本地构建：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you intend to use NVMe offload you will also need to include `DS_BUILD_AIO=1`
    in the instructions above (and also install *libaio-dev* system-wide).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打算使用 NVMe 卸载，还需要在上述说明中包含 `DS_BUILD_AIO=1`（并在系统范围内安装 *libaio-dev*）。
- en: 'Edit `TORCH_CUDA_ARCH_LIST` to insert the code for the architectures of the
    GPU cards you intend to use. Assuming all your cards are the same you can get
    the arch via:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑 `TORCH_CUDA_ARCH_LIST`，插入您打算使用的 GPU 显卡的架构代码。假设所有显卡都相同，您可以通过以下方式获取架构：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So if you get `8, 6`, then use `TORCH_CUDA_ARCH_LIST="8.6"`. If you have multiple
    different cards, you can list all of them like so `TORCH_CUDA_ARCH_LIST="6.1;8.6"`
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您获得`8, 6`，那么请使用`TORCH_CUDA_ARCH_LIST="8.6"`。如果您有多张不同的显卡，可以列出所有显卡，例如`TORCH_CUDA_ARCH_LIST="6.1;8.6"`。
- en: 'If you need to use the same setup on multiple machines, make a binary wheel:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在多台机器上使用相同的设置，请制作一个二进制 wheel：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: it will generate something like `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`
    which now you can install as `pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`
    locally or on any other machine.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它将生成类似于`dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`的内容，现在您可以在本地或任何其他机器上安装为`pip
    install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`。
- en: Again, remember to ensure to adjust `TORCH_CUDA_ARCH_LIST` to the target architectures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒确保调整`TORCH_CUDA_ARCH_LIST`以匹配目标架构。
- en: You can find the complete list of NVIDIA GPUs and their corresponding **Compute
    Capabilities** (same as arch in this context) [here](https://developer.nvidia.com/cuda-gpus).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[此处](https://developer.nvidia.com/cuda-gpus)找到NVIDIA GPU的完整列表及其对应的**计算能力**（在此上下文中与架构相同）。
- en: 'You can check the archs pytorch was built with using:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令检查PyTorch构建时使用的架构：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is how to find out the arch for one of the installed GPUs. For example,
    for GPU 0:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何查找已安装GPU之一的架构。例如，对于GPU 0：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If the output is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: then you know that this card’s arch is `8.6`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么您就知道这张卡的架构是`8.6`。
- en: You can also leave `TORCH_CUDA_ARCH_LIST` out completely and then the build
    program will automatically query the architecture of the GPUs the build is made
    on. This may or may not match the GPUs on the target machines, that’s why it’s
    best to specify the desired archs explicitly.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以完全不使用`TORCH_CUDA_ARCH_LIST`，然后构建程序将自动查询构建所在的GPU的架构。这可能与目标机器上的GPU不匹配，因此最好明确指定所需的架构。
- en: If after trying everything suggested you still encounter build issues, please,
    proceed with the GitHub Issue of [Deepspeed](https://github.com/microsoft/DeepSpeed/issues),
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试了所有建议的方法仍然遇到构建问题，请继续进行[Deepspeed](https://github.com/microsoft/DeepSpeed/issues)的GitHub问题处理，
- en: Deployment with multiple GPUs
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个GPU进行部署
- en: To deploy the DeepSpeed integration adjust the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments to include a new argument `--deepspeed ds_config.json`,
    where `ds_config.json` is the DeepSpeed configuration file as documented [here](https://www.deepspeed.ai/docs/config-json/).
    The file naming is up to you. It’s recommended to use DeepSpeed’s `add_config_arguments`
    utility to add the necessary command line arguments to your code. For more information
    please see [DeepSpeed’s Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing)
    doc.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署DeepSpeed集成，请调整[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数，包括一个新参数`--deepspeed
    ds_config.json`，其中`ds_config.json`是DeepSpeed配置文件，如[此处](https://www.deepspeed.ai/docs/config-json/)所述。文件命名由您决定。建议使用DeepSpeed的`add_config_arguments`实用程序向您的代码添加必要的命令行参数。有关更多信息，请参阅[DeepSpeed的参数解析](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing)文档。
- en: 'You can use a launcher of your choice here. You can continue using the pytorch
    launcher:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处使用您选择的启动器。您可以继续使用pytorch启动器：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'or use the launcher provided by `deepspeed`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 或者使用`deepspeed`提供的启动器：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see the arguments aren’t the same, but for most needs either of them
    works. The full details on how to configure various nodes and GPUs can be found
    [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的参数不同，但对于大多数需求，任何一个都可以。有关如何配置各个节点和GPU的完整详细信息，请参阅[此处](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)。
- en: When you use the `deepspeed` launcher and you want to use all available gpus
    you can just omit the `--num_gpus` flag.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用`deepspeed`启动器并且希望使用所有可用的GPU时，您可以只省略`--num_gpus`标志。
- en: 'Here is an example of running `run_translation.py` under DeepSpeed deploying
    all available GPUs:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在DeepSpeed下部署所有可用GPU运行`run_translation.py`的示例：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that in the DeepSpeed documentation you are likely to see `--deepspeed
    --deepspeed_config ds_config.json` - i.e. two DeepSpeed-related arguments, but
    for the sake of simplicity, and since there are already so many arguments to deal
    with, we combined the two into a single argument.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在DeepSpeed文档中，您可能会看到`--deepspeed --deepspeed_config ds_config.json` - 即两个与DeepSpeed相关的参数，但为了简单起见，并且已经有很多参数要处理，我们将两者合并为一个参数。
- en: For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有关一些实际用例示例，请参阅此[帖子](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400)。
- en: Deployment with one GPU
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用单个GPU进行部署
- en: 'To deploy DeepSpeed with one GPU adjust the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个GPU部署DeepSpeed时，请调整[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数如下：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly
    to use just one GPU via `--num_gpus=1`. By default, DeepSpeed deploys all GPUs
    it can see on the given node. If you have only 1 GPU to start with, then you don’t
    need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)
    discusses the launcher options.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这与多GPU几乎相同，但在这里我们明确告诉DeepSpeed仅使用一个GPU通过`--num_gpus=1`。默认情况下，DeepSpeed部署给定节点上可以看到的所有GPU。如果您一开始只有1个GPU，则不需要此参数。以下[文档](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)讨论了启动器选项。
- en: Why would you want to use DeepSpeed with just one GPU?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要仅使用一个GPU来使用DeepSpeed？
- en: It has a ZeRO-offload feature which can delegate some computations and memory
    to the host’s CPU and RAM, and thus leave more GPU resources for model’s needs
    - e.g. larger batch size, or enabling a fitting of a very big model which normally
    won’t fit.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它具有ZeRO-offload功能，可以将一些计算和内存委托给主机的CPU和RAM，从而为模型的需求留下更多的GPU资源 - 例如更大的批量大小，或者启用一个通常无法适应的非常大的模型。
- en: It provides a smart GPU memory management system, that minimizes memory fragmentation,
    which again allows you to fit bigger models and data batches.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它提供了一个智能的GPU内存管理系统，可以最小化内存碎片化，这样可以使您适应更大的模型和数据批次。
- en: 'While we are going to discuss the configuration in details next, the key to
    getting a huge improvement on a single GPU with DeepSpeed is to have at least
    the following configuration in the configuration file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将在接下来详细讨论配置，但在DeepSpeed中获得单个GPU上的巨大改进的关键是至少在配置文件中具有以下配置：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: which enables optimizer offload and some other important features. You may experiment
    with the buffer sizes, you will find more details in the discussion below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以启用优化器卸载和一些其他重要功能。您可以尝试不同的缓冲区大小，在下面的讨论中会找到更多细节。
- en: For a practical usage example of this type of deployment, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此类型部署的实际使用示例，请参见此[帖子](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685)。
- en: You may also try the ZeRO-3 with CPU and NVMe offload as explained further in
    this document.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以尝试使用CPU和NVMe卸载的ZeRO-3，如本文档中进一步解释的那样。
- en: 'Notes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注：
- en: 'if you need to run on a specific GPU, which is different from GPU 0, you can’t
    use `CUDA_VISIBLE_DEVICES` to limit the visible scope of available GPUs. Instead,
    you have to use the following syntax:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要在特定GPU上运行，而不是GPU 0，您不能使用`CUDA_VISIBLE_DEVICES`来限制可用GPU的可见范围。相反，您必须使用以下语法：
- en: '[PRE13]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example, we tell DeepSpeed to use GPU 1 (second gpu).
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此示例中，我们告诉DeepSpeed使用GPU 1（第二个GPU）。
- en: Deployment with multiple Nodes
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多节点部署
- en: The information in this section isn’t not specific to the DeepSpeed integration
    and is applicable to any multi-node program. But DeepSpeed provides a `deepspeed`
    launcher that is easier to use than other launchers unless you are in a SLURM
    environment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的信息不是特定于DeepSpeed集成的，适用于任何多节点程序。但DeepSpeed提供了一个比其他启动器更容易使用的`deepspeed`启动器，除非您在SLURM环境中。
- en: For the duration of this section let’s assume that you have 2 nodes with 8 gpus
    each. And you can reach the first node with `ssh hostname1` and second node with
    `ssh hostname2`, and both must be able to reach each other via ssh locally without
    a password. Of course, you will need to rename these host (node) names to the
    actual host names you are working with.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的持续时间内，让我们假设您有2个每个8个GPU的节点。您可以通过`ssh hostname1`到达第一个节点，通过`ssh hostname2`到达第二个节点，并且两个节点必须能够通过本地ssh无密码地相互到达。当然，您需要将这些主机（节点）名称重命名为您正在使用的实际主机名称。
- en: The torch.distributed.run(torchrun) launcher
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: torch.distributed.run(torchrun)启动器
- en: 'For example, to use `torch.distributed.run`, you could do:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要使用`torch.distributed.run`，您可以这样做：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You have to ssh to each node and run this same command on each one of them!
    There is no rush, the launcher will wait until both nodes will synchronize.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须ssh到每个节点并在每个节点上运行相同的命令！不用着急，启动器会等待直到两个节点同步。
- en: For more information please see [torchrun](https://pytorch.org/docs/stable/elastic/run.html).
    Incidentally, this is also the launcher that replaced `torch.distributed.launch`
    a few pytorch versions back.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参见[torchrun](https://pytorch.org/docs/stable/elastic/run.html)。顺便说一句，这也是几个pytorch版本前替代了`torch.distributed.launch`的启动器。
- en: The deepspeed launcher
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: deepspeed启动器
- en: 'To use the `deepspeed` launcher instead, you have to first create a `hostfile`
    file:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`deepspeed`启动器，您首先需要创建一个`hostfile`文件：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'and then you can launch it as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以这样启动：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unlike the `torch.distributed.run` launcher, `deepspeed` will automatically
    launch this command on both nodes!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与`torch.distributed.run`启动器不同，`deepspeed`将自动在两个节点上启动此命令！
- en: For more information please see [Resource Configuration (multi-node)](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参见[资源配置（多节点）](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)。
- en: Launching in a SLURM environment
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在SLURM环境中启动
- en: In the SLURM environment the following approach can be used. The following is
    a slurm script `launch.slurm` which you will need to adapt it to your specific
    SLURM environment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在SLURM环境中可以使用以下方法。以下是一个slurm脚本`launch.slurm`，您需要根据您特定的SLURM环境进行调整。
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'All is left is to schedule it to run:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是安排它运行：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`srun` will take care of launching the program simultaneously on all nodes.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`srun`将负责同时在所有节点上启动程序。'
- en: Use of Non-shared filesystem
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非共享文件系统的使用
- en: 'By default DeepSpeed expects that a multi-node environment uses a shared storage.
    If this is not the case and each node can only see the local filesystem, you need
    to adjust the config file to include a [`checkpoint`_section](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)
    with the following setting:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，DeepSpeed期望多节点环境使用共享存储。如果不是这种情况，每个节点只能看到本地文件系统，您需要调整配置文件以包含一个[`checkpoint`_section](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)，设置如下：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Alternatively, you can also use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)’s
    `--save_on_each_node` argument, and the above config will be added automatically
    for you.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您还可以使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的`--save_on_each_node`参数，上述配置将自动添加给您。
- en: Deployment in Notebooks
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 笔记本中的部署
- en: The problem with running notebook cells as a script is that there is no normal
    `deepspeed` launcher to rely on, so under certain setups we have to emulate it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将笔记本单元格作为脚本运行的问题在于没有正常的`deepspeed`启动器可供依赖，因此在某些设置下，我们必须模拟它。
- en: If you’re using only 1 GPU, here is how you’d have to adjust your training code
    in the notebook to use DeepSpeed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只使用1个GPU，以下是您必须调整笔记本中的训练代码以使用DeepSpeed的方式。
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note: `...` stands for the normal arguments that you’d pass to the functions.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：`...`代表您将传递给函数的常规参数。
- en: If you want to use more than 1 GPU, you must use a multi-process environment
    for DeepSpeed to work. That is, you have to use the launcher for that purpose
    and this cannot be accomplished by emulating the distributed environment presented
    at the beginning of this section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用多个GPU，必须使用多进程环境才能使DeepSpeed正常工作。也就是说，您必须使用该目的的启动器，而不能通过模拟本节开头介绍的分布式环境来实现。
- en: 'If you want to create the config file on the fly in the notebook in the current
    directory, you could have a dedicated cell with:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在当前目录的笔记本中即时创建配置文件，可以使用专用单元格：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If the training script is in a normal file and not in the notebook cells, you
    can launch `deepspeed` normally via shell from a cell. For example, to use `run_translation.py`
    you would launch it with:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练脚本在普通文件中而不是在笔记本单元格中，您可以从单元格中正常启动`deepspeed`。例如，要使用`run_translation.py`，您可以这样启动它：
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'or with `%%bash` magic, where you can write a multi-line code for the shell
    program to run:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 或者使用`%%bash`魔术，您可以编写多行代码供shell程序运行：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In such case you don’t need any of the code presented at the beginning of this
    section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您不需要本节开头呈现的任何代码。
- en: 'Note: While `%%bash` magic is neat, but currently it buffers the output so
    you won’t see the logs until the process completes.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：虽然`%%bash`魔术很好，但目前它会缓冲输出，因此在进程完成之前您看不到日志。
- en: Configuration
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置
- en: For the complete guide to the DeepSpeed configuration options that can be used
    in its configuration file please refer to the [following documentation](https://www.deepspeed.ai/docs/config-json/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有关DeepSpeed配置文件中可用的DeepSpeed配置选项的完整指南，请参阅[以下文档](https://www.deepspeed.ai/docs/config-json/)。
- en: 'You can find dozens of DeepSpeed configuration examples that address various
    practical needs in [the DeepSpeedExamples repo](https://github.com/microsoft/DeepSpeedExamples):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[DeepSpeedExamples存储库](https://github.com/microsoft/DeepSpeedExamples)中找到数十个解决各种实际需求的DeepSpeed配置示例：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Continuing the code from above, let’s say you’re looking to configure the Lamb
    optimizer. So you can search through the example `.json` files with:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '继续上面的代码，假设您想配置Lamb优化器。因此，您可以搜索示例`.json`文件： '
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Some more examples are to be found in the [main repo](https://github.com/microsoft/DeepSpeed)
    as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在[主存储库](https://github.com/microsoft/DeepSpeed)中还可以找到更多示例。
- en: When using DeepSpeed you always need to supply a DeepSpeed configuration file,
    yet some configuration parameters have to be configured via the command line.
    You will find the nuances in the rest of this guide.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DeepSpeed时，您始终需要提供一个DeepSpeed配置文件，但是某些配置参数必须通过命令行进行配置。您将在本指南的其余部分中找到细微差别。
- en: 'To get an idea of what DeepSpeed configuration file looks like, here is one
    that activates ZeRO stage 2 features, including optimizer states cpu offload,
    uses `AdamW` optimizer and `WarmupLR` scheduler and will enable mixed precision
    training if `--fp16` is passed:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解DeepSpeed配置文件的外观，这里有一个激活ZeRO阶段2功能的示例，包括优化器状态cpu卸载，使用`AdamW`优化器和`WarmupLR`调度程序，并且如果传递了`--fp16`，将启用混合精度训练：
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When you execute the program, DeepSpeed will log the configuration it received
    from the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    to the console, so you can see exactly what was the final configuration passed
    to it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当您执行程序时，DeepSpeed将记录从[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)接收到的配置到控制台，因此您可以看到最终传递给它的配置。
- en: Passing Configuration
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传递配置
- en: As discussed in this document normally the DeepSpeed configuration is passed
    as a path to a json file, but if you’re not using the command line interface to
    configure the training, and instead instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    via [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    then for the `deepspeed` argument you can pass a nested `dict`. This allows you
    to create the configuration on the fly and doesn’t require you to write it to
    the file system before passing it to [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如本文所述，通常将DeepSpeed配置作为json文件的路径传递，但如果您不使用命令行界面配置训练，而是通过[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)实例化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，那么对于`deepspeed`参数，您可以传递一个嵌套的`dict`。这允许您即时创建配置，而无需将其写入文件系统后再传递给[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)。
- en: 'To summarize you can do:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，您可以执行以下操作：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'or:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 或者：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Shared Configuration
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享配置
- en: This section is a must-read
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是必读的
- en: Some configuration values are required by both the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and DeepSpeed to function correctly, therefore, to prevent conflicting definitions,
    which could lead to hard to detect errors, we chose to configure those via the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 某些配置值对于[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)和DeepSpeed的正确运行都是必需的，因此，为了防止冲突的定义，可能导致难以检测的错误，我们选择通过[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数进行配置。
- en: Additionally, some configuration values are derived automatically based on the
    model’s configuration, so instead of remembering to manually adjust multiple values,
    it’s the best to let the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    do the majority of configuration for you.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些配置值是根据模型的配置自动派生的，因此，与其记住手动调整多个值，不如让[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)为您完成大部分配置。
- en: 'Therefore, in the rest of this guide you will find a special configuration
    value: `auto`, which when set will be automatically replaced with the correct
    or most efficient value. Please feel free to choose to ignore this recommendation
    and set the values explicitly, in which case be very careful that your the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    arguments and DeepSpeed configurations agree. For example, are you using the same
    learning rate, or batch size, or gradient accumulation settings? if these mismatch
    the training may fail in very difficult to detect ways. You have been warned.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本指南的其余部分中，您将找到一个特殊的配置值：`auto`，设置后将自动替换为正确或最有效的值。请随意选择忽略此建议并显式设置值，在这种情况下，请非常小心，确保您的[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)参数和DeepSpeed配置一致。例如，您是否使用相同的学习率、批量大小或梯度累积设置？如果这些不匹配，训练可能会以非常难以检测的方式失败。您已经被警告了。
- en: There are multiple other values that are specific to DeepSpeed-only and those
    you will have to set manually to suit your needs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 还有多个其他值是专门针对DeepSpeed的，您将需要手动设置以满足您的需求。
- en: 'In your own programs, you can also use the following approach if you’d like
    to modify the DeepSpeed config as a master and configure [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    based on that. The steps are:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在您自己的程序中，如果您想要以主控的方式修改DeepSpeed配置并基于此配置[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    ，您也可以使用以下方法。步骤如下：
- en: Create or load the DeepSpeed configuration to be used as a master configuration
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建或加载要用作主配置的DeepSpeed配置
- en: Create the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object based on these values
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些值创建[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)对象
- en: Do note that some values, such as `scheduler.params.total_num_steps` are calculated
    by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    during `train`, but you can of course do the math yourself.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一些值，例如`scheduler.params.total_num_steps`是由[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)在`train`期间计算的，但您当然也可以自己进行计算。
- en: ZeRO
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeRO
- en: '[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/)
    is the workhorse of DeepSpeed. It supports 3 different levels (stages) of optimization.
    The first one is not quite interesting for scalability purposes, therefore this
    document focuses on stages 2 and 3\. Stage 3 is further improved by the latest
    addition of ZeRO-Infinity. You will find more indepth information in the DeepSpeed
    documentation.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/)
    是DeepSpeed的主要工具。它支持3个不同级别（阶段）的优化。第一个对于可伸缩性目的并不太有趣，因此本文档侧重于阶段2和3。阶段3通过最新的ZeRO-Infinity进一步改进。您可以在DeepSpeed文档中找到更详细的信息。'
- en: The `zero_optimization` section of the configuration file is the most important
    part ([docs](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)),
    since that is where you define which ZeRO stages you want to enable and how to
    configure them. You will find the explanation for each parameter in the DeepSpeed
    docs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件中的`zero_optimization`部分是最重要的部分（[文档](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)），因为在那里您定义了要启用哪些ZeRO阶段以及如何配置它们。您可以在DeepSpeed文档中找到每个参数的解释。
- en: This section has to be configured exclusively via DeepSpeed configuration -
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    provides no equivalent command line arguments.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分必须通过DeepSpeed配置进行独占配置 - [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    不提供等效的命令行参数。
- en: 'Note: currently DeepSpeed doesn’t validate parameter names, so if you misspell
    any, it’ll use the default setting for the parameter that got misspelled. You
    can watch the DeepSpeed engine start up log messages to see what values it is
    going to use.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：目前DeepSpeed不验证参数名称，因此如果您拼写错误，它将使用拼写错误的参数的默认设置。您可以查看DeepSpeed引擎启动日志消息，以查看它将使用哪些值。
- en: ZeRO-2 Config
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-2配置
- en: 'The following is an example of configuration for ZeRO stage 2:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是ZeRO阶段2的配置示例：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Performance tuning:**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能调优：**'
- en: 'enabling `offload_optimizer` should reduce GPU RAM usage (it requires `"stage":
    2`)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '启用`offload_optimizer`应该减少GPU RAM的使用（需要`"stage": 2`）'
- en: '`"overlap_comm": true` trades off increased GPU RAM usage to lower all-reduce
    latency. `overlap_comm` uses 4.5x the `allgather_bucket_size` and `reduce_bucket_size`
    values. So if they are set to 5e8, this requires a 9GB footprint (`5e8 x 2Bytes
    x 2 x 4.5`). Therefore, if you have a GPU with 8GB or less RAM, to avoid getting
    OOM-errors you will need to reduce those parameters to about `2e8`, which would
    require 3.6GB. You will want to do the same on larger capacity GPU as well, if
    you’re starting to hit OOM.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"overlap_comm": true` 通过增加GPU RAM使用量来降低全局归约延迟。`overlap_comm` 使用4.5倍的`allgather_bucket_size`和`reduce_bucket_size`值。因此，如果它们设置为5e8，这将需要9GB的占用空间（`5e8
    x 2字节 x 2 x 4.5`）。因此，如果您的GPU具有8GB或更少的RAM，为了避免出现OOM错误，您需要将这些参数减少到约`2e8`，这将需要3.6GB。如果您的GPU容量更大，但开始出现OOM错误，您也需要做同样的操作。'
- en: when reducing these buffers you’re trading communication speed to avail more
    GPU RAM. The smaller the buffer size is, the slower the communication gets, and
    the more GPU RAM will be available to other tasks. So if a bigger batch size is
    important, getting a slightly slower training time could be a good trade.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当减少这些缓冲区时，您正在交换通信速度以获得更多的GPU RAM。缓冲区大小越小，通信速度越慢，可用于其他任务的GPU RAM就越多。因此，如果更大的批量大小很重要，稍微减慢训练时间可能是一个不错的交易。
- en: 'Additionally, `deepspeed==0.4.4` added a new option `round_robin_gradients`
    which you can enable with:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`deepspeed==0.4.4`添加了一个新选项`round_robin_gradients`，您可以通过以下方式启用：
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is a stage 2 optimization for CPU offloading that parallelizes gradient
    copying to CPU memory among ranks by fine-grained gradient partitioning. Performance
    benefit grows with gradient accumulation steps (more copying between optimizer
    steps) or GPU count (increased parallelism).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于CPU卸载的阶段2优化，通过细粒度梯度分区将梯度复制到CPU内存中，以在等级之间并行化。性能收益随着梯度累积步骤（在优化器步骤之间的更多复制）或GPU数量（增加并行性）而增加。
- en: ZeRO-3 Config
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-3配置
- en: 'The following is an example of configuration for ZeRO stage 3:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是ZeRO阶段3的配置示例：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you are getting OOMs, because your model or activations don’t fit into the
    GPU memory and you have unutilized CPU memory offloading the optimizer states
    and parameters to CPU memory with `"device": "cpu"` may solve this limitation.
    If you don’t want to offload to CPU memory, use `none` instead of `cpu` for the
    `device` entry. Offloading to NVMe is discussed further down.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您遇到OOM，因为您的模型或激活不适合GPU内存，并且您有未使用的CPU内存，将优化器状态和参数卸载到CPU内存并使用`"device": "cpu"`可能解决此限制。如果您不想卸载到CPU内存，请在`device`条目中使用`none`而不是`cpu`。有关卸载到NVMe的更多信息，请参阅下文。'
- en: Pinned memory is enabled with `pin_memory` set to `true`. This feature can improve
    the throughput at the cost of making less memory available to other processes.
    Pinned memory is set aside to the specific process that requested it and its typically
    accessed much faster than normal CPU memory.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`pin_memory`设置为`true`启用了固定内存。这个功能可以提高吞吐量，但会减少其他进程可用的内存。固定内存被保留给请求它的特定进程，通常比普通CPU内存访问速度快得多。
- en: '**Performance tuning:**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能调优：**'
- en: '`stage3_max_live_parameters`: `1e9`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_max_live_parameters`：`1e9`'
- en: '`stage3_max_reuse_distance`: `1e9`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_max_reuse_distance`：`1e9`'
- en: If hitting OOM reduce `stage3_max_live_parameters` and `stage3_max_reuse_distance`.
    They should have minimal impact on performance unless you are doing activation
    checkpointing. `1e9` would consume ~2GB. The memory is shared by `stage3_max_live_parameters`
    and `stage3_max_reuse_distance`, so it’s not additive, it’s just 2GB total.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到OOM，请减少`stage3_max_live_parameters`和`stage3_max_reuse_distance`。除非进行激活检查点，否则它们对性能影响很小。`1e9`将消耗约2GB。内存由`stage3_max_live_parameters`和`stage3_max_reuse_distance`共享，因此不是累加的，而是总共2GB。
- en: '`stage3_max_live_parameters` is the upper limit on how many full parameters
    you want to keep on the GPU at any given time. “reuse distance” is a metric we
    are using to figure out when will a parameter be used again in the future, and
    we use the `stage3_max_reuse_distance` to decide whether to throw away the parameter
    or to keep it. If a parameter is going to be used again in near future (less than
    `stage3_max_reuse_distance`) then we keep it to reduce communication overhead.
    This is super helpful when you have activation checkpointing enabled, where we
    do a forward recompute and backward passes a single layer granularity and want
    to keep the parameter in the forward recompute till the backward'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`stage3_max_live_parameters`是您希望在任何给定时间保留在GPU上的完整参数的上限。"重用距离"是我们使用的度量标准，用于确定参数在未来何时再次使用，我们使用`stage3_max_reuse_distance`来决定是丢弃参数还是保留参数。如果参数将在不久的将来（小于`stage3_max_reuse_distance`）再次使用，则我们保留它以减少通信开销。当启用激活检查点时，这非常有帮助，我们在前向重计算和反向传递中以单层粒度执行操作，并希望在前向重计算中保留参数直到反向传递。'
- en: 'The following configuration values depend on the model’s hidden size:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配置值取决于模型的隐藏大小：
- en: '`reduce_bucket_size`: `hidden_size*hidden_size`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_bucket_size`：`hidden_size*hidden_size`'
- en: '`stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_prefetch_bucket_size`：`0.9 * hidden_size * hidden_size`'
- en: '`stage3_param_persistence_threshold`: `10 * hidden_size`'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage3_param_persistence_threshold`：`10 * hidden_size`'
- en: therefore set these values to `auto` and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically assign the recommended values. But, of course, feel free to
    set these explicitly as well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此将这些值设置为`auto`，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将自动分配推荐值。当然，您也可以显式设置这些值。
- en: '`stage3_gather_16bit_weights_on_model_save` enables model fp16 weights consolidation
    when model gets saved. With large models and multiple GPUs this is an expensive
    operation both in terms of memory and speed. It’s currently required if you plan
    to resume the training. Watch out for future updates that will remove this limitation
    and make things more flexible.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`stage3_gather_16bit_weights_on_model_save`在模型保存时启用模型fp16权重合并。对于大型模型和多个GPU，这是一项昂贵的操作，无论是在内存还是速度方面。如果您计划恢复训练，则目前需要这样做。请注意未来的更新将消除此限制并使事情更加灵活。'
- en: If you’re migrating from ZeRO-2 configuration note that `allgather_partitions`,
    `allgather_bucket_size` and `reduce_scatter` configuration parameters are not
    used in ZeRO-3\. If you keep these in the config file they will just be ignored.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在从ZeRO-2配置迁移，请注意`allgather_partitions`、`allgather_bucket_size`和`reduce_scatter`配置参数在ZeRO-3中不使用。如果您将这些保留在配置文件中，它们将被忽略。
- en: '`sub_group_size`: `1e9`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sub_group_size`：`1e9`'
- en: '`sub_group_size` controls the granularity in which parameters are updated during
    optimizer steps. Parameters are grouped into buckets of `sub_group_size` and each
    buckets is updated one at a time. When used with NVMe offload in ZeRO-Infinity,
    `sub_group_size` therefore controls the granularity in which model states are
    moved in and out of CPU memory from NVMe during the optimizer step. This prevents
    running out of CPU memory for extremely large models.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`sub_group_size`控制参数在优化器步骤中更新的粒度。参数被分组到`sub_group_size`的桶中，每个桶依次更新。在ZeRO-Infinity中与NVMe卸载一起使用时，`sub_group_size`因此控制模型状态在优化器步骤期间从NVMe移入和移出CPU内存的粒度。这可以防止极大型模型耗尽CPU内存。'
- en: 'You can leave `sub_group_size` to its default value of *1e9* when not using
    NVMe offload. You may want to change its default value in the following cases:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不使用NVMe卸载，可以将`sub_group_size`保留为默认值*1e9*。在以下情况下，您可能需要更改其默认值：
- en: 'Running into OOM during optimizer step: Reduce `sub_group_size` to reduce memory
    utilization of temporary buffers'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在优化器步骤中遇到OOM：减少`sub_group_size`以减少临时缓冲区的内存利用
- en: 'Optimizer Step is taking a long time: Increase `sub_group_size` to improve
    bandwidth utilization as a result of the increased data buffers.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器步骤花费很长时间：增加`sub_group_size`以提高带宽利用率，因为数据缓冲区增加。
- en: ZeRO-0 Config
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-0配置
- en: Note that we’re listing Stage 0 and 1 last since they are rarely used.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将阶段0和1列在最后，因为它们很少被使用。
- en: 'Stage 0 is disabling all types of sharding and just using DeepSpeed as DDP.
    You can turn it on with:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段0是禁用所有类型的分片，只使用DeepSpeed作为DDP。您可以通过以下方式打开它：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This will essentially disable ZeRO without you needing to change anything else.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这将基本上禁用ZeRO，而无需更改其他任何内容。
- en: ZeRO-1 Config
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-1配置
- en: 'Stage 1 is Stage 2 minus gradient sharding. You can always try it to speed
    things a tiny bit to only shard the optimizer states with:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段1是阶段2减去梯度分片。您可以尝试将优化器状态分片，以加快速度：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: NVMe Support
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NVMe支持
- en: ZeRO-Infinity allows for training incredibly large models by extending GPU and
    CPU memory with NVMe memory. Thanks to smart partitioning and tiling algorithms
    each GPU needs to send and receive very small amounts of data during offloading
    so modern NVMe proved to be fit to allow for an even larger total memory pool
    available to your training process. ZeRO-Infinity requires ZeRO-3 enabled.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO-Infinity通过使用NVMe内存扩展GPU和CPU内存，允许训练非常大的模型。由于智能分区和平铺算法，每个GPU在卸载期间需要发送和接收非常少量的数据，因此现代NVMe被证明适合允许更大的总内存池可用于您的训练过程。ZeRO-Infinity需要启用ZeRO-3。
- en: 'The following configuration example enables NVMe to offload both optimizer
    states and the params:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配置示例启用了NVMe以卸载优化器状态和参数：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can choose to offload both optimizer states and params to NVMe, or just
    one of them or none. For example, if you have copious amounts of CPU memory available,
    by all means offload to CPU memory only as it’d be faster (hint: *“device”: “cpu”*).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以选择将优化器状态和参数都卸载到NVMe，或者只卸载其中一个或两者都不卸载。例如，如果您有大量的CPU内存可用，尽管将其仅卸载到CPU内存，因为这样会更快（提示：“device”:
    “cpu”）。'
- en: Here is the full documentation for offloading [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading)
    and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是有关卸载[优化器状态](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading)和[参数](https://www.deepspeed.ai/docs/config-json/#parameter-offloading)的完整文档。
- en: Make sure that your `nvme_path` is actually an NVMe, since it will work with
    the normal hard drive or SSD, but it’ll be much much slower. The fast scalable
    training was designed with modern NVMe transfer speeds in mind (as of this writing
    one can have ~3.5GB/s read, ~3GB/s write peak speeds).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的`nvme_path`实际上是一个NVMe，因为它可以与普通硬盘或固态硬盘一起使用，但速度会慢得多。快速可扩展的训练是根据现代NVMe传输速度设计的（截至本文撰写时，读取速度约为3.5GB/s，写入速度约为3GB/s）。
- en: In order to figure out the optimal `aio` configuration block you must run a
    benchmark on your target setup, as [explained here](https://github.com/microsoft/DeepSpeed/issues/998).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出最佳的`aio`配置块，您必须在目标设置上运行基准测试，如[此处所述](https://github.com/microsoft/DeepSpeed/issues/998)。
- en: ZeRO-2 vs ZeRO-3 Performance
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-2与ZeRO-3性能
- en: ZeRO-3 is likely to be slower than ZeRO-2 if everything else is configured the
    same because the former has to gather model weights in addition to what ZeRO-2
    does. If ZeRO-2 meets your needs and you don’t need to scale beyond a few GPUs
    then you may choose to stick to it. It’s important to understand that ZeRO-3 enables
    a much higher scalability capacity at a cost of speed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切配置相同，ZeRO-3可能比ZeRO-2慢，因为前者需要收集模型权重以外的内容。如果ZeRO-2满足您的需求，并且您不需要扩展到几个GPU之外，那么您可以选择坚持使用它。重要的是要了解，ZeRO-3在速度上的代价是实现更高的可伸缩性容量。
- en: 'It’s possible to adjust ZeRO-3 configuration to make it perform closer to ZeRO-2:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调整ZeRO-3配置，使其性能接近ZeRO-2：
- en: set `stage3_param_persistence_threshold` to a very large number - larger than
    the largest parameter, e.g., `6 * hidden_size * hidden_size`. This will keep the
    parameters on the GPUs.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`stage3_param_persistence_threshold`设置为一个非常大的数字 - 大于最大参数，例如`6 * hidden_size
    * hidden_size`。这将使参数保留在GPU上。
- en: turn off `offload_params` since ZeRO-2 doesn’t have that option.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关闭`offload_params`，因为ZeRO-2没有该选项。
- en: The performance will likely improve significantly with just `offload_params`
    turned off, even if you don’t change `stage3_param_persistence_threshold`. Of
    course, these changes will impact the size of the model you can train. So these
    help you to trade scalability for speed depending on your needs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您不更改`stage3_param_persistence_threshold`，只要关闭`offload_params`，性能可能会显着提高。当然，这些更改将影响您可以训练的模型大小。因此，这些帮助您根据需要在可伸缩性和速度之间进行权衡。
- en: ZeRO-2 Example
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-2示例
- en: 'Here is a full ZeRO-2 auto-configuration file `ds_config_zero2.json`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个完整的ZeRO-2自动配置文件`ds_config_zero2.json`：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here is a full ZeRO-2 all-enabled manually set configuration file. It is here
    mainly for you to see what the typical values look like, but we highly recommend
    using the one with multiple `auto` settings in it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个完整的ZeRO-2全启用手动设置的配置文件。这里主要是让您看看典型值是什么样的，但我们强烈建议使用其中带有多个`auto`设置的配置文件。
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ZeRO-3 Example
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeRO-3示例
- en: 'Here is a full ZeRO-3 auto-configuration file `ds_config_zero3.json`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个完整的ZeRO-3自动配置文件`ds_config_zero3.json`：
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here is a full ZeRO-3 all-enabled manually set configuration file. It is here
    mainly for you to see what the typical values look like, but we highly recommend
    using the one with multiple `auto` settings in it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个完整的ZeRO-3全启用手动设置的配置文件。这里主要是让您看看典型值是什么样的，但我们强烈建议使用其中带有多个`auto`设置的配置文件。
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How to Choose Which ZeRO Stage and Offloads To Use For Best Performance
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何选择最佳性能的ZeRO阶段和卸载方式
- en: So now you know there are all these different stages. How to decide which of
    them to use? This section will attempt to address this question.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道有所有这些不同的阶段。如何决定使用其中哪一个？本节将尝试回答这个问题。
- en: 'In general the following applies:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，以下内容适用：
- en: Speed-wise (left is faster than right)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度方面（左边比右边快）
- en: Stage 0 (DDP) > Stage 1 > Stage 2 > Stage 2 + offload > Stage 3 > Stage 3 +
    offloads
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段0（DDP）> 阶段1 > 阶段2 > 阶段2 + 卸载 > 阶段3 > 阶段3 + 卸载
- en: GPU Memory usage-wise (right is more GPU memory efficient than left)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU内存使用方面（右侧比左侧更节省GPU内存）
- en: Stage 0 (DDP) < Stage 1 < Stage 2 < Stage 2 + offload < Stage 3 < Stage 3 +
    offloads
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段0（DDP）<阶段1<阶段2<阶段2 +卸载<阶段3<阶段3 +卸载
- en: So when you want to get the fastest execution while fitting into minimal number
    of GPUs, here is the process you could follow. We start with the fastest approach
    and if running into GPU OOM we then go to the next slower approach, but which
    will use less GPU memory. And so on and so forth.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当您希望在最少数量的GPU中获得最快的执行时，请按照以下过程进行。我们从最快的方法开始，如果遇到GPU OOM，则转向下一个较慢的方法，但将使用更少的GPU内存。依此类推。
- en: First of all set batch size to 1 (you can always use gradient accumulation for
    any desired effective batch size).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 首先将批处理大小设置为1（您始终可以使用梯度累积来获得任何所需的有效批处理大小）。
- en: Enable `--gradient_checkpointing 1` (HF Trainer) or directly `model.gradient_checkpointing_enable()`
    - if OOM then
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`--gradient_checkpointing 1`（HF Trainer）或直接`model.gradient_checkpointing_enable()`-如果OOM，则
- en: Try ZeRO stage 2 first. if OOM then
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先尝试ZeRO阶段2。如果OOM，则
- en: Try ZeRO stage 2 + `offload_optimizer` - if OOM then
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试ZeRO阶段2 + `offload_optimizer`-如果OOM，则
- en: Switch to ZeRO stage 3 - if OOM then
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到ZeRO阶段3-如果OOM，则
- en: Enable `offload_param` to `cpu` - if OOM then
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`offload_param`到`cpu`-如果OOM，则
- en: Enable `offload_optimizer` to `cpu` - if OOM then
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`offload_optimizer`到`cpu`-如果OOM，则
- en: If you still can’t fit a batch size of 1 first check various default values
    and lower them if you can. For example, if you use `generate` and you don’t use
    a wide search beam make it narrower as it’d take a lot of memory.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您仍然无法适应批处理大小为1，请首先检查各种默认值，并在可能的情况下将其降低。例如，如果您使用`generate`，并且不使用宽搜索光束，请将其变窄，因为这将占用大量内存。
- en: Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher
    GPUs and fp16 on older gpu architectures.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绝对使用混合半精度而不是fp32-因此在Ampere及更高GPU上使用bf16，在旧的GPU架构上使用fp16。
- en: If you still OOM you could add more hardware or enable ZeRO-Infinity - that
    is switch offloads `offload_param` and `offload_optimizer` to `nvme`. You need
    to make sure it’s a very fast nvme. As an anecdote I was able to infer BLOOM-176B
    on a tiny GPU using ZeRO-Infinity except it was extremely slow. But it worked!
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您仍然OOM，您可以添加更多硬件或启用ZeRO-Infinity-即切换卸载`offload_param`和`offload_optimizer`到`nvme`。您需要确保它是一个非常快速的nvme。作为一个轶事，我能够在一个小型GPU上推断BLOOM-176B，使用ZeRO-Infinity，只是速度极慢。但它有效！
- en: You can, of course, work through these steps in reverse by starting with the
    most GPU memory efficient config and then going backwards. Or try bi-secting it.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以通过从最GPU内存高效的配置开始，然后向后进行，或者尝试二分法来逆向执行这些步骤。
- en: Once you have your batch size 1 not leading to OOM, measure your effective throughput.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的批处理大小为1不会导致OOM，请测量您的有效吞吐量。
- en: Next try to increase the batch size to as large as you can, since the higher
    the batch size the more efficient the GPUs are as they perform the best when matrices
    they multiply are huge.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来尝试将批处理大小增加到尽可能大，因为批处理大小越大，GPU的效率就越高，因为它们在乘法矩阵很大时表现最佳。
- en: Now the performance optimization game starts. You can turn off some offload
    features or step down in ZeRO stages and increase/decrease batch size and again
    measure your effective throughput. Rinse and repeat until satisfied.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在性能优化游戏开始了。您可以关闭一些卸载功能或降低ZeRO阶段，并增加/减少批处理大小，然后再测量您的有效吞吐量。反复进行，直到满意为止。
- en: Don’t spend forever on it, but if you’re about to start a 3 months training
    - do spend a few days on it to find the most effective throughput-wise setup.
    So that your training cost will be the lowest and you will finish training faster.
    In the current crazy-paced ML world, if it takes you an extra month to train something
    you are likely to miss a golden opportunity. Of course, this is only me sharing
    an observation and in no way I’m trying to rush you. Before beginning to train
    BLOOM-176B I spent 2 days on this process and was able to increase throughput
    from 90 to 150 TFLOPs! This effort saved us more than one month of training time.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 不要花太多时间在上面，但如果您即将开始为期3个月的培训-请花几天时间找到最有效的吞吐量设置。这样，您的培训成本将最低，您将更快地完成培训。在当前快节奏的ML世界中，如果您需要额外一个月来训练某些内容，您很可能会错过一个绝佳的机会。当然，这只是我分享的一个观察，我绝不会催促您。在开始训练BLOOM-176B之前，我花了2天时间进行这个过程，并且能够将吞吐量从90提高到150
    TFLOPs！这一努力为我们节省了一个多月的培训时间。
- en: These notes were written primarily for the training mode, but they should mostly
    apply for inference as well. For example, during inference Gradient Checkpointing
    is a no-op since it is only useful during training. Additionally, we found out
    that if you are doing a multi-GPU inference and not using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/),
    [Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) should
    provide a superior performance.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些注意事项主要是针对训练模式编写的，但它们在推断方面也应该大多适用。例如，在推断期间，梯度检查点是无效的，因为它只在训练期间有用。此外，我们发现，如果您正在进行多GPU推断并且不使用[DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/)，[Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts)应该提供更优越的性能。
- en: 'Other quick related performance notes:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其他快速相关的性能注意事项：
- en: if you are training something from scratch always try to have tensors with shapes
    that are divisible by 16 (e.g. hidden size). For batch size try divisible by 2
    at least. There are [wave and tile quanitization](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)
    divisibility that is hardware-specific if you want to squeeze even higher performance
    from your GPUs.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在从头开始训练某些内容，请始终尝试具有可被16整除的张量形状（例如隐藏大小）。对于批处理大小，请至少尝试可被2整除。如果您想从GPU中挤取更高的性能，则有硬件特定的[波和瓷砖量化](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)可被整除。
- en: Activation Checkpointing or Gradient Checkpointing
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活检查点或梯度检查点
- en: Activation checkpointing and gradient checkpointing are two distinct terms that
    refer to the same methodology. It’s very confusing but this is how it is.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 激活检查点和梯度检查点是指同一方法的两个不同术语。这很令人困惑，但事实就是如此。
- en: Gradient checkpointing allows one to trade speed for GPU memory, which either
    allows one to overcome a GPU OOM, or increase their batch size, which often leads
    to a better performance.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点允许将速度换成 GPU 内存，这样可以克服 GPU OOM，或者增加批量大小，通常会带来更好的性能。
- en: HF Transformers models don’t know anything about DeepSpeed’s activation checkpointing,
    so if you try to enable that feature in the DeepSpeed config file, nothing will
    happen.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: HF Transformers 模型对 DeepSpeed 的激活检查点一无所知，因此如果您尝试在 DeepSpeed 配置文件中启用该功能，将不会发生任何事情。
- en: 'Therefore you have two ways to take advantage of this very beneficial feature:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您有两种方法可以利用这个非常有益的功能：
- en: If you want to use a HF Transformers models you can do `model.gradient_checkpointing_enable()`
    or use `--gradient_checkpointing` in the HF Trainer, which will automatically
    enable this for you. `torch.utils.checkpoint` is used there.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想使用 HF Transformers 模型，可以使用 `model.gradient_checkpointing_enable()` 或在 HF
    Trainer 中使用 `--gradient_checkpointing`，这将自动为您启用此功能。在那里使用 `torch.utils.checkpoint`。
- en: If you write your own model and you want to use DeepSpeed’s activation checkpointing
    you can use the [API prescribed there](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html).
    You can also take the HF Transformers modeling code and replace `torch.utils.checkpoint`
    with the DeepSpeed’s API. The latter is more flexible since it allows you to offload
    the forward activations to the CPU memory instead of recalculating them.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您编写自己的模型并希望使用 DeepSpeed 的激活检查点，可以使用[那里规定的 API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html)。您还可以使用
    HF Transformers 建模代码，并将 `torch.utils.checkpoint` 替换为 DeepSpeed 的 API。后者更灵活，因为它允许您将前向激活卸载到
    CPU 内存，而不是重新计算它们。
- en: Optimizer and Scheduler
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器和调度器
- en: As long as you don’t enable `offload_optimizer` you can mix and match DeepSpeed
    and HuggingFace schedulers and optimizers.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 只要不启用 `offload_optimizer`，您可以混合使用 DeepSpeed 和 HuggingFace 调度器和优化器。
- en: It is possible to use a non-DeepSpeed optimizer when `offload_optimizer` is
    enabled, as long as it has both CPU and GPU implementation (except LAMB).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用 `offload_optimizer` 时，可以使用非 DeepSpeed 优化器，只要它具有 CPU 和 GPU 实现（除了 LAMB）。
- en: Optimizer
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化器
- en: DeepSpeed’s main optimizers are Adam, AdamW, OneBitAdam, and Lamb. These have
    been thoroughly tested with ZeRO and are thus recommended to be used. It, however,
    can import other optimizers from `torch`. The full documentation is [here](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed 的主要优化器是 Adam、AdamW、OneBitAdam 和 Lamb。这些已经通过 ZeRO 进行了彻底测试，因此建议使用它们。但是，它可以从
    `torch` 导入其他优化器。完整文档在[这里](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters)。
- en: 'If you don’t configure the `optimizer` entry in the configuration file, the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to `AdamW` and will use the supplied values or the defaults
    for the following command line arguments: `--learning_rate`, `--adam_beta1`, `--adam_beta2`,
    `--adam_epsilon` and `--weight_decay`.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在配置文件中不配置 `optimizer` 条目，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    将自动将其设置为 `AdamW`，并将使用提供的值或以下命令行参数的默认值：`--learning_rate`、`--adam_beta1`、`--adam_beta2`、`--adam_epsilon`
    和 `--weight_decay`。
- en: 'Here is an example of the auto-configured `optimizer` entry for `AdamW`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `AdamW` 的自动配置的 `optimizer` 条目的示例：
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that the command line arguments will set the values in the configuration
    file. This is so that there is one definitive source of the values and to avoid
    hard to find errors when for example, the learning rate is set to different values
    in different places. Command line rules. The values that get overridden are:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，命令行参数将设置配置文件中的值。这样一来，就有了一个明确的值来源，避免了例如学习率在不同地方设置为不同值时难以找到的错误。命令行规则。被覆盖的值包括：
- en: '`lr` with the value of `--learning_rate`'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr` 的值为 `--learning_rate`'
- en: '`betas` with the value of `--adam_beta1 --adam_beta2`'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`betas` 的值为 `--adam_beta1 --adam_beta2`'
- en: '`eps` with the value of `--adam_epsilon`'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps` 的值为 `--adam_epsilon`'
- en: '`weight_decay` with the value of `--weight_decay`'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay` 的值为 `--weight_decay`'
- en: Therefore please remember to tune the shared hyperparameters on the command
    line.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，请记住在命令行上调整共享的超参数。
- en: 'You can also set the values explicitly:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以显式设置这些值：
- en: '[PRE40]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要自行同步[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数和
    DeepSpeed 配置。
- en: If you want to use another optimizer which is not listed above, you will have
    to add to the top level configuration.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用上面未列出的其他优化器，则必须添加到顶级配置。
- en: '[PRE41]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Similarly to `AdamW`, you can configure other officially supported optimizers.
    Just remember that those may have different config values. e.g. for Adam you will
    want `weight_decay` around `0.01`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `AdamW` 类似，您可以配置其他官方支持的优化器。只需记住，这些可能具有不同的配置值。例如，对于 Adam，您将希望 `weight_decay`
    大约为 `0.01`。
- en: 'Additionally, offload works the best when it’s used with Deepspeed’s CPU Adam
    optimizer. If you want to use a different optimizer with offload, since `deepspeed==0.8.3`
    you need to also add:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当与 Deepspeed 的 CPU Adam 优化器一起使用时，卸载效果最佳。如果要使用不同的优化器进行卸载，自 `deepspeed==0.8.3`
    以来，您还需要添加：
- en: '[PRE42]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: to the top level configuration.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 到顶级配置。
- en: Scheduler
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调度器
- en: DeepSpeed supports `LRRangeTest`, `OneCycle`, `WarmupLR` and `WarmupDecayLR`
    learning rate schedulers. The full documentation is [here](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed 支持 `LRRangeTest`、`OneCycle`、`WarmupLR` 和 `WarmupDecayLR` 学习率调度器。完整文档在[这里](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters)。
- en: 'Here is where the schedulers overlap between 🤗 Transformers and DeepSpeed:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这是🤗 Transformers 和 DeepSpeed 之间调度器重叠的地方：
- en: '`WarmupLR` via `--lr_scheduler_type constant_with_warmup`'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `--lr_scheduler_type constant_with_warmup` 实现的 `WarmupLR`
- en: '`WarmupDecayLR` via `--lr_scheduler_type linear`. This is also the default
    value for `--lr_scheduler_type`, therefore, if you don’t configure the scheduler
    this is scheduler that will get configured by default.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`--lr_scheduler_type linear`配置`WarmupDecayLR`。这也是`--lr_scheduler_type`的默认值，因此，如果您没有配置调度程序，则默认将配置此调度程序。
- en: If you don’t configure the `scheduler` entry in the configuration file, the
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will use the values of `--lr_scheduler_type`, `--learning_rate` and `--warmup_steps`
    or `--warmup_ratio` to configure a 🤗 Transformers version of it.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有在配置文件中配置`scheduler`条目，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将使用`--lr_scheduler_type`、`--learning_rate`和`--warmup_steps`或`--warmup_ratio`的值来配置其🤗
    Transformers版本。
- en: 'Here is an example of the auto-configured `scheduler` entry for `WarmupLR`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`WarmupLR`的自动配置`scheduler`条目示例：
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Since *“auto”* is used the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    arguments will set the correct values in the configuration file. This is so that
    there is one definitive source of the values and to avoid hard to find errors
    when, for example, the learning rate is set to different values in different places.
    Command line rules. The values that get set are:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了*“auto”*，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)参数将在配置文件中设置正确的值。这样一来，数值就有了一个明确的来源，避免了例如学习率在不同地方设置为不同值时难以找到的错误。命令行规则。设置的值包括：
- en: '`warmup_min_lr` with the value of `0`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_min_lr`，其值为`0`。'
- en: '`warmup_max_lr` with the value of `--learning_rate`.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warmup_max_lr`，其值为`--learning_rate`。'
- en: '`warmup_num_steps` with the value of `--warmup_steps` if provided. Otherwise
    will use `--warmup_ratio` multiplied by the number of training steps and rounded
    up.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提供了`--warmup_steps`，则`warmup_num_steps`的值为`--warmup_steps`。否则，将使用`--warmup_ratio`乘以训练步数并四舍五入。
- en: '`total_num_steps` with either the value of `--max_steps` or if it is not provided,
    derived automatically at run time based on the environment and the size of the
    dataset and other command line arguments (needed for `WarmupDecayLR`).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_num_steps`，其值为`--max_steps`或者如果未提供，则在运行时根据环境和数据集大小以及其他命令行参数自动推导（对于`WarmupDecayLR`是必需的）。'
- en: 'You can, of course, take over any or all of the configuration values and set
    those yourself:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以接管任何或所有配置值，并自行设置：
- en: '[PRE44]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要自行同步[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数和DeepSpeed配置。
- en: 'For example, for `WarmupDecayLR`, you can use the following entry:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于`WarmupDecayLR`，您可以使用以下条目：
- en: '[PRE45]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: and `total_num_steps`, `warmup_max_lr`, `warmup_num_steps` and `total_num_steps`
    will be set at loading time.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`total_num_steps`、`warmup_max_lr`、`warmup_num_steps`和`total_num_steps`将在加载时设置。'
- en: fp32 Precision
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: fp32精度
- en: Deepspeed supports the full fp32 and the fp16 mixed precision.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Deepspeed支持完整的fp32和fp16混合精度。
- en: 'Because of the much reduced memory needs and faster speed one gets with the
    fp16 mixed precision, the only time you will want to not use it is when the model
    you’re using doesn’t behave well under this training mode. Typically this happens
    when the model wasn’t pretrained in the fp16 mixed precision (e.g. often this
    happens with bf16-pretrained models). Such models may overflow or underflow leading
    to `NaN` loss. If this is your case then you will want to use the full fp32 mode,
    by explicitly disabling the otherwise default fp16 mixed precision mode with:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用fp16混合精度可以大大减少内存需求并提高速度，唯一不使用它的情况是当您使用的模型在这种训练模式下表现不佳时。通常情况下，这种情况发生在模型没有在fp16混合精度下进行预训练时（例如，bf16预训练模型经常出现这种情况）。这样的模型可能会溢出或下溢，导致`NaN`损失。如果您遇到这种情况，那么您需要使用完整的fp32模式，通过显式禁用默认的fp16混合精度模式：
- en: '[PRE46]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: If you’re using the Ampere-architecture based GPU, pytorch version 1.7 and higher
    will automatically switch to using the much more efficient tf32 format for some
    operations, but the results will still be in fp32\. For details and benchmarks,
    please, see [TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices).
    The document includes instructions on how to disable this automatic conversion
    if for some reason you prefer not to use it.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用基于Ampere架构的GPU，pytorch版本1.7及更高版本将自动切换到使用更高效的tf32格式进行某些操作，但结果仍将是fp32。有关详细信息和基准，请参阅[TensorFloat-32(TF32)
    on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)。该文档包括如何在某种情况下禁用此自动转换的说明。
- en: With the 🤗 Trainer you can use `--tf32` to enable it, or disable it with `--tf32
    0` or `--no_tf32`. By default the PyTorch default is used.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 使用🤗 Trainer，您可以使用`--tf32`来启用它，或者使用`--tf32 0`或`--no_tf32`来禁用它。默认情况下使用PyTorch默认值。
- en: Automatic Mixed Precision
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动混合精度
- en: 'You can use automatic mixed precision with either a pytorch-like AMP way or
    the apex-like way:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用类似于pytorch的AMP方式或类似于apex的方式进行自动混合精度：
- en: fp16
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: fp16
- en: 'To configure pytorch AMP-like mode with fp16 (float16) set:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置带有fp16（float16）的pytorch AMP模式，请设置：
- en: '[PRE47]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically enable or disable it based on the value of `args.fp16_backend`.
    The rest of config values are up to you.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将根据`args.fp16_backend`的值自动启用或禁用它。其余的配置值由您决定。'
- en: This mode gets enabled when `--fp16 --fp16_backend amp` or `--fp16_full_eval`
    command line args are passed.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 当传递`--fp16 --fp16_backend amp`或`--fp16_full_eval`命令行参数时，此模式将被启用。
- en: 'You can also enable/disable this mode explicitly:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以显式启用/禁用此模式：
- en: '[PRE48]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要自行同步[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数和DeepSpeed配置。
- en: Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[文档](https://www.deepspeed.ai/docs/config-json/#fp16-training-options)。
- en: bf16
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bf16
- en: 'If bf16 (bfloat16) is desired instead of fp16 then the following configuration
    section is to be used:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果希望使用bf16（bfloat16）而不是fp16，则应使用以下配置部分：
- en: '[PRE49]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: bf16 has the same dynamic range as fp32 and thus doesn’t require loss scaling.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: bf16具有与fp32相同的动态范围，因此不需要损失缩放。
- en: This mode gets enabled when `--bf16` or `--bf16_full_eval` command line args
    are passed.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 当传递`--bf16`或`--bf16_full_eval`命令行参数时，将启用此模式。
- en: 'You can also enable/disable this mode explicitly:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以显式启用/禁用此模式：
- en: '[PRE50]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As of `deepspeed==0.6.0` the bf16 support is new and experimental.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 截至`deepspeed==0.6.0`，bf16支持是新的实验性功能。
- en: If you use [gradient accumulation](#gradient-accumulation) with bf16-enabled,
    you need to be aware that it’ll accumulate gradients in bf16, which may not be
    what you want due to this format’s low precision, as it may lead to a lossy accumulation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在启用bf16的情况下使用梯度累积，您需要注意它将在bf16中累积梯度，这可能不是您想要的，因为这种格式的低精度可能导致损失的累积。
- en: A work is being done to fix that and provide an option to use a higher precision
    `dtype` (fp16 or fp32).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正在进行修复工作，并提供使用更高精度`dtype`（fp16或fp32）的选项。
- en: NCCL Collectives
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NCCL集合
- en: There is the `dtype` of the training regime and there is a separate `dtype`
    that is used for communication collectives like various reduction and gathering/scattering
    operations.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练制度的`dtype`，还有一个用于通信集合的`dtype`。
- en: All gather/scatter ops are performed in the same `dtype` the data is in, so
    if you’re using bf16 training regime it gets gathered in bf16 - gathering is a
    non-lossy operation.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 所有收集/散布操作都以相同的`dtype`执行，因此如果您使用bf16训练制度，则会以bf16进行收集-收集是一个非损失操作。
- en: Various reduce operations can be quite lossy, for example when gradients are
    averaged across multiple-gpus, if the communications are done in fp16 or bf16
    the outcome is likely be lossy - since when one ads multiple numbers in low precision
    the result isn’t exact. More so with bf16 as it has a lower precision than fp16\.
    Often fp16 is good enough as the loss is minimal when averaging grads which are
    typically very small. Therefore, by default for half precision training fp16 is
    used as the default for reduction operations. But you have full control over this
    functionality and if you choose you can add a small overhead and ensure that reductions
    will be using fp32 as the accumulation dtype and only when the result is ready
    it’ll get downcast to the half precision `dtype` you’re training in.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 各种减少操作可能会导致很大的损失，例如当梯度在多个GPU上平均时，如果通信使用fp16或bf16，则结果可能会有损失-因为在低精度下相加多个数字时结果并不精确。bf16的精度比fp16低，因此更容易出现这种情况。通常情况下，fp16足够好，因为在平均梯度时损失很小。因此，默认情况下，对于半精度训练，减少操作的默认值是使用fp16。但是您可以完全控制此功能，如果选择，可以增加一些开销，并确保减少操作将使用fp32作为累积dtype，仅当结果准备就绪时才会将其降级为您正在训练的半精度`dtype`。
- en: 'In order to override the default you simply add a new configuration entry:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 要覆盖默认设置，只需添加一个新的配置条目：
- en: '[PRE51]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The valid values as of this writing are “fp16”, “bfp16”, “fp32”.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，有效值为“fp16”，“bfp16”，“fp32”。
- en: 'note: stage zero 3 had a bug with regards to bf16 comm dtype that was fixed
    in `deepspeed==0.8.1`'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：stage zero 3存在关于bf16 comm dtype的错误，已在`deepspeed==0.8.1`中修复。
- en: apex
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: apex
- en: 'To configure apex AMP-like mode set:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置apex AMP类似模式，请设置：
- en: '[PRE52]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically configure it based on the values of `args.fp16_backend` and
    `args.fp16_opt_level`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 训练器将根据`args.fp16_backend`和`args.fp16_opt_level`的值自动配置。
- en: This mode gets enabled when `--fp16 --fp16_backend apex --fp16_opt_level 01`
    command line args are passed.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当传递`--fp16 --fp16_backend apex --fp16_opt_level 01`命令行参数时，将启用此模式。
- en: 'You can also configure this mode explicitly:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以显式配置此模式：
- en: '[PRE53]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要自行同步[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数和DeepSpeed配置。
- en: Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[文档](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options)。
- en: Batch Size
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量大小
- en: 'To configure batch size, use:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置批量大小，请使用：
- en: '[PRE54]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set `train_micro_batch_size_per_gpu` to the value of `args.per_device_train_batch_size`
    and `train_batch_size` to `args.world_size * args.per_device_train_batch_size
    * args.gradient_accumulation_steps`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 训练器将自动将`train_micro_batch_size_per_gpu`设置为`args.per_device_train_batch_size`的值，将`train_batch_size`设置为`args.world_size
    * args.per_device_train_batch_size * args.gradient_accumulation_steps`的值。
- en: 'You can also set the values explicitly:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以显式设置这些值：
- en: '[PRE55]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要自行同步[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数和DeepSpeed配置。
- en: Gradient Accumulation
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度累积
- en: 'To configure gradient accumulation set:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置梯度累积，请设置：
- en: '[PRE56]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to the value of `args.gradient_accumulation_steps`.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 训练器将自动将其设置为`args.gradient_accumulation_steps`的值。
- en: 'You can also set the value explicitly:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以显式设置值：
- en: '[PRE57]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要自行同步[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数和DeepSpeed配置。
- en: Gradient Clipping
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度裁剪
- en: 'To configure gradient gradient clipping set:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 配置梯度裁剪设置：
- en: '[PRE58]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: and the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will automatically set it to the value of `args.max_grad_norm`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将自动将其设置为`args.max_grad_norm`的值。'
- en: 'You can also set the value explicitly:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以显式设置该值：
- en: '[PRE59]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: But then you’re on your own synchronizing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    command line arguments and the DeepSpeed configuration.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要自行同步[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)命令行参数和DeepSpeed配置。
- en: Getting The Model Weights Out
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取模型权重
- en: As long as you continue training and resuming using DeepSpeed you don’t need
    to worry about anything. DeepSpeed stores fp32 master weights in its custom checkpoint
    optimizer files, which are `global_step*/*optim_states.pt` (this is glob pattern),
    and are saved under the normal checkpoint.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 只要您继续使用DeepSpeed进行训练和恢复，您就不必担心任何事情。DeepSpeed将fp32主权重存储在其自定义检查点优化器文件中，这些文件是`global_step*/*optim_states.pt`（这是通配符），并保存在正常检查点下。
- en: '**FP16 Weights:**'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**FP16权重：**'
- en: When a model is saved under ZeRO-2, you end up having the normal `pytorch_model.bin`
    file with the model weights, but they are only the fp16 version of the weights.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型保存在ZeRO-2下时，您最终会得到带有模型权重的正常`pytorch_model.bin`文件，但它们只是权重的fp16版本。
- en: 'Under ZeRO-3, things are much more complicated, since the model weights are
    partitioned out over multiple GPUs, therefore `"stage3_gather_16bit_weights_on_model_save":
    true` is required to get the `Trainer` to save the fp16 version of the weights.
    If this setting is `False` `pytorch_model.bin` won’t be created. This is because
    by default DeepSpeed’s `state_dict` contains a placeholder and not the real weights.
    If we were to save this `state_dict` it won’t be possible to load it back.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '在ZeRO-3下，情况要复杂得多，因为模型权重被分区到多个GPU上，因此需要`"stage3_gather_16bit_weights_on_model_save":
    true`来让`Trainer`保存权重的fp16版本。如果此设置为`False`，将不会创建`pytorch_model.bin`。这是因为默认情况下DeepSpeed的`state_dict`包含一个占位符而不是真正的权重。如果我们保存这个`state_dict`，将无法加载回来。'
- en: '[PRE60]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '**FP32 Weights:**'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '**FP32权重：**'
- en: While the fp16 weights are fine for resuming training, if you finished finetuning
    your model and want to upload it to the [models hub](https://huggingface.co/models)
    or pass it to someone else you most likely will want to get the fp32 weights.
    This ideally shouldn’t be done during training since this is a process that requires
    a lot of memory, and therefore best to be performed offline after the training
    is complete. But if desired and you have plenty of free CPU memory it can be done
    in the same training script. The following sections will discuss both approaches.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然fp16权重适用于恢复训练，但如果您完成了微调模型并希望将其上传到[models hub](https://huggingface.co/models)或传递给其他人，您很可能希望获取fp32权重。最好不要在训练过程中执行此操作，因为这是一个需要大量内存的过程，因此最好在训练完成后离线执行。但如果需要并且您有足够的空闲CPU内存，可以在相同的训练脚本中执行。以下部分将讨论这两种方法。
- en: '**Live FP32 Weights Recovery:**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线FP32权重恢复：**'
- en: This approach may not work if you model is large and you have little free CPU
    memory left, at the end of the training.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型很大且剩余的CPU内存很少，这种方法可能不起作用。
- en: 'If you have saved at least one checkpoint, and you want to use the latest one,
    you can do the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您至少保存了一个检查点，并且想要使用最新的检查点，可以执行以下操作：
- en: '[PRE61]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'If you’re using the `--load_best_model_at_end` class:*~transformers.TrainingArguments*
    argument (to track the best checkpoint), then you can finish the training by first
    saving the final model explicitly and then do the same as above:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用`--load_best_model_at_end`类：*~transformers.TrainingArguments*参数（用于跟踪最佳检查点），那么您可以通过首先显式保存最终模型，然后执行与上述相同的操作来完成训练：
- en: '[PRE62]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Note, that once `load_state_dict_from_zero_checkpoint` was run, the `model`
    will no longer be usable in the DeepSpeed context of the same application. i.e.
    you will need to re-initialize the deepspeed engine, since `model.load_state_dict(state_dict)`
    will remove all the DeepSpeed magic from it. So do this only at the very end of
    the training.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦运行了`load_state_dict_from_zero_checkpoint`，`model`将不再在相同应用程序的DeepSpeed上下文中可用。即您需要重新初始化deepspeed引擎，因为`model.load_state_dict(state_dict)`将从中删除所有DeepSpeed的魔法。因此，只在训练的最后阶段执行此操作。
- en: Of course, you don’t have to use class:*~transformers.Trainer* and you can adjust
    the examples above to your own trainer.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您不必使用类：*~transformers.Trainer*，您可以根据自己的训练器调整上面的示例。
- en: 'If for some reason you want more refinement, you can also extract the fp32
    `state_dict` of the weights and apply these yourself as is shown in the following
    example:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出于某种原因您想要更多的细化，您还可以提取权重的fp32`state_dict`并按照以下示例自行应用：
- en: '[PRE63]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '**Offline FP32 Weights Recovery:**'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**离线FP32权重恢复：**'
- en: DeepSpeed creates a special conversion script `zero_to_fp32.py` which it places
    in the top-level of the checkpoint folder. Using this script you can extract the
    weights at any point. The script is standalone and you no longer need to have
    the configuration file or a `Trainer` to do the extraction.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed创建了一个特殊的转换脚本`zero_to_fp32.py`，并将其放在检查点文件夹的顶层。使用此脚本，您可以在任何时候提取权重。该脚本是独立的，您不再需要配置文件或`Trainer`来执行提取。
- en: 'Let’s say your checkpoint folder looks like this:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的检查点文件夹如下所示：
- en: '[PRE64]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In this example there is just one DeepSpeed checkpoint sub-folder *global_step1*.
    Therefore to reconstruct the fp32 weights just run:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中只有一个DeepSpeed检查点子文件夹*global_step1*。因此，要重建fp32权重，只需运行：
- en: '[PRE65]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This is it. `pytorch_model.bin` will now contain the full fp32 model weights
    consolidated from multiple GPUs.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。`pytorch_model.bin`现在将包含从多个GPU中整合的完整fp32模型权重。
- en: The script will automatically be able to handle either a ZeRO-2 or ZeRO-3 checkpoint.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将自动处理ZeRO-2或ZeRO-3检查点。
- en: '`python zero_to_fp32.py -h` will give you usage details.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`python zero_to_fp32.py -h`将为您提供使用详细信息。'
- en: The script will auto-discover the deepspeed sub-folder using the contents of
    the file `latest`, which in the current example will contain `global_step1`.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将使用文件`latest`的内容自动发现deepspeed子文件夹，当前示例中将包含`global_step1`。
- en: 'Note: currently the script requires 2x general RAM of the final fp32 model
    weights.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当前脚本需要最终fp32模型权重的2倍通用RAM。
- en: ZeRO-3 and Infinity Nuances
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeRO-3和Infinity细微差别
- en: ZeRO-3 is quite different from ZeRO-2 because of its param sharding feature.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO-3与ZeRO-2非常不同，因为它具有参数分片功能。
- en: ZeRO-Infinity further extends ZeRO-3 to support NVMe memory and multiple other
    speed and scalability improvements.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO-Infinity进一步扩展了ZeRO-3，以支持NVMe内存和多项其他速度和可伸缩性改进。
- en: While all the efforts were made for things to just work without needing any
    special changes to your models, in certain circumstances you may find the following
    information to be needed.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经尽力使事情能够正常工作，而无需对您的模型进行任何特殊更改，但在某些情况下，您可能会发现需要以下信息。
- en: Constructing Massive Models
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建大型模型
- en: 'DeepSpeed/ZeRO-3 can handle models with Trillions of parameters which may not
    fit onto the existing RAM. In such cases, but also if you want the initialization
    to happen much faster, initialize the model using *deepspeed.zero.Init()* context
    manager (which is also a function decorator), like so:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed/ZeRO-3可以处理具有数万亿参数的模型，这些参数可能无法适应现有的RAM。在这种情况下，但也如果您希望初始化速度更快，请使用*deepspeed.zero.Init()*上下文管理器（也是函数装饰器）初始化模型，如下所示：
- en: '[PRE66]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: As you can see this gives you a randomly initialized model.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这为您提供了一个随机初始化的模型。
- en: 'If you want to use a pretrained model, `model_class.from_pretrained` will activate
    this feature as long as `is_deepspeed_zero3_enabled()` returns `True`, which currently
    is setup by the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object if the passed DeepSpeed configuration file contains ZeRO-3 config section.
    Thus you must create the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    object **before** calling `from_pretrained`. Here is an example of a possible
    sequence:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用预训练模型，只要`is_deepspeed_zero3_enabled()`返回`True`，`model_class.from_pretrained`将激活此功能，当前情况下，这是由[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)对象设置的，如果传递的DeepSpeed配置文件包含ZeRO-3配置部分。因此，您必须在调用`from_pretrained`之前创建[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)对象**之前**。以下是可能的顺序示例：
- en: '[PRE67]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: If you’re using the official example scripts and your command line arguments
    include `--deepspeed ds_config.json` with ZeRO-3 config enabled, then everything
    is already done for you, since this is how example scripts are written.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用官方示例脚本，并且您的命令行参数包括`--deepspeed ds_config.json`并启用了ZeRO-3配置，则一切都已经为您完成，因为示例脚本是这样编写的。
- en: 'Note: If the fp16 weights of the model can’t fit onto the memory of a single
    GPU this feature must be used.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果模型的fp16权重无法适应单个GPU的内存，则必须使用此功能。
- en: For full details on this method and other related features please refer to [Constructing
    Massive Models](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此方法和其他相关功能的详细信息，请参阅[构建大型模型](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models)。
- en: Also when loading fp16-pretrained models, you will want to tell `from_pretrained`
    to use `torch_dtype=torch.float16`. For details, please, see [from_pretrained-torch-dtype](#from_pretrained-torch-dtype).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当加载fp16预训练模型时，您将希望告诉`from_pretrained`使用`torch_dtype=torch.float16`。有关详细信息，请参见[from_pretrained-torch-dtype](#from_pretrained-torch-dtype)。
- en: Gathering Parameters
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收集参数
- en: Under ZeRO-3 on multiple GPUs no single GPU has all the parameters unless it’s
    the parameters for the currently executing layer. So if you need to access all
    parameters from all layers at once there is a specific method to do it. Most likely
    you won’t need it, but if you do please refer to [Gathering Parameters](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个GPU上的ZeRO-3中，除了当前执行层的参数外，没有单个GPU拥有所有参数。因此，如果您需要一次访问所有层的所有参数，有一种特定的方法可以做到。您很可能不需要它，但如果需要，请参阅[收集参数](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)。
- en: We do however use it internally in several places, one such example is when
    loading pretrained model weights in `from_pretrained`. We load one layer at a
    time and immediately partition it to all participating GPUs, as for very large
    models it won’t be possible to load it on one GPU and then spread it out to multiple
    GPUs, due to memory limitations.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在几个地方内部使用它，一个例子是在`from_pretrained`中加载预训练模型权重时。我们一次加载一层，然后立即将其分区到所有参与的GPU上，因为对于非常大的模型，将其加载到一个GPU上然后分散到多个GPU上是不可能的，由于内存限制。
- en: 'Also under ZeRO-3, if you write your own code and run into a model parameter
    weight that looks like:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在ZeRO-3下，如果您编写自己的代码并遇到看起来像模型参数权重的问题：
- en: '[PRE68]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: stress on `tensor([1.])`, or if you get an error where it says the parameter
    is of size `1`, instead of some much larger multi-dimensional shape, this means
    that the parameter is partitioned and what you see is a ZeRO-3 placeholder.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 强调`tensor([1.])`，或者如果出现错误，指出参数大小为`1`，而不是某个更大的多维形状，这意味着参数被分区，您看到的是ZeRO-3占位符。
- en: ZeRO Inference
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZeRO推理
- en: ZeRO Inference uses the same config as ZeRO-3 Training. You just don’t need
    the optimizer and scheduler sections. In fact you can leave these in the config
    file if you want to share the same one with the training. They will just be ignored.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO推理使用与ZeRO-3训练相同的配置。您只需要不需要优化器和调度程序部分。实际上，如果要与训练共享相同的配置文件，可以将这些部分保留在配置文件中。它们将被忽略。
- en: 'Otherwise you just need to pass the usual [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    arguments. For example:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，您只需要传递通常的[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)参数。例如：
- en: '[PRE69]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The only important thing is that you need to use a ZeRO-3 configuration, since
    ZeRO-2 provides no benefit whatsoever for the inference as only ZeRO-3 performs
    sharding of parameters, whereas ZeRO-1 shards gradients and optimizer states.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一重要的是您需要使用ZeRO-3配置，因为ZeRO-2对推理没有任何好处，因为只有ZeRO-3执行参数分片，而ZeRO-1执行梯度和优化器状态的分片。
- en: 'Here is an example of running `run_translation.py` under DeepSpeed deploying
    all available GPUs:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在使用所有可用GPU部署DeepSpeed时运行`run_translation.py`的示例：
- en: '[PRE70]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Since for inference there is no need for additional large memory used by the
    optimizer states and the gradients you should be able to fit much larger batches
    and/or sequence length onto the same hardware.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在推理中不需要额外大内存用于优化器状态和梯度，您应该能够在相同的硬件上适应更大的批次和/或序列长度。
- en: Additionally DeepSpeed is currently developing a related product called Deepspeed-Inference
    which has no relationship to the ZeRO technology, but instead uses tensor parallelism
    to scale models that can’t fit onto a single GPU. This is a work in progress and
    we will provide the integration once that product is complete.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DeepSpeed目前正在开发一个名为Deepspeed-Inference的相关产品，它与ZeRO技术没有关系，而是使用张量并行性来扩展无法适应单个GPU的模型。这是一个正在进行的工作，一旦该产品完成，我们将提供集成。
- en: Memory Requirements
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存要求
- en: Since Deepspeed ZeRO can offload memory to CPU (and NVMe) the framework provides
    utils that allow one to tell how much CPU and GPU memory will be needed depending
    on the number of GPUs being used.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Deepspeed ZeRO可以将内存卸载到CPU（和NVMe），该框架提供了一些实用程序，允许您根据使用的GPU数量告诉需要多少CPU和GPU内存。
- en: 'Let’s estimate how much memory is needed to finetune “bigscience/T0_3B” on
    a single GPU:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们估计在单个GPU上对“bigscience/T0_3B”进行微调所需的内存：
- en: '[PRE71]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: So you can fit it on a single 80GB GPU and no CPU offload, or a tiny 8GB GPU
    but then need ~60GB of CPU memory. (Remember this is just the memory for params,
    optimizer states and gradients - you will need a bit more memory for cuda kernels,
    activations and temps.)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以将其放在单个80GB GPU上，不使用CPU卸载，或者使用一个小型的8GB GPU，但是需要大约60GB的CPU内存。请记住，这只是参数、优化器状态和梯度的内存
    - 您将需要更多内存用于cuda内核、激活和临时存储。
- en: Then it’s a tradeoff of cost vs speed. It’ll be cheaper to buy/rent a smaller
    GPU (or less GPUs since you can use multiple GPUs with Deepspeed ZeRO. But then
    it’ll be slower, so even if you don’t care about how fast something will be done,
    the slowdown has a direct impact on the duration of using the GPU and thus bigger
    cost. So experiment and compare which works the best.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 然后就是成本与速度的权衡。购买/租用较小的GPU（或较少的GPU，因为您可以使用Deepspeed ZeRO来使用多个GPU）。但这样会更慢，所以即使您不关心某件事情会多快完成，减速也会直接影响使用GPU的持续时间，从而增加成本。因此，请进行实验并比较哪种方法最好。
- en: If you have enough GPU memory make sure to disable the CPU/NVMe offload as it’ll
    make everything faster.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有足够的GPU内存，请确保禁用CPU/NVMe卸载，因为这将使一切更快。
- en: 'For example, let’s repeat the same for 2 GPUs:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们重复使用2个GPU：
- en: '[PRE72]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: So here you’d want 2x 32GB GPUs or higher without offloading to CPU.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可能需要2个32GB或更高内存的GPU，而不需要将内存卸载到CPU。
- en: For full information please see [memory estimators](https://deepspeed.readthedocs.io/en/latest/memory.html).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 有关完整信息，请参阅[memory estimators](https://deepspeed.readthedocs.io/en/latest/memory.html)。
- en: Filing Issues
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提交问题
- en: Here is how to file an issue so that we could quickly get to the bottom of the
    issue and help you to unblock your work.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何提交问题，以便我们可以快速找到问题的根源并帮助您解除工作阻塞。
- en: 'In your report please always include:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的报告中，请始终包括：
- en: the full Deepspeed config file in the report
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在报告中提供完整的Deepspeed配置文件
- en: either the command line arguments if you were using the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    or [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    arguments if you were scripting the Trainer setup yourself. Please do not dump
    the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    as it has dozens of entries that are irrelevant.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用的是[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的命令行参数，或者如果您自己编写了Trainer设置，则使用[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)参数。请不要转储[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)，因为它有数十个与问题无关的条目。
- en: 'Output of:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE73]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: If possible include a link to a Google Colab notebook that we can reproduce
    the problem with. You can use this [notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)
    as a starting point.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能的话，请包含一个链接到一个Google Colab笔记本，我们可以用它来重现问题。您可以使用这个[notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)作为起点。
- en: Unless it’s impossible please always use a standard dataset that we can use
    and not something custom.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除非不可能，请始终使用我们可以使用的标准数据集，而不是自定义数据集。
- en: If possible try to use one of the existing [examples](https://github.com/huggingface/transformers/tree/main/examples/pytorch)
    to reproduce the problem with.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能，请尝试使用现有的[examples](https://github.com/huggingface/transformers/tree/main/examples/pytorch)之一来重现问题。
- en: 'Things to consider:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的事项：
- en: Deepspeed is often not the cause of the problem.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deepspeed通常不是问题的原因。
- en: Some of the filed issues proved to be Deepspeed-unrelated. That is once Deepspeed
    was removed from the setup, the problem was still there.
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些提交的问题被证明与Deepspeed无关。也就是说，一旦从设置中移除了Deepspeed，问题仍然存在。
- en: Therefore, if it’s not absolutely obvious it’s a DeepSpeed-related problem,
    as in you can see that there is an exception and you can see that DeepSpeed modules
    are involved, first re-test your setup without DeepSpeed in it. And only if the
    problem persists then do mentioned Deepspeed and supply all the required details.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，如果不是绝对明显是Deepspeed相关的问题，例如您可以看到有异常并且可以看到涉及Deepspeed模块，首先在没有Deepspeed的设置中重新测试您的设置。只有在问题仍然存在时才提到Deepspeed并提供所有必要的细节。
- en: If it’s clear to you that the issue is in the DeepSpeed core and not the integration
    part, please file the Issue directly with [Deepspeed](https://github.com/microsoft/DeepSpeed/).
    If you aren’t sure, please do not worry, either Issue tracker will do, we will
    figure it out once you posted it and redirect you to another Issue tracker if
    need be.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您明确知道问题出在DeepSpeed核心而不是集成部分，请直接向[Deepspeed](https://github.com/microsoft/DeepSpeed/)
    提交问题。如果您不确定，请不要担心，任何一个问题跟踪器都可以，我们会在您发布后找出问题，并在需要时将您重定向到另一个问题跟踪器。
- en: Troubleshooting
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障排除
- en: the deepspeed process gets killed at startup without a traceback
  id: totrans-439
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度速度进程在启动时被终止，没有回溯
- en: If the `deepspeed` process gets killed at launch time without a traceback, that
    usually means that the program tried to allocate more CPU memory than your system
    has or your process is allowed to allocate and the OS kernel killed that process.
    This is because your configuration file most likely has either `offload_optimizer`
    or `offload_param` or both configured to offload to `cpu`. If you have NVMe, experiment
    with offloading to NVMe if you’re running under ZeRO-3\. Here is how you can [estimate
    how much memory is needed for a specific model](https://deepspeed.readthedocs.io/en/latest/memory.html).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`deepspeed`进程在启动时被终止，没有回溯，通常意味着程序尝试分配比您的系统具有的CPU内存更多的内存，或者您的进程被允许分配的内存，而操作系统内核终止了该进程。这是因为您的配置文件很可能已经配置了`offload_optimizer`或`offload_param`或两者都配置为转移到`cpu`。如果您有NVMe，尝试将其转移到NVMe，如果您正在运行ZeRO-3。这是如何[估算特定模型所需内存量](https://deepspeed.readthedocs.io/en/latest/memory.html)的方法。
- en: training and/or eval/predict loss is NaN
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练和/或评估/预测损失为NaN
- en: This often happens when one takes a model pre-trained in bf16 mixed precision
    mode and tries to use it under fp16 (with or without mixed precision). Most models
    trained on TPU and often the ones released by Google are in this category (e.g.
    almost all t5-based models). Here the solution is to either use fp32 or bf16 if
    your hardware supports it (TPU, Ampere GPUs or newer).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个以bf16混合精度模式预训练的模型尝试在fp16下使用时，通常会发生这种情况（无论是否使用混合精度）。大多数在TPU上训练的模型，通常是由Google发布的模型都属于这一类（例如，几乎所有基于t5的模型）。在这种情况下，解决方案是要么使用fp32，要么使用bf16，如果您的硬件支持的话（TPU、Ampere
    GPU或更新）。
- en: 'The other problem may have to do with using fp16\. When you configure this
    section:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题可能与使用fp16有关。当您配置此部分时：
- en: '[PRE74]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'and you see in your log that Deepspeed reports `OVERFLOW!` as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 并且您在日志中看到Deepspeed报告`OVERFLOW!`如下：
- en: '[PRE75]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: that means that the Deepspeed loss scaler can’t figure out a scaling co-efficient
    that overcomes loss overflow.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着Deepspeed损失缩放器无法找到一个缩放系数来克服损失溢出。
- en: (the log was massaged to be more readable here.)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: （此处的日志已经过处理，以便更易阅读。）
- en: 'In this case you usually need to raise the value of `initial_scale_power`.
    Setting it to `"initial_scale_power": 32` will typically resolve the problem.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，通常需要提高`initial_scale_power`的值。将其设置为`"initial_scale_power": 32`通常会解决问题。'
- en: Notes
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: While DeepSpeed has a pip installable PyPI package, it is highly recommended
    that it gets installed from [source](https://github.com/microsoft/deepspeed#installation)
    to best match your hardware and also if you need to enable certain features, like
    1-bit Adam, which aren’t available in the pypi distribution.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然DeepSpeed有一个可通过pip安装的PyPI软件包，但强烈建议从[源](https://github.com/microsoft/deepspeed#installation)安装，以最好地匹配您的硬件，并且如果您需要启用某些功能，比如1比特Adam，在pypi分发中是不可用的。
- en: You don’t have to use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    to use DeepSpeed with 🤗 Transformers - you can use any model with your own trainer,
    and you will have to adapt the latter according to [the DeepSpeed integration
    instructions](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models).
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不必使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)来使用🤗
    Transformers的DeepSpeed - 您可以使用任何模型与您自己的训练器，并且您将根据[DeepSpeed集成说明](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models)来调整后者。
- en: Non-Trainer Deepspeed Integration
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非Trainer Deepspeed集成
- en: The [HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    is used to integrate Deepspeed into the 🤗 Transformers core functionality, when
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    is not used. The only thing that it does is handling Deepspeed ZeRO-3 param gathering
    and automatically splitting the model onto multiple gpus during `from_pretrained`
    call. Everything else you have to do by yourself.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    用于将Deepspeed集成到🤗 Transformers核心功能中，当未使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)时。它唯一要做的就是处理Deepspeed
    ZeRO-3参数收集，并在`from_pretrained`调用期间自动将模型分割到多个GPU上。其他所有事情都需要您自己来做。'
- en: When using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    everything is automatically taken care of.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    时，一切都会自动处理。
- en: When not using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    to efficiently deploy DeepSpeed ZeRO-3, you must instantiate the [HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)
    object before instantiating the model and keep that object alive.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 当不使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)时，为了有效部署DeepSpeed
    ZeRO-3，您必须在实例化模型之前实例化[HfDeepSpeedConfig](/docs/transformers/v4.37.2/en/main_classes/deepspeed#transformers.integrations.HfDeepSpeedConfig)对象，并保持该对象处于活动状态。
- en: If you’re using Deepspeed ZeRO-1 or ZeRO-2 you don’t need to use `HfDeepSpeedConfig`
    at all.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Deepspeed ZeRO-1或ZeRO-2，则根本不需要使用`HfDeepSpeedConfig`。
- en: 'For example for a pretrained model:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于预训练模型：
- en: '[PRE76]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'or for non-pretrained model:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 或对于非预训练模型：
- en: '[PRE77]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Please note that if you’re not using the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    integration, you’re completely on your own. Basically follow the documentation
    on the [Deepspeed](https://www.deepspeed.ai/) website. Also you have to configure
    explicitly the config file - you can’t use `"auto"` values and you will have to
    put real values instead.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您没有使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)集成，您将完全独立。基本上遵循[Deepspeed](https://www.deepspeed.ai/)网站上的文档。此外，您必须明确配置配置文件
    - 不能使用`"auto"`值，而必须使用实际值。
- en: HfDeepSpeedConfig
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HfDeepSpeedConfig
- en: '### `class transformers.integrations.HfDeepSpeedConfig`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.HfDeepSpeedConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/deepspeed.py#L56)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/deepspeed.py#L56)'
- en: '[PRE78]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Parameters
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config_file_or_dict` (`Union[str, Dict]`) — path to DeepSpeed config file
    or dict.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_file_or_dict`（`Union[str, Dict]`）— DeepSpeed配置文件或字典的路径。'
- en: This object contains a DeepSpeed configuration dictionary and can be quickly
    queried for things like zero stage.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 此对象包含一个DeepSpeed配置字典，可以快速查询诸如零阶段之类的内容。
- en: A `weakref` of this object is stored in the module’s globals to be able to access
    the config from areas where things like the Trainer object is not available (e.g.
    `from_pretrained` and `_get_resized_embeddings`). Therefore it’s important that
    this object remains alive while the program is still running.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 此对象的`weakref`存储在模块的全局变量中，以便能够从Trainer对象不可用的区域访问配置（例如`from_pretrained`和`_get_resized_embeddings`）。因此，在程序仍在运行时，这个对象保持活动是很重要的。
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    uses the `HfTrainerDeepSpeedConfig` subclass instead. That subclass has logic
    to sync the configuration with values of [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    by replacing special placeholder values: `"auto"`. Without this special logic
    the DeepSpeed configuration is not modified in any way.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)使用`HfTrainerDeepSpeedConfig`子类。该子类具有将配置与[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)的值同步的逻辑，通过替换特殊占位符值：`"auto"`。如果没有这种特殊逻辑，DeepSpeed配置将不会以任何方式修改。'
- en: Custom DeepSpeed ZeRO Inference
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义DeepSpeed ZeRO推理
- en: Here is an example of how one could do DeepSpeed ZeRO Inference without using
    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    when one can’t fit a model onto a single GPU. The solution includes using additional
    GPUs or/and offloading GPU memory to CPU memory.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，演示如何在无法将模型放入单个GPU的情况下进行DeepSpeed ZeRO推理，而不使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)。解决方案包括使用额外的GPU和/或将GPU内存转移到CPU内存。
- en: The important nuance to understand here is that the way ZeRO is designed you
    can process different inputs on different GPUs in parallel.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要理解的重要细微差别是，ZeRO的设计方式使您可以并行处理不同GPU上的不同输入。
- en: The example has copious notes and is self-documenting.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 示例有大量注释并且是自我记录的。
- en: 'Make sure to:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 确保：
- en: disable CPU offload if you have enough GPU memory (since it slows things down)
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您有足够的GPU内存，请禁用CPU卸载（因为它会减慢速度）
- en: enable bf16 if you own an Ampere or a newer GPU to make things faster. If you
    don’t have that hardware you may enable fp16 as long as you don’t use any model
    that was pre-trained in bf16 mixed precision (such as most t5 models). These usually
    overflow in fp16 and you will see garbage as output.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您拥有Ampere或更新的GPU，请启用bf16以加快速度。如果您没有该硬件，可以启用fp16，只要不使用在bf16混合精度（例如大多数t5模型）中预训练的模型。这些通常在fp16中溢出，您将看到垃圾输出。
- en: '[PRE79]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Let’s save it as `t0.py` and run it:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其保存为`t0.py`并运行：
- en: '[PRE80]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This was a very basic example and you will want to adapt it to your needs.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常基本的示例，您将希望根据自己的需求进行调整。
- en: generate nuances
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成细微差别
- en: When using multiple GPUs with ZeRO Stage-3, one has to synchronize the GPUs
    by calling `generate(..., synced_gpus=True)`. If this is not done if one GPU finished
    generating before other GPUs the whole system will hang as the rest of the GPUs
    will not be able to received the shard of weights from the GPU that stopped generating.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个GPU与ZeRO Stage-3一起使用时，必须通过调用`generate(..., synced_gpus=True)`来同步GPU。如果不这样做，如果一个GPU在其他GPU之前完成生成，整个系统将挂起，因为其他GPU将无法接收停止生成的GPU的权重片段。
- en: Starting from `transformers>=4.28`, if `synced_gpus` isn’t explicitly specified,
    it’ll be set to `True` automatically if these conditions are detected. But you
    can still override the value of `synced_gpus` if need to.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 从`transformers>=4.28`开始，如果未明确指定`synced_gpus`，则在检测到这些条件时，它将自动设置为`True`。但是，如果需要，仍然可以覆盖`synced_gpus`的值。
- en: Testing Deepspeed Integration
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试Deepspeed集成
- en: If you submit a PR that involves DeepSpeed integration please note our CircleCI
    PR CI setup has no GPUs, so we only run tests requiring gpus on a different CI
    nightly. Therefore if you get a green CI report in your PR it doesn’t mean DeepSpeed
    tests pass.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您提交涉及DeepSpeed集成的PR，请注意我们的CircleCI PR CI设置没有GPU，因此我们只在另一个CI每晚运行需要GPU的测试。因此，如果您在PR中收到绿色CI报告，这并不意味着DeepSpeed测试通过。
- en: 'To run DeepSpeed tests, please run at least:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行DeepSpeed测试，请至少运行：
- en: '[PRE81]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'If you changed any of the modeling or pytorch examples code, then run the model
    zoo tests as well. The following will run all DeepSpeed tests:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 如果更改了建模或pytorch示例代码中的任何内容，则还要运行模型动物园测试。以下将运行所有DeepSpeed测试：
- en: '[PRE82]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Main DeepSpeed Resources
  id: totrans-492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主DeepSpeed资源
- en: '[Project’s github](https://github.com/microsoft/deepspeed)'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[项目的github](https://github.com/microsoft/deepspeed)'
- en: '[Usage docs](https://www.deepspeed.ai/getting-started/)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用文档](https://www.deepspeed.ai/getting-started/)'
- en: '[API docs](https://deepspeed.readthedocs.io/en/latest/index.html)'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[API文档](https://deepspeed.readthedocs.io/en/latest/index.html)'
- en: '[Blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[博客文章](https://www.microsoft.com/en-us/research/search/?q=deepspeed)'
- en: 'Papers:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 论文：
- en: '[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO：面向训练万亿参数模型的内存优化](https://arxiv.org/abs/1910.02054)'
- en: '[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Offload: 民主化十亿规模的模型训练](https://arxiv.org/abs/2101.06840)'
- en: '[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZeRO-Infinity: 打破 GPU 内存壁，实现极端规模的深度学习](https://arxiv.org/abs/2104.07857)'
- en: Finally, please, remember that, HuggingFace [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    only integrates DeepSpeed, therefore if you have any problems or questions with
    regards to DeepSpeed usage, please, file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请记住，HuggingFace [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    只集成了 DeepSpeed，因此如果您在使用 DeepSpeed 方面遇到任何问题或疑问，请在 [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues)
    上提交问题。
