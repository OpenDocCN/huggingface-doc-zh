- en: BertJapanese
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BertJapanese
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The BERT models trained on Japanese text.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在日文文本上训练的BERT模型。
- en: 'There are models with two different tokenization methods:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种不同的分词方法的模型：
- en: Tokenize with MeCab and WordPiece. This requires some extra dependencies, [fugashi](https://github.com/polm/fugashi)
    which is a wrapper around [MeCab](https://taku910.github.io/mecab/).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MeCab和WordPiece进行标记化。这需要一些额外的依赖项，[fugashi](https://github.com/polm/fugashi)是[MeCab](https://taku910.github.io/mecab/)的包装器。
- en: Tokenize into characters.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标记化为字符。
- en: To use *MecabTokenizer*, you should `pip install transformers["ja"]` (or `pip
    install -e .["ja"]` if you install from source) to install dependencies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用*MecabTokenizer*，您应该`pip install transformers["ja"]`（或者如果您从源代码安装，则应该`pip
    install -e .["ja"]`）来安装依赖项。
- en: See [details on cl-tohoku repository](https://github.com/cl-tohoku/bert-japanese).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[cl-tohoku存储库上的详细信息](https://github.com/cl-tohoku/bert-japanese)。
- en: 'Example of using a model with MeCab and WordPiece tokenization:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MeCab和WordPiece分词的模型的示例：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Example of using a model with Character tokenization:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字符分词的模型示例：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This model was contributed by [cl-tohoku](https://huggingface.co/cl-tohoku).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[cl-tohoku](https://huggingface.co/cl-tohoku)贡献的。
- en: This implementation is the same as BERT, except for tokenization method. Refer
    to [BERT documentation](bert) for API reference information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现与BERT相同，只是分词方法不同。有关API参考信息，请参考[BERT文档](bert)。
- en: BertJapaneseTokenizer
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BertJapaneseTokenizer
- en: '### `class transformers.BertJapaneseTokenizer`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BertJapaneseTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L107)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L107)'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to a one-wordpiece-per-line vocabulary file.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 指向一个每行一个wordpiece的词汇文件的路径。'
- en: '`spm_file` (`str`, *optional*) — Path to [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .spm or .model extension) that contains the vocabulary.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spm_file` (`str`, *可选*) — 指向包含词汇表的[SentencePiece](https://github.com/google/sentencepiece)文件的路径（通常具有.spm或.model扩展名）。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether to lower
    case the input. Only has an effect when do_basic_tokenize=True.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *可选*, 默认为`True`) — 是否将输入转换为小写。仅在do_basic_tokenize=True时有效。'
- en: '`do_word_tokenize` (`bool`, *optional*, defaults to `True`) — Whether to do
    word tokenization.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_word_tokenize` (`bool`, *可选*, 默认为`True`) — 是否进行词分词。'
- en: '`do_subword_tokenize` (`bool`, *optional*, defaults to `True`) — Whether to
    do subword tokenization.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_subword_tokenize` (`bool`, *可选*, 默认为`True`) — 是否进行子词分词。'
- en: '`word_tokenizer_type` (`str`, *optional*, defaults to `"basic"`) — Type of
    word tokenizer. Choose from [“basic”, “mecab”, “sudachi”, “jumanpp”].'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_tokenizer_type` (`str`, *可选*, 默认为`"basic"`) — 词分词器的类型。可选择从[“basic”, “mecab”,
    “sudachi”, “jumanpp”]中选择。'
- en: '`subword_tokenizer_type` (`str`, *optional*, defaults to `"wordpiece"`) — Type
    of subword tokenizer. Choose from [“wordpiece”, “character”, “sentencepiece”,].'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subword_tokenizer_type` (`str`, *可选*, 默认为`"wordpiece"`) — 子词分词器的类型。可选择从[“wordpiece”,
    “character”, “sentencepiece”]中选择。'
- en: '`mecab_kwargs` (`dict`, *optional*) — Dictionary passed to the `MecabTokenizer`
    constructor.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mecab_kwargs` (`dict`, *可选*) — 传递给`MecabTokenizer`构造函数的字典。'
- en: '`sudachi_kwargs` (`dict`, *optional*) — Dictionary passed to the `SudachiTokenizer`
    constructor.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sudachi_kwargs` (`dict`, *可选*) — 传递给`SudachiTokenizer`构造函数的字典。'
- en: '`jumanpp_kwargs` (`dict`, *optional*) — Dictionary passed to the `JumanppTokenizer`
    constructor.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jumanpp_kwargs` (`dict`, *可选*) — 传递给`JumanppTokenizer`构造函数的字典。'
- en: Construct a BERT tokenizer for Japanese text.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为日文文本构建一个BERT分词器。
- en: 'This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to: this superclass
    for more information regarding those methods.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考：这个超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L307)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L307)'
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表，将添加特殊标记。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 包含适当特殊标记的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A BERT sequence has the following
    format:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。BERT序列的格式如下：
- en: 'single sequence: `[CLS] X [SEP]`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`[CLS] X [SEP]`
- en: 'pair of sequences: `[CLS] A [SEP] B [SEP]`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`[CLS] A [SEP] B [SEP]`
- en: '#### `convert_tokens_to_string`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L299)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L299)'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Converts a sequence of tokens (string) in a single string.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记（字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L362)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L362)'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. A BERT sequence
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列中创建一个用于序列对分类任务的掩码。一个BERT序列
- en: 'pair mask has the following format:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 序列掩码的格式如下：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。
- en: '#### `get_special_tokens_mask`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L333)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L333)'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *可选*，默认为`False`) — 标记列表是否已经使用特殊标记格式化。'
- en: Returns
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围在[0, 1]之间：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。当使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。
