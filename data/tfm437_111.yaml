- en: Callbacks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回调函数
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/callback)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks are objects that can customize the behavior of the training loop in
    the PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    (this feature is not yet implemented in TensorFlow) that can inspect the training
    loop state (for progress reporting, logging on TensorBoard or other ML platforms…)
    and take decisions (like early stopping).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数是可以自定义PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)训练循环行为的对象（此功能尚未在TensorFlow中实现），可以检查训练循环状态（用于进度报告、在TensorBoard或其他ML平台上记录…）并做出决策（如提前停止）。
- en: Callbacks are “read only” pieces of code, apart from the [TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl)
    object they return, they cannot change anything in the training loop. For customizations
    that require changes in the training loop, you should subclass [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and override the methods you need (see [trainer](trainer) for examples).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数是“只读”代码片段，除了它们返回的[TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl)对象外，它们不能更改训练循环中的任何内容。对于需要更改训练循环的自定义内容，您应该子类化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)并覆盖您需要的方法（请参阅[trainer](trainer)以获取示例）。
- en: By default, `TrainingArguments.report_to` is set to `"all"`, so a [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will use the following callbacks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`TrainingArguments.report_to`设置为`"all"`，因此[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将使用以下回调函数。
- en: '[DefaultFlowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.DefaultFlowCallback)
    which handles the default behavior for logging, saving and evaluation.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DefaultFlowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.DefaultFlowCallback)处理日志记录、保存和评估的默认行为。'
- en: '[PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback)
    or [ProgressCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.ProgressCallback)
    to display progress and print the logs (the first one is used if you deactivate
    tqdm through the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments),
    otherwise it’s the second one).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback)或[ProgressCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.ProgressCallback)显示进度并打印日志（如果通过[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)停用tqdm，则使用第一个，否则使用第二个）。
- en: '[TensorBoardCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.TensorBoardCallback)
    if tensorboard is accessible (either through PyTorch >= 1.4 or tensorboardX).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果tensorboard可访问（通过PyTorch >= 1.4或tensorboardX），则使用[TensorBoardCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.TensorBoardCallback)。
- en: '[WandbCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.WandbCallback)
    if [wandb](https://www.wandb.com/) is installed.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[wandb](https://www.wandb.com/)，则使用[WandbCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.WandbCallback)。
- en: '[CometCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CometCallback)
    if [comet_ml](https://www.comet.ml/site/) is installed.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[comet_ml](https://www.comet.ml/site/)，则使用[CometCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CometCallback)。
- en: '[MLflowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.MLflowCallback)
    if [mlflow](https://www.mlflow.org/) is installed.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[mlflow](https://www.mlflow.org/)，则使用[MLflowCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.MLflowCallback)。
- en: '[NeptuneCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.NeptuneCallback)
    if [neptune](https://neptune.ai/) is installed.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[neptune](https://neptune.ai/)，则使用[NeptuneCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.NeptuneCallback)。
- en: '[AzureMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.AzureMLCallback)
    if [azureml-sdk](https://pypi.org/project/azureml-sdk/) is installed.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[azureml-sdk](https://pypi.org/project/azureml-sdk/)，则使用[AzureMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.AzureMLCallback)。
- en: '[CodeCarbonCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CodeCarbonCallback)
    if [codecarbon](https://pypi.org/project/codecarbon/) is installed.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[codecarbon](https://pypi.org/project/codecarbon/)，则使用[CodeCarbonCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.CodeCarbonCallback)。
- en: '[ClearMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.ClearMLCallback)
    if [clearml](https://github.com/allegroai/clearml) is installed.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[clearml](https://github.com/allegroai/clearml)，则使用[ClearMLCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.ClearMLCallback)。
- en: '[DagsHubCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DagsHubCallback)
    if [dagshub](https://dagshub.com/) is installed.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[dagshub](https://dagshub.com/)，则使用[DagsHubCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DagsHubCallback)。
- en: '[FlyteCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.FlyteCallback)
    if [flyte](https://flyte.org/) is installed.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[flyte](https://flyte.org/)，则使用[FlyteCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.FlyteCallback)。
- en: '[DVCLiveCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DVCLiveCallback)
    if [dvclive](https://dvc.org/doc/dvclive) is installed.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果安装了[dvclive](https://dvc.org/doc/dvclive)，则使用[DVCLiveCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.integrations.DVCLiveCallback)。
- en: If a package is installed but you don’t wish to use the accompanying integration,
    you can change `TrainingArguments.report_to` to a list of just those integrations
    you want to use (e.g. `["azure_ml", "wandb"]`).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果安装了某个软件包，但您不希望使用相应的集成，可以将`TrainingArguments.report_to`更改为您想要使用的集成列表（例如`["azure_ml",
    "wandb"]`）。
- en: The main class that implements callbacks is [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
    It gets the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    used to instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    can access that Trainer’s internal state via [TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState),
    and can take some actions on the training loop via [TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实现回调的主要类是[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。它获取用于实例化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)，可以通过[TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState)访问该Trainer的内部状态，并可以通过[TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl)对训练循环采取一些操作。
- en: Available Callbacks
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的回调
- en: 'Here is the list of the available [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    in the library:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是库中可用的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)列表：
- en: '### `class transformers.integrations.CometCallback`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.CometCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L833)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L833)'
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [Comet ML](https://www.comet.ml/site/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将日志发送到[Comet ML](https://www.comet.ml/site/)的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。
- en: '#### `setup`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L844)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L844)'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Setup the optional Comet.ml integration.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 设置可选的Comet.ml集成。
- en: 'Environment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：
- en: '`COMET_MODE` (`str`, *optional*, defaults to `ONLINE`): Whether to create an
    online, offline experiment or disable Comet logging. Can be `OFFLINE`, `ONLINE`,
    or `DISABLED`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_MODE` (`str`, *可选*，默认为`ONLINE`): 是否创建在线、离线实验或禁用Comet日志记录。可以是`OFFLINE`、`ONLINE`或`DISABLED`。'
- en: '`COMET_PROJECT_NAME` (`str`, *optional*): Comet project name for experiments.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_PROJECT_NAME` (`str`, *可选*): 用于实验的Comet项目名称。'
- en: '`COMET_OFFLINE_DIRECTORY` (`str`, *optional*): Folder to use for saving offline
    experiments when `COMET_MODE` is `OFFLINE`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_OFFLINE_DIRECTORY` (`str`, *可选*): 在`COMET_MODE`为`OFFLINE`时用于保存离线实验的文件夹。'
- en: '`COMET_LOG_ASSETS` (`str`, *optional*, defaults to `TRUE`): Whether or not
    to log training assets (tf event logs, checkpoints, etc), to Comet. Can be `TRUE`,
    or `FALSE`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMET_LOG_ASSETS` (`str`, *可选*，默认为`TRUE`): 是否将训练资产（tf事件日志、检查点等）记录到Comet。可以是`TRUE`或`FALSE`。'
- en: For a number of configurable items in the environment, see [here](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有关环境中可配置项目的详细信息，请参阅[此处](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables)。
- en: '### `class transformers.DefaultFlowCallback`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DefaultFlowCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L432)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L432)'
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that handles the default flow of the training loop for logs, evaluation and checkpoints.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 处理训练循环的默认流程，包括日志、评估和检查点的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。
- en: '### `class transformers.PrinterCallback`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PrinterCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L532)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L532)'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A bare [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that just prints the logs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)，只打印日志。
- en: '### `class transformers.ProgressCallback`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ProgressCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L482)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L482)'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that displays the progress of training or evaluation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显示训练或评估进度的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。
- en: '### `class transformers.EarlyStoppingCallback`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.EarlyStoppingCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L543)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L543)'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`early_stopping_patience` (`int`) — Use with `metric_for_best_model` to stop
    training when the specified metric worsens for `early_stopping_patience` evaluation
    calls.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_patience` (`int`) — 与`metric_for_best_model`一起使用，当指定的指标在`early_stopping_patience`次评估调用中恶化时停止训练。'
- en: '`early_stopping_threshold(float,` *optional*) — Use with TrainingArguments
    `metric_for_best_model` and `early_stopping_patience` to denote how much the specified
    metric must improve to satisfy early stopping conditions. `'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_threshold(float,` *可选*) — 与TrainingArguments的`metric_for_best_model`和`early_stopping_patience`一起使用，表示指定指标必须改善多少才能满足提前停止条件。'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that handles early stopping.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个处理提前停止的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。
- en: This callback depends on [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    argument *load_best_model_at_end* functionality to set best_metric in [TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState).
    Note that if the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    argument *save_steps* differs from *eval_steps*, the early stopping will not occur
    until the next save step.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此回调取决于[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)参数*load_best_model_at_end*功能，以在[TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState)中设置best_metric。请注意，如果[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)参数*save_steps*与*eval_steps*不同，则直到下一个保存步骤才会发生早停。
- en: '### `class transformers.integrations.TensorBoardCallback`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.TensorBoardCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L579)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L579)'
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tb_writer` (`SummaryWriter`, *optional*) — The writer to use. Will instantiate
    one if not set.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tb_writer`（`SummaryWriter`，*可选*）—要使用的写入器。如果未设置，将实例化一个。'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)，将日志发送到[TensorBoard](https://www.tensorflow.org/tensorboard)。
- en: '### `class transformers.integrations.WandbCallback`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.WandbCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L665)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L665)'
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that logs metrics, media, model checkpoints to [Weight and Biases](https://www.wandb.com/).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)，将指标、媒体、模型检查点记录到[Weights
    and Biases](https://www.wandb.com/)。
- en: '#### `setup`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L690)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L690)'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Setup the optional Weights & Biases (*wandb*) integration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 设置可选的Weights & Biases（*wandb*）集成。
- en: 'One can subclass and override this method to customize the setup if needed.
    Find more information [here](https://docs.wandb.ai/guides/integrations/huggingface).
    You can also override the following environment variables:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以子类化并重写此方法以自定义设置。在[这里](https://docs.wandb.ai/guides/integrations/huggingface)找到更多信息。您还可以重写以下环境变量：
- en: 'Environment:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：
- en: '`WANDB_LOG_MODEL` (`str`, *optional*, defaults to `"false"`): Whether to log
    model and checkpoints during training. Can be `"end"`, `"checkpoint"` or `"false"`.
    If set to `"end"`, the model will be uploaded at the end of training. If set to
    `"checkpoint"`, the checkpoint will be uploaded every `args.save_steps` . If set
    to `"false"`, the model will not be uploaded. Use along with `load_best_model_at_end()`
    to upload best model.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_LOG_MODEL`（`str`，*可选*，默认为`"false"`）：是否在训练期间记录模型和检查点。可以是`"end"`，`"checkpoint"`或`"false"`。如果设置为`"end"`，模型将在训练结束时上传。如果设置为`"checkpoint"`，将在每次`args.save_steps`保存时上传检查点。如果设置为`"false"`，模型将不会上传。与`load_best_model_at_end()`一起使用以上传最佳模型。'
- en: Deprecated in 5.0
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在5.0中已弃用
- en: Setting `WANDB_LOG_MODEL` as `bool` will be deprecated in version 5 of 🤗 Transformers.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在🤗 Transformers的第5版中，将废弃将`WANDB_LOG_MODEL`设置为`bool`。
- en: '`WANDB_WATCH` (`str`, *optional* defaults to `"false"`): Can be `"gradients"`,
    `"all"`, `"parameters"`, or `"false"`. Set to `"all"` to log gradients and parameters.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_WATCH`（`str`，*可选*，默认为`"false"`）：可以是`"gradients"`，`"all"`，`"parameters"`或`"false"`。设置为`"all"`以记录梯度和参数。'
- en: '`WANDB_PROJECT` (`str`, *optional*, defaults to `"huggingface"`): Set this
    to a custom string to store results in a different project.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_PROJECT`（`str`，*可选*，默认为`"huggingface"`）：将其设置为自定义字符串以将结果存储在不同的项目中。'
- en: '`WANDB_DISABLED` (`bool`, *optional*, defaults to `False`): Whether to disable
    wandb entirely. Set `WANDB_DISABLED=true` to disable.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB_DISABLED`（`bool`，*可选*，默认为`False`）：是否完全禁用wandb。设置`WANDB_DISABLED=true`以禁用。'
- en: '### `class transformers.integrations.MLflowCallback`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.MLflowCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L933)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L933)'
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [MLflow](https://www.mlflow.org/). Can be disabled by setting
    environment variable `DISABLE_MLFLOW_INTEGRATION = TRUE`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)，将日志发送到[MLflow](https://www.mlflow.org/)。可以通过设置环境变量`DISABLE_MLFLOW_INTEGRATION
    = TRUE`来禁用。
- en: '#### `setup`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L952)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L952)'
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Setup the optional MLflow integration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 设置可选的MLflow集成。
- en: 'Environment:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：
- en: '`HF_MLFLOW_LOG_ARTIFACTS` (`str`, *optional*): Whether to use MLflow `.log_artifact()`
    facility to log artifacts. This only makes sense if logging to a remote server,
    e.g. s3 or GCS. If set to `True` or *1*, will copy each saved checkpoint on each
    save in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)’s
    `output_dir` to the local or remote artifact storage. Using it without a remote
    storage will just copy the files to your artifact location.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_MLFLOW_LOG_ARTIFACTS`（`str`，*可选*）：是否使用MLflow的`.log_artifact()`功能来记录工件。只有在将日志记录到远程服务器（例如s3或GCS）时才有意义。如果设置为`True`或*1*，将在每次在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)的`output_dir`中保存时将每个保存的检查点复制到本地或远程工件存储。在没有远程存储的情况下使用它将只是将文件复制到您的工件位置。'
- en: '`MLFLOW_EXPERIMENT_NAME` (`str`, *optional*, defaults to `None`): Whether to
    use an MLflow experiment_name under which to launch the run. Default to `None`
    which will point to the `Default` experiment in MLflow. Otherwise, it is a case
    sensitive name of the experiment to be activated. If an experiment with this name
    does not exist, a new experiment with this name is created.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_EXPERIMENT_NAME` (`str`, *可选*, 默认为`None`)：是否使用MLflow实验名称来启动运行。默认为`None`，将指向MLflow中的`Default`实验。否则，它是要激活的实验的区分大小写名称。如果不存在具有此名称的实验，则将创建一个具有此名称的新实验。'
- en: '`MLFLOW_TAGS` (`str`, *optional*): A string dump of a dictionary of key/value
    pair to be added to the MLflow run as tags. Example: `os.environ[''MLFLOW_TAGS'']=''{"release.candidate":
    "RC1", "release.version": "2.2.0"}''`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_TAGS` (`str`, *可选*)：要添加到MLflow运行中的标签的键/值对的字符串转储。示例：`os.environ[''MLFLOW_TAGS'']=''{"release.candidate":
    "RC1", "release.version": "2.2.0"}''`。'
- en: '`MLFLOW_NESTED_RUN` (`str`, *optional*): Whether to use MLflow nested runs.
    If set to `True` or *1*, will create a nested run inside the current run.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_NESTED_RUN` (`str`, *可选*)：是否使用MLflow嵌套运行。如果设置为`True`或*1*，将在当前运行内创建一个嵌套运行。'
- en: '`MLFLOW_RUN_ID` (`str`, *optional*): Allow to reattach to an existing run which
    can be usefull when resuming training from a checkpoint. When `MLFLOW_RUN_ID`
    environment variable is set, `start_run` attempts to resume a run with the specified
    run ID and other parameters are ignored.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_RUN_ID` (`str`, *可选*)：允许重新附加到现有运行，这在从检查点恢复训练时可能很有用。当设置了`MLFLOW_RUN_ID`环境变量时，`start_run`尝试恢复具有指定运行ID的运行，其他参数将被忽略。'
- en: '`MLFLOW_FLATTEN_PARAMS` (`str`, *optional*, defaults to `False`): Whether to
    flatten the parameters dictionary before logging.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFLOW_FLATTEN_PARAMS` (`str`, *可选*, 默认为`False`)：是否在记录之前展平参数字典。'
- en: '### `class transformers.integrations.AzureMLCallback`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.AzureMLCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L910)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L910)'
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [AzureML](https://pypi.org/project/azureml-sdk/).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)，将日志发送到[AzureML](https://pypi.org/project/azureml-sdk/)。
- en: '### `class transformers.integrations.CodeCarbonCallback`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.CodeCarbonCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1399)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1399)'
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that tracks the CO2 emission of training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)，用于跟踪训练的CO2排放量。
- en: '### `class transformers.integrations.NeptuneCallback`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.NeptuneCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1128)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1128)'
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`api_token` (`str`, *optional*) — Neptune API token obtained upon registration.
    You can leave this argument out if you have saved your token to the `NEPTUNE_API_TOKEN`
    environment variable (strongly recommended). See full setup instructions in the
    [docs](https://docs.neptune.ai/setup/installation).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`api_token` (`str`, *可选*) — 在注册时获得的Neptune API令牌。如果已将令牌保存到`NEPTUNE_API_TOKEN`环境变量中，可以省略此参数（强烈建议）。在[文档](https://docs.neptune.ai/setup/installation)中查看完整的设置说明。'
- en: '`project` (`str`, *optional*) — Name of an existing Neptune project, in the
    form “workspace-name/project-name”. You can find and copy the name in Neptune
    from the project settings -> Properties. If None (default), the value of the `NEPTUNE_PROJECT`
    environment variable is used.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`project` (`str`, *可选*) — Neptune项目的名称，格式为“workspace-name/project-name”。您可以在Neptune中的项目设置
    -> 属性中找到并复制名称。如果为None（默认），则使用`NEPTUNE_PROJECT`环境变量的值。'
- en: '`name` (`str`, *optional*) — Custom name for the run.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str`, *可选*) — 运行的自定义名称。'
- en: '`base_namespace` (`str`, optional, defaults to “finetuning”) — In the Neptune
    run, the root namespace that will contain all of the metadata logged by the callback.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base_namespace` (`str`, 可选, 默认为“finetuning”) — 在Neptune运行中，将包含回调记录的所有元数据的根命名空间。'
- en: '`log_parameters` (`bool`, *optional*, defaults to `True`) — If True, logs all
    Trainer arguments and model parameters provided by the Trainer.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_parameters` (`bool`, *可选*, 默认为`True`) — 如果为True，则记录Trainer提供的所有参数和模型参数。'
- en: '`log_checkpoints` (`str`, *optional*) — If “same”, uploads checkpoints whenever
    they are saved by the Trainer. If “last”, uploads only the most recently saved
    checkpoint. If “best”, uploads the best checkpoint (among the ones saved by the
    Trainer). If `None`, does not upload checkpoints.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_checkpoints` (`str`, *可选*) — 如果为“same”，则在Trainer保存检查点时上传检查点。如果为“last”，则仅上传最近保存的检查点。如果为“best”，则上传最佳检查点（在Trainer保存的检查点中选择）。如果为`None`，则不上传检查点。'
- en: '`run` (`Run`, *optional*) — Pass a Neptune run object if you want to continue
    logging to an existing run. Read more about resuming runs in the [docs](https://docs.neptune.ai/logging/to_existing_object).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run` (`Run`, *可选*) — 如果要继续记录到现有运行中，请传递一个Neptune运行对象。在[文档](https://docs.neptune.ai/logging/to_existing_object)中了解更多关于恢复运行的信息。'
- en: '*`*neptune_run_kwargs` (*optional*) — Additional keyword arguments to be passed
    directly to the [`neptune.init_run()`](https://docs.neptune.ai/api/neptune#init_run)
    function when a new run is created.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*neptune_run_kwargs` (*可选*) — 传递给[`neptune.init_run()`](https://docs.neptune.ai/api/neptune#init_run)函数的其他关键字参数，当创建新运行时。'
- en: TrainerCallback that sends the logs to [Neptune](https://app.neptune.ai).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志发送到[Neptune](https://app.neptune.ai)的TrainerCallback。
- en: For instructions and examples, see the [Transformers integration guide](https://docs.neptune.ai/integrations/transformers)
    in the Neptune documentation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有关说明和示例，请参阅Neptune文档中的[Transformers集成指南](https://docs.neptune.ai/integrations/transformers)。
- en: '### `class transformers.integrations.ClearMLCallback`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.ClearMLCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1428)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1428)'
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [ClearML](https://clear.ml/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将日志发送到[ClearML](https://clear.ml/)的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。
- en: 'Environment:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：
- en: '`CLEARML_PROJECT` (`str`, *optional*, defaults to `HuggingFace Transformers`):
    ClearML project name.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLEARML_PROJECT` (`str`, *可选*, 默认为 `HuggingFace Transformers`): ClearML项目名称。'
- en: '`CLEARML_TASK` (`str`, *optional*, defaults to `Trainer`): ClearML task name.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLEARML_TASK` (`str`, *可选*, 默认为 `Trainer`): ClearML任务名称。'
- en: '`CLEARML_LOG_MODEL` (`bool`, *optional*, defaults to `False`): Whether to log
    models as artifacts during training.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLEARML_LOG_MODEL` (`bool`, *可选*, 默认为 `False`): 是否在训练期间将模型记录为工件。'
- en: '### `class transformers.integrations.DagsHubCallback`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.DagsHubCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1068)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1068)'
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that logs to [DagsHub](https://dagshub.com/). Extends `MLflowCallback`
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将日志记录到[DagsHub](https://dagshub.com/)的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。扩展`MLflowCallback`
- en: '#### `setup`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1082)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1082)'
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Setup the DagsHub’s Logging integration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 设置DagsHub的日志记录集成。
- en: 'Environment:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：
- en: '`HF_DAGSHUB_LOG_ARTIFACTS` (`str`, *optional*): Whether to save the data and
    model artifacts for the experiment. Default to `False`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_DAGSHUB_LOG_ARTIFACTS` (`str`, *可选*): 是否保存实验的数据和模型工件。默认为`False`。'
- en: '### `class transformers.integrations.FlyteCallback`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.FlyteCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1549)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1549)'
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_log_history` (`bool`, *optional*, defaults to `True`) — When set to True,
    the training logs are saved as a Flyte Deck.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_log_history` (`bool`, *可选*, 默认为 `True`) — 当设置为True时，训练日志将保存为Flyte Deck。'
- en: '`sync_checkpoints` (`bool`, *optional*, defaults to `True`) — When set to True,
    checkpoints are synced with Flyte and can be used to resume training in the case
    of an interruption.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sync_checkpoints` (`bool`, *可选*, 默认为 `True`) — 当设置为True时，检查点将与Flyte同步，并可用于在中断的情况下恢复训练。'
- en: 'A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [Flyte](https://flyte.org/). NOTE: This callback only works
    within a Flyte task.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将日志发送到[Flyte](https://flyte.org/)的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。注意：此回调仅在Flyte任务内起作用。
- en: 'Example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '### `class transformers.integrations.DVCLiveCallback`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.integrations.DVCLiveCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1612)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1612)'
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`live` (`dvclive.Live`, *optional*, defaults to `None`) — Optional Live instance.
    If None, a new instance will be created using **kwargs.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`live` (`dvclive.Live`, *可选*, 默认为 `None`) — 可选的Live实例。如果为None，则将使用**kwargs创建一个新实例。'
- en: '`log_model` (Union[Literal[“all”], bool], *optional*, defaults to `None`) —
    Whether to use `dvclive.Live.log_artifact()` to log checkpoints created by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If set to `True`, the final checkpoint is logged at the end of training. If set
    to `"all"`, the entire [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)’s
    `output_dir` is logged at each checkpoint.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_model` (Union[Literal[“all”], bool], *可选*, 默认为 `None`) — 是否使用`dvclive.Live.log_artifact()`来记录[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)创建的检查点。如果设置为`True`，则在训练结束时记录最终检查点。如果设置为`"all"`，则在每个检查点处记录整个[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)的`output_dir`。'
- en: A [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    that sends the logs to [DVCLive](https://www.dvc.org/doc/dvclive).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将日志发送到[DVCLive](https://www.dvc.org/doc/dvclive)的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。
- en: Use the environment variables below in `setup` to configure the integration.
    To customize this callback beyond those environment variables, see [here](https://dvc.org/doc/dvclive/ml-frameworks/huggingface).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在`setup`中使用下面的环境变量来配置集成。要在这些环境变量之外自定义此回调，请参阅[此处](https://dvc.org/doc/dvclive/ml-frameworks/huggingface)。
- en: '#### `setup`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `setup`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1648)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/integrations/integration_utils.py#L1648)'
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Setup the optional DVCLive integration. To customize this callback beyond the
    environment variables below, see [here](https://dvc.org/doc/dvclive/ml-frameworks/huggingface).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 设置可选的DVCLive集成。要在环境变量之外自定义此回调，请参阅[此处](https://dvc.org/doc/dvclive/ml-frameworks/huggingface)。
- en: 'Environment:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：
- en: '`HF_DVCLIVE_LOG_MODEL` (`str`, *optional*): Whether to use `dvclive.Live.log_artifact()`
    to log checkpoints created by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
    If set to `True` or *1*, the final checkpoint is logged at the end of training.
    If set to `all`, the entire [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)’s
    `output_dir` is logged at each checkpoint.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_DVCLIVE_LOG_MODEL` (`str`, *可选*): 是否使用`dvclive.Live.log_artifact()`来记录[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)创建的检查点。如果设置为`True`或*1*，则在训练结束时记录最终检查点。如果设置为`all`，则在每个检查点处记录整个[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)的`output_dir`。'
- en: TrainerCallback
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainerCallback
- en: '### `class transformers.TrainerCallback`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainerCallback`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L175)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L175)'
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments))
    — The training arguments used to instantiate the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments))
    — 用于实例化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的训练参数。'
- en: '`state` ([TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState))
    — The current state of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state` ([TrainerState](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerState))
    — [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的当前状态。'
- en: '`control` ([TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl))
    — The object that is returned to the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and can be used to make some decisions.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`control` ([TrainerControl](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerControl))
    — 返回给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)并可用于做出一些决策的对象。'
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or `torch.nn.Module`) — The model being trained.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)或`torch.nn.Module`)
    — 正在训练的模型。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer used for encoding the data.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 用于编码数据的分词器。'
- en: '`optimizer` (`torch.optim.Optimizer`) — The optimizer used for the training
    steps.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`torch.optim.Optimizer`) — 用于训练步骤的优化器。'
- en: '`lr_scheduler` (`torch.optim.lr_scheduler.LambdaLR`) — The scheduler used for
    setting the learning rate.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_scheduler` (`torch.optim.lr_scheduler.LambdaLR`) — 用于设置学习率的调度器。'
- en: '`train_dataloader` (`torch.utils.data.DataLoader`, *optional*) — The current
    dataloader used for training.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataloader` (`torch.utils.data.DataLoader`, *optional*) — 用于训练的当前数据加载器。'
- en: '`eval_dataloader` (`torch.utils.data.DataLoader`, *optional*) — The current
    dataloader used for training.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataloader` (`torch.utils.data.DataLoader`, *optional*) — 用于训练的当前数据加载器。'
- en: '`metrics` (`Dict[str, float]`) — The metrics computed by the last evaluation
    phase.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` (`Dict[str, float]`) — 上次评估阶段计算的指标。'
- en: Those are only accessible in the event `on_evaluate`.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些只能在事件`on_evaluate`中访问。
- en: '`logs` (`Dict[str, float]`) — The values to log.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logs` (`Dict[str, float]`) — 要记录的值。'
- en: Those are only accessible in the event `on_log`.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些只能在事件`on_log`中访问。
- en: 'A class for objects that will inspect the state of the training loop at some
    events and take some decisions. At each of those events the following arguments
    are available:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于在某些事件中检查训练循环状态并做出一些决策的对象类。在每个事件中，以下参数都是可用的：
- en: The `control` object is the only one that can be changed by the callback, in
    which case the event that changes it should return the modified version.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`control`对象是唯一可以被回调函数更改的对象，如果更改了`control`的事件应该返回修改后的版本。'
- en: The argument `args`, `state` and `control` are positionals for all events, all
    the others are grouped in `kwargs`. You can unpack the ones you need in the signature
    of the event using them. As an example, see the code of the simple [PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`args`、`state`和`control`对于所有事件都是位置参数，其他参数都被分组在`kwargs`中。您可以在事件的签名中解包您需要的参数。例如，查看简单的[PrinterCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.PrinterCallback)的代码。
- en: 'Example:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#### `on_epoch_begin`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_epoch_begin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L244)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L244)'
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Event called at the beginning of an epoch.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个epoch开始时调用的事件。
- en: '#### `on_epoch_end`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_epoch_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L250)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L250)'
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Event called at the end of an epoch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个epoch结束时调用的事件。
- en: '#### `on_evaluate`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_evaluate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L276)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L276)'
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Event called after an evaluation phase.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估阶段后调用的事件。
- en: '#### `on_init_end`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_init_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L226)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L226)'
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Event called at the end of the initialization of the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)初始化结束时调用的事件。
- en: '#### `on_log`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_log`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L294)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L294)'
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Event called after logging the last logs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在记录最后日志后调用的事件。
- en: '#### `on_predict`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_predict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L282)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L282)'
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Event called after a successful prediction.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功预测后调用的事件。
- en: '#### `on_prediction_step`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_prediction_step`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L300)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L300)'
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Event called after a prediction step.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测步骤后调用的事件。
- en: '#### `on_save`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_save`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L288)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L288)'
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Event called after a checkpoint save.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在保存检查点后调用的事件。
- en: '#### `on_step_begin`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_step_begin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L256)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L256)'
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Event called at the beginning of a training step. If using gradient accumulation,
    one training step might take several inputs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练步骤开始时调用的事件。如果使用梯度累积，一个训练步骤可能需要多个输入。
- en: '#### `on_step_end`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_step_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L269)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L269)'
- en: '[PRE32]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Event called at the end of a training step. If using gradient accumulation,
    one training step might take several inputs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练步骤结束时调用的事件。如果使用梯度累积，一个训练步骤可能需要多个输入。
- en: '#### `on_substep_end`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_substep_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L263)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L263)'
- en: '[PRE33]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Event called at the end of an substep during gradient accumulation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度累积期间子步骤结束时调用的事件。
- en: '#### `on_train_begin`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_train_begin`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L232)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L232)'
- en: '[PRE34]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Event called at the beginning of training.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始时调用的事件。
- en: '#### `on_train_end`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`on_train_end`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L238)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L238)'
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Event called at the end of training.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练结束时调用的事件。
- en: 'Here is an example of how to register a custom callback with the PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如何在PyTorch [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    中注册自定义回调的示例：
- en: '[PRE36]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Another way to register a callback is to call `trainer.add_callback()` as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种注册回调的方法是调用`trainer.add_callback()`如下：
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: TrainerState
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainerState
- en: '### `class transformers.TrainerState`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainerState`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L34)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L34)'
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`epoch` (`float`, *optional*) — Only set during training, will represent the
    epoch the training is at (the decimal part being the percentage of the current
    epoch completed).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epoch` (`float`, *可选*) — 仅在训练期间设置，表示训练所处的时代（小数部分表示当前时代完成的百分比）。'
- en: '`global_step` (`int`, *optional*, defaults to 0) — During training, represents
    the number of update steps completed.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_step` (`int`, *可选*, 默认为0) — 在训练过程中，表示已完成的更新步骤数量。'
- en: '`max_steps` (`int`, *optional*, defaults to 0) — The number of update steps
    to do during the current training.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps` (`int`, *可选*, 默认为0) — 当前训练要执行的更新步骤数量。'
- en: '`logging_steps` (`int`, *optional*, defaults to 500) — Log every X updates
    steps'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logging_steps` (`int`, *可选*, 默认为500) — 每X个更新步骤记录一次日志'
- en: '`eval_steps` (`int`, *optional*) — Run an evaluation every X steps.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_steps` (`int`, *可选*) — 每X步运行一次评估。'
- en: '`save_steps` (`int`, *optional*, defaults to 500) — Save checkpoint every X
    updates steps.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps` (`int`, *可选*, 默认为500) — 每X个更新步骤保存一次检查点。'
- en: '`train_batch_size` (`int`, *optional*) — The batch size for the training dataloader.
    Only needed when `auto_find_batch_size` has been used.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_batch_size` (`int`, *可选*) — 训练数据加载器的批量大小。仅在使用`auto_find_batch_size`时需要。'
- en: '`num_input_tokens_seen` (`int`, *optional*, defaults to 0) — The number of
    tokens seen during training (number of input tokens, not the number of prediction
    tokens).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_input_tokens_seen` (`int`, *可选*, 默认为0) — 训练期间看到的标记数量（输入标记数量，而不是预测标记数量）。'
- en: '`total_flos` (`float`, *optional*, defaults to 0) — The total number of floating
    operations done by the model since the beginning of training (stored as floats
    to avoid overflow).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_flos` (`float`, *可选*, 默认为0) — 自训练开始以来模型执行的浮点操作总数（存储为浮点数以避免溢出）。'
- en: '`log_history` (`List[Dict[str, float]]`, *optional*) — The list of logs done
    since the beginning of training.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_history` (`List[Dict[str, float]]`, *可选*) — 自训练开始以来完成的日志列表。'
- en: '`best_metric` (`float`, *optional*) — When tracking the best model, the value
    of the best metric encountered so far.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_metric` (`float`, *可选*) — 跟踪最佳模型时，迄今为止遇到的最佳指标的值。'
- en: '`best_model_checkpoint` (`str`, *optional*) — When tracking the best model,
    the value of the name of the checkpoint for the best model encountered so far.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_model_checkpoint` (`str`, *可选*) — 跟踪最佳模型时，迄今为止遇到的最佳模型的检查点名称的值。'
- en: '`is_local_process_zero` (`bool`, *optional*, defaults to `True`) — Whether
    or not this process is the local (e.g., on one machine if training in a distributed
    fashion on several machines) main process.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_local_process_zero` (`bool`, *可选*, 默认为`True`) — 是否这个进程是本地（例如，在多台机器上以分布式方式训练时，是一台机器上的主进程）。'
- en: '`is_world_process_zero` (`bool`, *optional*, defaults to `True`) — Whether
    or not this process is the global main process (when training in a distributed
    fashion on several machines, this is only going to be `True` for one process).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_world_process_zero` (`bool`, *可选*, 默认为`True`) — 是否这个进程是全局主进程（在多台机器上以分布式方式训练时，只有一个进程会是`True`）。'
- en: '`is_hyper_param_search` (`bool`, *optional*, defaults to `False`) — Whether
    we are in the process of a hyper parameter search using Trainer.hyperparameter_search.
    This will impact the way data will be logged in TensorBoard.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_hyper_param_search` (`bool`, *可选*, 默认为`False`) — 是否正在使用Trainer.hyperparameter_search进行超参数搜索。这将影响数据在TensorBoard中记录的方式。'
- en: A class containing the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    inner state that will be saved along the model and optimizer when checkpointing
    and passed to the [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含将在检查点时保存的[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)内部状态的类，并传递给[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)。
- en: 'In all this class, one step is to be understood as one update step. When using
    gradient accumulation, one update step may require several forward and backward
    passes: if you use `gradient_accumulation_steps=n`, then one update step requires
    going through *n* batches.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，一个步骤被理解为一个更新步骤。当使用梯度累积时，一个更新步骤可能需要多次前向和后向传递：如果使用`gradient_accumulation_steps=n`，则一个更新步骤需要通过*n*批次。
- en: '#### `load_from_json`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_from_json`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L117)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L117)'
- en: '[PRE39]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Create an instance from the content of `json_path`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从`json_path`的内容创建一个实例。
- en: '#### `save_to_json`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_to_json`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L111)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L111)'
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Save the content of this instance in JSON format inside `json_path`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 将此实例的内容以JSON格式保存在`json_path`中。
- en: TrainerControl
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainerControl
- en: '### `class transformers.TrainerControl`'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TrainerControl`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L125)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L125)'
- en: '[PRE41]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`should_training_stop` (`bool`, *optional*, defaults to `False`) — Whether
    or not the training should be interrupted.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_training_stop`（`bool`，*可选*，默认为`False`）—是否应中断训练。'
- en: If `True`, this variable will not be set back to `False`. The training will
    just stop.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果为`True`，则此变量不会被设置回`False`。训练将直接停止。
- en: '`should_epoch_stop` (`bool`, *optional*, defaults to `False`) — Whether or
    not the current epoch should be interrupted.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_epoch_stop`（`bool`，*可选*，默认为`False`）—当前轮是否应中断。'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next epoch.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果为`True`，则此变量将在下一轮开始时设置回`False`。
- en: '`should_save` (`bool`, *optional*, defaults to `False`) — Whether or not the
    model should be saved at this step.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_save`（`bool`，*可选*，默认为`False`）—模型是否应在此步骤保存。'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果为`True`，则此变量将在下一步开始时设置回`False`。
- en: '`should_evaluate` (`bool`, *optional*, defaults to `False`) — Whether or not
    the model should be evaluated at this step.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_evaluate`（`bool`，*可选*，默认为`False`）—模型是否应在此步骤进行评估。'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果为`True`，则此变量将在下一步开始时设置回`False`。
- en: '`should_log` (`bool`, *optional*, defaults to `False`) — Whether or not the
    logs should be reported at this step.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`should_log`（`bool`，*可选*，默认为`False`）—是否应在此步骤报告日志。'
- en: If `True`, this variable will be set back to `False` at the beginning of the
    next step.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果为`True`，则此变量将在下一步开始时设置回`False`。
- en: A class that handles the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    control flow. This class is used by the [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)
    to activate some switches in the training loop.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 处理[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)控制流的类。此类由[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)使用，以在训练循环中激活一些开关。
