# GIT

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/git`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/git)

## æ¦‚è¿°

GIT æ¨¡å‹æ˜¯ç”± Jianfeng Wangã€Zhengyuan Yangã€Xiaowei Huã€Linjie Liã€Kevin Linã€Zhe Ganã€Zicheng Liuã€Ce Liuã€Lijuan Wang åœ¨[ã€ŠGIT: A Generative Image-to-text Transformer for Vision and Languageã€‹](https://arxiv.org/abs/2205.14100)ä¸­æå‡ºçš„ã€‚GIT æ˜¯ä¸€ç§ä»…è§£ç çš„ Transformerï¼Œåˆ©ç”¨ CLIP çš„è§†è§‰ç¼–ç å™¨æ¥é™¤äº†æ–‡æœ¬å¤–è¿˜å¯¹æ¨¡å‹è¿›è¡Œè§†è§‰è¾“å…¥çš„æ¡ä»¶ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒå­—å¹•å’Œè§†è§‰é—®ç­”åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡å¹¶è®­ç»ƒäº†ä¸€ä¸ªç”Ÿæˆå¼å›¾åƒæ–‡æœ¬ Transformerï¼ŒGITï¼Œä»¥ç»Ÿä¸€å›¾åƒ/è§†é¢‘å­—å¹•å’Œé—®é¢˜å›ç­”ç­‰è§†è§‰-è¯­è¨€ä»»åŠ¡ã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´æä¾›äº†ä¸€è‡´çš„ç½‘ç»œæ¶æ„ï¼Œä½†ç°æœ‰å·¥ä½œé€šå¸¸åŒ…å«å¤æ‚çš„ç»“æ„ï¼ˆå•/å¤šæ¨¡æ€ç¼–ç å™¨/è§£ç å™¨ï¼‰å¹¶ä¾èµ–äºå¤–éƒ¨æ¨¡å—ï¼Œå¦‚ç›®æ ‡æ£€æµ‹å™¨/æ ‡è®°å™¨å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ã€‚åœ¨ GIT ä¸­ï¼Œæˆ‘ä»¬ç®€åŒ–äº†æ¶æ„ï¼Œå°†å…¶ä½œä¸ºä¸€ä¸ªå›¾åƒç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æœ¬è§£ç å™¨åœ¨å•ä¸€è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸‹ã€‚æˆ‘ä»¬è¿˜æ‰©å¤§äº†é¢„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ²¡æœ‰èŠ±å“¨çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬çš„ GIT åœ¨ 12 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå·®è·å¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é¦–æ¬¡åœ¨ TextCaps ä¸Šè¶…è¶Šäº†äººç±»è¡¨ç°ï¼ˆCIDEr ä¸­çš„ 138.2 vs. 125.5ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºç”Ÿæˆçš„å›¾åƒåˆ†ç±»å’Œåœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹æ¡ˆï¼Œåœ¨æ ‡å‡†åŸºå‡†ä¸Šå–å¾—äº†ä¸é”™çš„è¡¨ç°ã€‚*

![ç»˜å›¾](img/6ed8ec46ede23e2981ae0fdca0d97f2b.png) GIT æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2205.14100)ã€‚

è¯¥æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/microsoft/GenerativeImage2Text)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   GIT çš„å®ç°æ–¹å¼ä¸ GPT-2 éå¸¸ç›¸ä¼¼ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºæ¨¡å‹è¿˜å—åˆ°`pixel_values`çš„å½±å“ã€‚

## èµ„æº

å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ GITã€‚

+   å…³äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œæ¨ç†+å¾®è°ƒ GIT çš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GIT)æ‰¾åˆ°ã€‚

+   å¦è¯·å‚é˜…ï¼šå› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ã€‚èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## GitVisionConfig

### `class transformers.GitVisionConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/configuration_git.py#L30)

```py
( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 16 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 **kwargs )
```

å‚æ•°

+   `hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 768ï¼‰â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `intermediate_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 3072ï¼‰â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 12ï¼‰â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚

+   `num_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 12ï¼‰â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `image_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 224ï¼‰â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `patch_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 16ï¼‰â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `hidden_act`ï¼ˆ`str`æˆ–`function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"quick_gelu"`ï¼‰â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ä»¥åŠ`"quick_gelu"`ã€‚

+   `layer_norm_eps` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1e-5) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `attention_dropout` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚

+   `initializer_range` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ GitVisionModel é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– GIT è§†è§‰ç¼–ç å™¨ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº GIT [microsoft/git-base](https://huggingface.co/microsoft/git-base)æ¶æ„çš„è§†è§‰ç¼–ç å™¨çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import GitVisionConfig, GitVisionModel

>>> # Initializing a GitVisionConfig with microsoft/git-base style configuration
>>> configuration = GitVisionConfig()

>>> # Initializing a GitVisionModel (with random weights) from the microsoft/git-base style configuration
>>> model = GitVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## GitVisionModel

### `class transformers.GitVisionModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/modeling_git.py#L962)

```py
( config: GitVisionConfig )
```

å‚æ•°

+   `config` (GitConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

CLIP ä¸­ä½¿ç”¨çš„è§†è§‰æ¨¡å‹ï¼Œåœ¨ GIT ä¸­æ²¡æœ‰é¡¶éƒ¨çš„å¤´éƒ¨æˆ–æŠ•å½±ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/modeling_git.py#L980)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹ä¼šå¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… CLIPImageProcessor.`call`()ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_outputs.BaseModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.BaseModelOutput æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.git.configuration_git.GitVisionConfig'>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥å±‚çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

GitVisionModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, GitVisionModel

>>> processor = AutoProcessor.from_pretrained("microsoft/git-base")
>>> model = GitVisionModel.from_pretrained("microsoft/git-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
```

## GitConfig

`class transformers.GitConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/configuration_git.py#L128)

```py
( vision_config = None vocab_size = 30522 hidden_size = 768 num_hidden_layers = 6 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 1024 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True tie_word_embeddings = False bos_token_id = 101 eos_token_id = 102 num_image_with_embedding = None **kwargs )
```

å‚æ•°

+   `vision_config` (`dict`ï¼Œ*å¯é€‰*) â€” ç”¨äºåˆå§‹åŒ– GitVisionConfig çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚

+   `vocab_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 30522) â€” GIT æ¨¡å‹çš„è¯æ±‡é‡å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ GitModel æ—¶å¯ä»¥ç”± `inputs_ids` è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 6) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `intermediate_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` æˆ– `Callable`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`, `"relu"`, `"silu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚

+   `max_position_embeddings` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1024) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512 æˆ– 1024 æˆ– 2048ï¼‰ã€‚

+   `initializer_range` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) â€” ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚é€‰æ‹© `"absolute"`ã€`"relative_key"`ã€`"relative_key_query"` ä¸­çš„ä¸€ä¸ªã€‚å¯¹äºä½ç½®åµŒå…¥ï¼Œè¯·ä½¿ç”¨ `"absolute"`ã€‚æœ‰å…³ `"relative_key"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)ã€‚æœ‰å…³ `"relative_key_query"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)] ä¸­çš„ *Method 4* (https://arxiv.org/abs/2009.13658)ã€‚

+   `use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

+   `num_image_with_embedding` (`int`, *optional*) â€” è¦æ·»åŠ çš„æ—¶é—´åµŒå…¥æ•°é‡ï¼Œå¦‚æœæ¨¡å‹ç”¨äºè§†é¢‘å­—å¹•/VQAã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ GitModel é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– GIT æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº GIT [microsoft/git-base](https://huggingface.co/microsoft/git-base) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import GitConfig, GitModel

>>> # Initializing a GIT microsoft/git-base style configuration
>>> configuration = GitConfig()

>>> # Initializing a model (with random weights) from the microsoft/git-base style configuration
>>> model = GitModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## GitProcessor

### `class transformers.GitProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/processing_git.py#L23)

```py
( image_processor tokenizer )
```

å‚æ•°

+   `image_processor` (AutoImageProcessor) â€” å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer` (AutoTokenizer) â€” Tokenizer æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ª GIT å¤„ç†å™¨ï¼Œå°† CLIP å›¾åƒå¤„ç†å™¨å’Œ BERT åˆ†è¯å™¨åŒ…è£…æˆå•ä¸ªå¤„ç†å™¨ã€‚

GitProcessor æä¾›äº† CLIPImageProcessor å’Œ BertTokenizerFast çš„æ‰€æœ‰åŠŸèƒ½ã€‚æŸ¥çœ‹ **call**() å’Œ `decode()` ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/processing_git.py#L45)

```py
( text = None images = None return_tensors = None **kwargs ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `text` (`str`, `List[str]`, `List[List[str]]`) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœåºåˆ—ä»¥å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰çš„å½¢å¼æä¾›ï¼Œåˆ™å¿…é¡»è®¾ç½® `is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸åºåˆ—æ‰¹æ¬¡çš„æ­§ä¹‰ï¼‰ã€‚

+   `images` (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`) â€” è¦å‡†å¤‡çš„å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚æ¯ä¸ªå›¾åƒå¯ä»¥æ˜¯ PIL å›¾åƒã€NumPy æ•°ç»„æˆ– PyTorch å¼ é‡ã€‚å¦‚æœæ˜¯ NumPy æ•°ç»„/PyTorch å¼ é‡ï¼Œåˆ™æ¯ä¸ªå›¾åƒåº”ä¸ºå½¢çŠ¶ (C, H, W)ï¼Œå…¶ä¸­ C æ˜¯é€šé“æ•°ï¼ŒH å’Œ W æ˜¯å›¾åƒé«˜åº¦å’Œå®½åº¦ã€‚

+   `return_tensors` (`str` æˆ– TensorType, *optional*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›ç‰¹å®šæ¡†æ¶çš„å¼ é‡ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚

    +   `'pt'`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚

    +   `'np'`: è¿”å› NumPy `np.ndarray` å¯¹è±¡ã€‚

    +   `'jax'`: è¿”å› JAX `jnp.ndarray` å¯¹è±¡ã€‚

è¿”å›

BatchEncoding

ä¸€ä¸ªå¸¦æœ‰ä»¥ä¸‹å­—æ®µçš„ BatchEncodingï¼š

+   `input_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„æ ‡è®° id åˆ—è¡¨ã€‚å½“`text`ä¸ä¸º`None`æ—¶è¿”å›ã€‚

+   `attention_mask` â€” æŒ‡å®šå“ªäº›æ ‡è®°åº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–è€…*`attention_mask`*åœ¨`self.model_input_names`ä¸­ï¼Œä¸”`text`ä¸ä¸º`None`æ—¶ï¼‰ã€‚

+   `pixel_values` â€” è¦æä¾›ç»™æ¨¡å‹çš„åƒç´ å€¼ã€‚å½“`images`ä¸ä¸º`None`æ—¶è¿”å›ã€‚

ä¸ºæ¨¡å‹å‡†å¤‡ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å’Œå›¾åƒçš„ä¸»è¦æ–¹æ³•ã€‚å¦‚æœ`text`ä¸ä¸º`None`ï¼Œåˆ™æ­¤æ–¹æ³•å°†`text`å’Œ`kwargs`å‚æ•°è½¬å‘ç»™ BertTokenizerFast çš„**call**()ä»¥å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚ä¸ºå‡†å¤‡å›¾åƒï¼Œå¦‚æœ`images`ä¸ä¸º`None`ï¼Œåˆ™æ­¤æ–¹æ³•å°†`images`å’Œ`kwrags`å‚æ•°è½¬å‘ç»™ CLIPImageProcessor çš„**call**()ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## GitModel

### `class transformers.GitModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/modeling_git.py#L1033)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆGitConfigï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ç”± CLIP å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬è§£ç å™¨ç»„æˆçš„åŸºæœ¬ GIT æ¨¡å‹å˜å‹å™¨ï¼Œè¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/modeling_git.py#L1128)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None pixel_values: Optional = None head_mask: Optional = None inputs_embeds: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š

    +   1 è¡¨ç¤ºâ€œæœªè¢«æ©ç â€çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºâ€œè¢«æ©ç â€çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… CLIPImageProcessor.`call`()ã€‚

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œé•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å« 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå³æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„é‚£äº›ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.BaseModelOutputWithPooling æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.BaseModelOutputWithPooling æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆGitConfigï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) â€” åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼ˆç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼‰ã€‚ä¾‹å¦‚ï¼Œå¯¹äº BERT ç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œ tanh æ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯ä»é¢„è®­ç»ƒæœŸé—´çš„ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

GitModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, AutoModel
>>> import requests
>>> from PIL import Image

>>> processor = AutoProcessor.from_pretrained("microsoft/git-base")
>>> model = AutoModel.from_pretrained("microsoft/git-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> text = "this is an image of two cats"

>>> inputs = processor(text, images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
```

## GitForCausalLM

### `class transformers.GitForCausalLM`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/modeling_git.py#L1297)

```py
( config )
```

å‚æ•°

+   `config` (GitConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´éƒ¨çš„ GIT æ¨¡å‹ï¼Œç”¨äºè‡ªå›å½’è¯­è¨€å»ºæ¨¡ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/git/modeling_git.py#L1318)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None pixel_values: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æŸ¥çœ‹ CLIPImageProcessor.`call`()è·å–è¯¦ç»†ä¿¡æ¯ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç›–â€ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—ä»å·¦åˆ°å³çš„è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆä¸‹ä¸€ä¸ªå•è¯é¢„æµ‹ï¼‰çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­ï¼ˆè¯·å‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç›–ï¼‰ï¼Œä»…å¯¹å…·æœ‰æ ‡ç­¾ n `[0, ..., config.vocab_size]`çš„æ ‡è®°è®¡ç®—æŸå¤±

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„ 4 ä¸ªå¼ é‡ï¼‰â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›ä¸å…·æœ‰å…¶è¿‡å»é”®å€¼çŠ¶æ€çš„æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.CausalLMOutputWithPast æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.CausalLMOutputWithPast æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆGitConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values`ï¼ˆ*å¯é€‰*ï¼Œåœ¨ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„ 2 ä¸ªå¼ é‡ï¼‰

    åŒ…å«é¢„è®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„å—ä¸­çš„é”®å’Œå€¼ï¼‰å¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥å±‚çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

GitForCausalLM çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

å›¾åƒå­—å¹•ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, AutoModelForCausalLM
>>> import requests
>>> from PIL import Image

>>> processor = AutoProcessor.from_pretrained("microsoft/git-base-coco")
>>> model = AutoModelForCausalLM.from_pretrained("microsoft/git-base-coco")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> pixel_values = processor(images=image, return_tensors="pt").pixel_values

>>> generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
>>> generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_caption)
two cats sleeping on a pink blanket next to remotes.
```

è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, AutoModelForCausalLM
>>> from huggingface_hub import hf_hub_download
>>> from PIL import Image

>>> processor = AutoProcessor.from_pretrained("microsoft/git-base-textvqa")
>>> model = AutoModelForCausalLM.from_pretrained("microsoft/git-base-textvqa")

>>> file_path = hf_hub_download(repo_id="nielsr/textvqa-sample", filename="bus.png", repo_type="dataset")
>>> image = Image.open(file_path).convert("RGB")

>>> pixel_values = processor(images=image, return_tensors="pt").pixel_values

>>> question = "what does the front of the bus say at the top?"

>>> input_ids = processor(text=question, add_special_tokens=False).input_ids
>>> input_ids = [processor.tokenizer.cls_token_id] + input_ids
>>> input_ids = torch.tensor(input_ids).unsqueeze(0)

>>> generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)
>>> print(processor.batch_decode(generated_ids, skip_special_tokens=True))
['what does the front of the bus say at the top? special']
```

è§†é¢‘å­—å¹•ç¤ºä¾‹ï¼š

```py
>>> import av
>>> import numpy as np
>>> from PIL import Image
>>> from huggingface_hub import hf_hub_download
>>> from transformers import AutoProcessor, AutoModelForCausalLM

>>> processor = AutoProcessor.from_pretrained("microsoft/git-base-vatex")
>>> model = AutoModelForCausalLM.from_pretrained("microsoft/git-base-vatex")

>>> # set seed for reproducability
>>> np.random.seed(45)

>>> def read_video_pyav(container, indices):
...     '''
...     Decode the video with PyAV decoder.
...     Args:
...         container (`av.container.input.InputContainer`): PyAV container.
...         indices (`List[int]`): List of frame indices to decode.
...     Returns:
...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
...     '''
...     frames = []
...     container.seek(0)
...     start_index = indices[0]
...     end_index = indices[-1]
...     for i, frame in enumerate(container.decode(video=0)):
...         if i > end_index:
...             break
...         if i >= start_index and i in indices:
...             frames.append(frame)
...     return np.stack([x.to_ndarray(format="rgb24") for x in frames])

>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
...     '''
...     Sample a given number of frame indices from the video.
...     Args:
...         clip_len (`int`): Total number of frames to sample.
...         frame_sample_rate (`int`): Sample every n-th frame.
...         seg_len (`int`): Maximum allowed index of sample's last frame.
...     Returns:
...         indices (`List[int]`): List of sampled frame indices
...     '''
...     converted_len = int(clip_len * frame_sample_rate)
...     end_idx = np.random.randint(converted_len, seg_len)
...     start_idx = end_idx - converted_len
...     indices = np.linspace(start_idx, end_idx, num=clip_len)
...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
...     return indices

>>> # load video
>>> file_path = hf_hub_download(
...     repo_id="nielsr/video-demo", filename="eating_spaghetti.mp4", repo_type="dataset"
... )
>>> container = av.open(file_path)

>>> # sample frames
>>> num_frames = model.config.num_image_with_embedding
>>> indices = sample_frame_indices(
...     clip_len=num_frames, frame_sample_rate=4, seg_len=container.streams.video[0].frames
... )
>>> frames = read_video_pyav(container, indices)

>>> pixel_values = processor(images=list(frames), return_tensors="pt").pixel_values

>>> generated_ids = model.generate(pixel_values=pixel_values, max_length=50)

>>> print("Generated caption:", processor.batch_decode(generated_ids, skip_special_tokens=True))
Generated caption: ['a woman is sitting at a table and she is talking about the food she is holding.']
```
