# RWKV

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rwkv)

## 概述

RWKV 模型是在[此存储库](https://github.com/BlinkDL/RWKV-LM)中提出的。

它建议对传统 Transformer 注意力进行微调，使其线性化。这样，模型可以用作循环网络：同时传递时间戳 0 和时间戳 1 的输入与在时间戳 0 传递输入，然后在时间戳 1 传递输入以及时间戳 0 的状态是相同的（见下面的示例）。

这比常规 Transformer 更有效，并且可以处理任意长度的句子（即使模型在训练时使用固定的上下文长度）。

这个模型是由[sgugger](https://huggingface.co/sgugger)贡献的。原始代码可以在[这里](https://github.com/BlinkDL/RWKV-LM)找到。

## 用法示例

```py
import torch
from transformers import AutoTokenizer, RwkvConfig, RwkvModel

model = RwkvModel.from_pretrained("sgugger/rwkv-430M-pile")
tokenizer = AutoTokenizer.from_pretrained("sgugger/rwkv-430M-pile")

inputs = tokenizer("This is an example.", return_tensors="pt")
# Feed everything to the model
outputs = model(inputs["input_ids"])
output_whole = outputs.last_hidden_state

outputs = model(inputs["input_ids"][:, :2])
output_one = outputs.last_hidden_state

# Using the state computed on the first inputs, we will get the same output
outputs = model(inputs["input_ids"][:, 2:], state=outputs.state)
output_two = outputs.last_hidden_state

torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5)
```

如果要确保模型在检测到`'\n\n'`时停止生成，我们建议使用以下停止标准：

```py
from transformers import StoppingCriteria

class RwkvStoppingCriteria(StoppingCriteria):
    def __init__(self, eos_sequence = [187,187], eos_token_id = 537):
        self.eos_sequence = eos_sequence
        self.eos_token_id = eos_token_id

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        last_2_ids = input_ids[:,-2:].tolist()
        return self.eos_sequence in last_2_ids

output = model.generate(inputs["input_ids"], max_new_tokens=64, stopping_criteria = [RwkvStoppingCriteria()])
```

## RwkvConfig

### `class transformers.RwkvConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/configuration_rwkv.py#L38)

```py
( vocab_size = 50277 context_length = 1024 hidden_size = 4096 num_hidden_layers = 32 attention_hidden_size = None intermediate_size = None layer_norm_epsilon = 1e-05 bos_token_id = 0 eos_token_id = 0 rescale_every = 6 tie_word_embeddings = False use_cache = True **kwargs )
```

参数

+   `vocab_size` (`int`，*可选*，默认为 50277) — RWKV 模型的词汇量。定义了在调用 RwkvModel 时可以表示的不同标记数量。

+   `context_length` (`int`，*可选*，默认为 1024) — 此模型可以在单个前向传播中使用的最大序列长度（在 RNN 模式中使用任何序列长度）。

+   `hidden_size` (`int`，*可选*，默认为 4096) — 嵌入和隐藏状态的维度。

+   `num_hidden_layers` (`int`，*可选*，默认为 32) — 模型中的隐藏层数量。

+   `attention_hidden_size` (`int`，*可选*) — 注意力隐藏状态的维度。如果未设置，将默认为`hidden_size`。

+   `intermediate_size` (`int`，*可选*) — 内部前馈层的维度。如果未设置，将默认为`hidden_size`的 4 倍。

+   `layer_norm_epsilon` (`float`，*可选*，默认为 1e-05) — 在层归一化层中使用的 epsilon。

+   `bos_token_id` (`int`，*可选*，默认为 0) — 词汇表中句子开头标记的 id。默认为 0，因为 RWKV 使用与 GPTNeoX 相同的分词器。

+   `eos_token_id` (`int`，*可选*，默认为 0) — 词汇表中句子结尾标记的 id。默认为 0，因为 RWKV 使用与 GPTNeoX 相同的分词器。

+   `rescale_every` (`int`，*可选*，默认为 6) — 推理时，隐藏状态（以及相应输出层的权重）每`rescale_every`层除以 2。如果设置为 0 或负数，则不进行重新缩放。

+   `tie_word_embeddings` (`bool`，*可选*，默认为`False`) — 是否将单词嵌入与输入标记嵌入相结合。

+   `use_cache` (`bool`，*可选*，默认为`True`) — 模型是否应返回最后一个状态。

这是存储 RwkvModel 配置的配置类。它用于根据指定的参数实例化一个 RWKV 模型，定义模型架构。使用默认值实例化配置将产生类似于 RWVK-4 [RWKV/rwkv-4-169m-pile](https://huggingface.co/RWKV/rwkv-4-169m-pile)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import RwkvConfig, RwkvModel

>>> # Initializing a Rwkv configuration
>>> configuration = RwkvConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = RwkvModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## RwkvModel

### `class transformers.RwkvModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L592)

```py
( config )
```

参数

+   `config`（RwkvConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 RWKV 模型变压器输出原始隐藏状态，没有特定的头部。

这个模型继承自 PreTrainedModel。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `前进`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L617)

```py
( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None state: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.rwkv.modeling_rwkv.RwkvOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）— 如果`past_key_values`为`None`，则`input_ids_length` = `sequence_length`，否则为`past_key_values[0][0].shape[-2]`（输入过去关键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用`past_key_values`，则只有那些没有计算过去的`input_ids`应该作为`input_ids`传递。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被“掩码”的标记为 1，

    +   对于被“掩码”的标记为 0。

    目前`RwkvModel`不使用这个，但将在未来支持。

    什么是注意力掩码？

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。

+   `state`（五个形状为`(batch_size, hidden_size, num_hidden_layers)`的`torch.FloatTensor`元组，*可选*）— 如果传递，模型将在所有块中使用先前的状态（这将为提供的`input_ids`提供输出，就好像模型将`state_input_ids + input_ids`作为上下文）。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回上一个状态，并可用于快速生成下一个对数。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.rwkv.modeling_rwkv.RwkvOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.rwkv.modeling_rwkv.RwkvOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（RwkvConfig）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `state`（形状为`(batch_size, hidden_size, num_hidden_layers)`的五个`torch.FloatTensor`列表）- 模型在最后一个时间步的状态。可以在前向方法中与下一个`input_ids`一起使用，以避免提供旧的`input_ids`。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。

    模型在每一层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

RwkvModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, RwkvModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("RWKV/rwkv-4-169m-pile")
>>> model = RwkvModel.from_pretrained("RWKV/rwkv-4-169m-pile")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## RwkvLMHeadModel

### `class transformers.RwkvForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L757)

```py
( config )
```

参数

+   `config`（RwkvConfig）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

带有语言建模头部的 RWKV 模型变压器（线性层，其权重与输入嵌入绑定）。

该模型继承自 PreTrainedModel。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。

该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rwkv/modeling_rwkv.py#L795)

```py
( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None state: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）- 如果`past_key_values`为`None`，则`input_ids_length`=`sequence_length`，否则`input_ids_length`=`past_key_values[0][0].shape[-2]`（输入过去关键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用`past_key_values`，则只应将未计算其过去的`input_ids`作为`input_ids`传递。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.LongTensor`，形状为`(batch_size, input_ids_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1 表示`未被掩盖`的标记，

    +   0 表示`被掩盖`的标记。

    这目前`RwkvModel`没有使用，但将来会支持。

    什么是注意力掩码？

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `state`（五个形状为`(batch_size, hidden_size, num_hidden_layers)`的`torch.FloatTensor`元组，*可选*） — 如果传递，模型将在所有块中使用先前的状态（这将为提供的`input_ids`产生输出，就好像模型将`state_input_ids + input_ids`作为上下文）。

+   `use_cache` (`bool`，*可选*) — 如果设置为`True`，则返回上一个状态，并可用于快速生成下一个 logits。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 ModelOutput 而不是一个普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 语言建模的标签。请注意，标签**在模型内部被移位**，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（掩盖），损失仅计算在`[0, ..., config.vocab_size]`中的标签。

返回

`transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（RwkvConfig）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `state`（五个形状为`(batch_size, hidden_size, num_hidden_layers)`的`torch.FloatTensor`列表） — 模型在最后一个时间步的状态。可以在前向方法中与下一个`input_ids`一起使用，以避免提供旧的`input_ids`。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出一个，+每个层的输出一个）。

    模型每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

RwkvForCausalLM 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

例如：

```py
>>> import torch
>>> from transformers import AutoTokenizer, RwkvForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("RWKV/rwkv-4-169m-pile")
>>> model = RwkvForCausalLM.from_pretrained("RWKV/rwkv-4-169m-pile")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs, labels=inputs["input_ids"])
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## Rwkv 注意力和循环公式

在传统的自回归 Transformer 中，注意力写为<math display="block"><semantics><mrow><mi>O</mi><mo>=</mo><mrow><mtext>softmax</mtext></mrow><mo stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">O = \hbox{softmax}(QK^{T} / \sqrt{d}) V</annotation></semantics></math>O=softmax(QKT/d​)V

其中<math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math>Q，<math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math>K 和<math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math>V 是形状为`seq_len x hidden_size`的矩阵，分别命名为查询、键和值（实际上它们是带有批处理维度和注意力头维度的更大矩阵，但我们只关心最后两个，这是矩阵乘积发生的地方，所以为了简单起见，我们只考虑这两个）。乘积<math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^{T}</annotation></semantics></math>QKT 然后具有形状`seq_len x seq_len`，我们可以将其与<math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math>V 进行矩阵乘积，得到与其他相同形状的输出<math><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math>O。

用其值替换 softmax 得到：<math display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mi mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">O_{i} = \frac{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \sqrt{d}} V_{j}}{\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \sqrt{d}}}</annotation></semantics></math>Oi​=∑j=1i​eQi​KjT​/d​∑j=1i​eQi​KjT​/d​Vj​​

请注意，QKT 中与 j>i 对应的条目被屏蔽（总和在 j 处停止），因为注意力不允许查看未来的令牌（只能查看过去的令牌）。

相比之下，RWKV 注意力由 Oi = σ(Ri)（∑j=1i eWi−j + Kj Vj）/（∑j=1i eWi−j + Kj）给出。

作者称之为接受度的新矩阵 R，K 和 V 仍然是关键和值（这里σ是 Sigmoid 函数）。W 是代表令牌位置的新向量，由 W0 = u 和 Wk = (k-1)w（对于 k≥1）给出。

u 和 w 是可学习的参数，分别在代码中称为`time_first`和`time_decay`。分子和分母都可以递归表示。将它们命名为 Ni 和 Di，我们有：Ni = e^(u + Ki)Vi + N^i 其中 N^i = e^(Ki-1)Vi-1 + e^(w + Ki-2)Vi-2 + ... + e^((i-2)w + K1)Vi-1

所以 N^i（在代码中称为`numerator_state`）满足 N⁰ = 0 和 N^j+1 = e^Kj Vj + e^w N^j

和 Di = e^(u + Ki) + D^i 其中 D^i = e^(Ki-1) + e^(w + Ki-2) + ... + e^((i-2)w + K1)

因此，\(\hat{D}^{i}\)（在代码中称为`denominator_state`）满足\(\hat{D}_{0} = 0\)和\(\hat{D}_{j+1} = e^{K_{j}} + e^{w} \hat{D}_{j}\)。

实际使用的递归公式稍微复杂一些，因为为了数值稳定性，我们不希望计算大数的指数。通常，softmax 不是按原样计算的，而是将最大项的指数除以分子和分母：\(\frac{e^{x_{i}}}{\sum_{j=1}^{n} e^{x_{j}}} = \frac{e^{x_{i} - M}}{\sum_{j=1}^{n} e^{x_{j} - M}}\)。

M 是所有 xj 的最大值。因此，在保存分子状态（\(\hat{N}\)）和分母状态（\(\hat{D}\)）的同时，我们还跟踪遇到的所有指数项的最大值。因此，我们实际上使用\(\tilde{N}_{i} = e^{-M_{i}} \hat{N}_{i}\)和\(\tilde{D}_{i} = e^{-M_{i}} \hat{D}_{i}\)。

根据以下递归公式定义：<math display="block"><semantics><mrow><msub><mover accent="true"><mi>N</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> 和 </mtext></mrow><msub><mover accent="true"><mi>N</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>j</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover accent="true"><mi>N</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> 其中 </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{N}_{0} = 0 \hbox{ 和 } \tilde{N}_{j+1} = e^{K_{j} - q} V_{j} + e^{w + M_{j} - q} \tilde{N}_{j} \hbox{ 其中 } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>N~0​=0 和 N~j+1​=eKj​−qVj​+ew+Mj​−qN~j​ 其中 q=max(Kj​,w+Mj​)

根据以下递归公式定义：<math display="block"><semantics><mrow><msub><mover accent="true"><mi>D</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0</mn><mrow><mtext> 和 </mtext></mrow><msub><mover accent="true"><mi>D</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>K</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mover accent="true"><mi>D</mi><mo>~</mo></mover><mi>j</mi></msub><mrow><mtext> 其中 </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><mi>w</mi><mo>+</mo><msub><mi>M</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{D}_{0} = 0 \hbox{ 和 } \tilde{D}_{j+1} = e^{K_{j} - q} + e^{w + M_{j} - q} \tilde{D}_{j} \hbox{ 其中 } q = \max(K_{j}, w + M_{j})</annotation></semantics></math>D~0​=0 和 D~j+1​=eKj​−q+ew+Mj​−qD~j​ 其中 q=max(Kj​,w+Mj​)

和<math><semantics><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">M_{j+1} = q</annotation></semantics></math>Mj+1​=q。有了这些，我们可以计算<math display="block"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo>−</mo><mi>q</mi></mrow></msup><msub><mi>V</mi><mi>i</mi></sub><mo>+</mo><msup><mi>e</mi><msub><mi>M</mi><mi>i</mi></msub></msup><msub><mover accent="true"><mi>N</mi><mo>~</mo></mover><mi>i</mi></msub><mrow><mtext> 其中 </mtext></mrow><mi>q</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>u</mi><mo>+</mo><msub><mi>K</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N_{i} = e^{u + K_{i} - q} V_{i} + e^{M_{i}} \tilde{N}_{i} \hbox{ 其中 } q = \max(u + K_{i}, M_{i})</annotation></semantics></math>Ni​=eu+Ki​−qVi​+eMi​N~i​ 其中 q=max(u+Ki​,Mi​)

和 Di = e^(u + Ki - q) + e^Mi D~i，其中 q = max(u + Ki, Mi)

最终给出了 Oi = σ(Ri) Ni / Di
