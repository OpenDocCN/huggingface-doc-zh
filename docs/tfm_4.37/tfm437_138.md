# BERTweet

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/bertweet`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bertweet)

## 概述

BERTweet 模型是由 Dat Quoc Nguyen、Thanh Vu 和 Anh Tuan Nguyen 在[BERTweet: A pre-trained language model for English Tweets](https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf)中提出的。

从论文中摘录如下：

*我们提出了 BERTweet，这是第一个用于英文推文的公开大规模预训练语言模型。我们的 BERTweet 与 BERT-base（Devlin 等人，2019）具有相同的架构，使用 RoBERTa 预训练过程进行训练（Liu 等人，2019）。实验表明，BERTweet 优于强基线 RoBERTa-base 和 XLM-R-base（Conneau 等人，2020），在三个推文 NLP 任务（词性标注、命名实体识别和文本分类）上产生比先前最先进模型更好的性能结果。*

此模型由[dqnguyen](https://huggingface.co/dqnguyen)贡献。原始代码可在[此处](https://github.com/VinAIResearch/BERTweet)找到。

## 用法示例

```py
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> bertweet = AutoModel.from_pretrained("vinai/bertweet-base")

>>> # For transformers v4.x+:
>>> tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

>>> # For transformers v3.x:
>>> # tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

>>> # INPUT TWEET IS ALREADY NORMALIZED!
>>> line = "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"

>>> input_ids = torch.tensor([tokenizer.encode(line)])

>>> with torch.no_grad():
...     features = bertweet(input_ids)  # Models outputs are now tuples

>>> # With TensorFlow 2.0+:
>>> # from transformers import TFAutoModel
>>> # bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")
```

此实现与 BERT 相同，仅在分词方法上有所不同。有关 API 参考信息，请参考 BERT 文档。

## BertweetTokenizer

### `class transformers.BertweetTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L68)

```py
( vocab_file merges_file normalization = False bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。

+   `merges_file` (`str`) — 合并文件的路径。

+   `normalization` (`bool`, *optional*, defaults to `False`) — 是否应用规范化预处理。

+   `bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开始标记。可用作序列分类器标记。

    在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于进行序列分类时使用的分类器标记（对整个序列进行分类而不是每个标记进行分类）。在使用特殊标记构建时，它是序列的第一个标记。

+   `unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

构建一个使用字节对编码的 BERTweet 分词器。

此分词器继承自 PreTrainedTokenizer，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `add_from_file`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L418)

```py
( f )
```

从文本文件加载预先存在的字典，并将其符号添加到此实例。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L183)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— 要添加特殊标记的 ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

返回

`List[int]`

带有适当特殊标记的 input IDs 列表。

从序列或序列对构建用于序列分类任务的模型输入，通过连接和添加特殊标记。BERTweet 序列的格式如下：

+   单个序列：`<s> X </s>`

+   序列对：`<s> A </s></s> B </s>`

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L384)

```py
( tokens )
```

将一系列标记（字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L237)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

返回

`List[int]`

零列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。BERTweet 不使用标记类型 ID，因此返回一个零列表。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L209)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经使用特殊标记格式化。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 ID。在使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。

#### `normalizeToken`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L357)

```py
( token )
```

规范化推文中的标记

#### `normalizeTweet`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bertweet/tokenization_bertweet.py#L323)

```py
( tweet )
```

规范化原始推文
