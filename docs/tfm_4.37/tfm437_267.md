# EfficientFormer

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/efficientformer`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/efficientformer)

## 概述

EfficientFormer 模型是由 Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren 在[EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)中提出的。EfficientFormer 提出了一个维度一致的纯 Transformer，可以在移动设备上运行，用于像图像分类、目标检测和语义分割这样的密集预测任务。

论文摘要如下：

*Vision Transformers（ViT）在计算机视觉任务中取得了快速进展，在各种基准测试中取得了令人满意的结果。然而，由于参数数量庞大和模型设计（如注意力机制）等原因，基于 ViT 的模型通常比轻量级卷积网络慢。因此，将 ViT 部署到实时应用中尤为具有挑战性，特别是在资源受限的硬件上，如移动设备。最近的努力通过网络架构搜索或与 MobileNet 块混合设计来减少 ViT 的计算复杂性，但推理速度仍然不尽人意。这带来了一个重要问题：可以让 transformers 像 MobileNet 一样快速运行并获得高性能吗？为了回答这个问题，我们首先重新审视了 ViT-based 模型中使用的网络架构和运算符，并确定了低效的设计。然后，我们引入了一个维度一致的纯 Transformer（不包含 MobileNet 块）作为设计范式。最后，我们进行了基于延迟的精简，得到了一系列被称为 EfficientFormer 的最终模型。大量实验证明了 EfficientFormer 在移动设备上性能和速度方面的优越性。我们最快的模型 EfficientFormer-L1，在 iPhone 12 上（使用 CoreML 编译），仅需 1.6 毫秒的推理延迟就能实现 ImageNet-1K 的 79.2% top-1 准确率，这与 MobileNetV2×1.4（1.6 毫秒，74.7% top-1）一样快，而我们最大的模型 EfficientFormer-L7，在仅 7.0 毫秒的延迟下获得了 83.3%的准确率。我们的工作证明了经过合理设计的 transformers 可以在移动设备上达到极低的延迟，同时保持高性能。*

这个模型是由[novice03](https://huggingface.co/novice03)和[Bearnardd](https://huggingface.co/Bearnardd)贡献的。原始代码可以在[这里](https://github.com/snap-research/EfficientFormer)找到。这个模型的 TensorFlow 版本是由[D-Roberts](https://huggingface.co/D-Roberts)添加的。

## 文档资源

+   图像分类任务指南

## EfficientFormerConfig

### `class transformers.EfficientFormerConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/configuration_efficientformer.py#L32)

```py
( depths: List = [3, 2, 6, 4] hidden_sizes: List = [48, 96, 224, 448] downsamples: List = [True, True, True, True] dim: int = 448 key_dim: int = 32 attention_ratio: int = 4 resolution: int = 7 num_hidden_layers: int = 5 num_attention_heads: int = 8 mlp_expansion_ratio: int = 4 hidden_dropout_prob: float = 0.0 patch_size: int = 16 num_channels: int = 3 pool_size: int = 3 downsample_patch_size: int = 3 downsample_stride: int = 2 downsample_pad: int = 1 drop_path_rate: float = 0.0 num_meta3d_blocks: int = 1 distillation: bool = True use_layer_scale: bool = True layer_scale_init_value: float = 1e-05 hidden_act: str = 'gelu' initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 image_size: int = 224 batch_norm_eps: float = 1e-05 **kwargs )
```

参数

+   `depths` (`List(int)`, *可选*, 默认为`[3, 2, 6, 4]`) — 每个阶段的深度。

+   `hidden_sizes` (`List(int)`, *可选*, 默认为`[48, 96, 224, 448]`) — 每个阶段的维度。

+   `downsamples` (`List(bool)`, *可选*, 默认为`[True, True, True, True]`) — 是否在两个阶段之间对输入进行下采样。

+   `dim` (`int`, *可选*, 默认为 448) — Meta3D 层中的通道数量

+   `key_dim` (`int`, *可选*, 默认为 32) — meta3D 块中键的大小。

+   `attention_ratio` (`int`, *可选*, 默认为 4) — MSHA 块中查询和值的维度与键的维度之比

+   `resolution` (`int`, *可选*, 默认为 7) — 每个 patch 的大小

+   `num_hidden_layers` (`int`, *可选*, 默认为 5) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *可选*, 默认为 8) — 3D MetaBlock 中每个注意力层的注意力头数量。

+   `mlp_expansion_ratio` (`int`，*可选*，默认为 4) — MLP 隐藏维度大小与其输入维度大小的比率。

+   `hidden_dropout_prob` (`float`，*可选*，默认为 0.1) — 嵌入和编码器中所有全连接层的丢弃概率。

+   `patch_size` (`int`，*可选*，默认为 16) — 每个补丁的大小（分辨率）。

+   `num_channels` (`int`，*可选*，默认为 3) — 输入通道的数量。

+   `pool_size` (`int`，*可选*，默认为 3) — 池化层的核大小。

+   `downsample_patch_size` (`int`，*可选*，默认为 3) — 下采样层中补丁的大小。

+   `downsample_stride` (`int`，*可选*，默认为 2) — 下采样层中卷积核的步幅。

+   `downsample_pad` (`int`，*可选*，默认为 1) — 下采样层中的填充。

+   `drop_path_rate` (`int`，*可选*，默认为 0) — 在 DropPath 中增加丢失概率的速率。

+   `num_meta3d_blocks` (`int`，*可选*，默认为 1) — 最后阶段中的 3D MetaBlocks 的数量。

+   `distillation` (`bool`，*可选*，默认为 `True`) — 是否添加蒸馏头。

+   `use_layer_scale` (`bool`，*可选*，默认为 `True`) — 是否对标记混合器的输出进行缩放。

+   `layer_scale_init_value` (`float`，*可选*，默认为 1e-5) — 从标记混合器输出进行缩放的因子。

+   `hidden_act` (`str` 或 `function`，*可选*，默认为 `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"selu"` 和 `"gelu_new"`。

+   `initializer_range` (`float`，*可选*，默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`，*可选*，默认为 1e-12) — 层归一化层使用的 epsilon。

+   `image_size` (`int`，*可选*，默认为 `224`) — 每个图像的大小（分辨率）。

这是一个配置类，用于存储 EfficientFormerModel 的配置。根据指定的参数实例化 EfficientFormer 模型，定义模型架构。使用默认值实例化配置将产生类似于 EfficientFormer [snap-research/efficientformer-l1](https://huggingface.co/snap-research/efficientformer-l1) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import EfficientFormerConfig, EfficientFormerModel

>>> # Initializing a EfficientFormer efficientformer-l1 style configuration
>>> configuration = EfficientFormerConfig()

>>> # Initializing a EfficientFormerModel (with random weights) from the efficientformer-l3 style configuration
>>> model = EfficientFormerModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## EfficientFormerImageProcessor

### `class transformers.EfficientFormerImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/image_processing_efficientformer.py#L45)

```py
( do_resize: bool = True size: Optional = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 crop_size: Dict = None do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )
```

参数

+   `do_resize` (`bool`，*可选*，默认为 `True`) — 是否将图像的（高度、宽度）维度调整为指定的 `(size["height"], size["width"])`。可以被 `preprocess` 方法中的 `do_resize` 参数覆盖。

+   `size` (`dict`，*可选*，默认为 `{"height" -- 224, "width": 224}`)：调整大小后的输出图像大小。可以被 `preprocess` 方法中的 `size` 参数覆盖。

+   `resample` (`PILImageResampling`，*可选*，默认为 `PILImageResampling.BILINEAR`) — 调整图像大小时要使用的重采样滤波器。可以被 `preprocess` 方法中的 `resample` 参数覆盖。

+   `do_center_crop` (`bool`，*可选*，默认为 `True`) — 是否将图像居中裁剪到指定的 `crop_size`。可以被 `preprocess` 方法中的 `do_center_crop` 覆盖。

+   `crop_size` (`Dict[str, int]` *可选*，默认为 224) — 应用`center_crop`后输出图像的大小。可以被`preprocess`方法中的`crop_size`覆盖。

+   `do_rescale` (`bool`，*可选*，默认为`True`) — 是否按指定比例`rescale_factor`重新缩放图像。可以被`preprocess`方法中的`do_rescale`参数覆盖。

+   `rescale_factor` (`int`或`float`，*可选*，默认为`1/255`) — 如果重新缩放图像，则使用的比例因子。可以被`preprocess`方法中的`rescale_factor`参数覆盖。do_normalize — 是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。

+   `image_mean` (`float`或`List[float]`，*可选*，默认为`IMAGENET_STANDARD_MEAN`) — 如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float`或`List[float]`，*可选*，默认为`IMAGENET_STANDARD_STD`) — 如果`do_normalize`设置为`True`，则使用的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_std`参数覆盖。

构建一个 EfficientFormer 图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/image_processing_efficientformer.py#L160)

```py
( images: Union do_resize: Optional = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: int = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置`do_rescale=False`。

+   `do_resize` (`bool`，*可选*，默认为`self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`，*可选*，默认为`self.size`) — 以`{"height": h, "width": w}`格式指定调整大小后输出图像的大小的字典。

+   `resample` (`PILImageResampling`过滤器，*可选*，默认为`self.resample`) — 调整图像大小时要使用的`PILImageResampling`过滤器，例如`PILImageResampling.BILINEAR`。仅在`do_resize`设置为`True`时有效。

+   `do_center_crop` (`bool`，*可选*，默认为`self.do_center_crop`) — 是否对图像进行中心裁剪。

+   `do_rescale` (`bool`，*可选*，默认为`self.do_rescale`) — 是否将图像值重新缩放在[0 - 1]之间。

+   `rescale_factor` (`float`，*可选*，默认为`self.rescale_factor`) — 如果`do_rescale`设置为`True`，则重新缩放图像的缩放因子。

+   `crop_size` (`Dict[str, int]`，*可选*，默认为`self.crop_size`) — 中心裁剪的大小。仅在`do_center_crop`设置为`True`时有效。

+   `do_normalize` (`bool`，*可选*，默认为`self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float`或`List[float]`，*可选*，默认为`self.image_mean`) — 如果`do_normalize`设置为`True`，则使用的图像均值。

+   `image_std` (`float`或`List[float]`，*可选*，默认为`self.image_std`) — 如果`do_normalize`设置为`True`，则使用的图像标准差。

+   `return_tensors` (`str`或`TensorType`，*可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个`np.ndarray`列表。

    +   `TensorType.TENSORFLOW`或`'tf'`：返回类型为`tf.Tensor`的批处理。

    +   `TensorType.PYTORCH`或`'pt'`：返回类型为`torch.Tensor`的批处理。

    +   `TensorType.NUMPY`或`'np'`：返回类型为`np.ndarray`的批处理。

    +   `TensorType.JAX`或`'jax'`：返回类型为`jax.numpy.ndarray`的批处理。

+   `data_format` (`ChannelDimension`或`str`，*可选*，默认为`ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format`（`ChannelDimension`或`str`，*可选*） — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   `"none"`或`ChannelDimension.NONE`：图像以（高度，宽度）格式。

预处理一张图像或一批图像。

PytorchHide Pytorch 内容

## EfficientFormerModel

### `class transformers.EfficientFormerModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L553)

```py
( config: EfficientFormerConfig )
```

参数

+   `config`（EfficientFormerConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

EfficientFormer 模型是一个裸的 transformer 模型，输出原始的隐藏状态，没有特定的头部。这个模型是 PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L569)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`） — 像素值。可以使用 ViTImageProcessor 获取像素值。有关详细信息，请参阅 ViTImageProcessor.preprocess()。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*） — 是否返回一个 ModelOutput 而不是一个普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（EfficientFormerConfig）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`） — 模型最后一层的输出的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`） — 序列的最后一层隐藏状态的第一个标记（分类标记）经过用于辅助预训练任务的层进一步处理后的隐藏状态。例如，对于 BERT 系列模型，这返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重是从预训练期间的下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    在自注意力头中用于计算加权平均值的注意力 softmax 之后的注意力权重。

EfficientFormerModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例:

```py
>>> from transformers import AutoImageProcessor, EfficientFormerModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("snap-research/efficientformer-l1-300")
>>> model = EfficientFormerModel.from_pretrained("snap-research/efficientformer-l1-300")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 49, 448]
```

## 高效的图像分类器

### `class transformers.EfficientFormerForImageClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L612)

```py
( config: EfficientFormerConfig )
```

参数

+   `config` (EfficientFormerConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

高效的图像分类器模型变换器，顶部带有一个图像分类头（在[CLS]标记的最终隐藏状态之上的线性层），例如用于 ImageNet。

这个模型是 PyTorch 的[nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L634)

```py
( pixel_values: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用 ViTImageProcessor 获取。查看 ViTImageProcessor.preprocess()获取详细信息。

+   `output_attentions` (`bool`, *可选的*) — 是否返回所有注意力层的注意力张量。查看返回的张量中的`attentions`以获取更多细节。

+   `output_hidden_states` (`bool`, *可选的*) — 是否返回所有层的隐藏状态。查看返回的张量中的`hidden_states`以获取更多细节。

+   `return_dict` (`bool`, *可选的*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选的*) — 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

transformers.modeling_outputs.ImageClassifierOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.ImageClassifierOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置(EfficientFormerConfig)和输入的各种元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果 config.num_labels==1 则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果 config.num_labels==1 则为回归）得分（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

EfficientFormerForImageClassification 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, EfficientFormerForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("snap-research/efficientformer-l1-300")
>>> model = EfficientFormerForImageClassification.from_pretrained("snap-research/efficientformer-l1-300")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
Egyptian cat
```

## EfficientFormerForImageClassificationWithTeacher

### `class transformers.EfficientFormerForImageClassificationWithTeacher`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L734)

```py
( config: EfficientFormerConfig )
```

参数

+   `config` (EfficientFormerConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

EfficientFormer 模型变压器，顶部带有图像分类头（在[CLS]令牌的最终隐藏状态上方有一个线性层，以及在蒸馏令牌的最终隐藏状态上方有一个线性层），例如用于 ImageNet。

此模型仅支持推断。目前不支持使用蒸馏（即使用教师）进行微调。

这个模型是 PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L766)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.efficientformer.modeling_efficientformer.EfficientFormerForImageClassificationWithTeacherOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用 ViTImageProcessor 获取。有关详细信息，请参阅 ViTImageProcessor.preprocess()。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.efficientformer.modeling_efficientformer.EfficientFormerForImageClassificationWithTeacherOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.efficientformer.modeling_efficientformer.EfficientFormerForImageClassificationWithTeacherOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（EfficientFormerConfig）和输入的各种元素。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) — 预测分数，作为 cls_logits 和蒸馏 logits 的平均值。

+   `cls_logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) — 分类头的预测分数（即类令牌的最终隐藏状态之上的线性层）。

+   `distillation_logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) — 蒸馏头部的预测分数（即蒸馏令牌的最终隐藏状态之上的线性层）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每个层的输出隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

EfficientFormerForImageClassificationWithTeacher 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, EfficientFormerForImageClassificationWithTeacher
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("snap-research/efficientformer-l1-300")
>>> model = EfficientFormerForImageClassificationWithTeacher.from_pretrained("snap-research/efficientformer-l1-300")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
Egyptian cat
```

TensorFlow 隐藏 TensorFlow 内容

## TFEfficientFormerModel

### `class transformers.TFEfficientFormerModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L943)

```py
( config: EfficientFormerConfig **kwargs )
```

参数

+   `config` (EfficientFormerConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 EfficientFormer 模型变压器输出原始隐藏状态，没有特定的头部。此模型是一个 TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)。将其用作常规 TensorFlow 模块，并参考 TensorFlow 文档以获取与一般用法和行为相关的所有事项。

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L953)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)
```

参数

+   `pixel_values`（形状为 `(batch_size, num_channels, height, width)` 的 `tf.Tensor`） — 像素值。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 EfficientFormerImageProcessor.`call`()。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回一个 ModelOutput 而不是一个普通元组。

返回

transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling 或 `tuple(tf.Tensor)`

一个 transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling 或一个 `tf.Tensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False` 时）包含根据配置（EfficientFormerConfig）和输入的各种元素。

+   `last_hidden_state` (`tf.Tensor`，形状为 `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`tf.Tensor`，形状为 `(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和 Tanh 激活函数进一步处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

    这个输出通常*不*是输入语义内容的良好摘要，通常最好对整个输入序列的隐藏状态序列进行平均或池化。

+   `hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入输出，一个用于每一层的输出）。

    模型在每一层输出处的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`，*optional*，当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

TFEfficientFormerModel 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFEfficientFormerModel
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("snap-research/efficientformer-l1-300")
>>> model = TFEfficientFormerModel.from_pretrained("snap-research/efficientformer-l1-300")

>>> inputs = image_processor(image, return_tensors="tf")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 49, 448]
```

## TFEfficientFormerForImageClassification

### `class transformers.TFEfficientFormerForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L988)

```py
( config: EfficientFormerConfig )
```

参数

+   `config`（EfficientFormerConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

在池化的最后隐藏状态之上具有图像分类头的 EfficientFormer 模型变压器，例如用于 ImageNet。

此模型是一个 TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)。将其用作常规的 TensorFlow 模块，并参考 TensorFlow 文档以获取与一般用法和行为相关的所有内容。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L1010)

```py
( pixel_values: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFImageClassifierOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`） — 像素值。像素值可以使用 AutoImageProcessor 获取。有关详细信息，请参阅 EfficientFormerImageProcessor.`call`()。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

+   `labels` (`tf.Tensor`，形状为`(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.modeling_tf_outputs.TFImageClassifierOutput`或`tuple(tf.Tensor)`

一个`transformers.modeling_tf_outputs.TFImageClassifierOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（EfficientFormerConfig）和输入的各种元素。

+   `loss` (`tf.Tensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（或回归，如果`config.num_labels==1`）损失。

+   `logits` (`tf.Tensor`，形状为`(batch_size, config.num_labels)`) — 分类（或回归，如果`config.num_labels==1`）得分（SoftMax 之前）。

+   `hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（如果模型具有嵌入层，则为嵌入的输出 + 每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

TFEfficientFormerForImageClassification 前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFEfficientFormerForImageClassification
>>> import tensorflow as tf
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("snap-research/efficientformer-l1-300")
>>> model = TFEfficientFormerForImageClassification.from_pretrained("snap-research/efficientformer-l1-300")

>>> inputs = image_processor(image, return_tensors="tf")
>>> logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = int(tf.math.argmax(logits, axis=-1))
>>> print(model.config.id2label[predicted_label])
LABEL_281
```

## TFEfficientFormerForImageClassificationWithTeacher

### `class transformers.TFEfficientFormerForImageClassificationWithTeacher`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L1102)

```py
( config: EfficientFormerConfig )
```

参数

+   `config`（EfficientFormerConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

EfficientFormer 模型转换器，顶部带有图像分类头（位于最终隐藏状态的顶部的线性层和位于蒸馏令牌的最终隐藏状态的顶部的线性层），例如用于 ImageNet。

.. 警告:: 此模型仅支持推断。尚不支持使用蒸馏进行微调（即使用教师）。

此模型是一个 TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)。将其用作常规的 TensorFlow 模块，并参考 TensorFlow 文档以获取与一般用法和行为相关的所有事项。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L1132)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None training: bool = False ) → export const metadata = 'undefined';transformers.models.efficientformer.modeling_tf_efficientformer.TFEfficientFormerForImageClassificationWithTeacherOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`） — 像素值。像素值可以使用 AutoImageProcessor 获取。有关详细信息，请参阅 EfficientFormerImageProcessor.`call`()。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.efficientformer.modeling_tf_efficientformer.TFEfficientFormerForImageClassificationWithTeacherOutput`或`tuple(tf.Tensor)`

一个`transformers.models.efficientformer.modeling_tf_efficientformer.TFEfficientFormerForImageClassificationWithTeacherOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（EfficientFormerConfig）和输入的各种元素。

TFEfficientFormerForImageClassificationWithTeacher 前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

+   EfficientFormerForImageClassificationWithTeacher 的`Output`类型。logits（形状为`(batch_size, config.num_labels)`的`tf.Tensor`）- 预测分数，作为 cls_logits 和蒸馏 logits 的平均值。cls_logits（形状为`(batch_size, config.num_labels)`的`tf.Tensor`）- 分类头部的预测分数（即类令牌最终隐藏状态顶部的线性层）。distillation_logits（形状为`(batch_size, config.num_labels)`的`tf.Tensor`）- 蒸馏头部的预测分数（即蒸馏令牌最终隐藏状态顶部的线性层）。hidden_states（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每个层的输出状态加上初始嵌入输出。attentions（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

示例：

```py
>>> from transformers import AutoImageProcessor, TFEfficientFormerForImageClassificationWithTeacher
>>> import tensorflow as tf
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("snap-research/efficientformer-l1-300")
>>> model = TFEfficientFormerForImageClassificationWithTeacher.from_pretrained("snap-research/efficientformer-l1-300")

>>> inputs = image_processor(image, return_tensors="tf")
>>> logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = int(tf.math.argmax(logits, axis=-1))
>>> print(model.config.id2label[predicted_label])
LABEL_281
```
