# FocalNet

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/focalnet`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/focalnet)

## 概述

FocalNet 模型是由 Jianwei Yang、Chunyuan Li、Xiyang Dai、Lu Yuan、Jianfeng Gao 在[焦点调制网络](https://arxiv.org/abs/2203.11926)中提出的。FocalNets 完全用焦点调制机制取代了自注意力（在模型中使用，如 ViT 和 Swin），用于建模视觉中的令牌交互。作者声称，FocalNets 在图像分类、目标检测和分割任务上优于基于自注意力的模型，且具有类似的计算成本。

该论文的摘要如下：

*我们提出了焦点调制网络（简称 FocalNets），其中自注意力（SA）完全被焦点调制机制取代，用于建模视觉中的令牌交互。焦点调制包括三个组件：（i）分层上下文化，使用一堆深度卷积层实现，以编码从短到长范围的视觉上下文，（ii）门控聚合，根据其内容选择性地收集每个查询令牌的上下文，以及（iii）逐元调制或仿射变换，将聚合的上下文注入查询中。大量实验证明，FocalNets 在图像分类、目标检测和分割任务上优于最先进的自注意力对应物（例如 Swin 和 Focal Transformers），并且具有类似的计算成本。具体而言，尺寸微小和基础尺寸的 FocalNets 在 ImageNet-1K 上分别实现了 82.3%和 83.9%的 top-1 准确率。在 224 分辨率上在 ImageNet-22K 上预训练后，当分别使用 224 和 384 分辨率进行微调时，它分别达到了 86.5%和 87.3%的 top-1 准确率。转移到下游任务时，FocalNets 表现出明显的优势。对于使用 Mask R-CNN 进行目标检测，基础训练的 FocalNet 优于 Swin 对应物 2.1 个点，并且已经超过了使用 3 倍计划训练的 Swin（49.0 对 48.5）。对于使用 UPerNet 进行语义分割，单尺度的 FocalNet 优于 Swin 2.4 个点，并且在多尺度上击败了 Swin（50.5 对 49.7）。使用大型 FocalNet 和 Mask2former，我们在 ADE20K 语义分割上实现了 58.5 的 mIoU，以及在 COCO Panoptic 分割上实现了 57.9 的 PQ。使用巨大的 FocalNet 和 DINO，我们在 COCO minival 和 test-dev 上分别实现了 64.3 和 64.4 的 mAP，建立了新的 SoTA，超越了像 Swinv2-G 和 BEIT-3 这样的更大的基于注意力的模型。*

该模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/microsoft/FocalNet)找到。

## FocalNetConfig

### `class transformers.FocalNetConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/configuration_focalnet.py#L29)

```py
( image_size = 224 patch_size = 4 num_channels = 3 embed_dim = 96 use_conv_embed = False hidden_sizes = [192, 384, 768, 768] depths = [2, 2, 6, 2] focal_levels = [2, 2, 2, 2] focal_windows = [3, 3, 3, 3] hidden_act = 'gelu' mlp_ratio = 4.0 hidden_dropout_prob = 0.0 drop_path_rate = 0.1 use_layerscale = False layerscale_value = 0.0001 use_post_layernorm = False use_post_layernorm_in_modulation = False normalize_modulator = False initializer_range = 0.02 layer_norm_eps = 1e-05 encoder_stride = 32 out_features = None out_indices = None **kwargs )
```

参数

+   图像大小（`int`，*可选*，默认为 224）— 每个图像的大小（分辨率）。

+   `patch_size`（`int`，*可选*，默认为 4）— 嵌入层中每个补丁的大小（分辨率）。

+   `num_channels`（`int`，*可选*，默认为 3）— 输入通道数。

+   `embed_dim`（`int`，*可选*，默认为 96）— 补丁嵌入的维度。

+   `use_conv_embed`（`bool`，*可选*，默认为`False`）— 是否使用卷积嵌入。作者指出，使用卷积嵌入通常会提高性能，但默认情况下不使用。

+   `hidden_sizes`（`List[int]`，*可选*，默认为`[192, 384, 768, 768]`）— 每个阶段的维度（隐藏大小）。

+   `depths`（`list(int)`，*可选*，默认为`[2, 2, 6, 2]`）— 编码器中每个阶段的深度（层数）。

+   `focal_levels`（`list(int)`，*可选*，默认为`[2, 2, 2, 2]`）— 编码器中各阶段的每层中的焦点级别数。

+   `focal_windows` (`list(int)`, *可选*, 默认为`[3, 3, 3, 3]`) — 编码器中各阶段的各层中的焦点窗口大小。

+   `hidden_act` (`str`或`function`, *可选*, 默认为`"gelu"`) — 编码器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `mlp_ratio` (`float`, *可选*, 默认为 4.0) — MLP 隐藏维度与嵌入维度的比率。

+   `hidden_dropout_prob` (`float`, *可选*, 默认为 0.0) — 嵌入和编码器中所有全连接层的丢失概率。

+   `drop_path_rate` (`float`, *可选*, 默认为 0.1) — 随机深度率。

+   `use_layerscale` (`bool`, *可选*, 默认为`False`) — 是否在编码器中使用层比例。

+   `layerscale_value` (`float`, *可选*, 默认为 0.0001) — 层比例的初始值。

+   `use_post_layernorm` (`bool`, *可选*, 默认为`False`) — 是否在编码器中使用后层归一化。

+   `use_post_layernorm_in_modulation` (`bool`, *可选*, 默认为`False`) — 是否在调制层中使用后层归一化。

+   `normalize_modulator` (`bool`, *可选*, 默认为`False`) — 是否对调制器进行归一化。

+   `initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *可选*, 默认为 1e-05) — 层归一化层使用的 epsilon。

+   `encoder_stride` (`int`, *可选*, 默认为 32) — 用于掩蔽图像建模中解码器头部增加空间分辨率的因子。

+   `out_features` (`List[str]`, *可选*) — 如果用作骨干，要输出的特征列表。可以是任何一个`"stem"`、`"stage1"`、`"stage2"`等（取决于模型有多少阶段）。如果未设置且设置了`out_indices`，将默认为相应的阶段。如果未设置且`out_indices`未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。

+   `out_indices` (`List[int]`, *可选*) — 如果用作骨干，要输出的特征的索引列表。可以是 0、1、2 等（取决于模型有多少阶段）。如果未设置且设置了`out_features`，将默认为相应的阶段。如果未设置且`out_features`未设置，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。

这是用于存储 FocalNetModel 配置的配置类。它用于根据指定的参数实例化一个 FocalNet 模型，定义模型架构。使用默认值实例化配置将产生类似于[FocalNet](https://huggingface.co/microsoft/focalnet-tiny)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import FocalNetConfig, FocalNetModel

>>> # Initializing a FocalNet microsoft/focalnet-tiny style configuration
>>> configuration = FocalNetConfig()

>>> # Initializing a model (with random weights) from the microsoft/focalnet-tiny style configuration
>>> model = FocalNetModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FocalNetModel

### `class transformers.FocalNetModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L681)

```py
( config add_pooling_layer = True use_mask_token = False )
```

参数

+   `config` (FocalNetConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 FocalNet 模型输出原始隐藏状态，没有特定的头部。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L704)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用 AutoImageProcessor 获取。查看`AutoImageProcessor.__call__()`获取详细信息。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。查看返回张量中的`hidden_states`获取更多细节。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

+   `bool_masked_pos` (`torch.BoolTensor`，形状为`(batch_size, num_patches)`) — 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。

返回

`transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput`或一个`torch.FloatTensor`元组（如果传入`return_dict=False`或者`config.return_dict=False`时）包含根据配置（FocalNetConfig）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`，*可选*，当传入`add_pooling_layer=True`时返回) — 最后一层隐藏状态的平均池化。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传入`output_hidden_states=True`或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每个阶段的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `reshaped_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传入`output_hidden_states=True`或者`config.output_hidden_states=True`时返回） — 形状为`(batch_size, hidden_size, height, width)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每个阶段的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出重塑以包含空间维度。

FocalNetModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, FocalNetModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/focalnet-tiny")
>>> model = FocalNetModel.from_pretrained("microsoft/focalnet-tiny")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 49, 768]
```

## FocalNetForMaskedImageModeling

### `class transformers.FocalNetForMaskedImageModeling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L761)

```py
( config )
```

参数

+   `config`（FocalNetConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

FocalNet 模型，顶部带有解码器，用于对遮蔽图像进行建模。

这遵循与[SimMIM](https://arxiv.org/abs/2111.09886)中相同的实现。

请注意，我们在我们的[示例目录](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining)中提供了一个脚本，用于在自定义数据上预训练此模型。

这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L793)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用 AutoImageProcessor 获取。有关详细信息，请参阅`AutoImageProcessor.__call__()`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 ModelOutput 而不是一个普通元组。

+   `bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`) — 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。

返回值

`transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含各种元素，具体取决于配置（FocalNetConfig）和输入。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos` is provided) — 遮蔽图像建模（MLM）损失。

+   `reconstruction` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 重建的像素值。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.

    模型在每一层的输出处的隐藏状态以及初始嵌入输出。

+   `reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of shape `(batch_size, hidden_size, height, width)`。

    模型在每一层的输出处的隐藏状态以及重塑以包含空间维度的初始嵌入输出。

FocalNetForMaskedImageModeling 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, FocalNetConfig, FocalNetForMaskedImageModeling
>>> import torch
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/focalnet-base-simmim-window6-192")
>>> config = FocalNetConfig()
>>> model = FocalNetForMaskedImageModeling(config)

>>> num_patches = (model.config.image_size // model.config.patch_size) ** 2
>>> pixel_values = image_processor(images=image, return_tensors="pt").pixel_values
>>> # create random boolean mask of shape (batch_size, num_patches)
>>> bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()

>>> outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
>>> loss, reconstructed_pixel_values = outputs.loss, outputs.logits
>>> list(reconstructed_pixel_values.shape)
[1, 3, 192, 192]
```

## FocalNetForImageClassification

### `class transformers.FocalNetForImageClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L876)

```py
( config )
```

参数

+   `config`（FocalNetConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained() 方法以加载模型权重。

在顶部带有图像分类头部的 FocalNet 模型（在池化输出之上的线性层），例如用于 ImageNet。

此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/focalnet/modeling_focalnet.py#L899)

```py
( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 `AutoImageProcessor.__call__()`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 ModelOutput 而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput` 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，取决于配置（FocalNetConfig）和输入。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 分类（或回归，如果 `config.num_labels==1`）损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, config.num_labels)`) — 分类（或回归，如果 `config.num_labels==1`）分数（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。

    模型在每个层的输出以及初始嵌入输出的隐藏状态。

+   `reshaped_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, hidden_size, height, width)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出，重塑以包括空间维度。

FocalNetForImageClassification 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, FocalNetForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/focalnet-tiny")
>>> model = FocalNetForImageClassification.from_pretrained("microsoft/focalnet-tiny")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```
