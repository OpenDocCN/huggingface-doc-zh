# CLVP

> 原始文本: [`huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp)

## 概述

CLVP（对比语言-声音预训练变压器）模型由 James Betker 在[通过缩放实现更好的语音合成](https://arxiv.org/abs/2305.07243)中提出。

论文摘要如下：

*近年来，图像生成领域已经通过自回归变压器和 DDPMs 的应用而发生了革命。这些方法将图像生成过程建模为逐步的概率过程，并利用大量计算和数据来学习图像分布。提高性能的这种方法不一定局限于图像。本文描述了一种将图像生成领域的进展应用于语音合成的方法。结果是 TorToise - 一种富有表现力的、多声音的文本到语音系统。*

该模型由[Susnato Dhar](https://huggingface.co/susnato)贡献。原始代码可在[此处](https://github.com/neonbjb/tortoise-tts)找到。

## 使用提示

1.  CLVP 是 Tortoise TTS 模型的一个重要部分。

1.  CLVP 可用于将不同生成的语音候选与提供的文本进行比较，并将最佳语音标记转发到扩散模型。

1.  强烈建议使用`ClvpModelForConditionalGeneration.generate()`方法进行龟速使用。

1.  请注意，CLVP 模型期望音频采样率为 22.05 kHz，而其他音频模型期望为 16 kHz。

## 简要说明：

+   ClvpTokenizer 对文本输入进行标记化处理，而 ClvpFeatureExtractor 从所需音频中提取对数梅尔频谱图。

+   `ClvpConditioningEncoder` 获取这些文本标记和音频表示，并将它们转换为在文本和音频上进行条件化的嵌入。

+   ClvpForCausalLM 使用这些嵌入来生成多个语音候选。

+   每个语音候选通过语音编码器（ClvpEncoder）传递，将它们转换为矢量表示，文本编码器（ClvpEncoder）将文本标记转换为相同的潜在空间。

+   最后，我们将每个语音向量与文本向量进行比较，以查看哪个语音向量与文本向量最相似。

+   `ClvpModelForConditionalGeneration.generate()` 将上述所有逻辑压缩为一个方法。

示例：

```py
>>> import datasets
>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library).
>>> text = "This is an example text."

>>> ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.cast_column("audio", datasets.Audio(sampling_rate=22050))
>>> sample = ds[0]["audio"]

>>> # Define processor and model.
>>> processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
>>> model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

>>> # Generate processor output and model output.
>>> processor_output = processor(raw_speech=sample["array"], sampling_rate=sample["sampling_rate"], text=text, return_tensors="pt")
>>> generated_output = model.generate(**processor_output)
```

## ClvpConfig

### `class transformers.ClvpConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L341)

```py
( text_config = None speech_config = None decoder_config = None projection_dim = 768 logit_scale_init_value = 2.6592 initializer_factor = 1.0 **kwargs )
```

参数

+   `text_config` (`dict`, *可选*) — 用于初始化 CLVP 文本编码器的配置选项字典。

+   `speech_config` (`dict`, *可选*) — 用于初始化 CLVP 语音编码器的配置选项字典。

+   `decoder_config` (`dict`, *可选*) — 用于初始化 ClvpDecoderConfig 的配置选项字典。

+   `projection_dim` (`int`, *可选*, 默认为 768) — 文本和语音投影层的维度。

+   `logit_scale_init_value` (`float`, *可选*, 默认为 2.6592) — *logit_scale*参数的初始值。默认值根据原始 CLVP 实现使用。

+   `initializer_factor` (`float`, *可选*, 默认为 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1.0，用于内部初始化测试）。

+   `kwargs` (*可选*) — 关键字参数字典。

ClvpConfig 是用于存储 ClvpModelForConditionalGeneration 配置的类。它用于根据指定的参数实例化 CLVP 模型，定义文本模型、语音模型和解码器模型配置。使用默认值实例化配置将产生类似于 CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import ClvpConfig, ClvpModelForConditionalGeneration

>>> # Initializing a ClvpConfig with susnato/clvp_dev style configuration
>>> configuration = ClvpConfig()

>>> # Initializing a ClvpModelForConditionalGeneration (with random weights) from the susnato/clvp_dev style configuration
>>> model = ClvpModelForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a CLVPConfig from a CLVPTextConfig, CLVPSpeechConfig and a CLVPAutoRegressiveConfig
>>> from transformers import ClvpEncoderConfig, ClvpDecoderConfig

>>> # Initializing a CLVP text, CLVP speech and CLVP decoder configuration
>>> config_text = ClvpEncoderConfig()
>>> config_speech = ClvpEncoderConfig()
>>> decoder_config = ClvpDecoderConfig()

>>> config = ClvpConfig.from_sub_model_configs(config_text, config_speech, decoder_config)
```

#### `from_sub_model_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L428)

```py
( text_config: ClvpEncoderConfig speech_config: ClvpEncoderConfig decoder_config: ClvpDecoderConfig **kwargs ) → export const metadata = 'undefined';ClvpConfig
```

参数

+   `text_config` (`ClvpEncoderConfig`) — 类型为 ClvpEncoderConfig 的文本模型配置。

+   `speech_config` (`ClvpEncoderConfig`) — 类型为 ClvpEncoderConfig 的语音模型配置。

+   `decoder_config` (`ClvpDecoderConfig`) — 类型为 ClvpDecoderConfig 的解码器模型配置。

返回

ClvpConfig

配置对象的一个实例

从 CLVP 文本模型配置、CLVP 语音模型配置和 CLVP 解码器模型配置实例化一个 ClvpConfig（或派生类）。

## ClvpEncoderConfig

### `class transformers.ClvpEncoderConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L36)

```py
( vocab_size = 256 hidden_size = 768 intermediate_size = 1536 projection_dim = 768 num_hidden_layers = 20 num_attention_heads = 12 hidden_act = 'gelu' layer_norm_eps = 1e-05 attention_dropout = 0.1 dropout = 0.1 use_rotary_embedding = True use_attention_bias = False summary_type = 'mean' initializer_factor = 1.0 bos_token_id = 255 eos_token_id = 0 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 256) — CLVP 编码器模型的词汇表大小。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 1536) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `projection_dim` (`int`, *optional*, defaults to 768) — 投影向量的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 20) — Transformer 编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"selu"`、`"gelu_new"` 和 `"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的 dropout 比率。

+   `dropout` (`float`, *optional*, defaults to 0.1) — `ClvpEncoderMLP` 中前馈层的 dropout 比率。

+   `use_rotary_embedding` (`bool`, *optional*, defaults to `True`) — 是否使用旋转嵌入。

+   `use_attention_bias` (`bool`, *optional*, defaults to `False`) — 在自注意力期间是否使用 Query、Key 和 Value 层中的偏置。

+   `summary_type` (`str`, *optional*, defaults to `"mean"`) — 从 last_hidden_state 获取 pooler_output 的策略。支持 `"last"`、`"first"`、`"mean"` 和 `"cls_index"`。

+   `initializer_factor` (`float`, *optional*, 默认为 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1.0，用于内部初始化测试）。

+   `bos_token_id` (`int`, *optional*, 默认为 255) — 序列开始标记 id。

+   `eos_token_id` (`int`, *optional*, 默认为 0) — 序列结束标记 id。

这是用于存储 ClvpEncoder 配置的配置类。根据指定的参数实例化一个 CLVP 文本或 CLVP 语音编码器。使用默认值实例化配置将产生与 CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)架构的编码器类似的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import ClvpEncoderConfig, ClvpEncoder

>>> # Initializing a ClvpEncoderConfig with susnato/clvp_dev style configuration
>>> encoder_configuration = ClvpEncoderConfig()

>>> # Initializing a ClvpEncoder (with random weights) from the susnato/clvp_dev style configuration
>>> model = ClvpEncoder(encoder_configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ClvpDecoderConfig

### `class transformers.ClvpDecoderConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L167)

```py
( vocab_size = 8194 max_position_embeddings = 608 max_text_tokens = 404 hidden_size = 1024 num_hidden_layers = 30 num_attention_heads = 16 n_inner = None num_mel_attn_blocks = 6 activation_function = 'gelu_new' resid_pdrop = 0.1 embd_pdrop = 0.1 attention_dropout = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 summary_type = 'cls_index' summary_use_proj = True summary_activation = None summary_proj_to_labels = True summary_first_dropout = 0.1 use_cache = True bos_token_id = 8192 eos_token_id = 8193 feature_size = 80 use_attention_bias = True initializer_factor = 1.0 decoder_fixing_codes = [83, 45, 45, 248] **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 8194) — 模型的词汇表大小。

+   `max_position_embeddings` (`int`, *optional*, 默认为 608) — 此模型可能用于的最大 mel 标记序列长度。类似于`GPT2Config`中的`n_positions`。

+   `max_text_tokens` (`int`, *optional*, 默认为 404) — 此模型可能用于的文本标记的最大序列长度。类似于`GPT2Config`中的`n_positions`。

+   `hidden_size` (`int`, *optional*, 默认为 1024) — 嵌入和隐藏状态的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为 30) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 16) — Transformer 编码器中每个注意力层的注意力头数。

+   `n_inner` (`int`, *optional*) — 内部前馈层的维度。`None`将将其设置为`hidden_size`的 4 倍。

+   `num_mel_attn_blocks` (`int`, *optional*, 默认为 6) — 表示`ClvpConditioningEncoder`中的自注意力层数量。

+   `activation_function` (`str`, *optional*, 默认为`"gelu_new"`) — 激活函数，可在列表`["relu", "silu", "gelu", "tanh", "gelu_new"]`中选择。

+   `resid_pdrop` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `embd_pdrop` (`float`, *optional*, 默认为 0.1) — 嵌入的丢弃比率。

+   `attention_dropout` (`float`, *optional*, 默认为 0.1) — 注意力的丢弃比率。

+   `layer_norm_epsilon` (`float`, *optional*, 默认为 1e-05) — 在层归一化层中使用的 epsilon。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `summary_type` (`string`, *optional*, 默认为`"cls_index"`) — 在进行序列摘要时使用的参数。

    必须是以下选项之一：

    +   `"last"`: 获取最后一个标记的隐藏状态（类似 XLNet）。

    +   `"first"`: 获取第一个标记的隐藏状态（类似 BERT）。

    +   `"mean"`: 获取所有标记的隐藏状态的平均值。

    +   `"cls_index"`: 提供分类标记位置的张量（类似 GPT/GPT-2）。

    +   `"attn"`: 目前未实现，使用多头注意力。

+   `summary_use_proj` (`bool`, *optional*, 默认为`True`) — 是否在向量提取后添加投影。

+   `summary_activation` (`str`, *optional*) — 将`"tanh"`传递给输出以获得 tanh 激活，任何其他值将导致无激活。

+   `summary_proj_to_labels` (`bool`, *可选*, 默认为 `True`) — 投影输出是否应具有 `config.num_labels` 或 `config.hidden_size` 类别。

+   `summary_first_dropout` (`float`, *可选*, 默认为 0.1) — 投影和激活后要使用的丢弃比率。

+   `use_cache` (`bool`, *可选*, 默认为 `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `bos_token_id` (`int`, *可选*, 默认为 8192) — 序列开始标记的 ID，在生成开始时使用。

+   `eos_token_id` (`int`, *可选*, 默认为 8193) — 序列结束标记的 ID，在方法 `ClvpModelForConditionalGeneration.fix_speech_decoder_output()` 中用于修正解码器输出。

+   `feature_size` (`int`, *可选*, 默认为 80) — 提取的 mel 特征的特征维度。此值在 `ClvpConditioningEncoder` 中使用。

+   `use_attention_bias` (`bool`, *可选*, 默认为 `True`) — 在自注意力中是否使用 Query、Key 和 Value 层的偏置。

+   `initializer_factor` (`float`, *可选*, 默认为 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1.0，用于内部初始化测试）。

+   `decoder_fixing_codes` (`list`, *可选*, 默认为 `[83, 45, 45, 248]`) — 这些值在方法 `fix_speech_decoder_output` 中用于修正解码器生成的输出。

这是一个配置类，用于存储 ClvpDecoder 的配置。它用于根据指定的参数实例化一个 CLVP 解码器模型，定义模型架构。使用默认值实例化配置将产生与 CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev) 架构的解码器部分类似的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

该架构类似于 GPT2。

示例：

```py
>>> from transformers import ClvpDecoderConfig, ClvpDecoder

>>> # Initializing a ClvpDecoderConfig with susnato/clvp_dev style configuration
>>> decoder_configuration = ClvpDecoderConfig()

>>> # Initializing a ClvpDecoder (with random weights) from the susnato/clvp_dev style configuration
>>> model = ClvpDecoder(decoder_configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ClvpTokenizer

### `class transformers.ClvpTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L91)

```py
( vocab_file merges_file errors = 'replace' unk_token = '[UNK]' bos_token = '<|endoftext|>' eos_token = '[STOP]' pad_token = '[STOP]' add_prefix_space = False add_bos_token = False add_eos_token = False **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。

+   `merges_file` (`str`) — 合并文件的路径。

+   `errors` (`str`, *可选*, 默认为 `"replace"`) — 解码字节为 UTF-8 时要遵循的范例。更多信息请参考 [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。

+   `unk_token` (`str`, *可选*, 默认为 `"[UNK]"`) — 未知标记。词汇表中没有的标记无法转换为 ID，而是设置为此标记。

+   `bos_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--序列标记的开头。

+   `eos_token` (`str`, *可选*, 默认为 `"[STOP]"`) — 序列结束标记。

+   `pad_token` (`str`, *可选*, 默认为 `"[STOP]"`) — 序列的填充标记。

+   `add_prefix_space` (`bool`, *可选*, 默认为 `False`) — 是否在输入前添加一个初始空格。这允许将开头的单词视为任何其他单词。（CLVP 分词器通过前导空格检测单词的开头）。

+   `add_bos_token` (`bool`, *可选*, 默认为 `False`) — 当 `add_special_tokens=True` 时，是否在序列前添加 `bos_token`。

+   `add_eos_token` (`bool`, *可选*, 默认为 `False`) — 当 `add_special_tokens=True` 时，是否在序列末尾添加 `eos_token`。

构建一个 CLVP 分词器。基于字节级字节对编码。

该分词器已经训练成将空格视为标记的一部分（有点像 sentencepiece），因此一个单词将会在句子中的不同位置被编码成不同的标记。

在句子开头（无空格）或不是时，将以不同方式编码：

```py
>>> from transformers import ClvpTokenizer

>>> tokenizer = ClvpTokenizer.from_pretrained("susnato/clvp_dev")
>>> tokenizer("Hello world")["input_ids"]
[62, 84, 28, 2, 179, 79]

>>> tokenizer(" Hello world")["input_ids"]
[2, 62, 84, 28, 2, 179, 79]
```

通过在实例化此分词器时或在对某些文本调用时传递 `add_prefix_space=True`，可以避免这种行为，但由于模型不是以这种方式进行预训练的，可能会导致性能下降。

当与 `is_split_into_words=True` 一起使用时，此分词器将在每个单词之前添加一个空格（甚至是第一个单词）。

此分词器继承自 PreTrainedTokenizer，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L352)

```py
( save_directory: str filename_prefix: Optional = None )
```

## ClvpFeatureExtractor

### `class transformers.ClvpFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L33)

```py
( feature_size = 80 sampling_rate = 22050 default_audio_length = 6 hop_length = 256 chunk_length = 30 n_fft = 1024 padding_value = 0.0 mel_norms = None return_attention_mask = False **kwargs )
```

参数

+   `feature_size`（`int`，*可选*，默认为 80）— 提取特征的特征维度。

+   `sampling_rate`（`int`，*可选*，默认为 22050）— 音频文件应数字化的采样率，以赫兹（Hz）表示。

+   `default_audio_length`（`int`，*可选*，默认为 6）— 原始音频的默认长度（以秒为单位）。如果在 `__call__` 中未设置 `max_length`，则将自动设置为 default_audio_length * `self.sampling_rate`。

+   `hop_length`（`int`，*可选*，默认为 256）— 用于获取梅尔频率系数的 STFT 中的重叠窗口的长度。

+   `chunk_length`（`int`，*可选*，默认为 30）— 用于修剪和填充较长或较短音频序列的 `sampling_rate` 个样本块的最大数量。

+   `n_fft`（`int`，*可选*，默认为 1024）— 傅立叶变换的大小。

+   `padding_value`（`float`，*可选*，默认为 0.0）— 用于填充音频的填充值。应对应于静音。

+   `mel_norms`（长度为 `feature_size` 的 `list`，*可选*）— 如果提供了 `mel_norms`，则将用于沿每个梅尔滤波器对数梅尔频谱进行归一化。

+   `return_attention_mask`（`bool`，*可选*，默认为 `False`）— 是否返回注意力掩码。如果保持默认设置，将返回注意力掩码。

    什么是注意力掩码？

构建一个 CLVP 特征提取器。

此特征提取器继承自 SequenceFeatureExtractor，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

此类使用自定义 numpy 实现的 `Short Time Fourier Transform` 从原始语音中提取对数梅尔频谱特征，该实现应与 pytorch 的 `torch.stft` 等效。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L131)

```py
( raw_speech: Union sampling_rate: Optional = None truncation: bool = True pad_to_multiple_of: Optional = None return_tensors: Union = None return_attention_mask: Optional = True padding: Optional = 'max_length' max_length: Optional = None **kwargs )
```

参数

+   `raw_speech`（`np.ndarray`，`List[float]`，`List[np.ndarray]`，`List[List[float]]`）— 要填充的序列或序列批次。每个序列可以是一个 numpy 数组，一个浮点值列表，一个 numpy 数组列表或一个浮点值列表的列表。必须是单声道音频，不是立体声，即每个时间步长一个浮点数。

+   `sampling_rate`（`int`，*可选*）— `raw_speech` 输入的采样率。强烈建议在前向调用时传递 `sampling_rate`，以防止静默错误并允许自动语音识别流水线。

+   `truncation`（`bool`，*可选*，默认为 `True`）— 激活截断以将输入序列截断为比 *max_length* 更长的输入序列。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将填充序列到提供的值的倍数。

    这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 或在受益于序列长度为 128 的 TPU 上使用特别有用。

+   `return_attention_mask` (`bool`，*可选*，默认为 `True`) — 是否返回注意力掩码。如果保持默认设置，将返回注意力掩码。

    什么是注意力掩码？

+   `return_tensors` (`str` 或 TensorType，*可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

+   `padding_value` (`float`，默认为 0.0) — 用于填充填充值/向量的值。

+   `max_length` (`int`，*可选*) — 输入的最大长度。

`ClvpFeatureExtractor` 用于从样本声音或 `raw_speech` 中提取各种声音特定属性，如声音的音高和音调、说话速度，甚至说话缺陷，如口吃或结巴。

首先，声音被填充或截断，使其成为 `self.default_audio_length` 秒长的波形，然后从中提取对数梅尔频谱图。

## ClvpProcessor

### `class transformers.ClvpProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L24)

```py
( feature_extractor tokenizer )
```

参数

+   `feature_extractor` (`ClvpFeatureExtractor`) — ClvpFeatureExtractor 的一个实例。特征提取器是必需的输入。

+   `tokenizer` (`ClvpTokenizer`) — ClvpTokenizer 的一个实例。分词器是必需的输入。

构建一个 CLVP 处理器，将 CLVP 特征提取器和 CLVP 分词器封装成一个单一处理器。

ClvpProcessor 提供了 ClvpFeatureExtractor 和 ClvpTokenizer 的所有功能。查看 **call**()、decode() 和 batch_decode() 获取更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L49)

```py
( *args **kwargs )
```

将 `audio` 和 `sampling_rate` 参数转发到 **call**()，将 `text` 参数转发到 **call**()。有关更多信息，请参阅上述两种方法的文档字符串。

#### `decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L86)

```py
( *args **kwargs )
```

此方法将其所有参数转发给 ClvpTokenizer 的 decode()。有关更多信息，请参阅此方法的文档字符串。

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L78)

```py
( *args **kwargs )
```

此方法将其所有参数转发给 ClvpTokenizer 的 batch_decode()。有关更多信息，请参阅此方法的文档字符串。

## ClvpModelForConditionalGeneration

### `class transformers.ClvpModelForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1509)

```py
( config: ClvpConfig )
```

参数

+   `config`（ClvpConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

具有文本编码器、语音编码器和语音解码器模型的复合 CLVP 模型。语音解码器模型从文本生成语音 ID，文本编码器和语音编码器一起工作以过滤出最佳的语音 ID。此模型继承自 PreTrainedModel。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1737)

```py
( input_ids: LongTensor = None input_features: FloatTensor = None conditioning_encoder_inputs_embeds: Optional = None text_encoder_inputs_embeds: Optional = None attention_mask: Optional = None return_loss: Optional = None output_hidden_states: Optional = None output_attentions: Optional = False return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clvp.modeling_clvp.ClvpOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `input_features`（形状为`(batch_size, feature_size, time_dim)`的`torch.FloatTensor`）— 表示音频返回的 log mel-spectrogram 表示，由 ClvpFeatureExtractor 返回。

+   `conditioning_encoder_inputs_embeds`（`torch.FloatTensor`，*可选*）— 用于`ClvpConditioningEncoder`的 inputs_embeds。可用于替代`input_ids`。

+   `text_encoder_inputs_embeds`（`torch.FloatTensor`，*可选*）— 用于文本编码器模型的 inputs_embeds，代替`input_ids`。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免对填充文本标记索引执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于未被“掩码”的标记为 1，

    +   对于被“掩码”的标记为 0。

    什么是注意力掩码？

+   `return_loss`（`bool`，*可选*）— 是否返回对比损失。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.clvp.modeling_clvp.ClvpOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clvp.modeling_clvp.ClvpOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置(`<class 'transformers.models.clvp.configuration_clvp.ClvpConfig'>`)和输入不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当`return_loss`为`True`时返回) — 语音-文本相似性的对比损失。

+   `speech_ids` (`torch.LongTensor`，*可选*) — 由`ClvpForCausalLM`模型生成的语音 id（或语音候选）。

+   `logits_per_speech` (`torch.FloatTensor`，形状为`(speech_batch_size, text_batch_size)`) — `speech_embeds`和`text_embeds`之间的缩放点积分数。这代表了语音-文本相似性分数。

+   `logits_per_text` (`torch.FloatTensor`，形状为`(text_batch_size, speech_batch_size)`) — `text_embeds`和`speech_embeds`之间的缩放点积分数。这代表了文本-语音相似性分数。

+   `text_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将文本编码器模型的汇总输出应用到投影层获得的文本嵌入。

+   `speech_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将语音编码器模型的汇总输出应用到投影层获得的语音嵌入。

+   `text_model_output` (`BaseModelOutputWithPooling`) — 文本编码器模型的`last_hidden_state`的汇总输出。

+   `speech_model_output` (`BaseModelOutputWithPooling`) — 语音编码器模型的`last_hidden_state`的汇总输出。

+   `decoder_hidden_states` (`torch.FloatTensor`, *可选*) — 解码器模型的隐藏状态。

+   `text_encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 文本编码器模型的隐藏状态。

+   `speech_encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 语音编码器模型的隐藏状态。

ClvpModelForConditionalGeneration 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import datasets
>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library)
>>> text = "This is an example text."

>>> ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.cast_column("audio", datasets.Audio(sampling_rate=22050))
>>> _, audio, sr = ds.sort("id").select(range(1))[:1]["audio"][0].values()

>>> # Define processor and model
>>> processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
>>> model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

>>> # processor outputs and model outputs
>>> processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors="pt")
>>> outputs = model(
...     input_ids=processor_output["input_ids"],
...     input_features=processor_output["input_features"],
...     return_dict=True,
... )
```

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1869)

```py
( input_ids: LongTensor = None input_features: FloatTensor = None attention_mask: Optional = None generation_config: Optional = None pad_to_max_mel_tokens: Optional = None output_hidden_states: Optional = None **kwargs ) → export const metadata = 'undefined';ClvpOutput or tuple
```

参数

+   `input_ids` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 输入文本标记。从 ClvpTokenizer 处理而来。

+   `input_features` (`torch.FloatTensor`，形状为`(batch_size, feature_size, time_dim)`，*可选*) — 表示音频的 log-melspectrogram 表示，由 ClvpFeatureExtractor 返回。

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充文本标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1。

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `generation_config` (`~generation.GenerationConfig`, *optional*) — 用作生成调用的基本参数化的生成配置。传递给 generate 的`**kwargs`匹配`generation_config`的属性将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）来自`generation_config.json`模型文件，如果存在；2）来自模型配置。请注意，未指定的参数将继承 GenerationConfig 的默认值，其文档应该被检查以参数化生成。

+   `pad_to_max_mel_tokens` (`int`, *optional*) — 将生成的 speech_ids 填充到指定值。这是为了实现与官方 repo 相同的逻辑，链接：[`github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430`](https://github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430) 并确保对数相同。这不会影响生成质量，因此请不要考虑使用它，因为效率较低。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回解码器模型、文本编码器和语音编码器模型的隐藏状态。

返回

`ClvpOutput` 或 元组

一个`ClvpOutput`（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个元组。

`ClvpModelForConditionalGeneration`的生成方法，此方法调用`ClvpForCausalLM`的`generate`方法，然后使用生成的`speech_ids`来处理`text_embeds`和`speech_embeds`，使用`ClvpEncoder`。

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1583)

```py
( input_ids: Optional = None text_encoder_inputs_embeds: Optional = None attention_mask: Optional = None ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    什么是输入 ID？

+   `text_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — 用于文本编码器模型的 inputs_embeds，代替`input_ids`。

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

返回

`torch.FloatTensor` of shape `(batch_size, output_dim)`

通过将投影层应用于 CLVP 文本模型的池化输出获得的文本嵌入。

此方法可用于从文本中提取 text_embeds。通过将投影层应用于 CLVP 文本编码器模型的池化输出获得的文本嵌入。

示例：

```py
>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

>>> # Define the Text
>>> text = "This is an example text."

>>> # Define processor and model
>>> processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
>>> model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

>>> # Generate processor output and text embeds
>>> processor_output = processor(text=text, return_tensors="pt")
>>> text_embeds = model.get_text_features(input_ids=processor_output["input_ids"])
```

#### `get_speech_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1640)

```py
( speech_ids: Optional = None input_ids: Optional = None input_features: Optional = None conditioning_encoder_inputs_embeds: Optional = None attention_mask: Optional = None generation_config: Optional = None **kwargs ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `speech_ids` (`torch.LongTensor` of shape `(batch_size, num_speech_ids)`, *optional*) — 语音标记。默认情况下将忽略填充。如果提供了 speech_ids，则将自动忽略 input_ids 和 input_features。

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 输入文本标记。从 ClvpTokenizer 处理。如果未提供 speech_ids，则将使用 input_ids 和 input_features。

+   `input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size, time_dim)`, *optional*) — 指示音频的 log-melspectrogram 表示，由 ClvpFeatureExtractor 返回。如果未提供 speech_ids，则将使用 input_ids 和 input_features。

+   `conditioning_encoder_inputs_embeds`（`torch.FloatTensor`，*可选*）- 用于`ClvpConditioningEncoder`的 inputs_embeds。可以代替`input_ids`使用。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 用于避免在填充语音标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `generation_config`（`GenerationConfig`，*可选*）- 用于控制生成语音 ID 的生成配置，如果它们未提供。

返回

`torch.FloatTensor`的形状为`(batch_size, output_dim)`

通过将投影层应用于 CLVP 语音模型的汇聚输出获得的语音嵌入。

此方法可用于提取语音嵌入。通过将语音模型应用于语音 ID 获得语音嵌入。如果不存在语音 ID，但提供了 input_ids 和 input_features，则解码器模型将首先用于生成语音 ID，然后应用语音模型。

示例：

```py
>>> import datasets
>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library)
>>> text = "This is an example text."
>>> ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.cast_column("audio", datasets.Audio(sampling_rate=22050))
>>> _, audio, sr = ds.sort("id").select(range(1))[:1]["audio"][0].values()

>>> # Define processor and model
>>> processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
>>> model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

>>> # Generate processor output and model output
>>> processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors="pt")
>>> speech_embeds = model.get_speech_features(
...     input_ids=processor_output["input_ids"], input_features=processor_output["input_features"]
... )
```

## ClvpForCausalLM

### `class transformers.ClvpForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1280)

```py
( config )
```

参数

+   `config`（ClvpConfig）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

具有语言建模头部的 CLVP 解码器模型。此模型继承自 PreTrainedModel。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1421)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值），如下面的`past_key_values`输出所示。可用于加速顺序解码。将其过去传递给此模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    如果使用了`past_key_values`，则`attention_mask`需要包含用于`past_key_values`的掩码策略。换句话说，`attention_mask`的长度始终必须为：`len(past_key_values) + len(input_ids)`

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, input_ids_length)`，*optional*) — 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`中：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示头部是`not masked`，

    +   0 表示头部是`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵。

    如果使用`past_key_values`，则可选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于语言建模的标签。请注意，模型内部的标签**已移位**，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算标签在`[0, ..., config.vocab_size]`中的标签

ClvpForCausalLM 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是前者，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。

## ClvpModel

### `class transformers.ClvpModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1209)

```py
( config: ClvpDecoderConfig )
```

参数

+   `config` (ClvpConfig) — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

裸 Clvp 解码器模型输出原始隐藏状态，没有特定的头部。此模型继承自 PreTrainedModel。检查超类文档以了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1231)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。将其过去的`input_ids`不应作为`input_ids`传递给此模型，因为它们已经计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示未被`屏蔽`的标记，

    +   0 表示被`屏蔽`的标记。

    如果使用了`past_key_values`，则`attention_mask`需要包含用于`past_key_values`的掩码策略。换句话说，`attention_mask`的长度始终为：`len(past_key_values) + len(input_ids)`

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）- 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0 对应于*句子 A*的标记，

    +   1 对应于*句子 B*的标记。

    什么是标记类型 ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记的位置嵌入的索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示头部是`未屏蔽`，

    +   0 表示头部是`屏蔽`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果使用了`past_key_values`，则可选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

ClvpModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

## Clvp 编码器

### `类 transformers.Clvp 编码器`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L874)

```py
( config: ClvpConfig )
```

由`config.num_hidden_layers`个自注意层组成的 Transformer 编码器。每一层都是一个`ClvpEncoderLayer`。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L906)

```py
( input_ids: Optional = None inputs_embeds: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）— 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 模型的输入嵌入。这将绕过模型的内部嵌入查找矩阵。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于`未屏蔽`的标记为 1，

    +   对于`已屏蔽`的标记为 0。

    什么是注意力掩码？

+   `position_ids`（`torch.LongTensor`，*可选*）— 表示`input_ids`的位置 id。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

## Clvp 解码器

### `类 transformers.Clvp 解码器`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1030)

```py
( config )
```

由*config.num_hidden_layers*层组成的 Transformer 解码器。每一层都是一个`ClvpDecoderLayer`

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1065)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含模型计算的预计算隐藏状态（注意力块中的键和值），如下面的`past_key_values`输出所示。可用于加速顺序解码。已经计算过其过去的`input_ids`的`input_ids`不应作为`input_ids`传递，因为它们已经被计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示未被`掩码`的标记，

    +   0 表示被`掩码`的标记。

    如果使用了`past_key_values`，则`attention_mask`需要包含用于`past_key_values`的掩码策略。换句话说，`attention_mask`的长度始终必须为：`len(past_key_values) + len(input_ids)`

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）- 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示头部未被`掩码`，

    +   0 表示头部被`掩码`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

    如果使用了`past_key_values`，则可选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

ClvpDecoder 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
