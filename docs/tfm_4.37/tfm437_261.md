# DETA

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/deta`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deta)

## æ¦‚è¿°

DETA æ¨¡å‹æ˜¯ç”± Jeffrey Ouyang-Zhangã€Jang Hyun Choã€Xingyi Zhouã€Philipp KrÃ¤henbÃ¼hl åœ¨[NMS Strikes Back](https://arxiv.org/abs/2212.06137)ä¸­æå‡ºçš„ã€‚DETAï¼ˆDetection Transformers with Assignmentï¼‰é€šè¿‡å°†ä¼ ç»Ÿæ£€æµ‹å™¨ä¸­ä½¿ç”¨çš„ä¸€å¯¹ä¸€äºŒéƒ¨åŒ¹é…æŸå¤±æ›¿æ¢ä¸ºä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰çš„ä¸€å¯¹å¤šæ ‡ç­¾åˆ†é…æ¥æ”¹è¿› Deformable DETRï¼Œä»è€Œå®ç°äº†é«˜è¾¾ 2.5 mAP çš„æ˜¾ç€å¢ç›Šã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æ£€æµ‹å˜æ¢å™¨ï¼ˆDETRï¼‰åœ¨è®­ç»ƒæœŸé—´é€šè¿‡ä¸€å¯¹ä¸€çš„äºŒéƒ¨åŒ¹é…ç›´æ¥å°†æŸ¥è¯¢è½¬æ¢ä¸ºå”¯ä¸€å¯¹è±¡ï¼Œå¹¶å®ç°ç«¯åˆ°ç«¯çš„ç›®æ ‡æ£€æµ‹ã€‚æœ€è¿‘ï¼Œè¿™äº›æ¨¡å‹åœ¨ COCO ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„æ£€æµ‹å™¨ï¼Œå…·æœ‰ä¸å¯å¦è®¤çš„ä¼˜é›…ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šä¸ªè®¾è®¡æ–¹é¢ä¸ä¼ ç»Ÿæ£€æµ‹å™¨ä¸åŒï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè®¡åˆ’ï¼Œå› æ­¤ä¸€å¯¹ä¸€åŒ¹é…çš„æœ‰æ•ˆæ€§å°šæœªå®Œå…¨ç†è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨ DETR ä¸­è¿›è¡Œäº†ä¸€é¡¹ä¸¥æ ¼çš„æ¯”è¾ƒï¼Œä¸ä¼ ç»Ÿæ£€æµ‹å™¨ä¸­çš„ä¸€å¯¹å¤šæ ‡ç­¾åˆ†é…ç›¸æ¯”ï¼Œä¼ ç»Ÿæ£€æµ‹å™¨ä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼ŒNMS ä¸­çš„ä¸€å¯¹å¤šåˆ†é…å§‹ç»ˆä¼˜äºæ ‡å‡†çš„ä¸€å¯¹ä¸€åŒ¹é…ï¼Œè·å¾—äº†é«˜è¾¾ 2.5 mAP çš„æ˜¾ç€å¢ç›Šã€‚æˆ‘ä»¬çš„æ£€æµ‹å™¨ä½¿ç”¨ä¼ ç»Ÿçš„ IoU-based æ ‡ç­¾åˆ†é…è®­ç»ƒ Deformable-DETRï¼Œåœ¨ ResNet50 éª¨å¹²ç½‘ç»œä¸‹åœ¨ 12 ä¸ªæ—¶æœŸï¼ˆ1x è®¡åˆ’ï¼‰å†…å®ç°äº† 50.2 çš„ COCO mAPï¼Œä¼˜äºæ­¤è®¾ç½®ä¸­çš„æ‰€æœ‰ç°æœ‰ä¼ ç»Ÿæˆ–åŸºäºå˜æ¢å™¨çš„æ£€æµ‹å™¨ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ã€è®¡åˆ’å’Œæ¶æ„ä¸Šï¼Œæˆ‘ä»¬å§‹ç»ˆè¡¨æ˜äºŒéƒ¨åŒ¹é…å¯¹äºé«˜æ€§èƒ½æ£€æµ‹å˜æ¢å™¨æ˜¯ä¸å¿…è¦çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ£€æµ‹å˜æ¢å™¨çš„æˆåŠŸå½’å› äºå…¶å¯Œæœ‰è¡¨ç°åŠ›çš„å˜æ¢å™¨æ¶æ„ã€‚*

![å›¾ç¤º](img/a8a832a7e12fee10abab0f5395417bf4.png) DETA æ¦‚è¿°ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2212.06137)ã€‚

æ­¤æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/jozhang97/DETA)æ‰¾åˆ°ã€‚

## èµ„æº

ä¸€ä¸ªå®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ DETAã€‚

+   DETA çš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETA)æ‰¾åˆ°ã€‚

+   å¦è¯·å‚é˜…ï¼šç›®æ ‡æ£€æµ‹ä»»åŠ¡æŒ‡å—

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨è¿™é‡Œï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯å¤åˆ¶ç°æœ‰èµ„æºã€‚

## DetaConfig

### `class transformers.DetaConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/configuration_deta.py#L30)

```py
( backbone_config = None num_queries = 900 max_position_embeddings = 2048 encoder_layers = 6 encoder_ffn_dim = 2048 encoder_attention_heads = 8 decoder_layers = 6 decoder_ffn_dim = 1024 decoder_attention_heads = 8 encoder_layerdrop = 0.0 is_encoder_decoder = True activation_function = 'relu' d_model = 256 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 init_xavier_std = 1.0 return_intermediate = True auxiliary_loss = False position_embedding_type = 'sine' num_feature_levels = 5 encoder_n_points = 4 decoder_n_points = 4 two_stage = True two_stage_num_proposals = 300 with_box_refine = True assign_first_stage = True assign_second_stage = True class_cost = 1 bbox_cost = 5 giou_cost = 2 mask_loss_coefficient = 1 dice_loss_coefficient = 1 bbox_loss_coefficient = 5 giou_loss_coefficient = 2 eos_coefficient = 0.1 focal_alpha = 0.25 **kwargs )
```

å‚æ•°

+   `backbone_config`ï¼ˆ`PretrainedConfig`æˆ–`dict`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`ResNetConfig()`ï¼‰â€”éª¨å¹²æ¨¡å‹çš„é…ç½®ã€‚

+   `num_queries`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 900ï¼‰â€”å¯¹è±¡æŸ¥è¯¢çš„æ•°é‡ï¼Œå³æ£€æµ‹æ§½ä½ã€‚è¿™æ˜¯ DetaModel åœ¨å•ä¸ªå›¾åƒä¸­å¯ä»¥æ£€æµ‹åˆ°çš„å¯¹è±¡çš„æœ€å¤§æ•°é‡ã€‚å¦‚æœ`two_stage`è®¾ç½®ä¸º`True`ï¼Œåˆ™ä½¿ç”¨`two_stage_num_proposals`ã€‚

+   `d_model`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 256ï¼‰â€”å±‚çš„ç»´åº¦ã€‚

+   `encoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 6ï¼‰â€”ç¼–ç å™¨å±‚æ•°ã€‚

+   `decoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 6ï¼‰â€”è§£ç å™¨å±‚æ•°ã€‚

+   `encoder_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 8ï¼‰â€”å˜æ¢å™¨ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `decoder_attention_heads` (`int`, *optional*, defaults to 8) â€” Transformer è§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 2048) â€” è§£ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `encoder_ffn_dim` (`int`, *optional*, defaults to 2048) â€” è§£ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `activation_function` (`str` or `function`, *optional*, defaults to `"relu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™æ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚

+   `dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ dropout æ¦‚ç‡ã€‚

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout æ¯”ç‡ã€‚

+   `activation_dropout` (`float`, *optional*, defaults to 0.0) â€” å…¨è¿æ¥å±‚å†…æ¿€æ´»çš„ dropout æ¯”ç‡ã€‚

+   `init_std` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„ truncated_normal_initializer çš„æ ‡å‡†å·®ã€‚

+   `init_xavier_std` (`float`, *optional*, defaults to 1) â€” ç”¨äº HM Attention map æ¨¡å—ä¸­ Xavier åˆå§‹åŒ–å¢ç›Šçš„ç¼©æ”¾å› å­ã€‚

+   `encoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” ç¼–ç å™¨çš„ LayerDrop æ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LayerDrop paper](https://arxiv.org/abs/1909.11556)ã€‚

+   `auxiliary_loss` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨è¾…åŠ©è§£ç æŸå¤±ï¼ˆæ¯ä¸ªè§£ç å™¨å±‚çš„æŸå¤±ï¼‰ã€‚

+   `position_embedding_type` (`str`, *optional*, defaults to `"sine"`) â€” ç”¨äºå›¾åƒç‰¹å¾ä¹‹ä¸Šçš„ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚å¯ä»¥æ˜¯`"sine"`æˆ–`"learned"`ä¹‹ä¸€ã€‚

+   `class_cost` (`float`, *optional*, defaults to 1) â€” åŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­åˆ†ç±»é”™è¯¯çš„ç›¸å¯¹æƒé‡ã€‚

+   `bbox_cost` (`float`, *optional*, defaults to 5) â€” åŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­è¾¹ç•Œæ¡†åæ ‡çš„ L1 è¯¯å·®çš„ç›¸å¯¹æƒé‡ã€‚

+   `giou_cost` (`float`, *optional*, defaults to 2) â€” åŒˆç‰™åˆ©åŒ¹é…æˆæœ¬ä¸­è¾¹ç•Œæ¡†å¹¿ä¹‰ IoU æŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚

+   `mask_loss_coefficient` (`float`, *optional*, defaults to 1) â€” Focal loss åœ¨å…¨æ™¯åˆ†å‰²æŸå¤±ä¸­çš„ç›¸å¯¹æƒé‡ã€‚

+   `dice_loss_coefficient` (`float`, *optional*, defaults to 1) â€” å…¨æ™¯åˆ†å‰²æŸå¤±ä¸­ DICE/F-1 æŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚

+   `bbox_loss_coefficient` (`float`, *optional*, defaults to 5) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­ L1 è¾¹ç•Œæ¡†æŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚

+   `giou_loss_coefficient` (`float`, *optional*, defaults to 2) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­å¹¿ä¹‰ IoU æŸå¤±çš„ç›¸å¯¹æƒé‡ã€‚

+   `eos_coefficient` (`float`, *optional*, defaults to 0.1) â€” ç›®æ ‡æ£€æµ‹æŸå¤±ä¸­â€œæ— å¯¹è±¡â€ç±»çš„ç›¸å¯¹åˆ†ç±»æƒé‡ã€‚

+   `num_feature_levels` (`int`, *optional*, defaults to 5) â€” è¾“å…¥ç‰¹å¾çº§åˆ«çš„æ•°é‡ã€‚

+   `encoder_n_points` (`int`, *optional*, defaults to 4) â€” ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æ¯ä¸ªç‰¹å¾çº§åˆ«ä¸­é‡‡æ ·çš„é”®çš„æ•°é‡ã€‚

+   `decoder_n_points` (`int`, *optional*, defaults to 4) â€” è§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æ¯ä¸ªç‰¹å¾çº§åˆ«ä¸­é‡‡æ ·çš„é”®çš„æ•°é‡ã€‚

+   `two_stage` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åº”ç”¨ä¸¤é˜¶æ®µå¯å˜å½¢ DETRï¼Œå…¶ä¸­åŒºåŸŸæè®®ä¹Ÿæ˜¯ç”± DETA çš„å˜ä½“ç”Ÿæˆçš„ï¼Œç„¶åè¿›ä¸€æ­¥é¦ˆå…¥è§£ç å™¨è¿›è¡Œè¿­ä»£è¾¹ç•Œæ¡†ç»†åŒ–ã€‚

+   `two_stage_num_proposals` (`int`, *optional*, defaults to 300) â€” è¦ç”Ÿæˆçš„åŒºåŸŸæè®®æ•°é‡ï¼Œå¦‚æœ`two_stage`è®¾ç½®ä¸º`True`ã€‚

+   `with_box_refine` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åº”ç”¨è¿­ä»£è¾¹ç•Œæ¡†ç»†åŒ–ï¼Œå…¶ä¸­æ¯ä¸ªè§£ç å™¨å±‚æ ¹æ®å‰ä¸€å±‚çš„é¢„æµ‹å¯¹è¾¹ç•Œæ¡†è¿›è¡Œç»†åŒ–ã€‚

+   `focal_alpha` (`float`, *optional*, defaults to 0.25) â€” Focal loss ä¸­çš„ Alpha å‚æ•°ã€‚

+   `assign_first_stage` (`bool`, *optional*, defaults to `True`) â€” å¦‚æœé‡å å¤§äºé˜ˆå€¼ 0.7ï¼Œåˆ™å°†æ¯ä¸ªé¢„æµ‹ i åˆ†é…ç»™æœ€é«˜é‡å çš„åœ°é¢çœŸå®å¯¹è±¡ã€‚

+   `assign_second_stage` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨ç¬¬äºŒé˜¶æ®µç´§éšç¬¬ä¸€é˜¶æ®µåˆ†é…ç¨‹åºè¿›è¡Œç¬¬äºŒæ¬¡åˆ†é…ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ DetaModel çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª DETA æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿä¸ DETA [SenseTime/deformable-detr](https://huggingface.co/SenseTime/deformable-detr) æ¶æ„ç±»ä¼¼çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import DetaConfig, DetaModel

>>> # Initializing a DETA SenseTime/deformable-detr style configuration
>>> configuration = DetaConfig()

>>> # Initializing a model (with random weights) from the SenseTime/deformable-detr style configuration
>>> model = DetaModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## DetaImageProcessor

### `class transformers.DetaImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/image_processing_deta.py#L465)

```py
( format: Union = <AnnotationFormat.COCO_DETECTION: 'coco_detection'> do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True **kwargs )
```

å‚æ•°

+   `format` (`str`, *optional*, defaults to `"coco_detection"`) â€” æ³¨é‡Šçš„æ•°æ®æ ¼å¼ã€‚å…¶ä¸­ä¹‹ä¸€ä¸º`"coco_detection"`æˆ–â€œcoco_panopticâ€ã€‚

+   `do_resize` (`bool`, *optional*, defaults to `True`) â€” æ§åˆ¶æ˜¯å¦å°†å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å°ºå¯¸è°ƒæ•´ä¸ºæŒ‡å®šçš„ `size`ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_resize` å‚æ•°è¦†ç›–ã€‚

+   `size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 800, "longest_edge": 1333}`): è°ƒæ•´å¤§å°åçš„å›¾åƒï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å°ºå¯¸ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `size` å‚æ•°è¦†ç›–ã€‚

+   `resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚

+   `do_rescale` (`bool`, *optional*, defaults to `True`) â€” æ§åˆ¶æ˜¯å¦æŒ‰æŒ‡å®šçš„æ¯”ä¾‹ `rescale_factor` å¯¹å›¾åƒè¿›è¡Œé‡æ–°ç¼©æ”¾ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_rescale` å‚æ•°è¦†ç›–ã€‚

+   `rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„æ¯”ä¾‹å› å­ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `rescale_factor` å‚æ•°è¦†ç›–ã€‚ do_normalize â€” æ§åˆ¶æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_normalize` å‚æ•°è¦†ç›–ã€‚

+   `image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`) â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„å‡å€¼ã€‚å¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–æ¯ä¸ªé€šé“çš„å€¼åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_mean` å‚æ•°è¦†ç›–ã€‚

+   `image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`) â€” åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„æ ‡å‡†å·®å€¼ã€‚å¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–æ¯ä¸ªé€šé“çš„å€¼åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_std` å‚æ•°è¦†ç›–ã€‚

+   `do_pad` (`bool`, *optional*, defaults to `True`) â€” æ§åˆ¶æ˜¯å¦å°†å›¾åƒå¡«å……åˆ°æ‰¹å¤„ç†ä¸­æœ€å¤§çš„å›¾åƒå¹¶åˆ›å»ºåƒç´ æ©ç ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_pad` å‚æ•°è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ªå¯å˜å½¢ DETR å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/image_processing_deta.py#L771)

```py
( images: Union annotations: Union = None return_segmentation_masks: bool = None masks_path: Union = None do_resize: Optional = None size: Optional = None resample = None do_rescale: Optional = None rescale_factor: Union = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None format: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒæˆ–å›¾åƒæ‰¹å¤„ç†ã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–åƒç´ å€¼èŒƒå›´ä» 0 åˆ° 255 çš„å›¾åƒæ‰¹å¤„ç†ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½® `do_rescale=False`ã€‚

+   `annotations`ï¼ˆ`List[Dict]`æˆ–`List[List[Dict]]`ï¼Œ*å¯é€‰*ï¼‰- ä¸å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒç›¸å…³è”çš„æ³¨é‡Šåˆ—è¡¨ã€‚å¦‚æœæ³¨é‡Šç”¨äºç›®æ ‡æ£€æµ‹ï¼Œåˆ™æ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå¸¦æœ‰ä»¥ä¸‹é”®çš„å­—å…¸ï¼š

    +   â€œimage_idâ€ï¼ˆ`int`ï¼‰ï¼šå›¾åƒ IDã€‚

    +   â€œannotationsâ€ï¼ˆ`List[Dict]`ï¼‰ï¼šå›¾åƒçš„æ³¨é‡Šåˆ—è¡¨ã€‚æ¯ä¸ªæ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ã€‚ä¸€ä¸ªå›¾åƒå¯èƒ½æ²¡æœ‰æ³¨é‡Šï¼Œæ­¤æ—¶åˆ—è¡¨åº”ä¸ºç©ºã€‚å¦‚æœæ³¨é‡Šç”¨äºåˆ†å‰²ï¼Œæ³¨é‡Šåº”è¯¥æ˜¯ä¸€ä¸ªå¸¦æœ‰ä»¥ä¸‹é”®çš„å­—å…¸ï¼š

    +   â€œimage_idâ€ï¼ˆ`int`ï¼‰ï¼šå›¾åƒ IDã€‚

    +   â€œsegments_infoâ€ï¼ˆ`List[Dict]`ï¼‰ï¼šå›¾åƒçš„æ®µåˆ—è¡¨ã€‚æ¯ä¸ªæ®µåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ã€‚ä¸€ä¸ªå›¾åƒå¯èƒ½æ²¡æœ‰æ®µï¼Œæ­¤æ—¶åˆ—è¡¨åº”ä¸ºç©ºã€‚

    +   â€œfile_nameâ€ï¼ˆ`str`ï¼‰ï¼šå›¾åƒçš„æ–‡ä»¶åã€‚

+   `return_segmentation_masks`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.return_segmentation_masksï¼‰- æ˜¯å¦è¿”å›åˆ†å‰²æ©æ¨¡ã€‚

+   `masks_path`ï¼ˆ`str`æˆ–`pathlib.Path`ï¼Œ*å¯é€‰*ï¼‰- åŒ…å«åˆ†å‰²æ©æ¨¡çš„ç›®å½•è·¯å¾„ã€‚

+   `do_resize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.do_resizeï¼‰- æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size`ï¼ˆ`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.sizeï¼‰- è°ƒæ•´å¤§å°åçš„å›¾åƒå¤§å°ã€‚

+   `resample`ï¼ˆ`PILImageResampling`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.resampleï¼‰- è°ƒæ•´å›¾åƒå¤§å°æ—¶ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚

+   `do_rescale`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.do_rescaleï¼‰- æ˜¯å¦é‡æ–°ç¼©æ”¾å›¾åƒã€‚

+   `rescale_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.rescale_factorï¼‰- é‡æ–°ç¼©æ”¾å›¾åƒæ—¶ä½¿ç”¨çš„é‡æ–°ç¼©æ”¾å› å­ã€‚

+   `do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.do_normalizeï¼‰- æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.image_meanï¼‰- åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„å‡å€¼ã€‚

+   `image_std`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.image_stdï¼‰- åœ¨å½’ä¸€åŒ–å›¾åƒæ—¶ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚

+   `do_pad`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.do_padï¼‰- æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå¡«å……ã€‚

+   `format`ï¼ˆ`str`æˆ–`AnnotationFormat`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.formatï¼‰- æ³¨é‡Šçš„æ ¼å¼ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ–`TensorType`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º self.return_tensorsï¼‰- è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¦‚æœä¸º`None`ï¼Œå°†è¿”å›å›¾åƒåˆ—è¡¨ã€‚

+   `data_format`ï¼ˆ`ChannelDimension`æˆ–`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`ChannelDimension.FIRST`ï¼‰- è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆnum_channelsï¼Œheightï¼Œwidthï¼‰æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆheightï¼Œwidthï¼Œnum_channelsï¼‰æ ¼å¼ã€‚

    +   æœªè®¾ç½®ï¼šä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚

+   `input_data_format`ï¼ˆ`ChannelDimension`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰- è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆnum_channelsï¼Œheightï¼Œwidthï¼‰æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆheightï¼Œwidthï¼Œnum_channelsï¼‰æ ¼å¼ã€‚

    +   `"none"`æˆ–`ChannelDimension.NONE`ï¼šå›¾åƒä»¥ï¼ˆheightï¼Œwidthï¼‰æ ¼å¼ã€‚

å¯¹å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥ä½¿ç”¨ã€‚

#### `post_process_object_detection`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/image_processing_deta.py#L993)

```py
( outputs threshold: float = 0.5 target_sizes: Union = None nms_threshold: float = 0.7 ) â†’ export const metadata = 'undefined';List[Dict]
```

å‚æ•°

+   `outputs`ï¼ˆ`DetrObjectDetectionOutput`ï¼‰- æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.5ï¼‰- ä¿ç•™ç›®æ ‡æ£€æµ‹é¢„æµ‹çš„åˆ†æ•°é˜ˆå€¼ã€‚

+   `target_sizes`ï¼ˆ`torch.Tensor`æˆ–`List[Tuple[int, int]]`ï¼Œ*å¯é€‰*ï¼‰- å½¢çŠ¶ä¸º`(batch_size, 2)`çš„å¼ é‡æˆ–åŒ…å«æ¯ä¸ªå›¾åƒæ‰¹æ¬¡ä¸­ç›®æ ‡å¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„å…ƒç»„åˆ—è¡¨ï¼ˆ`Tuple[int, int]`ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º Noneï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

+   `nms_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.7ï¼‰- NMS é˜ˆå€¼ã€‚

è¿”å›

`List[Dict]`

ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¨¡å‹é¢„æµ‹çš„æ‰¹æ¬¡ä¸­å›¾åƒçš„åˆ†æ•°ã€æ ‡ç­¾å’Œæ¡†ã€‚

å°† DetaForObjectDetection çš„è¾“å‡ºè½¬æ¢ä¸ºæœ€ç»ˆçš„è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º(top_left_x, top_left_y, bottom_right_x, bottom_right_y)ã€‚ä»…æ”¯æŒ PyTorchã€‚

## DetaModel

### `class transformers.DetaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/modeling_deta.py#L1345)

```py
( config: DetaConfig )
```

å‚æ•°

+   `config` (DetaConfig) â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ DETA æ¨¡å‹ï¼ˆç”±éª¨å¹²å’Œç¼–ç å™¨-è§£ç å™¨ Transformer ç»„æˆï¼‰ï¼Œè¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/modeling_deta.py#L1515)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.deta.modeling_deta.DetaModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`AutoImageProcessor.__call__()`ã€‚

+   `pixel_mask` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, height, width)`ï¼Œ*å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   å¯¹äºçœŸå®åƒç´ ï¼ˆå³`æœªå±è”½`ï¼‰ä¸º 1ï¼Œ

    +   å¯¹äºå¡«å……åƒç´ ï¼ˆå³`å±è”½`ï¼‰ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_attention_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries)`ï¼Œ*å¯é€‰*) â€” é»˜è®¤æƒ…å†µä¸‹ä¸ä½¿ç”¨ã€‚å¯ç”¨äºå±è”½å¯¹è±¡æŸ¥è¯¢ã€‚

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *å¯é€‰*) â€” å…ƒç»„åŒ…å«(`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’å›¾åƒçš„æ‰å¹³åŒ–è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’æ‰å¹³åŒ–ç‰¹å¾å›¾ï¼ˆéª¨å¹²ç½‘ç»œå’ŒæŠ•å½±å±‚çš„è¾“å‡ºï¼‰ã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä½¿ç”¨é›¶å¼ é‡åˆå§‹åŒ–æŸ¥è¯¢ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›å€¼

`transformers.models.deta.modeling_deta.DetaModelOutput`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.deta.modeling_deta.DetaModelOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆDetaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `init_reference_points`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, 4)`çš„`torch.FloatTensor`ï¼‰â€” é€šè¿‡ Transformer è§£ç å™¨å‘é€çš„åˆå§‹å‚è€ƒç‚¹ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `intermediate_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.decoder_layers, num_queries, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” å †å çš„ä¸­é—´éšè—çŠ¶æ€ï¼ˆè§£ç å™¨æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

+   `intermediate_reference_points`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.decoder_layers, num_queries, 4)`çš„`torch.FloatTensor`ï¼‰â€” å †å çš„ä¸­é—´å‚è€ƒç‚¹ï¼ˆè§£ç å™¨æ¯å±‚çš„å‚è€ƒç‚¹ï¼‰ã€‚

+   `decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚è§£ç å™¨åœ¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, num_queries, num_queries)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_queries, num_heads, 4, 4)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨åœ¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_queries, num_heads, 4, 4)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `enc_outputs_class`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“`config.with_box_refine=True`å’Œ`config.two_stage=True`æ—¶è¿”å›ï¼‰â€” é¢„æµ‹çš„è¾¹ç•Œæ¡†åˆ†æ•°ï¼Œå…¶ä¸­é€‰æ‹©å‰`config.two_stage_num_proposals`ä¸ªå¾—åˆ†æœ€é«˜çš„è¾¹ç•Œæ¡†ä½œä¸ºç¬¬ä¸€é˜¶æ®µçš„åŒºåŸŸæè®®ã€‚è¾¹ç•Œæ¡†äºŒå…ƒåˆ†ç±»çš„è¾“å‡ºï¼ˆå³å‰æ™¯å’ŒèƒŒæ™¯ï¼‰ã€‚

+   `enc_outputs_coord_logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, 4)`ï¼Œ*å¯é€‰*ï¼Œå½“`config.with_box_refine=True`å’Œ`config.two_stage=True`æ—¶è¿”å›ï¼‰â€” ç¬¬ä¸€é˜¶æ®µä¸­é¢„æµ‹çš„è¾¹ç•Œæ¡†åæ ‡çš„ logitsã€‚

+   `output_proposals`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, 4)`ï¼Œ*å¯é€‰*ï¼Œå½“`config.two_stage=True`æ—¶è¿”å›ï¼‰â€” åœ¨ gen_encoder_output_proposals ä¸­æè®®è¾¹ç•Œæ¡†åæ ‡çš„ logitsã€‚

DetaModel çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, DetaModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("jozhang97/deta-swin-large-o365")
>>> model = DetaModel.from_pretrained("jozhang97/deta-swin-large-o365", two_stage=False)

>>> inputs = image_processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 900, 256]
```

## DetaForObjectDetection

### `class transformers.DetaForObjectDetection`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/modeling_deta.py#L1762)

```py
( config: DetaConfig )
```

å‚æ•°

+   `config`ï¼ˆDetaConfigï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

DETA æ¨¡å‹ï¼ˆç”±éª¨å¹²å’Œç¼–ç å™¨-è§£ç å™¨ Transformer ç»„æˆï¼‰ï¼Œé¡¶éƒ¨å¸¦æœ‰ç›®æ ‡æ£€æµ‹å¤´ï¼Œç”¨äºè¯¸å¦‚ COCO æ£€æµ‹ä¹‹ç±»çš„ä»»åŠ¡ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ PyTorch çš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deta/modeling_deta.py#L1827)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.deta.modeling_deta.DetaObjectDetectionOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å¾—ã€‚æŸ¥çœ‹`AutoImageProcessor.__call__()`ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚

+   `pixel_mask`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, height, width)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºçœŸå®åƒç´ ï¼ˆå³`æœªè¢«æ©ç `ï¼‰ï¼Œ

    +   0 è¡¨ç¤ºå¡«å……åƒç´ ï¼ˆå³`å·²æ©ç `ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_attention_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries)`ï¼Œ*å¯é€‰*ï¼‰â€” é»˜è®¤æƒ…å†µä¸‹ä¸ä½¿ç”¨ã€‚å¯ç”¨äºå±è”½å¯¹è±¡æŸ¥è¯¢ã€‚

+   `encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰â€” å…ƒç»„åŒ…å«ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`ï¼‰`last_hidden_state`å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’å›¾åƒçš„æ‰å¹³ç‰¹å¾å›¾ï¼ˆéª¨å¹²+æŠ•å½±å±‚çš„è¾“å‡ºï¼‰ï¼Œè€Œä¸æ˜¯ä¼ é€’å›¾åƒçš„æ‰å¹³è¡¨ç¤ºã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*) â€” å¯é€‰ï¼Œå¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç”¨é›¶å¼ é‡åˆå§‹åŒ–æŸ¥è¯¢ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`List[Dict]` of len `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—äºŒåˆ†åŒ¹é…æŸå¤±çš„æ ‡ç­¾ã€‚å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸è‡³å°‘åŒ…å«ä»¥ä¸‹ 2 ä¸ªé”®ï¼š'class_labels'å’Œ'boxes'ï¼ˆåˆ†åˆ«æ˜¯æ‰¹å¤„ç†ä¸­å›¾åƒçš„ç±»æ ‡ç­¾å’Œè¾¹ç•Œæ¡†ï¼‰ã€‚ç±»æ ‡ç­¾æœ¬èº«åº”è¯¥æ˜¯é•¿åº¦ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†æ•°é‡,)`çš„`torch.LongTensor`ï¼Œè€Œè¾¹ç•Œæ¡†æ˜¯å½¢çŠ¶ä¸º`(å›¾åƒä¸­è¾¹ç•Œæ¡†æ•°é‡, 4)`çš„`torch.FloatTensor`ã€‚

è¿”å›

`transformers.models.deta.modeling_deta.DetaObjectDetectionOutput`æˆ–`tuple(torch.FloatTensor)`

`transformers.models.deta.modeling_deta.DetaObjectDetectionOutput`æˆ–`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆDetaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€” æ€»æŸå¤±ï¼Œä½œä¸ºè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆäº¤å‰ç†µï¼‰å’Œè¾¹ç•Œæ¡†æŸå¤±çš„çº¿æ€§ç»„åˆã€‚åè€…è¢«å®šä¹‰ä¸º L1 æŸå¤±å’Œå¹¿ä¹‰å°ºåº¦ä¸å˜ IoU æŸå¤±çš„çº¿æ€§ç»„åˆã€‚

+   `loss_dict` (`Dict`, *optional*) â€” åŒ…å«å„ä¸ªæŸå¤±çš„å­—å…¸ã€‚ç”¨äºè®°å½•ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) â€” æ‰€æœ‰æŸ¥è¯¢çš„åˆ†ç±» logitsï¼ˆåŒ…æ‹¬æ— å¯¹è±¡ï¼‰ã€‚

+   `pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) â€” æ‰€æœ‰æŸ¥è¯¢çš„æ ‡å‡†åŒ–æ¡†åæ ‡ï¼Œè¡¨ç¤ºä¸ºï¼ˆä¸­å¿ƒ _xï¼Œä¸­å¿ƒ _yï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰ã€‚è¿™äº›å€¼åœ¨[0, 1]èŒƒå›´å†…æ ‡å‡†åŒ–ï¼Œç›¸å¯¹äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªå•ç‹¬å›¾åƒçš„å¤§å°ï¼ˆå¿½ç•¥å¯èƒ½çš„å¡«å……ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`~DetaProcessor.post_process_object_detection`æ¥æ£€ç´¢æœªæ ‡å‡†åŒ–çš„è¾¹ç•Œæ¡†ã€‚

+   `auxiliary_outputs` (`list[Dict]`, *optional*) â€” å¯é€‰ï¼Œä»…åœ¨è¾…åŠ©æŸå¤±è¢«æ¿€æ´»æ—¶è¿”å›ï¼ˆå³`config.auxiliary_loss`è®¾ç½®ä¸º`True`ï¼‰å¹¶æä¾›æ ‡ç­¾æ—¶è¿”å›ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªè§£ç å™¨å±‚çš„ä¸Šè¿°ä¸¤ä¸ªé”®ï¼ˆ`logits`å’Œ`pred_boxes`ï¼‰çš„å­—å…¸åˆ—è¡¨ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—çš„è¾“å‡ºã€‚

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚è§£ç å™¨åœ¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, num_queries, num_queries)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å…ƒç»„çš„`torch.FloatTensor`ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_queries, num_heads, 4, 4)`ã€‚è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰ â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å…ƒç»„çš„`torch.FloatTensor`ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å…ƒç»„çš„`torch.FloatTensor`ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, num_heads, 4, 4)`ã€‚ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `intermediate_hidden_states` (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`) â€” å †å çš„ä¸­é—´éšè—çŠ¶æ€ï¼ˆè§£ç å™¨æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

+   `intermediate_reference_points` (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`) â€” å †å çš„ä¸­é—´å‚è€ƒç‚¹ï¼ˆè§£ç å™¨æ¯ä¸€å±‚çš„å‚è€ƒç‚¹ï¼‰ã€‚

+   `init_reference_points` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) â€” é€šè¿‡ Transformer è§£ç å™¨å‘é€çš„åˆå§‹å‚è€ƒç‚¹ã€‚

+   `enc_outputs_class` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`ï¼Œ*å¯é€‰*ï¼Œå½“`config.with_box_refine=True`å’Œ`config.two_stage=True`æ—¶è¿”å›ï¼‰ â€” é¢„æµ‹çš„è¾¹ç•Œæ¡†åˆ†æ•°ï¼Œå…¶ä¸­é€‰æ‹©å‰`config.two_stage_num_proposals`ä¸ªå¾—åˆ†æœ€é«˜çš„è¾¹ç•Œæ¡†ä½œä¸ºç¬¬ä¸€é˜¶æ®µçš„åŒºåŸŸæè®®ã€‚è¾¹ç•Œæ¡†äºŒå…ƒåˆ†ç±»çš„è¾“å‡ºï¼ˆå³å‰æ™¯å’ŒèƒŒæ™¯ï¼‰ã€‚

+   `enc_outputs_coord_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`ï¼Œ*å¯é€‰*ï¼Œå½“`config.with_box_refine=True`å’Œ`config.two_stage=True`æ—¶è¿”å›ï¼‰ â€” ç¬¬ä¸€é˜¶æ®µä¸­é¢„æµ‹çš„è¾¹ç•Œæ¡†åæ ‡çš„ logitsã€‚

+   `output_proposals` (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`ï¼Œ*å¯é€‰*ï¼Œå½“`config.two_stage=True`æ—¶è¿”å›ï¼‰ â€” åœ¨ gen_encoder_output_proposals ä¸­çš„æè®®è¾¹ç•Œæ¡†åæ ‡çš„ logitsã€‚

DetaForObjectDetection çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, DetaForObjectDetection
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("jozhang97/deta-swin-large")
>>> model = DetaForObjectDetection.from_pretrained("jozhang97/deta-swin-large")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)

>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> target_sizes = torch.tensor([image.size[::-1]])
>>> results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[
...     0
... ]
>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected cat with confidence 0.683 at location [345.85, 23.68, 639.86, 372.83]
Detected cat with confidence 0.683 at location [8.8, 52.49, 316.93, 473.45]
Detected remote with confidence 0.568 at location [40.02, 73.75, 175.96, 117.33]
Detected remote with confidence 0.546 at location [333.68, 77.13, 370.12, 187.51]
```
