# ä½¿ç”¨ IDEFICS è¿›è¡Œå›¾åƒä»»åŠ¡

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics`](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/idefics)

è™½ç„¶å¯ä»¥é€šè¿‡å¾®è°ƒä¸“é—¨çš„æ¨¡å‹æ¥è§£å†³å•ä¸ªä»»åŠ¡ï¼Œä½†æœ€è¿‘å‡ºç°å¹¶å—åˆ°æ¬¢è¿çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å¤§å‹æ¨¡å‹å¤„ç†å„ç§ä»»åŠ¡è€Œæ— éœ€å¾®è°ƒã€‚ä¾‹å¦‚ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å¤„ç†è¯¸å¦‚æ‘˜è¦ã€ç¿»è¯‘ã€åˆ†ç±»ç­‰ NLP ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•ä¸å†å±€é™äºå•ä¸€æ¨¡æ€ï¼Œæ¯”å¦‚æ–‡æœ¬ï¼Œåœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•ä½¿ç”¨åä¸º IDEFICS çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è§£å†³å›¾åƒæ–‡æœ¬ä»»åŠ¡ã€‚

IDEFICS æ˜¯ä¸€ä¸ªåŸºäº[Flamingo](https://huggingface.co/papers/2204.14198)çš„å¼€æ”¾å¼è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ŒFlamingo æ˜¯ç”± DeepMind æœ€åˆå¼€å‘çš„æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ¥å—ä»»æ„åºåˆ—çš„å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ä½œä¸ºè¾“å‡ºã€‚å®ƒå¯ä»¥å›ç­”å…³äºå›¾åƒçš„é—®é¢˜ï¼Œæè¿°è§†è§‰å†…å®¹ï¼Œåˆ›å»ºåŸºäºå¤šä¸ªå›¾åƒçš„æ•…äº‹ç­‰ã€‚IDEFICS æœ‰ä¸¤ä¸ªå˜ä½“ - [80 äº¿å‚æ•°](https://huggingface.co/HuggingFaceM4/idefics-80b)å’Œ[90 äº¿å‚æ•°](https://huggingface.co/HuggingFaceM4/idefics-9b)ï¼Œè¿™ä¸¤ä¸ªå˜ä½“éƒ½å¯ä»¥åœ¨ğŸ¤— Hub ä¸Šæ‰¾åˆ°ã€‚å¯¹äºæ¯ä¸ªå˜ä½“ï¼Œæ‚¨è¿˜å¯ä»¥æ‰¾åˆ°ä¸ºå¯¹è¯ä½¿ç”¨æ¡ˆä¾‹è°ƒæ•´çš„æ¨¡å‹çš„å¾®è°ƒæŒ‡å¯¼ç‰ˆæœ¬ã€‚

è¿™ä¸ªæ¨¡å‹éå¸¸çµæ´»ï¼Œå¯ä»¥ç”¨äºå„ç§å›¾åƒå’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä½œä¸ºä¸€ä¸ªå¤§å‹æ¨¡å‹æ„å‘³ç€å®ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’ŒåŸºç¡€è®¾æ–½ã€‚æ‚¨éœ€è¦å†³å®šè¿™ç§æ–¹æ³•æ˜¯å¦æ¯”ä¸ºæ¯ä¸ªå•ç‹¬ä»»åŠ¡å¾®è°ƒä¸“é—¨çš„æ¨¡å‹æ›´é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š

+   åŠ è½½ IDEFICS å’ŒåŠ è½½æ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬

+   ä½¿ç”¨ IDEFICS è¿›è¡Œï¼š

    +   å›¾åƒåŠ æ ‡é¢˜

    +   æç¤ºçš„å›¾åƒåŠ æ ‡é¢˜

    +   å°‘æ ·æœ¬æç¤º

    +   è§†è§‰é—®ç­”

    +   å›¾åƒåˆ†ç±»

    +   å›¾åƒå¼•å¯¼æ–‡æœ¬ç”Ÿæˆ

+   æ‰¹å¤„ç†æ¨¡å¼ä¸‹è¿è¡Œæ¨ç†

+   è¿è¡Œ IDEFICS æŒ‡å¯¼è¿›è¡Œå¯¹è¯ä½¿ç”¨

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚

```py
pip install -q bitsandbytes sentencepiece accelerate transformers
```

è¦è¿è¡Œä»¥ä¸‹ç¤ºä¾‹ï¼Œæ‚¨å°†éœ€è¦è‡³å°‘ 20GB çš„ GPU å†…å­˜æ¥ä½¿ç”¨æ¨¡å‹æ£€æŸ¥ç‚¹çš„éé‡åŒ–ç‰ˆæœ¬ã€‚

## åŠ è½½æ¨¡å‹

è®©æˆ‘ä»¬ä»åŠ è½½æ¨¡å‹çš„ 90 äº¿å‚æ•°æ£€æŸ¥ç‚¹å¼€å§‹ï¼š

```py
>>> checkpoint = "HuggingFaceM4/idefics-9b"
```

å°±åƒå…¶ä»– Transformer æ¨¡å‹ä¸€æ ·ï¼Œæ‚¨éœ€è¦ä»æ£€æŸ¥ç‚¹åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹æœ¬èº«ã€‚IDEFICS å¤„ç†å™¨å°† LlamaTokenizer å’Œ IDEFICS å›¾åƒå¤„ç†å™¨åŒ…è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ï¼Œä»¥è´Ÿè´£ä¸ºæ¨¡å‹å‡†å¤‡æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ã€‚

```py
>>> import torch

>>> from transformers import IdeficsForVisionText2Text, AutoProcessor

>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map="auto")
```

å°†`device_map`è®¾ç½®ä¸º`"auto"`å°†è‡ªåŠ¨ç¡®å®šå¦‚ä½•ä»¥æœ€ä¼˜åŒ–çš„æ–¹å¼åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ï¼Œè€ƒè™‘åˆ°ç°æœ‰è®¾å¤‡ã€‚

### é‡åŒ–æ¨¡å‹

å¦‚æœé«˜å†…å­˜ GPU å¯ç”¨æ€§æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥åŠ è½½æ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬ã€‚è¦åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨çš„ 4 ä½ç²¾åº¦ç‰ˆæœ¬ï¼Œè¯·å°†`BitsAndBytesConfig`ä¼ é€’ç»™`from_pretrained`æ–¹æ³•ï¼Œæ¨¡å‹å°†åœ¨åŠ è½½æ—¶å³æ—¶å‹ç¼©ã€‚

```py
>>> import torch
>>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig

>>> quantization_config = BitsAndBytesConfig(
...     load_in_4bit=True,
...     bnb_4bit_compute_dtype=torch.float16,
... )

>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> model = IdeficsForVisionText2Text.from_pretrained(
...     checkpoint,
...     quantization_config=quantization_config,
...     device_map="auto"
... )
```

ç°åœ¨æ‚¨å·²ç»ä»¥å»ºè®®çš„æ–¹å¼ä¹‹ä¸€åŠ è½½äº†æ¨¡å‹ï¼Œè®©æˆ‘ä»¬ç»§ç»­æ¢ç´¢æ‚¨å¯ä»¥ä½¿ç”¨ IDEFICS çš„ä»»åŠ¡ã€‚

## å›¾åƒåŠ æ ‡é¢˜

å›¾åƒåŠ æ ‡é¢˜æ˜¯é¢„æµ‹ç»™å®šå›¾åƒçš„æ ‡é¢˜çš„ä»»åŠ¡ã€‚ä¸€ä¸ªå¸¸è§çš„åº”ç”¨æ˜¯å¸®åŠ©è§†éšœäººå£«åœ¨ä¸åŒæƒ…å†µä¸‹å¯¼èˆªï¼Œä¾‹å¦‚ï¼Œåœ¨çº¿æ¢ç´¢å›¾åƒå†…å®¹ã€‚

ä¸ºäº†è¯´æ˜ä»»åŠ¡ï¼Œè·å–ä¸€ä¸ªéœ€è¦åŠ æ ‡é¢˜çš„å›¾åƒï¼Œä¾‹å¦‚ï¼š

![èŠ±å›­é‡Œçš„å°ç‹—çš„å›¾ç‰‡](img/1ae9f6a999a65c594f8ffed6366c14b3.png)

ç…§ç‰‡ç”±[Hendo Wang](https://unsplash.com/@hendoo)æ‹æ‘„ã€‚

IDEFICS æ¥å—æ–‡æœ¬å’Œå›¾åƒæç¤ºã€‚ä½†æ˜¯ï¼Œè¦ä¸ºå›¾åƒæ·»åŠ å­—å¹•ï¼Œæ‚¨ä¸å¿…å‘æ¨¡å‹æä¾›æ–‡æœ¬æç¤ºï¼Œåªéœ€æä¾›é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒã€‚æ²¡æœ‰æ–‡æœ¬æç¤ºï¼Œæ¨¡å‹å°†ä» BOSï¼ˆåºåˆ—å¼€å§‹ï¼‰æ ‡è®°å¼€å§‹ç”Ÿæˆæ–‡æœ¬ï¼Œä»è€Œåˆ›å»ºå­—å¹•ã€‚

ä½œä¸ºæ¨¡å‹çš„å›¾åƒè¾“å…¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å›¾åƒå¯¹è±¡ï¼ˆ`PIL.Image`ï¼‰æˆ–å¯ä»¥ä»ä¸­æ£€ç´¢å›¾åƒçš„ urlã€‚

```py
>>> prompt = [
...     "https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80",
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
A puppy in a flower bed
```

åœ¨è°ƒç”¨`generate`æ—¶ï¼Œæœ€å¥½åŒ…å«`bad_words_ids`ï¼Œä»¥é¿å…åœ¨å¢åŠ `max_new_tokens`æ—¶å‡ºç°é”™è¯¯ï¼šå½“æ¨¡å‹è¦ç”Ÿæˆä¸€ä¸ªæ–°çš„`<image>`æˆ–`<fake_token_around_image>`æ ‡è®°æ—¶ï¼Œè€Œæ¨¡å‹æ²¡æœ‰ç”Ÿæˆå›¾åƒæ—¶ï¼Œä¼šå‡ºç°é”™è¯¯ã€‚æ‚¨å¯ä»¥åƒæœ¬æŒ‡å—ä¸­é‚£æ ·å³æ—¶è®¾ç½®å®ƒï¼Œæˆ–è€…åƒæ–‡æœ¬ç”Ÿæˆç­–ç•¥æŒ‡å—ä¸­æè¿°çš„é‚£æ ·å­˜å‚¨åœ¨`GenerationConfig`ä¸­ã€‚

## æç¤ºçš„å›¾åƒå­—å¹•

æ‚¨å¯ä»¥é€šè¿‡æä¾›æ–‡æœ¬æç¤ºæ¥æ‰©å±•å›¾åƒå­—å¹•ï¼Œæ¨¡å‹å°†ç»§ç»­ç»™å‡ºå›¾åƒã€‚è®©æˆ‘ä»¬æ‹¿å¦ä¸€å¼ å›¾ç‰‡æ¥è¯´æ˜ï¼š

![å¤œæ™šçš„åŸƒè²å°”é“å¡”çš„å›¾ç‰‡](img/21827a2f63908e6e4dca537122fd4df7.png)

ç…§ç‰‡ç”±[Denys Nevozhai](https://unsplash.com/@dnevozhai)æ‹æ‘„ã€‚

æ–‡æœ¬å’Œå›¾åƒæç¤ºå¯ä»¥ä½œä¸ºå•ä¸ªåˆ—è¡¨ä¼ é€’ç»™æ¨¡å‹çš„å¤„ç†å™¨ï¼Œä»¥åˆ›å»ºé€‚å½“çš„è¾“å…¥ã€‚

```py
>>> prompt = [
...     "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
...     "This is an image of ",
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
This is an image of the Eiffel Tower in Paris, France.
```

## å°‘é‡æç¤º

è™½ç„¶ IDEFICS å±•ç¤ºäº†å‡ºè‰²çš„é›¶-shot ç»“æœï¼Œä½†æ‚¨çš„ä»»åŠ¡å¯èƒ½éœ€è¦ä¸€å®šæ ¼å¼çš„å­—å¹•ï¼Œæˆ–è€…ä¼´éšå…¶ä»–é™åˆ¶æˆ–è¦æ±‚ï¼Œå¢åŠ ä»»åŠ¡çš„å¤æ‚æ€§ã€‚å°‘é‡æç¤ºå¯ç”¨äºå¯ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡åœ¨æç¤ºä¸­æä¾›ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆç±»ä¼¼äºç»™å®šç¤ºä¾‹æ ¼å¼çš„ç»“æœã€‚

è®©æˆ‘ä»¬ä»¥åŸƒè²å°”é“å¡”çš„ä¸Šä¸€å¼ å›¾ç‰‡ä½œä¸ºæ¨¡å‹çš„ç¤ºä¾‹ï¼Œå¹¶æ„å»ºä¸€ä¸ªæç¤ºï¼Œå‘æ¨¡å‹å±•ç¤ºé™¤äº†å­¦ä¹ å›¾åƒä¸­çš„å¯¹è±¡æ˜¯ä»€ä¹ˆä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›è·å¾—ä¸€äº›æœ‰è¶£çš„ä¿¡æ¯ã€‚ç„¶åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ï¼Œå¦‚æœæˆ‘ä»¬å¯ä»¥ä¸ºè‡ªç”±å¥³ç¥åƒçš„å›¾ç‰‡è·å¾—ç›¸åŒçš„å“åº”æ ¼å¼ï¼š

![è‡ªç”±å¥³ç¥åƒçš„å›¾ç‰‡](img/759caa1ce68c4cc710b290421bd9520d.png)

ç…§ç‰‡ç”±[Juan Mayobre](https://unsplash.com/@jmayobres)æ‹æ‘„ã€‚

```py
>>> prompt = ["User:",
...            "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
...            "Describe this image.\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building.\n",
...            "User:",
...            "https://images.unsplash.com/photo-1524099163253-32b7f0256868?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3387&q=80",
...            "Describe this image.\nAssistant:"
...            ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
User: Describe this image.
Assistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building. 
User: Describe this image.
Assistant: An image of the Statue of Liberty. Fun fact: the Statue of Liberty is 151 feet tall.
```

è¯·æ³¨æ„ï¼Œä»…ä»å•ä¸ªç¤ºä¾‹ï¼ˆå³ 1-shotï¼‰ä¸­ï¼Œæ¨¡å‹å·²ç»å­¦ä¼šäº†å¦‚ä½•æ‰§è¡Œä»»åŠ¡ã€‚å¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œè¯·éšæ—¶å°è¯•ä½¿ç”¨æ›´å¤šçš„ç¤ºä¾‹ï¼ˆä¾‹å¦‚ 3-shotï¼Œ5-shot ç­‰ï¼‰ã€‚

## è§†è§‰é—®é¢˜å›ç­”

è§†è§‰é—®é¢˜å›ç­”ï¼ˆVQAï¼‰æ˜¯æ ¹æ®å›¾åƒå›ç­”å¼€æ”¾å¼é—®é¢˜çš„ä»»åŠ¡ã€‚ä¸å›¾åƒå­—å¹•ç±»ä¼¼ï¼Œå®ƒå¯ä»¥ç”¨äºè¾…åŠ©åŠŸèƒ½åº”ç”¨ç¨‹åºï¼Œè¿˜å¯ä»¥ç”¨äºæ•™è‚²ï¼ˆå…³äºè§†è§‰ææ–™çš„æ¨ç†ï¼‰ã€å®¢æˆ·æœåŠ¡ï¼ˆåŸºäºå›¾åƒçš„äº§å“é—®é¢˜ï¼‰å’Œå›¾åƒæ£€ç´¢ã€‚

è®©æˆ‘ä»¬ä¸ºè¿™ä¸ªä»»åŠ¡è·å–ä¸€å¼ æ–°çš„å›¾ç‰‡ï¼š

![ä¸€å¯¹æ­£åœ¨é‡é¤çš„å¤«å¦‡çš„å›¾ç‰‡](img/d3e04f9a26ae586b92097eb4396b1db0.png)

ç…§ç‰‡ç”±[Jarritos Mexican Soda](https://unsplash.com/@jarritos)æ‹æ‘„ã€‚

æ‚¨å¯ä»¥é€šè¿‡é€‚å½“çš„æŒ‡ç¤ºå°†æ¨¡å‹ä»å›¾åƒå­—å¹•è½¬å‘è§†è§‰é—®é¢˜å›ç­”ï¼š

```py
>>> prompt = [
...     "Instruction: Provide an answer to the question. Use the image to answer.\n",
...     "https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...     "Question: Where are these people and what's the weather like? Answer:"
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
Instruction: Provide an answer to the question. Use the image to answer.
 Question: Where are these people and what's the weather like? Answer: They're in a park in New York City, and it's a beautiful day.
```

## å›¾åƒåˆ†ç±»

IDEFICS èƒ½å¤Ÿå°†å›¾åƒåˆ†ç±»ä¸ºä¸åŒçš„ç±»åˆ«ï¼Œè€Œæ— éœ€æ˜ç¡®åœ¨åŒ…å«æ¥è‡ªè¿™äº›ç‰¹å®šç±»åˆ«çš„æ ‡è®°ç¤ºä¾‹çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç»™å®šä¸€ç»„ç±»åˆ«å¹¶åˆ©ç”¨å…¶å›¾åƒå’Œæ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œæ¨¡å‹å¯ä»¥æ¨æ–­å›¾åƒå¯èƒ½å±äºå“ªä¸ªç±»åˆ«ã€‚

å‡è®¾æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªè”¬èœæ‘Šçš„å›¾ç‰‡ï¼š

![è”¬èœæ‘Šçš„å›¾ç‰‡](img/0f1d778c8084e2f63cd81d21fdb55d1b.png)

ç…§ç‰‡ç”±[Peter Wendt](https://unsplash.com/@peterwendt)æ‹æ‘„ã€‚

æˆ‘ä»¬å¯ä»¥æŒ‡ç¤ºæ¨¡å‹å°†å›¾åƒåˆ†ç±»ä¸ºæˆ‘ä»¬æ‹¥æœ‰çš„ç±»åˆ«ä¹‹ä¸€ï¼š

```py
>>> categories = ['animals','vegetables', 'city landscape', 'cars', 'office']
>>> prompt = [f"Instruction: Classify the following image into a single category from the following list: {categories}.\n",
...     "https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",    
...     "Category: "
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
Instruction: Classify the following image into a single category from the following list: ['animals', 'vegetables', 'city landscape', 'cars', 'office'].
Category: Vegetables
```

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æŒ‡ç¤ºæ¨¡å‹å°†å›¾åƒåˆ†ç±»ä¸ºå•ä¸ªç±»åˆ«ï¼Œä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥æç¤ºæ¨¡å‹è¿›è¡Œæ’ååˆ†ç±»ã€‚

## å›¾åƒå¼•å¯¼çš„æ–‡æœ¬ç”Ÿæˆ

å¯¹äºæ›´æœ‰åˆ›æ„çš„åº”ç”¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºäºå›¾åƒçš„æ–‡æœ¬ç”Ÿæˆæ¥æ ¹æ®å›¾åƒç”Ÿæˆæ–‡æœ¬ã€‚è¿™å¯ä»¥ç”¨äºåˆ›å»ºäº§å“æè¿°ã€å¹¿å‘Šã€åœºæ™¯æè¿°ç­‰ã€‚

è®©æˆ‘ä»¬æç¤º IDEFICS æ ¹æ®ä¸€æ‰‡çº¢é—¨çš„ç®€å•å›¾åƒæ’°å†™ä¸€ä¸ªæ•…äº‹ï¼š

![ä¸€æ‰‡çº¢é—¨ä¸Šæœ‰ä¸€ä¸ªå—ç“œçš„å›¾ç‰‡](img/6e5de95de1888a89f56e9744b5af7215.png)

ç…§ç‰‡ç”±[Craig Tidball](https://unsplash.com/@devonshiremedia)æä¾›ã€‚

```py
>>> prompt = ["Instruction: Use the image to write a story. \n",
...     "https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2203&q=80",
...     "Story: \n"]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, num_beams=2, max_new_tokens=200, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0]) 
Instruction: Use the image to write a story. 
 Story: 
Once upon a time, there was a little girl who lived in a house with a red door.  She loved her red door.  It was the prettiest door in the whole world.

One day, the little girl was playing in her yard when she noticed a man standing on her doorstep.  He was wearing a long black coat and a top hat.

The little girl ran inside and told her mother about the man.

Her mother said, â€œDonâ€™t worry, honey.  Heâ€™s just a friendly ghost.â€

The little girl wasnâ€™t sure if she believed her mother, but she went outside anyway.

When she got to the door, the man was gone.

The next day, the little girl was playing in her yard again when she noticed the man standing on her doorstep.

He was wearing a long black coat and a top hat.

The little girl ran
```

çœ‹èµ·æ¥ IDEFICS æ³¨æ„åˆ°äº†é—¨å»Šä¸Šçš„å—ç“œï¼Œå¹¶é€‰æ‹©äº†ä¸€ä¸ªå…³äºé¬¼é­‚çš„ææ€–ä¸‡åœ£èŠ‚æ•…äº‹ã€‚

å¯¹äºåƒè¿™æ ·çš„è¾ƒé•¿è¾“å‡ºï¼Œæ‚¨å°†å—ç›Šäºè°ƒæ•´æ–‡æœ¬ç”Ÿæˆç­–ç•¥ã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨æ˜¾ç€æé«˜ç”Ÿæˆè¾“å‡ºçš„è´¨é‡ã€‚æŸ¥çœ‹æ–‡æœ¬ç”Ÿæˆç­–ç•¥ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚

## æ‰¹é‡æ¨¡å¼ä¸‹è¿è¡Œæ¨ç†

ä¹‹å‰çš„æ‰€æœ‰éƒ¨åˆ†éƒ½å±•ç¤ºäº† IDEFICS çš„ä¸€ä¸ªç¤ºä¾‹ã€‚ä»¥éå¸¸ç›¸ä¼¼çš„æ–¹å¼ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¼ é€’æç¤ºåˆ—è¡¨æ¥ä¸ºä¸€æ‰¹ç¤ºä¾‹è¿è¡Œæ¨ç†ï¼š

```py
>>> prompts = [
...     [   "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
...         "This is an image of ",
...     ],
...     [   "https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...         "This is an image of ",
...     ],
...     [   "https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...         "This is an image of ",
...     ],
... ]

>>> inputs = processor(prompts, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> for i,t in enumerate(generated_text):
...     print(f"{i}:\n{t}\n") 
0:
This is an image of the Eiffel Tower in Paris, France.

1:
This is an image of a couple on a picnic blanket.

2:
This is an image of a vegetable stand.
```

## ç”¨äºä¼šè¯ä½¿ç”¨çš„ IDEFICS æŒ‡å¯¼

å¯¹äºä¼šè¯ä½¿ç”¨æƒ…å†µï¼Œæ‚¨å¯ä»¥åœ¨ğŸ¤— Hub ä¸Šæ‰¾åˆ°æ¨¡å‹çš„ç»è¿‡å¾®è°ƒçš„æŒ‡å¯¼ç‰ˆæœ¬ï¼š`HuggingFaceM4/idefics-80b-instruct`å’Œ`HuggingFaceM4/idefics-9b-instruct`ã€‚

è¿™äº›æ£€æŸ¥ç‚¹æ˜¯åœ¨æ··åˆç›‘ç£å’ŒæŒ‡å¯¼å¾®è°ƒæ•°æ®é›†ä¸Šå¯¹å„è‡ªåŸºæœ¬æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ç»“æœï¼Œè¿™å¯ä»¥æé«˜ä¸‹æ¸¸æ€§èƒ½ï¼ŒåŒæ—¶ä½¿æ¨¡å‹åœ¨ä¼šè¯è®¾ç½®ä¸­æ›´æ˜“äºä½¿ç”¨ã€‚

ä¼šè¯ä½¿ç”¨å’Œæç¤ºä¸ä½¿ç”¨åŸºæœ¬æ¨¡å‹éå¸¸ç›¸ä¼¼ï¼š

```py
>>> import torch
>>> from transformers import IdeficsForVisionText2Text, AutoProcessor

>>> device = "cuda" if torch.cuda.is_available() else "cpu"

>>> checkpoint = "HuggingFaceM4/idefics-9b-instruct"
>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)
>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> prompts = [
...     [
...         "User: What is in this image?",
...         "https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG",
...         "<end_of_utterance>",

...         "\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>",

...         "\nUser:",
...         "https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052",
...         "And who is that?<end_of_utterance>",

...         "\nAssistant:",
...     ],
... ]

>>> # --batched mode
>>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors="pt").to(device)
>>> # --single sample mode
>>> # inputs = processor(prompts[0], return_tensors="pt").to(device)

>>> # Generation args
>>> exit_condition = processor.tokenizer("<end_of_utterance>", add_special_tokens=False).input_ids
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> for i, t in enumerate(generated_text):
...     print(f"{i}:\n{t}\n")
```
