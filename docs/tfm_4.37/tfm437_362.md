# OWL-ViT

> 原文：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/owlvit`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/owlvit)

## 概述

OWL-ViT（Vision Transformer for Open-World Localization）是由 Matthias Minderer、Alexey Gritsenko、Austin Stone、Maxim Neumann、Dirk Weissenborn、Alexey Dosovitskiy、Aravindh Mahendran、Anurag Arnab、Mostafa Dehghani、Zhuoran Shen、Xiao Wang、Xiaohua Zhai、Thomas Kipf 和 Neil Houlsby 在[Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230)中提出的。OWL-ViT 是一个在各种（图像，文本）对上训练的开放词汇目标检测网络。它可以用于使用一个或多个文本查询查询图像，以搜索和检测文本中描述的目标对象。

来自论文的摘要如下：

*将简单的架构与大规模预训练相结合，已经在图像分类方面取得了巨大的改进。对于目标检测，预训练和扩展方法尚未建立良好的基础，特别是在长尾和开放词汇设置中，训练数据相对稀缺的情况下。在本文中，我们提出了一个强大的配方，将图像文本模型转移到开放词汇的目标检测中。我们使用标准的 Vision Transformer 架构进行最小修改，对比图像文本预训练，并进行端到端的检测微调。我们对这一设置的扩展属性进行了分析，结果表明增加图像级别的预训练和模型大小可以在下游检测任务中获得一致的改进。我们提供了适应策略和规范化，以实现零样本文本条件和一次样本图像条件的目标检测的非常强大的性能。代码和模型可在 GitHub 上获得。*

![drawing](img/7088942c2120476db885ea27155f090f.png) OWL-ViT 架构。摘自[原始论文](https://arxiv.org/abs/2205.06230)。

此模型由[adirik](https://huggingface.co/adirik)贡献。原始代码可在[此处](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)找到。

## 使用提示

OWL-ViT 是一个零样本文本条件的目标检测模型。OWL-ViT 使用 CLIP 作为其多模态骨干，具有类似 ViT 的 Transformer 来获取视觉特征和因果语言模型来获取文本特征。为了使用 CLIP 进行检测，OWL-ViT 移除了视觉模型的最终令牌池化层，并将轻量级分类和框头附加到每个 Transformer 输出令牌上。通过用从文本模型获得的类名嵌入替换固定的分类层权重，实现了开放词汇分类。作者首先从头开始训练 CLIP，然后在标准检测数据集上使用二部匹配损失对其进行端到端的微调，包括分类和框头。可以使用一个或多个文本查询来执行零样本文本条件的目标检测。

OwlViTImageProcessor 可用于调整（或重新缩放）和规范化模型的图像，而 CLIPTokenizer 用于对文本进行编码。OwlViTProcessor 将 OwlViTImageProcessor 和 CLIPTokenizer 包装成一个单一实例，用于同时对文本进行编码和准备图像。以下示例展示了如何使用 OwlViTProcessor 和 OwlViTForObjectDetection 执行目标检测。

```py
>>> import requests
>>> from PIL import Image
>>> import torch

>>> from transformers import OwlViTProcessor, OwlViTForObjectDetection

>>> processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
>>> model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = [["a photo of a cat", "a photo of a dog"]]
>>> inputs = processor(text=texts, images=image, return_tensors="pt")
>>> outputs = model(**inputs)

>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
>>> target_sizes = torch.Tensor([image.size[::-1]])
>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)
>>> i = 0  # Retrieve predictions for the first image for the corresponding text queries
>>> text = texts[i]
>>> boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]
>>> for box, score, label in zip(boxes, scores, labels):
...     box = [round(i, 2) for i in box.tolist()]
...     print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")
Detected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]
Detected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]
```

## 资源

可以在[这里](https://github.com/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)找到使用 OWL-ViT 进行零样本和一样本（图像引导）目标检测的演示笔记本。

## OwlViTConfig

### `class transformers.OwlViTConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/configuration_owlvit.py#L258)

```py
( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 return_dict = True **kwargs )
```

参数

+   `text_config` (`dict`, *optional*) — 用于初始化 OwlViTTextConfig 的配置选项字典。

+   `vision_config` (`dict`, *optional*) — 用于初始化 OwlViTVisionConfig 的配置选项字典。

+   `projection_dim` (`int`, *optional*, 默认为 512) — 文本和视觉投影层的维度。

+   `logit_scale_init_value` (`float`, *optional*, 默认为 2.6592) — *logit_scale* 参数的初始值。默认值根据原始的 OWL-ViT 实现而定。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 模型是否应返回一个字典。如果为 `False`，则返回一个元组。

+   `kwargs` (*optional*) — 关键字参数的字典。

OwlViTConfig 是用于存储 OwlViTModel 配置的配置类。根据指定的参数实例化 OWL-ViT 模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生类似于 OWL-ViT [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

#### `from_text_vision_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/configuration_owlvit.py#L327)

```py
( text_config: Dict vision_config: Dict **kwargs ) → export const metadata = 'undefined';OwlViTConfig
```

返回

OwlViTConfig

配置对象的一个实例

从 owlvit 文本模型配置和 owlvit 视觉模型配置实例化一个 OwlViTConfig（或派生类）。

## OwlViTTextConfig

### `class transformers.OwlViTTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/configuration_owlvit.py#L40)

```py
( vocab_size = 49408 hidden_size = 512 intermediate_size = 2048 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 16 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 0 bos_token_id = 49406 eos_token_id = 49407 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 49408) — OWL-ViT 文本模型的词汇量。定义了在调用 OwlViTTextModel 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为 512) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, 默认为 2048) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 8) — Transformer 编码器中每个注意力层的注意力头数。

+   `max_position_embeddings` (`int`, *optional*, 默认为 16) — 该模型可能使用的最大序列长度。通常设置为一个较大的值（例如 512、1024 或 2048）以防万一。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，则支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢失比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, defaults to 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1，用于内部初始化测试）。

+   `pad_token_id` (`int`, *optional*, defaults to 0) — 输入序列中填充标记的 id。

+   `bos_token_id` (`int`, *optional*, defaults to 49406) — 输入序列中起始标记的 id。

+   `eos_token_id` (`int`, *optional*, defaults to 49407) — 输入序列中终止标记的 id。

这是用于存储 OwlViTTextModel 配置的配置类。根据指定的参数实例化一个 OwlViT 文本编码器，定义模型架构。使用默认值实例化配置将产生与 OwlViT [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)架构类似的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import OwlViTTextConfig, OwlViTTextModel

>>> # Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration
>>> configuration = OwlViTTextConfig()

>>> # Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration
>>> model = OwlViTTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## OwlViTVisionConfig

### `class transformers.OwlViTVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/configuration_owlvit.py#L153)

```py
( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 768 patch_size = 32 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入图像中的通道数。

+   `image_size` (`int`, *optional*, defaults to 768) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 32) — 每个补丁的大小（分辨率）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，则支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢失比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, defaults to 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1，用于内部初始化测试）。

这是一个配置类，用于存储 OwlViTVisionModel 的配置。它用于根据指定的参数实例化一个 OWL-ViT 图像编码器，定义模型架构。使用默认值实例化配置将产生与 OWL-ViT [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)架构类似的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import OwlViTVisionConfig, OwlViTVisionModel

>>> # Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration
>>> configuration = OwlViTVisionConfig()

>>> # Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration
>>> model = OwlViTVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## OwlViTImageProcessor

### `class transformers.OwlViTImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/image_processing_owlvit.py#L91)

```py
( do_resize = True size = None resample = <Resampling.BICUBIC: 3> do_center_crop = False crop_size = None do_rescale = True rescale_factor = 0.00392156862745098 do_normalize = True image_mean = None image_std = None **kwargs )
```

参数

+   `do_resize` (`bool`，*可选*，默认为`True`) — 是否将输入的较短边调整为特定的`size`。

+   `size` (`Dict[str, int]`，*可选*，默认为{“height” — 768, “width”: 768})：用于调整图像大小的大小。仅在`do_resize`设置为`True`时有效。如果`size`是一个类似(h, w)的序列，输出大小将与之匹配。如果`size`是一个整数，则图像将被调整为(size, size)。

+   `resample` (`int`，*可选*，默认为`Resampling.BICUBIC`) — 可选的重采样滤波器。可以是`PIL.Image.Resampling.NEAREST`、`PIL.Image.Resampling.BOX`、`PIL.Image.Resampling.BILINEAR`、`PIL.Image.Resampling.HAMMING`、`PIL.Image.Resampling.BICUBIC`或`PIL.Image.Resampling.LANCZOS`之一。仅在`do_resize`设置为`True`时有效。

+   `do_center_crop` (`bool`，*可选*，默认为`False`) — 是否在中心裁剪输入。如果输入大小在任何边缘上小于`crop_size`，则图像将填充 0，然后进行中心裁剪。

+   `crop_size` (`int`，*可选*，默认为{“height” — 768, “width”: 768})：用于中心裁剪图像的大小。仅在`do_center_crop`设置为`True`时有效。

+   `do_rescale` (`bool`，*可选*，默认为`True`) — 是否按一定因子重新缩放输入。

+   `rescale_factor` (`float`，*可选*，默认为`1/255`) — 用于重新缩放图像的因子。仅在`do_rescale`设置为`True`时有效。

+   `do_normalize` (`bool`，*可选*，默认为`True`) — 是否使用`image_mean`和`image_std`对输入进行归一化。在应用中心裁剪时的期望输出大小。仅在`do_center_crop`设置为`True`时有效。

+   `image_mean` (`List[int]`，*可选*，默认为`[0.48145466, 0.4578275, 0.40821073]`) — 每个通道的均值序列，用于归一化图像时使用。

+   `image_std` (`List[int]`，*可选*，默认为`[0.26862954, 0.26130258, 0.27577711]`) — 每个通道的标准差序列，用于归一化图像时使用。

构建一个 OWL-ViT 图像处理器。

这个图像处理器继承自 ImageProcessingMixin，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/image_processing_owlvit.py#L270)

```py
( images: Union do_resize: Optional = None size: Optional = None resample: Resampling = None do_center_crop: Optional = None crop_size: Optional = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要准备的图像或图像批次。期望单个或批量像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置`do_rescale=False`。

+   `do_resize` (`bool`，*可选*，默认为`self.do_resize`) — 是否调整输入大小。如果为`True`，将输入调整为`size`指定的大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 调整输入大小的大小。仅在 `do_resize` 设置为 `True` 时有效。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `self.resample`) — 调整输入大小时使用的重采样滤波器。仅在 `do_resize` 设置为 `True` 时有效。

+   `do_center_crop` (`bool`, *可选*, 默认为 `self.do_center_crop`) — 是否对输入进行中心裁剪。如果为 `True`，将对输入进行中心裁剪，裁剪到由 `crop_size` 指定的大小。

+   `crop_size` (`Dict[str, int]`, *可选*, 默认为 `self.crop_size`) — 中心裁剪输入的大小。仅在 `do_center_crop` 设置为 `True` 时有效。

+   `do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否对输入进行重新缩放。如果为 `True`，将通过除以 `rescale_factor` 对输入进行重新缩放。

+   `rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 重新缩放输入的因子。仅在 `do_rescale` 设置为 `True` 时有效。

+   `do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对输入进行归一化。如果为 `True`，将通过减去 `image_mean` 并除以 `image_std` 对输入进行归一化。

+   `image_mean` (`Union[float, List[float]]`, *可选*, 默认为 `self.image_mean`) — 在归一化时从输入中减去的均值。仅在 `do_normalize` 设置为 `True` 时有效。

+   `image_std` (`Union[float, List[float]]`, *可选*, 默认为 `self.image_std`) — 在归一化时除以输入的标准差。仅在 `do_normalize` 设置为 `True` 时有效。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`：返回类型为 `tf.Tensor` 的批处理。

    +   `TensorType.PYTORCH` 或 `'pt'`：返回类型为 `torch.Tensor` 的批处理。

    +   `TensorType.NUMPY` 或 `'np'`：返回类型为 `np.ndarray` 的批处理。

    +   `TensorType.JAX` 或 `'jax'`：返回类型为 `jax.numpy.ndarray` 的批处理。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`：图像以 (通道数, 高度, 宽度) 格式。

    +   `ChannelDimension.LAST`：图像以 (高度, 宽度, 通道数) 格式。

    +   未设置：默认为输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以 (高度, 宽度, 通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`：图像以 (高度, 宽度) 格式。

为模型准备一张图像或一批图像。

#### `post_process_object_detection`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/image_processing_owlvit.py#L459)

```py
( outputs threshold: float = 0.1 target_sizes: Union = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`OwlViTObjectDetectionOutput`) — 模型的原始输出。

+   `threshold` (`float`, *可选*) — 保留对象检测预测的分数阈值。

+   `target_sizes` (`torch.Tensor` 或 `List[Tuple[int, int]]`, *可选*) — 形状为 `(batch_size, 2)` 的张量或包含每个图像批次中目标大小 `(高度, 宽度)` 的元组列表 (`Tuple[int, int]`)。如果未设置，预测将不会被调整大小。

返回

`List[Dict]`

一个字典列表，每个字典包含模型预测的图像批次中每个图像的分数、标签和框。

将 OwlViTForObjectDetection 的原始输出转换为最终边界框，格式为 (左上角 x 坐标, 左上角 y 坐标, 右下角 x 坐标, 右下角 y 坐标)。

#### `post_process_image_guided_detection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/image_processing_owlvit.py#L515)

```py
( outputs threshold = 0.0 nms_threshold = 0.3 target_sizes = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`OwlViTImageGuidedObjectDetectionOutput`) — 模型的原始输出。

+   `threshold` (`float`, *可选*, 默认为 0.0) — 用于过滤预测框的最小置信度阈值。

+   `nms_threshold` (`float`, *可选*, 默认为 0.3) — 用于非极大值抑制重叠框的 IoU 阈值。

+   `target_sizes` (`torch.Tensor`, *可选*) — 形状为(batch_size, 2)的张量，其中每个条目是批次中相应图像的(高度，宽度)。如果设置，预测的归一化边界框将重新缩放为目标大小。如果保持为 None，则预测不会被取消归一化。

返回

`List[Dict]`

一个字典列表，每个字典包含模型预测的批次中每张图像的分数、标签和框。所有标签都设置为 None，因为`OwlViTForObjectDetection.image_guided_detection`执行一次性目标检测。

将 OwlViTForObjectDetection.image_guided_detection()的输出转换为 COCO api 所期望的格式。

## OwlViTFeatureExtractor

### `class transformers.OwlViTFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/feature_extraction_owlvit.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

预处理一张图像或一批图像。

#### `post_process`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/image_processing_owlvit.py#L413)

```py
( outputs target_sizes ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`OwlViTObjectDetectionOutput`) — 模型的原始输出。

+   `target_sizes` (`torch.Tensor`的形状为`(batch_size, 2)`) — 包含批次中每个图像的大小(h, w)的张量。对于评估，这必须是原始图像大小(在任何数据增强之前)。对于可视化，这应该是数据增强后的图像大小，但在填充之前。

返回

`List[Dict]`

一个字典列表，每个字典包含模型预测的批次中每张图像的分数、标签和框。

将 OwlViTForObjectDetection 的原始输出转换为最终的边界框格式为(top_left_x, top_left_y, bottom_right_x, bottom_right_y)。

#### `post_process_image_guided_detection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/image_processing_owlvit.py#L515)

```py
( outputs threshold = 0.0 nms_threshold = 0.3 target_sizes = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`OwlViTImageGuidedObjectDetectionOutput`) — 模型的原始输出。

+   `threshold` (`float`, *可选*, 默认为 0.0) — 用于过滤预测框的最小置信度阈值。

+   `nms_threshold` (`float`, *可选*, 默认为 0.3) — 用于非极大值抑制重叠框的 IoU 阈值。

+   `target_sizes` (`torch.Tensor`, *可选*) — 形状为(batch_size, 2)的张量，其中每个条目是批次中相应图像的(高度，宽度)。如果设置，预测的归一化边界框将重新缩放为目标大小。如果保持为 None，则预测不会被取消归一化。

返回

`List[Dict]`

一个字典列表，每个字典包含模型预测的批次中每张图像的分数、标签和框。所有标签都设置为 None，因为`OwlViTForObjectDetection.image_guided_detection`执行一次性目标检测。

将 OwlViTForObjectDetection.image_guided_detection()的输出转换为 COCO api 所期望的格式。

## OwlViTProcessor

### `class transformers.OwlViTProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/processing_owlvit.py#L29)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor`（OwlViTImageProcessor，*可选*）— 图像处理器是必需的输入。

+   `tokenizer`（[`CLIPTokenizer`, `CLIPTokenizerFast`]，*可选*）— 分词器是必需的输入。

构建一个 OWL-ViT 处理器，将 OwlViTImageProcessor 和 CLIPTokenizer/CLIPTokenizerFast 包装成一个单一处理器，继承了图像处理器和分词器的功能。查看`__call__()`和 decode()以获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/processing_owlvit.py#L197)

```py
( *args **kwargs )
```

该方法将其所有参数转发到 CLIPTokenizerFast 的 batch_decode()。有关更多信息，请参考此方法的文档字符串。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/processing_owlvit.py#L204)

```py
( *args **kwargs )
```

该方法将其所有参数转发到 CLIPTokenizerFast 的 decode()。有关更多信息，请参考此方法的文档字符串。

#### `post_process`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/processing_owlvit.py#L176)

```py
( *args **kwargs )
```

该方法将其所有参数转发到 OwlViTImageProcessor.post_process()。有关更多信息，请参考此方法的文档字符串。

#### `post_process_image_guided_detection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/processing_owlvit.py#L190)

```py
( *args **kwargs )
```

该方法将其所有参数转发到`OwlViTImageProcessor.post_process_one_shot_object_detection`。有关更多信息，请参考此方法的文档字符串。

#### `post_process_object_detection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/processing_owlvit.py#L183)

```py
( *args **kwargs )
```

该方法将其所有参数转发到 OwlViTImageProcessor.post_process_object_detection()。有关更多信息，请参考此方法的文档字符串。

## OwlViTModel

### `class transformers.OwlViTModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L999)

```py
( config: OwlViTConfig )
```

参数

+   `config`（OwlViTConfig）— 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

该模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L1115)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_base_image_embeds: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.owlvit.modeling_owlvit.OwlViTOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。什么是输入 ID？

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   1 表示`未被 masked`的标记，

    +   0 表示被`masked`的标记。什么是注意力掩码？

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。

+   `return_loss` (`bool`，*可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.owlvit.modeling_owlvit.OwlViTOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.owlvit.modeling_owlvit.OwlViTOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（`<class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'>`）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当`return_loss`为`True`时返回) — 图像-文本相似性的对比损失。

+   `logits_per_image` (`torch.FloatTensor`，形状为`(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text` (`torch.FloatTensor`，形状为`(text_batch_size, image_batch_size)`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds` (`torch.FloatTensor`，形状为`(batch_size * num_max_text_queries, output_dim`) — 通过将投影层应用于 OwlViTTextModel 的汇聚输出获得的文本嵌入。

+   `image_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将投影层应用于 OwlViTVisionModel 的汇聚输出获得的图像嵌入。

+   `text_model_output` (Tuple`BaseModelOutputWithPooling`) — OwlViTTextModel 的输出。

+   `vision_model_output` (`BaseModelOutputWithPooling`) — OwlViTVisionModel 的输出。

OwlViTModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, OwlViTModel

>>> model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L1035)

```py
( input_ids: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size * num_max_text_queries, sequence_length)`) — 词汇表中输入序列标记的索引。可以使用 AutoTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。什么是输入 ID？

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, num_max_text_queries, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。什么是注意力掩码？

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

text_features (`torch.FloatTensor`，形状为`(batch_size, output_dim)`)

通过将投影层应用于 OwlViTTextModel 的池化输出获得的文本嵌入。

OwlViTModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, OwlViTModel

>>> model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
>>> inputs = processor(
...     text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
... )
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L1070)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

image_features (`torch.FloatTensor`，形状为`(batch_size, output_dim)`)

通过将投影层应用于 OwlViTVisionModel 的池化输出获得的图像嵌入。

OwlViTModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, OwlViTModel

>>> model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(images=image, return_tensors="pt")
>>> image_features = model.get_image_features(**inputs)
```

## OwlViTTextModel

### `class transformers.OwlViTTextModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L839)

```py
( config: OwlViTTextConfig )
```

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L854)

```py
( input_ids: Tensor attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size * num_max_text_queries, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, num_max_text_queries, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。什么是注意力掩码？

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）— 模型最后一层的隐藏状态序列输出。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）— 经过用于辅助预训练任务的层进一步处理后的序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这返回经过线性层和 tanh 激活函数处理后的分类标记。线性层权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为一个+每个层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

OwlViTTextModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, OwlViTTextModel

>>> model = OwlViTTextModel.from_pretrained("google/owlvit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
>>> inputs = processor(
...     text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
... )
>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## OwlViTVisionModel

### `class transformers.OwlViTVisionModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L949)

```py
( config: OwlViTVisionConfig )
```

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L962)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 ModelOutput 而不是一个普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置(`<class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'>`)和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 序列中第一个标记（分类标记）的最后一层隐藏状态，在通过用于辅助预训练任务的层进一步处理后。例如，对于 BERT 系列模型，这返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出处的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

OwlViTVisionModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, OwlViTVisionModel

>>> model = OwlViTVisionModel.from_pretrained("google/owlvit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```

## OwlViT 目标检测

### `class transformers.OwlViTForObjectDetection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L1282)

```py
( config: OwlViTConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L1584)

```py
( input_ids: Tensor pixel_values: FloatTensor attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`）— 像素值。

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size * num_max_text_queries, sequence_length)`，*可选*）— 输入序列标记在词汇表中的索引。可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。什么是输入 ID？。

+   `attention_mask`（`torch.Tensor`，形状为`(batch_size, num_max_text_queries, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。什么是注意力掩码？

+   `output_hidden_states`（`bool`，*可选*）— 是否返回最后一个隐藏状态。有关更多详细信息，请参阅返回的张量中的`text_model_last_hidden_state`和`vision_model_last_hidden_state`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通的元组。

返回

`transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（`<class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'>`）和输入的不同元素。

+   `loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`labels`时返回）— 总损失作为类别预测的负对数似然（交叉熵）和边界框损失的线性组合。后者被定义为 L1 损失和广义尺度不变 IoU 损失的线性组合。

+   `loss_dict`（`Dict`，*可选*）— 包含各个损失的字典。用于记录日志。

+   `logits`（`torch.FloatTensor`，形状为`(batch_size, num_patches, num_queries)`）— 所有查询的分类 logits（包括无对象）。

+   `pred_boxes`（`torch.FloatTensor`，形状为`(batch_size, num_patches, 4)`）— 所有查询的标准化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内标准化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未标准化的边界框。

+   `text_embeds` (`torch.FloatTensor`，形状为`(batch_size, num_max_text_queries, output_dim`) — 通过将池化输出应用于 OwlViTTextModel 的文本嵌入。

+   `image_embeds` (`torch.FloatTensor`，形状为`(batch_size, patch_size, patch_size, output_dim`) — OwlViTVisionModel 的池化输出。OWL-ViT 将图像表示为一组图像补丁，并为每个补丁计算图像嵌入。

+   `class_embeds` (`torch.FloatTensor`，形状为`(batch_size, num_patches, hidden_size)`) — 所有图像补丁的类嵌入。OWL-ViT 将图像表示为一组图像补丁，其中总补丁数为（图像大小/补丁大小）**2。

+   `text_model_output` (Tuple`BaseModelOutputWithPooling`) — OwlViTTextModel 的输出。

+   `vision_model_output` (`BaseModelOutputWithPooling`) — OwlViTVisionModel 的输出。

OwlViTForObjectDetection 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import requests
>>> from PIL import Image
>>> import torch
>>> from transformers import AutoProcessor, OwlViTForObjectDetection

>>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
>>> model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = [["a photo of a cat", "a photo of a dog"]]
>>> inputs = processor(text=texts, images=image, return_tensors="pt")
>>> outputs = model(**inputs)

>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
>>> target_sizes = torch.Tensor([image.size[::-1]])
>>> # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores
>>> results = processor.post_process_object_detection(
...     outputs=outputs, threshold=0.1, target_sizes=target_sizes
... )

>>> i = 0  # Retrieve predictions for the first image for the corresponding text queries
>>> text = texts[i]
>>> boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]

>>> for box, score, label in zip(boxes, scores, labels):
...     box = [round(i, 2) for i in box.tolist()]
...     print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")
Detected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]
Detected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]
```

#### `image_guided_detection`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/owlvit/modeling_owlvit.py#L1489)

```py
( pixel_values: FloatTensor query_pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.owlvit.modeling_owlvit.OwlViTImageGuidedObjectDetectionOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。

+   `query_pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 要检测的查询图像的像素值。每个目标图像传入一个查询图像。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.owlvit.modeling_owlvit.OwlViTImageGuidedObjectDetectionOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.owlvit.modeling_owlvit.OwlViTImageGuidedObjectDetectionOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（`<class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'>`）和输入。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, num_patches, num_queries)`) — 所有查询的分类 logits（包括无对象）。

+   `target_pred_boxes` (`torch.FloatTensor`，形状为`(batch_size, num_patches, 4)`) — 所有查询的标准化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内标准化，相对于批处理中每个单独目标图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未标准化的边界框。

+   `query_pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`) — 所有查询的归一化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独查询图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未归一化的边界框。

+   `image_embeds` (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`) — OwlViTVisionModel 的汇集输出。OWL-ViT 将图像表示为一组图像补丁，并为每个补丁计算图像嵌入。

+   `query_image_embeds` (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`) — OwlViTVisionModel 的汇集输出。OWL-ViT 将图像表示为一组图像补丁，并为每个补丁计算图像嵌入。

+   `class_embeds` (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`) — 所有图像补丁的类嵌入。OWL-ViT 将图像表示为一组图像补丁，其中补丁的总数为（图像大小/补丁大小）**2。

+   `text_model_output` (Tuple`BaseModelOutputWithPooling`) — OwlViTTextModel 的输出。

+   `vision_model_output` (`BaseModelOutputWithPooling`) — OwlViTVisionModel 的输出。

OwlViTForObjectDetection 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import requests
>>> from PIL import Image
>>> import torch
>>> from transformers import AutoProcessor, OwlViTForObjectDetection

>>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch16")
>>> model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch16")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> query_url = "http://images.cocodataset.org/val2017/000000001675.jpg"
>>> query_image = Image.open(requests.get(query_url, stream=True).raw)
>>> inputs = processor(images=image, query_images=query_image, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model.image_guided_detection(**inputs)
>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
>>> target_sizes = torch.Tensor([image.size[::-1]])
>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> results = processor.post_process_image_guided_detection(
...     outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes
... )
>>> i = 0  # Retrieve predictions for the first image
>>> boxes, scores = results[i]["boxes"], results[i]["scores"]
>>> for box, score in zip(boxes, scores):
...     box = [round(i, 2) for i in box.tolist()]
...     print(f"Detected similar object with confidence {round(score.item(), 3)} at location {box}")
Detected similar object with confidence 0.856 at location [10.94, 50.4, 315.8, 471.39]
Detected similar object with confidence 1.0 at location [334.84, 25.33, 636.16, 374.71]
```
