# SeamlessM4T-v2

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t_v2`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t_v2)

## 概述

SeamlessM4T-v2 模型是由 Meta AI 的 Seamless Communication 团队在[Seamless: Multilingual Expressive and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/)中提出的。

SeamlessM4T-v2 是一系列旨在提供高质量翻译的模型，使来自不同语言社区的人们能够通过语音和文本轻松交流。这是对[先前版本](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t)的改进。有关 v1 和 v2 之间的差异的更多详细信息，请参阅与 SeamlessM4T-v1 的差异部分。

SeamlessM4T-v2 实现多个任务，无需依赖单独的模型：

+   语音到语音翻译（S2ST）

+   语音到文本翻译（S2TT）

+   文本到语音翻译（T2ST）

+   文本到文本翻译（T2TT）

+   自动语音识别（ASR）

SeamlessM4Tv2Model 可以执行上述所有任务，但每个任务也有自己专用的子模型。

论文摘要如下：

*自动语音翻译的最新进展已经大大扩展了语言覆盖范围，改进了多模态功能，并实现了广泛的任务和功能。尽管如此，今天的大规模自动语音翻译系统缺乏与人与人对话相比帮助机器中介通信感觉无缝的关键功能。在这项工作中，我们介绍了一系列模型，这些模型能够以流式方式实现端到端的表达和多语言翻译。首先，我们贡献了一个改进版本的大规模多语言和多模态 SeamlessM4T 模型—SeamlessM4T v2。这个更新的模型采用了更新的 UnitY2 框架，训练了更多的低资源语言数据。扩展版本的 SeamlessAlign 增加了 114,800 小时的自动对齐数据，涵盖了 76 种语言。SeamlessM4T v2 为我们的两个最新模型 SeamlessExpressive 和 SeamlessStreaming 提供了基础。SeamlessExpressive 实现了保留语音风格和韵律的翻译。与以往在表达性语音研究中的努力相比，我们的工作解决了一些未充分探索的韵律方面，如语速和停顿，同时也保留了个人声音的风格。至于 SeamlessStreaming，我们的模型利用了高效单调多头注意力（EMMA）机制，生成低延迟的目标翻译，无需等待完整的源话语。作为首创，SeamlessStreaming 实现了多源和目标语言的同时语音到语音/文本翻译。为了了解这些模型的性能，我们结合了新颖和修改过的现有自动度量标准的版本，以评估韵律、延迟和稳健性。对于人类评估，我们改编了现有的旨在衡量保留意义、自然性和表现力最相关属性的协议。为了确保我们的模型可以安全和负责任地使用，我们实施了已知的第一个多模态机器翻译红队行动，一个用于检测和减轻添加毒性的系统，一个系统性评估性别偏见，以及一个设计用于减轻深度伪造影响的不可听见的本地化水印机制。因此，我们将 SeamlessExpressive 和 SeamlessStreaming 的主要组件结合起来，形成了 Seamless，这是第一个公开可用的系统，可以实时解锁具有表现力的跨语言交流。总的来说，Seamless 为我们提供了将通用语音翻译器从科幻概念变成现实技术所需的技术基础的关键视角。最后，这项工作中的贡献，包括模型、代码和水印检测器，已经公开发布并可在下面的链接中访问。*

## 用法

在以下示例中，我们将加载一个阿拉伯语音样本和一个英文文本样本，并将它们转换为俄语语音和法语文本。

首先，加载处理器和模型的检查点：

```py
>>> from transformers import AutoProcessor, SeamlessM4Tv2Model

>>> processor = AutoProcessor.from_pretrained("facebook/seamless-m4t-v2-large")
>>> model = SeamlessM4Tv2Model.from_pretrained("facebook/seamless-m4t-v2-large")
```

您可以无缝地在文本或音频上使用这个模型，生成翻译文本或翻译音频。

以下是如何使用处理器处理文本和音频的方法：

```py
>>> # let's load an audio sample from an Arabic speech corpus
>>> from datasets import load_dataset
>>> dataset = load_dataset("arabic_speech_corpus", split="test", streaming=True)
>>> audio_sample = next(iter(dataset))["audio"]

>>> # now, process it
>>> audio_inputs = processor(audios=audio_sample["array"], return_tensors="pt")

>>> # now, process some English text as well
>>> text_inputs = processor(text = "Hello, my dog is cute", src_lang="eng", return_tensors="pt")
```

### 语音

SeamlessM4Tv2Model 可以*无缝地*生成文本或语音，几乎没有或没有任何变化。让我们以俄语语音翻译为目标：

```py
>>> audio_array_from_text = model.generate(**text_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()
>>> audio_array_from_audio = model.generate(**audio_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()
```

使用基本相同的代码，我已经将英文文本和阿拉伯语音翻译成了俄语语音样本。

### 文本

同样，您可以使用相同的模型从音频文件或文本生成翻译文本。您只需要将`generate_speech=False`传递给 SeamlessM4Tv2Model.generate()。这次，让我们翻译成法语。

```py
>>> # from audio
>>> output_tokens = model.generate(**audio_inputs, tgt_lang="fra", generate_speech=False)
>>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)

>>> # from text
>>> output_tokens = model.generate(**text_inputs, tgt_lang="fra", generate_speech=False)
>>> translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)
```

### 提示

#### 1. 使用专用模型

SeamlessM4Tv2Model 是 transformers 的顶级模型，用于生成语音和文本，但您也可以使用专用模型执行任务而不需要额外组件，从而减少内存占用。例如，您可以用专门用于 S2ST 任务的模型替换音频到音频生成片段，其余代码完全相同：

```py
>>> from transformers import SeamlessM4Tv2ForSpeechToSpeech
>>> model = SeamlessM4Tv2ForSpeechToSpeech.from_pretrained("facebook/seamless-m4t-v2-large")
```

或者您可以用专门用于 T2TT 任务的模型替换文本到文本生成片段，只需删除`generate_speech=False`。

```py
>>> from transformers import SeamlessM4Tv2ForTextToText
>>> model = SeamlessM4Tv2ForTextToText.from_pretrained("facebook/seamless-m4t-v2-large")
```

随时尝试 SeamlessM4Tv2ForSpeechToText 和 SeamlessM4Tv2ForTextToSpeech。

#### 更改说话者身份

您可以使用`speaker_id`参数更改用于语音合成的说话者。一些`speaker_id`对某些语言效果更好！

#### 更改生成策略

您可以为文本生成使用不同的生成策略，例如`.generate(input_ids=input_ids, text_num_beams=4, text_do_sample=True)`，它将在文本模型上执行多项式波束搜索解码。请注意，语音生成只支持贪婪 - 默认情况下 - 或多项式采样，可以使用例如`.generate(..., speech_do_sample=True, speech_temperature=0.6)`。

#### 生成语音和文本同时进行

使用`return_intermediate_token_ids=True`与 SeamlessM4Tv2Model 来返回语音和文本！

## 模型架构

SeamlessM4T-v2 具有一个多功能的架构，可以平稳处理文本和语音的顺序生成。这个设置包括两个序列到序列（seq2seq）模型。第一个模型将输入模态转换为翻译文本，而第二个模型从翻译文本生成语音单元标记，称为“单元标记”。

每种模态都有自己独特的架构的专用编码器。此外，对于语音输出，一个受[HiFi-GAN](https://arxiv.org/abs/2010.05646)架构启发的声码器被放置在第二个 seq2seq 模型的顶部。

### 与 SeamlessM4T-v1 的不同之处

这个新版本的架构在几个方面与第一个版本不同：

#### 第二次模型的改进

第二个 seq2seq 模型，称为文本到单元模型，现在是非自回归的，这意味着它在**单次前向传递**中计算单元。这一成就得以实现是因为：

+   使用**字符级嵌入**，这意味着预测的翻译文本的每个字符都有自己的嵌入，然后用于预测单元标记。

+   使用中间持续预测器，在预测的翻译文本上以**字符级**预测语音持续时间。

+   使用一个新的文本到单元解码器，混合卷积和自注意力来处理更长的上下文。

#### 语音编码器的不同之处

在第一次生成过程中用于预测翻译文本的语音编码器，主要通过以下机制与以前的语音编码器不同：

+   使用分块注意力掩码来防止跨块的注意力，确保每个位置只关注其自己块内的位置和固定数量的先前块。

+   使用相对位置嵌入，它只考虑序列元素之间的距离，而不是绝对位置。更多细节请参考[Self-Attentionwith Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。

+   使用因果深度卷积而不是非因果卷积。

### 生成过程

生成过程如下：

+   输入文本或语音通过其特定编码器处理。

+   解码器以所需语言创建文本标记。

+   如果需要生成语音，第二个 seq2seq 模型以非自回归方式生成单元标记。

+   然后，这些单元标记通过最终的声码器传递，以产生实际的语音。

此模型由[ylacombe](https://huggingface.co/ylacombe)贡献。原始代码可以在[这里](https://github.com/facebookresearch/seamless_communication)找到。

## SeamlessM4Tv2Model

### `class transformers.SeamlessM4Tv2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L4288)

```py
( config current_modality = 'text' )
```

参数

+   `config` (~SeamlessM4Tv2Config) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

+   `current_modality` (`str`, *optional*, 默认为`"text"`) — 默认模态。仅用于初始化模型。可以设置为`"text"`或`"speech"`。这将根据传递给前向和生成传递的模态（文本的`input_ids`和音频的`input_features`）自动更新。

原始的 SeamlessM4Tv2 模型变压器，可用于所有可用任务（S2ST、S2TT、T2TT、T2ST）。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L4512)

```py
( input_ids: Optional = None input_features: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None speaker_id: Optional = 0 generate_speech: Optional = True **kwargs ) → export const metadata = 'undefined';Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor], ModelOutput]
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `input_features` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_banks)`，*optional*) — 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `return_intermediate_token_ids` (`bool`, *optional*) — 如果设置为`True`，还会返回生成的中间文本和单元标记。如果您还想获取音频旁边的翻译文本，请将其设置为`True`。请注意，如果`generate_speech=True`，则此参数将被忽略。

+   `tgt_lang` (`str`, *optional*) — 用作翻译目标语言的语言。

+   `speaker_id` (`int`, *optional*, 默认为 0) — 用于语音合成的说话者的 ID。必须小于`config.vocoder_num_spkrs`。

+   `generate_speech` (`bool`, *optional*, 默认为`True`) — 如果设置为`False`，将仅返回文本标记，不会生成语音。

+   `kwargs`（*可选*）—将传递给 GenerationMixin.generate()的剩余关键字参数字典。关键字参数有两种类型：

    +   没有前缀，它们将作为每个子模型的`generate`方法的`**kwargs`输入，除了`decoder_input_ids`将仅通过文本组件传递。

    +   使用*text_*或*speech_*前缀，它们将分别作为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。

    这意味着您可以为一个生成策略指定一个生成策略，但不能为另一个生成策略指定生成策略。

返回

`Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor], ModelOutput]`

+   如果`generate_speech`和`return_intermediate_token_ids`，则返回`SeamlessM4Tv2GenerationOutput`。

+   如果`generate_speech`且不是`return_intermediate_token_ids`，则返回一个由形状为`(batch_size, sequence_length)`的波形和给出每个样本长度的`waveform_lengths`组成的元组。

+   如果`generate_speech=False`，它将返回`ModelOutput`。

生成翻译的标记 ID 和/或翻译的音频波形。

此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的通用参数，或将传递给其中一个模型的带前缀的参数。

例如，调用`.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4Tv2ForTextToSpeech

### `class transformers.SeamlessM4Tv2ForTextToSpeech`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L3491)

```py
( config: SeamlessM4Tv2Config )
```

参数

+   `config`（~SeamlessM4Tv2Config](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

文本到语音 SeamlessM4Tv2 模型转换器，可用于 T2ST。该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L3652)

```py
( input_ids: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None speaker_id: Optional = 0 **kwargs ) → export const metadata = 'undefined';Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）—词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `return_intermediate_token_ids`（`bool`，*可选*）—如果为`True`，还返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，请将其设置为`True`。

+   `tgt_lang`（`str`，*可选*）- 用作翻译目标语言的语言。

+   `speaker_id`（`int`，*可选*，默认为 0）- 用于语音合成的说话者 ID。必须小于`config.vocoder_num_spkrs`。

+   `kwargs`（*可选*）- 剩余的关键字参数字典将传递给 GenerationMixin.generate()。关键字参数有两种类型：

    +   没有前缀，它们将作为每个子模型的`generate`方法的`**kwargs`输入，除了`decoder_input_ids`将仅通过文本组件传递。

    +   使用*text_*或*speech_*前缀，它们将分别作为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。

    这意味着您可以为一个生成指定一种生成策略，但对另一个生成不指定。

返回

`Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`

+   如果`return_intermediate_token_ids`，则返回`SeamlessM4Tv2GenerationOutput`。

+   如果不是`return_intermediate_token_ids`，则返回一个由形状为`(batch_size, sequence_length)`的波形和`waveform_lengths`组成的元组，其中给出了每个样本的长度。

生成翻译后的音频波形。

此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的一般参数，或者将传递给其中一个模型的带前缀的参数。

例如，调用`.generate(input_ids, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4Tv2ForSpeechToSpeech

### `class transformers.SeamlessM4Tv2ForSpeechToSpeech`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L3885)

```py
( config )
```

参数

+   `config`（~SeamlessM4Tv2Config）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

这是一个用于 S2ST 的语音到语音 SeamlessM4Tv2 模型转换器。该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L4049)

```py
( input_features: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None speaker_id: Optional = 0 **kwargs ) → export const metadata = 'undefined';Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]
```

参数

+   `input_features`（形状为`(batch_size, sequence_length, num_banks)`的`torch.FloatTensor`）- 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `return_intermediate_token_ids`（`bool`，*可选*）- 如果为`True`，还会返回中间生成的文本和单元标记。如果您还想在音频旁边获取翻译文本，则设置为`True`。

+   `tgt_lang`（`str`，*可选*）- 用作翻译目标语言的语言。

+   `speaker_id`（`int`，*可选*，默认为 0）— 用于语音合成的说话者 ID。必须小于`config.vocoder_num_spkrs`。

+   `kwargs`（*可选*）— 将传递给 GenerationMixin.generate()的剩余关键字参数字典。关键字参数有两种类型：

    +   没有前缀，它们将作为`**kwargs`输入到每个子模型的`generate`方法中，除了`decoder_input_ids`只会通过文本组件传递。

    +   使用*text_*或*speech_*前缀，它们将分别作为文本模型和语音模型的`generate`方法的输入。它优先于没有前缀的关键字。

    这意味着您可以为一个生成指定一种生成策略，但对另一个生成不指定。

返回

`Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`

+   如果`return_intermediate_token_ids`，则返回`SeamlessM4Tv2GenerationOutput`。

+   如果不是`return_intermediate_token_ids`，则返回一个由形状为`(batch_size, sequence_length)`的波形和`waveform_lengths`组成的元组，其中给出每个样本的长度。

生成翻译的音频波形。

此方法连续调用两个不同子模型的`.generate`函数。您可以在两个不同级别指定关键字参数：将传递给两个模型的通用参数，或者将传递给其中一个模型的前缀参数。

例如，调用`.generate(input_features, num_beams=4, speech_do_sample=True)`将在文本模型上连续执行波束搜索解码，并在语音模型上执行多项式波束搜索采样。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4Tv2ForTextToText

### `class transformers.SeamlessM4Tv2ForTextToText`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L2901)

```py
( config: SeamlessM4Tv2Config )
```

参数

+   `config`（~SeamlessM4Tv2Config）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

文本到文本 SeamlessM4Tv2 模型变压器，可用于 T2TT。该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L2954)

```py
( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于“未屏蔽”的标记，

    +   对于“屏蔽”的标记为 0。

    什么是注意力掩码？

+   `decoder_input_ids` (`torch.LongTensor`的形状为`(batch_size, target_sequence_length)`，*optional*) — 解码器输入序列标记在词汇表中的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是解码器输入 ID？

    Bart 使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，则只需输入最后的`decoder_input_ids`（参见`past_key_values`）。 

    对于翻译和摘要训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将通过将`input_ids`向右移动来创建此张量，用于去噪预训练，遵循论文。

+   `decoder_attention_mask` (`torch.LongTensor`的形状为`(batch_size, target_sequence_length)`，*optional*) — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

    如果要更改填充行为，应阅读`modeling_bart._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括(`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*) 是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。

    如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（这些没有将其过去的键值状态提供给此模型）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果要更好地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor`的形状为`(batch_size, target_sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则只需输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果要更好地控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（参见`input_ids`文档）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

+   `use_cache` (`bool`，*optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

SeamlessM4Tv2ForTextToText 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L3046)

```py
( input_ids = None tgt_lang = None generation_config = None logits_processor = None stopping_criteria = None prefix_allowed_tokens_fn = None synced_gpus = False **kwargs ) → export const metadata = 'undefined';ModelOutput or torch.LongTensor
```

参数

+   `input_ids` (`torch.Tensor`，根据模态性质的不同形状，*optional*) — 词汇表中输入序列标记的索引。

    可以使用 SeamlessM4TTokenizer 或 SeamlessM4TProcessor 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `tgt_lang` (`str`，*optional*) — 用作翻译目标语言的语言。

+   `generation_config` (`~generation.GenerationConfig`，*optional*) — 用作生成调用的基本参数化的生成配置。传递给生成的`**kwargs`与`generation_config`的属性匹配将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中获取；2）从模型配置中获取。请注意，未指定的参数将继承 GenerationConfig 的默认值，应检查文档以参数化生成。

+   `logits_processor` (`LogitsProcessorList`，*optional*) — 自定义对数处理器，补充了从参数和生成配置构建的默认对数处理器。如果传递的对数处理器已经使用参数或生成配置创建，则会抛出错误。此功能适用于高级用户。

+   `stopping_criteria` (`StoppingCriteriaList`, *optional*) — 自定义停止标准，补充了从参数和生成配置构建的默认停止标准。如果传递的停止标准已经使用参数或生成配置创建，则会抛出错误。此功能适用于高级用户。

+   `prefix_allowed_tokens_fn`（`Callable[[int, torch.Tensor], List[int]]`，*可选*）- 如果提供，此函数将在每个步骤中将束搜索限制为仅允许的令牌。如果未提供，则不应用约束。此函数接受 2 个参数：批次 ID`batch_id`和`input_ids`。它必须返回一个列表，其中包含下一代步骤的允许令牌，条件是批次 ID`batch_id`和先前生成的令牌`inputs_ids`。此参数对于受前缀约束的生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。

+   `synced_gpus`（`bool`，*可选*，默认为`False`）- 是否继续运行 while 循环直到 max_length（ZeRO 阶段 3 所需）

+   `kwargs`（`Dict[str, Any]`，*可选*）- `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定 kwargs 的临时参数化。

返回

ModelOutput 或`torch.LongTensor`

一个 ModelOutput（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个`torch.FloatTensor`。可能的 ModelOutput 类型为：

+   GenerateEncoderDecoderOutput，

+   GenerateBeamEncoderDecoderOutput

生成令牌 id 序列。

大多数生成控制参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应的参数传递给 generate()来覆盖任何`generation_config`，例如`.generate(inputs, num_beams=4, do_sample=True)`。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4Tv2ForSpeechToText

### `class transformers.SeamlessM4Tv2ForSpeechToText`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L3189)

```py
( config: SeamlessM4Tv2Config )
```

参数

+   `config`（~SeamlessM4Tv2Config）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

可用于 S2TT 的语音到文本 SeamlessM4Tv2 模型变压器。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L3244)

```py
( input_features: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs )
```

参数

+   `input_features`（形状为`(batch_size, sequence_length, num_banks)`的`torch.FloatTensor`）- 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 词汇表中解码器输入序列标记的索引。

    可以使用 AutoTokenizer 来获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是解码器输入 ID？

    Bart 使用`eos_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，可以选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。

    对于翻译和摘要训练，应提供`decoder_input_ids`。如果未提供`decoder_input_ids`，模型将通过将`input_ids`向右移动来创建此张量，以进行去噪预训练，遵循论文中的方法。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

    如果要更改填充行为，应阅读`modeling_bart._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

+   `encoder_outputs`（形状为`tuple(tuple(torch.FloatTensor)`，*可选*）- 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（形状为`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，以及 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（即未将其过去键值状态提供给此模型的那些）的形状为`(batch_size, 1)`，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更好地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则可选择仅输入最后的`decoder_inputs_embeds`（请参阅`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，则会返回`past_key_values`键值状态，并且可以用于加速解码（请参阅`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

SeamlessM4Tv2ForSpeechToText 的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py#L3345)

```py
( input_features = None tgt_lang = None generation_config = None logits_processor = None stopping_criteria = None prefix_allowed_tokens_fn = None synced_gpus = False **kwargs ) → export const metadata = 'undefined';ModelOutput or torch.LongTensor
```

参数

+   `input_features`（形状为`(batch_size, sequence_length, num_banks)`的`torch.FloatTensor`）- 输入音频特征。这应该由 SeamlessM4TFeatureExtractor 类或 SeamlessM4TProcessor 类返回。有关详细信息，请参阅 SeamlessM4TFeatureExtractor.`call`()。

+   `tgt_lang`（`str`，*可选*）- 用作翻译目标语言的语言。

+   `generation_config`（`~generation.GenerationConfig`，*可选*）- 用作生成调用的基本参数化的生成配置。传递给生成匹配`generation_config`属性的`**kwargs`将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中获取，如果存在；2）从模型配置中获取。请注意，未指定的参数将继承 GenerationConfig 的默认值，其文档应该被检查以参数化生成。

+   `logits_processor`（`LogitsProcessorList`，*可选*）—自定义 logits 处理器，补充从参数和生成配置构建的默认 logits 处理器。如果传递的 logit 处理器已经使用参数或生成配置创建，则会抛出错误。此功能适用于高级用户。

+   `stopping_criteria`（`StoppingCriteriaList`，*可选*）—自定义停止标准，补充从参数和生成配置构建的默认停止标准。如果传递的停止标准已经使用参数或生成配置创建，则会抛出错误。此功能适用于高级用户。

+   `prefix_allowed_tokens_fn`（`Callable[[int, torch.Tensor], List[int]]`，*可选*）—如果提供，此函数将在每个步骤将束搜索限制为仅允许的令牌。如果未提供，则不应用约束。此函数接受 2 个参数：批次 ID`batch_id`和`input_ids`。它必须返回一个列表，其中包含下一代步骤的允许令牌，条件是批次 ID`batch_id`和先前生成的令牌`inputs_ids`。此参数对于基于前缀的约束生成非常有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。

+   `synced_gpus`（`bool`，*可选*，默认为`False`）—是否继续运行 while 循环直到 max_length（需要 ZeRO 阶段 3）

+   `kwargs`（`Dict[str, Any]`，*可选*）—`generate_config`的特殊参数化和/或将转发到模型的`forward`函数的其他模型特定 kwargs。

返回

ModelOutput 或`torch.LongTensor`

ModelOutput（如果`return_dict_in_generate=True`或`config.return_dict_in_generate=True`）或`torch.FloatTensor`。可能的 ModelOutput 类型为：

+   GenerateEncoderDecoderOutput

+   GenerateBeamEncoderDecoderOutput

生成令牌 id 序列。

大多数生成控制参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应的参数传递给 generate()来覆盖任何`generation_config`，例如`.generate(inputs, num_beams=4, do_sample=True)`。

有关生成策略和代码示例的概述，请查看以下指南。

## SeamlessM4Tv2Config

### `class transformers.SeamlessM4Tv2Config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t_v2/configuration_seamless_m4t_v2.py#L28)

```py
( vocab_size = 256102 t2u_vocab_size = 10082 char_vocab_size = 10943 hidden_size = 1024 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True max_position_embeddings = 4096 is_encoder_decoder = True encoder_layerdrop = 0.05 decoder_layerdrop = 0.05 activation_function = 'relu' dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.0 scale_embedding = True encoder_layers = 24 encoder_ffn_dim = 8192 encoder_attention_heads = 16 decoder_layers = 24 decoder_ffn_dim = 8192 decoder_attention_heads = 16 decoder_start_token_id = 3 max_new_tokens = 256 pad_token_id = 0 bos_token_id = 2 eos_token_id = 3 speech_encoder_layers = 24 speech_encoder_attention_heads = 16 speech_encoder_intermediate_size = 4096 speech_encoder_hidden_act = 'swish' speech_encoder_dropout = 0.0 add_adapter = True speech_encoder_layerdrop = 0.1 feature_projection_input_dim = 160 adaptor_kernel_size = 8 adaptor_stride = 8 adaptor_dropout = 0.1 num_adapter_layers = 1 position_embeddings_type = 'relative_key' conv_depthwise_kernel_size = 31 left_max_position_embeddings = 64 right_max_position_embeddings = 8 speech_encoder_chunk_size = 20000 speech_encoder_left_chunk_num = 128 t2u_bos_token_id = 0 t2u_pad_token_id = 1 t2u_eos_token_id = 2 t2u_encoder_layers = 6 t2u_encoder_ffn_dim = 8192 t2u_encoder_attention_heads = 16 t2u_decoder_layers = 6 t2u_decoder_ffn_dim = 8192 t2u_decoder_attention_heads = 16 t2u_max_position_embeddings = 4096 t2u_variance_predictor_embed_dim = 1024 t2u_variance_predictor_hidden_dim = 256 t2u_variance_predictor_kernel_size = 3 t2u_variance_pred_dropout = 0.5 sampling_rate = 16000 upsample_initial_channel = 512 upsample_rates = [5, 4, 4, 2, 2] upsample_kernel_sizes = [11, 8, 8, 4, 4] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] leaky_relu_slope = 0.1 unit_hifi_gan_vocab_size = 10000 unit_embed_dim = 1280 lang_embed_dim = 256 spkr_embed_dim = 256 vocoder_num_langs = 36 vocoder_num_spkrs = 200 variance_predictor_kernel_size = 3 var_pred_dropout = 0.5 vocoder_offset = 4 **kwargs )
```

参数

+   `vocab_size`（`int`，*可选*，默认为 256102）—SeamlessM4Tv2 模型文本模态的词汇大小。定义了在调用~SeamlessM4Tv2Model、~SeamlessM4Tv2ForTextToSpeech 或~SeamlessM4Tv2ForTextToText 时传递的不同令牌数量。inputs_ids。

+   `t2u_vocab_size` (`int`, *optional*, 默认为 10082) — SeamlessM4Tv2 模型的单元词汇量。定义了在调用~SeamlessM4Tv2Model 的 Text-To-Units 子模型时可以表示的不同“单元标记”的数量。

+   `char_vocab_size` (`int`, *optional*, 默认为 10943) — SeamlessM4Tv2 模型的字符词汇量。定义了在调用~SeamlessM4Tv2Model 的 Text-To-Units 子模型时可以表示的不同字符标记的数量。

在子模型之间共享的参数

+   `hidden_size` (`int`, *optional*, 默认为 1024) — 架构中“中间”层的维度。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-05) — 层归一化层使用的 epsilon。

+   `use_cache` (`bool`, *optional*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `max_position_embeddings` (`int`, *optional*, 默认为 4096) — 此模型文本编码器和解码器可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如 512、1024 或 2048）。

+   `is_encoder_decoder` (`bool`, *optional*, 默认为`True`) — 模型是否用作编码器/解码器。

+   `encoder_layerdrop` (`float`, *optional*, 默认为 0.05) — 编码器的 LayerDrop 概率。有关更多详细信息，请参阅 LayerDrop 论文)。

+   `decoder_layerdrop` (`float`, *optional*, 默认为 0.05) — 解码器的 LayerDrop 概率。有关更多详细信息，请参阅 LayerDrop 论文)。

+   `activation_function` (`str`或`function`, *optional*, 默认为`"relu"`) — 解码器和前馈层中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`、`"swish"`和`"gelu_new"`。

+   `dropout` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器、解码器和池化器中所有全连接层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, 默认为 0.1) — 所有注意力层的 dropout 概率。

+   `activation_dropout` (`float`, *optional*, 默认为 0.0) — 模型中所有激活层的 dropout 概率。

+   `scale_embedding` (`bool`, *optional*, 默认为`True`) — 通过将 d_model 开方来缩放嵌入。

文本编码器和文本解码器特定参数

+   `encoder_layers` (`int`, *optional*, 默认为 24) — Transformer 文本编码器中的隐藏层数。

+   `encoder_ffn_dim` (`int`, *optional*, 默认为 8192) — Transformer 文本编码器中“中间”（即前馈）层的维度。

+   `encoder_attention_heads` (`int`, *optional*, 默认为 16) — Transformer 文本编码器中每个注意力层的注意力头数。

+   `decoder_layers` (`int`, *optional*, 默认为 24) — Transformer 文本解码器中的隐藏层数。

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Transformer 文本解码器中“中间”（即前馈）层的维度。

+   `decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 文本解码器中每个注意力层的注意力头数。

+   `decoder_start_token_id` (`int`, *optional*, defaults to 3) — 如果编码器-解码器模型以与 *bos* 不同的标记开始解码，则该标记的 id。仅适用于文本解码器。

+   `max_new_tokens` (`int`, *optional*, defaults to 256) — 要生成的文本标记的最大数量，忽略提示中的标记数量。

+   `pad_token_id` (`int`, *optional*, defaults to 0) — *padding* 文本标记的 id。仅适用于文本解码器模型。

+   `bos_token_id` (`int`, *optional*, defaults to 2) — *流开始* 文本标记的 id。仅适用于文本解码器模型。

+   `eos_token_id` (`int`, *optional*, defaults to 3) — *流结束* 文本标记的 id。仅适用于文本解码器模型。

语音编码器特定参数

+   `speech_encoder_layers` (`int`, *optional*, defaults to 24) — Transformer 语音编码器中的隐藏层数量。

+   `speech_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 语音编码器中每个注意力层的注意力头数。

+   `speech_encoder_intermediate_size` (`int`, *optional*, defaults to 4096) — Transformer 语音编码器中“中间”（即前馈）层的维度。

+   `speech_encoder_hidden_act` (`str` 或 `function`, *optional*, defaults to `"swish"`) — 语音编码器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"`, `"swish"` 和 `"gelu_new"`。

+   `speech_encoder_dropout` (`float`, *optional*, defaults to 0.0) — 语音编码器中所有层的 dropout 概率。

+   `add_adapter` (`bool`, *optional*, defaults to `True`) — 在语音编码器顶部添加一个适配器层。

+   `speech_encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 语音编码器的 LayerDrop 概率。有关更多详细信息，请参阅 LayerDrop 论文)。

+   `feature_projection_input_dim` (`int`, *optional*, defaults to 160) — 语音编码器输入特征投影的维度，即使用 SeamlessM4TFeatureExtractor 处理输入音频后的维度。

+   `adaptor_kernel_size` (`int`, *optional*, defaults to 8) — 适配器网络中卷积层的核大小。仅在 `add_adapter` 为 True 时相关。

+   `adaptor_stride` (`int`, *optional*, defaults to 8) — 适配器网络中卷积层的步幅。仅在 `add_adapter` 为 True 时相关。

+   `adaptor_dropout` (`float`, *optional*, defaults to 0.1) — 语音适配器中所有层的 dropout 概率。

+   `num_adapter_layers` (`int`, *optional*, defaults to 1) — 适配器网络中应使用的卷积层数量。仅在 `add_adapter` 为 True 时相关。

+   `position_embeddings_type` (`str`, *optional*, defaults to `"relative_key"`) — 可指定为 `relative_key`。如果保持为 `None`，则不应用相对位置嵌入。仅适用于语音编码器。有关 `"relative_key"` 的更多信息，请参阅 [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。

+   `conv_depthwise_kernel_size` (`int`, *optional*, defaults to 31) — Conformer 块中深度可分离 1D 卷积层的核大小。仅适用于语音编码器。

+   `left_max_position_embeddings` (`int`, *optional*, defaults to 64) — 相对位置的左裁剪值。

+   `right_max_position_embeddings` (`int`, *optional*, defaults to 8) — 相对位置的右裁剪值。

+   `speech_encoder_chunk_size` (`int`, *optional*, 默认为 20000) — 每个注意力块的大小。

+   `speech_encoder_left_chunk_num` (`int`, *optional*, 默认为 128) — 允许向左查看的块数。

文本到单元（t2u）模型特定参数

+   `t2u_bos_token_id` (`int`, *optional*, 默认为 0) — *流开始* 单元标记的 id。仅适用于文本到单元的 seq2seq 模型。

+   `t2u_pad_token_id` (`int`, *optional*, 默认为 1) — *填充* 单元标记的 id。仅适用于文本到单元的 seq2seq 模型。

+   `t2u_eos_token_id` (`int`, *optional*, 默认为 2) — *流结束* 单元标记的 id。仅适用于文本到单元的 seq2seq 模型。

+   `t2u_encoder_layers` (`int`, *optional*, 默认为 6) — Transformer 文本到单元编码器中的隐藏层数量。

+   `t2u_encoder_ffn_dim` (`int`, *optional*, 默认为 8192) — Transformer 文本到单元编码器中“中间”（即前馈）层的维度。

+   `t2u_encoder_attention_heads` (`int`, *optional*, 默认为 16) — Transformer 文本到单元编码器中每个注意力层的注意力头数量。

+   `t2u_decoder_layers` (`int`, *optional*, 默认为 6) — Transformer 文本到单元解码器中的隐藏层数量。

+   `t2u_decoder_ffn_dim` (`int`, *optional*, 默认为 8192) — Transformer 文本到单元解码器中“中间”（即前馈）层的维度。

+   `t2u_decoder_attention_heads` (`int`, *optional*, 默认为 16) — Transformer 文本到单元解码器中每个注意力层的注意力头数量。

+   `t2u_max_position_embeddings` (`int`, *optional*, 默认为 4096) — 该模型文本到单元组件可能被使用的最大序列长度。通常设置为较大的值以防万一（例如 512、1024 或 2048）。

+   `t2u_variance_predictor_embed_dim` (`int`, *optional*, 默认为 1024) — 文本到单元的持续时间预测器的投影维度。

+   `t2u_variance_predictor_hidden_dim` (`int`, *optional*, 默认为 256) — 文本到单元的持续时间预测器的内部维度。

+   `t2u_variance_predictor_kernel_size` (`int`, *optional*, 默认为 3) — 文本到单元的持续时间预测器的卷积层的内核大小。

+   `t2u_variance_pred_dropout` (`float`, *optional*, 默认为 0.5) — 文本到单元的持续时间预测器的 dropout 概率。

    > Hifi-Gan 声码器特定参数

+   `sampling_rate` (`int`, *optional*, 默认为 16000) — 生成输出音频的采样率，以赫兹（Hz）表示。

+   `upsample_initial_channel` (`int`, *optional*, 默认为 512) — hifi-gan 上采样网络的输入通道数。仅适用于声码器。

+   `upsample_rates` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[5, 4, 4, 2, 2]`) — 一个整数元组，定义声码器上采样网络中每个 1D 卷积层的步幅。*upsample_rates*的长度定义了卷积层的数量，并且必须与*upsample_kernel_sizes*的长度匹配。仅适用于声码器。

+   `upsample_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[11, 8, 8, 4, 4]`) — 一个整数元组，定义声码器上采样网络中每个 1D 卷积层的内核大小。*upsample_kernel_sizes*的长度定义了卷积层的数量，并且必须与*upsample_rates*的长度匹配。仅适用于声码器。

+   `resblock_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[3, 7, 11]`) — 一个整数元组，定义多接受域融合（MRF）模块中声码器 1D 卷积层的内核大小。仅适用于声码器。

+   `resblock_dilation_sizes` (`Tuple[Tuple[int]]` 或 `List[List[int]]`, *optional*, 默认为 `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — 一个嵌套的整数元组，定义了多接受域融合（MRF）模块中语音编码器膨胀 1D 卷积层的扩张率。仅适用于语音编码器。

+   `leaky_relu_slope` (`float`, *optional*, 默认为 0.1) — 语音编码器中 leaky ReLU 激活使用的负斜率的角度。仅适用于语音编码器。

+   `unit_hifi_gan_vocab_size` (`int`, *optional*, 默认为 10000) — SeamlessM4Tv2 语音编码器的词汇表大小。定义了在调用 ~SeamlessM4Tv2Model、~SeamlessM4Tv2ForSpeechToSpeech 或 ~SeamlessM4Tv2ForTextToSpeech 时可以表示的不同单元标记数量。

+   `unit_embed_dim` (`int`, *optional*, 默认为 1280) — 给予 hifi-gan 语音编码器的输入 id 的投影维度。仅适用于语音编码器。

+   `lang_embed_dim` (`int`, *optional*, 默认为 256) — 给予 hifi-gan 语音编码器的目标语言的投影维度。仅适用于语音编码器。

+   `spkr_embed_dim` (`int`, *optional*, 默认为 256) — 给予 hifi-gan 语音编码器的说话者 id 的投影维度。仅适用于语音编码器。

+   `vocoder_num_langs` (`int`, *optional*, 默认为 36) — 语音编码器支持的语言数量。可能与 `t2u_num_langs` 不同。

+   `vocoder_num_spkrs` (`int`, *optional*, 默认为 200) — 语音编码器支持的说话者数量。

+   `variance_predictor_kernel_size` (`int`, *optional*, 默认为 3) — 持续时间预测器的核大小。仅适用于语音编码器。

+   `var_pred_dropout` (`float`, *optional*, 默认为 0.5) — 持续时间预测器的 dropout 概率。仅适用于语音编码器。

+   `vocoder_offset` (`int`, *optional*, 默认为 4) — 将单元标记的 id 偏移此数字以考虑符号标记。仅适用于语音编码器。

这是用于存储 ~SeamlessM4Tv2Model 配置的配置类。根据指定的参数实例化一个 SeamlessM4Tv2 模型，定义模型架构。使用默认值实例化配置将产生类似于 SeamlessM4Tv2 [""](https://huggingface.co/%22%22) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

```py
>>> from transformers import SeamlessM4Tv2Model, SeamlessM4Tv2Config

>>> # Initializing a SeamlessM4Tv2 "" style configuration
>>> configuration = SeamlessM4Tv2Config()

>>> # Initializing a model from the "" style configuration
>>> model = SeamlessM4Tv2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```
