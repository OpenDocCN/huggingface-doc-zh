# REALM

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/realm`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/realm)

## 概述

REALM 模型是由 Kelvin Guu、Kenton Lee、Zora Tung、Panupong Pasupat 和 Ming-Wei Chang 在[REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)中提出的。这是一个检索增强语言模型，首先从文本知识语料库中检索文档，然后利用检索到的文档来处理问答任务。

该论文的摘要如下：

*语言模型预训练已被证明可以捕获大量世界知识，对于诸如问答等 NLP 任务至关重要。然而，这些知识隐式存储在神经网络的参数中，需要越来越大的网络来涵盖更多事实。为了以更模块化和可解释的方式捕获知识，我们通过潜在知识检索器增强了语言模型预训练，使模型能够从大型语料库（如维基百科）中检索和关注文档，这些文档在预训练、微调和推理过程中使用。我们首次展示了如何以无监督方式预训练这样一个知识检索器，使用掩码语言建模作为学习信号，并通过考虑数百万文档的检索步骤进行反向传播。我们通过在具有挑战性的开放域问答（Open-QA）任务上微调来展示检索增强语言模型预训练（REALM）的有效性。我们在三个流行的开放域问答基准上与最先进的显式和隐式知识存储模型进行比较，发现我们在绝对准确率上表现优异（4-16%），同时还提供了诸如可解释性和模块化等质量优势。*

该模型由[qqaatw](https://huggingface.co/qqaatw)贡献。原始代码可在[此处](https://github.com/google-research/language/tree/master/language/realm)找到。

## RealmConfig

### `class transformers.RealmConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/configuration_realm.py#L44)

```py
( vocab_size = 30522 hidden_size = 768 retriever_proj_size = 128 num_hidden_layers = 12 num_attention_heads = 12 num_candidates = 8 intermediate_size = 3072 hidden_act = 'gelu_new' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 span_hidden_size = 256 max_span_width = 10 reader_layer_norm_eps = 0.001 reader_beam_size = 5 reader_seq_len = 320 num_block_records = 13353718 searcher_beam_size = 5000 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 30522) — REALM 模型的词汇量。定义了在调用 RealmEmbedder、RealmScorer、RealmKnowledgeAugEncoder 或 RealmReader 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为 768) — 编码器层和池化器层的维度。

+   `retriever_proj_size` (`int`, *optional*, 默认为 128) — 检索器（嵌入器）投影的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `num_candidates` (`int`, *optional*, 默认为 8) — 输入到 RealmScorer 或 RealmKnowledgeAugEncoder 的候选项数量。

+   `intermediate_size` (`int`, *optional*, 默认为 3072) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str`或`function`, *optional*, 默认为`"gelu_new"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢弃比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512 或 1024 或 2048）。

+   `type_vocab_size` (`int`, *optional*, defaults to 2) — 在调用 RealmEmbedder、RealmScorer、RealmKnowledgeAugEncoder 或 RealmReader 时传递的`token_type_ids`的词汇表大小。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。

+   `span_hidden_size` (`int`, *optional*, defaults to 256) — 读者跨度的维度。

+   `max_span_width` (`int`, *optional*, defaults to 10) — 读者的最大跨度宽度。

+   `reader_layer_norm_eps` (`float`, *optional*, defaults to 1e-3) — 读者的层归一化层使用的 epsilon。

+   `reader_beam_size` (`int`, *optional*, defaults to 5) — 读者的波束大小。

+   `reader_seq_len` (`int`, *optional*, defaults to 288+32) — 读者的最大序列长度。

+   `num_block_records` (`int`, *optional*, defaults to 13353718) — 区块记录的数量。

+   `searcher_beam_size` (`int`, *optional*, defaults to 5000) — 搜索器的波束大小。请注意，当启用评估模式时，*searcher_beam_size*将与*reader_beam_size*相同。

这是用于存储配置的配置类

1.  RealmEmbedder

1.  RealmScorer

1.  RealmKnowledgeAugEncoder

1.  RealmRetriever

1.  RealmReader

1.  RealmForOpenQA

它用于根据指定的参数实例化一个 REALM 模型，定义模型架构。使用默认值实例化配置将产生类似于 REALM [google/realm-cc-news-pretrained-embedder](https://huggingface.co/google/realm-cc-news-pretrained-embedder) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import RealmConfig, RealmEmbedder

>>> # Initializing a REALM realm-cc-news-pretrained-* style configuration
>>> configuration = RealmConfig()

>>> # Initializing a model (with random weights) from the google/realm-cc-news-pretrained-embedder style configuration
>>> model = RealmEmbedder(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## RealmTokenizer

### `class transformers.RealmTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm.py#L95)

```py
( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `do_lower_case` (`bool`, *optional*, defaults to `True`) — 在标记化时是否将输入转换为小写。

+   `do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — 在 WordPiece 之前是否进行基本标记化。

+   `never_split` (`Iterable`, *optional*) — 在标记化过程中永远不会拆分的标记集合。仅在`do_basic_tokenize=True`时有效

+   `unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — 未知标记。词汇表中没有的标记无法转换为 ID，而是设置为此标记。

+   `sep_token` (`str`, *可选*, 默认为 `"[SEP]"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列，或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `pad_token` (`str`, *可选*, 默认为 `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `cls_token` (`str`, *可选*, 默认为 `"[CLS]"`) — 分类器标记，用于进行序列分类（对整个序列进行分类，而不是每个标记进行分类）。它是使用特殊标记构建时的序列的第一个标记。

+   `mask_token` (`str`, *可选*, 默认为 `"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `tokenize_chinese_chars` (`bool`, *可选*, 默认为 `True`) — 是否对中文字符进行分词。

    这可能应该在日语中停用（请参阅此[问题](https://github.com/huggingface/transformers/issues/328)）。

+   `strip_accents` (`bool`, *可选*) — 是否去除所有重音符号。如果未指定此选项，则将由 `lowercase` 的值确定（与原始 BERT 相同）。

构建一个 REALM 分词器。

RealmTokenizer 与 BertTokenizer 相同，并进行端到端的分词：标点符号拆分和词片段。

此分词器继承自 PreTrainedTokenizer，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[源代码](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm.py#L300)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 要添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

带有适当特殊标记的输入 ID 列表。

通过连接和添加特殊标记，为序列分类任务构建模型输入，可以从序列或序列对中构建。REALM 序列的格式如下：

+   单个序列：`[CLS] X [SEP]`

+   序列对：`[CLS] A [SEP] B [SEP]`

#### `get_special_tokens_mask`

[源代码](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm.py#L325)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens` (`bool`, *可选*, 默认为 `False`) — 标记列表是否已经格式化为模型的特殊标记。

返回

`List[int]`

一个整数列表，范围为 [0, 1]：1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 ID。在使用分词器的 `prepare_for_model` 方法添加特殊标记时调用此方法。

#### `create_token_type_ids_from_sequences`

[源代码](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm.py#L353)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

根据给定序列的标记类型 ID 列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。一个 REALM 序列

pair mask 的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0s）。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm.py#L382)

```py
( save_directory: str filename_prefix: Optional = None )
```

#### `batch_encode_candidates`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm.py#L227)

```py
( text **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`List[List[str]]`) — 要编码的序列批次。每个序列必须以此格式呈现：（批次大小，候选项数，文本）。

+   `text_pair` (`List[List[str]]`，*可选*） — 要编码的序列批次。每个序列必须以此格式呈现：（批次大小，候选项数，文本）。**kwargs — **call**方法的关键字参数。

返回

BatchEncoding

编码的文本或文本对。

编码一批文本或文本对。此方法类似于常规**call**方法，但具有以下区别：

1.  处理额外的候选项轴。（批次大小，候选项数，文本）

1.  始终将序列填充到*max_length*。

1.  必须指定*max_length*以将候选项堆叠成批次。

+   单个序列：`[CLS] X [SEP]`

+   序列对：`[CLS] A [SEP] B [SEP]`

示例：

```py
>>> from transformers import RealmTokenizer

>>> # batch_size = 2, num_candidates = 2
>>> text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

>>> tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
>>> tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
```

## RealmTokenizerFast

### `class transformers.RealmTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm_fast.py#L102)

```py
( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `do_lower_case` (`bool`，*可选*，默认为`True`) — 在标记化时是否将输入转换为小写。

+   `unk_token` (`str`，*可选*，默认为`"[UNK]"`) — 未知标记。词汇表中没有的标记无法转换为 ID，而是设置为此标记。

+   `sep_token` (`str`，*可选*，默认为`"[SEP]"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `pad_token` (`str`，*可选*，默认为`"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `cls_token` (`str`，*可选*，默认为`"[CLS]"`) — 在进行序列分类（对整个序列而不是每个标记的分类）时使用的分类器标记。这是构建具有特殊标记的序列时的第一个标记。

+   `mask_token` (`str`，*可选*，默认为`"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `clean_text` (`bool`，*可选*，默认为`True`) — 是否在标记化之前清理文本，通过删除任何控制字符并将所有空格替换为经典空格。

+   `tokenize_chinese_chars` (`bool`，*可选*，默认为`True`) — 是否对中文字符进行标记化。这可能应该在日语中停用（参见[此问题](https://github.com/huggingface/transformers/issues/328)）。

+   `strip_accents` (`bool`，*可选*） — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始 BERT 相同）。

+   `wordpieces_prefix` (`str`，*可选*，默认为`"##"`) — 子词的前缀。

构建一个“快速”REALM 标记器（由 HuggingFace 的*tokenizers*库支持）。基于 WordPiece。

RealmTokenizerFast 与 BertTokenizerFast 相同，并且运行端到端的标记化：标点符号拆分和 wordpiece。

此分词器继承自 PreTrainedTokenizerFast，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `batch_encode_candidates`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/tokenization_realm_fast.py#L193)

```py
( text **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`List[List[str]]`) — 要编码的序列批次。每个序列必须采用以下格式：(batch_size, num_candidates, text)。

+   `text_pair` (`List[List[str]]`, *可选*) — 要编码的序列批次。每个序列必须采用以下格式：(batch_size, num_candidates, text)。**kwargs — **call**方法的关键字参数。

返回

BatchEncoding

编码的文本或文本对。

编码一批文本或文本对。此方法类似于常规的**call**方法，但具有以下区别：

1.  处理额外的 num_candidate 轴。(batch_size, num_candidates, text)

1.  始终将序列填充到*max_length*。

1.  必须指定*max_length*以将候选项堆叠成批次。

+   单个序列：`[CLS] X [SEP]`

+   序列对：`[CLS] A [SEP] B [SEP]`

示例：

```py
>>> from transformers import RealmTokenizerFast

>>> # batch_size = 2, num_candidates = 2
>>> text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

>>> tokenizer = RealmTokenizerFast.from_pretrained("google/realm-cc-news-pretrained-encoder")
>>> tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
```

## RealmRetriever

### `class transformers.RealmRetriever`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/retrieval_realm.py#L72)

```py
( block_records tokenizer )
```

参数

+   `block_records` (`np.ndarray`) — 包含证据文本的 numpy 数组。

+   `tokenizer` (RealmTokenizer) — 用于编码检索文本的分词器。

REALM 的检索器输出检索到的证据块以及该块是否有答案以及答案位置。

#### `block_has_answer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/retrieval_realm.py#L129)

```py
( concat_inputs answer_ids )
```

检查 retrieved_blocks 是否有答案。

## RealmEmbedder

### `class transformers.RealmEmbedder`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1142)

```py
( config )
```

参数

+   `config` (RealmConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

REALM 的嵌入器输出投影分数，将用于计算相关性分数。该模型是 PyTorch 的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1162)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmEmbedderOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`） — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被遮蔽的标记，为 1，

    +   对于被`masked`的标记为 0。

    注意力掩码是什么？

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 ModelOutput 而不是一个普通元组。

返回

`transformers.models.realm.modeling_realm.RealmEmbedderOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.realm.modeling_realm.RealmEmbedderOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（RealmConfig）和输入的不同元素。

+   `projected_score` (`torch.FloatTensor`，形状为`(batch_size, config.retriever_proj_size)`) — 投影得分。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出以及初始嵌入输出的隐藏状态。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在自注意力头中用于计算加权平均值的注意力 softmax 之后的注意力权重。

RealmEmbedder 前向方法，覆盖`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, RealmEmbedder
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("google/realm-cc-news-pretrained-embedder")
>>> model = RealmEmbedder.from_pretrained("google/realm-cc-news-pretrained-embedder")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> projected_score = outputs.projected_score
```

## RealmScorer

### `class transformers.RealmScorer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1224)

```py
( config query_embedder = None )
```

参数

+   `config` (RealmConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

+   `query_embedder` (RealmEmbedder) — 用于输入序列的嵌入器。如果未指定，将使用与候选序列相同的嵌入器。

REALM 的评分器输出代表文档候选项得分（softmax 之前）的相关性分数。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1244)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None candidate_input_ids: Optional = None candidate_attention_mask: Optional = None candidate_token_type_ids: Optional = None candidate_inputs_embeds: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmScorerOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示`未被掩码的`标记，

    +   0 表示`被掩码的`标记。

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`之间：

    +   0 对应于*句子 A*标记。

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部是`未被掩码的`，

    +   0 表示头部是`被掩码的`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `candidate_input_ids` (`torch.LongTensor` of shape `(batch_size, num_candidates, sequence_length)`) — 词汇表中候选输入序列标记的索引。

    可以使用 AutoTokenizer 来获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是 input IDs？

+   `candidate_attention_mask`（形状为`(batch_size, num_candidates, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示*未被 mask*的标记，

    +   0 表示被`masked`的标记。

    什么是注意力掩码？

+   `candidate_token_type_ids`（形状为`(batch_size, num_candidates, sequence_length)`的`torch.LongTensor`，*可选*）- 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是 token type IDs？

+   `candidate_inputs_embeds`（形状为`(batch_size * num_candidates, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`candidate_input_ids`。如果您想要更多控制权，以便将*candidate_input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

返回

`transformers.models.realm.modeling_realm.RealmScorerOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.realm.modeling_realm.RealmScorerOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（RealmConfig）和输入的不同元素。

+   `relevance_score`（形状为`(batch_size, config.num_candidates)`的`torch.FloatTensor`）- 文档候选项的相关性分数（softmax 之前）。

+   `query_score`（形状为`(batch_size, config.retriever_proj_size)`的`torch.FloatTensor`）- 从查询嵌入器派生的查询分数。

+   `candidate_score`（形状为`(batch_size, config.num_candidates, config.retriever_proj_size)`的`torch.FloatTensor`）- 从嵌入器派生的候选分数。

RealmScorer 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, RealmScorer

>>> tokenizer = AutoTokenizer.from_pretrained("google/realm-cc-news-pretrained-scorer")
>>> model = RealmScorer.from_pretrained("google/realm-cc-news-pretrained-scorer", num_candidates=2)

>>> # batch_size = 2, num_candidates = 2
>>> input_texts = ["How are you?", "What is the item in the picture?"]
>>> candidates_texts = [["Hello world!", "Nice to meet you!"], ["A cute cat.", "An adorable dog."]]

>>> inputs = tokenizer(input_texts, return_tensors="pt")
>>> candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors="pt")

>>> outputs = model(
...     **inputs,
...     candidate_input_ids=candidates_inputs.input_ids,
...     candidate_attention_mask=candidates_inputs.attention_mask,
...     candidate_token_type_ids=candidates_inputs.token_type_ids,
... )
>>> relevance_score = outputs.relevance_score
```

## RealmKnowledgeAugEncoder

### `class transformers.RealmKnowledgeAugEncoder`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1371)

```py
( config )
```

参数

+   `config`（RealmConfig）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

REALM 的知识增强编码器输出掩码语言模型对数和边际对数似然损失。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1397)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None relevance_score: Optional = None labels: Optional = None mlm_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, num_candidates, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_candidates, sequence_length)`, *optional*) — 用于避免在填充令牌索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, num_candidates, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记。

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, num_candidates, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部无效的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示头部`未被掩码`，

    +   0 表示头部`被掩码`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_candidates, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `relevance_score` (`torch.FloatTensor` of shape `(batch_size, num_candidates)`, *optional*) — 从 RealmScorer 派生的相关性分数，如果要计算掩码语言建模损失，必须指定。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`中（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

+   `mlm_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在某些位置计算联合损失的掩码。如果未指定，损失将不会被掩码。掩码值选择在`[0, 1]`中：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

返回

transformers.modeling_outputs.MaskedLMOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.MaskedLMOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（RealmConfig）和输入的各种元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 掩码语言建模（MLM）损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）- 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

RealmKnowledgeAugEncoder 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, RealmKnowledgeAugEncoder

>>> tokenizer = AutoTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
>>> model = RealmKnowledgeAugEncoder.from_pretrained(
...     "google/realm-cc-news-pretrained-encoder", num_candidates=2
... )

>>> # batch_size = 2, num_candidates = 2
>>> text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

>>> inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
```

## RealmReader

### `class transformers.RealmReader`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1525)

```py
( config )
```

参数

+   `config`（RealmConfig）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

REALM 的读取器。这个模型是 PyTorch 的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1537)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None relevance_score: Optional = None block_mask: Optional = None start_positions: Optional = None end_positions: Optional = None has_answers: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmReaderOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(reader_beam_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    输入 ID 是什么？

+   `attention_mask`（形状为`(reader_beam_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于“未屏蔽”的标记为 1，

    +   对于被“屏蔽”的标记为 0。

    注意力掩码是什么？

+   `token_type_ids` (`torch.LongTensor` of shape `(reader_beam_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于一个*sentence A* token，

    +   1 对应于一个*sentence B* token。

    什么是 token type IDs?

+   `position_ids` (`torch.LongTensor` of shape `(reader_beam_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是 position IDs?

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1 表示头部未被屏蔽，

    +   0 表示头部被屏蔽。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(reader_beam_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

+   `relevance_score` (`torch.FloatTensor` of shape `(searcher_beam_size,)`, *optional*) — 相关性分数，如果要计算 logits 和边际对数损失，则必须指定。

+   `block_mask` (`torch.BoolTensor` of shape `(searcher_beam_size, sequence_length)`, *optional*) — 如果要计算 logits 和边际对数损失，则必须指定证据块的掩码。

+   `start_positions` (`torch.LongTensor` of shape `(searcher_beam_size,)`, *optional*) — 用于计算标记跨度开始位置的位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会计入损失计算。

+   `end_positions` (`torch.LongTensor` of shape `(searcher_beam_size,)`, *optional*) — 用于计算标记跨度结束位置的位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会计入损失计算。

+   `has_answers` (`torch.BoolTensor` of shape `(searcher_beam_size,)`, *optional*) — 证据块是否有答案。

返回

`transformers.models.realm.modeling_realm.RealmReaderOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.realm.modeling_realm.RealmReaderOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（RealmConfig）和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`start_positions`、`end_positions`、`has_answers`时返回) — 总损失。

+   `retriever_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`start_positions`、`end_positions`、`has_answers`时返回) — 检索器损失。

+   `reader_loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`start_positions`、`end_positions`、`has_answers`时返回) — 读者损失。

+   `retriever_correct`（形状为`(config.searcher_beam_size,)`的`torch.BoolTensor`，*可选*）- 证据块是否包含答案。

+   `reader_correct`（形状为`(config.reader_beam_size, num_candidates)`的`torch.BoolTensor`，*可选*）- 跨度候选是否包含答案。

+   `block_idx`（形状为`()`的`torch.LongTensor`）- 预测答案最有可能出现的检索证据块的索引。

+   `candidate`（形状为`()`的`torch.LongTensor`）- 预测答案最有可能出现的检索范围候选的索引。

+   `start_pos`（形状为`()`的`torch.IntTensor`）- *RealmReader*输入中预测答案的起始位置。

+   `end_pos`（形状为`()`的`torch.IntTensor`）- *RealmReader*输入中预测答案的结束位置。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

RealmReader 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

## RealmForOpenQA

### `class transformers.RealmForOpenQA`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1726)

```py
( config retriever = None )
```

参数

+   `config`（RealmConfig）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

用于端到端开放域问答的`RealmForOpenQA`。该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `block_embedding_to`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1753)

```py
( device )
```

参数

+   `device`（`str`或`torch.device`）- 将`self.block_emb`发送到的设备。

将`self.block_emb`发送到特定设备。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/realm/modeling_realm.py#L1763)

```py
( input_ids: Optional attention_mask: Optional = None token_type_ids: Optional = None answer_ids: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmForOpenQAOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(1, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 来获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(1, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   对于未被`屏蔽`的令牌为 1，

    +   对于被`屏蔽`的令牌为 0。

    注意力掩码是什么？

+   `token_type_ids`（形状为`(1, sequence_length)`的`torch.LongTensor`，*可选*）- 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记（按设计不应在此模型中使用）。

    令牌类型 ID 是什么？

+   `answer_ids`（形状为`(num_answers, answer_length)`的`list`，*可选*）- 用于计算边际对数似然损失的答案 ID。索引应在`[-1, 0, ..., config.vocab_size]`中（参见`input_ids`文档字符串）索引设置为`-1`的令牌将被忽略（屏蔽），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的令牌

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.realm.modeling_realm.RealmForOpenQAOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.realm.modeling_realm.RealmForOpenQAOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（RealmConfig）和输入的不同元素。

+   `reader_output`（`dict`）- 读者输出。

+   `predicted_answer_ids`（形状为`(answer_sequence_length)`的`torch.LongTensor`）- 预测的答案 ID。

RealmForOpenQA 的前向方法，覆盖`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import RealmForOpenQA, RealmRetriever, AutoTokenizer

>>> retriever = RealmRetriever.from_pretrained("google/realm-orqa-nq-openqa")
>>> tokenizer = AutoTokenizer.from_pretrained("google/realm-orqa-nq-openqa")
>>> model = RealmForOpenQA.from_pretrained("google/realm-orqa-nq-openqa", retriever=retriever)

>>> question = "Who is the pioneer in modern computer science?"
>>> question_ids = tokenizer([question], return_tensors="pt")
>>> answer_ids = tokenizer(
...     ["alan mathison turing"],
...     add_special_tokens=False,
...     return_token_type_ids=False,
...     return_attention_mask=False,
... ).input_ids

>>> reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)
>>> predicted_answer = tokenizer.decode(predicted_answer_ids)
>>> loss = reader_output.loss
```
