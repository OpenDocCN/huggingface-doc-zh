# 语音编码器解码器模型

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech-encoder-decoder)

SpeechEncoderDecoderModel 可用于使用任何预训练语音自编码模型作为编码器（例如 Wav2Vec2，Hubert）和任何预训练自回归模型作为解码器初始化语音到文本模型。

已经证明使用预训练检查点初始化语音序列到文本序列模型，用于语音识别和语音翻译，例如在[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)中由 Changhan Wang，Anne Wu，Juan Pino，Alexei Baevski，Michael Auli，Alexis Conneau 展示。

如何使用 SpeechEncoderDecoderModel 进行推理的示例可以在 Speech2Text2 中看到。

## 从模型配置随机初始化 SpeechEncoderDecoderModel。

SpeechEncoderDecoderModel 可以从编码器和解码器配置随机初始化。在以下示例中，我们展示了如何使用默认的 Wav2Vec2Model 配置作为编码器和默认的`BertForCausalLM`配置作为解码器。

```py
>>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel

>>> config_encoder = Wav2Vec2Config()
>>> config_decoder = BertConfig()

>>> config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)
>>> model = SpeechEncoderDecoderModel(config=config)
```

## 从预训练的编码器和预训练的解码器初始化 SpeechEncoderDecoderModel。

SpeechEncoderDecoderModel 可以从预训练的编码器检查点和预训练的解码器检查点初始化。请注意，任何预训练的基于 Transformer 的语音模型，例如 Wav2Vec2，Hubert 都可以作为编码器，以及预训练的自编码模型，例如 BERT，预训练的因果语言模型，例如 GPT2，以及序列到序列模型的预训练解码器部分，例如 BART 的解码器，都可以作为解码器。根据您选择的解码器架构，交叉注意力层可能会被随机初始化。从预训练的编码器和解码器检查点初始化 SpeechEncoderDecoderModel 需要对模型进行下游任务微调，正如在*Warm-starting-encoder-decoder blog post*中所示。为此，`SpeechEncoderDecoderModel`类提供了一个 SpeechEncoderDecoderModel.from_encoder_decoder_pretrained()方法。

```py
>>> from transformers import SpeechEncoderDecoderModel

>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "facebook/hubert-large-ll60k", "bert-base-uncased"
... )
```

## 加载现有的 SpeechEncoderDecoderModel 检查点并执行推理。

要加载`SpeechEncoderDecoderModel`类的微调检查点，SpeechEncoderDecoderModel 提供了`from_pretrained(...)`方法，就像 Transformers 中的任何其他模型架构一样。

要执行推理，可以使用`generate`方法，该方法允许自回归生成文本。此方法支持各种解码形式，例如贪婪、束搜索和多项式采样。

```py
>>> from transformers import Wav2Vec2Processor, SpeechEncoderDecoderModel
>>> from datasets import load_dataset
>>> import torch

>>> # load a fine-tuned speech translation model and corresponding processor
>>> model = SpeechEncoderDecoderModel.from_pretrained("facebook/wav2vec2-xls-r-300m-en-to-15")
>>> processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-xls-r-300m-en-to-15")

>>> # let's perform inference on a piece of English speech (which we'll translate to German)
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> input_values = processor(ds[0]["audio"]["array"], return_tensors="pt").input_values

>>> # autoregressively generate transcription (uses greedy decoding by default)
>>> generated_ids = model.generate(input_values)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_text)
Mr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen heißen zu können.
```

## 训练

创建模型后，可以像 BART、T5 或任何其他编码器解码器模型一样对（语音，文本）对数据集进行微调。如您所见，模型只需要 2 个输入才能计算损失：`input_values`（语音输入）和`labels`（编码目标序列的`input_ids`）。

```py
>>> from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel
>>> from datasets import load_dataset

>>> encoder_id = "facebook/wav2vec2-base-960h"  # acoustic model encoder
>>> decoder_id = "bert-base-uncased"  # text decoder

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)
>>> tokenizer = AutoTokenizer.from_pretrained(decoder_id)
>>> # Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model
>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)

>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id

>>> # load an audio input and pre-process (normalise mean/std to 0/1)
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> input_values = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt").input_values

>>> # load its corresponding transcription and tokenize to generate labels
>>> labels = tokenizer(ds[0]["text"], return_tensors="pt").input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(input_values=input_values, labels=labels).loss
>>> loss.backward()
```

## SpeechEncoderDecoderConfig

### `class transformers.SpeechEncoderDecoderConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L26)

```py
( **kwargs )
```

参数

+   `kwargs`（*可选*）— 关键字参数的字典。特别是：

    +   `encoder`（PretrainedConfig，*可选*）— 定义编码器配置的配置对象的实例。

    +   `decoder`（PretrainedConfig，*可选*）— 定义解码器配置的配置对象的实例。

SpeechEncoderDecoderConfig 是用于存储 SpeechEncoderDecoderModel 配置的配置类。根据指定的参数实例化一个编码器解码器模型，定义编码器和解码器配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel

>>> # Initializing a Wav2Vec2 & BERT style configuration
>>> config_encoder = Wav2Vec2Config()
>>> config_decoder = BertConfig()

>>> config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

>>> # Initializing a Wav2Vec2Bert model from a Wav2Vec2 & bert-base-uncased style configurations
>>> model = SpeechEncoderDecoderModel(config=config)

>>> # Accessing the model configuration
>>> config_encoder = model.config.encoder
>>> config_decoder = model.config.decoder
>>> # set decoder config to causal lm
>>> config_decoder.is_decoder = True
>>> config_decoder.add_cross_attention = True

>>> # Saving the model, including its configuration
>>> model.save_pretrained("my-model")

>>> # loading model and config from pretrained folder
>>> encoder_decoder_config = SpeechEncoderDecoderConfig.from_pretrained("my-model")
>>> model = SpeechEncoderDecoderModel.from_pretrained("my-model", config=encoder_decoder_config)
```

#### `from_encoder_decoder_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L93)

```py
( encoder_config: PretrainedConfig decoder_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';SpeechEncoderDecoderConfig
```

返回

SpeechEncoderDecoderConfig

配置对象的实例

从预训练的编码器模型配置和解码器模型配置实例化一个 SpeechEncoderDecoderConfig（或派生类）。

## SpeechEncoderDecoderModel

### `class transformers.SpeechEncoderDecoderModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L172)

```py
( config: Optional = None encoder: Optional = None decoder: Optional = None )
```

参数

+   `config`（SpeechEncoderDecoderConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

此类可用于初始化一个具有任何预训练语音自编码模型作为编码器和任何预训练文本自回归模型作为解码器的语音序列到文本序列模型。编码器通过 from_pretrained()函数加载，解码器通过 from_pretrained()函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如摘要）上进行微调。

在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu](https://arxiv.org/abs/1907.12461)的研究中展示了使用预训练检查点初始化序列到序列模型对序列生成任务的有效性。

此外，在[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)中展示了如何利用大型预训练语音模型进行语音翻译可以显著提高性能。

在训练/微调了这样一个语音编码器解码器模型之后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。

此模型继承自 PreTrainedModel。检查超类文档以了解库实现的所有模型的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

SpeechEncoderDecoderModel 是一个通用模型类，当使用:meth*~transformers.AutoModel.from_pretrained*类方法为编码器创建一个库的基本模型类，并使用:meth*~transformers.AutoModelForCausalLM.from_pretrained*类方法为解码器创建一个 transformer 架构时，将被实例化。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L438)

```py
( inputs: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None input_values: Optional = None input_features: Optional = None return_dict: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `inputs`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length, feature_dim)`的`torch.FloatTensor`，*可选*）— 输入原始语音波形或语音特征的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，*例如*通过 soundfile 库（`pip install soundfile`）。要将数组准备为`inputs`，应使用 Wav2Vec2Processor 或 Speech2TextProcessor 进行填充和转换为`torch.FloatTensor`类型的张量。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1 用于“未屏蔽”的标记，

    +   0 用于“屏蔽”的标记。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中解码器输入序列标记的索引。

    可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

    如果使用了`past_key_values`，则只需输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

    对于训练，模型会通过将`labels`向右移动，用`pad_token_id`替换-100，并在其前面加上`decoder_start_token_id`来自动创建`decoder_input_ids`。

+   `decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

+   `encoder_outputs` (`tuple(torch.FloatTensor)`, *optional*) — 此元组必须包含（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（那些没有将其过去键值状态提供给此模型的）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果您想要更多控制权来将`decoder_input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算解码器的掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`内（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 输入原始语音波形的浮点值。可以通过将*.flac*或*.wav*音频文件加载到*List[float]*或*numpy.ndarray*类型的数组中来获取值，例如通过 soundfile 库（*pip install soundfile*）。要准备数组为*input_values*，应使用 Wav2Vec2Processor 进行填充和转换为*torch.FloatTensor*类型的张量。有关详细信息，请参阅 Wav2Vec2Processor.`call`()。

+   `input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`, *optional*) — 从原始语音波形中提取的 fbank 特征的浮点值。原始语音波形可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过 soundfile 库（`pip install soundfile`）。要准备好数组为`input_features`，应使用 Speech2TextFeatureExtractor 来提取 fbank 特征，填充并转换为`torch.FloatTensor`类型的张量。参见`call`()

+   `return_dict` (`bool`, *optional*) — 如果设置为`True`，模型将返回一个`~utils.Seq2SeqLMOutput`而不是一个普通元组。

+   `kwargs` (*optional*) — 剩余的关键字参数字典。关键字参数有两种类型：

    +   在编码器前向函数中作为`**encoder_kwargs`输入的前缀。

    +   在解码器前向函数中作为`**decoder_kwargs`输入的*decoder_*前缀。

返回

transformers.modeling_outputs.Seq2SeqLMOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.Seq2SeqLMOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（SpeechEncoderDecoderConfig）和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) — 语言建模损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个层的输出）的形状为`(batch_size, sequence_length, hidden_size)`。

    每个层的解码器隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

    解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    编码器在每个层的输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

SpeechEncoderDecoderModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import SpeechEncoderDecoderModel, AutoProcessor
>>> from datasets import load_dataset
>>> import torch

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-xls-r-300m-en-to-15")
>>> model = SpeechEncoderDecoderModel.from_pretrained("facebook/wav2vec2-xls-r-300m-en-to-15")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

>>> input_values = processor(ds[0]["audio"]["array"], return_tensors="pt").input_values
>>> # Inference: Translate English speech to German
>>> generated = model.generate(input_values)
>>> decoded = processor.batch_decode(generated, skip_special_tokens=True)[0]
>>> decoded
'Mr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen heißen zu können.'

>>> # Training: Train model on English transcription
>>> labels = processor(text=ds[0]["text"], return_tensors="pt").input_ids

>>> loss = model(input_values, labels=labels).loss
>>> loss.backward()
```

#### `from_encoder_decoder_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L283)

```py
( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path: str = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path` (`str`, *optional*) — 初始化编码器所需的信息。可以是：

    +   一个字符串，托管在 huggingface.co 模型库中的预训练模型的*模型 id*。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用 save_pretrained()保存的模型权重，例如，`./my_model_directory/`。

    +   一个*tensorflow 索引检查点文件*的路径或 url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用提供的转换脚本将 TensorFlow 检查点转换为 PyTorch 模型并加载 PyTorch 模型的加载路径比较慢。

+   `decoder_pretrained_model_name_or_path` (`str`, *optional*, 默认为`None`) — 初始化解码器所需的信息。可以是：

    +   一个字符串，托管在 huggingface.co 模型库中的预训练模型的*模型 id*。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用 save_pretrained()保存的模型权重，例如，`./my_model_directory/`。

    +   一个*tensorflow 索引检查点文件*的路径或 url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用提供的转换脚本将 TensorFlow 检查点转换为 PyTorch 模型并加载 PyTorch 模型的加载路径比较慢。

+   `model_args`（剩余的位置参数，*optional*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数字典，*可选*）-可用于更新配置对象（加载后）并初始化模型（例如，`output_attentions=True`）。

    +   要更新编码器配置，请为每个配置参数使用前缀*encoder_*。

    +   要更新解码器配置，请为每个配置参数使用前缀*decoder_*。

    +   要更新父模型配置，请不要为每个配置参数使用前缀。

    根据是否提供`config`，行为会有所不同或自动加载。

从预训练模型检查点的库中实例化一个编码器和一个解码器。

默认情况下，使用`model.eval()`将模型设置为评估模式（Dropout 模块被停用）。要训练模型，首先需要使用`model.train()`将其设置回训练模式。

示例：

```py
>>> from transformers import SpeechEncoderDecoderModel

>>> # initialize a wav2vec2bert from a pretrained Wav2Vec2 and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "facebook/wav2vec2-base-960h", "bert-base-uncased"
... )
>>> # saving model after fine-tuning
>>> model.save_pretrained("./wav2vec2bert")
>>> # load fine-tuned model
>>> model = SpeechEncoderDecoderModel.from_pretrained("./wav2vec2bert")
```

## FlaxSpeechEncoderDecoderModel

### `class transformers.FlaxSpeechEncoderDecoderModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L328)

```py
( config: SpeechEncoderDecoderConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（SpeechEncoderDecoderConfig](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`）-计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和`jax.numpy.bfloat16`（在 TPU 上）之一。

    这可用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的数据类型执行。

    请注意，这仅指定了计算的数据类型，并不影响模型参数的数据类型。

    如果要更改模型参数的数据类型，请参阅 to_fp16()和 to_bf16()。

此类可用于使用任何预训练语音自编码模型作为编码器和任何预训练文本自回归模型作为解码器初始化语音序列到文本序列模型。编码器通过 from_pretrained()函数加载，解码器通过 from_pretrained()函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如摘要）上进行微调。

在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu](https://arxiv.org/abs/1907.12461)的论文中展示了使用预训练检查点初始化序列到序列模型以进行序列生成任务的有效性。

此外，在[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)中展示了如何利用大型预训练语音模型进行语音翻译，从而实现显著的性能提升。

在训练/微调了这样一个语音编码器解码器模型之后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。

这个模型继承自 FlaxPreTrainedModel。检查超类文档以了解库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 Flax 亚麻[flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规的 Flax 模块，并参考 Flax 文档以获取有关一般用法和行为的所有相关信息。

FlaxSpeechEncoderDecoderModel 是一个通用的模型类，当使用:meth*~transformers.FlaxAutoModel.from_pretrained*类方法为编码器创建时，将实例化为一个变压器架构，其中模块（flax.nn.Module）是库的一个基本模型类的编码器模块，另一个是解码器模块，为解码器创建时使用:meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained*类方法。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L660)

```py
( inputs: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False freeze_feature_encoder: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `inputs`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length, feature_dim)`的`jnp.ndarray`，*可选*）— 输入原始语音波形或语音特征的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，*例如*通过 soundfile 库（`pip install soundfile`）。要将数组准备成`inputs`，应使用 Wav2Vec2Processor 或 Speech2TextProcessor 进行填充和转换为`torch.FloatTensor`类型的张量。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`范围内：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`jnp.ndarray`，*可选*）— 词汇表中解码器输入序列标记的索引。

    可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。 

    什么是输入 ID？

    如果使用了`past_key_values`，则可选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。

    对于序列到序列训练，应提供`decoder_input_ids`。`decoder_input_ids`应在模型外部创建，方法是将`labels`向右移动，用`pad_token_id`替换-100，并在`decoder_start_token_id`之前添加它们。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`jnp.ndarray`，*可选*）— 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `decoder_position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 每个解码器输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.decoder.max_position_embeddings - 1]`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 如果设置为`True`，模型将返回一个`~utils.FlaxSeq2SeqLMOutput`而不是一个普通元组。

返回值

transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（SpeechEncoderDecoderConfig）和输入。

+   `logits` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。

    解码器每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`jnp.ndarray`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层输出的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。

    编码器每一层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

FlaxSpeechEncoderDecoderModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from transformers import FlaxSpeechEncoderDecoderModel, AutoTokenizer

>>> # load a fine-tuned wav2vec2-2-bart model
>>> model = FlaxSpeechEncoderDecoderModel.from_pretrained("patrickvonplaten/wav2vec2-2-bart-large")
>>> # load output tokenizer
>>> tokenizer_output = AutoTokenizer.from_pretrained("facebook/bart-large")

>>> inputs = jnp.ones((2, 5000), dtype=jnp.float32)

>>> # use bart's special bos, pad and eos tokens
>>> model.config.decoder_start_token_id = model.decoder.config.bos_token_id
>>> model.config.pad_token_id = model.decoder.config.pad_token_id
>>> model.config.eos_token_id = model.decoder.config.eos_token_id

>>> outputs = model.generate(inputs)
# Assert something? More interesting input? dtype correct?
```

#### `from_encoder_decoder_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py#L782)

```py
( encoder_pretrained_model_name_or_path: Union = None decoder_pretrained_model_name_or_path: Union = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path`（`Union[str, os.PathLike]`，*可选*） — 初始化编码器所需的信息。可以是：

    +   一个字符串，托管在 huggingface.co 模型存储库内的预训练模型的*模型 ID*。有效的模型 ID 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用 save_pretrained()保存的模型权重，例如，`./my_model_directory/`。

+   `decoder_pretrained_model_name_or_path`（`Union[str, os.PathLike]`，*可选*，默认为`None`） — 初始化解码器所需的信息。可以是：

    +   一个字符串，托管在 huggingface.co 模型存储库内的预训练模型的*模型 ID*。有效的模型 ID 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用 save_pretrained()保存的模型权重，例如，`./my_model_directory/`。

+   `model_args`（剩余的位置参数，*可选*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数字典，*可选*） — 可用于更新配置对象（在加载后）并初始化模型（例如，`output_attentions=True`）。

    +   要更新编码器配置，请为每个配置参数使用前缀*encoder_*。

    +   要更新解码器配置，请为每个配置参数使用前缀*decoder_*。

    +   要更新父模型配置，请不要为每个配置参数使用前缀。

    根据是否提供`config`或自动加载而表现不同。

从预训练模型检查点的一个或两个库基类实例化编码器和解码器。

示例：

```py
>>> from transformers import FlaxSpeechEncoderDecoderModel

>>> # initialize a wav2vec2-2-bart from pretrained wav2vec2 and bart models. Note that the cross-attention layers will be randomly initialized
>>> model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "facebook/wav2vec2-large-lv60", "facebook/bart-large"
... )
>>> # saving model after fine-tuning
>>> model.save_pretrained("./wav2vec2-2-bart-large")
>>> # load fine-tuned model
>>> model = FlaxSpeechEncoderDecoderModel.from_pretrained("./wav2vec2-2-bart-large")
```
