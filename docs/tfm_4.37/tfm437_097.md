# Transformer 模型家族

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_summary`](https://huggingface.co/docs/transformers/v4.37.2/en/model_summary)

自 2017 年引入[原始 Transformer](https://arxiv.org/abs/1706.03762)模型以来，它已经激发了许多新颖且令人兴奋的模型，超越了自然语言处理（NLP）任务。有用于[预测蛋白质的折叠结构](https://huggingface.co/blog/deep-learning-with-proteins)、[训练猎豹奔跑](https://huggingface.co/blog/train-decision-transformers)和[时间序列预测](https://huggingface.co/blog/time-series-transformers)的模型。有这么多 Transformer 变体可用，很容易忽略更大的画面。所有这些模型的共同之处是它们都基于原始 Transformer 架构。一些模型只使用编码器或解码器，而其他一些则同时使用两者。这提供了一个有用的分类法，可以对 Transformer 家族中的模型进行分类和检查高层次的差异，这将帮助您理解以前未遇到的 Transformer。

如果您不熟悉原始 Transformer 模型或需要复习，请查看 Hugging Face 课程中的[Transformer 工作原理](https://huggingface.co/course/chapter1/4?fw=pt)章节。

[`www.youtube.com/embed/H39Z_720T5s`](https://www.youtube.com/embed/H39Z_720T5s)

## 计算机视觉

[`www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1`](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1)

### 卷积网络

长时间以来，卷积网络（CNNs）一直是计算机视觉任务的主导范式，直到[视觉 Transformer](https://arxiv.org/abs/2010.11929)展示了其可扩展性和效率。即使如此，CNN 的一些最佳特性，如平移不变性，是如此强大（尤其对于某些任务），以至于一些 Transformer 在其架构中引入了卷积。ConvNeXt 颠倒了这种交换，并从 Transformer 中引入设计选择来现代化 CNN。例如，ConvNeXt 使用非重叠滑动窗口将图像分块化，并使用更大的内核来增加其全局感受野。ConvNeXt 还做出了几个层设计选择，以提高内存效率和性能，因此它与 Transformer 竞争有利！

### 编码器

视觉 Transformer（ViT）为计算机视觉任务打开了没有卷积的大门。ViT 使用标准 Transformer 编码器，但其主要突破在于它如何处理图像。它将图像分割成固定大小的补丁，并使用它们创建嵌入，就像将句子分割成标记一样。ViT 利用 Transformer 的高效架构展示了与当时的 CNN 竞争力的结果，同时需要更少的资源进行训练。ViT 很快被其他视觉模型跟随，这些模型也可以处理像分割和检测这样的密集视觉任务。

其中一个模型是 Swin Transformer。它从较小的补丁中构建分层特征图（类似于 CNN👀，不同于 ViT），并在更深层中将它们与相邻的补丁合并。注意力仅在局部窗口内计算，并且在注意力层之间移动窗口以创建连接以帮助模型学习更好。由于 Swin Transformer 可以生成分层特征图，因此它是密集预测任务（如分割和检测）的良好候选。SegFormer 也使用 Transformer 编码器构建分层特征图，但它在顶部添加了一个简单的多层感知器（MLP）解码器，以组合所有特征图并进行预测。

其他视觉模型，如 BeIT 和 ViTMAE，从 BERT 的预训练目标中汲取灵感。BeIT 通过*masked image modeling (MIM)* 进行预训练；图像补丁被随机屏蔽，图像也被标记为视觉标记。BeIT 被训练以预测与被屏蔽补丁对应的视觉标记。ViTMAE 有一个类似的预训练目标，只是它必须预测像素而不是视觉标记。不寻常的是，75%的图像补丁被屏蔽！解码器从被屏蔽的标记和编码的补丁中重建像素。预训练后，解码器被丢弃，编码器准备好用于下游任务。

### 解码器

仅解码器的视觉模型很少，因为大多数视觉模型依赖编码器来学习图像表示。但对于像图像生成这样的用例，解码器是一个自然的选择，正如我们从 GPT-2 等文本生成模型中看到的那样。ImageGPT 使用与 GPT-2 相同的架构，但它不是预测序列中的下一个标记，而是预测图像中的下一个像素。除了图像生成，ImageGPT 也可以进行微调以用于图像分类。

### 编码器-解码器

视觉模型通常使用编码器（也称为骨干）来提取重要的图像特征，然后将它们传递给 Transformer 解码器。DETR 有一个预训练的骨干，但它还使用完整的 Transformer 编码器-解码器架构进行目标检测。编码器学习图像表示，并将其与对象查询（每个对象查询是一个专注于图像中的区域或对象的学习嵌入）结合在解码器中。DETR 预测每个对象查询的边界框坐标和类别标签。

## 自然语言处理

[`www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1`](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1)

### 编码器

BERT 是一个仅包含编码器的 Transformer，它会随机屏蔽输入中的某些标记，以避免看到其他标记，这样可以防止其“作弊”。预训练的目标是基于上下文预测被屏蔽的标记。这使得 BERT 能够充分利用左右上下文来帮助学习输入的更深层和更丰富的表示。然而，BERT 的预训练策略仍有改进的空间。RoBERTa 通过引入一个新的预训练配方来改进这一点，该配方包括更长时间和更大批次的训练，在每个时代随机屏蔽标记，而不仅仅是在预处理期间一次，以及移除下一个句子预测目标。

提高性能的主要策略是增加模型大小。但是训练大型模型在计算上是昂贵的。减少计算成本的一种方法是使用像 DistilBERT 这样的较小模型。DistilBERT 使用[知识蒸馏](https://arxiv.org/abs/1503.02531) - 一种压缩技术 - 来创建一个较小版本的 BERT，同时保留几乎所有的语言理解能力。

然而，大多数 Transformer 模型继续朝着更多参数的方向发展，导致出现了专注于提高训练效率的新模型。ALBERT 通过两种方式降低参数数量来减少内存消耗：将更大的词汇嵌入分为两个较小的矩阵，并允许层共享参数。DeBERTa 添加了一个解耦的注意机制，其中单词及其位置分别编码在两个向量中。注意力是从这些单独的向量计算而来，而不是从包含单词和位置嵌入的单个向量中计算。Longformer 也专注于使注意力更加高效，特别是用于处理具有更长序列长度的文档。它使用局部窗口注意力（仅计算围绕每个标记的固定窗口大小的注意力）和全局注意力（仅用于特定任务标记，如`[CLS]`用于分类）的组合，以创建一个稀疏的注意力矩阵，而不是完整的注意力矩阵。

### 解码器

GPT-2 是一个仅解码器的 Transformer，用于预测序列中的下一个单词。它会屏蔽右侧的标记，以防模型通过向前查看来“作弊”。通过在大量文本上进行预训练，GPT-2 在生成文本方面表现得非常出色，即使文本有时并不准确或真实。但是 GPT-2 缺乏 BERT 预训练的双向上下文，这使得它不适用于某些任务。XLNET 结合了 BERT 和 GPT-2 的预训练目标的优点，使用排列语言建模目标（PLM）使其能够双向学习。

在 GPT-2 之后，语言模型变得更大，现在被称为*大型语言模型（LLMs）*。如果在足够大的数据集上进行预训练，LLMs 可以展示少量甚至零-shot 学习。GPT-J 是一个具有 6B 参数并在 400B 标记上训练的 LLM。GPT-J 之后是 OPT，一系列仅解码器模型，其中最大的模型为 175B，并在 180B 标记上训练。BLOOM 也在同一时间发布，该系列中最大的模型有 176B 参数，并在 46 种语言和 13 种编程语言中训练了 366B 标记。

### 编码器-解码器

BART 保留了原始的 Transformer 架构，但通过*文本填充*损坏修改了预训练目标，其中一些文本段被替换为单个`mask`标记。解码器预测未损坏的标记（未来标记被屏蔽），并使用编码器的隐藏状态来帮助它。Pegasus 类似于 BART，但 Pegasus 屏蔽整个句子而不是文本段。除了遮蔽语言建模，Pegasus 还通过间隙句子生成（GSG）进行预训练。GSG 目标屏蔽了对文档重要的整个句子，并用`mask`标记替换它们。解码器必须从剩余的句子中生成输出。T5 是一个更独特的模型，将所有 NLP 任务都转化为使用特定前缀的文本到文本问题。例如，前缀`Summarize:`表示一个总结任务。T5 通过监督（GLUE 和 SuperGLUE）训练和自监督训练（随机抽样并丢弃 15%的标记）进行预训练。

## 音频

[`www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1`](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1)

### 编码器

Wav2Vec2 使用 Transformer 编码器直接从原始音频波形中学习语音表示。它通过对比任务进行预训练，以确定一组错误的语音表示中的真实语音表示。HuBERT 类似于 Wav2Vec2，但训练过程不同。目标标签是通过聚类步骤创建的，其中相似音频片段被分配到一个成为隐藏单元的簇中。隐藏单元被映射到一个嵌入以进行预测。

### 编码器-解码器

Speech2Text 是一个专为自动语音识别（ASR）和语音翻译设计的语音模型。该模型接受从音频波形中提取的对数梅尔滤波器特征，并预训练自回归地生成转录或翻译。Whisper 也是一个 ASR 模型，但与许多其他语音模型不同，它是在大量✨标记的✨音频转录数据上进行预训练，以实现零样本性能。数据集中还包含大量非英语语言，这意味着 Whisper 也可以用于资源稀缺的语言。在结构上，Whisper 类似于 Speech2Text。音频信号被转换为由编码器编码的对数梅尔频谱图。解码器从编码器的隐藏状态和先前的标记中自回归地生成转录。

## 多模态

[`www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1`](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)

### 编码器

VisualBERT 是一个用于视觉-语言任务的多模态模型，发布于 BERT 之后不久。它结合了 BERT 和一个预训练的目标检测系统，将图像特征提取为视觉嵌入，与文本嵌入一起传递给 BERT。VisualBERT 基于未屏蔽的文本和视觉嵌入预测被屏蔽的文本，并且还必须预测文本是否与图像对齐。当 ViT 发布时，ViLT 采用了 ViT 的架构，因为这样更容易获取图像嵌入。图像嵌入与文本嵌入一起进行处理。从那里，ViLT 通过图像文本匹配、屏蔽语言建模和整词屏蔽进行预训练。

CLIP 采用了不同的方法，对(`图像`，`文本`)进行一对预测。一个图像编码器（ViT）和一个文本编码器（Transformer）在一个包含 4 亿个(`图像`，`文本`)对的数据集上进行联合训练，以最大化(`图像`，`文本`)对的图像和文本嵌入之间的相似性。在预训练之后，您可以使用自然语言指示 CLIP 预测给定图像的文本，反之亦然。OWL-ViT 在 CLIP 的基础上构建，将其作为零样本目标检测的骨干。在预训练之后，添加了一个目标检测头，以对(`类别`，`边界框`)对进行集合预测。

### 编码器-解码器

光学字符识别（OCR）是一个长期存在的文本识别任务，通常涉及几个组件来理解图像并生成文本。TrOCR 使用端到端的变换器简化了这个过程。编码器是一种 ViT 风格的模型，用于图像理解，并将图像处理为固定大小的补丁。解码器接受编码器的隐藏状态，并自回归地生成文本。Donut 是一个更通用的视觉文档理解模型，不依赖于基于 OCR 的方法。它使用 Swin 变换器作为编码器，多语言 BART 作为解码器。Donut 经过预训练，通过根据图像和文本注释预测下一个单词来阅读文本。解码器根据提示生成一个令牌序列。提示由每个下游任务的特殊令牌表示。例如，文档解析有一个特殊的`parsing`令牌，它与编码器的隐藏状态结合，将文档解析为结构化的输出格式（JSON）。

## 强化学习

[`www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1`](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)

### 解码器

决策和轨迹转换器将状态、动作和奖励视为序列建模问题。决策变换器生成一系列动作，这些动作基于回报、过去状态和动作，导致未来期望的回报。在最后*K*个时间步中，这三种模态都被转换为令牌嵌入，并由类似 GPT 的模型处理，以预测未来的动作令牌。轨迹变换器也对状态、动作和奖励进行标记化，并使用 GPT 架构处理它们。与专注于奖励条件的决策变换器不同，轨迹变换器使用波束搜索生成未来的动作。
