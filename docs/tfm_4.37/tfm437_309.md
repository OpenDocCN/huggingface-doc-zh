# MMS

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms)

## 概述

MMS 模型是由 Vineel Pratap、Andros Tjandra、Bowen Shi、Paden Tomasello、Arun Babu、Sayani Kundu、Ali Elkahky、Zhaoheng Ni、Apoorv Vyas、Maryam Fazel-Zarandi、Alexei Baevski、Yossi Adi、Xiaohui Zhang、Wei-Ning Hsu、Alexis Conneau、Michael Auli 在[将语音技术扩展到 1000 多种语言](https://arxiv.org/abs/2305.13516)中提出的。

论文的摘要如下：

*扩大语音技术的语言覆盖范围有可能提高更多人获取信息的机会。然而，当前的语音技术仅限于大约 100 种语言，这只是全球约 7000 种语言中的一小部分。大规模多语言语音（MMS）项目通过 10-40 倍增加了支持的语言数量，具体取决于任务。主要成分是基于公开可用宗教文本的新数据集，并有效地利用自监督学习。我们构建了覆盖 1406 种语言的预训练 wav2vec 2.0 模型，一种单一的支持 1107 种语言的多语言自动语音识别模型，以及相同数量语言的语音合成模型，以及支持 4017 种语言的语言识别模型。实验表明，我们的多语言语音识别模型在 FLEURS 基准测试的 54 种语言上将 Whisper 的词错误率减少了一半以上，同时在训练时仅使用了少量标记数据。*

以下是 MMS 项目中开源的不同模型。这些模型和代码最初发布在[这里](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)。我们已将它们添加到`transformers`框架中，使其更易于使用。

### 自动语音识别（ASR）

ASR 模型的检查点可以在这里找到：[mms-1b-fl102](https://huggingface.co/facebook/mms-1b-fl102)，[mms-1b-l1107](https://huggingface.co/facebook/mms-1b-l1107)，[mms-1b-all](https://huggingface.co/facebook/mms-1b-all)。为了获得最佳准确性，请使用`mms-1b-all`模型。

提示：

+   所有 ASR 模型都接受一个与语音信号的原始波形对应的浮点数组。原始波形应该使用 Wav2Vec2FeatureExtractor 进行预处理。

+   这些模型是使用连接主义时间分类（CTC）进行训练的，因此必须使用 Wav2Vec2CTCTokenizer 对模型输出进行解码。

+   您可以通过 load_adapter()为不同语言加载不同的语言适配器权重。语言适配器仅包含大约 200 万个参数，因此在需要时可以高效地动态加载。

#### 加载

默认情况下，MMS 会加载英语的适配器权重。如果您想加载另一种语言的适配器权重，请确保指定`target_lang=<your-chosen-target-lang>`以及`"ignore_mismatched_sizes=True`。必须传递`ignore_mismatched_sizes=True`关键字，以允许语言模型头根据指定语言的词汇重新调整大小。同样，处理器应该加载相同的目标语言。

```py
from transformers import Wav2Vec2ForCTC, AutoProcessor

model_id = "facebook/mms-1b-all"
target_lang = "fra"

processor = AutoProcessor.from_pretrained(model_id, target_lang=target_lang)
model = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang, ignore_mismatched_sizes=True)
```

您可以安全地忽略警告，例如：

```py
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:
- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([314]) in the model instantiated
- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([314, 1280]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

如果要使用 ASR 流程，可以按如下方式加载所选的目标语言：

```py
from transformers import pipeline

model_id = "facebook/mms-1b-all"
target_lang = "fra"

pipe = pipeline(model=model_id, model_kwargs={"target_lang": "fra", "ignore_mismatched_sizes": True})
```

#### 推理

接下来，让我们看看如何在调用`~PretrainedModel.from_pretrained`之后运行 MMS 的推理并更改适配器层。首先，我们使用[Datasets](https://github.com/huggingface/datasets)加载不同语言的音频数据。

```py
from datasets import load_dataset, Audio

# English
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
en_sample = next(iter(stream_data))["audio"]["array"]

# French
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "fr", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
fr_sample = next(iter(stream_data))["audio"]["array"]
```

接下来，我们加载模型和处理器

```py
from transformers import Wav2Vec2ForCTC, AutoProcessor
import torch

model_id = "facebook/mms-1b-all"

processor = AutoProcessor.from_pretrained(model_id)
model = Wav2Vec2ForCTC.from_pretrained(model_id)
```

现在我们处理音频数据，将处理后的音频数据传递给模型并转录模型输出，就像我们通常为 Wav2Vec2ForCTC 所做的那样。

```py
inputs = processor(en_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

ids = torch.argmax(outputs, dim=-1)[0]
transcription = processor.decode(ids)
# 'joe keton disapproved of films and buster also had reservations about the media'
```

现在我们可以将相同的模型保留在内存中，并通过调用方便的 load_adapter()函数为模型和 set_target_lang()为分词器切换语言适配器。我们将目标语言作为输入传递 - `"fra"`表示法语。

```py
processor.tokenizer.set_target_lang("fra")
model.load_adapter("fra")

inputs = processor(fr_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

ids = torch.argmax(outputs, dim=-1)[0]
transcription = processor.decode(ids)
# "ce dernier est volé tout au long de l'histoire romaine"
```

以同样的方式，语言可以切换为所有其他支持的语言。请查看：

```py
processor.tokenizer.vocab.keys()
```

查看所有支持的语言。

为了进一步提高 ASR 模型的性能，可以使用语言模型解码。有关更多详细信息，请参阅此处的文档[here](https://huggingface.co/facebook/mms-1b-all)。

### 语音合成（TTS）

MMS-TTS 使用与 VITS 相同的模型架构，该架构在 v4.33 中添加到🤗 Transformers 中。MMS 为项目中的 1100 多种语言中的每种语言训练了一个单独的模型检查点。所有可用的检查点都可以在 Hugging Face Hub 上找到：[facebook/mms-tts](https://huggingface.co/models?sort=trending&search=facebook%2Fmms-tts)，以及[VITS](https://huggingface.co/docs/transformers/main/en/model_doc/vits)下的推理文档。

#### 推理

要使用 MMS 模型，首先更新到 Transformers 库的最新版本：

```py
pip install --upgrade transformers accelerate
```

由于 VITS 中的基于流的模型是非确定性的，因此最好设置一个种子以确保输出的可重现性。

+   对于具有罗马字母表的语言，例如英语或法语，可以直接使用分词器对文本输入进行预处理。以下代码示例使用 MMS-TTS 英语检查点运行前向传递：

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
model = VitsModel.from_pretrained("facebook/mms-tts-eng")

inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

set_seed(555)  # make deterministic

with torch.no_grad():
   outputs = model(**inputs)

waveform = outputs.waveform[0]
```

生成的波形可以保存为`.wav`文件：

```py
import scipy

scipy.io.wavfile.write("synthesized_speech.wav", rate=model.config.sampling_rate, data=waveform)
```

或者在 Jupyter Notebook / Google Colab 中显示：

```py
from IPython.display import Audio

Audio(waveform, rate=model.config.sampling_rate)
```

对于具有非罗马字母表的某些语言，例如阿拉伯语、普通话或印地语，需要使用[`uroman`](https://github.com/isi-nlp/uroman) perl 包对文本输入进行预处理为罗马字母表。

您可以通过检查预训练的`tokenizer`的`is_uroman`属性来检查您的语言是否需要`uroman`包：

```py
from transformers import VitsTokenizer

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
print(tokenizer.is_uroman)
```

如果需要，在将文本输入传递给`VitsTokenizer`之前，应该将 uroman 包应用于您的文本输入**之前**，因为目前分词器不支持执行预处理。

为了做到这一点，首先将 uroman 存储库克隆到本地计算机，并将 bash 变量`UROMAN`设置为本地路径：

```py
git clone https://github.com/isi-nlp/uroman.git
cd uroman
export UROMAN=$(pwd)
```

然后，您可以使用以下代码片段对文本输入进行预处理。您可以依赖使用 bash 变量`UROMAN`指向 uroman 存储库，或者可以将 uroman 目录作为参数传递给`uromaize`函数：

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed
import os
import subprocess

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-kor")
model = VitsModel.from_pretrained("facebook/mms-tts-kor")

def uromanize(input_string, uroman_path):
    """Convert non-Roman strings to Roman using the `uroman` perl package."""
    script_path = os.path.join(uroman_path, "bin", "uroman.pl")

    command = ["perl", script_path]

    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    # Execute the perl command
    stdout, stderr = process.communicate(input=input_string.encode())

    if process.returncode != 0:
        raise ValueError(f"Error {process.returncode}: {stderr.decode()}")

    # Return the output as a string and skip the new-line character at the end
    return stdout.decode()[:-1]

text = "이봐 무슨 일이야"
uromaized_text = uromanize(text, uroman_path=os.environ["UROMAN"])

inputs = tokenizer(text=uromaized_text, return_tensors="pt")

set_seed(555)  # make deterministic
with torch.no_grad():
   outputs = model(inputs["input_ids"])

waveform = outputs.waveform[0]
```

**提示：**

+   MMS-TTS 检查点是在小写、无标点的文本上训练的。默认情况下，`VitsTokenizer`通过删除任何大小写和标点来*规范化*输入，以避免将超出词汇表的字符传递给模型。因此，模型对大小写和标点不敏感，因此应避免在文本提示中使用它们。您可以通过在调用分词器时设置`noramlize=False`来禁用规范化，但这将导致意外行为，不建议这样做。

+   通过将属性`model.speaking_rate`设置为所选值，可以改变说话速度。同样，噪声的随机性由`model.noise_scale`控制：

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
model = VitsModel.from_pretrained("facebook/mms-tts-eng")

inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

# make deterministic
set_seed(555)  

# make speech faster and more noisy
model.speaking_rate = 1.5
model.noise_scale = 0.8

with torch.no_grad():
   outputs = model(**inputs)
```

### 语言识别（LID）

根据能够识别的语言数量，提供不同的 LID 模型 - [126](https://huggingface.co/facebook/mms-lid-126)，[256](https://huggingface.co/facebook/mms-lid-256)，[512](https://huggingface.co/facebook/mms-lid-512)，[1024](https://huggingface.co/facebook/mms-lid-1024)，[2048](https://huggingface.co/facebook/mms-lid-2048)，[4017](https://huggingface.co/facebook/mms-lid-4017)。

#### 推理

首先，我们安装 transformers 和其他一些库

```py
pip install torch accelerate datasets[audio]
pip install --upgrade transformers
```

接下来，我们通过`datasets`加载一些音频样本。确保音频数据采样率为 16000 kHz。

```py
from datasets import load_dataset, Audio

# English
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
en_sample = next(iter(stream_data))["audio"]["array"]

# Arabic
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "ar", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
ar_sample = next(iter(stream_data))["audio"]["array"]
```

接下来，我们加载模型和处理器

```py
from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor
import torch

model_id = "facebook/mms-lid-126"

processor = AutoFeatureExtractor.from_pretrained(model_id)
model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)
```

现在我们处理音频数据，将处理后的音频数据传递给模型进行分类，就像我们通常对 Wav2Vec2 音频分类模型进行的操作一样，比如[ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)

```py
# English
inputs = processor(en_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

lang_id = torch.argmax(outputs, dim=-1)[0].item()
detected_lang = model.config.id2label[lang_id]
# 'eng'

# Arabic
inputs = processor(ar_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

lang_id = torch.argmax(outputs, dim=-1)[0].item()
detected_lang = model.config.id2label[lang_id]
# 'ara'
```

查看检查点支持的所有语言，可以按照以下方式打印语言标识符：

```py
processor.id2label.values()
```

### 音频预训练模型

预训练模型提供两种不同大小的模型 - [300M](https://huggingface.co/facebook/mms-300m)，[1Bil](https://huggingface.co/facebook/mms-1b)。

ASR 架构的 MMS 基于 Wav2Vec2 模型，请参考 Wav2Vec2 的文档页面以获取有关如何针对各种下游任务微调模型的更多详细信息。

MMS-TTS 使用与 VITS 相同的模型架构，请参考 VITS 的文档页面获取 API 参考。
