# Pix2Struct

> 原始文本: [`huggingface.co/docs/transformers/v4.37.2/en/model_doc/pix2struct`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pix2struct)

## 概述

Pix2Struct 模型是由 Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova 在[《Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding》](https://arxiv.org/abs/2210.03347)中提出的。

论文摘要如下:

> 视觉定位语言是无处不在的 — 源头包括带有图表的教科书、带有图像和表格的网页，以及带有按钮和表单的移动应用程序。也许正是由于这种多样性，先前的工作通常依赖于具有限制性的领域特定配方，对底层数据、模型架构和目标的共享有限。我们提出了 Pix2Struct，这是一个预训练的图像到文本模型，用于纯粹的视觉语言理解，可以在包含视觉定位语言的任务上进行微调。Pix2Struct 通过学习将屏幕截图的掩码解析为简化的 HTML 来进行预训练。网络，其丰富的视觉元素清晰地反映在 HTML 结构中，为下游任务的多样性提供了大量的预训练数据。直观地说，这个目标包含了常见的预训练信号，如 OCR、语言建模、图像字幕。除了新颖的预训练策略，我们还引入了可变分辨率的输入表示和更灵活的语言和视觉输入集成，其中语言提示（如问题）直接呈现在输入图像的顶部。我们首次展示，单个预训练模型可以在四个领域的九项任务中的六项中取得最先进的结果: 文档、插图、用户界面和自然图像。

提示:

Pix2Struct 已经在各种任务和数据集上进行了微调，包括图像字幕、视觉问答（VQA）以及不同输入（书籍、图表、科学图表）、字幕 UI 组件等。完整列表可以在论文的表 1 中找到。因此，我们建议您将这些模型用于它们已经进行微调的任务。例如，如果您想要将 Pix2Struct 用于 UI 字幕，您应该使用在 UI 数据集上进行微调的模型。如果您想要将 Pix2Struct 用于图像字幕，您应该使用在自然图像字幕数据集上进行微调的模型，依此类推。

如果您想要使用模型执行有条件的文本字幕，确保使用`add_special_tokens=False`的处理器。

该模型由[ybelkada](https://huggingface.co/ybelkada)贡献。原始代码可以在[这里](https://github.com/google-research/pix2struct)找到。

## 资源

+   [微调笔记本](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)

+   [所有模型](https://huggingface.co/models?search=pix2struct)

## Pix2StructConfig

### `class transformers.Pix2StructConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L292)

```py
( text_config = None vision_config = None initializer_factor = 1.0 initializer_range = 0.02 is_vqa = False tie_word_embeddings = False is_encoder_decoder = True **kwargs )
```

参数

+   `text_config` (`dict`, *可选*) — 用于初始化 Pix2StructTextConfig 的配置选项字典。

+   `vision_config` (`dict`, *可选*) — 用于初始化 Pix2StructVisionConfig 的配置选项字典。

+   `initializer_factor` (`float`, *可选*, 默认为 1.0) — 用于乘以初始化范围的因子。

+   `initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的 truncated_normal_initializer 的标准差。

+   `is_vqa` (`bool`, *可选*, 默认为`False`) — 模型是否已经为 VQA 进行了微调。

+   `kwargs` (*optional*) — 关键字参数的字典。

Pix2StructConfig 是用于存储 Pix2StructForConditionalGeneration 配置的配置类。根据指定的参数实例化 Pix2Struct 模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生类似于 Pix2Struct-base [google/pix2struct-base](https://huggingface.co/google/pix2struct-base) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import Pix2StructConfig, Pix2StructForConditionalGeneration

>>> # Initializing a Pix2StructConfig with google/pix2struct-base style configuration
>>> configuration = Pix2StructConfig()

>>> # Initializing a Pix2StructForConditionalGeneration (with random weights) from the google/pix2struct-base style configuration
>>> model = Pix2StructForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a Pix2StructConfig from a Pix2StructTextConfig and a Pix2StructVisionConfig

>>> # Initializing a Pix2Struct text and Pix2Struct vision configuration
>>> config_text = Pix2StructTextConfig()
>>> config_vision = Pix2StructVisionConfig()

>>> config = Pix2StructConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L378)

```py
( text_config: Pix2StructTextConfig vision_config: Pix2StructVisionConfig **kwargs ) → export const metadata = 'undefined';Pix2StructConfig
```

返回

Pix2StructConfig

配置对象的一个实例

从 pix2struct 文本模型配置和 pix2struct 视觉模型配置实例化一个 Pix2StructConfig（或派生类）。

## Pix2StructTextConfig

### `class transformers.Pix2StructTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L33)

```py
( vocab_size = 50244 hidden_size = 768 d_kv = 64 d_ff = 2048 num_layers = 12 num_heads = 12 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 dense_act_fn = 'gelu_new' decoder_start_token_id = 0 use_cache = False pad_token_id = 0 eos_token_id = 1 tie_word_embeddings = False is_decoder = True **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 50244) — `Pix2Struct` 文本模型的词汇量。定义了在调用 Pix2StructTextModel 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `d_kv` (`int`, *optional*, defaults to 64) — 每个注意力头中键、查询、值投影的维度。

+   `d_ff` (`int`, *optional*, defaults to 2048) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `num_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数量。

+   `num_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `relative_attention_num_buckets` (`int`, *optional*, defaults to 32) — 每个注意力层使用的桶数量。

+   `relative_attention_max_distance` (`int`, *optional*, defaults to 128) — 用于桶分离的较长序列的最大距离。

+   `dropout_rate` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `layer_norm_epsilon` (`float`, *optional*, defaults to 1e-6) — 层归一化层使用的 epsilon。

+   `initializer_factor` (`float`, *optional*, defaults to 1.0) — 初始化所有权重矩阵的因子（应保持为 1，用于内部初始化测试）。

+   `dense_act_fn` (`Union[Callable, str]`, *optional*, defaults to `"gelu_new"`) — 非线性激活函数（函数或字符串）。

+   `decoder_start_token_id` (`int`, *optional*, defaults to 0) — `decoder_start_token_id` 标记的 id。

+   `use_cache` (`bool`, *optional*, defaults to `False`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `pad_token_id` (`int`, *optional*, defaults to 0) — `padding` 标记的 id。

+   `eos_token_id` (`int`, *optional*, defaults to 1) — `end-of-sequence` 标记的 id。

这是用于存储 Pix2StructTextModel 配置的配置类。根据指定的参数实例化 Pix2Struct 文本模型，定义模型架构。使用默认实例化配置将产生类似于 Pix2Struct 文本解码器的配置，该解码器由[google/pix2struct-base](https://huggingface.co/google/pix2struct-base)架构使用。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import Pix2StructTextConfig, Pix2StructTextModel

>>> # Initializing a Pix2StructTextConfig with google/pix2struct-base style configuration
>>> configuration = Pix2StructTextConfig()

>>> # Initializing a Pix2StructTextModel (with random weights) from the google/pix2struct-base style configuration
>>> model = Pix2StructTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Pix2StructVisionConfig

### `class transformers.Pix2StructVisionConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L173)

```py
( hidden_size = 768 patch_embed_hidden_size = 768 d_ff = 2048 d_kv = 64 num_hidden_layers = 12 num_attention_heads = 12 dense_act_fn = 'gelu_new' layer_norm_eps = 1e-06 dropout_rate = 0.0 attention_dropout = 0.0 initializer_range = 1e-10 initializer_factor = 1.0 seq_len = 4096 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, 默认为 768) — 编码器层和池化器层的维度。

+   `patch_embed_hidden_size` (`int`, *optional*, 默认为 768) — Transformer 编码器中输入 patch_embedding 层的维度。

+   `d_ff` (`int`, *optional*, 默认为 2048) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `d_kv` (`int`, *optional*, 默认为 64) — 每个注意力头部的键、查询、值投影的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `dense_act_fn` (`str`或`function`, *optional*, 默认为`"gelu_new"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"` `"gelu"`。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-06) — 层归一化层使用的 epsilon。

+   `dropout_rate` (`float`, *optional*, 默认为 0.0) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, 默认为 0.0) — 注意力概率的 dropout 比率。

+   `initializer_range` (`float`, *optional*, 默认为 1e-10) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, 默认为 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1，用于内部初始化测试）。

+   `seq_len` (`int`, *optional*, 默认为 4096) — 模型支持的最大序列长度（这里是 patch 的数量）。

+   `relative_attention_num_buckets` (`int`, *optional*, 默认为 32) — 每个注意力层使用的桶数量。

+   `relative_attention_max_distance` (`int`, *optional*, 默认为 128) — 每个注意力层使用的最大距离（以标记为单位）。

这是用于存储 Pix2StructVisionModel 配置的配置类。根据指定的参数实例化 Pix2Struct 视觉模型，定义模型架构。使用默认实例化配置将产生类似于 Pix2Struct-base [google/pix2struct-base](https://huggingface.co/google/pix2struct-base)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import Pix2StructVisionConfig, Pix2StructVisionModel

>>> # Initializing a Pix2StructVisionConfig with google/pix2struct-base style configuration
>>> configuration = Pix2StructVisionConfig()

>>> # Initializing a Pix2StructVisionModel (with random weights) from the google/pix2struct-base style configuration
>>> model = Pix2StructVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Pix2StructProcessor

### `class transformers.Pix2StructProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L26)

```py
( image_processor tokenizer )
```

参数

+   `image_processor` (`Pix2StructImageProcessor`) — 一个 Pix2StructImageProcessor 的实例。图像处理器是必需的输入。

+   `tokenizer` (Union[`T5TokenizerFast`, `T5Tokenizer`]) — 一个[‘T5TokenizerFast`]或[‘T5Tokenizer`]的实例。Tokenizer 是必需的输入。

构建一个 PIX2STRUCT 处理器，将 BERT tokenizer 和 PIX2STRUCT 图像处理器封装成一个处理器。

Pix2StructProcessor 提供了 Pix2StructImageProcessor 和 T5TokenizerFast 的所有功能。查看`__call__()`和 decode()的文档字符串以获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L145)

```py
( *args **kwargs )
```

这个方法将所有参数转发给 Pix2StructTokenizerFast 的 batch_decode()。请参考此方法的文档字符串以获取更多信息。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L152)

```py
( *args **kwargs )
```

这个方法将所有参数转发给 Pix2StructTokenizerFast 的 decode()。请参考此方法的文档字符串以获取更多信息。

## Pix2StructImageProcessor

### `class transformers.Pix2StructImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/image_processing_pix2struct.py#L188)

```py
( do_convert_rgb: bool = True do_normalize: bool = True patch_size: Dict = None max_patches: int = 2048 is_vqa: bool = False **kwargs )
```

参数

+   `do_convert_rgb` (`bool`, *可选*, 默认为 `True`) — 是否将图像转换为 RGB 格式。

+   `do_normalize` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。根据 Pix2Struct 论文和代码，图像使用自己的均值和标准差进行归一化。

+   `patch_size` (`Dict[str, int]`, *可选*, 默认为 `{"height" -- 16, "width": 16}`): 用于图像的补丁大小。根据 Pix2Struct 论文和代码，补丁大小为 16x16。

+   `max_patches` (`int`, *可选*, 默认为 2048) — 从图像中提取的最大补丁数，根据[Pix2Struct 论文](https://arxiv.org/pdf/2210.03347.pdf)。

+   `is_vqa` (`bool`, *可选*, 默认为 `False`) — 图像处理器是否用于 VQA 任务。如果为`True`并且传入了`header_text`，则文本将呈现在输入图像上。

构建一个 Pix2Struct 图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/image_processing_pix2struct.py#L347)

```py
( images: Union header_text: Optional = None do_convert_rgb: bool = None do_normalize: Optional = None max_patches: Optional = None patch_size: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个图像或图像批次。

+   `header_text` (`Union[List[str], str]`, *可选*) — 要呈现为标题的文本。仅当`image_processor.is_vqa`为`True`时才有效。

+   `do_convert_rgb`（`bool`，*可选*，默认为 `self.do_convert_rgb`）— 是否将图像转换为 RGB。

+   `do_normalize`（`bool`，*可选*，默认为 `self.do_normalize`）— 是否对图像进行标准化。

+   `max_patches`（`int`，*可选*，默认为 `self.max_patches`）— 要提取的最大补丁数。

+   `patch_size`（`dict`，*可选*，默认为 `self.patch_size`）— 包含补丁高度和宽度的字典。

+   `return_tensors`（`str` 或 `TensorType`，*可选*）— 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`：返回类型为 `tf.Tensor` 的批次。

    +   `TensorType.PYTORCH` 或 `'pt'`：返回类型为 `torch.Tensor` 的批次。

    +   `TensorType.NUMPY` 或 `'np'`：返回类型为 `np.ndarray` 的批次。

    +   `TensorType.JAX` 或 `'jax'`：返回类型为 `jax.numpy.ndarray` 的批次。

+   `data_format`（`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`）— 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (num_channels, height, width) 格式表示。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以 (height, width, num_channels) 格式表示。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format`（`ChannelDimension` 或 `str`，*可选*）— 输入图像的通道维度格式。如果未设置，将从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (num_channels, height, width) 格式表示。

    +   `"channels_last"` 或 `ChannelDimension.LAST`：图像以 (height, width, num_channels) 格式表示。

    +   `"none"` 或 `ChannelDimension.NONE`：图像以 (height, width) 格式表示。

预处理图像或图像批次。处理器首先计算可以从图像中提取的保持纵横比的大小为 `patch_size` 的最大可能数量的补丁。然后，它使用零填充图像，使图像遵守 `max_patches` 的约束。在提取补丁之前，图像将按照 tensorflow 的 `per_image_standardization` 实现进行标准化（[`www.tensorflow.org/api_docs/python/tf/image/per_image_standardization`](https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization)）。

## Pix2StructTextModel

### `class transformers.Pix2StructTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1302)

```py
( config )
```

参数

+   `config`（Union[`Pix2StructConfig`, `Pix2StructTextConfig`]）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained() 方法以加载模型权重。

Pix2Struct 的独立文本解码器

Pix2Struct 模型由 Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova 在 [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) 中提出。它是一个在图像到文本设置下进行预训练的编码器解码器 transformer 模型。

该模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以了解所有与一般用法和行为相关的事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1371)

```py
( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None inputs_embeds: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。Pix2StructText 是一个带有相对位置嵌入的模型，因此您应该能够在右侧和左侧填充输入。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

    要了解有关如何为预训练准备`input_ids`的更多信息，请查看 Pix2StructText Training。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被`屏蔽`的标记，

    +   0 表示被`屏蔽`的标记。

    什么是注意力掩码？

+   `decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 词汇表中解码器输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是解码器输入 ID？

    Pix2StructText 使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，可选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。

    要了解有关如何为预训练准备`decoder_input_ids`的更多信息，请查看 Pix2StructText Training。

+   `decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 编码器中自注意力模块中选择性屏蔽头部的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被`屏蔽`，

    +   0 表示头部被`屏蔽`。

+   `decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 解码器中自注意力模块中选择性屏蔽头部的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被`屏蔽`。

    +   0 表示头部被`屏蔽`。

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 解码器中交叉注意力模块中选择性屏蔽头部的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被`屏蔽`。

    +   0 表示头部被`屏蔽`。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括（`last_hidden_state`，可选：*hidden_states*，可选：*attentions*）`last_hidden_state` 的形状为 `(batch_size, sequence_length, hidden_size)`，是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组有 4 个形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的张量） — 包含注意力层的预计算键和值隐藏状态。可用于加速解码。

    如果使用`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用`past_key_values`，可以选择仅输入最后一个`decoder_inputs_embeds`（请参阅`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_outputs.CausalLMOutputWithCrossAttentions 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.CausalLMOutputWithCrossAttentions 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`，或者当`config.return_dict=False`时）包含根据配置（Pix2StructConfig）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层的输出，则为一个 + 每层的输出）。

    每层模型的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    自注意力模块中注意力 softmax 后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回 — 长度为`config.n_layers`的`torch.FloatTensor`元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态，如果模型在编码器-解码器设置中使用，则相关。仅在`config.is_decoder = True`时相关。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（请参阅`past_key_values`输入）。

Pix2StructTextModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, Pix2StructTextModel

>>> processor = AutoProcessor.from_pretrained("google/pix2struct-textcaps-base")
>>> model = Pix2StructTextModel.from_pretrained("google/pix2struct-textcaps-base")

>>> inputs = processor(text="Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)
>>> loss = outputs.loss
```

## Pix2StructVisionModel

### `class transformers.Pix2StructVisionModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L537)

```py
( config: Pix2StructConfig )
```

参数

+   `config`（Pix2StructConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 Pix2StructVision Model 变压器输出原始隐藏状态，没有特定的头部。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L570)

```py
( flattened_patches: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `flattened_patches` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_channels x patch_height x patch_width)`） — 扁平化和填充的像素值。这些值可以使用 AutoImageProcessor 获得。有关详细信息，请参阅`Pix2StructVisionImageProcessor.__call__`。查看[原始论文](https://arxiv.org/abs/2210.03347)（图 5）以获取更多详细信息。

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充像素值上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部是`not masked`，

    +   0 表示头部是`masked`。

+   `output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）-是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（Pix2StructConfig）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-模型最后一层输出的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）-序列第一个标记（分类标记）的最后一层隐藏状态，在通过用于辅助预训练任务的层进一步处理后。例如，对于 BERT 系列模型，这将返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+一个用于每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

Pix2StructVisionModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import requests
>>> from PIL import Image
>>> from transformers import AutoProcessor, Pix2StructVisionModel

>>> image_processor = AutoProcessor.from_pretrained("google/pix2struct-textcaps-base")
>>> model = Pix2StructVisionModel.from_pretrained("google/pix2struct-textcaps-base")

>>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 2048, 768]
```

## Pix2StructForConditionalGeneration

### `class transformers.Pix2StructForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1574)

```py
( config: Pix2StructConfig )
```

参数

+   `config`（Union[`Pix2StructConfig`，`Pix2StructTextConfig`]）-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

具有语言建模头的条件生成模型。可用于序列生成任务。

Pix2Struct 模型是由 Kenton Lee，Mandar Joshi，Iulia Turc，Hexiang Hu，Fangyu Liu，Julian Eisenschlos，Urvashi Khandelwal，Peter Shaw，Ming-Wei Chang，Kristina Toutanova 在[Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347)中提出的。它是在图像到文本设置中预训练的编码器解码器变换器。

该模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1620)

```py
( flattened_patches: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None labels: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)
```

参数

+   `flattened_patches`（形状为`(batch_size, seq_length, hidden_size)`的`torch.FloatTensor`）— 扁平化的像素块。`hidden_size`通过以下公式获得：`hidden_size` = `num_channels` * `patch_size` * `patch_size`

    扁平化像素块的过程由`Pix2StructProcessor`完成。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`中：

    +   1 表示“未被掩盖”的标记，

    +   0 表示被“掩盖”的标记。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中解码器输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    解码器输入 ID 是什么？

    Pix2StructText 使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，则可以选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

    要了解有关如何为预训练准备`decoder_input_ids`的更多信息，请查看 Pix2StructText Training。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）— 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于将编码器中自注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`中：

    +   1 表示头部未被“掩盖”，

    +   0 表示头部被“掩盖”。

+   `decoder_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于将解码器中自注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`中：

    +   1 表示头部未被“掩盖”，

    +   0 表示头部被“掩盖”。

+   `cross_attn_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.Tensor`，*可选*）— 用于将解码器中交叉注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`中：

    +   1 表示头部未被“掩盖”，

    +   0 表示头部被“掩盖”。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：*hidden_states*，*可选*：*attentions*）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（长度为`config.n_layers`的元组（元组（torch.FloatTensor）））- 包含注意力层的预计算键和值隐藏状态。可用于加速解码。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `decoder_inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, target_sequence_length, hidden_size)`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则可以选择仅输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `labels`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）- 用于计算解码器的掩码语言建模损失的标签。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（请参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回一个 ModelOutput 而不是一个普通元组。

返回

transformers.modeling_outputs.Seq2SeqModelOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.Seq2SeqModelOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（Pix2StructConfig）和输入的各种元素。

+   `last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`）- 模型解码器最后一层的隐藏状态序列。

    如果使用了`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

+   `decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）的形状为`(batch_size, sequence_length, hidden_size)`。

    解码器每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每一层的输出）。

    编码器每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

Pix2StructForConditionalGeneration 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传播的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

推理：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Pix2StructForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("google/pix2struct-textcaps-base")
>>> model = Pix2StructForConditionalGeneration.from_pretrained("google/pix2struct-textcaps-base")

>>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> # autoregressive generation
>>> generated_ids = model.generate(**inputs, max_new_tokens=50)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_text)
A stop sign is on a street corner.

>>> # conditional generation
>>> text = "A picture of"
>>> inputs = processor(text=text, images=image, return_tensors="pt", add_special_tokens=False)

>>> generated_ids = model.generate(**inputs, max_new_tokens=50)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_text)
A picture of a stop sign with a red stop sign
```

训练：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Pix2StructForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("google/pix2struct-base")
>>> model = Pix2StructForConditionalGeneration.from_pretrained("google/pix2struct-base")

>>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text = "A stop sign is on the street corner."

>>> inputs = processor(images=image, return_tensors="pt")
>>> labels = processor(text=text, return_tensors="pt").input_ids

>>> # forward pass
>>> outputs = model(**inputs, labels=labels)
>>> loss = outputs.loss
>>> print(f"{loss.item():.5f}")
5.94282
```
