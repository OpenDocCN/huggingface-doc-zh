# InstructBLIP

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/instructblip`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/instructblip)

## 概述

InstructBLIP 模型是由 Wenliang Dai，Junnan Li，Dongxu Li，Anthony Meng Huat Tiong，Junqi Zhao，Weisheng Wang，Boyang Li，Pascale Fung，Steven Hoi 在[InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)中提出的。InstructBLIP 利用了 BLIP-2 架构进行视觉指令调整。

论文摘要如下：

*由于预训练和指令调整流程的推动，出现了可以解决各种语言领域任务的通用语言模型。然而，构建通用的视觉语言模型具有挑战性，因为额外的视觉输入引入了增加的任务差异。尽管视觉语言预训练已经得到广泛研究，但视觉语言指令调整仍然相对较少探索。在本文中，我们对基于预训练的 BLIP-2 模型的视觉语言指令调整进行了系统和全面的研究。我们收集了 26 个公开可用数据集的各种数据，将它们转换为指令调整格式，并将它们分类为两个集群，用于保留指令调整和保留零样本评估。此外，我们引入了指令感知的视觉特征提取，这是一种关键方法，使模型能够提取针对给定指令的信息特征。由此产生的 InstructBLIP 模型在所有 13 个保留数据集上实现了最先进的零样本性能，明显优于 BLIP-2 和更大的 Flamingo。我们的模型在个别下游任务（例如 ScienceQA IMG 上的 90.7%准确率）上也实现了最先进的性能。此外，我们定性地展示了 InstructBLIP 相对于同时进行的多模态模型的优势。*

InstructBLIP 架构。取自[原始论文。](https://arxiv.org/abs/2305.06500)

此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可以在[这里](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)找到。

## 使用提示

InstructBLIP 使用与 BLIP-2 相同的架构，但有一个微小但重要的区别：它还将文本提示（指令）提供给 Q-Former。

## InstructBlipConfig

### `class transformers.InstructBlipConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L253)

```py
( vision_config = None qformer_config = None text_config = None num_query_tokens = 32 **kwargs )
```

参数

+   `vision_config`（`dict`，*可选*）—用于初始化 InstructBlipVisionConfig 的配置选项字典。

+   `qformer_config`（`dict`，*可选*）—用于初始化 InstructBlipQFormerConfig 的配置选项字典。

+   `text_config`（`dict`，*可选*）—用于初始化任何 PretrainedConfig 的配置选项字典。

+   `num_query_tokens`（`int`，*可选*，默认为 32）—通过 Transformer 传递的查询标记数。

+   `kwargs`（*可选*）—关键字参数的字典。

InstructBlipConfig 是用于存储 InstructBlipForConditionalGeneration 配置的配置类。它用于根据指定的参数实例化一个 InstructBLIP 模型，定义视觉模型、Q-Former 模型和语言模型配置。使用默认实例化配置将产生类似于 InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import (
...     InstructBlipVisionConfig,
...     InstructBlipQFormerConfig,
...     OPTConfig,
...     InstructBlipConfig,
...     InstructBlipForConditionalGeneration,
... )

>>> # Initializing a InstructBlipConfig with Salesforce/instruct-blip-flan-t5 style configuration
>>> configuration = InstructBlipConfig()

>>> # Initializing a InstructBlipForConditionalGeneration (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration
>>> model = InstructBlipForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a InstructBlipConfig from a InstructBlipVisionConfig, InstructBlipQFormerConfig and any PretrainedConfig

>>> # Initializing InstructBLIP vision, InstructBLIP Q-Former and language model configurations
>>> vision_config = InstructBlipVisionConfig()
>>> qformer_config = InstructBlipQFormerConfig()
>>> text_config = OPTConfig()

>>> config = InstructBlipConfig.from_text_vision_configs(vision_config, qformer_config, text_config)
```

#### `from_vision_qformer_text_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L338)

```py
( vision_config: InstructBlipVisionConfig qformer_config: InstructBlipQFormerConfig text_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';InstructBlipConfig
```

返回

InstructBlipConfig

配置对象的实例

从 InstructBLIP 视觉模型、Q-Former 和语言模型配置实例化一个 InstructBlipConfig（或派生类）。

## InstructBlipVisionConfig

### `class transformers.InstructBlipVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L33)

```py
( hidden_size = 1408 intermediate_size = 6144 num_hidden_layers = 39 num_attention_heads = 16 image_size = 224 patch_size = 14 hidden_act = 'gelu' layer_norm_eps = 1e-06 attention_dropout = 0.0 initializer_range = 1e-10 qkv_bias = True **kwargs )
```

参数

+   `hidden_size` (`int`, *可选*，默认为 1408) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *可选*，默认为 6144) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *可选*，默认为 39) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *可选*，默认为 16) — Transformer 编码器中每个注意力层的注意力头数。

+   `image_size` (`int`, *可选*，默认为 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *可选*，默认为 14) — 每个补丁的大小（分辨率）。

+   `hidden_act` (`str`或`function`, *可选*，默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"` `"gelu"`。

+   `layer_norm_eps` (`float`, *可选*，默认为 1e-06) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *可选*，默认为 0.0) — 注意力概率的 dropout 比率。

+   `initializer_range` (`float`, *可选*，默认为 1e-10) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `qkv_bias` (`bool`, *可选*，默认为`True`) — 是否在自注意力层中为查询和值添加偏置。

这是用于存储 InstructBlipVisionModel 配置的配置类。它用于根据指定的参数实例化一个 InstructBLIP 视觉编码器，定义模型架构。使用默认实例化配置将产生类似于 InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) 架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import InstructBlipVisionConfig, InstructBlipVisionModel

>>> # Initializing a InstructBlipVisionConfig with Salesforce/instruct-blip-flan-t5 style configuration
>>> configuration = InstructBlipVisionConfig()

>>> # Initializing a InstructBlipVisionModel (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration
>>> model = InstructBlipVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## InstructBlipQFormerConfig

### `class transformers.InstructBlipQFormerConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L134)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' cross_attention_frequency = 2 encoder_hidden_size = 1408 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 30522) — Q-Former 模型的词汇量。定义了在调用模型时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为 768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `intermediate_size` (`int`, *optional*, 默认为 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str`或`Callable`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, 默认为 0.1) — 注意力概率的 dropout 比率。

+   `max_position_embeddings` (`int`, *optional*, 默认为 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如 512、1024 或 2048）。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-12) — 层归一化层使用的 epsilon。

+   `position_embedding_type` (`str`, *optional*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`、`"relative_key"`或`"relative_key_query"`中的一个。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参考[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参考[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658)中的*Method 4*。

+   `cross_attention_frequency` (`int`, *optional*, 默认为 2) — 向 Transformer 层添加交叉注意力的频率。

+   `encoder_hidden_size` (`int`, *optional*, 默认为 1408) — 用于交叉注意力的隐藏状态的隐藏大小。

这是一个配置类，用于存储 InstructBlipQFormerModel 的配置。它用于根据指定的参数实例化一个 InstructBLIP Querying Transformer (Q-Former)模型，定义模型架构。使用默认值实例化配置将产生类似于 InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)架构的配置。配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

请注意，InstructBlipQFormerModel 与 BertLMHeadModel 非常相似，具有交错的交叉注意力。

示例：

```py
>>> from transformers import InstructBlipQFormerConfig, InstructBlipQFormerModel

>>> # Initializing a InstructBLIP Salesforce/instruct-blip-flan-t5 style configuration
>>> configuration = InstructBlipQFormerConfig()

>>> # Initializing a model (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration
>>> model = InstructBlipQFormerModel(configuration)
>>> # Accessing the model configuration
>>> configuration = model.config
```

## InstructBlipProcessor

### `class transformers.InstructBlipProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L30)

```py
( image_processor tokenizer qformer_tokenizer )
```

参数

+   `image_processor` (`BlipImageProcessor`) — BlipImageProcessor 的一个实例。图像处理器是必需的输入。

+   `tokenizer` (`AutoTokenizer`) — [‘PreTrainedTokenizer`]的一个实例。分词器是必需的输入。

+   `qformer_tokenizer` (`AutoTokenizer`) — [‘PreTrainedTokenizer`]的一个实例。Q-Former tokenizer 是必需的输入。

构建一个 InstructBLIP 处理器，将 BLIP 图像处理器和 LLaMa/T5 分词器包装成一个单一处理器。

InstructBlipProcessor 提供了 BlipImageProcessor 和 AutoTokenizer 的所有功能。查看`__call__()`和 decode()的文档字符串以获取更多信息。

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L136)

```py
( *args **kwargs )
```

此方法将其所有参数转发给 PreTrainedTokenizer 的 batch_decode()。有关更多信息，请参阅此方法的文档字符串。

#### `decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L144)

```py
( *args **kwargs )
```

此方法将其所有参数转发给 PreTrainedTokenizer 的 decode()。有关更多信息，请参阅此方法的文档字符串。

## InstructBlipVisionModel

### `class transformers.InstructBlipVisionModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L490)

```py
( config: InstructBlipVisionConfig )
```

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L505)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。像素值可以使用 InstructBlipProcessor 获取。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（`<class 'transformers.models.instructblip.configuration_instructblip.InstructBlipVisionConfig'>`）和输入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后，序列中第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

InstructBlipVisionModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## InstructBlipQFormerModel

### `class transformers.InstructBlipQFormerModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1035)

```py
( config: InstructBlipQFormerConfig )
```

查询变换器（Q-Former），用于 InstructBLIP。与 BLIP-2 略有修改，因为它还将指令作为输入。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1108)

```py
( input_ids: LongTensor attention_mask: Optional = None position_ids: Optional = None query_embeds: Optional = None head_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

encoder_hidden_states（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）：编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。encoder_attention_mask（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）：避免对编码器输入的填充标记索引执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。选择的掩码值为`[0, 1]`：

+   对于未被`masked`的标记，值为 1。

+   对于被`masked`的标记，值为 0。past_key_values（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有 4 个张量，形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`）：包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size, 1)`，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。use_cache（`bool`，*可选*）：如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

## InstructBlipForConditionalGeneration

### `class transformers.InstructBlipForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1231)

```py
( config: InstructBlipConfig )
```

参数

+   `config`（InstructBlipConfig）—模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

InstructBLIP 模型用于根据图像和可选文本提示生成文本。该模型由视觉编码器、查询变换器（Q-Former）和语言模型组成。

可以选择向模型传递`input_ids`，作为文本提示，以使语言模型继续提示。否则，语言模型将从[BOS]（序列开始）标记开始生成文本。

此模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1314)

```py
( pixel_values: FloatTensor qformer_input_ids: FloatTensor qformer_attention_mask: Optional = None input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—像素值。像素值可以使用 InstructBlipProcessor 获取。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。

+   `qformer_input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—Q-Former 词汇表中输入序列标记的索引。可以选择提供输入标记作为文本提示，Q-Former 模型将对其进行编码。

    可以使用 InstructBlipProcessor 获取索引。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。

    什么是输入 ID？

+   `qformer_attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 输入序列标记在语言模型词汇中的索引。输入标记可以选择作为文本提示提供，语言模型可以继续。

    可以使用 InstructBlipProcessor 获取索引。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。

    什么是输入 ID？

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 解码器输入序列标记在语言模型词汇中的索引。仅在使用编码器-解码器语言模型（如 T5）时相关。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。什么是解码器输入 ID？

+   `decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

    仅在使用编码器-解码器语言模型（如 T5）时相关。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回的张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回的张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size - 1]`中。所有设置为`-100`的标签将被忽略（被`masked`），损失仅计算标签在`[0, ..., config.vocab_size]`中的标签

返回

`transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput` 或 `tuple(torch.FloatTensor)`

`transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput` 或 `torch.FloatTensor` 元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.instructblip.configuration_instructblip.InstructBlipVisionConfig'>`）和输入的各种元素。

+   `loss` (`torch.FloatTensor`, *optional*, 当提供`labels`时返回，形状为`(1,)`) — 语言建模损失来自语言模型。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言模型的语言建模头的预测分数。

+   `vision_outputs` (`BaseModelOutputWithPooling`) — 视觉编码器的输出。

+   `qformer_outputs` (`BaseModelOutputWithPoolingAndCrossAttentions`) — Q-Former（查询变换器）的输出。

+   `language_model_outputs` (`CausalLMOutputWithPast`或`Seq2SeqLMOutput`) — 语言模型的输出。

InstructBlipForConditionalGeneration 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration
>>> import torch
>>> from PIL import Image
>>> import requests

>>> model = InstructBlipForConditionalGeneration.from_pretrained("Salesforce/instructblip-vicuna-7b")
>>> processor = InstructBlipProcessor.from_pretrained("Salesforce/instructblip-vicuna-7b")

>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model.to(device)
>>> url = "https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
>>> prompt = "What is unusual about this image?"
>>> inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)

>>> outputs = model.generate(
...     **inputs,
...     do_sample=False,
...     num_beams=5,
...     max_length=256,
...     min_length=1,
...     top_p=0.9,
...     repetition_penalty=1.5,
...     length_penalty=1.0,
...     temperature=1,
... )
>>> generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV, which is parked in the middle of a busy city street. This is an unconventional approach to ironing clothes, as it requires the man to balance himself and his ironing equipment on top of the vehicle while navigating through traffic. Additionally, the presence of taxis and other vehicles in the scene further emphasizes the unusual nature of this situation.
```

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1469)

```py
( pixel_values: FloatTensor qformer_input_ids: Optional = None qformer_attention_mask: Optional = None input_ids: Optional = None attention_mask: Optional = None **generate_kwargs ) → export const metadata = 'undefined';captions (list)
```

参数

+   `pixel_values` (`torch.FloatTensor` of shape (batch_size, num_channels, height, width)) — 要处理的输入图像。

+   `qformer_input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*) — 用作输入到 Q-Former 模块的提示序列。

+   `qformer_attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*) — 用于避免在填充标记索引上执行注意力的掩码。

+   `input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*) — 用作生成提示的序列。

+   `attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*) — 用于避免在填充标记索引上执行注意力的掩码。

返回

字幕（列表）

一个长度为 batch_size * num_captions 的字符串列表。

覆盖`generate`函数以能够将模型用作有条件生成器。
