# 导出到 TorchScript

> 原文：[`huggingface.co/docs/transformers/v4.37.2/en/torchscript`](https://huggingface.co/docs/transformers/v4.37.2/en/torchscript)

这是我们使用 TorchScript 的实验的开始，我们仍在探索其对于可变输入大小模型的能力。这是我们感兴趣的焦点，我们将在即将发布的版本中深入分析，提供更多代码示例，更灵活的实现以及将 Python 代码与编译后的 TorchScript 进行比较的基准测试。

根据[TorchScript 文档](https://pytorch.org/docs/stable/jit.html)：

> TorchScript 是一种从 PyTorch 代码创建可序列化和可优化模型的方法。

有两个 PyTorch 模块[JIT 和 TRACE](https://pytorch.org/docs/stable/jit.html)，允许开发人员将他们的模型导出以便在其他程序中重复使用，比如面向效率的 C++程序。

我们提供了一个接口，允许您将🤗 Transformers 模型导出到 TorchScript，以便在与基于 PyTorch 的 Python 程序不同的环境中重复使用。在这里，我们解释了如何使用 TorchScript 导出和使用我们的模型。

导出模型需要两件事：

+   使用`torchscript`标志实例化模型

+   使用虚拟输入进行前向传递

这些必需品意味着开发人员应该注意以下几点。

## TorchScript 标志和绑定权重

`torchscript`标志是必需的，因为大多数🤗 Transformers 语言模型的`Embedding`层和`Decoding`层之间有绑定权重。TorchScript 不允许您导出具有绑定权重的模型，因此需要在此之前解开并克隆权重。

使用`torchscript`标志实例化的模型将它们的`Embedding`层和`Decoding`层分开，这意味着它们不应该在训练过程中进行训练。训练会使这两层不同步，导致意外结果。

对于没有语言模型头的模型，情况并非如此，因为这些模型没有绑定权重。这些模型可以安全地导出而不使用`torchscript`标志。

## 虚拟输入和标准长度

虚拟输入用于模型的前向传递。当输入的值通过层传播时，PyTorch 会跟踪每个张量上执行的不同操作。然后使用这些记录的操作来创建模型的*trace*。

跟踪是相对于输入维度创建的。因此，它受虚拟输入维度的限制，并且对于任何其他序列长度或批量大小都不起作用。当尝试使用不同大小时，会引发以下错误：

```py
`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`
```

我们建议您使用至少与推理过程中将馈送到模型的最大输入一样大的虚拟输入大小来跟踪模型。填充可以帮助填补缺失的值。然而，由于模型是使用较大的输入大小跟踪的，矩阵的维度也会很大，导致更多的计算。

要注意每个输入上执行的总操作数，并在导出不同序列长度模型时密切关注性能。

## 在 Python 中使用 TorchScript

本节演示了如何保存和加载模型以及如何使用跟踪进行推理。

### 保存模型

要导出带有 TorchScript 的`BertModel`，请从`BertConfig`类实例化`BertModel`，然后将其保存到磁盘上的文件名为`traced_bert.pt`：

```py
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

### 加载模型

现在，您可以加载先前保存的`BertModel`，`traced_bert.pt`，从磁盘上并在先前初始化的`dummy_input`上使用它：

```py
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

### 使用跟踪模型进行推理

通过使用其`__call__` dunder 方法对推理使用跟踪模型：

```py
traced_model(tokens_tensor, segments_tensors)
```

## 使用 Neuron SDK 将 Hugging Face TorchScript 模型部署到 AWS

AWS 推出了[Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/)实例系列，用于在云中进行低成本、高性能的机器学习推理。Inf1 实例由 AWS Inferentia 芯片提供动力，这是一种专门用于深度学习推理工作负载的定制硬件加速器。[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#)是用于 Inferentia 的 SDK，支持跟踪和优化 transformers 模型，以便在 Inf1 上部署。Neuron SDK 提供：

1.  易于使用的 API，只需更改一行代码即可跟踪和优化 TorchScript 模型，以便在云中进行推理。

1.  针对[改进的成本性能](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/%3E)进行即插即用的性能优化。

1.  支持使用[PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)或[TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)构建的 Hugging Face transformers 模型。

### 影响

基于[BERT（来自 Transformers 的双向编码器表示）](https://huggingface.co/docs/transformers/main/model_doc/bert)架构的 transformers 模型，或其变体，如[distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)和[roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)在非生成任务（如提取式问答、序列分类和标记分类）上在 Inf1 上运行效果最佳。然而，文本生成任务仍可以根据此[AWS Neuron MarianMT 教程](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)进行适应以在 Inf1 上运行。有关可以直接在 Inferentia 上转换的模型的更多信息，请参阅 Neuron 文档的[模型架构适配](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia)部分。

### 依赖

使用 AWS Neuron 转换模型需要一个[Neuron SDK 环境](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)，该环境预先配置在[AWS 深度学习 AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)上。

### 将模型转换为 AWS Neuron

使用与在 Python 中使用 TorchScript 相同的代码来为 AWS NEURON 转换模型，以跟踪`BertModel`。导入`torch.neuron`框架扩展以通过 Python API 访问 Neuron SDK 的组件：

```py
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```

您只需要修改以下行：

```py
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

这使得 Neuron SDK 能够跟踪模型并为 Inf1 实例进行优化。

要了解有关 AWS Neuron SDK 功能、工具、示例教程和最新更新的更多信息，请参阅[AWS NeuronSDK 文档](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)。
