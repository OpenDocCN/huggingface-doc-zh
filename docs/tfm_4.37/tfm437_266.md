# DPT

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpt`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpt)

## æ¦‚è¿°

DPT æ¨¡å‹ç”± RenÃ© Ranftlã€Alexey Bochkovskiyã€Vladlen Koltun åœ¨ [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) ä¸­æå‡ºã€‚DPT æ˜¯ä¸€ä¸ªåˆ©ç”¨ Vision Transformer (ViT) ä½œä¸ºå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼‰çš„éª¨å¹²çš„æ¨¡å‹ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬ä»‹ç»äº†å¯†é›†è§†è§‰å˜æ¢å™¨ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è§†è§‰å˜æ¢å™¨ä»£æ›¿å·ç§¯ç½‘ç»œä½œä¸ºå¯†é›†é¢„æµ‹ä»»åŠ¡éª¨å¹²çš„æ¶æ„ã€‚æˆ‘ä»¬ä»è§†è§‰å˜æ¢å™¨çš„å„ä¸ªé˜¶æ®µæ±‡é›†ä»¤ç‰Œï¼Œå°†å®ƒä»¬ç»„åˆæˆå„ç§åˆ†è¾¨ç‡çš„å›¾åƒè¡¨ç¤ºï¼Œå¹¶é€æ¸å°†å®ƒä»¬ç»“åˆæˆä½¿ç”¨å·ç§¯è§£ç å™¨è¿›è¡Œå…¨åˆ†è¾¨ç‡é¢„æµ‹ã€‚å˜æ¢å™¨éª¨å¹²ä»¥æ’å®šä¸”ç›¸å¯¹è¾ƒé«˜çš„åˆ†è¾¨ç‡å¤„ç†è¡¨ç¤ºï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶æ®µå…·æœ‰å…¨å±€æ„Ÿå—é‡ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—å¯†é›†è§†è§‰å˜æ¢å™¨åœ¨ä¸å®Œå…¨å·ç§¯ç½‘ç»œç›¸æ¯”æä¾›æ›´ç²¾ç»†å’Œæ›´å…¨å±€ä¸€è‡´çš„é¢„æµ‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ¶æ„åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯å½“æœ‰å¤§é‡è®­ç»ƒæ•°æ®å¯ç”¨æ—¶ã€‚å¯¹äºå•ç›®æ·±åº¦ä¼°è®¡ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸æœ€å…ˆè¿›çš„å®Œå…¨å·ç§¯ç½‘ç»œç›¸æ¯”ï¼Œæ€§èƒ½ç›¸å¯¹æé«˜äº†é«˜è¾¾ 28%ã€‚å½“åº”ç”¨äºè¯­ä¹‰åˆ†å‰²æ—¶ï¼Œå¯†é›†è§†è§‰å˜æ¢å™¨åœ¨ ADE20K ä¸Šå–å¾—äº† 49.02% mIoU çš„æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºï¼Œè¯¥æ¶æ„å¯ä»¥åœ¨è¾ƒå°çš„æ•°æ®é›†ï¼ˆå¦‚ NYUv2ã€KITTI å’Œ Pascal Contextï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¹Ÿåœ¨è¿™äº›æ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚*

![drawing](img/a0659410be29958ad5c55dce63aa3e01.png) DPT æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2103.13413)ã€‚

æ­¤æ¨¡å‹ç”± [nielsr](https://huggingface.co/nielsr) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/isl-org/DPT)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

DPT å…¼å®¹ `AutoBackbone` ç±»ã€‚è¿™å…è®¸ä½¿ç”¨åº“ä¸­æä¾›çš„å„ç§è®¡ç®—æœºè§†è§‰éª¨å¹²ï¼ˆå¦‚ `VitDetBackbone` æˆ– `Dinov2Backbone`ï¼‰ä¸ DPT æ¡†æ¶ä¸€èµ·ä½¿ç”¨ã€‚å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼åˆ›å»ºå®ƒï¼š

```py
from transformers import Dinov2Config, DPTConfig, DPTForDepthEstimation

# initialize with a Transformer-based backbone such as DINOv2
# in that case, we also specify `reshape_hidden_states=False` to get feature maps of shape (batch_size, num_channels, height, width)
backbone_config = Dinov2Config.from_pretrained("facebook/dinov2-base", out_features=["stage1", "stage2", "stage3", "stage4"], reshape_hidden_states=False)

config = DPTConfig(backbone_config=backbone_config)
model = DPTForDepthEstimation(config=config)
```

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆğŸŒ æ ‡å¿—ï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ DPTã€‚

+   DPTForDepthEstimation çš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DPT)æ‰¾åˆ°ã€‚

+   è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æŒ‡å—

+   å•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡æŒ‡å—

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## DPTConfig

`transformers.DPTConfig` ç±»

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/configuration_dpt.py#L33)

```py
( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 384 patch_size = 16 num_channels = 3 is_hybrid = False qkv_bias = True backbone_out_indices = [2, 5, 8, 11] readout_type = 'project' reassemble_factors = [4, 2, 1, 0.5] neck_hidden_sizes = [96, 192, 384, 768] fusion_hidden_size = 256 head_in_index = -1 use_batch_norm_in_fusion_residual = False use_bias_in_fusion_residual = None add_projection = False use_auxiliary_head = True auxiliary_loss_weight = 0.4 semantic_loss_ignore_index = 255 semantic_classifier_dropout = 0.1 backbone_featmap_shape = [1, 1024, 24, 24] neck_ignore_stages = [0, 1] backbone_config = None **kwargs )
```

å‚æ•°

+   `hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚

+   `num_attention_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` æˆ– `function`, *å¯é€‰*, é»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`, `"relu"`, `"selu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” åµŒå…¥ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ dropout æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout æ¯”ç‡ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `image_size` (`int`, *optional*, defaults to 384) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `patch_size` (`int`, *optional*, defaults to 16) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“æ•°ã€‚

+   `is_hybrid` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨æ··åˆä¸»å¹²ã€‚åœ¨åŠ è½½ DPT-Hybrid æ¨¡å‹çš„æƒ…å†µä¸‹å¾ˆæœ‰ç”¨ã€‚

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åç½®ã€‚

+   `backbone_out_indices` (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`) â€” è¦ä»ä¸»å¹²ä½¿ç”¨çš„ä¸­é—´éšè—çŠ¶æ€çš„ç´¢å¼•ã€‚

+   `readout_type` (`str`, *optional*, defaults to `"project"`) â€” å¤„ç† ViT ä¸»å¹²ä¸­é—´éšè—çŠ¶æ€çš„è¯»å‡ºæ ‡è®°ï¼ˆCLS æ ‡è®°ï¼‰æ—¶è¦ä½¿ç”¨çš„è¯»å‡ºç±»å‹ã€‚å¯ä»¥æ˜¯[`"ignore"`, `"add"`, `"project"`]ä¹‹ä¸€ã€‚

    +   â€œignoreâ€ ç®€å•åœ°å¿½ç•¥ CLS æ ‡è®°ã€‚

    +   â€œaddâ€ é€šè¿‡å°† CLS æ ‡è®°çš„ä¿¡æ¯æ·»åŠ åˆ°æ‰€æœ‰å…¶ä»–æ ‡è®°ä¸­ä¼ é€’è¡¨ç¤ºã€‚

    +   â€œprojectâ€ é€šè¿‡å°†è¯»å‡ºè¿æ¥åˆ°æ‰€æœ‰å…¶ä»–æ ‡è®°ï¼Œç„¶åä½¿ç”¨çº¿æ€§å±‚å°†è¡¨ç¤ºæŠ•å½±åˆ°åŸå§‹ç‰¹å¾ç»´åº¦ Dï¼Œæ¥ç€ä½¿ç”¨ GELU éçº¿æ€§ä¼ é€’ä¿¡æ¯ç»™å…¶ä»–æ ‡è®°ã€‚

+   `reassemble_factors` (`List[int]`, *optional*, defaults to `[4, 2, 1, 0.5]`) â€” é‡ç»„å±‚çš„ä¸Š/ä¸‹é‡‡æ ·å› å­ã€‚

+   `neck_hidden_sizes` (`List[str]`, *optional*, defaults to `[96, 192, 384, 768]`) â€” è¦æŠ•å½±åˆ°ä¸»å¹²ç‰¹å¾å›¾çš„éšè—å¤§å°ã€‚

+   `fusion_hidden_size` (`int`, *optional*, defaults to 256) â€” èåˆå‰çš„é€šé“æ•°ã€‚

+   `head_in_index` (`int`, *optional*, defaults to -1) â€” åœ¨å¤´éƒ¨ä¸­è¦ä½¿ç”¨çš„ç‰¹å¾çš„ç´¢å¼•ã€‚

+   `use_batch_norm_in_fusion_residual` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨èåˆå—çš„é¢„æ¿€æ´»æ®‹å·®å•å…ƒä¸­ä½¿ç”¨æ‰¹å½’ä¸€åŒ–ã€‚

+   `use_bias_in_fusion_residual` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨èåˆå—çš„é¢„æ¿€æ´»æ®‹å·®å•å…ƒä¸­ä½¿ç”¨åç½®ã€‚

+   `add_projection` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æ·±åº¦ä¼°è®¡å¤´ä¹‹å‰æ·»åŠ æŠ•å½±å±‚ã€‚

+   `use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” è®­ç»ƒæ—¶æ˜¯å¦ä½¿ç”¨è¾…åŠ©å¤´ã€‚

+   `auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” è¾…åŠ©å¤´çš„äº¤å‰ç†µæŸå¤±æƒé‡ã€‚

+   `semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚

+   `semantic_classifier_dropout` (`float`, *optional*, defaults to 0.1) â€” è¯­ä¹‰åˆ†ç±»å¤´çš„ dropout æ¯”ç‡ã€‚

+   `backbone_featmap_shape` (`List[int]`, *optional*, defaults to `[1, 1024, 24, 24]`) â€” ä»…ç”¨äº`hybrid`åµŒå…¥ç±»å‹ã€‚ä¸»å¹²ç‰¹å¾å›¾çš„å½¢çŠ¶ã€‚

+   `neck_ignore_stages` (`List[int]`, *optional*, defaults to `[0, 1]`) â€” ä»…ç”¨äº`hybrid`åµŒå…¥ç±»å‹ã€‚è¦å¿½ç•¥çš„è¯»å‡ºå±‚é˜¶æ®µã€‚

+   `backbone_config` (`Union[Dict[str, Any], PretrainedConfig]`, *optional*) â€” ä¸»å¹²æ¨¡å‹çš„é…ç½®ã€‚ä»…åœ¨`is_hybrid`ä¸º`True`æˆ–è€…æƒ³è¦åˆ©ç”¨`AutoBackbone` API æ—¶ä½¿ç”¨ã€‚

è¿™æ˜¯é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ DPTModel çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª DPT æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº[DPT Intel/dpt-large](https://huggingface.co/Intel/dpt-large)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import DPTModel, DPTConfig

>>> # Initializing a DPT dpt-large style configuration
>>> configuration = DPTConfig()

>>> # Initializing a model from the dpt-large style configuration
>>> model = DPTModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

#### `to_dict`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/configuration_dpt.py#L247)

```py
( )
```

å°†æ­¤å®ä¾‹åºåˆ—åŒ–ä¸º Python å­—å…¸ã€‚è¦†ç›–é»˜è®¤çš„ to_dict()ã€‚è¿”å›ï¼š`Dict[str, any]`ï¼šæ„æˆæ­¤é…ç½®å®ä¾‹çš„æ‰€æœ‰å±æ€§çš„å­—å…¸ï¼Œ

## DPTFeatureExtractor

### `class transformers.DPTFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/feature_extraction_dpt.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

#### `post_process_semantic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L422)

```py
( outputs target_sizes: List = None ) â†’ export const metadata = 'undefined';semantic_segmentation
```

å‚æ•°

+   `outputs`ï¼ˆDPTForSemanticSegmentationï¼‰â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes`ï¼ˆé•¿åº¦ä¸º`batch_size`çš„`List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

è¯­ä¹‰åˆ†å‰²

é•¿åº¦ä¸º`batch_size`çš„`List[torch.Tensor]`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº`target_sizes`æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†`target_sizes`ï¼‰ã€‚æ¯ä¸ª`torch.Tensor`çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºè¯­ä¹‰ç±»åˆ« IDã€‚

å°† DPTForSemanticSegmentation çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚

## DPTImageProcessor

### `class transformers.DPTImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L94)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> keep_aspect_ratio: bool = False ensure_multiple_of: int = 1 do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = False size_divisor: int = None **kwargs )
```

å‚æ•°

+   `do_resize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è°ƒæ•´å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_resize`è¦†ç›–ã€‚

+   `size`ï¼ˆ`Dict[str, int]` *å¯é€‰*ï¼Œé»˜è®¤ä¸º`{"height" -- 384, "width": 384}`ï¼‰ï¼šè°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`size`è¦†ç›–ã€‚

+   `resample`ï¼ˆ`PILImageResampling`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`Resampling.BICUBIC`ï¼‰â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™å®šä¹‰è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`resample`è¦†ç›–ã€‚

+   `keep_aspect_ratio`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” å¦‚æœä¸º`True`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´ä¸ºä¿æŒçºµæ¨ªæ¯”çš„æœ€å¤§å¯èƒ½å°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`keep_aspect_ratio`è¦†ç›–ã€‚

+   `ensure_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1ï¼‰â€” å¦‚æœ`do_resize`ä¸º`True`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´ä¸ºæ­¤å€¼çš„å€æ•°ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`ensure_multiple_of`è¦†ç›–ã€‚

+   `do_rescale`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹`rescale_factor`é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_rescale`è¦†ç›–ã€‚

+   `rescale_factor` (`int`æˆ–`float`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`rescale_factor`è¦†ç›–ã€‚

+   `do_normalize` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_normalize`å‚æ•°è¦†ç›–ã€‚

+   `image_mean` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`IMAGENET_STANDARD_MEAN`) â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_mean`å‚æ•°è¦†ç›–ã€‚

+   `image_std` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`IMAGENET_STANDARD_STD`) â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_std`å‚æ•°è¦†ç›–ã€‚

+   `do_pad` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åº”ç”¨ä¸­å¿ƒå¡«å……ã€‚è¿™åœ¨ DINOv2 è®ºæ–‡ä¸­å¼•å…¥ï¼Œè¯¥è®ºæ–‡å°†è¯¥æ¨¡å‹ä¸ DPT ç»“åˆä½¿ç”¨ã€‚

+   `size_divisor` (`int`ï¼Œ*optional*) â€” å¦‚æœ`do_pad`ä¸º`True`ï¼Œåˆ™å¡«å……å›¾åƒå°ºå¯¸ä½¿å…¶å¯è¢«è¯¥å€¼æ•´é™¤ã€‚è¿™åœ¨ DINOv2 è®ºæ–‡ä¸­å¼•å…¥ï¼Œè¯¥è®ºæ–‡å°†è¯¥æ¨¡å‹ä¸ DPT ç»“åˆä½¿ç”¨ã€‚

æ„é€ ä¸€ä¸ª DPT å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L267)

```py
( images: Union do_resize: bool = None size: int = None keep_aspect_ratio: bool = None ensure_multiple_of: int = None resample: Resampling = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None do_pad: bool = None size_divisor: int = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_rescale=False`ã€‚

+   `do_resize` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¦‚æœ`keep_aspect_ratio`ä¸º`True`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä¸ºä¿æŒçºµæ¨ªæ¯”çš„æœ€å¤§å¯èƒ½å°ºå¯¸ã€‚å¦‚æœè®¾ç½®äº†`ensure_multiple_of`ï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä¸ºè¯¥å€¼çš„å€æ•°ã€‚

+   `keep_aspect_ratio` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.keep_aspect_ratio`) â€” æ˜¯å¦ä¿æŒå›¾åƒçš„çºµæ¨ªæ¯”ã€‚å¦‚æœä¸º Falseï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä¸ºï¼ˆsizeï¼Œsizeï¼‰ã€‚å¦‚æœä¸º Trueï¼Œåˆ™å°†å›¾åƒè°ƒæ•´å¤§å°ä»¥ä¿æŒçºµæ¨ªæ¯”ï¼Œå¤§å°å°†æ˜¯æœ€å¤§å¯èƒ½çš„ã€‚

+   `ensure_multiple_of` (`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.ensure_multiple_of`) â€” ç¡®ä¿å›¾åƒå¤§å°æ˜¯è¯¥å€¼çš„å€æ•°ã€‚

+   `resample` (`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚è¿™å¯ä»¥æ˜¯æšä¸¾`PILImageResampling`ä¹‹ä¸€ï¼Œä»…åœ¨`do_resize`è®¾ç½®ä¸º`True`æ—¶æœ‰æ•ˆã€‚

+   `do_rescale` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒå€¼é‡æ–°ç¼©æ”¾åœ¨[0 - 1]ä¹‹é—´ã€‚

+   `rescale_factor` (`float`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.rescale_factor`) â€” å¦‚æœ`do_rescale`è®¾ç½®ä¸º`True`ï¼Œåˆ™ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒçš„ç¼©æ”¾å› å­ã€‚

+   `do_normalize` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.image_mean`) â€” å›¾åƒå‡å€¼ã€‚

+   `image_std` (`float`æˆ–`List[float]`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`self.image_std`) â€” å›¾åƒæ ‡å‡†å·®ã€‚

+   `return_tensors` (`str`æˆ–`TensorType`ï¼Œ*optional*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª`np.ndarray`åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW`æˆ–`'tf'`ï¼šè¿”å›ç±»å‹ä¸º`tf.Tensor`çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.PYTORCH`æˆ–`'pt'`ï¼šè¿”å›ç±»å‹ä¸º`torch.Tensor`çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.NUMPY`æˆ–`'np'`ï¼šè¿”å›ç±»å‹ä¸º`np.ndarray`çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.JAX`æˆ–`'jax'`ï¼šè¿”å›ç±»å‹ä¸º`jax.numpy.ndarray`çš„æ‰¹å¤„ç†ã€‚

+   `data_format` (`ChannelDimension`æˆ–`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`ChannelDimension.FIRST`) â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆnum_channelsï¼Œheightï¼Œwidthï¼‰æ ¼å¼ã€‚

    +   `ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

+   `input_data_format`ï¼ˆ`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   `"none"` æˆ– `ChannelDimension.NONE`: å›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

å¯¹å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡è¿›è¡Œé¢„å¤„ç†ã€‚

#### `post_process_semantic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/image_processing_dpt.py#L422)

```py
( outputs target_sizes: List = None ) â†’ export const metadata = 'undefined';semantic_segmentation
```

å‚æ•°

+   `outputs`ï¼ˆDPTForSemanticSegmentationï¼‰â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes`ï¼ˆé•¿åº¦ä¸º`batch_size`çš„ `List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›

semantic_segmentation

é•¿åº¦ä¸º`batch_size`çš„ `List[torch.Tensor]`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº `target_sizes` æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº† `target_sizes`ï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚

å°† DPTForSemanticSegmentation çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚

## DPTModel

### `class transformers.DPTModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L869)

```py
( config add_pooling_layer = True )
```

å‚æ•°

+   `config`ï¼ˆViTConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ DPT æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹çš„éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L905)

```py
( pixel_values: FloatTensor head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.dpt.modeling_dpt.BaseModelOutputWithPoolingAndIntermediateActivations or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„ `torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… DPTImageProcessor.`call`()ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„ `torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨ä¸º`æœªå±è”½`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨ä¸º`å·²å±è”½`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.models.dpt.modeling_dpt.BaseModelOutputWithPoolingAndIntermediateActivations` æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.dpt.modeling_dpt.BaseModelOutputWithPoolingAndIntermediateActivations`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆDPTConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) â€” åºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼ˆç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼‰çš„è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œå¯¹äº BERT ç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œ tanh æ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚æƒé‡æ˜¯ä»é¢„è®­ç»ƒæœŸé—´çš„ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `intermediate_activations` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*) â€” å¯ç”¨äºè®¡ç®—å„å±‚æ¨¡å‹éšè—çŠ¶æ€çš„ä¸­é—´æ¿€æ´»ã€‚

DPTModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, DPTModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("Intel/dpt-large")
>>> model = DPTModel.from_pretrained("Intel/dpt-large")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 577, 1024]
```

## DPTForDepthEstimation

### `class transformers.DPTForDepthEstimation`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1073)

```py
( config )
```

å‚æ•°

+   `config` (ViTConfig) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰æ·±åº¦ä¼°è®¡å¤´éƒ¨çš„ DPT æ¨¡å‹ï¼ˆåŒ…å« 3 ä¸ªå·ç§¯å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº KITTIã€NYUv2ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch çš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1098)

```py
( pixel_values: FloatTensor head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.DepthEstimatorOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… DPTImageProcessor.`call`()ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®æ·±åº¦ä¼°è®¡å›¾ã€‚

è¿”å›

transformers.modeling_outputs.DepthEstimatorOutput æˆ–`tuple(torch.FloatTensor)`

transformers.modeling_outputs.DepthEstimatorOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆDPTConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `predicted_depth`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.FloatTensor`ï¼‰â€” æ¯ä¸ªåƒç´ çš„é¢„æµ‹æ·±åº¦ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºä¸€ä¸ªï¼Œ+ æ¯ä¸€å±‚çš„è¾“å‡ºä¸€ä¸ªï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åã€‚

DPTForDepthEstimation çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, DPTForDepthEstimation
>>> import torch
>>> import numpy as np
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("Intel/dpt-large")
>>> model = DPTForDepthEstimation.from_pretrained("Intel/dpt-large")

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
...     predicted_depth = outputs.predicted_depth

>>> # interpolate to original size
>>> prediction = torch.nn.functional.interpolate(
...     predicted_depth.unsqueeze(1),
...     size=image.size[::-1],
...     mode="bicubic",
...     align_corners=False,
... )

>>> # visualize the prediction
>>> output = prediction.squeeze().cpu().numpy()
>>> formatted = (output * 255 / np.max(output)).astype("uint8")
>>> depth = Image.fromarray(formatted)
```

## DPTForSemanticSegmentation

### `class transformers.DPTForSemanticSegmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1259)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆViTConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰è¯­ä¹‰åˆ†å‰²å¤´çš„ DPT æ¨¡å‹ï¼Œä¾‹å¦‚ ADE20kï¼ŒCityScapesã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

`forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpt/modeling_dpt.py#L1281)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… DPTImageProcessor.`call`()ã€‚

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®ç½©ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®ç½©ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, height, width)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢çœŸå®è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`ä¸­ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.SemanticSegmenterOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.SemanticSegmenterOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆDPTConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ`config.num_labels==1`ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels, logits_height, logits_width)`) â€” æ¯ä¸ªåƒç´ çš„åˆ†ç±»åˆ†æ•°ã€‚

    <tip warning="{true}">è¿”å›çš„ logits ä¸ä¸€å®šä¸ä½œä¸ºè¾“å…¥ä¼ é€’çš„`pixel_values`å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡æ’å€¼å¹¶åœ¨ç”¨æˆ·éœ€è¦å°† logits è°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°æ—¶ä¸¢å¤±ä¸€äº›è´¨é‡ã€‚æ‚¨åº”è¯¥å§‹ç»ˆæ£€æŸ¥æ‚¨çš„ logits å½¢çŠ¶å¹¶æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚</tip>

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, patch_size, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºçš„ä¸€ä¸ª+æ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºæ—¶çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

DPTForSemanticSegmentation çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, DPTForSemanticSegmentation
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("Intel/dpt-large-ade")
>>> model = DPTForSemanticSegmentation.from_pretrained("Intel/dpt-large-ade")

>>> inputs = image_processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> logits = outputs.logits
```
