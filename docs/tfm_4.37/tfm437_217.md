# Persimmon

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/persimmon`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/persimmon)

## æ¦‚è¿°

Persimmon æ¨¡å‹æ˜¯ç”±[ADEPT](https://www.adept.ai/blog/persimmon-8b)åˆ›å»ºçš„ï¼Œä½œè€…æ˜¯ Erich Elsenï¼ŒAugustus Odenaï¼ŒMaxwell Nyeï¼ŒSaÄŸnak TaÅŸÄ±rlarï¼ŒTri Daoï¼ŒCurtis Hawthorneï¼ŒDeepak Moparthiï¼ŒArushi Somaniã€‚

ä½œè€…ä»‹ç»äº† Persimmon-8Bï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç»å…¸ transformers æ¶æ„çš„è§£ç å™¨æ¨¡å‹ï¼Œå…·æœ‰æŸ¥è¯¢å’Œé”®è§„èŒƒåŒ–ã€‚Persimmon-8B æ˜¯ä¸€ä¸ªå®Œå…¨å…è®¸è®¸å¯çš„æ¨¡å‹ï¼Œæ‹¥æœ‰çº¦ 80 äº¿ä¸ªå‚æ•°ï¼Œå‘å¸ƒåœ¨ Apache è®¸å¯ä¸‹ã€‚Persimmon-8B çš„ä¸€äº›å…³é”®å±æ€§æ˜¯é•¿ä¸Šä¸‹æ–‡å¤§å°ï¼ˆ16Kï¼‰ã€æ€§èƒ½å’Œå¤šæ¨¡æ€æ‰©å±•çš„èƒ½åŠ›ã€‚

ä½œè€…å±•ç¤ºäº†ä»–ä»¬å¯¹æ¨¡å‹è¯„ä¼°çš„æ–¹æ³•ï¼Œé‡ç‚¹æ”¾åœ¨å®é™…æ–‡æœ¬ç”Ÿæˆä¸Šï¼Œåæ˜ äº†ç”¨æˆ·ä¸è¯­è¨€æ¨¡å‹çš„äº¤äº’æ–¹å¼ã€‚è¯¥å·¥ä½œè¿˜åŒ…æ‹¬äº†ä¸€é¡¹æ¯”è¾ƒåˆ†æï¼Œå°† Persimmon-8B ä¸å…¶ä»–çŸ¥åæ¨¡å‹ï¼ˆMPT 7B Instruct å’Œ Llama 2 Base 7B 1-Shotï¼‰åœ¨å„ç§è¯„ä¼°ä»»åŠ¡ä¸­è¿›è¡Œå¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿è®­ç»ƒæ•°æ®æœ‰é™ï¼ŒPersimmon-8B çš„æ€§èƒ½ä¹Ÿå¾ˆæœ‰ç«äº‰åŠ›ã€‚

åœ¨æ¨¡å‹ç»†èŠ‚æ–¹é¢ï¼Œè¯¥å·¥ä½œæ¦‚è¿°äº† Persimmon-8B çš„æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ï¼Œæä¾›äº†å…³äºå…¶è®¾è®¡é€‰æ‹©ã€åºåˆ—é•¿åº¦å’Œæ•°æ®é›†ç»„æˆçš„è§è§£ã€‚ä½œè€…æä¾›äº†ä¸€ä¸ªå¿«é€Ÿæ¨ç†ä»£ç ï¼Œé€šè¿‡æ“ä½œèåˆå’Œ CUDA å›¾åˆ©ç”¨æ¥ä¼˜äºä¼ ç»Ÿå®ç°ï¼ŒåŒæ—¶ä¿æŒä»£ç çš„ä¸€è‡´æ€§ã€‚ä»–ä»¬è¡¨è¾¾äº†ä»–ä»¬å¯¹ç¤¾åŒºå¦‚ä½•åˆ©ç”¨è¿™ä¸€è´¡çŒ®æ¨åŠ¨åˆ›æ–°çš„æœŸå¾…ï¼Œå¹¶æš—ç¤ºå°†ä½œä¸ºä¸€ç³»åˆ—æŒç»­å‘å±•çš„ä¸€éƒ¨åˆ†å‘å¸ƒæ›´å¤šå³å°†æ¨å‡ºçš„ç‰ˆæœ¬ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[ArthurZ](https://huggingface.co/ArthurZ)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/persimmon-ai-labs/adept-inference)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

`Persimmon`æ¨¡å‹ä½¿ç”¨`bfloat16`è¿›è¡Œè®­ç»ƒï¼Œä½†åŸå§‹æ¨ç†ä½¿ç”¨`float16`ã€‚ä¸Šä¼ åˆ° hub çš„æ£€æŸ¥ç‚¹ä½¿ç”¨`torch_dtype = 'float16'`ï¼Œ`AutoModel` API å°†ä½¿ç”¨å®ƒå°†æ£€æŸ¥ç‚¹ä»`torch.float32`è½¬æ¢ä¸º`torch.float16`ã€‚

åœ¨çº¿æƒé‡çš„`dtype`å¤§å¤šæ•°æƒ…å†µä¸‹å¹¶ä¸é‡è¦ï¼Œé™¤éæ‚¨åœ¨ä½¿ç”¨`torch_dtype="auto"`åˆå§‹åŒ–æ¨¡å‹æ—¶ä½¿ç”¨`model = AutoModelForCausalLM.from_pretrained("path", torch_dtype = "auto")`ã€‚åŸå› æ˜¯æ¨¡å‹å°†é¦–å…ˆè¢«ä¸‹è½½ï¼ˆä½¿ç”¨åœ¨çº¿æ£€æŸ¥ç‚¹çš„`dtype`ï¼‰ï¼Œç„¶åå°†è¢«è½¬æ¢ä¸º`torch`çš„é»˜è®¤`dtype`ï¼ˆå˜ä¸º`torch.float32`ï¼‰ã€‚ç”¨æˆ·åº”è¯¥æŒ‡å®šä»–ä»¬æƒ³è¦çš„`torch_dtype`ï¼Œå¦‚æœä»–ä»¬ä¸è¿™æ ·åšï¼Œå®ƒå°†æ˜¯`torch.float32`ã€‚

ä¸å»ºè®®åœ¨`float16`ä¸­å¾®è°ƒæ¨¡å‹ï¼Œå·²çŸ¥ä¼šäº§ç”Ÿ`nan`ï¼Œå› æ­¤æ¨¡å‹åº”è¯¥åœ¨`bfloat16`ä¸­è¿›è¡Œå¾®è°ƒã€‚

æç¤ºï¼š

+   è¦è½¬æ¢æ¨¡å‹ï¼Œæ‚¨éœ€è¦å…‹éš†åŸå§‹å­˜å‚¨åº“ï¼Œä½¿ç”¨`git clone https://github.com/persimmon-ai-labs/adept-inference`ï¼Œç„¶åè·å–æ£€æŸ¥ç‚¹ï¼š

```py
git clone https://github.com/persimmon-ai-labs/adept-inference
wget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar
tar -xvf 8b_base_model_release.tar
python src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --input_dir /path/to/downloaded/persimmon/weights/ --output_dir /output/path \
    --pt_model_path /path/to/8b_chat_model_release/iter_0001251/mp_rank_00/model_optim_rng.pt
    --ada_lib_path /path/to/adept-inference
```

å¯¹äºèŠå¤©æ¨¡å‹ï¼š

```py
wget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar
tar -xvf 8b_base_model_release.tar
```

ä¹‹åï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½ï¼š

```py
from transformers import PersimmonForCausalLM, PersimmonTokenizer

model = PersimmonForCausalLM.from_pretrained("/output/path")
tokenizer = PersimmonTokenizer.from_pretrained("/output/path")
```

+   Perismmon ä½¿ç”¨åŸºäº`sentencepiece`çš„åˆ†è¯å™¨ï¼Œå…·æœ‰ä¸€ä¸ª`Unigram`æ¨¡å‹ã€‚å®ƒæ”¯æŒ bytefallbackï¼Œä»…åœ¨å¿«é€Ÿåˆ†è¯å™¨çš„`tokenizers==0.14.0`ä¸­å¯ç”¨ã€‚`LlamaTokenizer`è¢«ç”¨ä½œå®ƒæ˜¯ä¸€ä¸ªå›´ç»• sentencepiece çš„æ ‡å‡†åŒ…è£…å™¨ã€‚`chat`æ¨¡æ¿å°†åœ¨åç»­ PR ä¸­ä½¿ç”¨æ¨¡æ¿å‡½æ•°è¿›è¡Œæ›´æ–°ï¼

+   ä½œè€…å»ºè®®ä¸ºèŠå¤©æ¨¡å¼ä½¿ç”¨ä»¥ä¸‹æç¤ºæ ¼å¼ï¼š`f"human: {prompt}\n\nadept:"`

## PersimmonConfig

### `class transformers.PersimmonConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/configuration_persimmon.py#L28)

```py
( vocab_size = 262144 hidden_size = 4096 intermediate_size = 16384 num_hidden_layers = 36 num_attention_heads = 64 hidden_act = 'relu2' max_position_embeddings = 16384 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True tie_word_embeddings = False rope_theta = 25000.0 rope_scaling = None qk_layernorm = True hidden_dropout = 0.0 attention_dropout = 0.0 partial_rotary_factor = 0.5 pad_token_id = None bos_token_id = 1 eos_token_id = 2 **kwargs )
```

å‚æ•°

+   `vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 262144ï¼‰â€”Persimmon æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ PersimmonModel æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°çš„æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, defaults to 4096) â€” éšè—è¡¨ç¤ºçš„ç»´åº¦ã€‚

+   `intermediate_size` (`int`, *optional*, defaults to 16384) â€” MLP è¡¨ç¤ºçš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, defaults to 36) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 64) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"relu2"`) â€” è§£ç å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚

+   `max_position_embeddings` (`int`, *optional*, defaults to 16384) â€” è¯¥æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) â€” rms å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨`config.is_decoder=True`æ—¶ç›¸å…³ã€‚

+   `tie_word_embeddings(bool,` *optional*, defaults to `False`) â€” æ˜¯å¦ç»‘å®šæƒé‡åµŒå…¥

+   `rope_theta` (`float`, *optional*, defaults to 25000.0) â€” RoPE åµŒå…¥çš„åŸºæœ¬å‘¨æœŸã€‚

+   `rope_scaling` (`Dict`, *optional*) â€” åŒ…å« RoPE åµŒå…¥ç¼©æ”¾é…ç½®çš„å­—å…¸ã€‚å½“å‰æ”¯æŒä¸¤ç§ç¼©æ”¾ç­–ç•¥ï¼šçº¿æ€§å’ŒåŠ¨æ€ã€‚å®ƒä»¬çš„ç¼©æ”¾å› å­å¿…é¡»æ˜¯å¤§äº 1 çš„æµ®ç‚¹æ•°ã€‚æœŸæœ›çš„æ ¼å¼æ˜¯`{"type": ç­–ç•¥åç§°, "factor": ç¼©æ”¾å› å­}`ã€‚ä½¿ç”¨æ­¤æ ‡å¿—æ—¶ï¼Œä¸è¦å°†`max_position_embeddings`æ›´æ–°ä¸ºé¢„æœŸçš„æ–°æœ€å¤§å€¼ã€‚æœ‰å…³è¿™äº›ç¼©æ”¾ç­–ç•¥è¡Œä¸ºçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä»¥ä¸‹ä¸»é¢˜ï¼š[`www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/`](https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/)ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿç ´åæ€§ API æ›´æ”¹ã€‚

+   `qk_layernorm` (`bool`, *optional*, default to `True`) â€” åœ¨æŠ•å½±éšè—çŠ¶æ€åæ˜¯å¦å¯¹æŸ¥è¯¢å’Œé”®è¿›è¡Œå½’ä¸€åŒ–

+   `hidden_dropout` (`float`, *optional*, default to 0.0) â€” åœ¨å°† MLP åº”ç”¨äºéšè—çŠ¶æ€åçš„ dropout æ¯”ç‡ã€‚

+   `attention_dropout` (`float`, *optional*, default to 0.0) â€” è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°åçš„ dropout æ¯”ç‡ã€‚

+   `partial_rotary_factor` (`float`, *optional*, default to 0.5) â€” æŸ¥è¯¢å’Œé”®ä¸­å°†å…·æœ‰æ—‹è½¬åµŒå…¥çš„ç™¾åˆ†æ¯”ã€‚

    ç¤ºä¾‹ â€”

è¿™æ˜¯ç”¨äºå­˜å‚¨ PersimmonModel é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– Persimmon æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº[adept/persimmon-8b-base](https://huggingface.co/adept/persimmon-8b-base)çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

```py
>>> from transformers import PersimmonModel, PersimmonConfig

>>> # Initializing a Persimmon persimmon-7b style configuration
>>> configuration = PersimmonConfig()
```

## PersimmonModel

### `class transformers.PersimmonModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L544)

```py
( config: PersimmonConfig )
```

å‚æ•°

+   `config` (PersimmonConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚config â€” PersimmonConfig

è£¸çš„ Persimmon æ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´éƒ¨ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

ç”±*config.num_hidden_layers*å±‚ç»„æˆçš„ Transformer è§£ç å™¨ã€‚æ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ª`PersimmonDecoderLayer`

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L577)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰å®šä¸º`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºæœªè¢«â€œæ©ç›–â€çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«â€œæ©ç›–â€çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œæ©ç›–â€ã€‚

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç›–â€ã€‚

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `past_key_values` (`Cache` æˆ– `tuple(tuple(torch.FloatTensor))`, *å¯é€‰*) â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª Cache å®ä¾‹ï¼›

    +   é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå³æœªå°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„é‚£äº›ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å¾ˆæœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

PersimmonModel çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## PersimmonForCausalLM

### `class transformers.PersimmonForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L701)

```py
( config )
```

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L738)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   å¯¹äºâ€œæœªæ©ç â€çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºâ€œæ©ç â€çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœæ‚¨æƒ³è¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®æ‚¨çš„éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œæ©ç›–â€ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç›–â€ã€‚

+   `position_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.n_positions - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª Cache å®ä¾‹ï¼›

    +   é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„`tuple(torch.FloatTensor)`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆè¿™äº›`input_ids`æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

    å‚æ•° â€” æ ‡ç­¾ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰ï¼šç”¨äºè®¡ç®—æ©ç›–è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`æˆ–-100ï¼ˆè¯·å‚é˜…`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç›–ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æ ‡è®°ã€‚

è¿”å›

transformers.modeling_outputs.CausalLMOutputWithPast æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.CausalLMOutputWithPast æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆPersimmonConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰ â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºä¸€ä¸ª + æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

PersimmonForCausalLM çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, PersimmonForCausalLM

>>> model = PersimmonForCausalLM.from_pretrained("adept/persimmon-8b-base")
>>> tokenizer = AutoTokenizer.from_pretrained("adept/persimmon-8b-base")

>>> prompt = "human: Hey, what should I eat for dinner?"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> # Generate
>>> generate_ids = model.generate(inputs.input_ids, max_length=30)
>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
'human: Hey, what should I eat for dinner?\n\ncat: ğŸ±\n\nhuman: ğŸ˜\n\n'
```

## PersimmonForSequenceClassification

### `class transformers.PersimmonForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L893)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆPersimmonConfigï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰é¡¶éƒ¨åºåˆ—åˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰çš„ Persimmon å˜æ¢å™¨ã€‚

PersimmonForSequenceClassification ä½¿ç”¨æœ€åä¸€ä¸ªæ ‡è®°æ¥è¿›è¡Œåˆ†ç±»ï¼Œå°±åƒå…¶ä»–å› æœæ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰ä¸€æ ·ã€‚

ç”±äºå®ƒå¯¹æœ€åä¸€ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤éœ€è¦çŸ¥é“æœ€åä¸€ä¸ªæ ‡è®°çš„ä½ç½®ã€‚å¦‚æœåœ¨é…ç½®ä¸­å®šä¹‰äº†`pad_token_id`ï¼Œåˆ™åœ¨æ¯ä¸€è¡Œä¸­æ‰¾åˆ°ä¸æ˜¯å¡«å……æ ‡è®°çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚å¦‚æœæœªå®šä¹‰`pad_token_id`ï¼Œåˆ™ç®€å•åœ°å–æ¯ä¸€è¡Œæ‰¹æ¬¡ä¸­çš„æœ€åä¸€ä¸ªå€¼ã€‚ç”±äºåœ¨ä¼ é€’`inputs_embeds`è€Œä¸æ˜¯`input_ids`æ—¶æ— æ³•çŒœæµ‹å¡«å……æ ‡è®°ï¼Œå› æ­¤å®ƒæ‰§è¡Œç›¸åŒæ“ä½œï¼ˆå–æ¯ä¸€è¡Œæ‰¹æ¬¡ä¸­çš„æœ€åä¸€ä¸ªå€¼ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/persimmon/modeling_persimmon.py#L925)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºæœªè¢«æ©ç çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«æ©ç çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯é€‰åœ°åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç ã€‚

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*) â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨è§£ç çš„å…ˆå‰é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª Cache å®ä¾‹ï¼›

    +   é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼‰ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå³é‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

PersimmonForSequenceClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
