# VipLlava

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/vipllava`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vipllava)

## 概述

VipLlava 模型是由 Mu Cai、Haotian Liu、Siva Karthik Mustikovela、Gregory P. Meyer、Yuning Chai、Dennis Park、Yong Jae Lee 在[《Making Large Multimodal Models Understand Arbitrary Visual Prompts》](https://arxiv.org/abs/2312.00784)中提出的。

VipLlava 通过在训练过程中标记图像并使用自然提示（如“红色边界框”或“指向箭头”）与模型进行交互，增强了 Llava 的训练协议。

该论文的摘要如下：

*尽管现有的大型视觉-语言多模态模型侧重于整体图像理解，但在实现特定区域理解方面存在明显差距。目前使用文本坐标或空间编码的方法通常无法提供用户友好的视觉提示界面。为了解决这一挑战，我们引入了一种能够解码任意视觉提示的新型多模态模型。这使用户可以直观地标记图像，并使用自然提示与模型进行交互，如“红色边界框”或“指向箭头”。我们的简单设计直接将视觉标记叠加在 RGB 图像上，消除了复杂区域编码的需求，同时在 Visual7W、PointQA 和 Visual Commonsense Reasoning 基准等区域理解任务上实现了最先进的性能。此外，我们提出了 ViP-Bench，一个全面的基准，用于评估模型在理解多维视觉提示方面的能力，促进该领域的未来研究。代码、数据和模型均可公开获取。*

提示：

+   该架构与 llava 架构类似，只是多模态投影器采用一组连接的视觉隐藏状态，并在该模块上增加了一个 layernorm 层。

+   我们建议用户在计算批量生成时使用`padding_side="left"`，因为这会导致更准确的结果。只需确保在生成之前调用`processor.tokenizer.padding_side = "left"`。

+   请注意，该模型尚未明确训练以处理同一提示中的多个图像，尽管从技术上讲这是可能的，但您可能会遇到不准确的结果。

+   为了获得更好的结果，我们建议用户使用正确的提示格式提示模型：

```py
A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\n<prompt>###Assistant:
```

对于多轮对话:

```py
A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\n<prompt1>###Assistant: <answer1>###Human: <prompt2>###Assistant:
```

原始代码可在[此处](https://github.com/mu-cai/ViP-LLaVA)找到。

该模型由[Younes Belkada](https://huggingface.co/ybelkada)贡献

## VipLlavaConfig

### `class transformers.VipLlavaConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vipllava/configuration_vipllava.py#L28)

```py
( vision_config = None text_config = None ignore_index = -100 image_token_index = 32000 projector_hidden_act = 'gelu' projector_layernorm_eps = 1e-05 vision_feature_layers = [-2, -5, -8, -11, 6] vocab_size = 32000 **kwargs )
```

参数

+   `vision_config`（`VipLlavaVisionConfig`，*可选*）— 自定义视觉配置或字典

+   `text_config`（`Union[AutoConfig, dict]`，*可选*）— 文本主干的配置对象。可以是`LlamaConfig`或`MistralConfig`中的任何一个。

+   `ignore_index`（`int`，*可选*，默认为-100）— 损失函数的忽略索引。

+   `image_token_index`（`int`，*可选*，默认为 32000）— 用于编码图像提示的图像标记索引。

+   `projector_hidden_act`（`str`，*可选*，默认为`"gelu"`）— 多模态投影器使用的激活函数。

+   `projector_layernorm_eps`（`float`，*可选*，默认为 1e-05）— 投影器 layernorm 的层归一化 epsilon

+   `vision_feature_layers`（`List[int]`，*可选*，默认为`[-2, -5, -8, -11, 6]`）— 选择视觉特征的层列表。

+   `vocab_size`（`int`，*可选*，默认为 32000）— VipLlava 模型的词汇量。定义了在调用~VipLlavaForConditionalGeneration 时可以表示的不同标记数量。

这是存储 VipLlavaForConditionalGeneration 配置的配置类。用于根据指定参数实例化 VipLlava 模型，定义模型架构。使用默认值实例化配置将产生类似于 VipLlava-9B 的配置。

例如[ybelkada/vip-llava-7b-hf](https://huggingface.co/ybelkada/vip-llava-7b-hf)

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import VipLlavaForConditionalGeneration, VipLlavaConfig, CLIPVisionConfig, LlamaConfig

>>> # Initializing a CLIP-vision config
>>> vision_config = CLIPVisionConfig()

>>> # Initializing a Llama config
>>> text_config = LlamaConfig()

>>> # Initializing a VipLlava vipllava-7b style configuration
>>> configuration = VipLlavaConfig(vision_config, text_config)

>>> # Initializing a model from the vipllava-7b style configuration
>>> model = VipLlavaForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## VipLlavaForConditionalGeneration

### `class transformers.VipLlavaForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vipllava/modeling_vipllava.py#L240)

```py
( config: VipLlavaConfig )
```

参数

+   `config`（VipLlavaConfig 或`VipLlavaVisionConfig`）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

VIPLLAVA 模型由视觉主干和语言模型组成。此模型继承自 PreTrainedModel。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

前进

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vipllava/modeling_vipllava.py#L356)

```py
( input_ids: LongTensor = None pixel_values: FloatTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None vision_feature_layers: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.vipllava.modeling_vipllava.VipLlavaCausalLMOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    索引可以使用 AutoTokenizer 获取。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `pixel_values`（形状为`(batch_size, num_channels, image_size, image_size)`的`torch.FloatTensor`）- 对应输入图像的张量。像素值可以使用 AutoImageProcessor 获取。有关详细信息，请参阅 CLIPImageProcessor.__call__()（[`LlavaProcessor`]使用 CLIPImageProcessor 处理图像）。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 避免在填充标记索引上执行注意力的蒙版。蒙版值选择在`[0, 1]`之间：

    +   对于未被屏蔽的标记为 1，

    +   对于被屏蔽的标记为 0。

    什么是注意力蒙版？

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    如果使用`past_key_values`，则可以选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。

    如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。什么是位置 ID？

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（即未将其过去的键值状态提供给此模型的那些）的形状为`(batch_size, 1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多控制如何将`input_ids`索引转换为相关向量，则这很有用，而不是使用模型的内部嵌入查找矩阵。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

    参数 — 标签（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）：用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`或-100（参见`input_ids`文档字符串）。将索引设置为`-100`的标记将被忽略（masked），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

返回

`transformers.models.vipllava.modeling_vipllava.VipLlavaCausalLMOutputWithPast`或`tuple(torch.FloatTensor)`

一个`transformers.models.vipllava.modeling_vipllava.VipLlavaCausalLMOutputWithPast`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（VipLlavaConfig）和输入的不同元素。

+   `损失` (`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`labels`时返回) — 语言建模损失（用于下一个标记的预测）。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量）

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（查看`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

+   `image_hidden_states` (`tuple(torch.FloatTensor)`，*可选*) — 形状为`(batch_size, num_images, sequence_length, hidden_size)`的`torch.FloatTensor`元组（用于图像嵌入的输出）。

    由视觉编码器生成的模型的图像隐藏状态，以及可选的感知器

VipLlavaForConditionalGeneration 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, VipLlavaForConditionalGeneration

>>> model = VipLlavaForConditionalGeneration.from_pretrained("llava-hf/vip-llava-7b-hf", device_map="auto", torch_dtype=torch.float16)
>>> processor = AutoProcessor.from_pretrained("llava-hf/vip-llava-7b-hf")

>>> prompt = "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\n{}###Assistant:"
>>> question = "Can you please describe this image?"
>>> prompt = prompt.format(question)
>>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-neg.png"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=text, images=image, return_tensors="pt").to(0, torch.float16)

>>> # Generate
>>> generate_ids = model.generate(**inputs, max_new_tokens=20)
>>> processor.decode(generate_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
The image features a brown and white cat sitting on a green surface, with a red ball in its
```
