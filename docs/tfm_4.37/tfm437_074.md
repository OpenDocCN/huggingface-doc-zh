# 使用 TensorFlow 在 TPU 上训练

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/perf_train_tpu_tf`](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_tpu_tf)

如果您不需要长篇解释，只想要 TPU 代码示例来开始使用，请查看[我们的 TPU 示例笔记本！](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)

### 什么是 TPU？

TPU 是**张量处理单元**。它们是由 Google 设计的硬件，用于大大加速神经网络中的张量计算，类似于 GPU。它们可用于网络训练和推断。通常通过 Google 的云服务访问，但也可以通过 Google Colab 和 Kaggle Kernels 直接免费访问小型 TPU。

因为[🤗 Transformers 中的所有 TensorFlow 模型都是 Keras 模型](https://huggingface.co/blog/tensorflow-philosophy)，因此本文档中的大多数方法通常适用于任何 Keras 模型的 TPU 训练！但是，有一些点是特定于 HuggingFace 生态系统（hug-o-system？）的 Transformers 和 Datasets，当我们到达这些点时，我们将确保标记它们。

### 有哪些类型的 TPU 可用？

新用户经常对各种 TPU 和访问方式感到困惑。要理解的第一个关键区别是**TPU 节点**和**TPU VM**之间的区别。

当您使用**TPU 节点**时，实际上是间接访问远程 TPU。您将需要一个单独的 VM，该 VM 将初始化您的网络和数据管道，然后将它们转发到远程节点。当您在 Google Colab 上使用 TPU 时，您是以**TPU 节点**样式访问它。

对于不习惯使用 TPU 的人来说，使用 TPU 节点可能会产生一些意想不到的行为！特别是，因为 TPU 位于与运行 Python 代码的机器物理上不同的系统上，您的数据不能是本地的 - 从您机器的内部存储加载的任何数据管道将完全失败！相反，数据必须存储在 Google Cloud Storage 中，您的数据管道仍然可以访问它，即使管道在远程 TPU 节点上运行。

如果您可以将所有数据存储在内存中作为`np.ndarray`或`tf.Tensor`，那么即使在使用 Colab 或 TPU 节点时，也可以在该数据上进行`fit()`，而无需将其上传到 Google Cloud Storage。

**🤗具体的 Hugging Face 提示🤗：**`Dataset.to_tf_dataset()`方法及其更高级别的包装器`model.prepare_tf_dataset()`，您将在我们的 TF 代码示例中看到，都会在 TPU 节点上失败。原因是即使它们创建了一个`tf.data.Dataset`，它也不是“纯粹”的`tf.data`管道，并且使用`tf.numpy_function`或`Dataset.from_generator()`从底层 HuggingFace`Dataset`中流式传输数据。这个 HuggingFace`Dataset`由存储在本地磁盘上的数据支持，远程 TPU 节点将无法读取。

第二种访问 TPU 的方式是通过**TPU VM**。在使用 TPU VM 时，您直接连接到 TPU 连接的机器，就像在 GPU VM 上进行训练一样。TPU VM 通常更容易使用，特别是在处理数据管道时。所有上述警告不适用于 TPU VM！

这是一份主观的文件，所以这是我们的意见：**尽量避免使用 TPU Node。**它比 TPU VM 更令人困惑，更难以调试。未来也可能不受支持 - 谷歌最新的 TPU，TPUv4，只能作为 TPU VM 访问，这表明 TPU Node 越来越可能成为“传统”访问方法。但是，我们了解到唯一免费的 TPU 访问是在 Colab 和 Kaggle Kernels 上，它们使用 TPU Node - 因此，如果必须使用，我们将尝试解释如何处理！查看[TPU 示例笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)以获取更详细的代码示例。

### 可用的 TPU 尺寸是多少？

单个 TPU（v2-8/v3-8/v4-8）运行 8 个副本。TPU 存在于可以同时运行数百或数千个副本的**pod**中。当您使用多个 TPU 但少于整个 pod 时（例如 v3-32），您的 TPU 群被称为**pod slice**。

当您通过 Colab 访问免费的 TPU 时，通常会获得一个 v2-8 TPU。

### 我一直听说这个 XLA。XLA 是什么，它与 TPU 有什么关系？

XLA 是一个优化编译器，被 TensorFlow 和 JAX 同时使用。在 JAX 中，它是唯一的编译器，而在 TensorFlow 中是可选的（但在 TPU 上是强制的！）。在训练 Keras 模型时启用它的最简单方法是将参数`jit_compile=True`传递给`model.compile()`。如果没有出现任何错误且性能良好，那么这是一个很好的迹象，表明您已准备好转移到 TPU！

在 TPU 上进行调试通常比在 CPU/GPU 上更困难，因此我们建议在尝试在 TPU 上运行之前，先在 CPU/GPU 上使用 XLA 使您的代码能够运行。当然，您不必训练很长时间 - 只需进行几个步骤，以确保您的模型和数据流水线按照您的预期工作。

XLA 编译的代码通常更快 - 因此，即使您不打算在 TPU 上运行，添加`jit_compile=True`也可以提高性能。但是，请注意下面关于 XLA 兼容性的注意事项！

**基于痛苦经验的提示：**虽然使用`jit_compile=True`是获得速度提升并测试您的 CPU/GPU 代码是否与 XLA 兼容的好方法，但如果在实际在 TPU 上训练时保留它，可能会导致许多问题。XLA 编译将在 TPU 上隐式发生，因此在实际在 TPU 上运行代码之前，请记得删除那行！

### 如何使我的模型与 XLA 兼容？

在许多情况下，您的代码可能已经与 XLA 兼容！但是，有一些在普通 TensorFlow 中有效但在 XLA 中无效的事情。我们将它们概括为以下三条核心规则：

**🤗具体的 HuggingFace 提示🤗：**我们已经付出了很多努力，将我们的 TensorFlow 模型和损失函数重写为 XLA 兼容。我们的模型和损失函数通常默认遵守规则＃1 和＃2，因此如果您使用`transformers`模型，则可以跳过它们。但是，在编写自己的模型和损失函数时，请不要忘记这些规则！

#### XLA 规则＃1：您的代码不能具有“数据相关条件”

这意味着任何`if`语句都不能依赖于`tf.Tensor`内部的值。例如，此代码块无法使用 XLA 编译！

```py
if tf.reduce_sum(tensor) > 10:
    tensor = tensor / 2.0
```

这一开始可能看起来非常受限制，但大多数神经网络代码不需要这样做。您通常可以通过使用`tf.cond`（请参阅[此处](https://www.tensorflow.org/api_docs/python/tf/cond)的文档）或通过删除条件并找到一个巧妙的数学技巧来绕过此限制，例如：

```py
sum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)
tensor = tensor / (1.0 + sum_over_10)
```

这段代码与上面的代码具有完全相同的效果，但通过避免条件语句，我们确保它将在 XLA 中编译而无问题！

#### XLA 规则＃2：您的代码不能具有“数据相关形状”

这意味着代码中所有的`tf.Tensor`对象的形状不能依赖于它们的值。例如，函数`tf.unique`不能与 XLA 一起编译，因为它返回一个包含输入中每个唯一值的`tensor`。这个输出的形状显然会根据输入`Tensor`的重复程度而不同，因此 XLA 拒绝处理它！

一般来说，大多数神经网络代码默认遵守规则＃2。但是，在一些常见情况下，这可能会成为一个问题。一个非常常见的情况是当您使用**标签屏蔽**时，将标签设置为负值以指示在计算损失时应忽略这些位置。如果您查看支持标签屏蔽的 NumPy 或 PyTorch 损失函数，您经常会看到类似于使用[布尔索引](https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing)的代码：

```py
label_mask = labels >= 0
masked_outputs = outputs[label_mask]
masked_labels = labels[label_mask]
loss = compute_loss(masked_outputs, masked_labels)
mean_loss = torch.mean(loss)
```

这段代码在 NumPy 或 PyTorch 中完全正常，但在 XLA 中会出错！为什么？因为`masked_outputs`和`masked_labels`的形状取决于有多少位置被屏蔽 - 这使其成为**数据相关形状。**然而，就像规则＃1 一样，我们通常可以重写这段代码，以产生完全相同的输出，而不涉及任何数据相关形状。

```py
label_mask = tf.cast(labels >= 0, tf.float32)
loss = compute_loss(outputs, labels)
loss = loss * label_mask  # Set negative label positions to 0
mean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)
```

在这里，我们通过为每个位置计算损失，但在计算均值时将被屏蔽的位置在分子和分母中归零，从而获得与第一个块完全相同的结果，同时保持 XLA 兼容性。请注意，我们使用与规则＃1 相同的技巧 - 将`tf.bool`转换为`tf.float32`并将其用作指示变量。这是一个非常有用的技巧，所以如果您需要将自己的代码转换为 XLA，请记住它！

#### XLA 规则＃3：XLA 将需要为每个不同的输入形状重新编译您的模型

这是一个重要的规则。这意味着如果您的输入形状非常不同，XLA 将不得不一遍又一遍地重新编译您的模型，这将导致巨大的性能问题。这在 NLP 模型中经常出现，因为输入文本在标记化后长度不同。在其他模态中，静态形状更常见，这个规则就不是那么大的问题了。

如何避开规则＃3？关键是**填充** - 如果您将所有输入填充到相同的长度，然后使用`attention_mask`，您可以获得与可变形状相同的结果，但没有任何 XLA 问题。然而，过度填充也会导致严重的减速 - 如果您将所有样本填充到整个数据集中的最大长度，您可能会得到由无尽填充标记组成的批次，这将浪费大量计算和内存！

解决这个问题并没有完美的方法。但是，你可以尝试一些技巧。一个非常有用的技巧是**将样本批次填充到 32 或 64 个标记的倍数。**这通常只会增加少量标记的数量，但会大大减少唯一输入形状的数量，因为现在每个输入形状都必须是 32 或 64 的倍数。更少的唯一输入形状意味着更少的 XLA 编译！

**🤗HuggingFace 专属提示🤗：**我们的分词器和数据整理器有助于解决这个问题的方法。在调用分词器时，您可以使用`padding="max_length"`或`padding="longest"`来获取填充数据。我们的分词器和数据整理器还有一个`pad_to_multiple_of`参数，可以减少您看到的唯一输入形状的数量！

### 我如何在 TPU 上实际训练我的模型？

一旦您的训练是 XLA 兼容的，并且（如果您正在使用 TPU 节点/Colab）您的数据集已经准备就绪，那么在 TPU 上运行实际上非常容易！您真正需要在代码中做的改变只是添加几行代码来初始化您的 TPU，并确保您的模型和数据集都在`TPUStrategy`范围内创建。查看[我们的 TPU 示例笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)以查看实际操作！

### 总结

这里有很多内容，让我们用一个快速的清单来总结，当您想要准备好您的模型进行 TPU 训练时可以遵循：

+   确保您的代码遵循 XLA 的三条规则

+   在 CPU/GPU 上使用`jit_compile=True`编译您的模型，并确认您可以使用 XLA 进行训练

+   要么将数据集加载到内存中，要么使用兼容 TPU 的数据集加载方法（请参阅[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)）

+   将您的代码迁移到 Colab（加速器设置为“TPU”）或 Google Cloud 上的 TPU VM

+   添加 TPU 初始化代码（请参阅[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)）

+   创建您的`TPUStrategy`，并确保数据集加载和模型创建在`strategy.scope()`内（请参阅[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)）

+   当您转移到 TPU 时，不要忘记再次将`jit_compile=True`去掉！

+   🙏🙏🙏🥺🥺🥺

+   调用 model.fit()

+   你做到了！
