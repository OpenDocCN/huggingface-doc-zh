# 社区

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/community`](https://huggingface.co/docs/transformers/v4.37.2/en/community)

此页面汇集了由社区开发的🤗 Transformers 周围的资源。

## 社区资源：

| 资源 | 描述 | 作者 |
| :-- | :-- | --: |
| [Hugging Face Transformers 词汇表闪卡](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | 基于 Transformers Docs 词汇表的一套闪卡，已经制作成易于使用[Anki](https://apps.ankiweb.net/)学习/复习的形式，Anki 是一款专门设计用于长期知识保留的开源、跨平台应用程序。查看这个[介绍性视频，了解如何使用闪卡](https://www.youtube.com/watch?v=Dji_h7PILrw)。 | [Darigov Research](https://www.darigovresearch.com/) |

## 社区笔记本：

| 笔记本 | 描述 | 作者 |  |
| :-- | :-- | :-- | --: |
| [微调预训练的 Transformer 以生成歌词](https://github.com/AlekseyKorshuk/huggingartists) | 如何通过微调 GPT-2 模型生成您最喜爱艺术家风格的歌词 | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | ![在 Colab 中打开](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |
| [在 Tensorflow 2 中训练 T5](https://github.com/snapthat/TF-T5-text-to-text) | 如何使用 Tensorflow 2 为任何任务训练 T5。这个笔记本演示了在 Tensorflow 2 中实现的一个问答任务，使用 SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron) | ![在 Colab 中打开](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |
| [在 TPU 上训练 T5](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb) | 如何使用 Transformers 和 Nlp 在 SQUAD 上训练 T5 | [Suraj Patil](https://github.com/patil-suraj) | ![在 Colab 中打开](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |
| [为分类和多项选择微调 T5](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) | 如何使用 PyTorch Lightning 以文本-文本格式微调 T5 以进行分类和多项选择任务 | [Suraj Patil](https://github.com/patil-suraj) | ![在 Colab 中打开](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |
| [在新数据集和语言上微调 DialoGPT](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) | 如何在新数据集上微调 DialoGPT 模型，用于开放对话聊天机器人 | [Nathan Cooper](https://github.com/ncoop57) | ![在 Colab 中打开](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |
| [使用 Reformer 进行长序列建模](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb) | 如何使用 Reformer 在长度为 500,000 个标记的序列上进行训练 | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb) |
| [为摘要微调 BART](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) | 如何使用 blurr 使用 fastai 微调 BART 进行摘要 | [Wayde Gilliam](https://ohmeow.com/) | ![在 Colab 中打开](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) |
| [为任何人的推文微调预训练 Transformer](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) | 如何通过微调 GPT-2 模型生成您喜爱的 Twitter 账户风格的推文 | [Boris Dayma](https://github.com/borisdayma) | ![在 Colab 中打开](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) |
| [使用 Weights＆Biases 优化🤗Hugging Face 模型](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) | 一个完整的教程，展示了 W＆B 与 Hugging Face 的集成 | [Boris Dayma](https://github.com/borisdayma) | ![在 Colab 中打开](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) |
| [预训练 Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) | 如何构建现有预训练模型的“长”版本 | [Iz Beltagy](https://beltagy.net) | ![在 Colab 中打开](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) |
| [为 QA 微调 Longformer](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) | 如何为 QA 任务微调 Longformer 模型 | [Suraj Patil](https://github.com/patil-suraj) | ![在 Colab 中打开](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) |
| [使用🤗nlp 评估模型](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb) | 如何使用`nlp`在 TriviaQA 上评估 Longformer | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing) |
| [为情感跨度提取微调 T5](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) | 如何使用 PyTorch Lightning 以文本到文本格式微调 T5 进行情感跨度提取 | [Lorenzo Ampil](https://github.com/enzoampil) | ![在 Colab 中打开](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |
| [为多类分类微调 DistilBert](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | 如何使用 PyTorch 微调 DistilBert 进行多类分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | ![在 Colab 中打开](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) |
| [微调 BERT 进行多标签分类](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb) | 如何使用 PyTorch 微调 BERT 进行多标签分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | ![在 Colab 中打开](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb) |
| [微调 T5 进行摘要](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) | 如何在 PyTorch 中微调 T5 进行摘要，并使用 WandB 跟踪实验 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | ![在 Colab 中打开](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) |
| [使用动态填充/分桶加速 Transformer 中的微调](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb) | 如何通过动态填充/分桶将微调加速 2 倍 | [Michael Benesty](https://github.com/pommedeterresautee) | ![在 Colab 中打开](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing) |
| [为遮蔽语言建模预训练 Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb) | 如何训练具有双向自注意力层的 Reformer 模型 | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing) |
| [扩展和微调 Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb) | 如何在 CORD 数据集上增加预训练的 SciBERT 模型的词汇量并进行流水线处理。 | [Tanmay Thakur](https://github.com/lordtt13) | ![在 Colab 中打开](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8) |
| [使用 Trainer API 微调 BlenderBotSmall 进行摘要](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb) | 如何在自定义数据集上使用 Trainer API 微调 BlenderBotSmall 进行摘要 | [Tanmay Thakur](https://github.com/lordtt13) | ![在 Colab 中打开](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing) |
| [微调 Electra 并使用 Integrated Gradients 进行解释](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) | 如何微调 Electra 进行情感分析，并使用 Captum Integrated Gradients 解释预测 | [Eliza Szczechla](https://elsanns.github.io) | ![在 Colab 中打开](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) |
| [使用 Trainer 类微调非英语 GPT-2 模型](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) | 如何使用 Trainer 类微调非英语 GPT-2 模型 | [Philipp Schmid](https://www.philschmid.de) | ![在 Colab 中打开](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) |
| [为多标签分类任务微调 DistilBERT 模型](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) | 如何为多标签分类任务微调 DistilBERT 模型 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | ![在 Colab 中打开](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) |
| [为句对分类微调 ALBERT](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | 如何为句对分类任务微调 ALBERT 模型或其他基于 BERT 的模型 | [Nadir El Manouzi](https://github.com/NadirEM) | ![在 Colab 中打开](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) |
| [Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | 如何为情感分析微调一个 Roberta 模型 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | ![在 Colab 中打开](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) |
| [评估问题生成模型](https://github.com/flexudy-pipe/qugeev) | 您的 seq2seq 变压器模型生成的问题的答案有多准确？ | [Pascal Zoleko](https://github.com/zolekode) | ![在 Colab 中打开](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing) |
| [使用 DistilBERT 和 Tensorflow 对文本进行分类](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | 如何在 TensorFlow 中为文本分类微调 DistilBERT | [Peter Bayerle](https://github.com/peterbayerle) | ![在 Colab 中打开](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) |
| [利用 BERT 进行 CNN/Dailymail 的编码器-解码器摘要](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) | 如何使用*bert-base-uncased*检查点对 CNN/Dailymail 的摘要进行热启动*EncoderDecoderModel* | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) |
| [利用 RoBERTa 进行 BBC XSum 的编码器-解码器摘要](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) | 如何使用*roberta-base*检查点对 BBC/XSum 的摘要进行热启动共享*EncoderDecoderModel* | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) |
| [在顺序问答（SQA）上微调 TAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) | 如何在顺序问答（SQA）数据集上使用*tapas-base*检查点微调*TapasForQuestionAnswering* | [Niels Rogge](https://github.com/nielsrogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) |
| [在 Table Fact Checking（TabFact）上评估 TAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) | 如何使用🤗数据集和🤗transformers 库的组合评估经过微调的*TapasForSequenceClassification*，使用*tapas-base-finetuned-tabfact*检查点 | [Niels Rogge](https://github.com/nielsrogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) |
| [为翻译微调 mBART](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) | 如何使用 Seq2SeqTrainer 为印地语到英语翻译微调 mBART | [Vasudev Gupta](https://github.com/vasudevgupta7) | ![在 Colab 中打开](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) |
| [在 FUNSD 上微调 LayoutLM（一种表单理解数据集）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) | 如何在 FUNSD 数据集上微调*LayoutLMForTokenClassification*，从扫描文档中提取信息 | [Niels Rogge](https://github.com/nielsrogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) |
| [微调 DistilGPT2 并生成文本](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) | 如何微调 DistilGPT2 并生成文本 | [Aakash Tripathi](https://github.com/tripathiaakash) | ![在 Colab 中打开](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) |
| [在最多 8K 标记上微调 LED](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb) | 如何在 pubmed 上微调 LED 进行长距离摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb) |
| [在 Arxiv 上评估 LED](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) | 如何有效评估 LED 进行长距离摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) |
| [在 RVL-CDIP 上微调 LayoutLM（一种文档图像分类数据集）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) | 如何在 RVL-CDIP 数据集上微调*LayoutLMForSequenceClassification*，用于扫描文档分类 | [Niels Rogge](https://github.com/nielsrogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) |
| - [使用 GPT2 调整进行 Wav2Vec2 CTC 解码](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb) | 如何使用语言模型调整解码 CTC 序列 | [Eric Lam](https://github.com/voidful) | ![在 Colab 中打开](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing) |
| - [使用 Trainer 类在两种语言中对 BART 进行摘要微调](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) | 如何使用 Trainer 类在两种语言中对 BART 进行摘要微调 | [Eliza Szczechla](https://github.com/elsanns) | ![在 Colab 中打开](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) |
| - [在 Trivia QA 上评估 Big Bird](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) | 如何在 Trivia QA 上评估 BigBird 在长文档问答上的表现 | [Patrick von Platen](https://github.com/patrickvonplaten) | ![在 Colab 中打开](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) |
| - [使用 Wav2Vec2 创建视频字幕](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | 如何通过使用 Wav2Vec 转录音频来从任何视频创建 YouTube 字幕 | [Niklas Muennighoff](https://github.com/Muennighoff) | ![在 Colab 中打开](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) |
| - [使用 PyTorch Lightning 在 CIFAR-10 上微调 Vision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) | 如何使用 HuggingFace Transformers、Datasets 和 PyTorch Lightning 在 CIFAR-10 上微调 Vision Transformer（ViT） | [Niels Rogge](https://github.com/nielsrogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) |
| - [使用🤗 Trainer 在 CIFAR-10 上微调 Vision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) | 如何使用 HuggingFace Transformers、Datasets 和🤗 Trainer 在 CIFAR-10 上微调 Vision Transformer（ViT） | [Niels Rogge](https://github.com/nielsrogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) |
| - [在 Open Entity 上评估 LUKE，一个实体类型数据集](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) | 如何在 Open Entity 数据集上评估*LukeForEntityClassification* | [Ikuya Yamada](https://github.com/ikuyamada) | ![在 Colab 中打开](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) |
| - [在 TACRED 上评估 LUKE，一个关系抽取数据集](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) | 如何在 TACRED 数据集上评估*LukeForEntityPairClassification* | [Ikuya Yamada](https://github.com/ikuyamada) | ![在 Colab 中打开](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) |
| [在 CoNLL-2003 上评估 LUKE，一个重要的 NER 基准](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) | 如何在 CoNLL-2003 数据集上评估*LukeForEntitySpanClassification* | [Ikuya Yamada](https://github.com/ikuyamada) | ![在 Colab 中打开](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) |
| [在 PubMed 数据集上评估 BigBird-Pegasus](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) | 如何在 PubMed 数据集上评估*BigBirdPegasusForConditionalGeneration* | [Vasudev Gupta](https://github.com/vasudevgupta7) | ![在 Colab 中打开](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) |
| [使用 Wav2Vec2 进行语音情感分类](https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) | 如何利用预训练的 Wav2Vec2 模型对 MEGA 数据集进行情感分类 | [Mehrdad Farahani](https://github.com/m3hrdadfi) | ![在 Colab 中打开](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |
| [使用 DETR 在图像中检测对象](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) | 如何使用训练好的*DetrForObjectDetection*模型在图像中检测对象并可视化注意力 | [Niels Rogge](https://github.com/NielsRogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) |
| [在自定义对象检测数据集上微调 DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) | 如何在自定义对象检测数据集上微调*DetrForObjectDetection* | [Niels Rogge](https://github.com/NielsRogge) | ![在 Colab 中打开](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) |
| [为命名实体识别微调 T5](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb) | 如何在命名实体识别任务上微调*T5* | [Ogundepo Odunayo](https://github.com/ToluClassics) | ![在 Colab 中打开](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing) |
