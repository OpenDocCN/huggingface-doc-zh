# 分词器总结

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/tokenizer_summary`](https://huggingface.co/docs/transformers/v4.37.2/en/tokenizer_summary)

在这个页面上，我们将更仔细地看一下分词。

[`www.youtube-nocookie.com/embed/VFp38yj8h3A`](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)

正如我们在预处理教程中看到的，将文本分词是将其分割成单词或子词，然后通过查找表将其转换为 id。将单词或子词转换为 id 是直接的，因此在本摘要中，我们将专注于将文本分割成单词或子词（即文本分词）。更具体地说，我们将看一下🤗 Transformers 中使用的三种主要分词器类型：Byte-Pair Encoding (BPE)、WordPiece 和 SentencePiece，并展示哪种模型使用了哪种分词器类型的示例。

请注意，在每个模型页面上，您可以查看相关分词器的文档，以了解预训练模型使用了哪种分词器类型。例如，如果我们查看 BertTokenizer，我们可以看到该模型使用了 WordPiece。

## 介绍

将文本分割成较小的块是一项看起来比较困难的任务，有多种方法可以实现。例如，让我们看一下句子`"Don't you love 🤗 Transformers? We sure do."`

[`www.youtube-nocookie.com/embed/nhJxYji1aho`](https://www.youtube-nocookie.com/embed/nhJxYji1aho)

将这段文本简单地按空格分割，会得到：

```py
["Don't", "you", "love", "🤗", "Transformers?", "We", "sure", "do."]
```

这是一个明智的第一步，但是如果我们看一下标记`"Transformers?"`和`"do."`，我们会注意到标点符号附加在单词`"Transformer"`和`"do"`上，这是不够理想的。我们应该考虑标点符号，这样模型就不必学习一个单词的不同表示以及可能跟随它的每个可能的标点符号，这将导致模型需要学习的表示数量激增。考虑标点符号，对我们的示例文本进行分词会得到：

```py
["Don", "'", "t", "you", "love", "🤗", "Transformers", "?", "We", "sure", "do", "."]
```

更好。然而，分词处理单词`"Don't"`的方式是不利的。`"Don't"`代表`"do not"`，所以最好将其分词为`["Do", "n't"]`。这就是事情开始变得复杂的地方，也是每个模型都有自己的分词器类型的原因之一。根据我们应用于文本分词的规则，相同文本会生成不同的分词输出。预训练模型只有在输入与训练数据分词时使用的规则相同的情况下才能正常运行。

[spaCy](https://spacy.io/)和[Moses](http://www.statmt.org/moses/?n=Development.GetStarted)是两种流行的基于规则的分词器。在我们的示例上应用它们，*spaCy*和*Moses*可能会输出类似以下内容：

```py
["Do", "n't", "you", "love", "🤗", "Transformers", "?", "We", "sure", "do", "."]
```

可以看到，这里使用了空格和标点分词，以及基于规则的分词。空格和标点分词以及基于规则的分词都是单词分词的示例，它们被宽泛地定义为将句子分割成单词。虽然这是将文本分割成较小块的最直观的方法，但这种分词方法可能会导致大规模文本语料库出现问题。在这种情况下，空格和标点分词通常会生成一个非常庞大的词汇表（所有使用的唯一单词和标记的集合）。*例如*，Transformer XL 使用空格和标点分词，导致词汇量为 267,735！

如此庞大的词汇量迫使模型具有巨大的嵌入矩阵作为输入和输出层，这会导致内存和时间复杂度增加。一般来说，transformers 模型很少有词汇量超过 50,000 的情况，尤其是如果它们只在单一语言上进行了预训练。

因此，如果简单的空格和标点符号分词不尽如人意，为什么不简单地在字符上进行分词呢？

[`www.youtube-nocookie.com/embed/ssLq_EK2jLE`](https://www.youtube-nocookie.com/embed/ssLq_EK2jLE)

虽然字符分词非常简单且可以大大减少内存和时间复杂度，但这使得模型更难学习有意义的输入表示。例如，学习字母`"t"`的有意义的上下文无关表示要比学习单词`"today"`的上下文无关表示困难得多。因此，字符分词通常伴随着性能损失。因此，为了兼顾两者的优势，transformers 模型使用了介于单词级和字符级分词之间的混合称为**子词**分词的方法。

## 子词分词

[`www.youtube-nocookie.com/embed/zHvTiHr506c`](https://www.youtube-nocookie.com/embed/zHvTiHr506c)

子词分词算法依赖于这样一个原则，即经常使用的单词不应该被分割为更小的子词，但罕见的单词应该被分解为有意义的子词。例如，`"annoyingly"`可能被认为是一个罕见单词，可以被分解为`"annoying"`和`"ly"`。`"annoying"`和`"ly"`作为独立的子词会更频繁地出现，同时`"annoyingly"`的含义由`"annoying"`和`"ly"`的组合含义保留。这在像土耳其这样的聚合语言中特别有用，您可以通过串联子词形成（几乎）任意长的复杂单词。

子词分词使模型能够拥有合理的词汇量，同时能够学习有意义的上下文无关表示。此外，子词分词使模型能够处理它以前从未见过的单词，通过将它们分解为已知的子词。例如，BertTokenizer 将`"I have a new GPU!"`分词如下：

```py
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
>>> tokenizer.tokenize("I have a new GPU!")
["i", "have", "a", "new", "gp", "##u", "!"]
```

因为我们考虑的是不区分大小写的模型，所以首先将句子转换为小写。我们可以看到单词`["i", "have", "a", "new"]`存在于分词器的词汇表中，但单词`"gpu"`不在其中。因此，分词器将`"gpu"`分割为已知的子词：`["gp" 和 "##u"]`。`"##"`表示剩余的标记应该附加到前一个标记上，没有空格（用于解码或反向分词）。

作为另一个例子，XLNetTokenizer 将我们之前的示例文本分词如下：

```py
>>> from transformers import XLNetTokenizer

>>> tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
>>> tokenizer.tokenize("Don't you love 🤗 Transformers? We sure do.")
["▁Don", "'", "t", "▁you", "▁love", "▁", "🤗", "▁", "Transform", "ers", "?", "▁We", "▁sure", "▁do", "."]
```

当我们查看 SentencePiece 时，我们将回到那些`"▁"`的含义。正如大家所看到的，罕见单词`"Transformers"`已被分割为更常见的子词`"Transform"`和`"ers"`。

现在让我们看看不同的子词分词算法是如何工作的。请注意，所有这些分词算法都依赖于某种形式的训练，通常是在相应模型将被训练的语料库上进行的。

### 字节对编码（BPE）

字节对编码（BPE）是在[Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)中引入的。BPE 依赖于一个预分词器，将训练数据分割成单词。预分词可以简单到空格分词，例如 GPT-2，RoBERTa。更高级的预分词包括基于规则的分词，例如 XLM，FlauBERT 使用 Moses 用于大多数语言，或者 GPT 使用 Spacy 和 ftfy，来计算训练语料库中每个单词的频率。

在预分词之后，已创建了一组唯一的单词，并确定了每个单词在训练数据中出现的频率。接下来，BPE 创建一个基本词汇，其中包含所有出现在唯一单词集合中的符号，并学习合并规则，以从基本词汇的两个符号形成一个新符号。它会一直这样做，直到词汇表达到所需的词汇量。请注意，所需的词汇量是在训练分词器之前定义的一个超参数。

举个例子，假设在预分词之后，已确定了以下包含频率的单词集合：

```py
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

因此，基本词汇是`["b", "g", "h", "n", "p", "s", "u"]`。将所有单词分割为基本词汇的符号，我们得到：

```py
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

BPE 然后计算每对可能符号的频率，并选择出现最频繁的符号对。在上面的例子中，`"h"`后跟`"u"`出现了*10 + 5 = 15*次（在 10 次`"hug"`出现中的 10 次，以及在 5 次`"hugs"`出现中的 5 次）。然而，最频繁的符号对是`"u"`后跟`"g"`，总共出现了*10 + 5 + 5 = 20*次。因此，分词器学习的第一个合并规则是将所有跟在`"u"`符号后面的`"g"`符号组合在一起。接下来，`"ug"`被添加到词汇表中。然后单词集合变为

```py
("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

BPE 然后识别下一个最常见的符号对。它是`"u"`后跟`"n"`，出现了 16 次。`"u"`、`"n"`被合并为`"un"`并添加到词汇表中。下一个最频繁的符号对是`"h"`后跟`"ug"`，出现了 15 次。再次合并这对，并且`"hug"`可以被添加到词汇表中。

在这个阶段，词汇表是`["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]`，我们的唯一单词集合表示为

```py
("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

假设字节对编码训练在这一点停止，那么学习到的合并规则将被应用于新单词（只要这些新单词不包含基本词汇中没有的符号）。例如，单词`"bug"`将被分词为`["b", "ug"]`，但`"mug"`将被分词为`["<unk>", "ug"]`，因为符号`"m"`不在基本词汇中。通常情况下，像`"m"`这样的单个字母不会被`"<unk>"`符号替换，因为训练数据通常至少包含每个字母的一个出现，但对于非常特殊的字符，比如表情符号，可能会发生这种情况。

如前所述，词汇量，即基本词汇量+合并次数，是一个需要选择的超参数。例如 GPT 的词汇量为 40,478，因为它们有 478 个基本字符，并选择在 40,000 次合并后停止训练。

#### 字节级 BPE

如果将所有 Unicode 字符视为基本字符，那么包含所有可能基本字符的基本词汇可能会非常庞大。为了获得更好的基本词汇，[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 使用字节作为基本词汇，这是一个巧妙的技巧，可以强制基本词汇的大小为 256，同时确保每个基本字符都包含在词汇中。通过一些额外的规则来处理标点符号，GPT2 的分词器可以对每个文本进行分词，而无需使用<unk>符号。GPT-2 的词汇量为 50,257，对应于 256 个字节基本标记、一个特殊的文本结束标记和通过 50,000 次合并学习的符号。

### WordPiece

WordPiece 是用于 BERT、DistilBERT 和 Electra 的子词分词算法。该算法在[Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)中概述，与 BPE 非常相似。WordPiece 首先将词汇表初始化为包含训练数据中的每个字符，并逐渐学习一定数量的合并规则。与 BPE 不同，WordPiece 不选择最频繁的符号对，而是选择一旦添加到词汇表中就最大化训练数据的可能性的符号对。

那么这到底意味着什么呢？参考前面的例子，最大化训练数据的可能性等同于找到符号对，其概率除以其第一个符号后跟第二个符号的概率在所有符号对中最大。例如，`"u"`后跟`"g"`只有在`"ug"`的概率除以`"u"`、`"g"`的概率大于任何其他符号对时才会被合并。直觉上，WordPiece 与 BPE 略有不同，因为它评估合并两个符号会损失什么，以确保值得。

### Unigram

Unigram 是一种子词分词算法，由[Kudo, 2018](https://arxiv.org/pdf/1804.10959.pdf)引入。与 BPE 或 WordPiece 相比，Unigram 将其基本词汇初始化为大量符号，并逐渐修剪每个符号以获得较小的词汇表。基本词汇表可以对应于所有预分词的单词和最常见的子字符串。Unigram 不直接用于 transformers 中的任何模型，但与 SentencePiece 一起使用。

在每个训练步骤中，Unigram 算法根据当前词汇表和 unigram 语言模型定义了一个损失（通常定义为对数似然）。然后，对于词汇表中的每个符号，该算法计算如果将该符号从词汇表中移除会导致整体损失增加多少。Unigram 然后删除损失增加最低的 p（通常为 10%或 20%）百分比的符号，即那些对训练数据整体损失影响最小的符号。这个过程重复进行，直到词汇表达到所需大小。Unigram 算法始终保留基本字符，以便任何单词都可以被分词。

由于 Unigram 不基于合并规则（与 BPE 和 WordPiece 相反），该算法在训练后有几种分词新文本的方式。例如，如果经过训练的 Unigram 分词器展示以下词汇表：

```py
["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"],
```

`"hugs"`可以被分词为`["hug", "s"]`、`["h", "ug", "s"]`或`["h", "u", "g", "s"]`。那么应该选择哪一个？Unigram 在保存词汇的同时还保存了训练语料库中每个标记的概率，以便在训练后计算每种可能的分词的概率。该算法实际上只选择最有可能的分词，但也提供了根据它们的概率对可能的分词进行抽样的可能性。

这些概率由标记器训练时定义的损失来确定。假设训练数据由单词 x1​,…,xN​ 组成，并且对于单词 xi​ 的所有可能标记化集合定义为 S(xi​)，则总损失定义为 L=−i=1∑N​log​x∈S(xi​)∑​p(x)​。

### SentencePiece

到目前为止，所有描述的标记化算法都有同样的问题：假设输入文本使用空格来分隔单词。然而，并非所有语言都使用空格来分隔单词。一个可能的解决方案是使用特定语言的预分词器，例如 XLM 使用特定的中文、日文和泰文预分词器。为了更普遍地解决这个问题，[SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) 将输入视为原始输入流，因此包括空格在要使用的字符集中。然后使用 BPE 或 unigram 算法构建适当的词汇表。

XLNetTokenizer 例如使用了 SentencePiece，这也是为什么在前面的例子中包含了 `"▁"` 字符在词汇表中。使用 SentencePiece 进行解码非常容易，因为所有标记只需连接在一起，而 `"▁"` 被替换为一个空格。

库中所有使用 SentencePiece 的变压器模型都与 unigram 结合使用。使用 SentencePiece 的模型示例包括 ALBERT, XLNet, Marian 和 T5。
