# ALIGN

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/align`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/align)

## æ¦‚è¿°

ALIGN æ¨¡å‹åœ¨[é€šè¿‡å˜ˆæ‚æ–‡æœ¬ç›‘ç£æ‰©å±•è§†è§‰å’Œè§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ](https://arxiv.org/abs/2102.05918)ä¸­ç”± Chao Jiaã€Yinfei Yangã€Ye Xiaã€Yi-Ting Chenã€Zarana Parekhã€Hieu Phamã€Quoc V. Leã€Yunhsuan Sungã€Zhen Liã€Tom Duerig æå‡ºã€‚ALIGN æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è§†è§‰å’Œè¯­è¨€æ¨¡å‹ã€‚å®ƒå¯ç”¨äºå›¾åƒæ–‡æœ¬ç›¸ä¼¼åº¦å’Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚ALIGN å…·æœ‰åŒç¼–ç å™¨æ¶æ„ï¼Œå…¶ä¸­ EfficientNet ä½œä¸ºå…¶è§†è§‰ç¼–ç å™¨ï¼ŒBERT ä½œä¸ºå…¶æ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ å­¦ä¹ å¯¹é½è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚ä¸ä»¥å¾€çš„å·¥ä½œä¸åŒï¼ŒALIGN åˆ©ç”¨äº†ä¸€ä¸ªåºå¤§çš„å˜ˆæ‚æ•°æ®é›†ï¼Œå¹¶è¡¨æ˜è¯­æ–™åº“çš„è§„æ¨¡å¯ä»¥ç”¨æ¥å®ç°å…·æœ‰ç®€å•é…æ–¹çš„ SOTA è¡¨ç¤ºã€‚

è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*é¢„è®­ç»ƒè¡¨ç¤ºå¯¹è®¸å¤š NLP å’Œæ„ŸçŸ¥ä»»åŠ¡å˜å¾—è‡³å…³é‡è¦ã€‚è™½ç„¶ NLP ä¸­çš„è¡¨ç¤ºå­¦ä¹ å·²ç»è¿‡æ¸¡åˆ°åœ¨æ²¡æœ‰äººç±»æ³¨é‡Šçš„åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†è§†è§‰å’Œè§†è§‰è¯­è¨€è¡¨ç¤ºä»ç„¶ä¸¥é‡ä¾èµ–äºæ˜‚è´µæˆ–éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„ç­–åˆ’è®­ç»ƒæ•°æ®é›†ã€‚å¯¹äºè§†è§‰åº”ç”¨ï¼Œè¡¨ç¤ºä¸»è¦æ˜¯ä½¿ç”¨å…·æœ‰æ˜¾å¼ç±»æ ‡ç­¾çš„æ•°æ®é›†å­¦ä¹ ï¼Œå¦‚ ImageNet æˆ– OpenImagesã€‚å¯¹äºè§†è§‰è¯­è¨€ï¼Œåƒ Conceptual Captionsã€MSCOCO æˆ– CLIP è¿™æ ·çš„æµè¡Œæ•°æ®é›†éƒ½æ¶‰åŠåˆ°ä¸€ä¸ªä¸å®¹æ˜“çš„æ•°æ®æ”¶é›†ï¼ˆå’Œæ¸…ç†ï¼‰è¿‡ç¨‹ã€‚è¿™ç§æ˜‚è´µçš„ç­–åˆ’è¿‡ç¨‹é™åˆ¶äº†æ•°æ®é›†çš„è§„æ¨¡ï¼Œä»è€Œé˜»ç¢äº†è®­ç»ƒæ¨¡å‹çš„æ‰©å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¸€ä¸ªè¶…è¿‡åäº¿ä¸ªå›¾åƒæ›¿ä»£æ–‡æœ¬å¯¹çš„å˜ˆæ‚æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯åœ¨ Conceptual Captions æ•°æ®é›†ä¸­æ²¡æœ‰æ˜‚è´µçš„è¿‡æ»¤æˆ–åå¤„ç†æ­¥éª¤çš„æƒ…å†µä¸‹è·å¾—çš„ã€‚ä¸€ä¸ªç®€å•çš„åŒç¼–ç å™¨æ¶æ„å­¦ä¹ ä½¿ç”¨å¯¹æ¯”æŸå¤±å¯¹å›¾åƒå’Œæ–‡æœ¬å¯¹çš„è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬è¯­æ–™åº“çš„è§„æ¨¡å¯ä»¥å¼¥è¡¥å…¶å™ªå£°ï¼Œå¹¶å¯¼è‡´å³ä½¿ä½¿ç”¨å¦‚æ­¤ç®€å•çš„å­¦ä¹ æ–¹æ¡ˆä¹Ÿèƒ½å®ç°æœ€å…ˆè¿›çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„è§†è§‰è¡¨ç¤ºåœ¨è½¬ç§»åˆ° ImageNet å’Œ VTAB ç­‰åˆ†ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ã€‚å¯¹é½çš„è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºä½¿é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æˆä¸ºå¯èƒ½ï¼Œå¹¶åœ¨ Flickr30K å’Œ MSCOCO å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œå³ä½¿ä¸æ›´å¤æ‚çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å‹ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™äº›è¡¨ç¤ºè¿˜ä½¿å¾—å¯ä»¥è¿›è¡Œå…·æœ‰å¤æ‚æ–‡æœ¬å’Œæ–‡æœ¬+å›¾åƒæŸ¥è¯¢çš„è·¨æ¨¡æ€æœç´¢ã€‚*

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[Alara Dirik](https://huggingface.co/adirik)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç æœªå‘å¸ƒï¼Œè¿™ä¸ªå®ç°æ˜¯åŸºäº Kakao Brain æ ¹æ®åŸå§‹è®ºæ–‡å®ç°çš„ã€‚

## ç”¨æ³•ç¤ºä¾‹

ALIGN ä½¿ç”¨ EfficientNet è·å–è§†è§‰ç‰¹å¾ï¼Œä½¿ç”¨ BERT è·å–æ–‡æœ¬ç‰¹å¾ã€‚ç„¶åï¼Œæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾éƒ½æŠ•å½±åˆ°å…·æœ‰ç›¸åŒç»´åº¦çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚ç„¶åä½¿ç”¨æŠ•å½±å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼åº¦åˆ†æ•°ã€‚

AlignProcessor å°† EfficientNetImageProcessor å’Œ BertTokenizer å°è£…æˆä¸€ä¸ªå•ä¸€å®ä¾‹ï¼Œç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œç¼–ç å’Œé¢„å¤„ç†å›¾åƒã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ AlignProcessor å’Œ AlignModel è·å–å›¾åƒæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ•°ã€‚

```py
import requests
import torch
from PIL import Image
from transformers import AlignProcessor, AlignModel

processor = AlignProcessor.from_pretrained("kakaobrain/align-base")
model = AlignModel.from_pretrained("kakaobrain/align-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
candidate_labels = ["an image of a cat", "an image of a dog"]

inputs = processor(text=candidate_labels, images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

# this is the image-text similarity score
logits_per_image = outputs.logits_per_image

# we can take the softmax to get the label probabilities
probs = logits_per_image.softmax(dim=1)
print(probs)
```

## èµ„æº

ä¸€ç³»åˆ—å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ ALIGNã€‚

+   æœ‰å…³ [ALIGN å’Œ COYO-700M æ•°æ®é›†](https://huggingface.co/blog/vit-align) çš„åšå®¢æ–‡ç« ã€‚

+   ä¸€ä¸ªé›¶æ ·æœ¬å›¾åƒåˆ†ç±» [æ¼”ç¤º](https://huggingface.co/spaces/adirik/ALIGN-zero-shot-image-classification)ã€‚

+   `kakaobrain/align-base` æ¨¡å‹çš„ [æ¨¡å‹å¡ç‰‡](https://huggingface.co/kakaobrain/align-base)ã€‚

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªæ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ã€‚èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## AlignConfig

### `class transformers.AlignConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L298)

```py
( text_config = None vision_config = None projection_dim = 640 temperature_init_value = 1.0 initializer_range = 0.02 **kwargs )
```

å‚æ•°

+   `text_config` (`dict`, *optional*) â€” ç”¨äºåˆå§‹åŒ– AlignTextConfig çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚

+   `vision_config` (`dict`, *optional*) â€” ç”¨äºåˆå§‹åŒ– AlignVisionConfig çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚

+   `projection_dim` (`int`, *optional*, é»˜è®¤ä¸º 640) â€” æ–‡æœ¬å’Œè§†è§‰æŠ•å½±å±‚çš„ç»´åº¦ã€‚

+   `temperature_init_value` (`float`, *optional*, é»˜è®¤ä¸º 1.0) â€” *temperature* å‚æ•°çš„åˆå§‹å€¼ã€‚é»˜è®¤å€¼ä¸åŸå§‹ ALIGN å®ç°ç›¸åŒã€‚

+   `initializer_range` (`float`, *optional*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `kwargs` (*optional*) â€” å…³é”®å­—å‚æ•°å­—å…¸ã€‚

AlignConfig æ˜¯ç”¨äºå­˜å‚¨ AlignModel é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª ALIGN æ¨¡å‹ï¼Œå®šä¹‰æ–‡æœ¬æ¨¡å‹å’Œè§†è§‰æ¨¡å‹é…ç½®ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿä¸ ALIGN [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base) æ¶æ„ç±»ä¼¼çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AlignConfig, AlignModel

>>> # Initializing a AlignConfig with kakaobrain/align-base style configuration
>>> configuration = AlignConfig()

>>> # Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration
>>> model = AlignModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig
>>> from transformers import AlignTextConfig, AlignVisionConfig

>>> # Initializing ALIGN Text and Vision configurations
>>> config_text = AlignTextConfig()
>>> config_vision = AlignVisionConfig()

>>> config = AlignConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L374)

```py
( text_config: AlignTextConfig vision_config: AlignVisionConfig **kwargs ) â†’ export const metadata = 'undefined';AlignConfig
```

è¿”å›

AlignConfig

é…ç½®å¯¹è±¡çš„ä¸€ä¸ªå®ä¾‹

ä»å¯¹é½æ–‡æœ¬æ¨¡å‹é…ç½®å’Œå¯¹é½è§†è§‰æ¨¡å‹é…ç½®å®ä¾‹åŒ–ä¸€ä¸ª AlignConfigï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚

## AlignTextConfig

### `class transformers.AlignTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L35)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, é»˜è®¤ä¸º 30522) â€” Align æ–‡æœ¬æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ AlignTextModel æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, é»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`, *optional*, é»˜è®¤ä¸º 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str`æˆ–`Callable`, *optional*, é»˜è®¤ä¸º`"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ dropout æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout æ¯”ç‡ã€‚

+   `max_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º 512) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ 512ã€1024 æˆ– 2048ï¼‰ã€‚

+   `type_vocab_size` (`int`, *optional*, é»˜è®¤ä¸º 2) â€” åœ¨è°ƒç”¨ AlignTextModel æ—¶ä¼ é€’çš„`token_type_ids`çš„è¯æ±‡è¡¨å¤§å°ã€‚

+   `initializer_range` (`float`, *optional*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, é»˜è®¤ä¸º 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `pad_token_id` (`int`, *optional*, é»˜è®¤ä¸º 0) â€” å¡«å……æ ‡è®° idã€‚

+   `position_embedding_type` (`str`, *optional*, é»˜è®¤ä¸º`"absolute"`) â€” ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚é€‰æ‹©`"absolute"`ã€`"relative_key"`ã€`"relative_key_query"`ä¸­çš„ä¸€ä¸ªã€‚å¯¹äºä½ç½®åµŒå…¥ï¼Œè¯·ä½¿ç”¨`"absolute"`ã€‚æœ‰å…³`"relative_key"`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)ã€‚æœ‰å…³`"relative_key_query"`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)]ä¸­çš„*Method 4* (https://arxiv.org/abs/2009.13658)ã€‚

+   `use_cache` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨`config.is_decoder=True`æ—¶ç›¸å…³ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ AlignTextModel é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª ALIGN æ–‡æœ¬ç¼–ç å™¨ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿä¸ ALIGN [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base)æ¶æ„çš„æ–‡æœ¬ç¼–ç å™¨ç±»ä¼¼çš„é…ç½®ã€‚è¿™é‡Œçš„é»˜è®¤å€¼æ˜¯ä» BERT å¤åˆ¶çš„ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AlignTextConfig, AlignTextModel

>>> # Initializing a AlignTextConfig with kakaobrain/align-base style configuration
>>> configuration = AlignTextConfig()

>>> # Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration
>>> model = AlignTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## AlignVisionConfig

### `class transformers.AlignVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L159)

```py
( num_channels: int = 3 image_size: int = 600 width_coefficient: float = 2.0 depth_coefficient: float = 3.1 depth_divisor: int = 8 kernel_sizes: List = [3, 3, 5, 3, 5, 5, 3] in_channels: List = [32, 16, 24, 40, 80, 112, 192] out_channels: List = [16, 24, 40, 80, 112, 192, 320] depthwise_padding: List = [] strides: List = [1, 2, 2, 2, 1, 2, 1] num_block_repeats: List = [1, 2, 2, 3, 3, 4, 1] expand_ratios: List = [1, 6, 6, 6, 6, 6, 6] squeeze_expansion_ratio: float = 0.25 hidden_act: str = 'swish' hidden_dim: int = 2560 pooling_type: str = 'mean' initializer_range: float = 0.02 batch_norm_eps: float = 0.001 batch_norm_momentum: float = 0.99 drop_connect_rate: float = 0.2 **kwargs )
```

å‚æ•°

+   `num_channels` (`int`, *optional*, é»˜è®¤ä¸º 3) â€” è¾“å…¥é€šé“æ•°ã€‚

+   `image_size` (`int`, *optional*, é»˜è®¤ä¸º 600) â€” è¾“å…¥å›¾åƒå¤§å°ã€‚

+   `width_coefficient` (`float`, *optional*, é»˜è®¤ä¸º 2.0) â€” æ¯ä¸ªé˜¶æ®µç½‘ç»œå®½åº¦çš„ç¼©æ”¾ç³»æ•°ã€‚

+   `depth_coefficient` (`float`, *optional*, é»˜è®¤ä¸º 3.1) â€” æ¯ä¸ªé˜¶æ®µç½‘ç»œæ·±åº¦çš„ç¼©æ”¾ç³»æ•°ã€‚

+   `depth_divisor` `int`, *optional*, é»˜è®¤ä¸º 8) â€” ç½‘ç»œå®½åº¦çš„å•ä½ã€‚

+   `kernel_sizes` (`List[int]`, *optional*, é»˜è®¤ä¸º`[3, 3, 5, 3, 5, 5, 3]`) â€” æ¯ä¸ªå—ä¸­è¦ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°åˆ—è¡¨ã€‚

+   `in_channels` (`List[int]`, *optional*, é»˜è®¤ä¸º`[32, 16, 24, 40, 80, 112, 192]`) â€” ç”¨äºå·ç§¯å±‚ä¸­æ¯ä¸ªå—çš„è¾“å…¥é€šé“å¤§å°åˆ—è¡¨ã€‚

+   `out_channels` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[16, 24, 40, 80, 112, 192, 320]`) â€” ç”¨äºå·ç§¯å±‚ä¸­æ¯ä¸ªå—ä¸­ä½¿ç”¨çš„è¾“å‡ºé€šé“å¤§å°åˆ—è¡¨ã€‚

+   `depthwise_padding` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[]`) â€” å…·æœ‰æ–¹å½¢å¡«å……çš„å—ç´¢å¼•åˆ—è¡¨ã€‚

+   `strides` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[1, 2, 2, 2, 1, 2, 1]`) â€” ç”¨äºå·ç§¯å±‚ä¸­æ¯ä¸ªå—ä¸­ä½¿ç”¨çš„æ­¥å¹…å¤§å°åˆ—è¡¨ã€‚

+   `num_block_repeats` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[1, 2, 2, 3, 3, 4, 1]`) â€” æ¯ä¸ªå—é‡å¤çš„æ¬¡æ•°åˆ—è¡¨ã€‚

+   `expand_ratios` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[1, 6, 6, 6, 6, 6, 6]`) â€” æ¯ä¸ªå—çš„ç¼©æ”¾ç³»æ•°åˆ—è¡¨ã€‚

+   `squeeze_expansion_ratio` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.25) â€” æŒ¤å‹æ‰©å±•æ¯”ç‡ã€‚

+   `hidden_act` (`str`æˆ–`function`, *å¯é€‰*, é»˜è®¤ä¸º`"silu"`) â€” æ¯ä¸ªå—ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`, `"relu"`, `"selu"`, "gelu_new", "silu"å’Œ"mish"ã€‚

+   `hiddem_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1280) â€” åˆ†ç±»å¤´ä¹‹å‰å±‚çš„éšè—ç»´åº¦ã€‚

+   `pooling_type` (`str`æˆ–`function`, *å¯é€‰*, é»˜è®¤ä¸º`"mean"`) â€” åº”ç”¨äºå¯†é›†åˆ†ç±»å¤´ä¹‹å‰çš„æœ€ç»ˆæ± åŒ–ç±»å‹ã€‚å¯ç”¨é€‰é¡¹ä¸º[`"mean"`, `"max"`]

+   `initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `batch_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1e-3) â€” æ‰¹é‡å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `batch_norm_momentum` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.99) â€” æ‰¹é‡å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„åŠ¨é‡ã€‚

+   `drop_connect_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.2) â€” è·³è¿‡è¿æ¥çš„ä¸¢å¼ƒç‡ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ AlignVisionModel çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª ALIGN è§†è§‰ç¼–ç å™¨ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿä¸ ALIGN [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base)æ¶æ„çš„è§†è§‰ç¼–ç å™¨ç±»ä¼¼çš„é…ç½®ã€‚é»˜è®¤å€¼æ¥è‡ª EfficientNet (efficientnet-b7)

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AlignVisionConfig, AlignVisionModel

>>> # Initializing a AlignVisionConfig with kakaobrain/align-base style configuration
>>> configuration = AlignVisionConfig()

>>> # Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration
>>> model = AlignVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## AlignProcessor

### `class transformers.AlignProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/processing_align.py#L24)

```py
( image_processor tokenizer )
```

å‚æ•°

+   `image_processor` (EfficientNetImageProcessor) â€” å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer` ([`BertTokenizer`, `BertTokenizerFast`]) â€” åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ª ALIGN å¤„ç†å™¨ï¼Œå°† EfficientNetImageProcessor å’Œ BertTokenizer/BertTokenizerFast åŒ…è£…æˆä¸€ä¸ªåŒæ—¶ç»§æ‰¿å›¾åƒå¤„ç†å™¨å’Œåˆ†è¯å™¨åŠŸèƒ½çš„å•ä¸ªå¤„ç†å™¨ã€‚æŸ¥çœ‹`__call__()`å’Œ decode()ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/processing_align.py#L104)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ BertTokenizerFast çš„ batch_decode()ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/processing_align.py#L111)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ BertTokenizerFast çš„ decode()ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²è·å–æ›´å¤šä¿¡æ¯ã€‚

## AlignModel

### `class transformers.AlignModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1411)

```py
( config: AlignConfig )
```

å‚æ•°

+   `config` (AlignConfig) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1543)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    ç´¢å¼•å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å¾—ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ attention_mask (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*): é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 å¯¹åº”äº`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0 å¯¹åº”äº`è¢«æ©ç `çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ position_ids (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*): æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ token_type_ids (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`({0})`ï¼Œ*å¯é€‰*): æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ head_mask (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*): ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚

    inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*): å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°† `input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`): åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… EfficientNetImageProcessor.`call`()ã€‚return_loss (`bool`, *optional*): æ˜¯å¦è¿”å›å¯¹æ¯”æŸå¤±ã€‚output_attentions (`bool`, *optional*): æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `attentions`ã€‚output_hidden_states (`bool`, *optional*): æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `hidden_states`ã€‚return_dict (`bool`, *optional*): æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

AlignModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1445)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºæœªè¢«å±è”½çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«å±è”½çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`({0}, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

text_features (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, output_dim`)

é€šè¿‡å°†æŠ•å½±å±‚åº”ç”¨äº AlignTextModel çš„æ±‡èšè¾“å‡ºè·å¾—çš„æ–‡æœ¬åµŒå…¥ã€‚

AlignModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, AlignModel

>>> model = AlignModel.from_pretrained("kakaobrain/align-base")
>>> tokenizer = AutoTokenizer.from_pretrained("kakaobrain/align-base")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1498)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¦‚æœæä¾›ï¼Œå¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… EfficientNetImageProcessor.`call`()ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

image_features (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, output_dim)`)

é€šè¿‡å°†æŠ•å½±å±‚åº”ç”¨äº AlignVisionModel çš„æ±‡èšè¾“å‡ºè·å¾—çš„å›¾åƒåµŒå…¥ã€‚

AlignModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AlignModel

>>> model = AlignModel.from_pretrained("kakaobrain/align-base")
>>> processor = AutoProcessor.from_pretrained("kakaobrain/align-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> image_features = model.get_image_features(**inputs)
```

## AlignTextModel

### `class transformers.AlignTextModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1196)

```py
( config: AlignTextConfig add_pooling_layer: bool = True )
```

å‚æ•°

+   `config` (AlignConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

æ¥è‡ª ALIGN çš„æ–‡æœ¬æ¨¡å‹ï¼Œæ²¡æœ‰é¡¶éƒ¨çš„å¤´éƒ¨æˆ–æŠ•å½±ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1221)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›ï¼Œå°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer æ¥è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºæœªè¢«æ©ç›–çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«æ©ç›–çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `token_type_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`({0})`ï¼Œ*å¯é€‰*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`({0}, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.align.configuration_align.AlignTextConfig'>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰- æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰- ç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼Œåºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œå¯¹äº BERT ç³»åˆ—æ¨¡å‹ï¼Œè¿™å°†è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œ tanh æ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯ä»é¢„è®­ç»ƒæœŸé—´çš„ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`å’Œ`config.add_cross_attention=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–å½“`config.use_cache=True`æ—¶è¿”å›ï¼‰- é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œå¦‚æœ`config.is_encoder_decoder=True`è¿˜æœ‰ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ä»¥åŠå¯é€‰åœ°åœ¨äº¤å‰æ³¨æ„åŠ›å—ä¸­ä½¿ç”¨`config.is_encoder_decoder=True`ï¼‰å¯ç”¨ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ä»¥åŠ é€Ÿé¡ºåºè§£ç ã€‚

AlignTextModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, AlignTextModel

>>> model = AlignTextModel.from_pretrained("kakaobrain/align-base")
>>> tokenizer = AutoTokenizer.from_pretrained("kakaobrain/align-base")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## AlignVisionModel

### `class transformers.AlignVisionModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1322)

```py
( config: AlignVisionConfig )
```

å‚æ•°

+   `config`ï¼ˆAlignConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ALIGN ä¸­çš„è§†è§‰æ¨¡å‹ï¼Œæ²¡æœ‰ä»»ä½•å¤´éƒ¨æˆ–é¡¶éƒ¨çš„æŠ•å½±ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1351)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ï¼Œå¦‚æœæ‚¨æä¾›å¡«å……ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… EfficientNetImageProcessor.`call`()ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ`<class 'transformers.models.align.configuration_align.AlignVisionConfig'>`ï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`ï¼‰â€” åœ¨ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œæ± åŒ–æ“ä½œåçš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºçš„ä¸€ä¸ª+æ¯ä¸€å±‚çš„è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

AlignVisionModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AlignVisionModel

>>> model = AlignVisionModel.from_pretrained("kakaobrain/align-base")
>>> processor = AutoProcessor.from_pretrained("kakaobrain/align-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```
