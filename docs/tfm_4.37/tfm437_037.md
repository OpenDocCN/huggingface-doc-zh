# å›¾åƒåˆ°å›¾åƒä»»åŠ¡æŒ‡å—

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/tasks/image_to_image`](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/image_to_image)

å›¾åƒåˆ°å›¾åƒä»»åŠ¡æ˜¯ä¸€ä¸ªåº”ç”¨ç¨‹åºæ¥æ”¶å›¾åƒå¹¶è¾“å‡ºå¦ä¸€å¹…å›¾åƒçš„ä»»åŠ¡ã€‚è¿™åŒ…æ‹¬å„ç§å­ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒå¢å¼ºï¼ˆè¶…åˆ†è¾¨ç‡ã€ä½å…‰å¢å¼ºã€å»é›¨ç­‰ï¼‰ã€å›¾åƒä¿®è¡¥ç­‰ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

+   ä½¿ç”¨å›¾åƒåˆ°å›¾åƒç®¡é“è¿›è¡Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ï¼Œ

+   è¿è¡Œç›¸åŒä»»åŠ¡çš„å›¾åƒåˆ°å›¾åƒæ¨¡å‹ï¼Œè€Œä¸ä½¿ç”¨ç®¡é“ã€‚

è¯·æ³¨æ„ï¼Œæˆªè‡³æœ¬æŒ‡å—å‘å¸ƒæ—¶ï¼Œ`å›¾åƒåˆ°å›¾åƒ`ç®¡é“ä»…æ”¯æŒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ã€‚

è®©æˆ‘ä»¬å¼€å§‹å®‰è£…å¿…è¦çš„åº“ã€‚

```py
pip install transformers
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[Swin2SR æ¨¡å‹](https://huggingface.co/caidas/swin2SR-lightweight-x2-64)åˆå§‹åŒ–ç®¡é“ã€‚ç„¶åï¼Œé€šè¿‡è°ƒç”¨å›¾åƒæ¥æ¨æ–­ç®¡é“ã€‚ç›®å‰ï¼Œæ­¤ç®¡é“ä»…æ”¯æŒ[Swin2SR æ¨¡å‹](https://huggingface.co/models?sort=trending&search=swin2sr)ã€‚

```py
from transformers import pipeline

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
pipe = pipeline(task="image-to-image", model="caidas/swin2SR-lightweight-x2-64", device=device)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬åŠ è½½ä¸€å¼ å›¾åƒã€‚

```py
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg"
image = Image.open(requests.get(url, stream=True).raw)

print(image.size)
```

```py
# (532, 432)
```

![ä¸€åªçŒ«çš„ç…§ç‰‡](img/5579617dcca3856fe157c96460c7ccc9.png)

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®¡é“è¿›è¡Œæ¨æ–­ã€‚æˆ‘ä»¬å°†å¾—åˆ°ä¸€å¼ çŒ«å›¾åƒçš„æ”¾å¤§ç‰ˆæœ¬ã€‚

```py
upscaled = pipe(image)
print(upscaled.size)
```

```py
# (1072, 880)
```

å¦‚æœæ‚¨å¸Œæœ›è‡ªå·±è¿›è¡Œæ¨æ–­è€Œä¸ä½¿ç”¨ç®¡é“ï¼Œå¯ä»¥ä½¿ç”¨ transformers çš„`Swin2SRForImageSuperResolution`å’Œ`Swin2SRImageProcessor`ç±»ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚è®©æˆ‘ä»¬åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨ã€‚

```py
from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor 

model = Swin2SRForImageSuperResolution.from_pretrained("caidas/swin2SR-lightweight-x2-64").to(device)
processor = Swin2SRImageProcessor("caidas/swin2SR-lightweight-x2-64")
```

`pipeline`æŠ½è±¡äº†æˆ‘ä»¬å¿…é¡»è‡ªå·±å®Œæˆçš„é¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œå› æ­¤è®©æˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ã€‚æˆ‘ä»¬å°†å›¾åƒä¼ é€’ç»™å¤„ç†å™¨ï¼Œç„¶åå°†åƒç´ å€¼ç§»åŠ¨åˆ° GPUã€‚

```py
pixel_values = processor(image, return_tensors="pt").pixel_values
print(pixel_values.shape)

pixel_values = pixel_values.to(device)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†åƒç´ å€¼ä¼ é€’ç»™æ¨¡å‹æ¥æ¨æ–­å›¾åƒã€‚

```py
import torch

with torch.no_grad():
  outputs = model(pixel_values)
```

è¾“å‡ºæ˜¯ä¸€ä¸ªç±»å‹ä¸º`ImageSuperResolutionOutput`çš„å¯¹è±¡ï¼Œçœ‹èµ·æ¥åƒä¸‹é¢è¿™æ ·ğŸ‘‡

```py
(loss=None, reconstruction=tensor([[[[0.8270, 0.8269, 0.8275,  ..., 0.7463, 0.7446, 0.7453],
          [0.8287, 0.8278, 0.8283,  ..., 0.7451, 0.7448, 0.7457],
          [0.8280, 0.8273, 0.8269,  ..., 0.7447, 0.7446, 0.7452],
          ...,
          [0.5923, 0.5933, 0.5924,  ..., 0.0697, 0.0695, 0.0706],
          [0.5926, 0.5932, 0.5926,  ..., 0.0673, 0.0687, 0.0705],
          [0.5927, 0.5914, 0.5922,  ..., 0.0664, 0.0694, 0.0718]]]],
       device='cuda:0'), hidden_states=None, attentions=None)
```

æˆ‘ä»¬éœ€è¦è·å–`reconstruction`å¹¶å¯¹å…¶è¿›è¡Œåå¤„ç†ä»¥è¿›è¡Œå¯è§†åŒ–ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚

```py
outputs.reconstruction.data.shape
# torch.Size([1, 3, 880, 1072])
```

æˆ‘ä»¬éœ€è¦æŒ¤å‹è¾“å‡ºå¹¶å»æ‰è½´ 0ï¼Œè£å‰ªå€¼ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸º numpy æµ®ç‚¹æ•°ã€‚ç„¶åæˆ‘ä»¬å°†æ’åˆ—è½´ä»¥è·å¾—å½¢çŠ¶[1072, 880]ï¼Œæœ€åå°†è¾“å‡ºå¸¦å›èŒƒå›´[0, 255]ã€‚

```py
import numpy as np

# squeeze, take to CPU and clip the values
output = outputs.reconstruction.data.squeeze().cpu().clamp_(0, 1).numpy()
# rearrange the axes
output = np.moveaxis(output, source=0, destination=-1)
# bring values back to pixel values range
output = (output * 255.0).round().astype(np.uint8)
Image.fromarray(output)
```

![ä¸€åªçŒ«çš„æ”¾å¤§ç…§ç‰‡](img/bddb9797cb61a73629120e3f5fb0035f.png)
