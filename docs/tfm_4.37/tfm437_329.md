# XLS-R

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/xls_r`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xls_r)

## 概述

XLS-R 模型由 Arun Babu、Changhan Wang、Andros Tjandra、Kushal Lakhotia、Qiantong Xu、Naman Goyal、Kritika Singh、Patrick von Platen、Yatharth Saraf、Juan Pino、Alexei Baevski、Alexis Conneau、Michael Auli 在[XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296)中提出。

论文的摘要如下：

*本文介绍了 XLS-R，这是一个基于 wav2vec 2.0 的大规模跨语言语音表示学习模型。我们在 128 种语言中使用多达 20 亿参数的模型进行训练，使用近 50 万小时的公开可用语音音频数据，比已知最大的先前工作的公共数据量大一个数量级。我们的评估涵盖了各种任务、领域、数据制度和语言，包括高资源和低资源语言。在 CoVoST-2 语音翻译基准测试中，我们将先前的最新技术平均提高了 7.4 BLEU，涵盖了 21 个翻译方向到英语。对于语音识别，XLS-R 在 BABEL、MLS、CommonVoice 以及 VoxPopuli 等最佳已知先前工作上的错误率平均降低了 14-34%。XLS-R 还在 VoxLingua107 语言识别上树立了新的技术水平。此外，我们展示了在足够大的模型尺寸下，跨语言预训练可以在将英语语音翻译成其他语言时胜过仅英语预训练，这种情况有利于单语预训练。我们希望 XLS-R 可以帮助改进世界上更多语言的语音处理任务。*

相关的检查点可以在[`huggingface.co/models?other=xls_r`](https://huggingface.co/models?other=xls_r)下找到。

原始代码可以在[这里](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec)找到。

## 使用提示

+   XLS-R 是一个语音模型，接受与语音信号的原始波形对应的浮点数组。

+   XLS-R 模型是使用连接主义时间分类（CTC）进行训练的，因此模型输出必须使用 Wav2Vec2CTCTokenizer 进行解码。

XLS-R 的架构基于 Wav2Vec2 模型，请参考 Wav2Vec2 的文档页面获取 API 参考。
