# VITS

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits)

## 概述

VITS 模型是由 Jaehyeon Kim，Jungil Kong，Juhee Son 在[端到端文本到语音的条件变分自动编码器与对抗学习](https://arxiv.org/abs/2106.06103)中提出的。

VITS（端到端文本到语音的变分推断与对抗学习）是一种端到端语音合成模型，根据输入文本序列预测语音波形。它是由后验编码器、解码器和条件先验组成的条件变分自动编码器（VAE）。

基于流的模块预测了一组基于频谱图的声学特征，该模块由基于 Transformer 的文本编码器和多个耦合层组成。使用一堆转置卷积层对频谱图进行解码，这与 HiFi-GAN 声码器的风格非常相似。受 TTS 问题的一对多性质的启发，其中相同的文本输入可以以多种方式发音，该模型还包括一个随机持续时间预测器，允许模型从相同的输入文本中合成具有不同节奏的语音。

该模型通过从变分下界和对抗训练导出的损失的组合进行端到端训练。为了提高模型的表现力，将归一化流应用于条件先验分布。在推断过程中，基于持续时间预测模块对文本编码进行上采样，然后使用一系列流模块和 HiFi-GAN 解码器将其映射到波形中。由于持续时间预测器的随机性质，该模型是非确定性的，因此需要一个固定的种子来生成相同的语音波形。

论文的摘要如下：

*最近提出了几种端到端文本到语音（TTS）模型，实现了单阶段训练和并行采样，但它们的样本质量不及两阶段 TTS 系统。在这项工作中，我们提出了一种并行端到端 TTS 方法，其生成的音频听起来比当前的两阶段模型更自然。我们的方法采用了变分推断，增加了归一化流和对抗训练过程，提高了生成建模的表现力。我们还提出了一个随机持续时间预测器，用于从输入文本中合成具有不同节奏的语音。通过对潜在变量进行不确定性建模和随机持续时间预测器，我们的方法表达了自然的一对多关系，即文本输入可以以不同的音高和节奏发音。对 LJ Speech（单个说话者数据集）的主观人类评估（平均意见分数，或 MOS）显示，我们的方法优于最佳公开可用的 TTS 系统，并实现了与地面真相相当的 MOS。*

这个模型也可以与[大规模多语言语音（MMS）](https://arxiv.org/abs/2305.13516)的 TTS 检查点一起使用，因为这些检查点使用相同的架构和稍作修改的分词器。

这个模型是由[Matthijs](https://huggingface.co/Matthijs)和[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)贡献的。原始代码可以在[这里](https://github.com/jaywalnut310/vits)找到。

## 用法示例

VITS 和 MMS-TTS 检查点都可以使用相同的 API。由于基于流的模型是非确定性的，最好设置一个种子以确保输出的可重现性。对于使用罗马字母表的语言，如英语或法语，可以直接使用分词器对文本输入进行预处理。以下代码示例运行了一个前向传递，使用了 MMS-TTS 英语检查点：

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
model = VitsModel.from_pretrained("facebook/mms-tts-eng")

inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

set_seed(555)  # make deterministic

with torch.no_grad():
   outputs = model(**inputs)

waveform = outputs.waveform[0]
```

生成的波形可以保存为`.wav`文件：

```py
import scipy

scipy.io.wavfile.write("techno.wav", rate=model.config.sampling_rate, data=waveform)
```

或在 Jupyter Notebook / Google Colab 中显示：

```py
from IPython.display import Audio

Audio(waveform, rate=model.config.sampling_rate)
```

对于某些具有非罗马字母表的语言，如阿拉伯语、普通话或印地语，需要使用[`uroman`](https://github.com/isi-nlp/uroman) perl 包对文本输入进行预处理，转换为罗马字母表。

您可以通过检查预训练的`tokenizer`的`is_uroman`属性来确定您的语言是否需要`uroman`包。

```py
from transformers import VitsTokenizer

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
print(tokenizer.is_uroman)
```

如果需要，应在将文本输入传递给`VitsTokenizer`之前**先**对其应用 uroman 包，因为目前分词器不支持执行预处理。

要执行此操作，首先将 uroman 存储库克隆到本地计算机，并将 bash 变量`UROMAN`设置为本地路径：

```py
git clone https://github.com/isi-nlp/uroman.git
cd uroman
export UROMAN=$(pwd)
```

然后，您可以使用以下代码片段对文本输入进行预处理。您可以依赖使用 bash 变量`UROMAN`指向 uroman 存储库，或者将 uroman 目录作为参数传递给`uromaize`函数：

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed
import os
import subprocess

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-kor")
model = VitsModel.from_pretrained("facebook/mms-tts-kor")

def uromanize(input_string, uroman_path):
    """Convert non-Roman strings to Roman using the `uroman` perl package."""
    script_path = os.path.join(uroman_path, "bin", "uroman.pl")

    command = ["perl", script_path]

    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    # Execute the perl command
    stdout, stderr = process.communicate(input=input_string.encode())

    if process.returncode != 0:
        raise ValueError(f"Error {process.returncode}: {stderr.decode()}")

    # Return the output as a string and skip the new-line character at the end
    return stdout.decode()[:-1]

text = "이봐 무슨 일이야"
uromaized_text = uromanize(text, uroman_path=os.environ["UROMAN"])

inputs = tokenizer(text=uromaized_text, return_tensors="pt")

set_seed(555)  # make deterministic
with torch.no_grad():
   outputs = model(inputs["input_ids"])

waveform = outputs.waveform[0]
```

## VitsConfig

### `class transformers.VitsConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/configuration_vits.py#L29)

```py
( vocab_size = 38 hidden_size = 192 num_hidden_layers = 6 num_attention_heads = 2 window_size = 4 use_bias = True ffn_dim = 768 layerdrop = 0.1 ffn_kernel_size = 3 flow_size = 192 spectrogram_bins = 513 hidden_act = 'relu' hidden_dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 use_stochastic_duration_prediction = True num_speakers = 1 speaker_embedding_size = 0 upsample_initial_channel = 512 upsample_rates = [8, 8, 2, 2] upsample_kernel_sizes = [16, 16, 4, 4] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] leaky_relu_slope = 0.1 depth_separable_channels = 2 depth_separable_num_layers = 3 duration_predictor_flow_bins = 10 duration_predictor_tail_bound = 5.0 duration_predictor_kernel_size = 3 duration_predictor_dropout = 0.5 duration_predictor_num_flows = 4 duration_predictor_filter_channels = 256 prior_encoder_num_flows = 4 prior_encoder_num_wavenet_layers = 4 posterior_encoder_num_wavenet_layers = 16 wavenet_kernel_size = 5 wavenet_dilation_rate = 1 wavenet_dropout = 0.0 speaking_rate = 1.0 noise_scale = 0.667 noise_scale_duration = 0.8 sampling_rate = 16000 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 38) — VITS 模型的词汇量。定义了可以由传递给 VitsModel 的`inputs_ids`表示的不同令牌数量。

+   `hidden_size` (`int`, *optional*, defaults to 192) — 文本编码器层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 6) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 2) — Transformer 编码器中每个注意力层的注意力头数。

+   `window_size` (`int`, *optional*, defaults to 4) — Transformer 编码器中注意力层的相对位置嵌入的窗口大小。

+   `use_bias` (`bool`, *optional*, defaults to `True`) — 是否在 Transformer 编码器中的键、查询、值投影层中使用偏置。

+   `ffn_dim` (`int`, *optional*, defaults to 768) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `layerdrop` (`float`, *optional*, defaults to 0.1) — 编码器的 LayerDrop 概率。有关更多详细信息，请参阅 LayerDrop paper)。

+   `ffn_kernel_size` (`int`, *optional*, defaults to 3) — Transformer 编码器中前馈网络使用的 1D 卷积层的核大小。

+   `flow_size` (`int`, *optional*, defaults to 192) — 流层的维度。

+   `spectrogram_bins` (`int`, *optional*, defaults to 513) — 目标频谱图中的频率箱数。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"relu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层和编码器中所有全连接层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的 dropout 比率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.1) — 全连接层内激活的 dropout 比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的 epsilon。

+   `use_stochastic_duration_prediction` (`bool`, *optional*, defaults to `True`) — 是否使用随机持续时间预测模块或常规持续时间预测器。

+   `num_speakers` (`int`, *optional*, defaults to 1) — 如果这是多说话者模型，则说话者数量。

+   `speaker_embedding_size` (`int`, *optional*, 默认为 0) — 说话者嵌入使用的通道数量。对于单说话者模型，该值为零。

+   `upsample_initial_channel` (`int`, *optional*, 默认为 512) — HiFi-GAN 上采样网络的输入通道数。

+   `upsample_rates` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[8, 8, 2, 2]`) — 一个整数元组，定义 HiFi-GAN 上采样网络中每个 1D 卷积层的步幅。`upsample_rates`的长度定义了卷积层的数量，并且必须与`upsample_kernel_sizes`的长度匹配。

+   `upsample_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[16, 16, 4, 4]`) — 一个整数元组，定义 HiFi-GAN 上采样网络中每个 1D 卷积层的核大小。`upsample_kernel_sizes`的长度定义了卷积层的数量，并且必须与`upsample_rates`的长度匹配。

+   `resblock_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[3, 7, 11]`) — 一个整数元组，定义 HiFi-GAN 多接受域融合（MRF）模块中 1D 卷积层的核大小。

+   `resblock_dilation_sizes` (`Tuple[Tuple[int]]` 或 `List[List[int]]`, *optional*, 默认为`[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — 一个嵌套的整数元组，定义 HiFi-GAN 多接受域融合（MRF）模块中扩张 1D 卷积层的扩张率。

+   `leaky_relu_slope` (`float`, *optional*, 默认为 0.1) — leaky ReLU 激活中使用的负斜率的角度。

+   `depth_separable_channels` (`int`, *optional*, 默认为 2) — 每个深度可分离块中要使用的通道数。

+   `depth_separable_num_layers` (`int`, *optional*, 默认为 3) — 每个深度可分离块中要使用的卷积层数量。

+   `duration_predictor_flow_bins` (`int`, *optional*, 默认为 10) — 在持续时间预测模型中使用无约束有理样条映射的通道数量。

+   `duration_predictor_tail_bound` (`float`, *optional*, 默认为 5.0) — 计算持续时间预测模型中无约束有理样条时的尾部区间边界值。

+   `duration_predictor_kernel_size` (`int`, *optional*, 默认为 3) — 用于持续时间预测模型中的 1D 卷积层的核大小。

+   `duration_predictor_dropout` (`float`, *optional*, 默认为 0.5) — 持续时间预测模型的丢失比率。

+   `duration_predictor_num_flows` (`int`, *optional*, 默认为 4) — 持续时间预测模型使用的流阶段数量。

+   `duration_predictor_filter_channels` (`int`, *optional*, 默认为 256) — 持续时间预测模型中使用的卷积层的通道数。

+   `prior_encoder_num_flows` (`int`, *optional*, 默认为 4) — 先验编码器流模型使用的流阶段数量。

+   `prior_encoder_num_wavenet_layers` (`int`, *optional*, 默认为 4) — 先验编码器流模型使用的 WaveNet 层数量。

+   `posterior_encoder_num_wavenet_layers` (`int`, *optional*, 默认为 16) — 后验编码器模型使用的 WaveNet 层数量。

+   `wavenet_kernel_size` (`int`, *optional*, 默认为 5) — WaveNet 模型中使用的 1D 卷积层的核大小。

+   `wavenet_dilation_rate` (`int`, *optional*, 默认为 1) — WaveNet 模型中使用的扩张 1D 卷积层的扩张率。

+   `wavenet_dropout` (`float`, *optional*, 默认为 0.0) — WaveNet 层的丢失比率。

+   `speaking_rate` (`float`, *optional*, 默认为 1.0) — 说话速率。较大的值会导致合成语音速度更快。

+   `noise_scale` (`float`, *optional*, 默认为 0.667) — 语音预测的随机程度。较大的值会导致预测语音的变化更大。

+   `noise_scale_duration` (`float`, *optional*, 默认为 0.8) — 持续时间预测的随机程度。较大的值会导致预测持续时间的变化更大。

+   `sampling_rate` (`int`, *optional*, 默认为 16000) — 输出音频波形的数字化采样率，以赫兹（Hz）表示。

这是用于存储 VitsModel 配置的配置类。它用于根据指定的参数实例化 VITS 模型，定义模型架构。使用默认值实例化配置将产生类似于[VITS](https://huggingface.co/facebook/mms-tts-eng)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例:

```py
>>> from transformers import VitsModel, VitsConfig

>>> # Initializing a "facebook/mms-tts-eng" style configuration
>>> configuration = VitsConfig()

>>> # Initializing a model (with random weights) from the "facebook/mms-tts-eng" style configuration
>>> model = VitsModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## VitsTokenizer

### `class transformers.VitsTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L57)

```py
( vocab_file pad_token = '<pad>' unk_token = '<unk>' language = None add_blank = True normalize = True phonemize = True is_uroman = False **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇表文件的路径。

+   `language` (`str`, *optional*) — 语言标识符。

+   `add_blank` (`bool`, *optional*, 默认为`True`) — 是否在其他标记之间插入标记 id 0。

+   `normalize` (`bool`, *optional*, 默认为`True`) — 是否通过删除所有大小写和标点来规范化输入文本。

+   `phonemize` (`bool`, *optional*, 默认为`True`) — 是否将输入文本转换为音素。

+   `is_uroman` (`bool`, *optional*, 默认为`False`) — 是否在分词之前需要将`uroman`罗马化器应用于输入文本。

构建一个 VITS 分词器。还支持 MMS-TTS。

此分词器继承自 PreTrainedTokenizer，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `add_special_tokens` (`bool`, *optional*, 默认为`True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding`（`bool`，`str`或 PaddingStrategy，*可选*，默认为`False`）— 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。

    +   `'max_length'`: 填充到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则填充到模型可接受的最大输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：不进行填充（即，可以输出具有不同长度序列的批次）。

+   `truncation`（`bool`，`str`或 TruncationStrategy，*可选*，默认为`False`）— 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 仅截断到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则将逐个标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`: 仅截断到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则仅会截断第一序列。

    +   `'only_second'`: 仅截断到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则仅会截断第二序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。

+   `max_length`（`int`，*可选*）— 控制截断/填充参数使用的最大长度。

    如果未设置或设置为`None`，则将使用预定义的模型最大长度，如果截断/填充参数中的一个需要最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为 0）— 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。

+   `is_split_into_words`（`bool`，*可选*，默认为`False`）— 输入是否已经预分词（例如，已分割为单词）。如果设置为`True`，则分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行分词。这对于 NER 或标记分类很有用。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于启用具有计算能力`>= 7.5`（Volta）的 NVIDIA 硬件上的 Tensor Cores 特别有用。

+   `return_tensors`（`str`或 TensorType，*可选*）— 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回 Numpy `np.ndarray`对象。

+   `return_token_type_ids`（`bool`，*可选*）— 是否返回标记类型 ID。如果保持默认设置，将根据特定分词器的默认设置返回标记类型 ID，由`return_outputs`属性定义。

    什么是标记类型 ID？

+   `return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定标记化器的默认值返回注意力掩码，由 `return_outputs` 属性定义。

    什么是注意力掩码？

+   `return_overflowing_tokens` (`bool`, *可选*, 默认为 `False`) — 是否返回溢出的标记序列。如果提供了一对输入 ID 序列（或一批对）并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出的标记。

+   `return_special_tokens_mask` (`bool`, *可选*, 默认为 `False`) — 是否返回特殊标记掩码信息。

+   `return_offsets_mapping` (`bool`, *可选*, 默认为 `False`) — 是否返回每个标记的 `(char_start, char_end)`。

    这仅适用于继承自 PreTrainedTokenizerFast 的快速标记化器，如果使用 Python 的标记化器，此方法将引发 `NotImplementedError`。

+   `return_length` (`bool`, *可选*, 默认为 `False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *可选*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给 `self.tokenize()` 方法

返回

BatchEncoding

一个包含以下字段的 BatchEncoding：

+   `input_ids` — 要提供给模型的标记 ID 列表。

    什么是输入 IDs？

+   `token_type_ids` — 要提供给模型的标记类型 ID 列表（当 `return_token_type_ids=True` 或 *`token_type_ids`* 在 `self.model_input_names` 中时）。

    什么是 token type IDs？

+   `attention_mask` — 指定哪些标记应该被模型关注的索引列表（当 `return_attention_mask=True` 或 *`attention_mask`* 在 `self.model_input_names` 中时）。

    什么是注意力掩码？

+   `overflowing_tokens` — 溢出标记序列的列表（当指定了 `max_length` 并且 `return_overflowing_tokens=True` 时）。

+   `num_truncated_tokens` — 截断的标记数量（当指定了 `max_length` 并且 `return_overflowing_tokens=True` 时）。

+   `special_tokens_mask` — 由 0 和 1 组成的列表，其中 1 指定添加的特殊标记，0 指定常规序列标记（当 `add_special_tokens=True` 和 `return_special_tokens_mask=True` 时）。

+   `length` — 输入的长度（当 `return_length=True` 时）

标记化和准备模型一个或多个序列或一个或多个序列对的主要方法。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L238)

```py
( save_directory: str filename_prefix: Optional = None )
```

## VitsModel

### `class transformers.VitsModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/modeling_vits.py#L1326)

```py
( config: VitsConfig )
```

参数

+   `config` (VitsConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained() 方法以加载模型权重。

用于文本到语音合成的完整 VITS 模型。此模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/modeling_vits.py#L1360)

```py
( input_ids: Optional = None attention_mask: Optional = None speaker_id: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.models.vits.modeling_vits.VitsModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选的*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `speaker_id` (`int`, *可选的*) — 要使用的说话者嵌入。仅用于多说话者模型。

+   `output_attentions` (`bool`, *可选的*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选的*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选的*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

+   `labels` (`torch.FloatTensor`，形状为`(batch_size, config.spectrogram_bins, sequence_length)`，*可选的*) — 目标频谱图的浮点值。时间步设置为`-100.0`将被忽略（掩码）用于损失计算。

返回

`transformers.models.vits.modeling_vits.VitsModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.vits.modeling_vits.VitsModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或者当`config.return_dict=False`时）包含根据配置（VitsConfig）和输入的各种元素。

+   `waveform` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 模型预测的最终音频波形。

+   `sequence_lengths` (`torch.FloatTensor`，形状为`(batch_size,)`) — `waveform`批次中每个元素的样本长度。

+   `spectrogram` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_bins)`) — 在流模型输出处预测的对数梅尔频谱图。此频谱图传递给 Hi-Fi GAN 解码器模型以获得最终的音频波形。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

VitsModel 的前向方法覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。

示例：

```py
>>> from transformers import VitsTokenizer, VitsModel, set_seed
>>> import torch

>>> tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
>>> model = VitsModel.from_pretrained("facebook/mms-tts-eng")

>>> inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

>>> set_seed(555)  # make deterministic

>>> with torch.no_grad():
...     outputs = model(inputs["input_ids"])
>>> outputs.waveform.shape
torch.Size([1, 45824])
```
