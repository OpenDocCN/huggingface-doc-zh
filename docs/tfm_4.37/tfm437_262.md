# DETR

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/detr)

## 概述

DETR 模型是由 Nicolas Carion、Francisco Massa、Gabriel Synnaeve、Nicolas Usunier、Alexander Kirillov 和 Sergey Zagoruyko 在[使用变压器进行端到端目标检测](https://arxiv.org/abs/2005.12872)中提出的。DETR 由一个卷积主干后面跟着一个编码器-解码器变压器组成，可以进行端到端的目标检测训练。它极大地简化了像 Faster-R-CNN 和 Mask-R-CNN 这样的模型的复杂性，这些模型使用区域提议、非极大值抑制程序和锚点生成等技术。此外，DETR 还可以自然地扩展到执行全景分割，只需在解码器输出之上添加一个蒙版头。

论文摘要如下：

*我们提出了一种将目标检测视为直接集合预测问题的新方法。我们的方法简化了检测流程，有效地消除了许多手工设计的组件，如非极大值抑制程序或明确编码我们对任务的先验知识的锚点生成。新框架 DEtection TRansformer 或 DETR 的主要组成部分是通过二部匹配强制唯一预测的基于集合的全局损失，以及一个变压器编码器-解码器架构。给定一组固定的学习目标查询，DETR 推理对象之间的关系和全局图像上下文，直接并行输出最终的预测集。这个新模型在概念上简单，不需要专门的库，不像许多其他现代检测器。DETR 在具有挑战性的 COCO 目标检测数据集上表现出与经过充分优化的 Faster RCNN 基线相当的准确性和运行时性能。此外，DETR 可以轻松推广为以统一方式产生全景分割。我们展示它明显优于竞争基线。*

这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。原始代码可以在[这里](https://github.com/facebookresearch/detr)找到。

## DETR 的工作原理

以下是解释 DetrForObjectDetection 工作原理的 TLDR：

首先，将图像通过预训练的卷积主干（在论文中，作者使用 ResNet-50/ResNet-101）。假设我们也添加了一个批处理维度。这意味着主干的输入是一个形状为`(batch_size, 3, height, width)`的张量，假设图像有 3 个颜色通道（RGB）。CNN 主干输出一个新的低分辨率特征图，通常形状为`(batch_size, 2048, height/32, width/32)`。然后，将其投影到 DETR 变压器的隐藏维度，该维度默认为`256`，使用`nn.Conv2D`层。现在，我们有一个形状为`(batch_size, 256, height/32, width/32)`的张量。接下来，特征图被展平并转置，以获得形状为`(batch_size, seq_len, d_model)` = `(batch_size, width/32*height/32, 256)`的张量。因此，与 NLP 模型的一个区别是，序列长度实际上比通常更长，但`d_model`较小（在 NLP 中通常为 768 或更高）。

接下来，这通过编码器发送，输出相同形状的`encoder_hidden_states`（您可以将这些视为图像特征）。接下来，所谓的**对象查询**通过解码器发送。这是一个形状为`(batch_size, num_queries, d_model)`的张量，其中`num_queries`通常设置为 100，并用零初始化。这些输入嵌入是学习的位置编码，作者将其称为对象查询，类似于编码器，它们被添加到每个注意力层的输入中。每个对象查询将在图像中寻找特定对象。解码器通过多个自注意力和编码器-解码器注意力层更新这些嵌入，以输出相同形状的`decoder_hidden_states`：`(batch_size, num_queries, d_model)`。接下来，顶部添加了两个头用于对象检测：一个线性层用于将每个对象查询分类为对象或“无对象”之一，以及一个 MLP 用于预测每个查询的边界框。

该模型使用**二部匹配损失**进行训练：实际上我们比较每个 N = 100 个对象查询的预测类别+边界框与地面真实注释，填充到相同长度 N（因此如果图像仅包含 4 个对象，则 96 个注释将只有一个“无对象”作为类别和一个“无边界框”作为边界框）。使用[匈牙利匹配算法](https://en.wikipedia.org/wiki/Hungarian_algorithm)找到每个 N 查询与每个 N 注释的最佳一对一映射。接下来，使用标准交叉熵（用于类别）和 L1 的线性组合以及[广义 IoU 损失](https://giou.stanford.edu/)（用于边界框）来优化模型的参数。

DETR 可以自然扩展以执行全景分割（将语义分割和实例分割统一起来）。DetrForSegmentation 在 DetrForObjectDetection 的顶部添加了一个分割掩码头。掩码头可以同时训练，或者在两个步骤的过程中训练，首先训练一个 DetrForObjectDetection 模型来检测“事物”（实例）和“物品”（背景物品，如树木、道路、天空）周围的边界框，然后冻结所有权重，仅训练掩码头 25 个时代。实验上，这两种方法给出了类似的结果。请注意，为了使训练成为可能，预测框是必需的，因为匈牙利匹配是使用框之间的距离计算的。

## 使用提示

+   DETR 使用所谓的**对象查询**来检测图像中的对象。查询的数量确定了单个图像中可以检测到的对象的最大数量，默认设置为 100（请参阅 DetrConfig 的参数`num_queries`）。请注意，最好有一些余地（在 COCO 中，作者使用了 100，而 COCO 图像中的最大对象数量约为 70）。

+   DETR 的解码器并行更新查询嵌入。这与像 GPT-2 这样使用自回归解码而不是并行的语言模型不同。因此，不使用因果关注掩码。

+   在将隐藏状态投影到查询和键之前，DETR 在每个自注意力和交叉注意力层中添加位置嵌入。对于图像的位置嵌入，可以在固定正弦或学习的绝对位置嵌入之间进行选择。默认情况下，DetrConfig 的参数`position_embedding_type`设置为`"sine"`。

+   在训练期间，DETR 的作者确实发现在解码器中使用辅助损失是有帮助的，特别是为了帮助模型输出每个类别的正确对象数量。如果将 DetrConfig 的参数`auxiliary_loss`设置为`True`，则在每个解码器层之后添加预测前馈神经网络和匈牙利损失（FFN 共享参数）。

+   如果您想在跨多个节点的分布式环境中训练模型，则应该在*modeling_detr.py*中的*DetrLoss*类中更新*num_boxes*变量。在多个节点上训练时，应将其设置为所有节点上目标框的平均数量，可以在原始实现中看到[这里](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232)。

+   DetrForObjectDetection 和 DetrForSegmentation 可以使用[timm 库](https://github.com/rwightman/pytorch-image-models)中可用的任何卷积骨干进行初始化。例如，可以通过将 DetrConfig 的`backbone`属性设置为`"tf_mobilenetv3_small_075"`，然后使用该配置初始化模型来使用 MobileNet 骨干。

+   DETR 调整输入图像的大小，使最短边至少为一定数量的像素，而最长边至多为 1333 像素。在训练时，使用尺度增强，使最短边随机设置为至少 480 像素，最多 800 像素。在推断时，最短边设置为 800。可以使用 DetrImageProcessor 为模型准备图像（以及可选的以 COCO 格式的注释）。由于这种调整大小，批处理中的图像可能具有不同的大小。DETR 通过将图像填充到批处理中的最大大小，并创建一个像素掩码来指示哪些像素是真实的/哪些是填充来解决这个问题。另外，也可以定义一个自定义的`collate_fn`来批处理图像，使用`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`。

+   图像的大小将决定所使用的内存量，从而确定`batch_size`。建议每个 GPU 使用批量大小为 2。有关更多信息，请参阅[此 Github 线程](https://github.com/facebookresearch/detr/issues/150)。

有三种实例化 DETR 模型的方法（取决于您的偏好）：

选项 1：使用整个模型的预训练权重实例化 DETR

```py
>>> from transformers import DetrForObjectDetection

>>> model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
```

选项 2：使用随机初始化的 Transformer 权重实例化 DETR，但使用骨干的预训练权重

```py
>>> from transformers import DetrConfig, DetrForObjectDetection

>>> config = DetrConfig()
>>> model = DetrForObjectDetection(config)
```

选项 3：使用随机初始化的骨干+Transformer 实例化 DETR

```py
>>> config = DetrConfig(use_pretrained_backbone=False)
>>> model = DetrForObjectDetection(config)
```

总之，请参考以下表格：

| 任务 | 目标检测 | 实例分割 | 全景分割 |
| --- | --- | --- | --- |
| **描述** | 预测图像中物体周围的边界框和类标签 | 预测图像中物体（即实例）周围的掩模 | 预测图像中物体（即实例）以及“物质”（即背景物品如树木和道路）周围的掩模 |
| **模型** | DetrForObjectDetection | DetrForSegmentation | DetrForSegmentation |
| **示例数据集** | COCO 检测 | COCO 检测，COCO 全景 | COCO 全景 |
| 提供给 DetrImageProcessor 的注释格式 | {‘image_id’: `int`, ‘annotations’: `List[Dict]`}，每个 Dict 是一个 COCO 对象注释 | {‘image_id’: `int`, ‘annotations’: `List[Dict]`}（在 COCO 检测的情况下）或{‘file_name’: `str`, ‘image_id’: `int`, ‘segments_info’: `List[Dict]`}（在 COCO 全景的情况下） | {‘file_name’: `str`, ‘image_id’: `int`, ‘segments_info’: `List[Dict]`}和 masks_path（包含 PNG 文件的掩模目录的路径） |
| **后处理**（即将模型输出转换为 Pascal VOC 格式） | `post_process()` | `post_process_segmentation()` | `post_process_segmentation()`, `post_process_panoptic()` |
| **评估器** | `CocoEvaluator` with `iou_types="bbox"` | `CocoEvaluator` with `iou_types="bbox"` or `"segm"` | `CocoEvaluator` with `iou_tupes="bbox"` or `"segm"`, `PanopticEvaluator` |

简而言之，应该准备数据以 COO 检测或 COO 全景格式，然后使用 DetrImageProcessor 创建`pixel_values`、`pixel_mask`和可选的`labels`，然后可以用于训练（或微调）模型。对于评估，应该首先使用 DetrImageProcessor 的其中一种后处理方法转换模型的输出。这些可以提供给`CocoEvaluator`或`PanopticEvaluator`，这些评估器允许您计算像平均精度（mAP）和全景质量（PQ）这样的指标。后者对象在[原始存储库](https://github.com/facebookresearch/detr)中实现。有关评估的更多信息，请参见[示例笔记本](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)。

## 资源

一个官方 Hugging Face 和社区（由🌎表示）资源列表，可帮助您开始使用 DETR。

目标检测

+   所有示例笔记本说明在自定义数据集上对 DetrForObjectDetection 和 DetrForSegmentation 进行微调的示例可以在[此处](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)找到。

+   参见：目标检测任务指南

如果您有兴趣提交资源以包含在此处，请随时打开一个 Pull Request，我们将进行审查！资源应该展示一些新东西，而不是重复现有资源。

## DetrConfig

### `class transformers.DetrConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L36)

```py
( use_timm_backbone = True backbone_config = None num_channels = 3 num_queries = 100 encoder_layers = 6 encoder_ffn_dim = 2048 encoder_attention_heads = 8 decoder_layers = 6 decoder_ffn_dim = 2048 decoder_attention_heads = 8 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 is_encoder_decoder = True activation_function = 'relu' d_model = 256 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 init_xavier_std = 1.0 auxiliary_loss = False position_embedding_type = 'sine' backbone = 'resnet50' use_pretrained_backbone = True dilation = False class_cost = 1 bbox_cost = 5 giou_cost = 2 mask_loss_coefficient = 1 dice_loss_coefficient = 1 bbox_loss_coefficient = 5 giou_loss_coefficient = 2 eos_coefficient = 0.1 **kwargs )
```

参数

+   `use_timm_backbone`（`bool`，*可选*，默认为`True`）— 是否使用`timm`库作为骨干。如果设置为`False`，将使用`AutoBackbone` API。

+   `backbone_config`（`PretrainedConfig`或`dict`，*可选*）— 骨干模型的配置。仅在`use_timm_backbone`设置为`False`时使用，默认为`ResNetConfig()`。

+   `num_channels`（`int`，*可选*，默认为 3）— 输入通道的数量。

+   `num_queries`（`int`，*可选*，默认为 100）— 对象查询的数量，即检测槽的数量。这是 DetrModel 在单个图像中可以检测的对象的最大数量。对于 COCO，我们建议使用 100 个查询。

+   `d_model`（`int`，*可选*，默认为 256）— 层的维度。

+   `encoder_layers`（`int`，*可选*，默认为 6）— 编码器层数。

+   `decoder_layers`（`int`，*可选*，默认为 6）— 解码器层数。

+   `encoder_attention_heads`（`int`，*可选*，默认为 8）— Transformer 编码器中每个注意力层的注意力头数。

+   `decoder_attention_heads` (`int`, *optional*, defaults to 8) — Transformer 解码器中每个注意力层的注意力头数。

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 2048) — 解码器中“中间”（通常称为前馈）层的维度。

+   `encoder_ffn_dim` (`int`, *optional*, defaults to 2048) — 解码器中“中间”（通常称为前馈）层的维度。

+   `activation_function` (`str` or `function`, *optional*, defaults to `"relu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的 dropout 比率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.0) — 全连接层内激活的 dropout 比率。

+   `init_std` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `init_xavier_std` (`float`, *optional*, defaults to 1) — 用于 HM Attention map 模块中 Xavier 初始化增益的缩放因子。

+   `encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的 LayerDrop 概率。更多细节请参阅[LayerDrop paper](https://arxiv.org/abs/1909.11556)。

+   `decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 解码器的 LayerDrop 概率。更多细节请参阅[LayerDrop paper](https://arxiv.org/abs/1909.11556)。

+   `auxiliary_loss` (`bool`, *optional*, defaults to `False`) — 是否使用辅助解码损失（每个解码器层的损失）。

+   `position_embedding_type` (`str`, *optional*, defaults to `"sine"`) — 在图像特征之上使用的位置嵌入的类型。可以是`"sine"`或`"learned"`之一。

+   `backbone` (`str`, *optional*, defaults to `"resnet50"`) — 在`use_timm_backbone` = `True`时要使用的卷积骨干网络的名称。支持 timm 包中的任何卷积骨干网络。有关所有可用模型的列表，请参阅[此页面](https://rwightman.github.io/pytorch-image-models/#load-a-pretrained-model)。

+   `use_pretrained_backbone` (`bool`, *optional*, defaults to `True`) — 是否在骨干网络中使用预训练权重。仅在`use_timm_backbone` = `True`时支持。

+   `dilation` (`bool`, *optional*, defaults to `False`) — 是否在最后的卷积块（DC5）中用扩张替换步幅。仅在`use_timm_backbone` = `True`时支持。

+   `class_cost` (`float`, *optional*, defaults to 1) — 匈牙利匹配成本中分类错误的相对权重。

+   `bbox_cost` (`float`, *optional*, defaults to 5) — 相对于匈牙利匹配成本中边界框坐标的 L1 误差的权重。

+   `giou_cost` (`float`, *optional*, defaults to 2) — 相对于匈牙利匹配成本中边界框广义 IoU 损失的权重。

+   `mask_loss_coefficient` (`float`, *optional*, defaults to 1) — 泛光分割损失中 Focal 损失的相对权重。

+   `dice_loss_coefficient` (`float`, *optional*, defaults to 1) — 泛光分割损失中 DICE/F-1 损失的相对权重。

+   `bbox_loss_coefficient` (`float`, *optional*, defaults to 5) — 目标检测损失中 L1 边界框损失的相对权重。

+   `giou_loss_coefficient` (`float`, *optional*, defaults to 2) — 目标检测损失中广义 IoU 损失的相对权重。

+   `eos_coefficient` (`float`, *optional*, defaults to 0.1) — 目标检测损失中“无对象”类别的相对分类权重。

这是一个配置类，用于存储 DetrModel 的配置。它用于根据指定的参数实例化一个 DETR 模型，定义模型架构。使用默认值实例化配置将产生类似于 DETR [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import DetrConfig, DetrModel

>>> # Initializing a DETR facebook/detr-resnet-50 style configuration
>>> configuration = DetrConfig()

>>> # Initializing a model (with random weights) from the facebook/detr-resnet-50 style configuration
>>> model = DetrModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

#### `from_backbone_config`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/configuration_detr.py#L239)

```py
( backbone_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';DetrConfig
```

参数

+   `backbone_config` (PretrainedConfig) — 骨干配置。

返回

DetrConfig

配置对象的实例

从预训练的骨干模型配置实例化一个 DetrConfig（或派生类）。

## DetrImageProcessor

### `class transformers.DetrImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L742)

```py
( format: Union = <AnnotationFormat.COCO_DETECTION: 'coco_detection'> do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True **kwargs )
```

参数

+   `format` (`str`, *optional*, 默认为`"coco_detection"`) — 注释的数据格式。可以是`"coco_detection"`或“coco_panoptic”之一。

+   `do_resize` (`bool`, *optional*, 默认为 `True`) — 控制是否将图像的`(height, width)`维度调整为指定的`size`。可以被`preprocess`方法中的`do_resize`参数覆盖。

+   `size` (`Dict[str, int]` *optional*, 默认为 `{"shortest_edge" -- 800, "longest_edge": 1333}`): 调整大小后的图像的`(height, width)`维度大小。可以被`preprocess`方法中的`size`参数覆盖。

+   `resample` (`PILImageResampling`, *optional*, 默认为 `PILImageResampling.BILINEAR`) — 如果调整图像大小，则要使用的重采样滤波器。

+   `do_rescale` (`bool`, *optional*, 默认为 `True`) — 控制是否按指定比例因子`rescale_factor`重新缩放图像。可以被`preprocess`方法中的`do_rescale`参数覆盖。

+   `rescale_factor` (`int` 或 `float`, *optional*, 默认为 `1/255`) — 如果重新调整图像，则要使用的比例因子。可以被`preprocess`方法中的`rescale_factor`参数覆盖。do_normalize — 控制是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。

+   `image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_DEFAULT_MEAN`) — 在归一化图像时使用的均值。可以是单个值或每个通道的值列表。可以被`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_DEFAULT_STD`) — 在归一化图像时使用的标准差值。可以是单个值或每个通道的值列表。可以被`preprocess`方法中的`image_std`参数覆盖。

+   `do_pad` (`bool`, *optional*, 默认为 `True`) — 控制是否将图像填充到批处理中最大的图像并创建像素掩码。可以被`preprocess`方法中的`do_pad`参数覆盖。

构造一个 Detr 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1070)

```py
( images: Union annotations: Union = None return_segmentation_masks: bool = None masks_path: Union = None do_resize: Optional = None size: Optional = None resample = None do_rescale: Optional = None rescale_factor: Union = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None format: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像或图像批次。期望单个或批量像素值范围从 0 到 255 的图像。如果传入像素值在 0 到 1 之间的图像，请设置`do_rescale=False`。

+   `annotations` (`AnnotationType` or `List[AnnotationType]`, *optional*) — 与图像或图像批次相关联的注释列表。如果注释是用于对象检测的，则注释应该是一个带有以下键的字典：

    +   “image_id” (`int`): 图像 id。

    +   “annotations” (`List[Dict]`): 图像的注释列表。每个注释应该是一个字典。一个图像可以没有注释，此时列表应为空。如果注释是用于分割的，注释应该是一个带有以下键的字典：

    +   “image_id” (`int`): 图像 id。

    +   “segments_info” (`List[Dict]`): 图像的段列表。每个段应该是一个字典。一个图像可以没有段，此时列表应为空。

    +   “file_name” (`str`): 图像的文件名。

+   `return_segmentation_masks` (`bool`, *optional*, 默认为 self.return_segmentation_masks) — 是否返回分割蒙版。

+   `masks_path` (`str` or `pathlib.Path`, *optional*) — 包含分割蒙版的目录路径。

+   `do_resize` (`bool`, *optional*, 默认为 self.do_resize) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *optional*, 默认为 self.size) — 调整大小后的图像尺寸。

+   `resample` (`PILImageResampling`, *optional*, 默认为 self.resample) — 调整图像大小时使用的重采样滤波器。

+   `do_rescale` (`bool`, *optional*, 默认为 self.do_rescale) — 是否重新缩放图像。

+   `rescale_factor` (`float`, *optional*, 默认为 self.rescale_factor) — 调整图像时使用的缩放因子。

+   `do_normalize` (`bool`, *optional*, 默认为 self.do_normalize) — 是否规范化图像。

+   `image_mean` (`float` or `List[float]`, *optional*, 默认为 self.image_mean) — 在规范化图像时使用的均值。

+   `image_std` (`float` or `List[float]`, *optional*, 默认为 self.image_std) — 在规范化图像时使用的标准差。

+   `do_pad` (`bool`, *optional*, 默认为 self.do_pad) — 是否填充图像。

+   `format` (`str` or `AnnotationFormat`, *optional*, 默认为 self.format) — 注释的格式。

+   `return_tensors` (`str` or `TensorType`, *optional*, 默认为 self.return_tensors) — 要返回的张量类型。如果为`None`，将返回图像列表。

+   `data_format` (`ChannelDimension` or `str`, *optional*, 默认为`ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以（通道数，高度，宽度）格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以（高度，宽度，通道数）格式。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` or `str`, *optional*) — 输入图像的通道维度格式。如果未设置，将从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以（通道数，高度，宽度）格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以（高度，宽度，通道数）格式。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像以（高度，宽度）格式。

预处理图像或图像批次，以便模型可以使用。

#### `post_process_object_detection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)

```py
( outputs threshold: float = 0.5 target_sizes: Union = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`DetrObjectDetectionOutput`) — 模型的原始输出。

+   `threshold` (`float`, *optional*) — 保留对象检测预测的分数阈值。

+   `target_sizes` (`torch.Tensor`或`列表[元组[int, int]]`, *可选*) — 形状为`(batch_size, 2)`的张量或包含每个图像的目标大小`(高度，宽度)`的元组列表(`元组[int, int]`)。如果未设置，预测将不会被调整大小。

返回

`列表[字典]`

一个字典列表，每个字典包含模型预测的批次中图像的分数、标签和框。

将 DetrForObjectDetection 的原始输出转换为最终的边界框，格式为（左上角 _x，左上角 _y，右下角 _x，右下角 _y）。仅支持 PyTorch。

#### `后处理语义分割`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)

```py
( outputs target_sizes: List = None ) → export const metadata = 'undefined';List[torch.Tensor]
```

参数

+   `outputs` (DetrForSegmentation) — 模型的原始输出。

+   `target_sizes` (`列表[元组[int, int]]`, *可选*) — 一个元组列表(`元组[int, int]`)，包含批次中每个图像的目标大小（高度，宽度）。如果未设置，预测将不会被调整大小。

返回

`列表[torch.Tensor]`

一个长度为`batch_size`的列表，其中每个项是一个形状为(高度，宽度)的语义分割地图，对应于目标大小条目（如果指定了`target_sizes`）。每个`torch.Tensor`的每个条目对应于一个语义类别 id。

将 DetrForSegmentation 的输出转换为语义分割地图。仅支持 PyTorch。

#### `后处理实例分割`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (DetrForSegmentation) — 模型的原始输出。

+   `threshold` (`float`, *可选*, 默认为 0.5) — 保留预测实例掩模的概率分数阈值。

+   `mask_threshold` (`float`, *可选*, 默认为 0.5) — 将预测的掩模转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold` (`float`, *可选*, 默认为 0.8) — 合并或丢弃每个二进制实例掩模中的小不连续部分的重叠掩模区域阈值。

+   `target_sizes` (`列表[元组]`, *可选*) — 长度为(batch_size)的列表，其中每个列表项(`元组[int, int]]`)对应于每个预测的请求最终大小(高度，宽度)。如果未设置，预测将不会被调整大小。

+   `return_coco_annotation` (`bool`, *可选*) — 默认为`False`。如果设置为`True`，则以 COCO 运行长度编码（RLE）格式返回分割地图。

返回

`列表[字典]`

一个字典列表，每个图像一个字典，每个字典包含两个键：

+   `segmentation` — 形状为`(高度，宽度)`的张量，其中每个像素代表`segment_id`或`列表[列表]`的运行长度编码（RLE）的分割地图，如果 return_coco_annotation 设置为`True`。如果未找到高于`threshold`的掩模，则设置为`None`。

+   `segments_info` — 包含每个段的附加信息的字典。

    +   `id` — 代表`segment_id`的整数。

    +   `label_id` — 代表与`segment_id`对应的标签/语义类别 id 的整数。

    +   `score` — 具有`segment_id`的段的预测分数。

将 DetrForSegmentation 的输出转换为实例分割预测。仅支持 PyTorch。

#### `后处理全景分割`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (DetrForSegmentation) — 来自 DetrForSegmentation 的输出。

+   `threshold` (`float`, *可选*, 默认为 0.5) — 保留预测实例掩模的概率分数阈值。

+   `mask_threshold` (`float`, *可选*, 默认为 0.5) — 在将预测的掩模转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold` (`float`, *可选*, 默认为 0.8) — 合并或丢弃每个二进制实例掩模中的小断开部分的重叠掩模面积阈值。

+   `label_ids_to_fuse` (`Set[int]`, *可选*) — 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说一张图像中只能有一个天空，但可以有几个人，因此天空的标签 ID 将在该集合中，但人的标签 ID 不在其中。

+   `target_sizes` (`List[Tuple]`, *可选*) — 长度为(batch_size)的列表，其中每个列表项(`Tuple[int, int]]`)对应于批次中每个预测的请求最终大小(高度、宽度)。如果未设置，预测将不会被调整大小。

返回

`List[Dict]`

一个字典列表，每个图像一个字典，每个字典包含两个键：

+   `segmentation` — 形状为`(height, width)`的张量，其中每个像素表示一个`segment_id`，如果在`threshold`以上找不到掩模，则表示为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。

+   `segments_info` — 一个包含每个段的额外信息的字典。

    +   `id` — 代表`segment_id`的整数。

    +   `label_id` — 代表与`segment_id`对应的标签/语义类别 id 的整数。

    +   `was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。同一类别/标签的多个实例被融合并分配一个单独的`segment_id`。

    +   `score` — 带有`segment_id`的段的预测分数。

将 DetrForSegmentation 的输出转换为图像全景分割预测。仅支持 PyTorch。

## DetrFeatureExtractor

### `class transformers.DetrFeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/feature_extraction_detr.py#L36)

```py
( *args **kwargs )
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

预处理一张图像或一批图像。

#### `post_process_object_detection`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1572)

```py
( outputs threshold: float = 0.5 target_sizes: Union = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`DetrObjectDetectionOutput`) — 模型的原始输出。

+   `threshold` (`float`, *可选*) — 保留对象检测预测的分数阈值。

+   `target_sizes` (`torch.Tensor`或`List[Tuple[int, int]]`, *可选*) — 形状为`(batch_size, 2)`的张量或包含批次中每个图像的目标大小`(height, width)`的元组列表(`Tuple[int, int]`)。如果未设置，预测将不会被调整大小。

返回

`List[Dict]`

一个字典列表，每个字典包含模型预测的批次中每个图像的分数、标签和框。

将 DetrForObjectDetection 的原始输出转换为(top_left_x, top_left_y, bottom_right_x, bottom_right_y)格式的最终边界框。仅支持 PyTorch。

#### `post_process_semantic_segmentation`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1625)

```py
( outputs target_sizes: List = None ) → export const metadata = 'undefined';List[torch.Tensor]
```

参数

+   `outputs` (DetrForSegmentation) — 模型的原始输出。

+   `target_sizes`（`List[Tuple[int, int]]`，*可选*） - 一个元组列表（`Tuple[int, int]`），包含批处理中每个图像的目标大小（高度，宽度）。如果未设置，预测将不会被调整大小。

返回

`List[torch.Tensor]`

一个长度为`batch_size`的列表，其中每个项目都是一个形状为（高度，宽度）的语义分割地图，对应于`target_sizes`条目（如果指定了`target_sizes`）。每个`torch.Tensor`的条目对应于一个语义类别 id。

将 DetrForSegmentation 的输出转换为语义分割地图。仅支持 PyTorch。

#### `post_process_instance_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1673)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs`（DetrForSegmentation） - 模型的原始输出。

+   `threshold`（`float`，*可选*，默认为 0.5） - 保留预测实例掩码的概率分数阈值。

+   `mask_threshold`（`float`，*可选*，默认为 0.5） - 在将预测的掩码转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold`（`float`，*可选*，默认为 0.8） - 合并或丢弃每个二进制实例掩码中的小断开部分的重叠掩码区域阈值。

+   `target_sizes`（`List[Tuple]`，*可选*） - 长度为（batch_size）的列表，其中每个列表项（`Tuple[int, int]`）对应于每个预测的请求最终大小（高度，宽度）。如果未设置，预测将不会被调整大小。

+   `return_coco_annotation`（`bool`，*可选*） - 默认为`False`。如果设置为`True`，则以 COCO 运行长度编码（RLE）格式返回分割地图。

返回

`List[Dict]`

一个字典列表，每个图像一个字典，每个字典包含两个键：

+   `segmentation` - 一个形状为（高度，宽度）的张量，其中每个像素表示`segment_id`或分割地图的`List[List]`运行长度编码（RLE），如果`return_coco_annotation`设置为`True`。如果未找到高于`threshold`的掩码，则设置为`None`。

+   `segments_info` - 包含每个段的附加信息的字典。

    +   `id` - 表示`segment_id`的整数。

    +   `label_id` - 表示与`segment_id`对应的标签/语义类别 id 的整数。

    +   `score` - 具有`segment_id`的段的预测分数。

将 DetrForSegmentation 的输出转换为实例分割预测。仅支持 PyTorch。

#### `post_process_panoptic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/image_processing_detr.py#L1757)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs`（DetrForSegmentation） - 来自 DetrForSegmentation 的输出。

+   `threshold`（`float`，*可选*，默认为 0.5） - 保留预测实例掩码的概率分数阈值。

+   `mask_threshold`（`float`，*可选*，默认为 0.5） - 在将预测的掩码转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold`（`float`，*可选*，默认为 0.8） - 合并或丢弃每个二进制实例掩码中的小断开部分的重叠掩码区域阈值。

+   `label_ids_to_fuse`（`Set[int]`，*可选*） - 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说图像中只能有一个天空，但可以有几个人，因此天空的标签 ID 将在该集合中，但人的标签 ID 不在其中。

+   `target_sizes` (`List[Tuple]`, *optional*) — 长度为`(batch_size)`的列表，每个列表项(`Tuple[int, int]]`)对应于批处理中每个预测的请求最终大小(高度，宽度)。如果未设置，预测将不会被调整大小。

返回值

`List[Dict]`

一个字典列表，每个图像一个字典，每个字典包含两个键:

+   `segmentation` — 形状为`(height, width)`的张量，每个像素代表一个`segment_id`，如果找不到遮罩，则为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。

+   `segments_info` — 包含每个段的附加信息的字典。

    +   `id` — 代表`segment_id`的整数。

    +   `label_id` — 代表与`segment_id`对应的标签/语义类别 id 的整数。

    +   `was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。相同类别/标签的多个实例被融合并分配一个单一的`segment_id`。

    +   `score` — 具有`segment_id`的段的预测分数。

将 DetrForSegmentation 的输出转换为图像全景分割预测。仅支持 PyTorch。

## DETR 特定输出

### `class transformers.models.detr.modeling_detr.DetrModelOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L94)

```py
( last_hidden_state: FloatTensor = None past_key_values: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None intermediate_hidden_states: Optional = None )
```

参数

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`。每层解码器的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`。每层编码器的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组，每层一个`torch.FloatTensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `intermediate_hidden_states`（`torch.FloatTensor`，形状为`(config.decoder_layers, batch_size, sequence_length, hidden_size)`，*可选*，当`config.auxiliary_loss=True`时返回）— 中间解码器激活，即每个解码器层的输出，每个输出都经过了 layernorm。

DETR 编码器-解码器模型输出的基类。该类在 Seq2SeqModelOutput 中添加了一个属性，即一个可选的中间解码器激活堆栈，即每个解码器层的输出，每个输出都经过了 layernorm。在使用辅助解码损失训练模型时，这是有用的。

### `class transformers.models.detr.modeling_detr.DetrObjectDetectionOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L134)

```py
( loss: Optional = None loss_dict: Optional = None logits: FloatTensor = None pred_boxes: FloatTensor = None auxiliary_outputs: Optional = None last_hidden_state: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None )
```

参数

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 总损失，作为类预测的负对数似然（交叉熵）和边界框损失的线性组合。后者定义为 L1 损失和广义比例不变 IoU 损失的线性组合。

+   `loss_dict`（`Dict`，*可选*）— 包含各个损失的字典。用于记录日志。

+   `logits`（形状为`(batch_size, num_queries, num_classes + 1)`的`torch.FloatTensor`）— 所有查询的分类 logits（包括无对象）。

+   `pred_boxes`（形状为`(batch_size, num_queries, 4)`的`torch.FloatTensor`）— 所有查询的归一化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未归一化的边界框。

+   `auxiliary_outputs`（`list[Dict]`，*可选*）— 仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。它是一个包含两个上述键（`logits`和`pred_boxes`）的字典列表，每个字典对应一个解码器层。

+   `last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。每个层的解码器的隐藏状态加上初始嵌入输出。

+   `decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。编码器在每一层的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

DetrForObjectDetection 的输出类型。

### `class transformers.models.detr.modeling_detr.DetrSegmentationOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L197)

```py
( loss: Optional = None loss_dict: Optional = None logits: FloatTensor = None pred_boxes: FloatTensor = None pred_masks: FloatTensor = None auxiliary_outputs: Optional = None last_hidden_state: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None )
```

参数

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 作为负对数似然（交叉熵）和边界框损失的线性组合的总损失。后者定义为 L1 损失和广义尺度不变 IoU 损失的线性组合。

+   `loss_dict` (`Dict`, *optional*) — 包含各个损失的字典。用于记录日志。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) — 所有查询的分类 logits（包括无对象）。

+   `pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) — 所有查询的归一化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未归一化的边界框。

+   `pred_masks` (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4, width/4)`) — 所有查询的分割掩模 logits。另请参阅 post_process_semantic_segmentation()或 post_process_instance_segmentation()post_process_panoptic_segmentation()分别评估语义、实例和全景分割掩模。

+   `auxiliary_outputs` (`list[Dict]`, *optional*) — 可选，仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。它是一个包含每个解码器层的上述两个键（`logits`和`pred_boxes`）的字典列表。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。解码器在每一层的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。编码器在每层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

DetrForSegmentation 的输出类型。

## DetrModel

### `class transformers.DetrModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1296)

```py
( config: DetrConfig )
```

参数

+   `config` (DetrConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

裸的 DETR 模型（由骨干和编码器-解码器 Transformer 组成），输出原始隐藏状态，没有特定的头部。

这个模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1337)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.detr.modeling_detr.DetrModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。

    可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 DetrImageProcessor.`call`()。

+   `pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免在填充像素值上执行注意力的遮罩。遮罩值选择在`[0, 1]`中：

    +   对于真实的像素（即`未被遮罩`），

    +   对于填充像素（即`被遮罩`）为 0。

    什么是注意力遮罩？

+   `decoder_attention_mask`（形状为`(batch_size, num_queries)`的`torch.FloatTensor`，*可选*）— 默认情况下不使用。可用于屏蔽对象查询。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层的隐藏状态序列。用于解码器的交叉注意力。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递一个图像的扁平表示，而不是传递扁平特征图（骨干网络输出+投影层的输出）。

+   `decoder_inputs_embeds`（形状为`(batch_size, num_queries, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递一个嵌入表示，而不是用零张量初始化查询。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通的元组。

返回

transformers.models.detr.modeling_detr.DetrModelOutput 或`tuple(torch.FloatTensor)`

一个 transformers.models.detr.modeling_detr.DetrModelOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包括根据配置（DetrConfig）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）— 模型解码器最后一层的隐藏状态序列的输出。

+   `decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 元组包括（每个层的嵌入输出+每个层的输出的`torch.FloatTensor`）形状为`(batch_size, sequence_length, hidden_size)`。解码器在每一层的输出隐藏状态加上初始嵌入输出。

+   `decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 元组包括（每层一个）形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`。解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—编码器的隐藏状态元组，形状为`(batch_size, sequence_length, hidden_size)`（每层一个）。每层的编码器隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—编码器的注意力权重元组，形状为`(batch_size, num_heads, sequence_length, sequence_length)`（每层一个）。用于计算自注意力头中的加权平均值的编码器的注意力 softmax 后的注意力权重。

+   `intermediate_hidden_states`（形状为`(config.decoder_layers, batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*，当`config.auxiliary_loss=True`时返回）—中间解码器激活，即每个解码器层的输出，每个都经过一个 layernorm。

DetrModel 的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, DetrModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
>>> model = DetrModel.from_pretrained("facebook/detr-resnet-50")

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**inputs)

>>> # the last hidden states are the final query embeddings of the Transformer decoder
>>> # these are of shape (batch_size, num_queries, hidden_size)
>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 100, 256]
```

## DetrForObjectDetection

### `class transformers.DetrForObjectDetection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1464)

```py
( config: DetrConfig )
```

参数

+   `config`（DetrConfig）—模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

DETR 模型（由骨干和编码器-解码器 Transformer 组成），顶部带有目标检测头，用于诸如 COCO 检测之类的任务。

这个模型继承自 PreTrainedModel。查看超类文档以获取库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1497)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.detr.modeling_detr.DetrObjectDetectionOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—像素值。默认情况下，如果提供填充，则将忽略填充。

    像素值可以使用 AutoImageProcessor 获取。有关详细信息，请参阅 DetrImageProcessor.`call`()。

+   `pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）—用于避免在填充像素值上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示真实像素（即`未屏蔽`），

    +   0 表示填充像素（即`屏蔽`）。

    什么是注意力掩码？

+   `decoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*) — 默认情况下不使用。可用于屏蔽对象查询。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组由(`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)组成，`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递图像的扁平化特征图（骨干网络+投影层的输出），而不是传递扁平化表示。

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是用零张量初始化查询。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `labels` (`List[Dict]` of len `(batch_size,)`, *optional*) — 用于计算二部匹配损失的标签。字典列表，每个字典至少包含以下 2 个键：'class_labels'和'boxes'（分别是批处理中图像的类别标签和边界框）。类别标签本身应该是长度为`(图像中边界框数量,)`的`torch.LongTensor`，而边界框应该是形状为`(图像中边界框数量, 4)`的`torch.FloatTensor`。

返回

transformers.models.detr.modeling_detr.DetrObjectDetectionOutput 或 `tuple(torch.FloatTensor)`

transformers.models.detr.modeling_detr.DetrObjectDetectionOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（DetrConfig）和输入。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)) — 作为类别预测的负对数似然（交叉熵）和边界框损失的线性组合的总损失。后者被定义为 L1 损失和广义尺度不变 IoU 损失的线性组合。

+   `loss_dict` (`Dict`, *optional*) — 包含各个损失的字典。用于记录日志。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) — 所有查询的分类 logits（包括无对象）。

+   `pred_boxes` (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) — 所有查询的归一化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未归一化的边界框。

+   `auxiliary_outputs` (`list[Dict]`, *optional*) — 可选，仅在激活辅助损失（即`config.auxiliary_loss`设置为`True`）并提供标签时返回。它是一个包含每个解码器层的上述两个键（`logits`和`pred_boxes`）的字典列表。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 解码器的隐藏状态元组，每层一个，形状为`(batch_size, sequence_length, hidden_size)`。每层解码器的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 解码器的注意力权重元组，每层一个，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 解码器的交叉注意力层的注意力权重元组，每层一个，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 编码器的隐藏状态元组，每层一个，形状为`(batch_size, sequence_length, hidden_size)`。每层编码器的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 编码器的注意力权重元组，每层一个，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

DetrForObjectDetection 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, DetrForObjectDetection
>>> import torch
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
>>> model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)

>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> target_sizes = torch.tensor([image.size[::-1]])
>>> results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[
...     0
... ]

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]
Detected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]
Detected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]
Detected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]
Detected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]
```

## DetrForSegmentation

### `class transformers.DetrForSegmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1637)

```py
( config: DetrConfig )
```

参数

+   `config` (DetrConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

DETR 模型（由骨干和编码器-解码器 Transformer 组成），顶部带有分割头，用于诸如 COCO 全景等任务。

此模型继承自 PreTrainedModel。查看超类文档以获取库实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/detr/modeling_detr.py#L1667)

```py
( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.detr.modeling_detr.DetrSegmentationOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。

    可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 DetrImageProcessor.`call`()。

+   `pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免在填充像素值上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于真实的像素（即`未被遮罩`），

    +   对于填充像素（即`遮罩`）的像素为 0。

    什么是注意力遮罩？

+   `decoder_attention_mask`（形状为`(batch_size, num_queries)`的`torch.FloatTensor`，*可选*）— 默认情况下不使用。可用于屏蔽对象查询。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递图像的扁平特征图（骨干+投影层的输出）而不是传递它。

+   `decoder_inputs_embeds`（形状为`(batch_size, num_queries, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示来初始化查询，而不是使用零张量。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

+   `labels`（长度为`(batch_size,)`的`List[Dict]`，*可选*）— 用于计算二部匹配损失、DICE/F-1 损失和 Focal 损失的标签。字典列表，每个字典至少包含以下 3 个键：‘class_labels’、‘boxes’和‘masks’（分别是批次中图像的类标签、边界框和分割掩码）。类标签本身应该是长度为`(图像中边界框的数量,)`的`torch.LongTensor`，边界框是形状为`(图像中边界框的数量, 4)`的`torch.FloatTensor`，掩码是形状为`(图像中边界框的数量, height, width)`的`torch.FloatTensor`。

返回

transformers.models.detr.modeling_detr.DetrSegmentationOutput 或`tuple(torch.FloatTensor)`

一个 transformers.models.detr.modeling_detr.DetrSegmentationOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（DetrConfig）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回） — 总损失，作为负对数似然（交叉熵）和边界框损失的线性组合。后者被定义为 L1 损失和广义尺度不变 IoU 损失的线性组合。

+   `loss_dict` (`Dict`，*optional*) — 包含各个损失的字典。用于记录。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, num_queries, num_classes + 1)`） — 所有查询的分类 logits（包括无对象）。

+   `pred_boxes` (`torch.FloatTensor`，形状为`(batch_size, num_queries, 4)`） — 所有查询的归一化框坐标，表示为（中心 _x，中心 _y，宽度，高度）。这些值在[0, 1]范围内归一化，相对于批处理中每个单独图像的大小（忽略可能的填充）。您可以使用 post_process_object_detection()来检索未归一化的边界框。

+   `pred_masks` (`torch.FloatTensor`，形状为`(batch_size, num_queries, height/4, width/4)`） — 所有查询的分割掩模 logits。另请参阅 post_process_semantic_segmentation()或 post_process_instance_segmentation()post_process_panoptic_segmentation()分别评估语义、实例和全景分割掩模。

+   `auxiliary_outputs` (`list[Dict]`, *optional*) — 当辅助损失被激活时（即`config.auxiliary_loss`设置为`True`）并且提供了标签时才返回。这是一个包含每个解码器层的两个上述键（`logits`和`pred_boxes`）的字典列表。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型解码器最后一层的隐藏状态序列。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。解码器在每个层的输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。编码器每一层的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

DetrForSegmentation 的前向方法重写了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import io
>>> import requests
>>> from PIL import Image
>>> import torch
>>> import numpy

>>> from transformers import AutoImageProcessor, DetrForSegmentation
>>> from transformers.image_transforms import rgb_to_id

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50-panoptic")
>>> model = DetrForSegmentation.from_pretrained("facebook/detr-resnet-50-panoptic")

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**inputs)

>>> # Use the `post_process_panoptic_segmentation` method of the `image_processor` to retrieve post-processed panoptic segmentation maps
>>> # Segmentation results are returned as a list of dictionaries
>>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[(300, 500)])

>>> # A tensor of shape (height, width) where each value denotes a segment id, filled with -1 if no segment is found
>>> panoptic_seg = result[0]["segmentation"]
>>> # Get prediction score and segment_id to class_id mapping of each segment
>>> panoptic_segments_info = result[0]["segments_info"]
```
