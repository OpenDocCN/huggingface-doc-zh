# LLaVa

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava)

## 概述

LLaVa 是通过在 GPT 生成的多模态指令遵循数据上进行 LlamA/Vicuna 微调而训练的开源聊天机器人。它是一种基于变压器架构的自回归语言模型。换句话说，它是为聊天/指令微调的 LLMs 的多模态版本。

LLaVa 模型最初在[视觉指导调整](https://arxiv.org/abs/2304.08485)中提出，并在[通过视觉指导调整改进基线](https://arxiv.org/pdf/2310.03744)中由 Haotian Liu、Chunyuan Li、Yuheng Li 和 Yong Jae Lee 改进。

论文摘要如下：

*最近，大型多模态模型（LMM）在视觉指导调整方面取得了令人鼓舞的进展。在这篇文章中，我们展示了 LLaVA 中的全连接视觉-语言跨模态连接器出人意料地强大且高效。通过对 LLaVA 进行简单修改，即使用 CLIP-ViT-L-336px 与 MLP 投影，并添加学术任务导向的 VQA 数据以及简单的响应格式提示，我们建立了更强的基线，实现了 11 个基准测试中的最新技术。我们的最终 13B 检查点仅使用了 120 万个公开可用的数据，并在单个 8-A100 节点上的约 1 天内完成了完整训练。我们希望这可以使最先进的 LMM 研究更易于访问。代码和模型将会公开发布*

![drawing](img/616d246acc70828a994631a4667a609e.png) LLaVa 架构。摘自[原始论文。](https://arxiv.org/abs/2304.08485)

该模型由[ArthurZ](https://huggingface.co/ArthurZ)和[ybelkada](https://huggingface.co/ybelkada)贡献。原始代码可以在[这里](https://github.com/haotian-liu/LLaVA/tree/main/llava)找到。

## 使用提示

+   我们建议用户在计算批量生成时使用`padding_side="left"`，因为这会导致更准确的结果。只需确保在生成之前调用`processor.tokenizer.padding_side = "left"`。

+   请注意，该模型尚未明确训练以处理同一提示中的多个图像，尽管从技术上讲这是可能的，但您可能会遇到不准确的结果。

+   为了获得更好的结果，我们建议用户使用正确的提示格式提示模型：

```py
"USER: <image>\n<prompt>ASSISTANT:"
```

对于多轮对话：

```py
"USER: <image>\n<prompt1>ASSISTANT: <answer1>USER: <prompt2>ASSISTANT: <answer2>USER: <prompt3>ASSISTANT:"
```

### 使用 Flash Attention 2

Flash Attention 2 是先前优化的更快、更优化的版本，请参阅[性能文档中的 Flash Attention 2 部分](https://huggingface.co/docs/transformers/perf_infer_gpu_one)。

## 资源

一份官方 Hugging Face 和社区（由🌎表示）资源列表，可帮助您开始使用 BEiT。

图像到文本

+   关于如何在免费的 Google Colab 实例上运行 Llava 的[Google Colab 演示](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing)，利用 4 位推理。

+   展示批量推理的[类似笔记本](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb)。🌎

## LlavaConfig

### `class transformers.LlavaConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/configuration_llava.py#L28)

```py
( vision_config = None text_config = None ignore_index = -100 image_token_index = 32000 projector_hidden_act = 'gelu' vision_feature_select_strategy = 'default' vision_feature_layer = -2 vocab_size = 32000 **kwargs )
```

参数

+   `vision_config`（`LlavaVisionConfig`，*可选*）— 自定义视觉配置或字典

+   `text_config`（`Union[AutoConfig, dict]`，*可选*）— 文本主干的配置对象。可以是`LlamaConfig`或`MistralConfig`之一。

+   `ignore_index`（`int`，*可选*，默认为-100）— 损失函数的忽略索引。

+   `image_token_index`（`int`，*可选*，默认为 32000）— 用于编码图像提示的图像标记索引。

+   `projector_hidden_act`（`str`，*可选*，默认为`"gelu"`）— 多模态投影器使用的激活函数。

+   `vision_feature_select_strategy` (`str`, *optional*, 默认为`"default"`) — 用于从 CLIP 骨干中选择视觉特征的特征选择策略。

+   `vision_feature_layer` (`int`, *optional*, 默认为-2) — 选择视觉特征的层的索引。

+   `vocab_size` (`int`, *optional*, 默认为 32000) — Llava 模型的词汇表大小。定义了在调用~LlavaForConditionalGeneration 时可以由`inputs_ids`表示的不同标记数量。

这是用于存储 LlavaForConditionalGeneration 配置的配置类。它用于根据指定的参数实例化一个 Llava 模型，定义模型架构。使用默认值实例化配置将产生类似于 Llava-9B 的配置。

例如 [llava-hf/llava-9b](https://huggingface.co/llava-hf/llava-9b)

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig

>>> # Initializing a CLIP-vision config
>>> vision_config = CLIPVisionConfig()

>>> # Initializing a Llama config
>>> text_config = LlamaConfig()

>>> # Initializing a Llava llava-1.5-7b style configuration
>>> configuration = LlavaConfig(vision_config, text_config)

>>> # Initializing a model from the llava-1.5-7b style configuration
>>> model = LlavaForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LlavaProcessor

### `class transformers.LlavaProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L29)

```py
( image_processor = None tokenizer = None )
```

参数

+   `image_processor` (CLIPImageProcessor, *optional*) — 图像处理器是必需的输入。

+   `tokenizer` (LlamaTokenizerFast, *optional*) — Tokenizer 是必需的输入。

构建一个 Llava 处理器，将 Llava 图像处理器和 Llava 分词器封装成一个单一处理器。

LlavaProcessor 提供了 CLIPImageProcessor 和 LlamaTokenizerFast 的所有功能。查看`__call__()`和 decode()以获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L116)

```py
( *args **kwargs )
```

此方法将所有参数转发给 LlamaTokenizerFast 的 batch_decode()。请参考此方法的文档字符串以获取更多信息。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L124)

```py
( *args **kwargs )
```

此方法将所有参数转发给 LlamaTokenizerFast 的 decode()。请参考此方法的文档字符串以获取更多信息。

## LlavaForConditionalGeneration

### `class transformers.LlavaForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L233)

```py
( config: LlavaConfig )
```

参数

+   `config`（LlavaConfig 或`LlavaVisionConfig`）—模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

LLAVA 模型由视觉主干和语言模型组成。此模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L348)

```py
( input_ids: LongTensor = None pixel_values: FloatTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None vision_feature_layer: Optional = None vision_feature_select_strategy: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）—词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将忽略填充。

    索引可以使用 AutoTokenizer 获得。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)) -- 输入图像对应的张量。像素值可以使用 AutoImageProcessor 获得。有关详细信息，请参阅 CLIPImageProcessor.__call__()（[]`LlavaProcessor`]使用 CLIPImageProcessor 来处理图像）。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）—用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

    索引可以使用 AutoTokenizer 获得。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    如果使用`past_key_values`，则只需输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

    如果要更改填充行为，应阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表 1。

    +   1 表示头部`未被掩码`，

    +   0 表示头部`被掩码`。

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。什么是位置 ID？

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，以及 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（这些没有将其过去的键值状态提供给此模型）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*） — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*） — 是否返回一个 ModelOutput 而不是一个普通元组。

    参数 — labels (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）：用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`范围内，或者为-100（请参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

返回

`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast` 或 `tuple(torch.FloatTensor)`

一个`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（LlavaConfig）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回） — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出 + 每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

+   `image_hidden_states` (`tuple(torch.FloatTensor)`, *可选的*) — 图像嵌入输出的元组`torch.FloatTensor`（形状为`(batch_size, num_images, sequence_length, hidden_size)`）。

    由视觉编码器产生的模型的图像隐藏状态，以及可选的由感知器产生的隐藏状态。

LlavaForConditionalGeneration 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

例如：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, LlavaForConditionalGeneration

>>> model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf")
>>> processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

>>> prompt = "<image>\nUSER: What's the content of the image?\nASSISTANT:"
>>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=prompt, images=image, return_tensors="pt")

>>> # Generate
>>> generate_ids = model.generate(**inputs, max_length=30)
>>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
"\nUSER: What's the content of the image?\nASSISTANT: The image features a stop sign on a street corner"
```
