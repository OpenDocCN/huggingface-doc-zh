# LLaVa

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llava)

## æ¦‚è¿°

LLaVa æ˜¯é€šè¿‡åœ¨ GPT ç”Ÿæˆçš„å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªæ•°æ®ä¸Šè¿›è¡Œ LlamA/Vicuna å¾®è°ƒè€Œè®­ç»ƒçš„å¼€æºèŠå¤©æœºå™¨äººã€‚å®ƒæ˜¯ä¸€ç§åŸºäºå˜å‹å™¨æ¶æ„çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯ä¸ºèŠå¤©/æŒ‡ä»¤å¾®è°ƒçš„ LLMs çš„å¤šæ¨¡æ€ç‰ˆæœ¬ã€‚

LLaVa æ¨¡å‹æœ€åˆåœ¨[è§†è§‰æŒ‡å¯¼è°ƒæ•´](https://arxiv.org/abs/2304.08485)ä¸­æå‡ºï¼Œå¹¶åœ¨[é€šè¿‡è§†è§‰æŒ‡å¯¼è°ƒæ•´æ”¹è¿›åŸºçº¿](https://arxiv.org/pdf/2310.03744)ä¸­ç”± Haotian Liuã€Chunyuan Liã€Yuheng Li å’Œ Yong Jae Lee æ”¹è¿›ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æœ€è¿‘ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨è§†è§‰æŒ‡å¯¼è°ƒæ•´æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº† LLaVA ä¸­çš„å…¨è¿æ¥è§†è§‰-è¯­è¨€è·¨æ¨¡æ€è¿æ¥å™¨å‡ºäººæ„æ–™åœ°å¼ºå¤§ä¸”é«˜æ•ˆã€‚é€šè¿‡å¯¹ LLaVA è¿›è¡Œç®€å•ä¿®æ”¹ï¼Œå³ä½¿ç”¨ CLIP-ViT-L-336px ä¸ MLP æŠ•å½±ï¼Œå¹¶æ·»åŠ å­¦æœ¯ä»»åŠ¡å¯¼å‘çš„ VQA æ•°æ®ä»¥åŠç®€å•çš„å“åº”æ ¼å¼æç¤ºï¼Œæˆ‘ä»¬å»ºç«‹äº†æ›´å¼ºçš„åŸºçº¿ï¼Œå®ç°äº† 11 ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æœ€æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æœ€ç»ˆ 13B æ£€æŸ¥ç‚¹ä»…ä½¿ç”¨äº† 120 ä¸‡ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®ï¼Œå¹¶åœ¨å•ä¸ª 8-A100 èŠ‚ç‚¹ä¸Šçš„çº¦ 1 å¤©å†…å®Œæˆäº†å®Œæ•´è®­ç»ƒã€‚æˆ‘ä»¬å¸Œæœ›è¿™å¯ä»¥ä½¿æœ€å…ˆè¿›çš„ LMM ç ”ç©¶æ›´æ˜“äºè®¿é—®ã€‚ä»£ç å’Œæ¨¡å‹å°†ä¼šå…¬å¼€å‘å¸ƒ*

![drawing](img/616d246acc70828a994631a4667a609e.png) LLaVa æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡ã€‚](https://arxiv.org/abs/2304.08485)

è¯¥æ¨¡å‹ç”±[ArthurZ](https://huggingface.co/ArthurZ)å’Œ[ybelkada](https://huggingface.co/ybelkada)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/haotian-liu/LLaVA/tree/main/llava)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   æˆ‘ä»¬å»ºè®®ç”¨æˆ·åœ¨è®¡ç®—æ‰¹é‡ç”Ÿæˆæ—¶ä½¿ç”¨`padding_side="left"`ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´æ›´å‡†ç¡®çš„ç»“æœã€‚åªéœ€ç¡®ä¿åœ¨ç”Ÿæˆä¹‹å‰è°ƒç”¨`processor.tokenizer.padding_side = "left"`ã€‚

+   è¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹å°šæœªæ˜ç¡®è®­ç»ƒä»¥å¤„ç†åŒä¸€æç¤ºä¸­çš„å¤šä¸ªå›¾åƒï¼Œå°½ç®¡ä»æŠ€æœ¯ä¸Šè®²è¿™æ˜¯å¯èƒ½çš„ï¼Œä½†æ‚¨å¯èƒ½ä¼šé‡åˆ°ä¸å‡†ç¡®çš„ç»“æœã€‚

+   ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨æ­£ç¡®çš„æç¤ºæ ¼å¼æç¤ºæ¨¡å‹ï¼š

```py
"USER: <image>\n<prompt>ASSISTANT:"
```

å¯¹äºå¤šè½®å¯¹è¯ï¼š

```py
"USER: <image>\n<prompt1>ASSISTANT: <answer1>USER: <prompt2>ASSISTANT: <answer2>USER: <prompt3>ASSISTANT:"
```

### ä½¿ç”¨ Flash Attention 2

Flash Attention 2 æ˜¯å…ˆå‰ä¼˜åŒ–çš„æ›´å¿«ã€æ›´ä¼˜åŒ–çš„ç‰ˆæœ¬ï¼Œè¯·å‚é˜…[æ€§èƒ½æ–‡æ¡£ä¸­çš„ Flash Attention 2 éƒ¨åˆ†](https://huggingface.co/docs/transformers/perf_infer_gpu_one)ã€‚

## èµ„æº

ä¸€ä»½å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ BEiTã€‚

å›¾åƒåˆ°æ–‡æœ¬

+   å…³äºå¦‚ä½•åœ¨å…è´¹çš„ Google Colab å®ä¾‹ä¸Šè¿è¡Œ Llava çš„[Google Colab æ¼”ç¤º](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing)ï¼Œåˆ©ç”¨ 4 ä½æ¨ç†ã€‚

+   å±•ç¤ºæ‰¹é‡æ¨ç†çš„[ç±»ä¼¼ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb)ã€‚ğŸŒ

## LlavaConfig

### `class transformers.LlavaConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/configuration_llava.py#L28)

```py
( vision_config = None text_config = None ignore_index = -100 image_token_index = 32000 projector_hidden_act = 'gelu' vision_feature_select_strategy = 'default' vision_feature_layer = -2 vocab_size = 32000 **kwargs )
```

å‚æ•°

+   `vision_config`ï¼ˆ`LlavaVisionConfig`ï¼Œ*å¯é€‰*ï¼‰â€” è‡ªå®šä¹‰è§†è§‰é…ç½®æˆ–å­—å…¸

+   `text_config`ï¼ˆ`Union[AutoConfig, dict]`ï¼Œ*å¯é€‰*ï¼‰â€” æ–‡æœ¬ä¸»å¹²çš„é…ç½®å¯¹è±¡ã€‚å¯ä»¥æ˜¯`LlamaConfig`æˆ–`MistralConfig`ä¹‹ä¸€ã€‚

+   `ignore_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º-100ï¼‰â€” æŸå¤±å‡½æ•°çš„å¿½ç•¥ç´¢å¼•ã€‚

+   `image_token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 32000ï¼‰â€” ç”¨äºç¼–ç å›¾åƒæç¤ºçš„å›¾åƒæ ‡è®°ç´¢å¼•ã€‚

+   `projector_hidden_act`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"gelu"`ï¼‰â€” å¤šæ¨¡æ€æŠ•å½±å™¨ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚

+   `vision_feature_select_strategy` (`str`, *optional*, é»˜è®¤ä¸º`"default"`) â€” ç”¨äºä» CLIP éª¨å¹²ä¸­é€‰æ‹©è§†è§‰ç‰¹å¾çš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚

+   `vision_feature_layer` (`int`, *optional*, é»˜è®¤ä¸º-2) â€” é€‰æ‹©è§†è§‰ç‰¹å¾çš„å±‚çš„ç´¢å¼•ã€‚

+   `vocab_size` (`int`, *optional*, é»˜è®¤ä¸º 32000) â€” Llava æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨~LlavaForConditionalGeneration æ—¶å¯ä»¥ç”±`inputs_ids`è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ LlavaForConditionalGeneration é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª Llava æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Llava-9B çš„é…ç½®ã€‚

ä¾‹å¦‚ [llava-hf/llava-9b](https://huggingface.co/llava-hf/llava-9b)

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig

>>> # Initializing a CLIP-vision config
>>> vision_config = CLIPVisionConfig()

>>> # Initializing a Llama config
>>> text_config = LlamaConfig()

>>> # Initializing a Llava llava-1.5-7b style configuration
>>> configuration = LlavaConfig(vision_config, text_config)

>>> # Initializing a model from the llava-1.5-7b style configuration
>>> model = LlavaForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LlavaProcessor

### `class transformers.LlavaProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L29)

```py
( image_processor = None tokenizer = None )
```

å‚æ•°

+   `image_processor` (CLIPImageProcessor, *optional*) â€” å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer` (LlamaTokenizerFast, *optional*) â€” Tokenizer æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ª Llava å¤„ç†å™¨ï¼Œå°† Llava å›¾åƒå¤„ç†å™¨å’Œ Llava åˆ†è¯å™¨å°è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚

LlavaProcessor æä¾›äº† CLIPImageProcessor å’Œ LlamaTokenizerFast çš„æ‰€æœ‰åŠŸèƒ½ã€‚æŸ¥çœ‹`__call__()`å’Œ decode()ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L116)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ LlamaTokenizerFast çš„ batch_decode()ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/processing_llava.py#L124)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘ç»™ LlamaTokenizerFast çš„ decode()ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## LlavaForConditionalGeneration

### `class transformers.LlavaForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L233)

```py
( config: LlavaConfig )
```

å‚æ•°

+   `config`ï¼ˆLlavaConfig æˆ–`LlavaVisionConfig`ï¼‰â€”æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

LLAVA æ¨¡å‹ç”±è§†è§‰ä¸»å¹²å’Œè¯­è¨€æ¨¡å‹ç»„æˆã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llava/modeling_llava.py#L348)

```py
( input_ids: LongTensor = None pixel_values: FloatTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None vision_feature_layer: Optional = None vision_feature_select_strategy: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€”è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    ç´¢å¼•å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)) -- è¾“å…¥å›¾åƒå¯¹åº”çš„å¼ é‡ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… CLIPImageProcessor.__call__()ï¼ˆ[]`LlavaProcessor`]ä½¿ç”¨ CLIPImageProcessor æ¥å¤„ç†å›¾åƒï¼‰ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

    ç´¢å¼•å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç `ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.n_positions - 1]`ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œä»¥åŠ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰ â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

    å‚æ•° â€” labels (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰ï¼šç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆè¯·å‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…çš„æ ‡è®°ã€‚

è¿”å›

`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast` æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆLlavaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–è€…`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `image_hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*) â€” å›¾åƒåµŒå…¥è¾“å‡ºçš„å…ƒç»„`torch.FloatTensor`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_images, sequence_length, hidden_size)`ï¼‰ã€‚

    ç”±è§†è§‰ç¼–ç å™¨äº§ç”Ÿçš„æ¨¡å‹çš„å›¾åƒéšè—çŠ¶æ€ï¼Œä»¥åŠå¯é€‰çš„ç”±æ„ŸçŸ¥å™¨äº§ç”Ÿçš„éšè—çŠ¶æ€ã€‚

LlavaForConditionalGeneration çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ä¾‹å¦‚ï¼š

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, LlavaForConditionalGeneration

>>> model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf")
>>> processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

>>> prompt = "<image>\nUSER: What's the content of the image?\nASSISTANT:"
>>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=prompt, images=image, return_tensors="pt")

>>> # Generate
>>> generate_ids = model.generate(**inputs, max_length=30)
>>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
"\nUSER: What's the content of the image?\nASSISTANT: The image features a stop sign on a street corner"
```
