# è§†é¢‘åˆ†ç±»

> åŽŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/tasks/video_classification`](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/video_classification)

è§†é¢‘åˆ†ç±»æ˜¯å°†æ ‡ç­¾æˆ–ç±»åˆ«åˆ†é…ç»™æ•´ä¸ªè§†é¢‘çš„ä»»åŠ¡ã€‚é¢„æœŸæ¯ä¸ªè§†é¢‘åªæœ‰ä¸€ä¸ªç±»åˆ«ã€‚è§†é¢‘åˆ†ç±»æ¨¡åž‹å°†è§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›žå…³äºŽè§†é¢‘å±žäºŽå“ªä¸ªç±»åˆ«çš„é¢„æµ‹ã€‚è¿™äº›æ¨¡åž‹å¯ç”¨äºŽå¯¹è§†é¢‘å†…å®¹è¿›è¡Œåˆ†ç±»ã€‚è§†é¢‘åˆ†ç±»çš„çŽ°å®žåº”ç”¨æ˜¯åŠ¨ä½œ/æ´»åŠ¨è¯†åˆ«ï¼Œå¯¹äºŽå¥èº«åº”ç”¨éžå¸¸æœ‰ç”¨ã€‚å¯¹äºŽè§†åŠ›å—æŸçš„ä¸ªä½“ï¼Œå°¤å…¶æ˜¯åœ¨é€šå‹¤æ—¶ï¼Œè¿™ä¹Ÿæ˜¯æœ‰å¸®åŠ©çš„ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1.  åœ¨[UCF101](https://www.crcv.ucf.edu/data/UCF101.php)æ•°æ®é›†çš„å­é›†ä¸Šå¯¹[VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae)è¿›è¡Œå¾®è°ƒã€‚

1.  ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡åž‹è¿›è¡ŒæŽ¨æ–­ã€‚

æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡åž‹æž¶æž„æ”¯æŒï¼š

TimeSformer, VideoMAE, ViViT

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```py
pip install -q pytorchvideo transformers evaluate
```

æ‚¨å°†ä½¿ç”¨[PyTorchVideo](https://pytorchvideo.org/)ï¼ˆç§°ä¸º`pytorchvideo`ï¼‰æ¥å¤„ç†å’Œå‡†å¤‡è§†é¢‘ã€‚

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œä¸Žç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡åž‹ã€‚æç¤ºæ—¶ï¼Œè¯·è¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ UCF101 æ•°æ®é›†

é¦–å…ˆåŠ è½½[UCF-101 æ•°æ®é›†](https://www.crcv.ucf.edu/data/UCF101.php)çš„å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®žéªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åŽå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

```py
>>> from huggingface_hub import hf_hub_download

>>> hf_dataset_identifier = "sayakpaul/ucf101-subset"
>>> filename = "UCF101_subset.tar.gz"
>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type="dataset")
```

åœ¨ä¸‹è½½å­é›†åŽï¼Œæ‚¨éœ€è¦æå–åŽ‹ç¼©å­˜æ¡£ï¼š

```py
>>> import tarfile

>>> with tarfile.open(file_path) as t:
...      t.extractall(".")
```

åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œæ•°æ®é›†çš„ç»„ç»‡æ–¹å¼å¦‚ä¸‹ï¼š

```py
UCF101_subset/
    train/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    val/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    test/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
```

ï¼ˆæŽ’åºåŽçš„ï¼‰è§†é¢‘è·¯å¾„çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

```py
...
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'
...
```

æ‚¨ä¼šæ³¨æ„åˆ°æœ‰å±žäºŽåŒä¸€ç»„/åœºæ™¯çš„è§†é¢‘ç‰‡æ®µï¼Œå…¶ä¸­ç»„åœ¨è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸­ç”¨`g`è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œ`v_ApplyEyeMakeup_g07_c04.avi`å’Œ`v_ApplyEyeMakeup_g07_c06.avi`ã€‚

å¯¹äºŽéªŒè¯å’Œè¯„ä¼°æ‹†åˆ†ï¼Œæ‚¨ä¸å¸Œæœ›ä»ŽåŒä¸€ç»„/åœºæ™¯ä¸­èŽ·å–è§†é¢‘ç‰‡æ®µï¼Œä»¥é˜²æ­¢[æ•°æ®æ³„æ¼](https://www.kaggle.com/code/alexisbcook/data-leakage)ã€‚æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„å­é›†è€ƒè™‘äº†è¿™äº›ä¿¡æ¯ã€‚

æŽ¥ä¸‹æ¥ï¼Œæ‚¨å°†æŽ¨å¯¼æ•°æ®é›†ä¸­å­˜åœ¨çš„æ ‡ç­¾é›†ã€‚è¿˜è¦åˆ›å»ºä¸¤ä¸ªåœ¨åˆå§‹åŒ–æ¨¡åž‹æ—¶æœ‰ç”¨çš„å­—å…¸ï¼š

+   `label2id`ï¼šå°†ç±»åæ˜ å°„åˆ°æ•´æ•°ã€‚

+   `id2label`ï¼šå°†æ•´æ•°æ˜ å°„åˆ°ç±»åã€‚

```py
>>> class_labels = sorted({str(path).split("/")[2] for path in all_video_file_paths})
>>> label2id = {label: i for i, label in enumerate(class_labels)}
>>> id2label = {i: label for label, i in label2id.items()}

>>> print(f"Unique classes: {list(label2id.keys())}.")

# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].
```

æœ‰ 10 ä¸ªç‹¬ç‰¹çš„ç±»åˆ«ã€‚æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†ä¸­æœ‰ 30 ä¸ªè§†é¢‘ã€‚

## åŠ è½½ä¸€ä¸ªæ¨¡åž‹è¿›è¡Œå¾®è°ƒ

ä»Žé¢„è®­ç»ƒçš„æ£€æŸ¥ç‚¹å’Œå…¶å…³è”çš„å›¾åƒå¤„ç†å™¨å®žä¾‹åŒ–ä¸€ä¸ªè§†é¢‘åˆ†ç±»æ¨¡åž‹ã€‚æ¨¡åž‹çš„ç¼–ç å™¨å¸¦æœ‰é¢„è®­ç»ƒå‚æ•°ï¼Œåˆ†ç±»å¤´æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚å½“ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ç¼–å†™é¢„å¤„ç†æµæ°´çº¿æ—¶ï¼Œå›¾åƒå¤„ç†å™¨ä¼šæ´¾ä¸Šç”¨åœºã€‚

```py
>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification

>>> model_ckpt = "MCG-NJU/videomae-base"
>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
>>> model = VideoMAEForVideoClassification.from_pretrained(
...     model_ckpt,
...     label2id=label2id,
...     id2label=id2label,
...     ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
... )
```

å½“æ¨¡åž‹åŠ è½½æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ä»¥ä¸‹è­¦å‘Šï¼š

```py
Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']
- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

è­¦å‘Šå‘Šè¯‰æˆ‘ä»¬ï¼Œæˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆä¾‹å¦‚`classifier`å±‚çš„æƒé‡å’Œåå·®ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡å’Œåå·®ï¼ˆæ–°`classifier`å±‚çš„æƒé‡å’Œåå·®ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨æ·»åŠ ä¸€ä¸ªæ–°çš„å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“è­¦å‘Šæˆ‘ä»¬åœ¨ä½¿ç”¨å®ƒè¿›è¡ŒæŽ¨æ–­ä¹‹å‰åº”è¯¥å¾®è°ƒè¿™ä¸ªæ¨¡åž‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„ã€‚

**è¯·æ³¨æ„**ï¼Œ[æ­¤æ£€æŸ¥ç‚¹](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨çŽ°æ›´å¥½ï¼Œå› ä¸ºè¯¥æ£€æŸ¥ç‚¹æ˜¯åœ¨ä¸€ä¸ªå…·æœ‰ç›¸å½“å¤§é¢†åŸŸé‡å çš„ç±»ä¼¼ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒå¾—åˆ°çš„ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹[æ­¤æ£€æŸ¥ç‚¹](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)ï¼Œè¯¥æ£€æŸ¥ç‚¹æ˜¯é€šè¿‡å¾®è°ƒ`MCG-NJU/videomae-base-finetuned-kinetics`èŽ·å¾—çš„ã€‚

## ä¸ºè®­ç»ƒå‡†å¤‡æ•°æ®é›†

ä¸ºäº†å¯¹è§†é¢‘è¿›è¡Œé¢„å¤„ç†ï¼Œæ‚¨å°†åˆ©ç”¨[PyTorchVideo åº“](https://pytorchvideo.org/)ã€‚é¦–å…ˆå¯¼å…¥æˆ‘ä»¬éœ€è¦çš„ä¾èµ–é¡¹ã€‚

```py
>>> import pytorchvideo.data

>>> from pytorchvideo.transforms import (
...     ApplyTransformToKey,
...     Normalize,
...     RandomShortSideScale,
...     RemoveKey,
...     ShortSideScale,
...     UniformTemporalSubsample,
... )

>>> from torchvision.transforms import (
...     Compose,
...     Lambda,
...     RandomCrop,
...     RandomHorizontalFlip,
...     Resize,
... )
```

å¯¹äºŽè®­ç»ƒæ•°æ®é›†çš„è½¬æ¢ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´å­é‡‡æ ·ã€åƒç´ å½’ä¸€åŒ–ã€éšæœºè£å‰ªå’Œéšæœºæ°´å¹³ç¿»è½¬çš„ç»„åˆã€‚å¯¹äºŽéªŒè¯å’Œè¯„ä¼°æ•°æ®é›†çš„è½¬æ¢ï¼Œä¿æŒç›¸åŒçš„è½¬æ¢é“¾ï¼Œé™¤äº†éšæœºè£å‰ªå’Œæ°´å¹³ç¿»è½¬ã€‚è¦äº†è§£è¿™äº›è½¬æ¢çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[PyTorchVideo çš„å®˜æ–¹æ–‡æ¡£](https://pytorchvideo.org)ã€‚

ä½¿ç”¨ä¸Žé¢„è®­ç»ƒæ¨¡åž‹ç›¸å…³è”çš„`image_processor`æ¥èŽ·å–ä»¥ä¸‹ä¿¡æ¯ï¼š

+   ç”¨äºŽå½’ä¸€åŒ–è§†é¢‘å¸§åƒç´ çš„å›¾åƒå‡å€¼å’Œæ ‡å‡†å·®ã€‚

+   å°†è§†é¢‘å¸§è°ƒæ•´ä¸ºçš„ç©ºé—´åˆ†è¾¨çŽ‡ã€‚

é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ã€‚

```py
>>> mean = image_processor.image_mean
>>> std = image_processor.image_std
>>> if "shortest_edge" in image_processor.size:
...     height = width = image_processor.size["shortest_edge"]
>>> else:
...     height = image_processor.size["height"]
...     width = image_processor.size["width"]
>>> resize_to = (height, width)

>>> num_frames_to_sample = model.config.num_frames
>>> sample_rate = 4
>>> fps = 30
>>> clip_duration = num_frames_to_sample * sample_rate / fps
```

çŽ°åœ¨ï¼Œåˆ†åˆ«å®šä¹‰æ•°æ®é›†ç‰¹å®šçš„è½¬æ¢å’Œæ•°æ®é›†ã€‚ä»Žè®­ç»ƒé›†å¼€å§‹ï¼š

```py
>>> train_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     RandomShortSideScale(min_size=256, max_size=320),
...                     RandomCrop(resize_to),
...                     RandomHorizontalFlip(p=0.5),
...                 ]
...             ),
...         ),
...     ]
... )

>>> train_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "train"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
...     decode_audio=False,
...     transform=train_transform,
... )
```

ç›¸åŒçš„å·¥ä½œæµç¨‹é¡ºåºå¯ä»¥åº”ç”¨äºŽéªŒè¯é›†å’Œè¯„ä¼°é›†ï¼š

```py
>>> val_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     Resize(resize_to),
...                 ]
...             ),
...         ),
...     ]
... )

>>> val_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "val"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )

>>> test_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "test"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )
```

**æ³¨æ„**ï¼šä¸Šè¿°æ•°æ®é›†ç®¡é“å–è‡ª[å®˜æ–¹ PyTorchVideo ç¤ºä¾‹](https://pytorchvideo.org/docs/tutorial_classification#dataset)ã€‚æˆ‘ä»¬ä½¿ç”¨[`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101)å‡½æ•°ï¼Œå› ä¸ºå®ƒä¸“ä¸º UCF-101 æ•°æ®é›†å®šåˆ¶ã€‚åœ¨å†…éƒ¨ï¼Œå®ƒè¿”å›žä¸€ä¸ª[`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset)å¯¹è±¡ã€‚`LabeledVideoDataset`ç±»æ˜¯ PyTorchVideo æ•°æ®é›†ä¸­æ‰€æœ‰è§†é¢‘ç›¸å…³å†…å®¹çš„åŸºç±»ã€‚å› æ­¤ï¼Œå¦‚æžœæ‚¨æƒ³ä½¿ç”¨ PyTorchVideo ä¸æ”¯æŒçš„è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¯ä»¥ç›¸åº”åœ°æ‰©å±•`LabeledVideoDataset`ç±»ã€‚è¯·å‚è€ƒ`data` API [æ–‡æ¡£](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)ä»¥äº†è§£æ›´å¤šã€‚æ­¤å¤–ï¼Œå¦‚æžœæ‚¨çš„æ•°æ®é›†éµå¾ªç±»ä¼¼çš„ç»“æž„ï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰ï¼Œé‚£ä¹ˆä½¿ç”¨`pytorchvideo.data.Ucf101()`åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚

æ‚¨å¯ä»¥è®¿é—®`num_videos`å‚æ•°ä»¥äº†è§£æ•°æ®é›†ä¸­çš„è§†é¢‘æ•°é‡ã€‚

```py
>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)
# (300, 30, 75)
```

## å¯è§†åŒ–é¢„å¤„ç†åŽçš„è§†é¢‘ä»¥è¿›è¡Œæ›´å¥½çš„è°ƒè¯•

```py
>>> import imageio
>>> import numpy as np
>>> from IPython.display import Image

>>> def unnormalize_img(img):
...     """Un-normalizes the image pixels."""
...     img = (img * std) + mean
...     img = (img * 255).astype("uint8")
...     return img.clip(0, 255)

>>> def create_gif(video_tensor, filename="sample.gif"):
...     """Prepares a GIF from a video tensor.
...     
...     The video tensor is expected to have the following shape:
...     (num_frames, num_channels, height, width).
...     """
...     frames = []
...     for video_frame in video_tensor:
...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())
...         frames.append(frame_unnormalized)
...     kargs = {"duration": 0.25}
...     imageio.mimsave(filename, frames, "GIF", **kargs)
...     return filename

>>> def display_gif(video_tensor, gif_name="sample.gif"):
...     """Prepares and displays a GIF from a video tensor."""
...     video_tensor = video_tensor.permute(1, 0, 2, 3)
...     gif_filename = create_gif(video_tensor, gif_name)
...     return Image(filename=gif_filename)

>>> sample_video = next(iter(train_dataset))
>>> video_tensor = sample_video["video"]
>>> display_gif(video_tensor)
```

![æ‰“ç¯®çƒçš„äºº](img/094cb43675960282973ed2c77587e204.png)

## è®­ç»ƒæ¨¡åž‹

åˆ©ç”¨ðŸ¤— Transformers ä¸­çš„[`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer)æ¥è®­ç»ƒæ¨¡åž‹ã€‚è¦å®žä¾‹åŒ–ä¸€ä¸ª`Trainer`ï¼Œæ‚¨éœ€è¦å®šä¹‰è®­ç»ƒé…ç½®å’Œä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚æœ€é‡è¦çš„æ˜¯[`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰å±žæ€§ä»¥é…ç½®è®­ç»ƒçš„ç±»ã€‚å®ƒéœ€è¦ä¸€ä¸ªè¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼Œç”¨äºŽä¿å­˜æ¨¡åž‹çš„æ£€æŸ¥ç‚¹ã€‚å®ƒè¿˜æœ‰åŠ©äºŽå°†æ¨¡åž‹å­˜å‚¨åº“ä¸­çš„æ‰€æœ‰ä¿¡æ¯åŒæ­¥åˆ°ðŸ¤— Hub ä¸­ã€‚

å¤§å¤šæ•°è®­ç»ƒå‚æ•°éƒ½æ˜¯ä¸è¨€è‡ªæ˜Žçš„ï¼Œä½†è¿™é‡Œæœ‰ä¸€ä¸ªéžå¸¸é‡è¦çš„å‚æ•°æ˜¯`remove_unused_columns=False`ã€‚è¿™ä¸ªå‚æ•°å°†åˆ é™¤æ¨¡åž‹è°ƒç”¨å‡½æ•°æœªä½¿ç”¨çš„ä»»ä½•ç‰¹å¾ã€‚é»˜è®¤æƒ…å†µä¸‹æ˜¯`True`ï¼Œå› ä¸ºé€šå¸¸æœ€å¥½åˆ é™¤æœªä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼Œè¿™æ ·æ›´å®¹æ˜“å°†è¾“å…¥è§£åŽ‹ç¼©åˆ°æ¨¡åž‹çš„è°ƒç”¨å‡½æ•°ä¸­ã€‚ä½†æ˜¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦æœªä½¿ç”¨çš„ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯â€˜videoâ€™ï¼‰ä»¥ä¾¿åˆ›å»º`pixel_values`ï¼ˆè¿™æ˜¯æˆ‘ä»¬çš„æ¨¡åž‹åœ¨è¾“å…¥ä¸­æœŸæœ›çš„ä¸€ä¸ªå¿…éœ€é”®ï¼‰ã€‚

```py
>>> from transformers import TrainingArguments, Trainer

>>> model_name = model_ckpt.split("/")[-1]
>>> new_model_name = f"{model_name}-finetuned-ucf101-subset"
>>> num_epochs = 4

>>> args = TrainingArguments(
...     new_model_name,
...     remove_unused_columns=False,
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=5e-5,
...     per_device_train_batch_size=batch_size,
...     per_device_eval_batch_size=batch_size,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,
... )
```

`pytorchvideo.data.Ucf101()`è¿”å›žçš„æ•°æ®é›†æ²¡æœ‰å®žçŽ°`__len__`æ–¹æ³•ã€‚å› æ­¤ï¼Œåœ¨å®žä¾‹åŒ–`TrainingArguments`æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»å®šä¹‰`max_steps`ã€‚

æŽ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—ä»Žé¢„æµ‹ä¸­å¾—å‡ºçš„æŒ‡æ ‡ï¼Œè¯¥å‡½æ•°å°†ä½¿ç”¨æ‚¨çŽ°åœ¨å°†åŠ è½½çš„`metric`ã€‚æ‚¨å”¯ä¸€éœ€è¦åšçš„é¢„å¤„ç†æ˜¯å–å‡ºæˆ‘ä»¬é¢„æµ‹çš„ logits çš„ argmaxï¼š

```py
import evaluate

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```

**å…³äºŽè¯„ä¼°çš„è¯´æ˜Ž**ï¼š

åœ¨[VideoMAE è®ºæ–‡](https://arxiv.org/abs/2203.12602)ä¸­ï¼Œä½œè€…ä½¿ç”¨ä»¥ä¸‹è¯„ä¼°ç­–ç•¥ã€‚ä»–ä»¬åœ¨æµ‹è¯•è§†é¢‘çš„å‡ ä¸ªå‰ªè¾‘ä¸Šè¯„ä¼°æ¨¡åž‹ï¼Œå¹¶å¯¹è¿™äº›å‰ªè¾‘åº”ç”¨ä¸åŒçš„è£å‰ªï¼Œå¹¶æŠ¥å‘Šèšåˆå¾—åˆ†ã€‚ç„¶è€Œï¼Œå‡ºäºŽç®€å•å’Œç®€æ´çš„è€ƒè™‘ï¼Œæˆ‘ä»¬åœ¨æœ¬æ•™ç¨‹ä¸­ä¸è€ƒè™‘è¿™ä¸€ç‚¹ã€‚

æ­¤å¤–ï¼Œå®šä¹‰ä¸€ä¸ª`collate_fn`ï¼Œç”¨äºŽå°†ç¤ºä¾‹æ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚æ¯ä¸ªæ‰¹æ¬¡åŒ…æ‹¬ 2 ä¸ªé”®ï¼Œå³`pixel_values`å’Œ`labels`ã€‚

```py
>>> def collate_fn(examples):
...     # permute to (num_frames, num_channels, height, width)
...     pixel_values = torch.stack(
...         [example["video"].permute(1, 0, 2, 3) for example in examples]
...     )
...     labels = torch.tensor([example["label"] for example in examples])
...     return {"pixel_values": pixel_values, "labels": labels}
```

ç„¶åŽï¼Œå°†æ‰€æœ‰è¿™äº›ä¸Žæ•°æ®é›†ä¸€èµ·ä¼ é€’ç»™`Trainer`ï¼š

```py
>>> trainer = Trainer(
...     model,
...     args,
...     train_dataset=train_dataset,
...     eval_dataset=val_dataset,
...     tokenizer=image_processor,
...     compute_metrics=compute_metrics,
...     data_collator=collate_fn,
... )
```

æ‚¨å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆåœ¨é¢„å¤„ç†æ•°æ®æ—¶å°†`image_processor`ä½œä¸ºæ ‡è®°å™¨ä¼ é€’ã€‚è¿™åªæ˜¯ä¸ºäº†ç¡®ä¿å›¾åƒå¤„ç†å™¨é…ç½®æ–‡ä»¶ï¼ˆå­˜å‚¨ä¸º JSONï¼‰ä¹Ÿå°†ä¸Šä¼ åˆ° Hub ä¸Šçš„å­˜å‚¨åº“ä¸­ã€‚

çŽ°åœ¨é€šè¿‡è°ƒç”¨`train`æ–¹æ³•å¯¹æˆ‘ä»¬çš„æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼š

```py
>>> train_results = trainer.train()
```

è®­ç»ƒå®ŒæˆåŽï¼Œä½¿ç”¨ push_to_hub()æ–¹æ³•å°†æ‚¨çš„æ¨¡åž‹å…±äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡åž‹ï¼š

```py
>>> trainer.push_to_hub()
```

## æŽ¨æ–­

å¾ˆå¥½ï¼ŒçŽ°åœ¨æ‚¨å·²ç»å¯¹æ¨¡åž‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥å°†å…¶ç”¨äºŽæŽ¨æ–­ï¼

åŠ è½½è§†é¢‘è¿›è¡ŒæŽ¨æ–­ï¼š

```py
>>> sample_test_video = next(iter(test_dataset))
```

![ç¯®çƒæ¯”èµ›çš„é˜Ÿä¼](img/27c05b85bfaaba1e373898da43772d52.png)

å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡åž‹è¿›è¡ŒæŽ¨æ–­çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline)ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨æ‚¨çš„æ¨¡åž‹å®žä¾‹åŒ–ä¸€ä¸ªè§†é¢‘åˆ†ç±»çš„`pipeline`ï¼Œå¹¶å°†è§†é¢‘ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> video_cls = pipeline(model="my_awesome_video_cls_model")
>>> video_cls("https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi")
[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},
 {'score': 0.017777055501937866, 'label': 'BabyCrawling'},
 {'score': 0.01663011871278286, 'label': 'BalanceBeam'},
 {'score': 0.009560945443809032, 'label': 'BandMarching'},
 {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]
```

å¦‚æžœæ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æžœã€‚

```py
>>> def run_inference(model, video):
...     # (num_frames, num_channels, height, width)
...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)
...     inputs = {
...         "pixel_values": perumuted_sample_test_video.unsqueeze(0),
...         "labels": torch.tensor(
...             [sample_test_video["label"]]
...         ),  # this can be skipped if you don't have labels available.
...     }

...     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
...     inputs = {k: v.to(device) for k, v in inputs.items()}
...     model = model.to(device)

...     # forward pass
...     with torch.no_grad():
...         outputs = model(**inputs)
...         logits = outputs.logits

...     return logits
```

çŽ°åœ¨ï¼Œå°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡åž‹å¹¶è¿”å›ž`logits`ï¼š

```py
>>> logits = run_inference(trained_model, sample_test_video["video"])
```

è§£ç `logits`ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

```py
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
# Predicted class: BasketballDunk
```
