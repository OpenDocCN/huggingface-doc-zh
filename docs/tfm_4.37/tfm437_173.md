# Fuyu

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/fuyu`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/fuyu)

## 概述

Fuyu 模型由[ADEPT](https://www.adept.ai/blog/fuyu-8b)创建，作者是 Rohan Bavishi、Erich Elsen、Curtis Hawthorne、Maxwell Nye、Augustus Odena、Arushi Somani、Sağnak Taşırlar。

作者介绍了 Fuyu-8B，这是一个仅解码器的基于经典 transformers 架构的多模态模型，具有查询和键规范化。线性编码器被添加以从图像输入创建多模态嵌入。

通过将图像标记视为文本标记，并使用特殊的图像换行符，模型知道图像行何时结束。移除了图像位置嵌入。这避免了为各种图像分辨率进行不同训练阶段的需要。Fuyu-8B 具有 80 亿个参数，并在 CC-BY-NC 许可下发布，以其处理文本和图像的能力、令人印象深刻的 16K 上下文大小和整体性能而闻名。

`Fuyu`模型是使用`bfloat16`训练的，但原始推理使用`float16`。上传到 hub 的检查点使用`torch_dtype = 'float16'`，`AutoModel` API 将使用它将检查点从`torch.float32`转换为`torch.float16`。

在线权重的`dtype`大多数情况下并不重要，除非在使用`torch_dtype="auto"`初始化模型时使用`model = AutoModelForCausalLM.from_pretrained("path", torch_dtype = "auto")`。原因是模型将首先被下载（使用在线检查点的`dtype`），然后将被转换为`torch`的默认`dtype`（变为`torch.float32`）。用户应该指定他们想要的`torch_dtype`，如果他们不这样做，它将是`torch.float32`。

不建议在`float16`中微调模型，因为已知会产生`nan`，因此应该在`bfloat16`中微调模型。

提示：

+   要转换模型，您需要使用`git clone https://github.com/persimmon-ai-labs/adept-inference`克隆原始存储库，然后获取检查点：

```py
git clone https://github.com/persimmon-ai-labs/adept-inference
wget path/to/fuyu-8b-model-weights.tar
tar -xvf fuyu-8b-model-weights.tar
python src/transformers/models/fuyu/convert_fuyu_weights_to_hf.py  --input_dir /path/to/downloaded/fuyu/weights/ --output_dir /output/path \
    --pt_model_path /path/to/fuyu_8b_release/iter_0001251/mp_rank_00/model_optim_rng.pt
    --ada_lib_path /path/to/adept-inference
```

对于聊天模型：

```py
wget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar
tar -xvf 8b_base_model_release.tar
```

然后，模型可以通过以下方式加载：

```py
from transformers import FuyuConfig, FuyuForCausalLM
model_config = FuyuConfig()
model = FuyuForCausalLM(model_config).from_pretrained('/output/path')
```

需要通过特定的处理器传递输入以获得正确的格式。处理器需要一个图像处理器和一个分词器。因此，输入可以通过以下方式加载：

```py
from PIL import Image
from transformers import AutoTokenizer
from transformers.models.fuyu.processing_fuyu import FuyuProcessor
from transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor

tokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')
image_processor = FuyuImageProcessor()

processor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)
text_prompt = "Generate a coco-style caption.\\n"

bus_image_url = "https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png"
bus_image_pil = Image.open(io.BytesIO(requests.get(bus_image_url).content))
inputs_to_model = processor(text=text_prompt, images=image_pil)

```

此模型由[Molbap](https://huggingface.co/Molbap)贡献。原始代码可以在[此处](https://github.com/persimmon-ai-labs/adept-inference)找到。

+   Fuyu 使用基于`sentencepiece`的分词器，采用`Unigram`模型。它支持字节回退，仅在快速分词器的`tokenizers==0.14.0`中可用。`LlamaTokenizer`被用作它是句子片段的标准包装器。

+   作者建议为图像字幕使用以下提示：`f"生成一个 coco 风格的字幕。\\n"`

## FuyuConfig

### `class transformers.FuyuConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/configuration_fuyu.py#L29)

```py
( vocab_size = 262144 hidden_size = 4096 intermediate_size = 16384 num_hidden_layers = 36 num_attention_heads = 64 hidden_act = 'relu2' max_position_embeddings = 16384 image_size = 300 patch_size = 30 num_channels = 3 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True tie_word_embeddings = False rope_theta = 25000.0 rope_scaling = None qk_layernorm = True hidden_dropout = 0.0 attention_dropout = 0.0 partial_rotary_factor = 0.5 pad_token_id = None bos_token_id = 1 eos_token_id = 2 text_config = None **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 262144) — Fuyu 模型的词汇表大小。定义了在调用 FuyuForCausalLM 时可以由`inputs_ids`表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为 4096) — 隐藏表示的维度。

+   `intermediate_size` (`int`, *optional*, 默认为 16384) — MLP 表示的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为 36) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 64) — Transformer 编码器中每个注意力层的注意力头数量。

+   `hidden_act` (`str`或`function`, *optional*, 默认为`"relu2"`) — 解码器中的非线性激活函数（函数或字符串）。

+   `max_position_embeddings`（`int`，*可选*，默认为 16384）-此模型可能使用的最大序列长度。

+   `image_size`（`int`，*可选*，默认为 300）-输入图像大小。

+   `patch_size`（`int`，*可选*，默认为 30）-输入视觉变换器编码块大小。

+   `num_channels`（`int`，*可选*，默认为 3）-输入图像通道数。

+   `initializer_range`（`float`，*可选*，默认为 0.02）-用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps`（`float`，*可选*，默认为 1e-05）-rms 归一化层使用的 epsilon。

+   `use_cache`（`bool`，*可选*，默认为`True`）-模型是否应返回最后的键/值注意力（不是所有模型都使用）。仅在`config.is_decoder=True`时相关。是否绑定权重嵌入

+   `tie_word_embeddings`（`bool`，*可选*，默认为`False`）-是否绑定输入和输出嵌入。

+   `rope_theta`（`float`，*可选*，默认为 25000.0）-RoPE 嵌入的基本周期。

+   `rope_scaling`（`Dict`，*可选*）-包含 RoPE 嵌入的缩放配置的字典。当前支持两种缩放策略：线性和动态。它们的缩放因子必须是大于 1 的浮点数。预期格式为`{"type"：策略名称，"factor"：缩放因子}`。使用此标志时，不要将`max_position_embeddings`更新为预期的新最大值。有关这些缩放策略行为的更多信息，请参见以下线程：[`www.reddit.com/r/LocalFuyu/comments/14mrgpr/dynamically_scaled_rope_further_increases/`](https://www.reddit.com/r/LocalFuyu/comments/14mrgpr/dynamically_scaled_rope_further_increases/)。这是一个实验性功能，可能在将来的版本中会有破坏性的 API 更改。

+   `qk_layernorm`（`bool`，*可选*，默认为`True`）-在投影隐藏状态后是否规范查询和键

+   `hidden_dropout`（`float`，*可选*，默认为 0.0）-在将 MLP 应用于隐藏状态后的丢失比率。

+   `attention_dropout`（`float`，*可选*，默认为 0.0）-计算注意力分数后的丢失比率。

+   `partial_rotary_factor`（`float`，*可选*，默认为 0.5）-查询和键的百分比，将具有旋转嵌入。

+   `pad_token_id`（`int`，*可选*）- *填充* 标记的 ID。

+   `bos_token_id`（`int`，*可选*，默认为 1）- *序列开始* 标记的 ID。

+   `eos_token_id`（`Union[int，List[int]]`，*可选*，默认为 2）- *序列结束* 标记的 ID。可选择使用列表设置多个*序列结束* 标记。

+   `text_config`（`dict`，*可选*）-用于初始化`language```pyAut`.

This is the configuration class to store the configuration of a FuyuForCausalLM. It is used to instantiate an Fuyu model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the [adept/fuyu-8b](https://huggingface.co/adept/fuyu-8b).

Configuration objects inherit from PretrainedConfig and can be used to control the model outputs. Read the documentation from PretrainedConfig for more information.

````的配置选项字典

从转换器导入 FuyuConfig

初始化 Fuyu fuyu-7b 风格配置

配置= FuyuConfig（）

```py

## FuyuForCausalLM

 ### `class transformers.FuyuForCausalLM`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/modeling_fuyu.py#L143)

```

（配置：FuyuConfig）

```py

Parameters

*   `config` (FuyuConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model weights.

Fuyu Model with a language modeling head on top for causal language model conditioned on image patches and text. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.

 #### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/modeling_fuyu.py#L210)

```

（input_ids：LongTensor = None image_patches：Tensor = None image_patches_indices：Tensor = None attention_mask：Optional = None position_ids：Optional = None past_key_values：Optional = None inputs_embeds：Optional = None use_cache：Optional = None labels：Optional = None output_attentions：Optional = None output_hidden_states：Optional = None return_dict：Optional = None）→导出常量元数据='未定义';transformers.modeling_outputs.CausalLMOutputWithPast 或元组（torch.FloatTensor）

```py

Parameters

*   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide it.

    Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.`call`() for details.

    What are input IDs? 
*   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

    *   1 for tokens that are `not masked`,
    *   0 for tokens that are `masked`.

    What are attention masks?

    Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.`call`() for details.

    If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).

    If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask` and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.

    *   1 indicates the head is `not masked`,
    *   0 indicates the head is `masked`. 
*   `image_patches` (`torch.FloatTensor` of shape `(batch_size, num_total_patches, patch_size_ x patch_size x num_channels)`, *optional*) — Image patches to be used as continuous embeddings. The patches are flattened and then projected to the hidden size of the model.
*   `image_patches_indices` (`torch.LongTensor` of shape `(batch_size, num_total_patches + number_of_newline_tokens + number_of_text_tokens, patch_size_ x patch_size x num_channels )`, *optional*) — Indices indicating at which position the image_patches have to be inserted in input_embeds.
*   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0, config.n_positions - 1]`.

    What are position IDs? 
*   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

    Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

    If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don’t have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`. 
*   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert `input_ids` indices into associated vectors than the model’s internal embedding lookup matrix.
*   `use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see `past_key_values`).
*   `output_attentions` (`bool`, *optional*) — Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned tensors for more detail.
*   `output_hidden_states` (`bool`, *optional*) — Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for more detail.
*   `return_dict` (`bool`, *optional*) — Whether or not to return a ModelOutput instead of a plain tuple.
*   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — Labels for computing the masked language modeling loss. Indices should either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

Returns

transformers.modeling_outputs.CausalLMOutputWithPast or `tuple(torch.FloatTensor)`

A transformers.modeling_outputs.CausalLMOutputWithPast or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various elements depending on the configuration (FuyuConfig) and inputs.

*   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) — Language modeling loss (for next-token prediction).

*   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).

*   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

    Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

*   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

    Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

*   `attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

    Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

The FuyuForCausalLM forward method, overrides the `__call__` special method.

Although the recipe for forward pass needs to be defined within this function, one should call the `Module` instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.

Examples:

```

从转换器导入 FuyuProcessor，FuyuForCausalLM

从 PIL 导入图像

导入请求

处理器= FuyuProcessor.from_pretrained（“adept/fuyu-8b”）

模型= FuyuForCausalLM.from_pretrained（“adept/fuyu-8b”）

网址=“http://images.cocodataset.org/val2017/000000039769.jpg”

图像 = Image.open(requests.get(url, stream=True).raw)

提示=“生成一个 coco 风格的标题。\n”

输入=处理器（文本= text_prompt，图像=图像，return_tensors=“pt”）

输出=模型（**输入）

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=7)

>>> generation_text = processor.batch_decode(generated_ids, skip_special_tokens=True)

>>> print(generation_text)

“停在路边的公共汽车。”

```py

## FuyuImageProcessor

 ### `class transformers.FuyuImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/image_processing_fuyu.py#L180)

```

（do_resize: bool=True size: Optional=None resample: Resampling=<Resampling.BILINEAR: 2> do_pad: bool=True padding_value: float=1.0 padding_mode: str='constant' do_normalize: bool=True image_mean: Union=0.5 image_std: Union=0.5 do_rescale: bool=True rescale_factor: float=0.00392156862745098 patch_size: Optional=None **kwargs）

```py

Parameters

*   `do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the image to `size`.
*   `size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 1080, "width": 1920}`): Dictionary in the format `{"height": int, "width": int}` specifying the size of the output image.
*   `resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`) — `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.
*   `do_pad` (`bool`, *optional*, defaults to `True`) — Whether to pad the image to `size`.
*   `padding_value` (`float`, *optional*, defaults to 1.0) — The value to pad the image with.
*   `padding_mode` (`str`, *optional*, defaults to `"constant"`) — The padding mode to use when padding the image.
*   `do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize the image.
*   `image_mean` (`float`, *optional*, defaults to 0.5) — The mean to use when normalizing the image.
*   `image_std` (`float`, *optional*, defaults to 0.5) — The standard deviation to use when normalizing the image.
*   `do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale the image.
*   `rescale_factor` (`float`, *optional*, defaults to `1 / 255`) — The factor to use when rescaling the image.
*   `patch_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 30, "width": 30}`): Dictionary in the format `{"height": int, "width": int}` specifying the size of the patches.

This class should handle the image processing part before the main FuyuForCausalLM. In particular, it should handle:

*   Processing Images: Taking a batch of images as input. If the images are variable-sized, it resizes them based on the desired patch dimensions. The image output is always img_h, img_w of (1080, 1920)

    Then, it patches up these images using the patchify_image function.

*   Creating Image Input IDs: For each patch, a placeholder ID is given to identify where these patches belong in a token sequence. For variable-sized images, each line of patches is terminated with a newline ID.

*   Image Patch Indices: For each image patch, the code maintains an index where these patches should be inserted in a token stream.

 #### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```

（images **kwargs）

```py

Preprocess an image or a batch of images.

## FuyuProcessor

 ### `class transformers.FuyuProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/processing_fuyu.py#L309)

```

（图像处理器 标记器）

```py

Parameters

*   `image_processor` (FuyuImageProcessor) — The image processor is a required input.
*   `tokenizer` (LlamaTokenizerFast) — The tokenizer is a required input.

Constructs a Fuyu processor which wraps a Fuyu image processor and a Llama tokenizer into a single processor.

FuyuProcessor offers all the functionalities of FuyuImageProcessor and LlamaTokenizerFast. See the **call**() and `decode()` for more information.

 #### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/processing_fuyu.py#L451)

```

（text=None images=None add_special_tokens: bool=True return_attention_mask: bool=True padding: Union=False truncation: Union=None max_length: Optional=None stride: int=0 pad_to_multiple_of: Optional=None return_overflowing_tokens: bool=False return_special_tokens_mask: bool=False return_offsets_mapping: bool=False return_token_type_ids: bool=False return_length: bool=False verbose: bool=True return_tensors: Union=None **kwargs）→ export const metadata = 'undefined';FuyuBatchEncoding

```

参数

+   `text`（`str`，`List[str]`）— 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预标记化字符串）。如果将序列提供为字符串列表（预标记化），则必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。

+   `images`（`PIL.Image.Image`，`List[PIL.Image.Image]`）— 要准备的图像或图像批次。每个图像可以是 PIL 图像、NumPy 数组或 PyTorch 张量。对于 NumPy 数组/PyTorch 张量，每个图像的形状应为（C，H，W），其中 C 是通道数，H 和 W 是图像的高度和宽度。

返回

`FuyuBatchEncoding`

一个具有以下字段的`FuyuBatchEncoding`：

+   `input_ids` — 要提供给模型的标记 id 张量。当`text`不是`None`时返回。

+   `image_patches` — 图像补丁张量列表。当`images`不是`None`时返回。

+   `image_patches_indices` — 指示模型应插入补丁嵌入的索引张量。

+   `attention_mask` — 指定模型在`return_attention_mask=True`时应关注哪些标记的索引列表。

准备模型一个或多个序列和图像的主要方法。如果`text`不是`None`，此方法将`text`和`kwargs`参数转发给 LlamaTokenizerFast 的**call**()以对文本进行编码。要准备图像，如果`images`不是`None`，此方法将`images`和`kwargs`参数转发给 FuyuImageProcessor 的**call**()。有关更多信息，请参考上述两种方法的文档。
