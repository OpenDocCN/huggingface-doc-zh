# ä½¿ç”¨ğŸ¤— PEFT åŠ è½½é€‚é…å™¨

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/peft`](https://huggingface.co/docs/transformers/v4.37.2/en/peft)

[å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰](https://huggingface.co/blog/peft)æ–¹æ³•åœ¨å¾®è°ƒæœŸé—´å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨å…¶ä¸Šæ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆé€‚é…å™¨ï¼‰ã€‚é€‚é…å™¨è¢«è®­ç»ƒä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜åœ¨ä½¿ç”¨æ›´ä½çš„è®¡ç®—èµ„æºçš„åŒæ—¶äº§ç”Ÿä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸åª²ç¾çš„ç»“æœæ—¶éå¸¸èŠ‚çœå†…å­˜ã€‚

ä½¿ç”¨ PEFT è®­ç»ƒçš„é€‚é…å™¨é€šå¸¸æ¯”å®Œæ•´æ¨¡å‹å°ä¸€ä¸ªæ•°é‡çº§ï¼Œè¿™æ ·æ–¹ä¾¿åˆ†äº«ã€å­˜å‚¨å’ŒåŠ è½½ã€‚

![](img/5e34ae8912ca7fcb5554d98cb511bc58.png)

å­˜å‚¨åœ¨ Hub ä¸Šçš„ OPTForCausalLM æ¨¡å‹çš„é€‚é…å™¨æƒé‡ä»…çº¦ä¸º 6MBï¼Œè€Œæ¨¡å‹æƒé‡çš„å®Œæ•´å¤§å°å¯èƒ½çº¦ä¸º 700MBã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºğŸ¤— PEFT åº“çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚

## è®¾ç½®

é€šè¿‡å®‰è£…ğŸ¤— PEFT æ¥å¼€å§‹ï¼š

```py
pip install peft
```

å¦‚æœæ‚¨æƒ³å°è¯•å…¨æ–°çš„åŠŸèƒ½ï¼Œæ‚¨å¯èƒ½ä¼šå¯¹ä»æºä»£ç å®‰è£…åº“æ„Ÿå…´è¶£ï¼š

```py
pip install git+https://github.com/huggingface/peft.git
```

## æ”¯æŒçš„ PEFT æ¨¡å‹

ğŸ¤— Transformers åŸç”Ÿæ”¯æŒä¸€äº› PEFT æ–¹æ³•ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥åŠ è½½æœ¬åœ°æˆ– Hub ä¸Šå­˜å‚¨çš„é€‚é…å™¨æƒé‡ï¼Œå¹¶ä½¿ç”¨å‡ è¡Œä»£ç è½»æ¾è¿è¡Œæˆ–è®­ç»ƒå®ƒä»¬ã€‚æ”¯æŒä»¥ä¸‹æ–¹æ³•ï¼š

+   [ä½ç§©é€‚é…å™¨](https://huggingface.co/docs/peft/conceptual_guides/lora)

+   [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)

+   [AdaLoRA](https://arxiv.org/abs/2303.10512)

å¦‚æœæ‚¨æƒ³ä½¿ç”¨å…¶ä»– PEFT æ–¹æ³•ï¼Œå¦‚æç¤ºå­¦ä¹ æˆ–æç¤ºè°ƒæ•´ï¼Œæˆ–è€…äº†è§£ğŸ¤— PEFT åº“çš„ä¸€èˆ¬ä¿¡æ¯ï¼Œè¯·å‚è€ƒ[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚

## åŠ è½½ PEFT é€‚é…å™¨

åŠ è½½å’Œä½¿ç”¨ğŸ¤— Transformers ä¸­çš„ PEFT é€‚é…å™¨æ¨¡å‹æ—¶ï¼Œè¯·ç¡®ä¿ Hub å­˜å‚¨åº“æˆ–æœ¬åœ°ç›®å½•åŒ…å«ä¸€ä¸ª`adapter_config.json`æ–‡ä»¶å’Œé€‚é…å™¨æƒé‡ï¼Œå¦‚ä¸Šé¢çš„ç¤ºä¾‹å›¾æ‰€ç¤ºã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»åŠ è½½ PEFT é€‚é…å™¨æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè¦åŠ è½½ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„ PEFT é€‚é…å™¨æ¨¡å‹ï¼š

1.  æŒ‡å®š PEFT æ¨¡å‹ ID

1.  å°†å…¶ä¼ é€’ç»™ AutoModelForCausalLM ç±»

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

æ‚¨å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»æˆ–åŸºæœ¬æ¨¡å‹ç±»ï¼ˆå¦‚`OPTForCausalLM`æˆ–`LlamaForCausalLM`ï¼‰åŠ è½½ PEFT é€‚é…å™¨ã€‚

æ‚¨è¿˜å¯ä»¥é€šè¿‡è°ƒç”¨`load_adapter`æ–¹æ³•åŠ è½½ PEFT é€‚é…å™¨ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## ä»¥ 8 ä½æˆ– 4 ä½åŠ è½½

`bitsandbytes`é›†æˆæ”¯æŒ 8 ä½å’Œ 4 ä½ç²¾åº¦æ•°æ®ç±»å‹ï¼Œå¯¹äºåŠ è½½å¤§å‹æ¨¡å‹å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒèŠ‚çœå†…å­˜ï¼ˆè¯·å‚é˜…`bitsandbytes`é›†æˆæŒ‡å—ä»¥äº†è§£æ›´å¤šï¼‰ã€‚å°†`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°æ·»åŠ åˆ° from_pretrained()ä¸­ï¼Œå¹¶è®¾ç½®`device_map="auto"`ä»¥æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°æ‚¨çš„ç¡¬ä»¶ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map="auto", load_in_8bit=True)
```

## æ·»åŠ ä¸€ä¸ªæ–°é€‚é…å™¨

æ‚¨å¯ä»¥ä½¿ç”¨`~peft.PeftModel.add_adapter`å°†ä¸€ä¸ªæ–°é€‚é…å™¨æ·»åŠ åˆ°å…·æœ‰ç°æœ‰é€‚é…å™¨çš„æ¨¡å‹ä¸­ï¼Œåªè¦æ–°é€‚é…å™¨ä¸å½“å‰é€‚é…å™¨çš„ç±»å‹ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªå·²ç»è¿æ¥åˆ°æ¨¡å‹çš„ç°æœ‰ LoRA é€‚é…å™¨ï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

æ·»åŠ ä¸€ä¸ªæ–°é€‚é…å™¨ï¼š

```py
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨`~peft.PeftModel.set_adapter`æ¥è®¾ç½®è¦ä½¿ç”¨çš„é€‚é…å™¨ï¼š

```py
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## å¯ç”¨å’Œç¦ç”¨é€‚é…å™¨

ä¸€æ—¦æ‚¨å‘æ¨¡å‹æ·»åŠ äº†é€‚é…å™¨ï¼Œæ‚¨å¯ä»¥å¯ç”¨æˆ–ç¦ç”¨é€‚é…å™¨æ¨¡å—ã€‚è¦å¯ç”¨é€‚é…å™¨æ¨¡å—ï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

è¦ç¦ç”¨é€‚é…å™¨æ¨¡å—ï¼š

```py
model.disable_adapters()
output = model.generate(**inputs)
```

## è®­ç»ƒä¸€ä¸ª PEFT é€‚é…å™¨

PEFT é€‚é…å™¨å— Trainer ç±»æ”¯æŒï¼Œå› æ­¤æ‚¨å¯ä»¥ä¸ºç‰¹å®šç”¨ä¾‹è®­ç»ƒä¸€ä¸ªé€‚é…å™¨ã€‚åªéœ€è¦æ·»åŠ å‡ è¡Œä»£ç ã€‚ä¾‹å¦‚ï¼Œè¦è®­ç»ƒä¸€ä¸ª LoRA é€‚é…å™¨ï¼š

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ Trainer å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ•™ç¨‹ã€‚

1.  ä½¿ç”¨ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°å®šä¹‰æ‚¨çš„é€‚é…å™¨é…ç½®ï¼ˆæœ‰å…³è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`~peft.LoraConfig`ï¼‰ã€‚

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

1.  å°†é€‚é…å™¨æ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚

```py
model.add_adapter(peft_config)
```

1.  ç°åœ¨æ‚¨å¯ä»¥å°†æ¨¡å‹ä¼ é€’ç»™ Trainerï¼

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

ä¿å­˜æ‚¨è®­ç»ƒè¿‡çš„é€‚é…å™¨å¹¶åŠ è½½å›æ¥ï¼š

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

## å‘ PEFT é€‚é…å™¨æ·»åŠ é¢å¤–çš„å¯è®­ç»ƒå±‚

æ‚¨è¿˜å¯ä»¥é€šè¿‡åœ¨ PEFT é…ç½®ä¸­ä¼ é€’`modules_to_save`æ¥åœ¨å·²é™„åŠ é€‚é…å™¨çš„æ¨¡å‹é¡¶éƒ¨å¾®è°ƒé¢å¤–çš„å¯è®­ç»ƒé€‚é…å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³åœ¨å…·æœ‰ LoRA é€‚é…å™¨çš„æ¨¡å‹é¡¶éƒ¨ä¹Ÿå¾®è°ƒ lm_headï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    modules_to_save=["lm_head"],
)

model.add_adapter(lora_config)
```
