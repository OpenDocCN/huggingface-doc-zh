# åˆ†è¯å™¨

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer)

åˆ†è¯å™¨è´Ÿè´£ä¸ºæ¨¡å‹å‡†å¤‡è¾“å…¥ã€‚è¯¥åº“åŒ…å«æ‰€æœ‰æ¨¡å‹çš„åˆ†è¯å™¨ã€‚å¤§å¤šæ•°åˆ†è¯å™¨æœ‰ä¸¤ç§ç‰ˆæœ¬ï¼šå®Œæ•´çš„ Python å®ç°å’ŒåŸºäº Rust åº“çš„â€œå¿«é€Ÿâ€å®ç°[ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)ã€‚ â€œå¿«é€Ÿâ€å®ç°å…è®¸ï¼š

1.  ç‰¹åˆ«æ˜¯åœ¨è¿›è¡Œæ‰¹é‡åˆ†è¯æ—¶ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«é€Ÿåº¦ã€‚

1.  é¢å¤–çš„æ–¹æ³•ç”¨äºåœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰å’Œæ ‡è®°ç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„ï¼ˆä¾‹å¦‚ï¼Œè·å–åŒ…å«ç»™å®šå­—ç¬¦çš„æ ‡è®°çš„ç´¢å¼•æˆ–ä¸ç»™å®šæ ‡è®°å¯¹åº”çš„å­—ç¬¦èŒƒå›´ï¼‰ã€‚

åŸºç±» PreTrainedTokenizer å’Œ PreTrainedTokenizerFast å®ç°äº†å¯¹æ¨¡å‹è¾“å…¥ä¸­çš„å­—ç¬¦ä¸²è¾“å…¥è¿›è¡Œç¼–ç çš„å¸¸ç”¨æ–¹æ³•ï¼ˆè§ä¸‹æ–‡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•æˆ–ä»åº“æä¾›çš„é¢„è®­ç»ƒåˆ†è¯å™¨ï¼ˆä» HuggingFace çš„ AWS S3 å­˜å‚¨åº“ä¸‹è½½ï¼‰å®ä¾‹åŒ–/ä¿å­˜ Python å’Œâ€œå¿«é€Ÿâ€åˆ†è¯å™¨ã€‚å®ƒä»¬éƒ½ä¾èµ–äºåŒ…å«å¸¸ç”¨æ–¹æ³•çš„ PreTrainedTokenizerBaseï¼Œä»¥åŠ SpecialTokensMixinã€‚

PreTrainedTokenizer å’Œ PreTrainedTokenizerFast å› æ­¤å®ç°äº†ä½¿ç”¨æ‰€æœ‰åˆ†è¯å™¨çš„ä¸»è¦æ–¹æ³•ï¼š

+   åˆ†è¯ï¼ˆå°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå­è¯æ ‡è®°å­—ç¬¦ä¸²ï¼‰ï¼Œå°†æ ‡è®°å­—ç¬¦ä¸²è½¬æ¢ä¸º ID å¹¶è¿”å›ï¼Œä»¥åŠç¼–ç /è§£ç ï¼ˆå³ï¼Œåˆ†è¯å’Œè½¬æ¢ä¸ºæ•´æ•°ï¼‰ã€‚

+   ä»¥ç‹¬ç«‹äºåº•å±‚ç»“æ„ï¼ˆBPEï¼ŒSentencePiece ç­‰ï¼‰çš„æ–¹å¼å‘è¯æ±‡è¡¨ä¸­æ·»åŠ æ–°æ ‡è®°ã€‚

+   ç®¡ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚æ©ç ï¼Œå¥å­å¼€å¤´ç­‰ï¼‰ï¼šæ·»åŠ å®ƒä»¬ï¼Œå°†å®ƒä»¬åˆ†é…ç»™åˆ†è¯å™¨ä¸­çš„å±æ€§ä»¥ä¾¿è½»æ¾è®¿é—®ï¼Œå¹¶ç¡®ä¿å®ƒä»¬åœ¨åˆ†è¯è¿‡ç¨‹ä¸­ä¸è¢«æ‹†åˆ†ã€‚

BatchEncoding ä¿å­˜äº† PreTrainedTokenizerBase çš„ç¼–ç æ–¹æ³•ï¼ˆ`__call__`ï¼Œ`encode_plus`å’Œ`batch_encode_plus`ï¼‰çš„è¾“å‡ºï¼Œå¹¶ä» Python å­—å…¸æ´¾ç”Ÿã€‚å½“åˆ†è¯å™¨æ˜¯çº¯ Python åˆ†è¯å™¨æ—¶ï¼Œæ­¤ç±»çš„è¡Œä¸ºå°±åƒæ ‡å‡† Python å­—å…¸ä¸€æ ·ï¼Œå¹¶ä¿å­˜è¿™äº›æ–¹æ³•è®¡ç®—çš„å„ç§æ¨¡å‹è¾“å…¥ï¼ˆ`input_ids`ï¼Œ`attention_mask`ç­‰ï¼‰ã€‚å½“åˆ†è¯å™¨æ˜¯â€œå¿«é€Ÿâ€åˆ†è¯å™¨ï¼ˆå³ç”± HuggingFace çš„[tokenizers åº“](https://github.com/huggingface/tokenizers)æ”¯æŒï¼‰æ—¶ï¼Œæ­¤ç±»è¿˜æä¾›äº†å‡ ç§é«˜çº§å¯¹é½æ–¹æ³•ï¼Œå¯ç”¨äºåœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰å’Œæ ‡è®°ç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„ï¼ˆä¾‹å¦‚ï¼Œè·å–åŒ…å«ç»™å®šå­—ç¬¦çš„æ ‡è®°çš„ç´¢å¼•æˆ–ä¸ç»™å®šæ ‡è®°å¯¹åº”çš„å­—ç¬¦èŒƒå›´ï¼‰ã€‚

## PreTrainedTokenizer

### `class transformers.PreTrainedTokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L335)

```py
( **kwargs )
```

å‚æ•°

+   `model_max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥åˆ°å˜æ¢å™¨æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°æ•°è®¡ï¼‰ã€‚å½“ä½¿ç”¨ from_pretrained()åŠ è½½åˆ†è¯å™¨æ—¶ï¼Œè¿™å°†è®¾ç½®ä¸ºå­˜å‚¨åœ¨`max_model_input_sizes`ä¸­çš„ç›¸å…³æ¨¡å‹çš„å€¼ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚å¦‚æœæœªæä¾›å€¼ï¼Œå°†é»˜è®¤ä¸º VERY_LARGE_INTEGERï¼ˆ`int(1e30)`ï¼‰ã€‚

+   `padding_side`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹åº”è¯¥åº”ç”¨å¡«å……çš„ä¸€ä¾§ã€‚åº”è¯¥åœ¨[â€˜rightâ€™ï¼Œâ€˜leftâ€™]ä¹‹é—´é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåçš„ç±»å±æ€§ä¸­é€‰æ‹©ã€‚

+   `truncation_side`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹åº”è¯¥åº”ç”¨æˆªæ–­çš„ä¸€ä¾§ã€‚åº”è¯¥åœ¨[â€˜rightâ€™ï¼Œâ€˜leftâ€™]ä¹‹é—´é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåçš„ç±»å±æ€§ä¸­é€‰æ‹©ã€‚

+   `chat_template`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯åˆ—è¡¨çš„ Jinja æ¨¡æ¿å­—ç¬¦ä¸²ã€‚æŸ¥çœ‹[`huggingface.co/docs/transformers/chat_templating`](https://huggingface.co/docs/transformers/chat_templating)è·å–å®Œæ•´æè¿°ã€‚

+   `model_input_names`ï¼ˆ`List[string]`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹å‰å‘ä¼ é€’æ¥å—çš„è¾“å…¥åˆ—è¡¨ï¼ˆå¦‚`"token_type_ids"`æˆ–`"attention_mask"`ï¼‰ã€‚é»˜è®¤å€¼ä»åŒåçš„ç±»å±æ€§ä¸­é€‰æ‹©ã€‚

+   `bos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºå¥å­å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸`self.bos_token`å’Œ`self.bos_token_id`ç›¸å…³è”ã€‚

+   `eos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºå¥å­ç»“æŸçš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸`self.eos_token`å’Œ`self.eos_token_id`ç›¸å…³è”ã€‚

+   `unk_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºè¯æ±‡å¤–æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸`self.unk_token`å’Œ`self.unk_token_id`ç›¸å…³è”ã€‚

+   `sep_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨åŒä¸€è¾“å…¥ä¸­åˆ†éš”ä¸¤ä¸ªä¸åŒå¥å­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚ BERT ä½¿ç”¨ï¼‰ã€‚å°†ä¸`self.sep_token`å’Œ`self.sep_token_id`ç›¸å…³è”ã€‚

+   `pad_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿æ ‡è®°æ•°ç»„å¤§å°ç›¸åŒä»¥è¿›è¡Œæ‰¹å¤„ç†çš„ç‰¹æ®Šæ ‡è®°ã€‚ç„¶åå°†è¢«æ³¨æ„æœºåˆ¶æˆ–æŸå¤±è®¡ç®—å¿½ç•¥ã€‚å°†ä¸`self.pad_token`å’Œ`self.pad_token_id`ç›¸å…³è”ã€‚

+   `cls_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºè¾“å…¥ç±»åˆ«çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚ BERT ä½¿ç”¨ï¼‰ã€‚å°†ä¸`self.cls_token`å’Œ`self.cls_token_id`ç›¸å…³è”ã€‚

+   `mask_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºæ©ç æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç”¨äºæ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒç›®æ ‡ï¼Œå¦‚ BERTï¼‰ã€‚å°†ä¸`self.mask_token`å’Œ`self.mask_token_id`ç›¸å…³è”ã€‚

+   `additional_special_tokens`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`çš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€” é™„åŠ ç‰¹æ®Šæ ‡è®°çš„å…ƒç»„æˆ–åˆ—è¡¨ã€‚åœ¨æ­¤æ·»åŠ å®ƒä»¬ä»¥ç¡®ä¿åœ¨è®¾ç½®`skip_special_tokens`ä¸º True æ—¶è§£ç æ—¶è·³è¿‡å®ƒä»¬ã€‚å¦‚æœå®ƒä»¬ä¸æ˜¯è¯æ±‡çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡çš„æœ«å°¾ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ¨¡å‹æ˜¯å¦åº”æ¸…é™¤åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ‹†åˆ†è¾“å…¥æ–‡æœ¬æ—¶æ·»åŠ çš„ç©ºæ ¼ã€‚

+   `split_special_tokens` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚é»˜è®¤è¡Œä¸ºæ˜¯ä¸æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚è¿™æ„å‘³ç€å¦‚æœ `<s>` æ˜¯ `bos_token`ï¼Œé‚£ä¹ˆ `tokenizer.tokenize("<s>") = ['<s>`]`ã€‚å¦åˆ™ï¼Œå¦‚æœ `split_special_tokens=True`ï¼Œé‚£ä¹ˆ `tokenizer.tokenize("<s>")` å°†ä¼šç»™å‡º `['<', 's', '>']`ã€‚æ­¤å‚æ•°ç›®å‰ä»…æ”¯æŒ`slow`ç±»å‹çš„åˆ†è¯å™¨ã€‚

æ‰€æœ‰æ…¢åˆ†è¯å™¨çš„åŸºç±»ã€‚

ç»§æ‰¿è‡ª PreTrainedTokenizerBaseã€‚

å¤„ç†æ‰€æœ‰ç”¨äºæ ‡è®°åŒ–å’Œç‰¹æ®Šæ ‡è®°çš„å…±äº«æ–¹æ³•ï¼Œä»¥åŠç”¨äºä¸‹è½½/ç¼“å­˜/åŠ è½½é¢„è®­ç»ƒ tokenizer ä»¥åŠå‘è¯æ±‡è¡¨æ·»åŠ æ ‡è®°çš„æ–¹æ³•ã€‚

è¿™ä¸ªç±»è¿˜ä»¥ç»Ÿä¸€çš„æ–¹å¼åŒ…å«äº†æ‰€æœ‰ tokenizer çš„æ·»åŠ æ ‡è®°ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…å¤„ç†å„ç§åº•å±‚å­—å…¸ç»“æ„ï¼ˆBPEã€sentencepiece ç­‰ï¼‰çš„ç‰¹å®šè¯æ±‡å¢å¼ºæ–¹æ³•ã€‚

ç±»å±æ€§ï¼ˆç”±æ´¾ç”Ÿç±»è¦†ç›–ï¼‰

+   `vocab_files_names` (`Dict[str, str]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯æ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œå€¼æ˜¯ä¿å­˜ç›¸å…³æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚

+   `pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) â€” ä¸€ä¸ªå­—å…¸çš„å­—å…¸ï¼Œé«˜çº§é”®æ˜¯æ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œä½çº§é”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œå€¼æ˜¯ç›¸å…³é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶çš„`url`ã€‚

+   `max_model_input_sizes` (`Dict[str, Optional[int]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œå€¼æ˜¯è¯¥æ¨¡å‹çš„åºåˆ—è¾“å…¥çš„æœ€å¤§é•¿åº¦ï¼Œå¦‚æœæ¨¡å‹æ²¡æœ‰æœ€å¤§è¾“å…¥å¤§å°ï¼Œåˆ™ä¸º`None`ã€‚

+   `pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œå€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ä¼ é€’ç»™ tokenizer ç±»çš„`__init__`æ–¹æ³•çš„ç‰¹å®šå‚æ•°ã€‚

+   `model_input_names` (`List[str]`) â€” æ¨¡å‹å‰å‘ä¼ é€’ä¸­é¢„æœŸçš„è¾“å…¥åˆ—è¡¨ã€‚

+   `padding_side` (`str`) â€” æ¨¡å‹åº”ç”¨å¡«å……çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`'right'`æˆ–`'left'`ã€‚

+   `truncation_side` (`str`) â€” æ¨¡å‹åº”ç”¨æˆªæ–­çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`'right'`æˆ–`'left'`ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰ â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„ `PretrainedTokenizerBase.build_inputs_with_special_tokens` å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥ id çš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ  `bos` æˆ– `eos` æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `padding`ï¼ˆ`bool`ï¼Œ`str` æˆ– PaddingStrategyï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰ â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest'`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚

    +   `'max_length'`ï¼šå¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚

    +   `False` æˆ– `'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œå¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation`ï¼ˆ`bool`ï¼Œ`str` æˆ– TruncationStrategyï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰ â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest_first'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™å°†é€ä¸ªæ ‡è®°æˆªæ–­ï¼Œä»åºåˆ—å¯¹ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False` æˆ– `'do_not_truncate'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”±æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ï¼‰ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

+   `stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰ â€” å¦‚æœä¸ `max_length` ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“ `return_overflowing_tokens=True` æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚

+   `is_split_into_words`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰ â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„å…ˆåˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²ç»åˆ†æˆå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†æˆå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡ç©ºæ ¼åˆ†å‰²ï¼‰ï¼Œç„¶åè¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äºå‘½åå®ä½“è¯†åˆ«æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚

+   `pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´» `padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ› `>= 7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors`ï¼ˆ`str` æˆ– TensorTypeï¼Œ*å¯é€‰*ï¼‰ â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`ï¼šè¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚

    +   `'pt'`ï¼šè¿”å› PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`ï¼šè¿”å› Numpy `np.ndarray`å¯¹è±¡ã€‚

+   `return_token_type_ids`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› token ç±»å‹ IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å› token ç±»å‹ IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    ä»€ä¹ˆæ˜¯ token ç±»å‹ IDï¼Ÿ

+   `return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `return_overflowing_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„æ ‡è®°åºåˆ—ã€‚å¦‚æœæä¾›ä¸€å¯¹è¾“å…¥ id åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„æ ‡è®°ã€‚

+   `return_special_tokens_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç‰¹æ®Šæ ‡è®°æ©ç ä¿¡æ¯ã€‚

+   `return_offsets_mapping`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æ¯ä¸ªæ ‡è®°çš„`(char_start, char_end)`ã€‚

    ä»…é€‚ç”¨äºç»§æ‰¿è‡ª PreTrainedTokenizerFast çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨ Python çš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚

+   `return_length`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚

+   `verbose`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•

è¿”å›

BatchEncoding

ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„ BatchEncodingï¼š

+   `input_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„ token id åˆ—è¡¨ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `token_type_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„ token ç±»å‹ id åˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*`token_type_ids`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯ token ç±»å‹ IDï¼Ÿ

+   `attention_mask` â€” æŒ‡å®šå“ªäº›æ ‡è®°åº”ç”±æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*`attention_mask`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `overflowing_tokens` â€” æº¢å‡ºæ ‡è®°åºåˆ—çš„åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `num_truncated_tokens` â€” æˆªæ–­çš„æ ‡è®°æ•°ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `special_tokens_mask` â€” ç”± 0 å’Œ 1 ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­ 1 æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œ0 æŒ‡å®šå¸¸è§„åºåˆ—æ ‡è®°ï¼ˆå½“`add_special_tokens=True`å’Œ`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚

+   `length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰

ç”¨äºå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œåˆ†è¯å’Œå‡†å¤‡æ¨¡å‹çš„ä¸»è¦æ–¹æ³•ã€‚

#### `add_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)

```py
( new_tokens: Union special_tokens: bool = False ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `new_tokens`ï¼ˆ`str`ï¼Œ`tokenizers.AddedToken`æˆ–*str*åˆ—è¡¨æˆ–`tokenizers.AddedToken`ï¼‰â€” ä»…å½“å®ƒä»¬å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰æ·»åŠ æ ‡è®°ã€‚`tokenizers.AddedToken`åŒ…è£…ä¸€ä¸ªå­—ç¬¦ä¸²æ ‡è®°ï¼Œè®©æ‚¨ä¸ªæ€§åŒ–å…¶è¡Œä¸ºï¼šæ­¤æ ‡è®°æ˜¯å¦ä»…åŒ¹é…å•ä¸ªå•è¯ï¼Œæ­¤æ ‡è®°æ˜¯å¦åº”å‰¥ç¦»å·¦ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ï¼Œæ­¤æ ‡è®°æ˜¯å¦åº”å‰¥ç¦»å³ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ç­‰ã€‚

+   `special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- å¯ç”¨äºæŒ‡å®šæ ‡è®°æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°ã€‚è¿™ä¸»è¦ä¼šæ”¹å˜æ ‡å‡†åŒ–è¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œç‰¹æ®Šæ ‡è®°å¦‚ CLS æˆ–[MASK]é€šå¸¸ä¸ä¼šè¢«å°å†™ï¼‰ã€‚

    åœ¨ HuggingFace åˆ†è¯å™¨åº“ä¸­æŸ¥çœ‹`tokenizers.AddedToken`çš„è¯¦ç»†ä¿¡æ¯ã€‚

è¿”å›

`int`

æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°é‡ã€‚

å‘åˆ†è¯å™¨ç±»æ·»åŠ ä¸€ç»„æ–°æ ‡è®°ã€‚å¦‚æœæ–°æ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„é•¿åº¦å¼€å§‹ï¼Œå¹¶ä¸”åœ¨åº”ç”¨åˆ†è¯ç®—æ³•ä¹‹å‰å°†è¢«éš”ç¦»ã€‚å› æ­¤ï¼Œæ·»åŠ çš„æ ‡è®°å’Œåˆ†è¯ç®—æ³•çš„è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ä¸ä¼šä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†ã€‚

è¯·æ³¨æ„ï¼Œå½“å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”è¯¥ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µï¼Œä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚

ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨ resize_token_embeddings()æ–¹æ³•ã€‚

ç¤ºä¾‹ï¼š

```py
# Let's see how to increase the vocabulary of Bert model and tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

num_added_toks = tokenizer.add_tokens(["new_tok1", "my_new-tok2"])
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))
```

#### `add_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)

```py
( special_tokens_dict: Dict replace_additional_special_tokens = True ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `special_tokens_dict`ï¼ˆå­—å…¸*str*åˆ°*str*æˆ–`tokenizers.AddedToken`ï¼‰- é”®åº”è¯¥åœ¨é¢„å®šä¹‰ç‰¹æ®Šå±æ€§åˆ—è¡¨ä¸­ï¼š[`bos_token`ã€`eos_token`ã€`unk_token`ã€`sep_token`ã€`pad_token`ã€`cls_token`ã€`mask_token`ã€`additional_special_tokens`]ã€‚

    åªæœ‰å½“æ ‡è®°å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰ä¼šæ·»åŠ æ ‡è®°ï¼ˆé€šè¿‡æ£€æŸ¥åˆ†è¯å™¨æ˜¯å¦å°†`unk_token`çš„ç´¢å¼•åˆ†é…ç»™å®ƒä»¬è¿›è¡Œæµ‹è¯•ï¼‰ã€‚

+   `replace_additional_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰- å¦‚æœä¸º`True`ï¼Œåˆ™ç°æœ‰çš„é¢å¤–ç‰¹æ®Šæ ‡è®°åˆ—è¡¨å°†è¢«æ›¿æ¢ä¸º`special_tokens_dict`ä¸­æä¾›çš„åˆ—è¡¨ã€‚å¦åˆ™ï¼Œ`self._additional_special_tokens`å°†åªæ˜¯æ‰©å±•ã€‚åœ¨å‰ä¸€ç§æƒ…å†µä¸‹ï¼Œè¿™äº›æ ‡è®°ä¸ä¼šä»åˆ†è¯å™¨çš„å®Œæ•´è¯æ±‡è¡¨ä¸­åˆ é™¤-å®ƒä»¬åªè¢«æ ‡è®°ä¸ºéç‰¹æ®Šæ ‡è®°ã€‚è¯·è®°ä½ï¼Œè¿™åªå½±å“è§£ç æ—¶è·³è¿‡å“ªäº›æ ‡è®°ï¼Œè€Œä¸æ˜¯`added_tokens_encoder`å’Œ`added_tokens_decoder`ã€‚è¿™æ„å‘³ç€ä»¥å‰çš„`additional_special_tokens`ä»ç„¶æ˜¯æ·»åŠ çš„æ ‡è®°ï¼Œå¹¶ä¸”ä¸ä¼šè¢«æ¨¡å‹æ‹†åˆ†ã€‚

è¿”å›

`int`

æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°é‡ã€‚

å‘ç¼–ç å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®°å­—å…¸ï¼ˆeosã€padã€cls ç­‰ï¼‰å¹¶å°†å®ƒä»¬é“¾æ¥åˆ°ç±»å±æ€§ã€‚å¦‚æœç‰¹æ®Šæ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼ˆç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„æœ€åä¸€ä¸ªç´¢å¼•å¼€å§‹ï¼‰ã€‚

åœ¨å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”è¯¥ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µï¼Œä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚

ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨ resize_token_embeddings()æ–¹æ³•ã€‚

ä½¿ç”¨`add_special_tokens`å°†ç¡®ä¿æ‚¨çš„ç‰¹æ®Šæ ‡è®°å¯ä»¥ä»¥å¤šç§æ–¹å¼ä½¿ç”¨ï¼š

+   åœ¨è§£ç æ—¶å¯ä»¥é€šè¿‡`skip_special_tokens = True`è·³è¿‡ç‰¹æ®Šæ ‡è®°ã€‚

+   åˆ†è¯å™¨ä¼šä»”ç»†å¤„ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå®ƒä»¬æ°¸è¿œä¸ä¼šè¢«æ‹†åˆ†ï¼‰ï¼Œç±»ä¼¼äº`AddedTokens`ã€‚

+   æ‚¨å¯ä»¥ä½¿ç”¨åˆ†è¯å™¨ç±»å±æ€§å¦‚`tokenizer.cls_token`è½»æ¾å¼•ç”¨ç‰¹æ®Šæ ‡è®°ã€‚è¿™æ ·å¯ä»¥è½»æ¾å¼€å‘ä¸æ¨¡å‹æ— å…³çš„è®­ç»ƒå’Œå¾®è°ƒè„šæœ¬ã€‚

åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œç‰¹æ®Šæ ‡è®°å·²ç»ä¸ºæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹æ³¨å†Œï¼ˆä¾‹å¦‚ BertTokenizer `cls_token`å·²ç»æ³¨å†Œä¸º:obj*â€™[CLS]â€™*ï¼ŒXLM çš„ä¸€ä¸ªä¹Ÿå·²ç»æ³¨å†Œä¸º`'</s>'`ï¼‰ã€‚

ç¤ºä¾‹ï¼š

```py
# Let's see how to add a new classification token to GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2Model.from_pretrained("gpt2")

special_tokens_dict = {"cls_token": "<CLS>"}

num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))

assert tokenizer.cls_token == "<CLS>"
```

#### `apply_chat_template`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)

```py
( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `conversation`ï¼ˆUnion[List[Dict[str, str]]ï¼Œâ€œConversationâ€ï¼‰â€” ä¸€ä¸ª Conversation å¯¹è±¡æˆ–å…·æœ‰â€œroleâ€å’Œâ€œcontentâ€é”®çš„å­—å…¸åˆ—è¡¨ï¼Œè¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢çš„èŠå¤©å†å²ã€‚

+   `chat_template`ï¼ˆstrï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæ­¤è½¬æ¢çš„ Jinja æ¨¡æ¿ã€‚å¦‚æœæœªä¼ é€’æ­¤å‚æ•°ï¼Œåˆ™å°†ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤èŠå¤©æ¨¡æ¿ã€‚

+   `add_generation_prompt`ï¼ˆboolï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä»¥æŒ‡ç¤ºåŠ©æ‰‹æ¶ˆæ¯å¼€å§‹çš„æ ‡è®°ç»“æŸæç¤ºã€‚å½“æ‚¨æƒ³ä»æ¨¡å‹ç”Ÿæˆå“åº”æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚è¯·æ³¨æ„ï¼Œæ­¤å‚æ•°å°†ä¼ é€’ç»™èŠå¤©æ¨¡æ¿ï¼Œå› æ­¤æ¨¡æ¿å¿…é¡»æ”¯æŒæ­¤å‚æ•°æ‰èƒ½äº§ç”Ÿä»»ä½•æ•ˆæœã€‚

+   `tokenize`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å‡ºè¿›è¡Œæ ‡è®°åŒ–ã€‚å¦‚æœä¸º`False`ï¼Œè¾“å‡ºå°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

+   `padding`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å°†åºåˆ—å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚

+   `truncation`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨æœ€å¤§é•¿åº¦å¤„æˆªæ–­åºåˆ—ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚

+   `max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¡«å……æˆ–æˆªæ–­çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°ä¸ºå•ä½ï¼‰ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨åˆ†è¯å™¨çš„`max_length`å±æ€§ä½œä¸ºé»˜è®¤å€¼ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ– TensorTypeï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›ç‰¹å®šæ¡†æ¶çš„å¼ é‡ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`ï¼šè¿”å› TensorFlow `tf.Tensor`å¯¹è±¡ã€‚

    +   `'pt'`ï¼šè¿”å› PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`ï¼šè¿”å› NumPy `np.ndarray`å¯¹è±¡ã€‚

    +   `'jax'`ï¼šè¿”å› JAX `jnp.ndarray`å¯¹è±¡ã€‚**tokenizer_kwargs â€” è¦ä¼ é€’ç»™åˆ†è¯å™¨çš„å…¶ä»– kwargsã€‚

è¿”å›

`List[int]`

è¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢çš„æ ‡è®°åŒ–èŠå¤©çš„æ ‡è®° id åˆ—è¡¨ï¼ŒåŒ…æ‹¬æ§åˆ¶æ ‡è®°ã€‚æ­¤è¾“å‡ºå·²å‡†å¤‡å¥½ä¼ é€’ç»™æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’æˆ–é€šè¿‡`generate()`ç­‰æ–¹æ³•ä¼ é€’ã€‚

å°† Conversation å¯¹è±¡æˆ–å¸¦æœ‰`"role"`å’Œ`"content"`é”®çš„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºæ ‡è®° id åˆ—è¡¨ã€‚æ­¤æ–¹æ³•æ—¨åœ¨ä¸èŠå¤©æ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼Œå¹¶å°†è¯»å–åˆ†è¯å™¨çš„ chat_template å±æ€§ä»¥ç¡®å®šåœ¨è½¬æ¢æ—¶è¦ä½¿ç”¨çš„æ ¼å¼å’Œæ§åˆ¶æ ‡è®°ã€‚å½“ chat_template ä¸º None æ—¶ï¼Œå°†é€€å›åˆ°ç±»çº§åˆ«æŒ‡å®šçš„ default_chat_templateã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) â†’ export const metadata = 'undefined';List[str]
```

å‚æ•°

+   `sequences`ï¼ˆ`Union[List[int]ï¼ŒList[List[int]]ï¼Œnp.ndarrayï¼Œtorch.Tensorï¼Œtf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ id çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚

+   `kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`List[str]`

è§£ç åçš„å¥å­åˆ—è¡¨ã€‚

é€šè¿‡è°ƒç”¨è§£ç å°†æ ‡è®° id çš„åˆ—è¡¨åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) â†’ export const metadata = 'undefined';str
```

å‚æ•°

+   `token_ids`ï¼ˆ`Union[intï¼ŒList[int]ï¼Œnp.ndarrayï¼Œtorch.Tensorï¼Œtf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ id çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚

+   `kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`str`

è§£ç åçš„å¥å­ã€‚

ä½¿ç”¨æ ‡è®°å™¨å’Œè¯æ±‡è¡¨å°† id åºåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå…·æœ‰åˆ é™¤ç‰¹æ®Šæ ‡è®°å’Œæ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼çš„é€‰é¡¹ã€‚

ç±»ä¼¼äºæ‰§è¡Œ `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚

#### `encode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)

```py
( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) â†’ export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray
```

å‚æ•°

+   `text`ï¼ˆ`str`ï¼Œ`List[str]` æˆ– `List[int]`ï¼‰â€” è¦ç¼–ç çš„ç¬¬ä¸€ä¸ªåºåˆ—ã€‚å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨ `tokenize` æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–æ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨ `convert_tokens_to_ids` æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸² idï¼‰ã€‚

+   `text_pair`ï¼ˆ`str`ï¼Œ`List[str]` æˆ– `List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” è¦ç¼–ç çš„å¯é€‰ç¬¬äºŒä¸ªåºåˆ—ã€‚å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨ `tokenize` æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–æ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨ `convert_tokens_to_ids` æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸² idï¼‰ã€‚

+   `add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„ `PretrainedTokenizerBase.build_inputs_with_special_tokens` å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥ id çš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ  `bos` æˆ– `eos` æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `padding`ï¼ˆ`bool`ï¼Œ`str` æˆ– PaddingStrategyï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest'`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚

    +   `'max_length'`ï¼šå¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚

    +   `False` æˆ– `'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œå¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation`ï¼ˆ`bool`ï¼Œ`str` æˆ– TruncationStrategyï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest_first'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä¼šé€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­ç§»é™¤ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False` æˆ– `'do_not_truncate'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œæˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€éœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

+   `stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€” å¦‚æœè®¾ç½®ä¸ºæ•°å­—ï¼Œå¹¶ä¸”`max_length`ä¸€èµ·è®¾ç½®ï¼Œå½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰é‡å æ ‡è®°çš„æ•°é‡ã€‚

+   `is_split_into_words`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„åˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²ç»åˆ†æˆå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™åˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†æˆå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äº NER æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚

+   `pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´»`padding`ã€‚è¿™å¯¹äºå¯ç”¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šçš„ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ– TensorTypeï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`ï¼šè¿”å› TensorFlow `tf.constant`å¯¹è±¡ã€‚

    +   `'pt'`ï¼šè¿”å› PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`ï¼šè¿”å› Numpy `np.ndarray`å¯¹è±¡ã€‚

    **kwargs â€” ä¼ é€’ç»™`.tokenize()`æ–¹æ³•ã€‚

è¿”å›

`List[int]`ï¼Œ`torch.Tensor`ï¼Œ`tf.Tensor`æˆ–`np.ndarray`

æ–‡æœ¬çš„æ ‡è®°åŒ– idã€‚

ä½¿ç”¨åˆ†è¯å™¨å’Œè¯æ±‡è¡¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º idï¼ˆæ•´æ•°ï¼‰åºåˆ—ã€‚

ä¸æ‰§è¡Œ`self.convert_tokens_to_ids(self.tokenize(text))`ç›¸åŒã€‚

#### `push_to_hub`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

å‚æ•°

+   `repo_id`ï¼ˆ`str`ï¼‰â€” æ‚¨è¦å°†åˆ†è¯å™¨æ¨é€åˆ°çš„å­˜å‚¨åº“åç§°ã€‚åœ¨æ¨é€åˆ°ç»™å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚

+   `use_temp_dir`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨åœ¨æ¨é€åˆ° Hub ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰åä¸º`repo_id`çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚

+   `commit_message`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º`"Upload tokenizer"`ã€‚

+   `private`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” åˆ›å»ºçš„å­˜å‚¨åº“æ˜¯å¦åº”ä¸ºç§æœ‰ã€‚

+   `token`ï¼ˆ`bool`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ`huggingface-cli login`æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ä¸­ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š`repo_url`ï¼Œåˆ™é»˜è®¤ä¸º`True`ã€‚

+   `max_shard_size`ï¼ˆ`int`æˆ–`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"5GB"`ï¼‰â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹åˆ†ç‰‡å°†æ¯ä¸ªå¤§å°ä½äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚`"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤è®¾ç½®ä¸º`"5GB"`ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åœ¨å…è´¹çš„ Google Colab å®ä¾‹ä¸Šè½»æ¾åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°ä»»ä½• CPU OOM é—®é¢˜ã€‚

+   `create_pr`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åˆ›å»ºå…·æœ‰ä¸Šä¼ æ–‡ä»¶çš„ PR æˆ–ç›´æ¥æäº¤ã€‚

+   `safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å°†æ¨¡å‹æƒé‡è½¬æ¢ä¸º safetensors æ ¼å¼ä»¥è¿›è¡Œæ›´å®‰å…¨çš„åºåˆ—åŒ–ã€‚

+   `revision`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” è¦å°†ä¸Šä¼ çš„æ–‡ä»¶æ¨é€åˆ°çš„åˆ†æ”¯ã€‚

+   `commit_description`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” å°†è¦åˆ›å»ºçš„æäº¤çš„æè¿°

+   `tags`ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” è¦æ¨é€åˆ° Hub ä¸Šçš„æ ‡ç­¾åˆ—è¡¨ã€‚

å°†åˆ†è¯å™¨æ–‡ä»¶ä¸Šä¼ åˆ°ğŸ¤—æ¨¡å‹ Hubã€‚

ç¤ºä¾‹ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Push the tokenizer to your namespace with the name "my-finetuned-bert".
tokenizer.push_to_hub("my-finetuned-bert")

# Push the tokenizer to an organization with the name "my-finetuned-bert".
tokenizer.push_to_hub("huggingface/my-finetuned-bert")
```

#### `convert_ids_to_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L953)

```py
( ids: Union skip_special_tokens: bool = False ) â†’ export const metadata = 'undefined';str or List[str]
```

å‚æ•°

+   `ids`ï¼ˆ`int`æˆ–`List[int]`ï¼‰â€” è¦è½¬æ¢ä¸ºæ ‡è®°çš„æ ‡è®° idï¼ˆæˆ–æ ‡è®° idï¼‰ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

`str` æˆ– `List[str]`

è§£ç åçš„æ ‡è®°ã€‚

ä½¿ç”¨è¯æ±‡è¡¨å’Œæ·»åŠ çš„æ ‡è®°å°†å•ä¸ªç´¢å¼•æˆ–ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ ‡è®°æˆ–æ ‡è®°åºåˆ—ã€‚

#### `convert_tokens_to_ids`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L630)

```py
( tokens: Union ) â†’ export const metadata = 'undefined';int or List[int]
```

å‚æ•°

+   `tokens` (`str` æˆ– `List[str]`) â€” è¦è½¬æ¢ä¸ºæ ‡è®° ID çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ ‡è®°ã€‚

è¿”å›

`int` æˆ– `List[int]`

æ ‡è®° ID æˆ–æ ‡è®° ID åˆ—è¡¨ã€‚

å°†æ ‡è®°å­—ç¬¦ä¸²ï¼ˆæˆ–æ ‡è®°åºåˆ—ï¼‰è½¬æ¢ä¸ºå•ä¸ªæ•´æ•° IDï¼ˆæˆ– ID åºåˆ—ï¼‰ï¼Œä½¿ç”¨è¯æ±‡è¡¨ã€‚

#### `get_added_vocab`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L415)

```py
( ) â†’ export const metadata = 'undefined';Dict[str, int]
```

è¿”å›

`Dict[str, int]`

æ·»åŠ çš„æ ‡è®°ã€‚

å°†è¯æ±‡è¡¨ä¸­çš„æ·»åŠ æ ‡è®°ä½œä¸ºæ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸è¿”å›ã€‚ç»“æœå¯èƒ½ä¸å¿«é€Ÿè°ƒç”¨ä¸åŒï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬æ€»æ˜¯æ·»åŠ æ ‡è®°ï¼Œå³ä½¿å®ƒä»¬å·²ç»åœ¨è¯æ±‡è¡¨ä¸­ã€‚è¿™æ˜¯æˆ‘ä»¬åº”è¯¥æ›´æ”¹çš„äº‹æƒ…ã€‚

#### `num_special_tokens_to_add`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L518)

```py
( pair: bool = False ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `pair` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” åœ¨åºåˆ—å¯¹æˆ–å•ä¸ªåºåˆ—çš„æƒ…å†µä¸‹æ˜¯å¦åº”è®¡ç®—æ·»åŠ çš„æ ‡è®°æ•°ã€‚

è¿”å›

`int`

æ·»åŠ åˆ°åºåˆ—ä¸­çš„ç‰¹æ®Šæ ‡è®°æ•°ã€‚

è¿”å›ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ç¼–ç åºåˆ—æ—¶æ·»åŠ çš„æ ‡è®°æ•°ã€‚

è¿™ä¼šå¯¹ä¸€ä¸ªè™šæ‹Ÿè¾“å…¥è¿›è¡Œç¼–ç å¹¶æ£€æŸ¥æ·»åŠ çš„æ ‡è®°æ•°é‡ï¼Œå› æ­¤æ•ˆç‡ä¸é«˜ã€‚ä¸è¦å°†æ­¤æ”¾åœ¨è®­ç»ƒå¾ªç¯å†…ã€‚

#### `prepare_for_tokenization`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L891)

```py
( text: str is_split_into_words: bool = False **kwargs ) â†’ export const metadata = 'undefined';Tuple[str, Dict[str, Any]]
```

å‚æ•°

+   `text` (`str`) â€” è¦å‡†å¤‡çš„æ–‡æœ¬ã€‚

+   `is_split_into_words` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„å…ˆæ ‡è®°åŒ–ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†å‰²ä¸ºå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†å‰²ä¸ºå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ã€‚è¿™å¯¹äº NER æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚

+   `kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ç”¨äºæ ‡è®°åŒ–çš„å…³é”®å­—å‚æ•°ã€‚

è¿”å›

`Tuple[str, Dict[str, Any]]`

å‡†å¤‡å¥½çš„æ–‡æœ¬å’Œæœªä½¿ç”¨çš„ kwargsã€‚

åœ¨æ ‡è®°åŒ–ä¹‹å‰æ‰§è¡Œä»»ä½•å¿…è¦çš„è½¬æ¢ã€‚

æ­¤æ–¹æ³•åº”è¯¥ä» kwargs ä¸­å¼¹å‡ºå‚æ•°å¹¶è¿”å›å‰©ä½™çš„ `kwargs`ã€‚æˆ‘ä»¬åœ¨ç¼–ç è¿‡ç¨‹ç»“æŸæ—¶æµ‹è¯• `kwargs`ï¼Œä»¥ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½å·²ä½¿ç”¨ã€‚

#### `tokenize`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L541)

```py
( text: str **kwargs ) â†’ export const metadata = 'undefined';List[str]
```

å‚æ•°

+   `text` (`str`) â€” è¦ç¼–ç çš„åºåˆ—ã€‚

+   *`*kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼‰ â€” ä¼ é€’ç»™ç‰¹å®šäºæ¨¡å‹çš„ `prepare_for_tokenization` é¢„å¤„ç†æ–¹æ³•ã€‚

è¿”å›

`List[str]`

æ ‡è®°åˆ—è¡¨ã€‚

ä½¿ç”¨åˆ†è¯å™¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ ‡è®°åºåˆ—ã€‚

æŒ‰è¯æ±‡è¡¨ä¸­çš„å•è¯æˆ–å­è¯ï¼ˆBPE/SentencePieces/WordPiecesï¼‰æ‹†åˆ†ã€‚å¤„ç†æ·»åŠ çš„æ ‡è®°ã€‚

## PreTrainedTokenizerFast

PreTrainedTokenizerFast ä¾èµ–äº [tokenizers](https://huggingface.co/docs/tokenizers) åº“ã€‚ä» ğŸ¤— tokenizers åº“è·å–çš„ tokenizers å¯ä»¥éå¸¸ç®€å•åœ°åŠ è½½åˆ° ğŸ¤— transformers ä¸­ã€‚æŸ¥çœ‹ Using tokenizers from ğŸ¤— tokenizers é¡µé¢ä»¥äº†è§£å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œã€‚

### `class transformers.PreTrainedTokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L77)

```py
( *args **kwargs )
```

å‚æ•°

+   `model_max_length` (`int`, *optional*) â€” è¾“å…¥åˆ°å˜æ¢å™¨æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°æ•°è®¡ï¼‰ã€‚å½“ä½¿ç”¨ from_pretrained() åŠ è½½åˆ†è¯å™¨æ—¶ï¼Œè¿™å°†è®¾ç½®ä¸ºå­˜å‚¨åœ¨ `max_model_input_sizes` ä¸­å…³è”æ¨¡å‹çš„å€¼ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚å¦‚æœæœªæä¾›å€¼ï¼Œå°†é»˜è®¤ä¸º VERY_LARGE_INTEGER (`int(1e30)`ï¼‰ã€‚

+   `padding_side` (`str`, *optional*) â€” æ¨¡å‹åº”è¯¥åœ¨å“ªä¸€ä¾§åº”ç”¨å¡«å……ã€‚åº”è¯¥åœ¨ ['right', 'left'] ä¸­é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåç±»å±æ€§ä¸­é€‰æ‹©ã€‚

+   `truncation_side` (`str`, *optional*) â€” æ¨¡å‹åº”è¯¥åœ¨å“ªä¸€ä¾§åº”ç”¨æˆªæ–­ã€‚åº”è¯¥åœ¨ ['right', 'left'] ä¸­é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåç±»å±æ€§ä¸­é€‰æ‹©ã€‚

+   `chat_template` (`str`, *optional*) â€” ä¸€ä¸ª Jinja æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œç”¨äºæ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯åˆ—è¡¨ã€‚è¯¦ç»†æè¿°è¯·å‚é˜… [`huggingface.co/docs/transformers/chat_templating`](https://huggingface.co/docs/transformers/chat_templating)ã€‚

+   `model_input_names` (`List[string]`, *optional*) â€” æ¨¡å‹å‰å‘ä¼ é€’æ¥å—çš„è¾“å…¥åˆ—è¡¨ï¼ˆå¦‚ `"token_type_ids"` æˆ– `"attention_mask"`ï¼‰ã€‚é»˜è®¤å€¼ä»åŒåç±»å±æ€§ä¸­é€‰æ‹©ã€‚

+   `bos_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºå¥å­å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸ `self.bos_token` å’Œ `self.bos_token_id` å…³è”ã€‚

+   `eos_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºå¥å­ç»“å°¾çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸ `self.eos_token` å’Œ `self.eos_token_id` å…³è”ã€‚

+   `unk_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºè¯æ±‡å¤–æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸ `self.unk_token` å’Œ `self.unk_token_id` å…³è”ã€‚

+   `sep_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” ç”¨äºåœ¨åŒä¸€è¾“å…¥ä¸­åˆ†éš”ä¸¤ä¸ªä¸åŒå¥å­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚ BERT ä½¿ç”¨ï¼‰ã€‚å°†ä¸ `self.sep_token` å’Œ `self.sep_token_id` å…³è”ã€‚

+   `pad_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” ç”¨äºä½¿æ ‡è®°æ•°ç»„å¤§å°ç›¸åŒä»¥è¿›è¡Œæ‰¹å¤„ç†çš„ç‰¹æ®Šæ ‡è®°ã€‚ç„¶åå°†è¢«æ³¨æ„æœºåˆ¶æˆ–æŸå¤±è®¡ç®—å¿½ç•¥ã€‚å°†ä¸ `self.pad_token` å’Œ `self.pad_token_id` å…³è”ã€‚

+   `cls_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºè¾“å…¥ç±»åˆ«çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚ BERT ä½¿ç”¨ï¼‰ã€‚å°†ä¸ `self.cls_token` å’Œ `self.cls_token_id` å…³è”ã€‚

+   `mask_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºæ©ç æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç”¨äºæ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒç›®æ ‡ï¼Œå¦‚ BERTï¼‰ã€‚å°†ä¸ `self.mask_token` å’Œ `self.mask_token_id` å…³è”ã€‚

+   `additional_special_tokens` (å…ƒç»„æˆ–åˆ—è¡¨ï¼ŒåŒ…å« `str` æˆ– `tokenizers.AddedToken`, *optional*) â€” é™„åŠ ç‰¹æ®Šæ ‡è®°çš„å…ƒç»„æˆ–åˆ—è¡¨ã€‚åœ¨è¿™é‡Œæ·»åŠ å®ƒä»¬ä»¥ç¡®ä¿åœ¨ `skip_special_tokens` è®¾ç½®ä¸º True æ—¶è§£ç æ—¶è·³è¿‡å®ƒä»¬ã€‚å¦‚æœå®ƒä»¬ä¸æ˜¯è¯æ±‡çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡çš„æœ«å°¾ã€‚

+   `clean_up_tokenization_spaces` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¯¥æ¸…é™¤åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ‹†åˆ†è¾“å…¥æ–‡æœ¬æ—¶æ·»åŠ çš„ç©ºæ ¼ã€‚

+   `split_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ˜¯å¦åº”è¯¥æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚é»˜è®¤è¡Œä¸ºæ˜¯ä¸æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚è¿™æ„å‘³ç€å¦‚æœ `<s>` æ˜¯ `bos_token`ï¼Œé‚£ä¹ˆ `tokenizer.tokenize("<s>") = ['<s>`]ã€‚å¦åˆ™ï¼Œå¦‚æœ `split_special_tokens=True`ï¼Œé‚£ä¹ˆ `tokenizer.tokenize("<s>")` å°†ä¼šç»™å‡º `['<', 's', '>']`ã€‚æ­¤å‚æ•°ç›®å‰ä»…æ”¯æŒ `slow` tokenizersã€‚

+   `tokenizer_object` (`tokenizers.Tokenizer`) â€” ä¸€ä¸ªæ¥è‡ªğŸ¤— tokenizers çš„`tokenizers.Tokenizer`å¯¹è±¡ï¼Œç”¨äºå®ä¾‹åŒ–ã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜…ä½¿ç”¨ğŸ¤— tokenizersã€‚

+   `tokenizer_file` (`str`) â€” ä¸€ä¸ªæŒ‡å‘æœ¬åœ° JSON æ–‡ä»¶çš„è·¯å¾„ï¼Œè¡¨ç¤ºä»¥å‰åºåˆ—åŒ–çš„`tokenizers.Tokenizer`å¯¹è±¡ã€‚

æ‰€æœ‰å¿«é€Ÿåˆ†è¯å™¨çš„åŸºç±»ï¼ˆåŒ…è£… HuggingFace åˆ†è¯å™¨åº“ï¼‰ã€‚

ç»§æ‰¿è‡ª PreTrainedTokenizerBaseã€‚

å¤„ç†æ‰€æœ‰åˆ†è¯å’Œç‰¹æ®Šæ ‡è®°çš„å…±äº«æ–¹æ³•ï¼Œä»¥åŠç”¨äºä¸‹è½½/ç¼“å­˜/åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨çš„æ–¹æ³•ï¼Œä»¥åŠå‘è¯æ±‡è¡¨æ·»åŠ æ ‡è®°ã€‚

è¿™ä¸ªç±»è¿˜ä»¥ç»Ÿä¸€çš„æ–¹å¼åŒ…å«äº†æ‰€æœ‰åˆ†è¯å™¨çš„æ·»åŠ æ ‡è®°ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…å¤„ç†å„ç§åº•å±‚å­—å…¸ç»“æ„ï¼ˆBPEã€sentencepiece ç­‰ï¼‰çš„ç‰¹å®šè¯æ±‡å¢å¼ºæ–¹æ³•ã€‚

ç±»å±æ€§ï¼ˆæ´¾ç”Ÿç±»è¦†ç›–ï¼‰

+   `vocab_files_names` (`Dict[str, str]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºæ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œç›¸å…³å€¼ä¸ºä¿å­˜å…³è”æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚

+   `pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé«˜çº§é”®æ˜¯æ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œä½çº§é”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œç›¸å…³å€¼æ˜¯å…³è”çš„é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶çš„`url`ã€‚

+   `max_model_input_sizes` (`Dict[str, Optional[int]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œç›¸å…³å€¼ä¸ºè¯¥æ¨¡å‹çš„åºåˆ—è¾“å…¥çš„æœ€å¤§é•¿åº¦ï¼Œå¦‚æœæ¨¡å‹æ²¡æœ‰æœ€å¤§è¾“å…¥å¤§å°ï¼Œåˆ™ä¸º`None`ã€‚

+   `pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œç›¸å…³å€¼ä¸ºä¼ é€’ç»™åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ tokenizer ç±»çš„`__init__`æ–¹æ³•çš„ç‰¹å®šå‚æ•°å­—å…¸ï¼Œä½¿ç”¨ from_pretrained()æ–¹æ³•ã€‚

+   `model_input_names` (`List[str]`) â€” æ¨¡å‹å‰å‘ä¼ é€’ä¸­æœŸæœ›çš„è¾“å…¥åˆ—è¡¨ã€‚

+   `padding_side` (`str`) â€” æ¨¡å‹åº”ç”¨å¡«å……çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`'right'`æˆ–`'left'`ã€‚

+   `truncation_side` (`str`) â€” æ¨¡å‹åº”ç”¨æˆªæ–­çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`'right'`æˆ–`'left'`ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœåºåˆ—ä»¥å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰çš„å½¢å¼æä¾›ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸åºåˆ—æ‰¹æ¬¡çš„æ­§ä¹‰ï¼‰ã€‚

+   `add_special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„`PretrainedTokenizerBase.build_inputs_with_special_tokens`å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥ id çš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ `bos`æˆ–`eos`æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `padding` (`bool`, `str` æˆ– PaddingStrategy, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest'`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆæˆ–å¦‚æœåªæä¾›äº†å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚

    +   `'max_length'`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œå¡«å……ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚

    +   `False` æˆ– `'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation` (`bool`, `str` æˆ– TruncationStrategy, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest_first'`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™å°†é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸­åˆ é™¤æœ€é•¿åºåˆ—ä¸­çš„ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False` æˆ– `'do_not_truncate'`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length` (`int`, *å¯é€‰*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

+   `stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” å¦‚æœä¸`max_length`ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œå½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«è¢«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚

+   `is_split_into_words` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„åˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†å‰²ä¸ºå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™åˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†å‰²ä¸ºå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äºå‘½åå®ä½“è¯†åˆ«æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚

+   `pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´»`padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors` (`str`æˆ– TensorType, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`: è¿”å› TensorFlow `tf.constant`å¯¹è±¡ã€‚

    +   `'pt'`: è¿”å› PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`: è¿”å› Numpy `np.ndarray`å¯¹è±¡ã€‚

+   `return_token_type_ids` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å› token ç±»å‹ IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å› token ç±»å‹ IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    ä»€ä¹ˆæ˜¯ token ç±»å‹ IDï¼Ÿ

+   `return_attention_mask` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `return_overflowing_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„ token åºåˆ—ã€‚å¦‚æœæä¾›ä¸€å¯¹è¾“å…¥ id åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„ tokenã€‚

+   `return_special_tokens_mask` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›ç‰¹æ®Š token æ©ç ä¿¡æ¯ã€‚

+   `return_offsets_mapping` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›æ¯ä¸ª token çš„`(char_start, char_end)`ã€‚

    è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª PreTrainedTokenizerFast çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨ Python çš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚

+   `return_length` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚

+   `verbose` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•

è¿”å›

BatchEncoding

ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„ BatchEncodingï¼š

+   `input_ids` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„ token id åˆ—è¡¨ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `token_type_ids` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„ token ç±»å‹ id åˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*`token_type_ids`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯ token ç±»å‹ IDï¼Ÿ

+   `attention_mask` â€” æŒ‡å®šå“ªäº› token åº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*`attention_mask`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `overflowing_tokens` â€” æº¢å‡º token åºåˆ—çš„åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `num_truncated_tokens` â€” æˆªæ–­çš„ token æ•°é‡ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `special_tokens_mask` â€” ç”± 0 å’Œ 1 ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­ 1 æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Š tokenï¼Œ0 æŒ‡å®šå¸¸è§„åºåˆ— tokenï¼ˆå½“`add_special_tokens=True`å’Œ`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚

+   `length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰

å°†ä¸»è¦æ–¹æ³•æ ‡è®°åŒ–å¹¶ä¸ºæ¨¡å‹å‡†å¤‡ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹ã€‚

#### `add_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)

```py
( new_tokens: Union special_tokens: bool = False ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `new_tokens` (`str`, `tokenizers.AddedToken`æˆ–*str*åˆ—è¡¨æˆ–`tokenizers.AddedToken`) â€” ä»…å½“è¿™äº›æ ‡è®°å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰ä¼šæ·»åŠ è¿™äº›æ ‡è®°ã€‚`tokenizers.AddedToken`å°†å­—ç¬¦ä¸²æ ‡è®°åŒ…è£…èµ·æ¥ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ä¸ªæ€§åŒ–å…¶è¡Œä¸ºï¼šè¿™ä¸ªæ ‡è®°æ˜¯å¦åªåŒ¹é…å•ä¸ªå•è¯ï¼Œè¿™ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥å»é™¤å·¦ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ï¼Œè¿™ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥å»é™¤å³ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ç­‰ã€‚

+   `special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” å¯ç”¨äºæŒ‡å®šè¯¥æ ‡è®°æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°ã€‚è¿™ä¸»è¦ä¼šæ”¹å˜æ ‡å‡†åŒ–è¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œç‰¹æ®Šæ ‡è®°å¦‚ CLS æˆ–[MASK]é€šå¸¸ä¸ä¼šè¢«è½¬æ¢ä¸ºå°å†™ï¼‰ã€‚

    åœ¨ HuggingFace åˆ†è¯å™¨åº“ä¸­æŸ¥çœ‹`tokenizers.AddedToken`çš„è¯¦ç»†ä¿¡æ¯ã€‚

è¿”å›

`int`

æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°ã€‚

å‘åˆ†è¯å™¨ç±»æ·»åŠ æ–°æ ‡è®°åˆ—è¡¨ã€‚å¦‚æœæ–°æ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°å…¶ä¸­ï¼Œç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„é•¿åº¦å¼€å§‹ï¼Œå¹¶ä¸”åœ¨åº”ç”¨åˆ†è¯ç®—æ³•ä¹‹å‰å°†è¢«éš”ç¦»ã€‚å› æ­¤ï¼Œæ·»åŠ çš„æ ‡è®°å’Œåˆ†è¯ç®—æ³•çš„è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ä¸ä¼šä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†ã€‚

è¯·æ³¨æ„ï¼Œå½“å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µå¤§å°ï¼Œä»¥ä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚

ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨ resize_token_embeddings()æ–¹æ³•ã€‚

ç¤ºä¾‹ï¼š

```py
# Let's see how to increase the vocabulary of Bert model and tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

num_added_toks = tokenizer.add_tokens(["new_tok1", "my_new-tok2"])
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))
```

#### `add_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)

```py
( special_tokens_dict: Dict replace_additional_special_tokens = True ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `special_tokens_dict`ï¼ˆå­—å…¸*str*åˆ°*str*æˆ–`tokenizers.AddedToken`ï¼‰ â€” é”®åº”åœ¨é¢„å®šä¹‰ç‰¹æ®Šå±æ€§åˆ—è¡¨ä¸­ï¼š[`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`]ã€‚

    ä»…å½“è¿™äº›æ ‡è®°å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰ä¼šæ·»åŠ è¿™äº›æ ‡è®°ï¼ˆé€šè¿‡æ£€æŸ¥åˆ†è¯å™¨æ˜¯å¦å°†`unk_token`çš„ç´¢å¼•åˆ†é…ç»™å®ƒä»¬è¿›è¡Œæµ‹è¯•ï¼‰ã€‚

+   `replace_additional_special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” å¦‚æœä¸º`True`ï¼Œåˆ™ç°æœ‰çš„é¢å¤–ç‰¹æ®Šæ ‡è®°åˆ—è¡¨å°†è¢«`special_tokens_dict`ä¸­æä¾›çš„åˆ—è¡¨æ›¿æ¢ã€‚å¦åˆ™ï¼Œ`self._additional_special_tokens`å°†ä»…è¢«æ‰©å±•ã€‚åœ¨å‰ä¸€ç§æƒ…å†µä¸‹ï¼Œè¿™äº›æ ‡è®°å°†ä¸ä¼šä»åˆ†è¯å™¨çš„å®Œæ•´è¯æ±‡è¡¨ä¸­åˆ é™¤ - å®ƒä»¬åªè¢«æ ‡è®°ä¸ºéç‰¹æ®Šæ ‡è®°ã€‚è¯·è®°ä½ï¼Œè¿™ä»…å½±å“è§£ç è¿‡ç¨‹ä¸­è·³è¿‡å“ªäº›æ ‡è®°ï¼Œè€Œä¸å½±å“`added_tokens_encoder`å’Œ`added_tokens_decoder`ã€‚è¿™æ„å‘³ç€ä»¥å‰çš„`additional_special_tokens`ä»ç„¶æ˜¯æ·»åŠ çš„æ ‡è®°ï¼Œå¹¶ä¸”ä¸ä¼šè¢«æ¨¡å‹æ‹†åˆ†ã€‚

è¿”å›

`int`

æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°ã€‚

å‘ç¼–ç å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®°å­—å…¸ï¼ˆeosï¼Œpadï¼Œcls ç­‰ï¼‰å¹¶å°†å®ƒä»¬é“¾æ¥åˆ°ç±»å±æ€§ã€‚å¦‚æœç‰¹æ®Šæ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°å…¶ä¸­ï¼ˆç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„æœ€åä¸€ä¸ªç´¢å¼•å¼€å§‹ï¼‰ã€‚

å½“å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µå¤§å°ï¼Œä»¥ä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚

ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨ resize_token_embeddings()æ–¹æ³•ã€‚

ä½¿ç”¨`add_special_tokens`å°†ç¡®ä¿æ‚¨çš„ç‰¹æ®Šæ ‡è®°å¯ä»¥ä»¥å¤šç§æ–¹å¼ä½¿ç”¨ï¼š

+   è§£ç æ—¶å¯ä»¥é€šè¿‡`skip_special_tokens = True`è·³è¿‡ç‰¹æ®Šæ ‡è®°ã€‚

+   ç‰¹æ®Šæ ‡è®°ç”±åˆ†è¯å™¨ä»”ç»†å¤„ç†ï¼ˆå®ƒä»¬æ°¸è¿œä¸ä¼šè¢«æ‹†åˆ†ï¼‰ï¼Œç±»ä¼¼äº`AddedTokens`ã€‚

+   æ‚¨å¯ä»¥é€šè¿‡åˆ†è¯å™¨ç±»å±æ€§ï¼ˆå¦‚`tokenizer.cls_token`ï¼‰è½»æ¾å¼•ç”¨ç‰¹æ®Šæ ‡è®°ã€‚è¿™ä½¿å¾—å¼€å‘ä¸æ¨¡å‹æ— å…³çš„è®­ç»ƒå’Œå¾®è°ƒè„šæœ¬å˜å¾—å®¹æ˜“ã€‚

åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå·²ç»ä¸ºæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹æ³¨å†Œäº†ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚ BertTokenizer `cls_token`å·²ç»æ³¨å†Œä¸ºï¼šobj*â€™[CLS]â€™*ï¼ŒXLM çš„ä¸€ä¸ªä¹Ÿå·²ç»æ³¨å†Œä¸º`'</s>'`ï¼‰ã€‚

ç¤ºä¾‹ï¼š

```py
# Let's see how to add a new classification token to GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2Model.from_pretrained("gpt2")

special_tokens_dict = {"cls_token": "<CLS>"}

num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))

assert tokenizer.cls_token == "<CLS>"
```

#### `apply_chat_template`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)

```py
( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `conversation`ï¼ˆUnion[List[Dict[str, str]], â€œConversationâ€ï¼‰â€” ä¸€ä¸ª Conversation å¯¹è±¡æˆ–å¸¦æœ‰â€œroleâ€å’Œâ€œcontentâ€é”®çš„å­—å…¸åˆ—è¡¨ï¼Œè¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢çš„èŠå¤©å†å²ã€‚

+   `chat_template`ï¼ˆstrï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæ­¤è½¬æ¢çš„ Jinja æ¨¡æ¿ã€‚å¦‚æœæœªä¼ é€’æ­¤å‚æ•°ï¼Œåˆ™å°†ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤èŠå¤©æ¨¡æ¿ã€‚

+   `add_generation_prompt`ï¼ˆboolï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä»¥æŒ‡ç¤ºåŠ©æ‰‹æ¶ˆæ¯å¼€å§‹çš„æ ‡è®°ç»“æŸæç¤ºã€‚å½“æ‚¨æƒ³è¦ä»æ¨¡å‹ç”Ÿæˆå“åº”æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚è¯·æ³¨æ„ï¼Œæ­¤å‚æ•°å°†ä¼ é€’ç»™èŠå¤©æ¨¡æ¿ï¼Œå› æ­¤æ¨¡æ¿å¿…é¡»æ”¯æŒæ­¤å‚æ•°æ‰èƒ½äº§ç”Ÿä»»ä½•æ•ˆæœã€‚

+   `tokenize`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å‡ºè¿›è¡Œåˆ†è¯ã€‚å¦‚æœä¸º`False`ï¼Œè¾“å‡ºå°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

+   `padding`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å°†åºåˆ—å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚

+   `truncation`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨æœ€å¤§é•¿åº¦å¤„æˆªæ–­åºåˆ—ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚

+   `max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¡«å……æˆ–æˆªæ–­çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°ä¸ºå•ä½ï¼‰ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨åˆ†è¯å™¨çš„`max_length`å±æ€§ä½œä¸ºé»˜è®¤å€¼ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ– TensorTypeï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›ç‰¹å®šæ¡†æ¶çš„å¼ é‡ã€‚å¦‚æœ tokenize ä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`ï¼šè¿”å› TensorFlow `tf.Tensor`å¯¹è±¡ã€‚

    +   `'pt'`ï¼šè¿”å› PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`ï¼šè¿”å› NumPy `np.ndarray`å¯¹è±¡ã€‚

    +   `'jax'`ï¼šè¿”å› JAX `jnp.ndarray`å¯¹è±¡ã€‚**tokenizer_kwargs â€” ä¼ é€’ç»™åˆ†è¯å™¨çš„å…¶ä»– kwargsã€‚

è¿”å›

`List[int]`

è¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢æ ‡è®°åŒ–èŠå¤©çš„æ ‡è®° id åˆ—è¡¨ï¼ŒåŒ…æ‹¬æ§åˆ¶æ ‡è®°ã€‚æ­¤è¾“å‡ºå·²å‡†å¤‡å¥½ä¼ é€’ç»™æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’ï¼Œä¹Ÿå¯ä»¥é€šè¿‡`generate()`ç­‰æ–¹æ³•ä¼ é€’ã€‚

å°† Conversation å¯¹è±¡æˆ–å¸¦æœ‰`"role"`å’Œ`"content"`é”®çš„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºæ ‡è®° id åˆ—è¡¨ã€‚æ­¤æ–¹æ³•æ—¨åœ¨ä¸èŠå¤©æ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼Œå¹¶å°†è¯»å–åˆ†è¯å™¨çš„ chat_template å±æ€§ä»¥ç¡®å®šåœ¨è½¬æ¢æ—¶è¦ä½¿ç”¨çš„æ ¼å¼å’Œæ§åˆ¶æ ‡è®°ã€‚å½“ chat_template ä¸º None æ—¶ï¼Œå°†é€€å›åˆ°ç±»çº§åˆ«æŒ‡å®šçš„ default_chat_templateã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) â†’ export const metadata = 'undefined';List[str]
```

å‚æ•°

+   `sequences`ï¼ˆ`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ id çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åˆ é™¤è§£ç ä¸­çš„ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…é™¤åˆ†è¯ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚

+   `kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`List[str]`

è§£ç çš„å¥å­åˆ—è¡¨ã€‚

é€šè¿‡è°ƒç”¨ decode å°†æ ‡è®° id çš„åˆ—è¡¨åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) â†’ export const metadata = 'undefined';str
```

å‚æ•°

+   `token_ids`ï¼ˆ`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ id çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”åœ¨è§£ç æ—¶æ˜¯å¦åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€”æ˜¯å¦æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚

+   `kwargs`ï¼ˆé™„åŠ å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€”å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`str`

è§£ç åçš„å¥å­ã€‚

å°†ä¸€ç³»åˆ— id è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨æ ‡è®°å™¨å’Œè¯æ±‡è¡¨ï¼Œå¯ä»¥é€‰æ‹©åˆ é™¤ç‰¹æ®Šæ ‡è®°å¹¶æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚

ç±»ä¼¼äºæ‰§è¡Œ`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚

#### `encode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)

```py
( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) â†’ export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray
```

å‚æ•°

+   `text`ï¼ˆ`str`ï¼Œ`List[str]`æˆ–`List[int]`ï¼‰â€”è¦ç¼–ç çš„ç¬¬ä¸€ä¸ªåºåˆ—ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨`tokenize`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨`convert_tokens_to_ids`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸² idï¼‰ã€‚

+   `text_pair`ï¼ˆ`str`ï¼Œ`List[str]`æˆ–`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€”è¦ç¼–ç çš„å¯é€‰ç¬¬äºŒä¸ªåºåˆ—ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨`tokenize`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨`convert_tokens_to_ids`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸² idï¼‰ã€‚

+   `add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€”åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„`PretrainedTokenizerBase.build_inputs_with_special_tokens`å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥ id çš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ `bos`æˆ–`eos`æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `padding`ï¼ˆ`bool`ï¼Œ`str`æˆ– PaddingStrategyï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True`æˆ–`'longest'`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚

    +   `'max_length'`ï¼šå¡«å……åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚

    +   `False`æˆ–`'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation`ï¼ˆ`bool`ï¼Œ`str`æˆ– TruncationStrategyï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True`æˆ–`'longest_first'`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™å°†é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False`æˆ–`'do_not_truncate'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”ç”±æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€æ§åˆ¶è¦ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¸­éœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

+   `stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” å¦‚æœä¸ `max_length` ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“ `return_overflowing_tokens=True` æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰é‡å æ ‡è®°çš„æ•°é‡ã€‚

+   `is_split_into_words` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„å…ˆåˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†æˆå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ†è¯å™¨å°†å‡å®šè¾“å…¥å·²ç»åˆ†æˆå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äº NER æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚

+   `pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´» `padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ› `>= 7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors` (`str` æˆ– TensorType, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚

    +   `'pt'`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚

    +   `'np'`: è¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚

    **kwargs â€” ä¼ é€’ç»™ `.tokenize()` æ–¹æ³•ã€‚

è¿”å›å€¼

`List[int]`, `torch.Tensor`, `tf.Tensor` æˆ– `np.ndarray`

æ–‡æœ¬çš„æ ‡è®°åŒ– idã€‚

ä½¿ç”¨åˆ†è¯å™¨å’Œè¯æ±‡è¡¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º idï¼ˆæ•´æ•°ï¼‰åºåˆ—ã€‚

ç›¸å½“äºæ‰§è¡Œ `self.convert_tokens_to_ids(self.tokenize(text))`ã€‚

#### `push_to_hub`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

å‚æ•°

+   `repo_id` (`str`) â€” è¦å°†åˆ†è¯å™¨æ¨é€åˆ°çš„å­˜å‚¨åº“åç§°ã€‚åœ¨æ¨é€åˆ°ç»™å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚

+   `use_temp_dir` (`bool`, *å¯é€‰*) â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨åœ¨æ¨é€åˆ° Hub ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰åä¸º `repo_id` çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º `True`ï¼Œå¦åˆ™ä¸º `False`ã€‚

+   `commit_message` (`str`, *å¯é€‰*) â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º `"Upload tokenizer"`ã€‚

+   `private` (`bool`, *å¯é€‰*) â€” æ˜¯å¦åˆ›å»ºçš„å­˜å‚¨åº“åº”ä¸ºç§æœ‰ã€‚

+   `token` (`bool` æˆ– `str`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface`ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š `repo_url`ï¼Œåˆ™é»˜è®¤ä¸º `True`ã€‚

+   `max_shard_size` (`int` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º `"5GB"`) â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹å°†åˆ†ç‰‡ï¼Œæ¯ä¸ªåˆ†ç‰‡çš„å¤§å°éƒ½å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚ `"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤ä¸º `"5GB"`ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åœ¨å…è´¹çš„ Google Colab å®ä¾‹ä¸Šè½»æ¾åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç° CPU OOM é—®é¢˜ã€‚

+   `create_pr` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ä¸Šä¼ æ–‡ä»¶çš„ PR æˆ–ç›´æ¥æäº¤ã€‚

+   `safe_serialization` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†æ¨¡å‹æƒé‡è½¬æ¢ä¸º safetensors æ ¼å¼ä»¥è¿›è¡Œæ›´å®‰å…¨çš„åºåˆ—åŒ–ã€‚

+   `revision` (`str`, *å¯é€‰*) â€” è¦å°†ä¸Šä¼ çš„æ–‡ä»¶æ¨é€åˆ°çš„åˆ†æ”¯ã€‚

+   `commit_description` (`str`, *å¯é€‰*) â€” å°†åˆ›å»ºçš„æäº¤çš„æè¿°

+   `tags` (`List[str]`, *å¯é€‰*) â€” è¦æ¨é€åˆ° Hub ä¸Šçš„æ ‡ç­¾åˆ—è¡¨ã€‚

å°†åˆ†è¯å™¨æ–‡ä»¶ä¸Šä¼ åˆ° ğŸ¤— Model Hubã€‚

ç¤ºä¾‹ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Push the tokenizer to your namespace with the name "my-finetuned-bert".
tokenizer.push_to_hub("my-finetuned-bert")

# Push the tokenizer to an organization with the name "my-finetuned-bert".
tokenizer.push_to_hub("huggingface/my-finetuned-bert")
```

#### `convert_ids_to_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L369)

```py
( ids: Union skip_special_tokens: bool = False ) â†’ export const metadata = 'undefined';str or List[str]
```

å‚æ•°

+   `ids`ï¼ˆ`int`æˆ–`List[int]`ï¼‰-è¦è½¬æ¢ä¸ºæ ‡è®°çš„æ ‡è®° idï¼ˆæˆ–æ ‡è®° idï¼‰ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰-æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

`str`æˆ–`List[str]`

è§£ç åçš„æ ‡è®°ã€‚

å°†å•ä¸ªç´¢å¼•æˆ–ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ ‡è®°æˆ–æ ‡è®°åºåˆ—ï¼Œä½¿ç”¨è¯æ±‡è¡¨å’Œæ·»åŠ çš„æ ‡è®°ã€‚

#### `convert_tokens_to_ids`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L314)

```py
( tokens: Union ) â†’ export const metadata = 'undefined';int or List[int]
```

å‚æ•°

+   `tokens`ï¼ˆ`str`æˆ–`List[str]`ï¼‰-è¦è½¬æ¢ä¸ºæ ‡è®° id çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ ‡è®°ã€‚

è¿”å›

`int`æˆ–`List[int]`

æ ‡è®° id æˆ–æ ‡è®° id åˆ—è¡¨ã€‚

å°†æ ‡è®°å­—ç¬¦ä¸²ï¼ˆæˆ–æ ‡è®°åºåˆ—ï¼‰è½¬æ¢ä¸ºå•ä¸ªæ•´æ•° idï¼ˆæˆ– id åºåˆ—ï¼‰ï¼Œä½¿ç”¨è¯æ±‡è¡¨ã€‚

#### `get_added_vocab`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L238)

```py
( ) â†’ export const metadata = 'undefined';Dict[str, int]
```

è¿”å›

`Dict[str, int]`

æ·»åŠ çš„æ ‡è®°ã€‚

å°†è¯æ±‡è¡¨ä¸­æ·»åŠ çš„æ ‡è®°ä½œä¸ºæ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸è¿”å›ã€‚

#### `num_special_tokens_to_add`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L348)

```py
( pair: bool = False ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `pair`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰-åœ¨åºåˆ—å¯¹æˆ–å•ä¸ªåºåˆ—çš„æƒ…å†µä¸‹æ˜¯å¦åº”è®¡ç®—æ·»åŠ çš„æ ‡è®°æ•°ã€‚

è¿”å›

`int`

æ·»åŠ åˆ°åºåˆ—çš„ç‰¹æ®Šæ ‡è®°æ•°ã€‚

åœ¨ä½¿ç”¨ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç æ—¶è¿”å›æ·»åŠ çš„æ ‡è®°æ•°ã€‚

è¿™ä¼šå¯¹è™šæ‹Ÿè¾“å…¥è¿›è¡Œç¼–ç å¹¶æ£€æŸ¥æ·»åŠ çš„æ ‡è®°æ•°ï¼Œå› æ­¤æ•ˆç‡ä¸é«˜ã€‚ä¸è¦å°†å…¶æ”¾åœ¨è®­ç»ƒå¾ªç¯å†…ã€‚

#### `set_truncation_and_padding`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L398)

```py
( padding_strategy: PaddingStrategy truncation_strategy: TruncationStrategy max_length: int stride: int pad_to_multiple_of: Optional )
```

å‚æ•°

+   `padding_strategy`ï¼ˆPaddingStrategyï¼‰-å°†åº”ç”¨äºè¾“å…¥çš„å¡«å……ç±»å‹

+   `truncation_strategy`ï¼ˆTruncationStrategyï¼‰-å°†åº”ç”¨äºè¾“å…¥çš„æˆªæ–­ç±»å‹

+   `max_length`ï¼ˆ`int`ï¼‰-åºåˆ—çš„æœ€å¤§å¤§å°ã€‚

+   `stride`ï¼ˆ`int`ï¼‰-å¤„ç†æº¢å‡ºæ—¶è¦ä½¿ç”¨çš„æ­¥å¹…ã€‚

+   `pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰-å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºå¯ç”¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šçš„å¼ é‡æ ¸å¿ƒç‰¹åˆ«æœ‰ç”¨ã€‚

å®šä¹‰å¿«é€Ÿæ ‡è®°å™¨çš„æˆªæ–­å’Œå¡«å……ç­–ç•¥ï¼ˆç”± HuggingFace æ ‡è®°å™¨åº“æä¾›ï¼‰ï¼Œå¹¶åœ¨æ¢å¤æ ‡è®°å™¨è®¾ç½®åæ¢å¤æ ‡è®°å™¨è®¾ç½®ã€‚

æä¾›çš„æ ‡è®°å™¨åœ¨å—ç®¡ç†éƒ¨åˆ†ä¹‹å‰æ²¡æœ‰å¡«å……/æˆªæ–­ç­–ç•¥ã€‚å¦‚æœæ‚¨çš„æ ‡è®°å™¨åœ¨ä¹‹å‰è®¾ç½®äº†å¡«å……/æˆªæ–­ç­–ç•¥ï¼Œåˆ™åœ¨é€€å‡ºå—ç®¡ç†éƒ¨åˆ†æ—¶å°†é‡ç½®ä¸ºæ— å¡«å……/æˆªæ–­ã€‚

#### `train_new_from_iterator`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L687)

```py
( text_iterator vocab_size length = None new_special_tokens = None special_tokens_map = None **kwargs ) â†’ export const metadata = 'undefined';PreTrainedTokenizerFast
```

å‚æ•°

+   `text_iterator`ï¼ˆ`List[str]`çš„ç”Ÿæˆå™¨ï¼‰-è®­ç»ƒè¯­æ–™åº“ã€‚åº”è¯¥æ˜¯æ–‡æœ¬æ‰¹æ¬¡çš„ç”Ÿæˆå™¨ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ‚¨å°†æ‰€æœ‰å†…å®¹å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œåˆ™åº”è¯¥æ˜¯æ–‡æœ¬åˆ—è¡¨çš„åˆ—è¡¨ã€‚

+   `vocab_size`ï¼ˆ`int`ï¼‰-æ‚¨è¦ä¸ºæ ‡è®°å™¨è®¾ç½®çš„è¯æ±‡è¡¨å¤§å°ã€‚

+   `length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰-è¿­ä»£å™¨ä¸­åºåˆ—çš„æ€»æ•°ã€‚è¿™ç”¨äºæä¾›æœ‰æ„ä¹‰çš„è¿›åº¦è·Ÿè¸ª

+   `new_special_tokens`ï¼ˆ`str`æˆ–`AddedToken`çš„åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰-è¦æ·»åŠ åˆ°æ­£åœ¨è®­ç»ƒçš„æ ‡è®°å™¨çš„æ–°ç‰¹æ®Šæ ‡è®°åˆ—è¡¨ã€‚

+   `special_tokens_map` (`Dict[str, str]`, *å¯é€‰*) â€” å¦‚æœæ‚¨æƒ³è¦é‡å‘½åæ­¤åˆ†è¯å™¨ä½¿ç”¨çš„ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼Œè¯·åœ¨æ­¤å‚æ•°ä¸­ä¼ é€’ä¸€ä¸ªæ—§ç‰¹æ®Šæ ‡è®°åç§°åˆ°æ–°ç‰¹æ®Šæ ‡è®°åç§°çš„æ˜ å°„ã€‚

+   `kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ä» ğŸ¤— Tokenizers åº“ä¼ é€’ç»™è®­ç»ƒå™¨çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

è¿”å›

PreTrainedTokenizerFast

ä¸€ä¸ªä¸åŸå§‹åˆ†è¯å™¨ç›¸åŒç±»å‹çš„æ–°åˆ†è¯å™¨ï¼Œè®­ç»ƒäº `text_iterator`ã€‚

ä½¿ç”¨ä¸å½“å‰ç›¸åŒçš„é»˜è®¤å€¼ï¼ˆç‰¹æ®Šæ ‡è®°æˆ–æ ‡è®°åŒ–æµæ°´çº¿æ–¹é¢ï¼‰åœ¨æ–°è¯­æ–™åº“ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†è¯å™¨ã€‚

## BatchEncoding

### `class transformers.BatchEncoding`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L176)

```py
( data: Optional = None encoding: Union = None tensor_type: Union = None prepend_batch_axis: bool = False n_sequences: Optional = None )
```

å‚æ•°

+   `data` (`dict`, *å¯é€‰*) â€” ç”± `__call__`/`encode_plus`/`batch_encode_plus` æ–¹æ³•è¿”å›çš„åˆ—è¡¨/æ•°ç»„/å¼ é‡çš„å­—å…¸ï¼ˆ'input_ids'ï¼Œ'attention_mask'ç­‰ï¼‰ã€‚

+   `encoding` (`tokenizers.Encoding` æˆ– `Sequence[tokenizers.Encoding]`, *å¯é€‰*) â€” å¦‚æœåˆ†è¯å™¨æ˜¯ä¸€ä¸ªå¿«é€Ÿåˆ†è¯å™¨ï¼Œè¾“å‡ºé¢å¤–ä¿¡æ¯å¦‚ä»å•è¯/å­—ç¬¦ç©ºé—´åˆ°æ ‡è®°ç©ºé—´çš„æ˜ å°„ï¼Œåˆ™ `tokenizers.Encoding` å®ä¾‹æˆ–å®ä¾‹åˆ—è¡¨ï¼ˆç”¨äºæ‰¹æ¬¡ï¼‰ä¿å­˜æ­¤ä¿¡æ¯ã€‚

+   `tensor_type` (`Union[None, str, TensorType]`, *å¯é€‰*) â€” æ‚¨å¯ä»¥åœ¨æ­¤å¤„æä¾›ä¸€ä¸ª tensor_typeï¼Œä»¥åœ¨åˆå§‹åŒ–æ—¶å°†æ•´æ•°åˆ—è¡¨è½¬æ¢ä¸º PyTorch/TensorFlow/Numpy å¼ é‡ã€‚

+   `prepend_batch_axis` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” åœ¨è½¬æ¢ä¸ºå¼ é‡æ—¶æ˜¯å¦æ·»åŠ æ‰¹æ¬¡è½´ï¼ˆå‚è§ä¸Šé¢çš„ `tensor_type`ï¼‰ã€‚

+   `n_sequences` (`Optional[int]`, *å¯é€‰*) â€” æ‚¨å¯ä»¥åœ¨æ­¤å¤„æä¾›ä¸€ä¸ª tensor_typeï¼Œä»¥åœ¨åˆå§‹åŒ–æ—¶å°†æ•´æ•°åˆ—è¡¨è½¬æ¢ä¸º PyTorch/TensorFlow/Numpy å¼ é‡ã€‚

ä¿å­˜äº† **call**(), encode_plus() å’Œ batch_encode_plus() æ–¹æ³•çš„è¾“å‡ºï¼ˆtokens, attention_masks ç­‰ï¼‰ã€‚

æ­¤ç±»æ´¾ç”Ÿè‡ª Python å­—å…¸ï¼Œå¯ç”¨ä½œå­—å…¸ã€‚æ­¤å¤–ï¼Œæ­¤ç±»å…¬å¼€äº†å®ç”¨æ–¹æ³•ï¼Œç”¨äºå°†å•è¯/å­—ç¬¦ç©ºé—´æ˜ å°„åˆ°æ ‡è®°ç©ºé—´ã€‚

#### `char_to_token`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L555)

```py
( batch_or_char_index: int char_index: Optional = None sequence_index: int = 0 ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `batch_or_char_index` (`int`) â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡ä»…åŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•

+   `char_index` (`int`, *å¯é€‰*) â€” å¦‚æœåœ¨ *batch_or_token_index* ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚

+   `sequence_index` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” å¦‚æœæ‰¹æ¬¡ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨äºæŒ‡å®šæä¾›çš„å­—ç¬¦ç´¢å¼•å±äºä¸€å¯¹åºåˆ—ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0 æˆ– 1ï¼‰ã€‚

è¿”å›

`int`

æ ‡è®°çš„ç´¢å¼•ã€‚

è·å–ç¼–ç è¾“å‡ºä¸­åŒ…å«åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„åºåˆ—çš„æ ‡è®°ç´¢å¼•ã€‚

å¯ä»¥è°ƒç”¨ä¸ºï¼š

+   `self.char_to_token(char_index)` å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º 1

+   `self.char_to_token(batch_index, char_index)` å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äºæˆ–ç­‰äº 1

å½“è¾“å…¥åºåˆ—ä»¥é¢„åˆ†è¯åºåˆ—ï¼ˆå³ç”¨æˆ·å®šä¹‰çš„å•è¯ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç çš„æ ‡è®°ä¸æä¾›çš„åˆ†è¯å•è¯å…³è”èµ·æ¥ã€‚

#### `char_to_word`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L641)

```py
( batch_or_char_index: int char_index: Optional = None sequence_index: int = 0 ) â†’ export const metadata = 'undefined';int or List[int]
```

å‚æ•°

+   `batch_or_char_index` (`int`) â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡ä»…åŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„ç´¢å¼•ã€‚

+   `char_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™å¯ä»¥æ˜¯åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„ç´¢å¼•ã€‚

+   `sequence_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€” å¦‚æœæ‰¹æ¬¡ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨æ¥æŒ‡å®šæä¾›çš„å­—ç¬¦ç´¢å¼•å±äºè¯¥å¯¹åºåˆ—ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0 æˆ– 1ï¼‰ã€‚

è¿”å›

`int`æˆ–`List[int]`

å…³è”ç¼–ç æ ‡è®°çš„ç´¢å¼•æˆ–ç´¢å¼•ã€‚

è·å–æ‰¹æ¬¡ä¸­åºåˆ—çš„åŸå§‹å­—ç¬¦ä¸²ä¸­ä¸æ ‡è®°çš„å­—ç¬¦å¯¹åº”çš„å•è¯ã€‚

å¯ä»¥è°ƒç”¨ä¸ºï¼š

+   å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º 1ï¼Œåˆ™ä¸º`self.char_to_word(char_index)`

+   å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äº 1ï¼Œåˆ™ä¸º`self.char_to_word(batch_index, char_index)`

å½“è¾“å…¥åºåˆ—ä»¥é¢„åˆ†è¯åºåˆ—ï¼ˆå³ç”¨æˆ·å®šä¹‰çš„å•è¯ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç çš„æ ‡è®°ä¸æä¾›çš„åˆ†è¯å•è¯å…³è”èµ·æ¥ã€‚

#### `convert_to_tensors`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L680)

```py
( tensor_type: Union = None prepend_batch_axis: bool = False )
```

å‚æ•°

+   `tensor_type`ï¼ˆ`str`æˆ– TensorTypeï¼Œ*å¯é€‰*ï¼‰â€” è¦ä½¿ç”¨çš„å¼ é‡ç±»å‹ã€‚å¦‚æœæ˜¯`str`ï¼Œåº”è¯¥æ˜¯æšä¸¾ TensorType å€¼ä¹‹ä¸€ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™ä¸è¿›è¡Œä¿®æ”¹ã€‚

+   `prepend_batch_axis`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” åœ¨è½¬æ¢è¿‡ç¨‹ä¸­æ˜¯å¦æ·»åŠ æ‰¹æ¬¡ç»´åº¦ã€‚

å°†å†…éƒ¨å†…å®¹è½¬æ¢ä¸ºå¼ é‡ã€‚

#### `sequence_ids`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L319)

```py
( batch_index: int = 0 ) â†’ export const metadata = 'undefined';List[Optional[int]]
```

å‚æ•°

+   `batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€” è¦è®¿é—®çš„æ‰¹æ¬¡ä¸­çš„ç´¢å¼•ã€‚

è¿”å›

`List[Optional[int]]`

ä¸€ä¸ªæŒ‡ç¤ºæ¯ä¸ªæ ‡è®°å¯¹åº”çš„åºåˆ— id çš„åˆ—è¡¨ã€‚ç”±åˆ†è¯å™¨æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ˜ å°„åˆ°`None`ï¼Œå…¶ä»–æ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”åºåˆ—çš„ç´¢å¼•ã€‚

è¿”å›å°†æ ‡è®°æ˜ å°„åˆ°å…¶åŸå§‹å¥å­çš„ id çš„åˆ—è¡¨ï¼š

+   å¯¹äºæ·»åŠ åœ¨åºåˆ—å‘¨å›´æˆ–ä¹‹é—´çš„ç‰¹æ®Šæ ‡è®°ï¼Œä¸º`None`ï¼Œ

+   `0`è¡¨ç¤ºå¯¹åº”äºç¬¬ä¸€ä¸ªåºåˆ—ä¸­çš„å•è¯çš„æ ‡è®°ï¼Œ

+   å½“ä¸€å¯¹åºåˆ—è¢«è”åˆç¼–ç æ—¶ï¼Œå¯¹äºç¬¬äºŒä¸ªåºåˆ—ä¸­çš„å•è¯å¯¹åº”çš„æ ‡è®°ï¼Œä¸º`1`ã€‚

#### `to`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L773)

```py
( device: Union ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `device`ï¼ˆ`str`æˆ–`torch.device`ï¼‰â€” è¦æ”¾ç½®å¼ é‡çš„è®¾å¤‡ã€‚

è¿”å›

BatchEncoding

ä¿®æ”¹åçš„ç›¸åŒå®ä¾‹ã€‚

é€šè¿‡è°ƒç”¨`v.to(device)`å°†æ‰€æœ‰å€¼å‘é€åˆ°è®¾å¤‡ï¼ˆä»…é€‚ç”¨äº PyTorchï¼‰ã€‚

#### `token_to_chars`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L516)

```py
( batch_or_token_index: int token_index: Optional = None ) â†’ export const metadata = 'undefined';CharSpan
```

å‚æ•°

+   `batch_or_token_index`ï¼ˆ`int`ï¼‰â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚

+   `token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°æˆ–æ ‡è®°çš„ç´¢å¼•ã€‚

è¿”å›

CharSpan

åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„èŒƒå›´ï¼Œå¦‚æœæ ‡è®°ï¼ˆä¾‹å¦‚~~,~~ï¼‰ä¸å¯¹åº”äºåŸå§‹å­—ç¬¦ä¸²ä¸­çš„ä»»ä½•å­—ç¬¦ï¼Œåˆ™ä¸º Noneã€‚

è·å–æ‰¹æ¬¡ä¸­åºåˆ—ä¸­ç¼–ç æ ‡è®°å¯¹åº”çš„å­—ç¬¦è·¨åº¦ã€‚

å­—ç¬¦è·¨åº¦ä»¥ CharSpan å½¢å¼è¿”å›ï¼Œå…·æœ‰ï¼š

+   `start`â€” ä¸æ ‡è®°å…³è”çš„åŸå§‹å­—ç¬¦ä¸²ä¸­ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•ã€‚

+   `end`â€” è·Ÿéšä¸æ ‡è®°å…³è”çš„åŸå§‹å­—ç¬¦ä¸²ä¸­æœ€åä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•ã€‚

å¯ä»¥è°ƒç”¨ä¸ºï¼š

+   å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º 1ï¼Œåˆ™ä¸º`self.token_to_chars(token_index)`

+   å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äºæˆ–ç­‰äº 1ï¼Œåˆ™ä¸º`self.token_to_chars(batch_index, token_index)`

#### `token_to_sequence`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L386)

```py
( batch_or_token_index: int token_index: Optional = None ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `batch_or_token_index`ï¼ˆ`int`ï¼‰â€”æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œè¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚

+   `token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚

è¿”å›

`int`

è¾“å…¥åºåˆ—ä¸­çš„å•è¯ç´¢å¼•ã€‚

è·å–ç»™å®šæ ‡è®°è¡¨ç¤ºçš„åºåˆ—çš„ç´¢å¼•ã€‚åœ¨ä¸€èˆ¬ç”¨ä¾‹ä¸­ï¼Œæ­¤æ–¹æ³•å¯¹äºå•ä¸ªåºåˆ—æˆ–ä¸€å¯¹åºåˆ—çš„ç¬¬ä¸€ä¸ªåºåˆ—è¿”å›`0`ï¼Œå¯¹äºä¸€å¯¹åºåˆ—çš„ç¬¬äºŒä¸ªåºåˆ—è¿”å›`1`

å¯ä»¥è°ƒç”¨ä¸ºï¼š

+   å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º 1ï¼Œåˆ™ä¸º`self.token_to_sequence(token_index)`

+   å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äº 1ï¼Œåˆ™ä¸º`self.token_to_sequence(batch_index, token_index)`

å½“è¾“å…¥åºåˆ—ä»¥é¢„æ ‡è®°åºåˆ—ï¼ˆå³ï¼Œå•è¯ç”±ç”¨æˆ·å®šä¹‰ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç æ ‡è®°ä¸æä¾›çš„æ ‡è®°åŒ–å•è¯å…³è”èµ·æ¥ã€‚

#### `token_to_word`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L425)

```py
( batch_or_token_index: int token_index: Optional = None ) â†’ export const metadata = 'undefined';int
```

å‚æ•°

+   `batch_or_token_index`ï¼ˆ`int`ï¼‰â€”æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œè¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚

+   `token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚

è¿”å›

`int`

è¾“å…¥åºåˆ—ä¸­çš„å•è¯ç´¢å¼•ã€‚

è·å–ä¸æ‰¹æ¬¡åºåˆ—ä¸­ç¼–ç æ ‡è®°å¯¹åº”çš„å•è¯çš„ç´¢å¼•ã€‚

å¯ä»¥è°ƒç”¨ä¸ºï¼š

+   å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º 1ï¼Œåˆ™ä¸º`self.token_to_word(token_index)`

+   å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äº 1ï¼Œåˆ™ä¸º`self.token_to_word(batch_index, token_index)`

å½“è¾“å…¥åºåˆ—ä»¥é¢„æ ‡è®°åºåˆ—ï¼ˆå³ï¼Œå•è¯ç”±ç”¨æˆ·å®šä¹‰ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç æ ‡è®°ä¸æä¾›çš„æ ‡è®°åŒ–å•è¯å…³è”èµ·æ¥ã€‚

#### `tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L301)

```py
( batch_index: int = 0 ) â†’ export const metadata = 'undefined';List[str]
```

å‚æ•°

+   `batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€”è¦è®¿é—®çš„æ‰¹æ¬¡ç´¢å¼•ã€‚

è¿”å›

`List[str]`

è¯¥ç´¢å¼•å¤„çš„æ ‡è®°åˆ—è¡¨ã€‚

è¿”å›ç»™å®šæ‰¹æ¬¡ç´¢å¼•å¤„çš„æ ‡è®°åˆ—è¡¨ï¼ˆåœ¨å•è¯/å­è¯æ‹†åˆ†åå’Œè½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•ä¹‹å‰çš„è¾“å…¥å­—ç¬¦ä¸²çš„å­éƒ¨åˆ†ï¼‰ï¼ˆä»…é€‚ç”¨äºå¿«é€Ÿæ ‡è®°å™¨çš„è¾“å‡ºï¼‰ã€‚

#### `word_ids`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L367)

```py
( batch_index: int = 0 ) â†’ export const metadata = 'undefined';List[Optional[int]]
```

å‚æ•°

+   `batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€”è¦è®¿é—®çš„æ‰¹æ¬¡ç´¢å¼•ã€‚

è¿”å›

`List[Optional[int]]`

ä¸€ä¸ªåˆ—è¡¨ï¼ŒæŒ‡ç¤ºæ¯ä¸ªæ ‡è®°å¯¹åº”çš„å•è¯ã€‚æ ‡è®°å™¨æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ˜ å°„åˆ°`None`ï¼Œå…¶ä»–æ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”å•è¯çš„ç´¢å¼•ï¼ˆå¦‚æœå®ƒä»¬æ˜¯è¯¥å•è¯çš„ä¸€éƒ¨åˆ†ï¼Œåˆ™å‡ ä¸ªæ ‡è®°å°†æ˜ å°„åˆ°ç›¸åŒçš„å•è¯ç´¢å¼•ï¼‰ã€‚

è¿”å›ä¸€ä¸ªå°†æ ‡è®°æ˜ å°„åˆ°åˆå§‹å¥å­ä¸­å®é™…å•è¯çš„åˆ—è¡¨ï¼Œç”¨äºå¿«é€Ÿæ ‡è®°å™¨ã€‚

#### `word_to_chars`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L596)

```py
( batch_or_word_index: int word_index: Optional = None sequence_index: int = 0 ) â†’ export const metadata = 'undefined';CharSpan or List[CharSpan]
```

å‚æ•°

+   `batch_or_word_index`ï¼ˆ`int`ï¼‰â€”æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œè¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•

+   `word_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚

+   `sequence_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€”å¦‚æœæ‰¹æ¬¡ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨äºæŒ‡å®šæä¾›çš„å•è¯ç´¢å¼•å±äºè¯¥å¯¹ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0 æˆ– 1ï¼‰ã€‚

è¿”å›

`CharSpan`æˆ–`List[CharSpan]`

ä¸å­—ç¬¦ä¸²ä¸­ç›¸å…³å­—ç¬¦æˆ–å­—ç¬¦çš„èŒƒå›´ã€‚CharSpan æ˜¯ NamedTupleï¼Œå…·æœ‰ï¼š

+   start: åŸå§‹å­—ç¬¦ä¸²ä¸­ä¸æ ‡è®°å…³è”çš„ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•

+   end: åŸå§‹å­—ç¬¦ä¸²ä¸­ä¸æ ‡è®°å…³è”çš„æœ€åä¸€ä¸ªå­—ç¬¦åé¢çš„å­—ç¬¦çš„ç´¢å¼•

è·å–æ‰¹å¤„ç†åºåˆ—ä¸­ç»™å®šå•è¯å¯¹åº”çš„åŸå§‹å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦èŒƒå›´ã€‚

å­—ç¬¦èŒƒå›´ä»¥ CharSpan NamedTuple å½¢å¼è¿”å›ï¼š

+   start: åŸå§‹å­—ç¬¦ä¸²ä¸­çš„ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•

+   end: åŸå§‹å­—ç¬¦ä¸²ä¸­æœ€åä¸€ä¸ªå­—ç¬¦åé¢çš„å­—ç¬¦çš„ç´¢å¼•

å¯ä»¥è°ƒç”¨ä¸ºï¼š

+   å¦‚æœæ‰¹å¤„ç†å¤§å°ä¸º 1ï¼Œåˆ™ä¸º`self.word_to_chars(word_index)`

+   å¦‚æœæ‰¹å¤„ç†å¤§å°å¤§äºæˆ–ç­‰äº 1ï¼Œåˆ™ä¸º`self.word_to_chars(batch_index, word_index)`

#### `word_to_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L463)

```py
( batch_or_word_index: int word_index: Optional = None sequence_index: int = 0 ) â†’ export const metadata = 'undefined';(TokenSpan, optional)
```

å‚æ•°

+   `batch_or_word_index`ï¼ˆ`int`ï¼‰â€” æ‰¹å¤„ç†ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹å¤„ç†ä»…åŒ…æ‹¬ä¸€ä¸ªåºåˆ—ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚

+   `word_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹å¤„ç†ç´¢å¼•ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚

+   `sequence_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€” å¦‚æœæ‰¹å¤„ç†ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨äºæŒ‡å®šæä¾›çš„å•è¯ç´¢å¼•å±äºä¸€å¯¹åºåˆ—ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0 æˆ– 1ï¼‰ã€‚

è¿”å›

(TokenSpanï¼Œ*å¯é€‰*)

ç¼–ç åºåˆ—ä¸­çš„æ ‡è®°èŒƒå›´ã€‚å¦‚æœæ²¡æœ‰æ ‡è®°ä¸è¯¥å•è¯å¯¹åº”ï¼Œåˆ™è¿”å›`None`ã€‚è¿™å¯èƒ½ä¼šå‘ç”Ÿï¼Œç‰¹åˆ«æ˜¯å½“æ ‡è®°æ˜¯ç”¨äºæ ¼å¼åŒ–æ ‡è®°åŒ–çš„ç‰¹æ®Šæ ‡è®°æ—¶ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬åœ¨æ ‡è®°åŒ–çš„å¼€å¤´æ·»åŠ ä¸€ä¸ªç±»æ ‡è®°æ—¶ã€‚

è·å–ä¸æ‰¹å¤„ç†åºåˆ—ä¸­çš„å•è¯å¯¹åº”çš„ç¼–ç æ ‡è®°èŒƒå›´ã€‚

æ ‡è®°èŒƒå›´ä»¥ TokenSpan å½¢å¼è¿”å›ï¼š

+   `start` â€” ç¬¬ä¸€ä¸ªæ ‡è®°çš„ç´¢å¼•ã€‚

+   `end` â€” æœ€åä¸€ä¸ªæ ‡è®°åé¢çš„æ ‡è®°çš„ç´¢å¼•ã€‚

å¯ä»¥è°ƒç”¨ä¸ºï¼š

+   å¦‚æœæ‰¹å¤„ç†å¤§å°ä¸º 1ï¼Œåˆ™ä¸º`self.word_to_tokens(word_index, sequence_index: int = 0)`

+   å¦‚æœæ‰¹å¤„ç†å¤§å°å¤§äºæˆ–ç­‰äº 1ï¼Œåˆ™ä¸º`self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)`

å½“è¾“å…¥åºåˆ—ä»¥é¢„åˆ†è¯åºåˆ—ï¼ˆå³ç”¨æˆ·å®šä¹‰çš„å•è¯ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç æ ‡è®°ä¸æä¾›çš„åˆ†è¯å•è¯å…³è”èµ·æ¥ã€‚

#### `words`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L343)

```py
( batch_index: int = 0 ) â†’ export const metadata = 'undefined';List[Optional[int]]
```

å‚æ•°

+   `batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰â€” è¦è®¿é—®çš„æ‰¹å¤„ç†ä¸­çš„ç´¢å¼•ã€‚

è¿”å›

`List[Optional[int]]`

æŒ‡ç¤ºæ¯ä¸ªæ ‡è®°å¯¹åº”çš„å•è¯çš„åˆ—è¡¨ã€‚æ ‡è®°å™¨æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ˜ å°„åˆ°`None`ï¼Œå…¶ä»–æ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”å•è¯çš„ç´¢å¼•ï¼ˆå¦‚æœå®ƒä»¬æ˜¯è¯¥å•è¯çš„ä¸€éƒ¨åˆ†ï¼Œåˆ™å‡ ä¸ªæ ‡è®°å°†æ˜ å°„åˆ°ç›¸åŒçš„å•è¯ç´¢å¼•ï¼‰ã€‚

è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå°†æ ‡è®°æ˜ å°„åˆ°åˆå§‹å¥å­ä¸­çš„å®é™…å•è¯ï¼Œä»¥ä¾¿å¿«é€Ÿæ ‡è®°åŒ–å™¨ä½¿ç”¨ã€‚
