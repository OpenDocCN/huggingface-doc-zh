# 分词器

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer)

分词器负责为模型准备输入。该库包含所有模型的分词器。大多数分词器有两种版本：完整的 Python 实现和基于 Rust 库的“快速”实现[🤗 Tokenizers](https://github.com/huggingface/tokenizers)。 “快速”实现允许：

1.  特别是在进行批量分词时，可以显著加快速度。

1.  额外的方法用于在原始字符串（字符和单词）和标记空间之间进行映射（例如，获取包含给定字符的标记的索引或与给定标记对应的字符范围）。

基类 PreTrainedTokenizer 和 PreTrainedTokenizerFast 实现了对模型输入中的字符串输入进行编码的常用方法（见下文），并且可以从本地文件或目录或从库提供的预训练分词器（从 HuggingFace 的 AWS S3 存储库下载）实例化/保存 Python 和“快速”分词器。它们都依赖于包含常用方法的 PreTrainedTokenizerBase，以及 SpecialTokensMixin。

PreTrainedTokenizer 和 PreTrainedTokenizerFast 因此实现了使用所有分词器的主要方法：

+   分词（将字符串拆分为子词标记字符串），将标记字符串转换为 ID 并返回，以及编码/解码（即，分词和转换为整数）。

+   以独立于底层结构（BPE，SentencePiece 等）的方式向词汇表中添加新标记。

+   管理特殊标记（如掩码，句子开头等）：添加它们，将它们分配给分词器中的属性以便轻松访问，并确保它们在分词过程中不被拆分。

BatchEncoding 保存了 PreTrainedTokenizerBase 的编码方法（`__call__`，`encode_plus`和`batch_encode_plus`）的输出，并从 Python 字典派生。当分词器是纯 Python 分词器时，此类的行为就像标准 Python 字典一样，并保存这些方法计算的各种模型输入（`input_ids`，`attention_mask`等）。当分词器是“快速”分词器（即由 HuggingFace 的[tokenizers 库](https://github.com/huggingface/tokenizers)支持）时，此类还提供了几种高级对齐方法，可用于在原始字符串（字符和单词）和标记空间之间进行映射（例如，获取包含给定字符的标记的索引或与给定标记对应的字符范围）。

## PreTrainedTokenizer

### `class transformers.PreTrainedTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L335)

```py
( **kwargs )
```

参数

+   `model_max_length`（`int`，*可选*）— 输入到变换器模型的最大长度（以标记数计）。当使用 from_pretrained()加载分词器时，这将设置为存储在`max_model_input_sizes`中的相关模型的值（见上文）。如果未提供值，将默认为 VERY_LARGE_INTEGER（`int(1e30)`）。

+   `padding_side`（`str`，*可选*）— 模型应该应用填充的一侧。应该在[‘right’，‘left’]之间选择。默认值从同名的类属性中选择。

+   `truncation_side`（`str`，*可选*）— 模型应该应用截断的一侧。应该在[‘right’，‘left’]之间选择。默认值从同名的类属性中选择。

+   `chat_template`（`str`，*可选*）— 用于格式化聊天消息列表的 Jinja 模板字符串。查看[`huggingface.co/docs/transformers/chat_templating`](https://huggingface.co/docs/transformers/chat_templating)获取完整描述。

+   `model_input_names`（`List[string]`，*可选*）— 模型前向传递接受的输入列表（如`"token_type_ids"`或`"attention_mask"`）。默认值从同名的类属性中选择。

+   `bos_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示句子开头的特殊标记。将与`self.bos_token`和`self.bos_token_id`相关联。

+   `eos_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示句子结束的特殊标记。将与`self.eos_token`和`self.eos_token_id`相关联。

+   `unk_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示词汇外标记的特殊标记。将与`self.unk_token`和`self.unk_token_id`相关联。

+   `sep_token`（`str`或`tokenizers.AddedToken`，*可选*）— 在同一输入中分隔两个不同句子的特殊标记（例如 BERT 使用）。将与`self.sep_token`和`self.sep_token_id`相关联。

+   `pad_token`（`str`或`tokenizers.AddedToken`，*可选*）— 用于使标记数组大小相同以进行批处理的特殊标记。然后将被注意机制或损失计算忽略。将与`self.pad_token`和`self.pad_token_id`相关联。

+   `cls_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示输入类别的特殊标记（例如 BERT 使用）。将与`self.cls_token`和`self.cls_token_id`相关联。

+   `mask_token`（`str`或`tokenizers.AddedToken`，*可选*）— 表示掩码标记的特殊标记（用于掩码语言建模预训练目标，如 BERT）。将与`self.mask_token`和`self.mask_token_id`相关联。

+   `additional_special_tokens`（`str`或`tokenizers.AddedToken`的元组或列表，*可选*）— 附加特殊标记的元组或列表。在此添加它们以确保在设置`skip_special_tokens`为 True 时解码时跳过它们。如果它们不是词汇的一部分，它们将被添加到词汇的末尾。

+   `clean_up_tokenization_spaces`（`bool`，*可选*，默认为`True`）— 模型是否应清除在标记化过程中拆分输入文本时添加的空格。

+   `split_special_tokens` (`bool`, *optional*, defaults to `False`) — 是否在标记化过程中拆分特殊标记。默认行为是不拆分特殊标记。这意味着如果 `<s>` 是 `bos_token`，那么 `tokenizer.tokenize("<s>") = ['<s>`]`。否则，如果 `split_special_tokens=True`，那么 `tokenizer.tokenize("<s>")` 将会给出 `['<', 's', '>']`。此参数目前仅支持`slow`类型的分词器。

所有慢分词器的基类。

继承自 PreTrainedTokenizerBase。

处理所有用于标记化和特殊标记的共享方法，以及用于下载/缓存/加载预训练 tokenizer 以及向词汇表添加标记的方法。

这个类还以统一的方式包含了所有 tokenizer 的添加标记，因此我们不必处理各种底层字典结构（BPE、sentencepiece 等）的特定词汇增强方法。

类属性（由派生类覆盖）

+   `vocab_files_names` (`Dict[str, str]`) — 一个字典，键是模型所需的每个词汇文件的`__init__`关键字名称，值是保存相关文件的文件名（字符串）。

+   `pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) — 一个字典的字典，高级键是模型所需的每个词汇文件的`__init__`关键字名称，低级键是预训练模型的`short-cut-names`，值是相关预训练词汇文件的`url`。

+   `max_model_input_sizes` (`Dict[str, Optional[int]]`) — 一个字典，键是预训练模型的`short-cut-names`，值是该模型的序列输入的最大长度，如果模型没有最大输入大小，则为`None`。

+   `pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) — 一个字典，键是预训练模型的`short-cut-names`，值是一个字典，包含加载预训练模型时传递给 tokenizer 类的`__init__`方法的特定参数。

+   `model_input_names` (`List[str]`) — 模型前向传递中预期的输入列表。

+   `padding_side` (`str`) — 模型应用填充的默认方向。应为`'right'`或`'left'`。

+   `truncation_side` (`str`) — 模型应用截断的默认方向。应为`'right'`或`'left'`。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `add_special_tokens`（`bool`，*可选*，默认为 `True`） — 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens` 函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加 `bos` 或 `eos` 标记，则这很有用。

+   `padding`（`bool`，`str` 或 PaddingStrategy，*可选*，默认为 `False`） — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。

    +   `'max_length'`：填充到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则填充到模型可接受的最大输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：不进行填充（即，可以输出长度不同的序列批次）。

+   `truncation`（`bool`，`str` 或 TruncationStrategy，*可选*，默认为 `False`） — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则将逐个标记截断，从序列对中最长的序列中删除一个标记。

    +   `'only_first'`：截断到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则只会截断第一个序列。

    +   `'only_second'`：截断到指定的最大长度，可以通过参数 `max_length` 指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length`（`int`，*可选*） — 由截断/填充参数之一使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为 0） — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `is_split_into_words`（`bool`，*可选*，默认为 `False`） — 输入是否已经预先分词（例如，已经分成单词）。如果设置为 `True`，分词器会假定输入已经分成单词（例如，通过空格分割），然后进行分词。这对于命名实体识别或标记分类很有用。

+   `pad_to_multiple_of`（`int`，*可选*） — 如果设置，将填充序列到提供的值的倍数。需要激活 `padding`。这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors`（`str` 或 TensorType，*可选*） — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor`对象。

    +   `'np'`：返回 Numpy `np.ndarray`对象。

+   `return_token_type_ids`（`bool`，*可选*）— 是否返回 token 类型 ID。如果保持默认设置，将根据特定分词器的默认值返回 token 类型 ID，由`return_outputs`属性定义。

    什么是 token 类型 ID？

+   `return_attention_mask`（`bool`，*可选*）— 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由`return_outputs`属性定义。

    什么是注意力掩码？

+   `return_overflowing_tokens`（`bool`，*可选*，默认为`False`）— 是否返回溢出的标记序列。如果提供一对输入 id 序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的标记。

+   `return_special_tokens_mask`（`bool`，*可选*，默认为`False`）— 是否返回特殊标记掩码信息。

+   `return_offsets_mapping`（`bool`，*可选*，默认为`False`）— 是否返回每个标记的`(char_start, char_end)`。

    仅适用于继承自 PreTrainedTokenizerFast 的快速分词器，如果使用 Python 的分词器，此方法将引发`NotImplementedError`。

+   `return_length`（`bool`，*可选*，默认为`False`）— 是否返回编码输入的长度。

+   `verbose`（`bool`，*可选*，默认为`True`）— 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

BatchEncoding

一个具有以下字段的 BatchEncoding：

+   `input_ids` — 要提供给模型的 token id 列表。

    什么是输入 ID？

+   `token_type_ids` — 要提供给模型的 token 类型 id 列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    什么是 token 类型 ID？

+   `attention_mask` — 指定哪些标记应由模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    什么是注意力掩码？

+   `overflowing_tokens` — 溢出标记序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 截断的标记数（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由 0 和 1 组成的列表，其中 1 指定添加的特殊标记，0 指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

用于对一个或多个序列或一个或多个序列对进行分词和准备模型的主要方法。

#### `add_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)

```py
( new_tokens: Union special_tokens: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `new_tokens`（`str`，`tokenizers.AddedToken`或*str*列表或`tokenizers.AddedToken`）— 仅当它们尚未在词汇表中时才添加标记。`tokenizers.AddedToken`包装一个字符串标记，让您个性化其行为：此标记是否仅匹配单个单词，此标记是否应剥离左侧的所有潜在空格，此标记是否应剥离右侧的所有潜在空格等。

+   `special_tokens`（`bool`，*可选*，默认为`False`）- 可用于指定标记是否为特殊标记。这主要会改变标准化行为（例如，特殊标记如 CLS 或[MASK]通常不会被小写）。

    在 HuggingFace 分词器库中查看`tokenizers.AddedToken`的详细信息。

返回

`int`

添加到词汇表中的标记数量。

向分词器类添加一组新标记。如果新标记不在词汇表中，则它们将被添加到词汇表中，索引从当前词汇表的长度开始，并且在应用分词算法之前将被隔离。因此，添加的标记和分词算法的词汇表中的标记不会以相同的方式处理。

请注意，当向词汇表添加新标记时，您应该确保还调整模型的标记嵌入矩阵，使其嵌入矩阵与分词器匹配。

为了实现这一点，请使用 resize_token_embeddings()方法。

示例：

```py
# Let's see how to increase the vocabulary of Bert model and tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

num_added_toks = tokenizer.add_tokens(["new_tok1", "my_new-tok2"])
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))
```

#### `add_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)

```py
( special_tokens_dict: Dict replace_additional_special_tokens = True ) → export const metadata = 'undefined';int
```

参数

+   `special_tokens_dict`（字典*str*到*str*或`tokenizers.AddedToken`）- 键应该在预定义特殊属性列表中：[`bos_token`、`eos_token`、`unk_token`、`sep_token`、`pad_token`、`cls_token`、`mask_token`、`additional_special_tokens`]。

    只有当标记尚未在词汇表中时才会添加标记（通过检查分词器是否将`unk_token`的索引分配给它们进行测试）。

+   `replace_additional_special_tokens`（`bool`，*可选*，默认为`True`）- 如果为`True`，则现有的额外特殊标记列表将被替换为`special_tokens_dict`中提供的列表。否则，`self._additional_special_tokens`将只是扩展。在前一种情况下，这些标记不会从分词器的完整词汇表中删除-它们只被标记为非特殊标记。请记住，这只影响解码时跳过哪些标记，而不是`added_tokens_encoder`和`added_tokens_decoder`。这意味着以前的`additional_special_tokens`仍然是添加的标记，并且不会被模型拆分。

返回

`int`

添加到词汇表中的标记数量。

向编码器添加特殊标记字典（eos、pad、cls 等）并将它们链接到类属性。如果特殊标记不在词汇表中，则它们将被添加到词汇表中（索引从当前词汇表的最后一个索引开始）。

在向词汇表添加新标记时，您应该确保还调整模型的标记嵌入矩阵，使其嵌入矩阵与分词器匹配。

为了实现这一点，请使用 resize_token_embeddings()方法。

使用`add_special_tokens`将确保您的特殊标记可以以多种方式使用：

+   在解码时可以通过`skip_special_tokens = True`跳过特殊标记。

+   分词器会仔细处理特殊标记（它们永远不会被拆分），类似于`AddedTokens`。

+   您可以使用分词器类属性如`tokenizer.cls_token`轻松引用特殊标记。这样可以轻松开发与模型无关的训练和微调脚本。

在可能的情况下，特殊标记已经为提供的预训练模型注册（例如 BertTokenizer `cls_token`已经注册为:obj*’[CLS]’*，XLM 的一个也已经注册为`'</s>'`）。

示例：

```py
# Let's see how to add a new classification token to GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2Model.from_pretrained("gpt2")

special_tokens_dict = {"cls_token": "<CLS>"}

num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))

assert tokenizer.cls_token == "<CLS>"
```

#### `apply_chat_template`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)

```py
( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) → export const metadata = 'undefined';List[int]
```

参数

+   `conversation`（Union[List[Dict[str, str]]，“Conversation”）— 一个 Conversation 对象或具有“role”和“content”键的字典列表，表示到目前为止的聊天历史。

+   `chat_template`（str，*可选*）— 用于此转换的 Jinja 模板。如果未传递此参数，则将使用模型的默认聊天模板。

+   `add_generation_prompt`（bool，*可选*）— 是否以指示助手消息开始的标记结束提示。当您想从模型生成响应时，这很有用。请注意，此参数将传递给聊天模板，因此模板必须支持此参数才能产生任何效果。

+   `tokenize`（`bool`，默认为`True`）— 是否对输出进行标记化。如果为`False`，输出将是一个字符串。

+   `padding`（`bool`，默认为`False`）— 是否将序列填充到最大长度。如果 tokenize 为`False`，则不起作用。

+   `truncation`（`bool`，默认为`False`）— 是否在最大长度处截断序列。如果 tokenize 为`False`，则不起作用。

+   `max_length`（`int`，*可选*）— 用于填充或截断的最大长度（以标记为单位）。如果 tokenize 为`False`，则不起作用。如果未指定，将使用分词器的`max_length`属性作为默认值。

+   `return_tensors`（`str`或 TensorType，*可选*）— 如果设置，将返回特定框架的张量。如果 tokenize 为`False`，则不起作用。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.Tensor`对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor`对象。

    +   `'np'`：返回 NumPy `np.ndarray`对象。

    +   `'jax'`：返回 JAX `jnp.ndarray`对象。**tokenizer_kwargs — 要传递给分词器的其他 kwargs。

返回

`List[int]`

表示到目前为止的标记化聊天的标记 id 列表，包括控制标记。此输出已准备好传递给模型，可以直接传递或通过`generate()`等方法传递。

将 Conversation 对象或带有`"role"`和`"content"`键的字典列表转换为标记 id 列表。此方法旨在与聊天模型一起使用，并将读取分词器的 chat_template 属性以确定在转换时要使用的格式和控制标记。当 chat_template 为 None 时，将退回到类级别指定的 default_chat_template。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';List[str]
```

参数

+   `sequences`（`Union[List[int]，List[List[int]]，np.ndarray，torch.Tensor，tf.Tensor]`）— 标记化输入 id 的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否在解码中删除特殊标记。

+   `clean_up_tokenization_spaces`（`bool`，*可选*）— 是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（其他关键字参数，*可选*）— 将传递给底层模型特定的解码方法。

返回

`List[str]`

解码后的句子列表。

通过调用解码将标记 id 的列表列表转换为字符串列表。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';str
```

参数

+   `token_ids`（`Union[int，List[int]，np.ndarray，torch.Tensor，tf.Tensor]`）— 标记化输入 id 的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否在解码中删除特殊标记。

+   `clean_up_tokenization_spaces`（`bool`，*可选*）— 是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（其他关键字参数，*可选*）— 将传递给底层模型特定的解码方法。

返回

`str`

解码后的句子。

使用标记器和词汇表将 id 序列转换为字符串，具有删除特殊标记和清理标记化空格的选项。

类似于执行 `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。

#### `encode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)

```py
( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) → export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray
```

参数

+   `text`（`str`，`List[str]` 或 `List[int]`）— 要编码的第一个序列。可以是字符串，字符串列表（使用 `tokenize` 方法进行标记化的字符串）或整数列表（使用 `convert_tokens_to_ids` 方法进行标记化的字符串 id）。

+   `text_pair`（`str`，`List[str]` 或 `List[int]`，*可选*）— 要编码的可选第二个序列。可以是字符串，字符串列表（使用 `tokenize` 方法进行标记化的字符串）或整数列表（使用 `convert_tokens_to_ids` 方法进行标记化的字符串 id）。

+   `add_special_tokens`（`bool`，*可选*，默认为 `True`）— 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens` 函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加 `bos` 或 `eos` 标记，则这很有用。

+   `padding`（`bool`，`str` 或 PaddingStrategy，*可选*，默认为 `False`）— 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。

    +   `'max_length'`：填充到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：不进行填充（即，可以输出长度不同的序列批次）。

+   `truncation`（`bool`，`str` 或 TruncationStrategy，*可选*，默认为 `False`）— 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则会逐标记截断，从一对序列中最长的序列中移除一个标记。

    +   `'only_first'`：截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第一个序列。

    +   `'only_second'`：截断到指定的最大长度，使用参数 `max_length`，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不进行截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length`（`int`，*可选*）— 控制截断/填充参数之一使用的最大长度。

    如果未设置或设置为 `None`，则如果截断/填充参数之一需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为 0）— 如果设置为数字，并且`max_length`一起设置，当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。

+   `is_split_into_words`（`bool`，*可选*，默认为`False`）— 输入是否已经预分词（例如，已经分成单词）。如果设置为`True`，则分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于 NER 或标记分类很有用。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将填充序列到提供的值的倍数。需要激活`padding`。这对于启用具有计算能力`>= 7.5`（Volta）的 NVIDIA 硬件上的 Tensor Cores 特别有用。

+   `return_tensors`（`str`或 TensorType，*可选*）— 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.constant`对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor`对象。

    +   `'np'`：返回 Numpy `np.ndarray`对象。

    **kwargs — 传递给`.tokenize()`方法。

返回

`List[int]`，`torch.Tensor`，`tf.Tensor`或`np.ndarray`

文本的标记化 id。

使用分词器和词汇表将字符串转换为 id（整数）序列。

与执行`self.convert_tokens_to_ids(self.tokenize(text))`相同。

#### `push_to_hub`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

参数

+   `repo_id`（`str`）— 您要将分词器推送到的存储库名称。在推送到给定组织时，应包含您的组织名称。

+   `use_temp_dir`（`bool`，*可选*）— 是否使用临时目录存储在推送到 Hub 之前保存的文件。如果没有名为`repo_id`的目录，则默认为`True`，否则为`False`。

+   `commit_message`（`str`，*可选*）— 推送时要提交的消息。默认为`"Upload tokenizer"`。

+   `private`（`bool`，*可选*）— 创建的存储库是否应为私有。

+   `token`（`bool`或`str`，*可选*）— 用作远程文件的 HTTP 令牌。如果为`True`，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。如果未指定`repo_url`，则默认为`True`。

+   `max_shard_size`（`int`或`str`，*可选*，默认为`"5GB"`）— 仅适用于模型。在分片之前的检查点的最大大小。然后，检查点分片将每个大小低于此大小。如果表示为字符串，需要是数字后跟一个单位（如`"5MB"`）。我们将其默认设置为`"5GB"`，以便用户可以在免费的 Google Colab 实例上轻松加载模型，而不会出现任何 CPU OOM 问题。

+   `create_pr`（`bool`，*可选*，默认为`False`）— 是否创建具有上传文件的 PR 或直接提交。

+   `safe_serialization`（`bool`，*可选*，默认为`True`）— 是否将模型权重转换为 safetensors 格式以进行更安全的序列化。

+   `revision`（`str`，*可选*）— 要将上传的文件推送到的分支。

+   `commit_description`（`str`，*可选*）— 将要创建的提交的描述

+   `tags`（`List[str]`，*可选*）— 要推送到 Hub 上的标签列表。

将分词器文件上传到🤗模型 Hub。

示例：

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Push the tokenizer to your namespace with the name "my-finetuned-bert".
tokenizer.push_to_hub("my-finetuned-bert")

# Push the tokenizer to an organization with the name "my-finetuned-bert".
tokenizer.push_to_hub("huggingface/my-finetuned-bert")
```

#### `convert_ids_to_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L953)

```py
( ids: Union skip_special_tokens: bool = False ) → export const metadata = 'undefined';str or List[str]
```

参数

+   `ids`（`int`或`List[int]`）— 要转换为标记的标记 id（或标记 id）。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否在解码中删除特殊标记。

返回

`str` 或 `List[str]`

解码后的标记。

使用词汇表和添加的标记将单个索引或索引序列转换为标记或标记序列。

#### `convert_tokens_to_ids`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L630)

```py
( tokens: Union ) → export const metadata = 'undefined';int or List[int]
```

参数

+   `tokens` (`str` 或 `List[str]`) — 要转换为标记 ID 的一个或多个标记。

返回

`int` 或 `List[int]`

标记 ID 或标记 ID 列表。

将标记字符串（或标记序列）转换为单个整数 ID（或 ID 序列），使用词汇表。

#### `get_added_vocab`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L415)

```py
( ) → export const metadata = 'undefined';Dict[str, int]
```

返回

`Dict[str, int]`

添加的标记。

将词汇表中的添加标记作为标记到索引的字典返回。结果可能与快速调用不同，因为现在我们总是添加标记，即使它们已经在词汇表中。这是我们应该更改的事情。

#### `num_special_tokens_to_add`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L518)

```py
( pair: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `pair` (`bool`, *可选*, 默认为 `False`) — 在序列对或单个序列的情况下是否应计算添加的标记数。

返回

`int`

添加到序列中的特殊标记数。

返回使用特殊标记编码序列时添加的标记数。

这会对一个虚拟输入进行编码并检查添加的标记数量，因此效率不高。不要将此放在训练循环内。

#### `prepare_for_tokenization`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L891)

```py
( text: str is_split_into_words: bool = False **kwargs ) → export const metadata = 'undefined';Tuple[str, Dict[str, Any]]
```

参数

+   `text` (`str`) — 要准备的文本。

+   `is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预先标记化（例如，已分割为单词）。如果设置为 `True`，分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行标记化。这对于 NER 或标记分类很有用。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 用于标记化的关键字参数。

返回

`Tuple[str, Dict[str, Any]]`

准备好的文本和未使用的 kwargs。

在标记化之前执行任何必要的转换。

此方法应该从 kwargs 中弹出参数并返回剩余的 `kwargs`。我们在编码过程结束时测试 `kwargs`，以确保所有参数都已使用。

#### `tokenize`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L541)

```py
( text: str **kwargs ) → export const metadata = 'undefined';List[str]
```

参数

+   `text` (`str`) — 要编码的序列。

+   *`*kwargs`（额外的关键字参数） — 传递给特定于模型的 `prepare_for_tokenization` 预处理方法。

返回

`List[str]`

标记列表。

使用分词器将字符串转换为标记序列。

按词汇表中的单词或子词（BPE/SentencePieces/WordPieces）拆分。处理添加的标记。

## PreTrainedTokenizerFast

PreTrainedTokenizerFast 依赖于 [tokenizers](https://huggingface.co/docs/tokenizers) 库。从 🤗 tokenizers 库获取的 tokenizers 可以非常简单地加载到 🤗 transformers 中。查看 Using tokenizers from 🤗 tokenizers 页面以了解如何执行此操作。

### `class transformers.PreTrainedTokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L77)

```py
( *args **kwargs )
```

参数

+   `model_max_length` (`int`, *optional*) — 输入到变换器模型的最大长度（以标记数计）。当使用 from_pretrained() 加载分词器时，这将设置为存储在 `max_model_input_sizes` 中关联模型的值（见上文）。如果未提供值，将默认为 VERY_LARGE_INTEGER (`int(1e30)`）。

+   `padding_side` (`str`, *optional*) — 模型应该在哪一侧应用填充。应该在 ['right', 'left'] 中选择。默认值从同名类属性中选择。

+   `truncation_side` (`str`, *optional*) — 模型应该在哪一侧应用截断。应该在 ['right', 'left'] 中选择。默认值从同名类属性中选择。

+   `chat_template` (`str`, *optional*) — 一个 Jinja 模板字符串，用于格式化聊天消息列表。详细描述请参阅 [`huggingface.co/docs/transformers/chat_templating`](https://huggingface.co/docs/transformers/chat_templating)。

+   `model_input_names` (`List[string]`, *optional*) — 模型前向传递接受的输入列表（如 `"token_type_ids"` 或 `"attention_mask"`）。默认值从同名类属性中选择。

+   `bos_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示句子开头的特殊标记。将与 `self.bos_token` 和 `self.bos_token_id` 关联。

+   `eos_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示句子结尾的特殊标记。将与 `self.eos_token` 和 `self.eos_token_id` 关联。

+   `unk_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示词汇外标记的特殊标记。将与 `self.unk_token` 和 `self.unk_token_id` 关联。

+   `sep_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 用于在同一输入中分隔两个不同句子的特殊标记（例如 BERT 使用）。将与 `self.sep_token` 和 `self.sep_token_id` 关联。

+   `pad_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 用于使标记数组大小相同以进行批处理的特殊标记。然后将被注意机制或损失计算忽略。将与 `self.pad_token` 和 `self.pad_token_id` 关联。

+   `cls_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示输入类别的特殊标记（例如 BERT 使用）。将与 `self.cls_token` 和 `self.cls_token_id` 关联。

+   `mask_token` (`str` 或 `tokenizers.AddedToken`, *optional*) — 表示掩码标记的特殊标记（用于掩码语言建模预训练目标，如 BERT）。将与 `self.mask_token` 和 `self.mask_token_id` 关联。

+   `additional_special_tokens` (元组或列表，包含 `str` 或 `tokenizers.AddedToken`, *optional*) — 附加特殊标记的元组或列表。在这里添加它们以确保在 `skip_special_tokens` 设置为 True 时解码时跳过它们。如果它们不是词汇的一部分，它们将被添加到词汇的末尾。

+   `clean_up_tokenization_spaces` (`bool`, *optional*, 默认为 `True`) — 模型是否应该清除在标记化过程中拆分输入文本时添加的空格。

+   `split_special_tokens` (`bool`, *optional*, 默认为 `False`) — 在标记化过程中是否应该拆分特殊标记。默认行为是不拆分特殊标记。这意味着如果 `<s>` 是 `bos_token`，那么 `tokenizer.tokenize("<s>") = ['<s>`]。否则，如果 `split_special_tokens=True`，那么 `tokenizer.tokenize("<s>")` 将会给出 `['<', 's', '>']`。此参数目前仅支持 `slow` tokenizers。

+   `tokenizer_object` (`tokenizers.Tokenizer`) — 一个来自🤗 tokenizers 的`tokenizers.Tokenizer`对象，用于实例化。更多信息请参阅使用🤗 tokenizers。

+   `tokenizer_file` (`str`) — 一个指向本地 JSON 文件的路径，表示以前序列化的`tokenizers.Tokenizer`对象。

所有快速分词器的基类（包装 HuggingFace 分词器库）。

继承自 PreTrainedTokenizerBase。

处理所有分词和特殊标记的共享方法，以及用于下载/缓存/加载预训练分词器的方法，以及向词汇表添加标记。

这个类还以统一的方式包含了所有分词器的添加标记，因此我们不必处理各种底层字典结构（BPE、sentencepiece 等）的特定词汇增强方法。

类属性（派生类覆盖）

+   `vocab_files_names` (`Dict[str, str]`) — 一个字典，键为模型所需的每个词汇文件的`__init__`关键字名称，相关值为保存关联文件的文件名（字符串）。

+   `pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) — 一个字典，高级键是模型所需的每个词汇文件的`__init__`关键字名称，低级键是预训练模型的`short-cut-names`，相关值是关联的预训练词汇文件的`url`。

+   `max_model_input_sizes` (`Dict[str, Optional[int]]`) — 一个字典，键为预训练模型的`short-cut-names`，相关值为该模型的序列输入的最大长度，如果模型没有最大输入大小，则为`None`。

+   `pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) — 一个字典，键为预训练模型的`short-cut-names`，相关值为传递给加载预训练模型时 tokenizer 类的`__init__`方法的特定参数字典，使用 from_pretrained()方法。

+   `model_input_names` (`List[str]`) — 模型前向传递中期望的输入列表。

+   `padding_side` (`str`) — 模型应用填充的默认方向。应为`'right'`或`'left'`。

+   `truncation_side` (`str`) — 模型应用截断的默认方向。应为`'right'`或`'left'`。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批次序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词的），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果序列以字符串列表（预分词）的形式提供，则必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。

+   `add_special_tokens` (`bool`, *可选*, 默认为`True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding` (`bool`, `str` 或 PaddingStrategy, *可选*, 默认为`False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（或如果只提供了单个序列，则不填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：无填充（即，可以输出长度不同的序列批次）。

+   `truncation` (`bool`, `str` 或 TruncationStrategy, *可选*, 默认为`False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则将逐标记截断，从中删除最长序列中的一个标记。

    +   `'only_first'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则只会截断第一个序列。

    +   `'only_second'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：无截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。

    如果未设置或设置为`None`，则如果截断/填充参数需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *可选*, 默认为 0) — 如果与`max_length`一起设置为一个数字，当`return_overflowing_tokens=True`时返回的溢出标记将包含被截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `is_split_into_words` (`bool`, *可选*, 默认为`False`) — 输入是否已经预分词（例如，已分割为单词）。如果设置为`True`，则分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行分词。这对于命名实体识别或标记分类很有用。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于在具有计算能力`>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors` (`str`或 TensorType, *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回 Numpy `np.ndarray`对象。

+   `return_token_type_ids` (`bool`, *可选*) — 是否返回 token 类型 ID。如果保持默认设置，将根据特定分词器的默认设置返回 token 类型 ID，由`return_outputs`属性定义。

    什么是 token 类型 ID？

+   `return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认设置返回注意力掩码，由`return_outputs`属性定义。

    什么是注意力掩码？

+   `return_overflowing_tokens` (`bool`, *可选*, 默认为`False`) — 是否返回溢出的 token 序列。如果提供一对输入 id 序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的 token。

+   `return_special_tokens_mask` (`bool`, *可选*, 默认为`False`) — 是否返回特殊 token 掩码信息。

+   `return_offsets_mapping` (`bool`, *可选*, 默认为`False`) — 是否返回每个 token 的`(char_start, char_end)`。

    这仅适用于继承自 PreTrainedTokenizerFast 的快速分词器，如果使用 Python 的分词器，此方法将引发`NotImplementedError`。

+   `return_length` (`bool`, *可选*, 默认为`False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *可选*, 默认为`True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

BatchEncoding

一个具有以下字段的 BatchEncoding：

+   `input_ids` — 要馈送给模型的 token id 列表。

    什么是输入 ID？

+   `token_type_ids` — 要馈送给模型的 token 类型 id 列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    什么是 token 类型 ID？

+   `attention_mask` — 指定哪些 token 应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    什么是注意力掩码？

+   `overflowing_tokens` — 溢出 token 序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 截断的 token 数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由 0 和 1 组成的列表，其中 1 指定添加的特殊 token，0 指定常规序列 token（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

将主要方法标记化并为模型准备一个或多个序列或一个或多个序列对。

#### `add_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)

```py
( new_tokens: Union special_tokens: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `new_tokens` (`str`, `tokenizers.AddedToken`或*str*列表或`tokenizers.AddedToken`) — 仅当这些标记尚未在词汇表中时才会添加这些标记。`tokenizers.AddedToken`将字符串标记包装起来，以便您可以个性化其行为：这个标记是否只匹配单个单词，这个标记是否应该去除左侧的所有潜在空格，这个标记是否应该去除右侧的所有潜在空格等。

+   `special_tokens` (`bool`, *可选*, 默认为 `False`) — 可用于指定该标记是否为特殊标记。这主要会改变标准化行为（例如，特殊标记如 CLS 或[MASK]通常不会被转换为小写）。

    在 HuggingFace 分词器库中查看`tokenizers.AddedToken`的详细信息。

返回

`int`

添加到词汇表中的标记数。

向分词器类添加新标记列表。如果新标记不在词汇表中，则它们将被添加到其中，索引从当前词汇表的长度开始，并且在应用分词算法之前将被隔离。因此，添加的标记和分词算法的词汇表中的标记不会以相同的方式处理。

请注意，当向词汇表添加新标记时，您应确保还调整模型的标记嵌入矩阵大小，以使其嵌入矩阵与分词器匹配。

为了实现这一点，请使用 resize_token_embeddings()方法。

示例：

```py
# Let's see how to increase the vocabulary of Bert model and tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

num_added_toks = tokenizer.add_tokens(["new_tok1", "my_new-tok2"])
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))
```

#### `add_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)

```py
( special_tokens_dict: Dict replace_additional_special_tokens = True ) → export const metadata = 'undefined';int
```

参数

+   `special_tokens_dict`（字典*str*到*str*或`tokenizers.AddedToken`） — 键应在预定义特殊属性列表中：[`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`]。

    仅当这些标记尚未在词汇表中时才会添加这些标记（通过检查分词器是否将`unk_token`的索引分配给它们进行测试）。

+   `replace_additional_special_tokens` (`bool`, *可选*, 默认为 `True`) — 如果为`True`，则现有的额外特殊标记列表将被`special_tokens_dict`中提供的列表替换。否则，`self._additional_special_tokens`将仅被扩展。在前一种情况下，这些标记将不会从分词器的完整词汇表中删除 - 它们只被标记为非特殊标记。请记住，这仅影响解码过程中跳过哪些标记，而不影响`added_tokens_encoder`和`added_tokens_decoder`。这意味着以前的`additional_special_tokens`仍然是添加的标记，并且不会被模型拆分。

返回

`int`

添加到词汇表中的标记数。

向编码器添加特殊标记字典（eos，pad，cls 等）并将它们链接到类属性。如果特殊标记不在词汇表中，则它们将被添加到其中（索引从当前词汇表的最后一个索引开始）。

当向词汇表添加新标记时，您应确保还调整模型的标记嵌入矩阵大小，以使其嵌入矩阵与分词器匹配。

为了实现这一点，请使用 resize_token_embeddings()方法。

使用`add_special_tokens`将确保您的特殊标记可以以多种方式使用：

+   解码时可以通过`skip_special_tokens = True`跳过特殊标记。

+   特殊标记由分词器仔细处理（它们永远不会被拆分），类似于`AddedTokens`。

+   您可以通过分词器类属性（如`tokenizer.cls_token`）轻松引用特殊标记。这使得开发与模型无关的训练和微调脚本变得容易。

在可能的情况下，已经为提供的预训练模型注册了特殊标记（例如 BertTokenizer `cls_token`已经注册为：obj*’[CLS]’*，XLM 的一个也已经注册为`'</s>'`）。

示例：

```py
# Let's see how to add a new classification token to GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2Model.from_pretrained("gpt2")

special_tokens_dict = {"cls_token": "<CLS>"}

num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))

assert tokenizer.cls_token == "<CLS>"
```

#### `apply_chat_template`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)

```py
( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) → export const metadata = 'undefined';List[int]
```

参数

+   `conversation`（Union[List[Dict[str, str]], “Conversation”）— 一个 Conversation 对象或带有“role”和“content”键的字典列表，表示到目前为止的聊天历史。

+   `chat_template`（str，*可选*）— 用于此转换的 Jinja 模板。如果未传递此参数，则将使用模型的默认聊天模板。

+   `add_generation_prompt`（bool，*可选*）— 是否以指示助手消息开始的标记结束提示。当您想要从模型生成响应时，这很有用。请注意，此参数将传递给聊天模板，因此模板必须支持此参数才能产生任何效果。

+   `tokenize`（`bool`，默认为`True`）— 是否对输出进行分词。如果为`False`，输出将是一个字符串。

+   `padding`（`bool`，默认为`False`）— 是否将序列填充到最大长度。如果 tokenize 为`False`，则无效。

+   `truncation`（`bool`，默认为`False`）— 是否在最大长度处截断序列。如果 tokenize 为`False`，则无效。

+   `max_length`（`int`，*可选*）— 用于填充或截断的最大长度（以标记为单位）。如果 tokenize 为`False`，则无效。如果未指定，将使用分词器的`max_length`属性作为默认值。

+   `return_tensors`（`str`或 TensorType，*可选*）— 如果设置，将返回特定框架的张量。如果 tokenize 为`False`，则无效。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.Tensor`对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor`对象。

    +   `'np'`：返回 NumPy `np.ndarray`对象。

    +   `'jax'`：返回 JAX `jnp.ndarray`对象。**tokenizer_kwargs — 传递给分词器的其他 kwargs。

返回

`List[int]`

表示到目前为止标记化聊天的标记 id 列表，包括控制标记。此输出已准备好传递给模型，可以直接传递，也可以通过`generate()`等方法传递。

将 Conversation 对象或带有`"role"`和`"content"`键的字典列表转换为标记 id 列表。此方法旨在与聊天模型一起使用，并将读取分词器的 chat_template 属性以确定在转换时要使用的格式和控制标记。当 chat_template 为 None 时，将退回到类级别指定的 default_chat_template。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';List[str]
```

参数

+   `sequences`（`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`）— 标记化输入 id 的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`）— 是否删除解码中的特殊标记。

+   `clean_up_tokenization_spaces`（`bool`，*可选*）— 是否清除分词空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（其他关键字参数，*可选*）— 将传递给底层模型特定的解码方法。

返回

`List[str]`

解码的句子列表。

通过调用 decode 将标记 id 的列表列表转换为字符串列表。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';str
```

参数

+   `token_ids`（`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`）— 标记化输入 id 的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`）—在解码时是否删除特殊标记。

+   `clean_up_tokenization_spaces`（`bool`，*可选*）—是否清理标记化空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（附加关键字参数，*可选*）—将传递给底层模型特定的解码方法。

返回

`str`

解码后的句子。

将一系列 id 转换为字符串，使用标记器和词汇表，可以选择删除特殊标记并清理标记化空格。

类似于执行`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。

#### `encode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)

```py
( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) → export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray
```

参数

+   `text`（`str`，`List[str]`或`List[int]`）—要编码的第一个序列。这可以是一个字符串，一个字符串列表（使用`tokenize`方法进行标记化的字符串）或一个整数列表（使用`convert_tokens_to_ids`方法进行标记化的字符串 id）。

+   `text_pair`（`str`，`List[str]`或`List[int]`，*可选*）—要编码的可选第二个序列。这可以是一个字符串，一个字符串列表（使用`tokenize`方法进行标记化的字符串）或一个整数列表（使用`convert_tokens_to_ids`方法进行标记化的字符串 id）。

+   `add_special_tokens`（`bool`，*可选*，默认为`True`）—在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding`（`bool`，`str`或 PaddingStrategy，*可选*，默认为`False`）—激活和控制填充。接受以下值：

    +   `True`或`'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。

    +   `'max_length'`：填充到由参数`max_length`指定的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False`或`'do_not_pad'`（默认）：不填充（即，可以输出具有不同长度序列的批次）。

+   `truncation`（`bool`，`str`或 TruncationStrategy，*可选*，默认为`False`）—激活和控制截断。接受以下值：

    +   `True`或`'longest_first'`：截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则将逐标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`：截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第一个序列。

    +   `'only_second'`：截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第二个序列。

    +   `False`或`'do_not_truncate'`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length`（`int`，*可选*）—由截断/填充参数之一控制要使用的最大长度。

    如果未设置或设置为 `None`，则如果截断/填充参数中需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。

+   `is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预先分词（例如，已分成单词）。如果设置为 `True`，分词器将假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于 NER 或标记分类很有用。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活 `padding`。这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors` (`str` 或 TensorType, *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

    **kwargs — 传递给 `.tokenize()` 方法。

返回值

`List[int]`, `torch.Tensor`, `tf.Tensor` 或 `np.ndarray`

文本的标记化 id。

使用分词器和词汇表将字符串转换为 id（整数）序列。

相当于执行 `self.convert_tokens_to_ids(self.tokenize(text))`。

#### `push_to_hub`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

参数

+   `repo_id` (`str`) — 要将分词器推送到的存储库名称。在推送到给定组织时，应包含您的组织名称。

+   `use_temp_dir` (`bool`, *可选*) — 是否使用临时目录存储在推送到 Hub 之前保存的文件。如果没有名为 `repo_id` 的目录，则默认为 `True`，否则为 `False`。

+   `commit_message` (`str`, *可选*) — 推送时要提交的消息。默认为 `"Upload tokenizer"`。

+   `private` (`bool`, *可选*) — 是否创建的存储库应为私有。

+   `token` (`bool` 或 `str`, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，将使用运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface`）。如果未指定 `repo_url`，则默认为 `True`。

+   `max_shard_size` (`int` 或 `str`, *可选*, 默认为 `"5GB"`) — 仅适用于模型。在分片之前的检查点的最大大小。然后，检查点将分片，每个分片的大小都小于此大小。如果表示为字符串，需要是数字后跟一个单位（如 `"5MB"`）。我们将其默认为 `"5GB"`，以便用户可以在免费的 Google Colab 实例上轻松加载模型，而不会出现 CPU OOM 问题。

+   `create_pr` (`bool`, *可选*, 默认为 `False`) — 是否创建一个带有上传文件的 PR 或直接提交。

+   `safe_serialization` (`bool`, *可选*, 默认为 `True`) — 是否将模型权重转换为 safetensors 格式以进行更安全的序列化。

+   `revision` (`str`, *可选*) — 要将上传的文件推送到的分支。

+   `commit_description` (`str`, *可选*) — 将创建的提交的描述

+   `tags` (`List[str]`, *可选*) — 要推送到 Hub 上的标签列表。

将分词器文件上传到 🤗 Model Hub。

示例：

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Push the tokenizer to your namespace with the name "my-finetuned-bert".
tokenizer.push_to_hub("my-finetuned-bert")

# Push the tokenizer to an organization with the name "my-finetuned-bert".
tokenizer.push_to_hub("huggingface/my-finetuned-bert")
```

#### `convert_ids_to_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L369)

```py
( ids: Union skip_special_tokens: bool = False ) → export const metadata = 'undefined';str or List[str]
```

参数

+   `ids`（`int`或`List[int]`）-要转换为标记的标记 id（或标记 id）。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`）-是否在解码中删除特殊标记。

返回

`str`或`List[str]`

解码后的标记。

将单个索引或索引序列转换为标记或标记序列，使用词汇表和添加的标记。

#### `convert_tokens_to_ids`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L314)

```py
( tokens: Union ) → export const metadata = 'undefined';int or List[int]
```

参数

+   `tokens`（`str`或`List[str]`）-要转换为标记 id 的一个或多个标记。

返回

`int`或`List[int]`

标记 id 或标记 id 列表。

将标记字符串（或标记序列）转换为单个整数 id（或 id 序列），使用词汇表。

#### `get_added_vocab`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L238)

```py
( ) → export const metadata = 'undefined';Dict[str, int]
```

返回

`Dict[str, int]`

添加的标记。

将词汇表中添加的标记作为标记到索引的字典返回。

#### `num_special_tokens_to_add`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L348)

```py
( pair: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `pair`（`bool`，*可选*，默认为`False`）-在序列对或单个序列的情况下是否应计算添加的标记数。

返回

`int`

添加到序列的特殊标记数。

在使用特殊标记对序列进行编码时返回添加的标记数。

这会对虚拟输入进行编码并检查添加的标记数，因此效率不高。不要将其放在训练循环内。

#### `set_truncation_and_padding`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L398)

```py
( padding_strategy: PaddingStrategy truncation_strategy: TruncationStrategy max_length: int stride: int pad_to_multiple_of: Optional )
```

参数

+   `padding_strategy`（PaddingStrategy）-将应用于输入的填充类型

+   `truncation_strategy`（TruncationStrategy）-将应用于输入的截断类型

+   `max_length`（`int`）-序列的最大大小。

+   `stride`（`int`）-处理溢出时要使用的步幅。

+   `pad_to_multiple_of`（`int`，*可选*）-如果设置，将序列填充到提供的值的倍数。这对于启用具有计算能力`>= 7.5`（Volta）的 NVIDIA 硬件上的张量核心特别有用。

定义快速标记器的截断和填充策略（由 HuggingFace 标记器库提供），并在恢复标记器设置后恢复标记器设置。

提供的标记器在受管理部分之前没有填充/截断策略。如果您的标记器在之前设置了填充/截断策略，则在退出受管理部分时将重置为无填充/截断。

#### `train_new_from_iterator`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L687)

```py
( text_iterator vocab_size length = None new_special_tokens = None special_tokens_map = None **kwargs ) → export const metadata = 'undefined';PreTrainedTokenizerFast
```

参数

+   `text_iterator`（`List[str]`的生成器）-训练语料库。应该是文本批次的生成器，例如，如果您将所有内容存储在内存中，则应该是文本列表的列表。

+   `vocab_size`（`int`）-您要为标记器设置的词汇表大小。

+   `length`（`int`，*可选*）-迭代器中序列的总数。这用于提供有意义的进度跟踪

+   `new_special_tokens`（`str`或`AddedToken`的列表，*可选*）-要添加到正在训练的标记器的新特殊标记列表。

+   `special_tokens_map` (`Dict[str, str]`, *可选*) — 如果您想要重命名此分词器使用的一些特殊标记，请在此参数中传递一个旧特殊标记名称到新特殊标记名称的映射。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 从 🤗 Tokenizers 库传递给训练器的额外关键字参数。

返回

PreTrainedTokenizerFast

一个与原始分词器相同类型的新分词器，训练于 `text_iterator`。

使用与当前相同的默认值（特殊标记或标记化流水线方面）在新语料库上训练一个分词器。

## BatchEncoding

### `class transformers.BatchEncoding`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L176)

```py
( data: Optional = None encoding: Union = None tensor_type: Union = None prepend_batch_axis: bool = False n_sequences: Optional = None )
```

参数

+   `data` (`dict`, *可选*) — 由 `__call__`/`encode_plus`/`batch_encode_plus` 方法返回的列表/数组/张量的字典（'input_ids'，'attention_mask'等）。

+   `encoding` (`tokenizers.Encoding` 或 `Sequence[tokenizers.Encoding]`, *可选*) — 如果分词器是一个快速分词器，输出额外信息如从单词/字符空间到标记空间的映射，则 `tokenizers.Encoding` 实例或实例列表（用于批次）保存此信息。

+   `tensor_type` (`Union[None, str, TensorType]`, *可选*) — 您可以在此处提供一个 tensor_type，以在初始化时将整数列表转换为 PyTorch/TensorFlow/Numpy 张量。

+   `prepend_batch_axis` (`bool`, *可选*, 默认为 `False`) — 在转换为张量时是否添加批次轴（参见上面的 `tensor_type`）。

+   `n_sequences` (`Optional[int]`, *可选*) — 您可以在此处提供一个 tensor_type，以在初始化时将整数列表转换为 PyTorch/TensorFlow/Numpy 张量。

保存了 **call**(), encode_plus() 和 batch_encode_plus() 方法的输出（tokens, attention_masks 等）。

此类派生自 Python 字典，可用作字典。此外，此类公开了实用方法，用于将单词/字符空间映射到标记空间。

#### `char_to_token`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L555)

```py
( batch_or_char_index: int char_index: Optional = None sequence_index: int = 0 ) → export const metadata = 'undefined';int
```

参数

+   `batch_or_char_index` (`int`) — 批次中序列的索引。如果批次仅包含一个序列，则这可以是序列中单词的索引

+   `char_index` (`int`, *可选*) — 如果在 *batch_or_token_index* 中提供了批次索引，则这可以是序列中单词的索引。

+   `sequence_index` (`int`, *可选*, 默认为 0) — 如果批次中编码了一对序列，则可以用于指定提供的字符索引属于一对序列中的哪个序列（0 或 1）。

返回

`int`

标记的索引。

获取编码输出中包含原始字符串中字符的序列的标记索引。

可以调用为：

+   `self.char_to_token(char_index)` 如果批次大小为 1

+   `self.char_to_token(batch_index, char_index)` 如果批次大小大于或等于 1

当输入序列以预分词序列（即用户定义的单词）提供时，此方法特别适用。在这种情况下，它允许轻松将编码的标记与提供的分词单词关联起来。

#### `char_to_word`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L641)

```py
( batch_or_char_index: int char_index: Optional = None sequence_index: int = 0 ) → export const metadata = 'undefined';int or List[int]
```

参数

+   `batch_or_char_index` (`int`) — 批次中序列的索引。如果批次仅包含一个序列，则这可以是原始字符串中字符的索引。

+   `char_index`（`int`，*可选*）— 如果在*batch_or_token_index*中提供了批次索引，则可以是原始字符串中字符的索引。

+   `sequence_index`（`int`，*可选*，默认为 0）— 如果批次中编码了一对序列，则可以用来指定提供的字符索引属于该对序列中的哪个序列（0 或 1）。

返回

`int`或`List[int]`

关联编码标记的索引或索引。

获取批次中序列的原始字符串中与标记的字符对应的单词。

可以调用为：

+   如果批次大小为 1，则为`self.char_to_word(char_index)`

+   如果批次大小大于 1，则为`self.char_to_word(batch_index, char_index)`

当输入序列以预分词序列（即用户定义的单词）提供时，此方法特别适用。在这种情况下，它允许轻松将编码的标记与提供的分词单词关联起来。

#### `convert_to_tensors`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L680)

```py
( tensor_type: Union = None prepend_batch_axis: bool = False )
```

参数

+   `tensor_type`（`str`或 TensorType，*可选*）— 要使用的张量类型。如果是`str`，应该是枚举 TensorType 值之一。如果为`None`，则不进行修改。

+   `prepend_batch_axis`（`int`，*可选*，默认为`False`）— 在转换过程中是否添加批次维度。

将内部内容转换为张量。

#### `sequence_ids`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L319)

```py
( batch_index: int = 0 ) → export const metadata = 'undefined';List[Optional[int]]
```

参数

+   `batch_index`（`int`，*可选*，默认为 0）— 要访问的批次中的索引。

返回

`List[Optional[int]]`

一个指示每个标记对应的序列 id 的列表。由分词器添加的特殊标记映射到`None`，其他标记映射到其对应序列的索引。

返回将标记映射到其原始句子的 id 的列表：

+   对于添加在序列周围或之间的特殊标记，为`None`，

+   `0`表示对应于第一个序列中的单词的标记，

+   当一对序列被联合编码时，对于第二个序列中的单词对应的标记，为`1`。

#### `to`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L773)

```py
( device: Union ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `device`（`str`或`torch.device`）— 要放置张量的设备。

返回

BatchEncoding

修改后的相同实例。

通过调用`v.to(device)`将所有值发送到设备（仅适用于 PyTorch）。

#### `token_to_chars`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L516)

```py
( batch_or_token_index: int token_index: Optional = None ) → export const metadata = 'undefined';CharSpan
```

参数

+   `batch_or_token_index`（`int`）— 批次中序列的索引。如果批次只包含一个序列，则可以是序列中标记的索引。

+   `token_index`（`int`，*可选*）— 如果在*batch_or_token_index*中提供了批次索引，则可以是序列中标记或标记的索引。

返回

CharSpan

原始字符串中字符的范围，如果标记（例如~~,~~）不对应于原始字符串中的任何字符，则为 None。

获取批次中序列中编码标记对应的字符跨度。

字符跨度以 CharSpan 形式返回，具有：

+   `start`— 与标记关联的原始字符串中第一个字符的索引。

+   `end`— 跟随与标记关联的原始字符串中最后一个字符的索引。

可以调用为：

+   如果批次大小为 1，则为`self.token_to_chars(token_index)`

+   如果批次大小大于或等于 1，则为`self.token_to_chars(batch_index, token_index)`

#### `token_to_sequence`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L386)

```py
( batch_or_token_index: int token_index: Optional = None ) → export const metadata = 'undefined';int
```

参数

+   `batch_or_token_index`（`int`）—批次中序列的索引。如果批次只包含一个序列，这可以是序列中标记的索引。

+   `token_index`（`int`，*可选*）—如果在*batch_or_token_index*中提供了批次索引，则这可以是序列中标记的索引。

返回

`int`

输入序列中的单词索引。

获取给定标记表示的序列的索引。在一般用例中，此方法对于单个序列或一对序列的第一个序列返回`0`，对于一对序列的第二个序列返回`1`

可以调用为：

+   如果批次大小为 1，则为`self.token_to_sequence(token_index)`

+   如果批次大小大于 1，则为`self.token_to_sequence(batch_index, token_index)`

当输入序列以预标记序列（即，单词由用户定义）提供时，此方法特别适用。在这种情况下，它允许轻松将编码标记与提供的标记化单词关联起来。

#### `token_to_word`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L425)

```py
( batch_or_token_index: int token_index: Optional = None ) → export const metadata = 'undefined';int
```

参数

+   `batch_or_token_index`（`int`）—批次中序列的索引。如果批次只包含一个序列，这可以是序列中标记的索引。

+   `token_index`（`int`，*可选*）—如果在*batch_or_token_index*中提供了批次索引，则这可以是序列中标记的索引。

返回

`int`

输入序列中的单词索引。

获取与批次序列中编码标记对应的单词的索引。

可以调用为：

+   如果批次大小为 1，则为`self.token_to_word(token_index)`

+   如果批次大小大于 1，则为`self.token_to_word(batch_index, token_index)`

当输入序列以预标记序列（即，单词由用户定义）提供时，此方法特别适用。在这种情况下，它允许轻松将编码标记与提供的标记化单词关联起来。

#### `tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L301)

```py
( batch_index: int = 0 ) → export const metadata = 'undefined';List[str]
```

参数

+   `batch_index`（`int`，*可选*，默认为 0）—要访问的批次索引。

返回

`List[str]`

该索引处的标记列表。

返回给定批次索引处的标记列表（在单词/子词拆分后和转换为整数索引之前的输入字符串的子部分）（仅适用于快速标记器的输出）。

#### `word_ids`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L367)

```py
( batch_index: int = 0 ) → export const metadata = 'undefined';List[Optional[int]]
```

参数

+   `batch_index`（`int`，*可选*，默认为 0）—要访问的批次索引。

返回

`List[Optional[int]]`

一个列表，指示每个标记对应的单词。标记器添加的特殊标记映射到`None`，其他标记映射到其对应单词的索引（如果它们是该单词的一部分，则几个标记将映射到相同的单词索引）。

返回一个将标记映射到初始句子中实际单词的列表，用于快速标记器。

#### `word_to_chars`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L596)

```py
( batch_or_word_index: int word_index: Optional = None sequence_index: int = 0 ) → export const metadata = 'undefined';CharSpan or List[CharSpan]
```

参数

+   `batch_or_word_index`（`int`）—批次中序列的索引。如果批次只包含一个序列，这可以是序列中单词的索引

+   `word_index`（`int`，*可选*）—如果在*batch_or_token_index*中提供了批次索引，则这可以是序列中单词的索引。

+   `sequence_index`（`int`，*可选*，默认为 0）—如果批次中编码了一对序列，则可以用于指定提供的单词索引属于该对中的哪个序列（0 或 1）。

返回

`CharSpan`或`List[CharSpan]`

与字符串中相关字符或字符的范围。CharSpan 是 NamedTuple，具有：

+   start: 原始字符串中与标记关联的第一个字符的索引

+   end: 原始字符串中与标记关联的最后一个字符后面的字符的索引

获取批处理序列中给定单词对应的原始字符串中的字符范围。

字符范围以 CharSpan NamedTuple 形式返回：

+   start: 原始字符串中的第一个字符的索引

+   end: 原始字符串中最后一个字符后面的字符的索引

可以调用为：

+   如果批处理大小为 1，则为`self.word_to_chars(word_index)`

+   如果批处理大小大于或等于 1，则为`self.word_to_chars(batch_index, word_index)`

#### `word_to_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L463)

```py
( batch_or_word_index: int word_index: Optional = None sequence_index: int = 0 ) → export const metadata = 'undefined';(TokenSpan, optional)
```

参数

+   `batch_or_word_index`（`int`）— 批处理中序列的索引。如果批处理仅包括一个序列，则可以是序列中单词的索引。

+   `word_index`（`int`，*可选*）— 如果在*batch_or_token_index*中提供了批处理索引，则可以是序列中单词的索引。

+   `sequence_index`（`int`，*可选*，默认为 0）— 如果批处理中编码了一对序列，则可以用于指定提供的单词索引属于一对序列中的哪个序列（0 或 1）。

返回

(TokenSpan，*可选*)

编码序列中的标记范围。如果没有标记与该单词对应，则返回`None`。这可能会发生，特别是当标记是用于格式化标记化的特殊标记时。例如，当我们在标记化的开头添加一个类标记时。

获取与批处理序列中的单词对应的编码标记范围。

标记范围以 TokenSpan 形式返回：

+   `start` — 第一个标记的索引。

+   `end` — 最后一个标记后面的标记的索引。

可以调用为：

+   如果批处理大小为 1，则为`self.word_to_tokens(word_index, sequence_index: int = 0)`

+   如果批处理大小大于或等于 1，则为`self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)`

当输入序列以预分词序列（即用户定义的单词）提供时，此方法特别适用。在这种情况下，它允许轻松将编码标记与提供的分词单词关联起来。

#### `words`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L343)

```py
( batch_index: int = 0 ) → export const metadata = 'undefined';List[Optional[int]]
```

参数

+   `batch_index`（`int`，*可选*，默认为 0）— 要访问的批处理中的索引。

返回

`List[Optional[int]]`

指示每个标记对应的单词的列表。标记器添加的特殊标记映射到`None`，其他标记映射到其对应单词的索引（如果它们是该单词的一部分，则几个标记将映射到相同的单词索引）。

返回一个列表，将标记映射到初始句子中的实际单词，以便快速标记化器使用。
