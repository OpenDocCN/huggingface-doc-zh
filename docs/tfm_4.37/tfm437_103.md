# 为 Web 服务器使用管道

> 原文：[`huggingface.co/docs/transformers/v4.37.2/en/pipeline_webserver`](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_webserver)

创建推断引擎是一个复杂的主题，“最佳”解决方案很可能取决于您的问题空间。您是在 CPU 还是 GPU 上？您想要最低的延迟、最高的吞吐量、对许多模型的支持，还是只是高度优化一个特定模型？解决这个问题的方法有很多，因此我们将提供一个很好的默认值来开始，这可能不一定是最优的解决方案。

关键的理解是，我们可以使用一个迭代器，就像你在数据集上使用的一样，因为 Web 服务器基本上是一个等待请求并按顺序处理它们的系统。

通常，Web 服务器是多路复用的（多线程、异步等），以处理各种请求。另一方面，管道（以及大多数底层模型）并不真正适合并行处理；它们占用大量 RAM，因此最好在运行时为它们提供所有可用的资源，或者这是一个计算密集型的工作。

我们将通过让 Web 服务器处理接收和发送请求的轻负载，并让单个线程处理实际工作来解决这个问题。这个示例将使用`starlette`。实际的框架并不是很重要，但如果您使用另一个框架来实现相同的效果，可能需要调整或更改代码。

创建`server.py`：

```py
from starlette.applications import Starlette
from starlette.responses import JSONResponse
from starlette.routing import Route
from transformers import pipeline
import asyncio

async def homepage(request):
    payload = await request.body()
    string = payload.decode("utf-8")
    response_q = asyncio.Queue()
    await request.app.model_queue.put((string, response_q))
    output = await response_q.get()
    return JSONResponse(output)

async def server_loop(q):
    pipe = pipeline(model="bert-base-uncased")
    while True:
        (string, response_q) = await q.get()
        out = pipe(string)
        await response_q.put(out)

app = Starlette(
    routes=[
        Route("/", homepage, methods=["POST"]),
    ],
)

@app.on_event("startup")
async def startup_event():
    q = asyncio.Queue()
    app.model_queue = q
    asyncio.create_task(server_loop(q))
```

现在您可以启动它：

```py
uvicorn server:app
```

你可以查询它：

```py
curl -X POST -d "test [MASK]" http://localhost:8000/
#[{"score":0.7742936015129089,"token":1012,"token_str":".","sequence":"test."},...]
```

现在，您已经了解如何创建 Web 服务器了！

真正重要的是，我们只**一次**加载模型，因此在 Web 服务器上没有模型的副本。这样，就不会使用不必要的 RAM。然后，排队机制允许您执行一些花哨的操作，比如可能在推断之前累积一些项目以使用动态批处理：

下面的代码示例故意以伪代码形式编写，以提高可读性。在未检查是否对您的系统资源有意义的情况下，请勿运行此代码！

```py
(string, rq) = await q.get()
strings = []
queues = []
while True:
    try:
        (string, rq) = await asyncio.wait_for(q.get(), timeout=0.001)  # 1ms
    except asyncio.exceptions.TimeoutError:
        break
    strings.append(string)
    queues.append(rq)
strings
outs = pipe(strings, batch_size=len(strings))
for rq, out in zip(queues, outs):
    await rq.put(out)
```

再次强调，建议的代码是为了可读性而优化的，而不是为了成为最佳代码。首先，没有批处理大小限制，这通常不是一个好主意。其次，超时在每次队列获取时重置，这意味着您可能需要等待比 1 毫秒更长的时间才能运行推断（延迟第一个请求）。

最好设置一个单独的 1 毫秒截止时间。

即使队列为空，这将始终等待 1 毫秒，这可能不是最好的，因为如果队列中没有东西，您可能希望开始进行推断。但如果批处理对您的用例非常关键，那么这可能是有意义的。再次强调，没有一个最佳解决方案。

## 你可能想考虑的几件事

### 错误检查

在生产中可能会出现很多问题：内存不足、空间不足、加载模型可能失败、查询可能错误、查询可能正确但由于模型配置错误而无法运行，等等。

通常，如果服务器将错误输出给用户，那么添加许多`try..except`语句来显示这些错误是一个好主意。但请记住，根据您的安全上下文，公开所有这些错误也可能是一个安全风险。

### 断路处理

当 Web 服务器过载时，通常最好进行断路处理。这意味着它们在过载时返回适当的错误，而不是无限期地等待查询。在等待超长时间后返回 503 错误，或者在很长时间后返回 504 错误。

在建议的代码中实现这个相对容易，因为有一个单一的队列。查看队列大小是在 Web 服务器在负载下失败之前开始返回错误的基本方法。

### 阻塞主线程

目前 PyTorch 不支持异步操作，计算时会阻塞主线程。这意味着最好让 PyTorch 在自己的线程/进程上运行。这里没有这样做是因为代码更加复杂（主要是因为线程、异步和队列不太兼容）。但最终它做的事情是一样的。

如果单个项目的推断时间很长（> 1 秒），这将是很重要的，因为在这种情况下，在推断期间每个查询都必须等待 1 秒才能收到错误。

### 动态批处理

一般来说，批处理不一定比逐个传递项目更好（有关更多信息，请参阅批处理详细信息）。但在正确的环境中使用时，它可能非常有效。在 API 中，默认情况下没有动态批处理（太容易导致减速）。但对于 BLOOM 推断 - 这是一个非常庞大的模型 - 动态批处理是**必不可少**的，以为每个人提供良好的体验。
