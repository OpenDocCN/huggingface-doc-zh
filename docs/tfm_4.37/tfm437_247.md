# XLM-RoBERTa

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-roberta`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-roberta)

![æ¨¡å‹](https://huggingface.co/models?filter=xlm-roberta) ![ç©ºé—´](https://huggingface.co/spaces/docs-demos/xlm-roberta-base)

## æ¦‚è¿°

XLM-RoBERTa æ¨¡å‹æ˜¯ç”± Alexis Conneauã€Kartikay Khandelwalã€Naman Goyalã€Vishrav Chaudharyã€Guillaume Wenzekã€Francisco GuzmÃ¡nã€Edouard Graveã€Myle Ottã€Luke Zettlemoyer å’Œ Veselin Stoyanov åœ¨ã€Šè§„æ¨¡åŒ–æ— ç›‘ç£è·¨è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‹ä¸€æ–‡ä¸­æå‡ºçš„ã€‚å®ƒåŸºäº Facebook äº 2019 å¹´å‘å¸ƒçš„ RoBERTa æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªå¤§å‹çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒäº† 2.5TB çš„ç»è¿‡æ»¤çš„ CommonCrawl æ•°æ®ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*è¿™ç¯‡è®ºæ–‡è¡¨æ˜ï¼Œè§„æ¨¡åŒ–é¢„è®­ç»ƒå¤šè¯­è¨€è¯­è¨€æ¨¡å‹å¯ä»¥æ˜¾è‘—æé«˜å„ç§è·¨è¯­è¨€è½¬ç§»ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸€ç™¾ç§è¯­è¨€ä¸Šè®­ç»ƒäº†åŸºäº Transformer çš„æ©ç è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨äº†è¶…è¿‡ä¸¤å¤ªå­—èŠ‚çš„ç»è¿‡æ»¤æ³¢çš„ CommonCrawl æ•°æ®ã€‚æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¢«ç§°ä¸º XLM-Rï¼Œåœ¨å„ç§è·¨è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­æ˜æ˜¾ä¼˜äºå¤šè¯­è¨€ BERTï¼ˆmBERTï¼‰ï¼ŒåŒ…æ‹¬ XNLI ä¸Šå¹³å‡å‡†ç¡®ç‡æé«˜äº†+13.8ï¼…ï¼ŒMLQA ä¸Šå¹³å‡ F1 åˆ†æ•°æé«˜äº†+12.3ï¼…ï¼ŒNER ä¸Šå¹³å‡ F1 åˆ†æ•°æé«˜äº†+2.1ï¼…ã€‚XLM-R åœ¨ä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ç‰¹åˆ«å‡ºè‰²ï¼Œåœ¨ XNLI å‡†ç¡®ç‡ä¸Šä¸ºæ–¯ç“¦å¸Œé‡Œè¯­æé«˜äº† 11.8ï¼…ï¼Œä¹Œå°”éƒ½è¯­æé«˜äº† 9.2ï¼…ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„ XLM æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å¯¹å®ç°è¿™äº›æ”¶ç›Šæ‰€éœ€çš„å…³é”®å› ç´ è¿›è¡Œäº†è¯¦ç»†çš„å®è¯è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ­£å‘è½¬ç§»å’Œå®¹é‡ç¨€é‡Šä¹‹é—´çš„æƒè¡¡ï¼Œä»¥åŠï¼ˆ2ï¼‰è§„æ¨¡ä¸Šé«˜ä½èµ„æºè¯­è¨€çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬é¦–æ¬¡å±•ç¤ºäº†å¤šè¯­è¨€å»ºæ¨¡çš„å¯èƒ½æ€§ï¼Œè€Œä¸ä¼šç‰ºç‰²æ¯ç§è¯­è¨€çš„æ€§èƒ½ï¼›XLM-R åœ¨ GLUE å’Œ XNLI åŸºå‡†æµ‹è¯•ä¸Šä¸å¼ºå¤§çš„å•è¯­æ¨¡å‹ç«äº‰åŠ›å¼ºã€‚æˆ‘ä»¬å°†å…¬å¼€æä¾› XLM-R çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚*

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[stefan-it](https://huggingface.co/stefan-it)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   XLM-RoBERTa æ˜¯åœ¨ 100 ç§ä¸åŒè¯­è¨€ä¸Šè®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹ã€‚ä¸ä¸€äº› XLM å¤šè¯­è¨€æ¨¡å‹ä¸åŒï¼Œå®ƒä¸éœ€è¦`lang`å¼ é‡æ¥ç†è§£ä½¿ç”¨çš„è¯­è¨€ï¼Œå¹¶ä¸”åº”è¯¥èƒ½å¤Ÿä»è¾“å…¥ id ä¸­ç¡®å®šæ­£ç¡®çš„è¯­è¨€ã€‚

+   ä½¿ç”¨ RoBERTa æŠ€å·§åœ¨ XLM æ–¹æ³•ä¸Šï¼Œä½†ä¸ä½¿ç”¨ç¿»è¯‘è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚å®ƒåªåœ¨æ¥è‡ªä¸€ç§è¯­è¨€çš„å¥å­ä¸Šä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡ã€‚

## èµ„æº

ä¸€ä¸ªå®˜æ–¹çš„ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯ä»¥å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ XLM-RoBERTaã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

æ–‡æœ¬åˆ†ç±»

+   ä¸€ç¯‡å…³äºå¦‚ä½•åœ¨ AWS ä¸Šä½¿ç”¨ Habana Gaudi å¯¹ XLM RoBERTa è¿›è¡Œå¤šç±»åˆ«åˆ†ç±»å¾®è°ƒçš„åšå®¢æ–‡ç« 

+   XLMRobertaForSequenceClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)æ”¯æŒã€‚

+   TFXLMRobertaForSequenceClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)æ”¯æŒã€‚

+   FlaxXLMRobertaForSequenceClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)æ”¯æŒã€‚

+   ğŸ¤— Hugging Face ä»»åŠ¡æŒ‡å—çš„[æ–‡æœ¬åˆ†ç±»](https://huggingface.co/docs/transformers/tasks/sequence_classification)ç« èŠ‚ã€‚

+   æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—

Token åˆ†ç±»

+   XLMRobertaForTokenClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)æ”¯æŒã€‚

+   TFXLMRobertaForTokenClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)æ”¯æŒã€‚

+   FlaxXLMRobertaForTokenClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)æ”¯æŒã€‚

+   ğŸ¤— Hugging Face è¯¾ç¨‹çš„[Token åˆ†ç±»](https://huggingface.co/course/chapter7/2?fw=pt)ç« èŠ‚ã€‚

+   Token åˆ†ç±»ä»»åŠ¡æŒ‡å—

æ–‡æœ¬ç”Ÿæˆ

+   XLMRobertaForCausalLM ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)æ”¯æŒã€‚

+   [å› æœè¯­è¨€å»ºæ¨¡](https://huggingface.co/docs/transformers/tasks/language_modeling)ç« èŠ‚çš„ğŸ¤— Hugging Face ä»»åŠ¡æŒ‡å—ã€‚

+   å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—

å¡«å……-æ©ç 

+   XLMRobertaForMaskedLM ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)æ”¯æŒã€‚

+   TFXLMRobertaForMaskedLM ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)æ”¯æŒã€‚

+   FlaxXLMRobertaForMaskedLM ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)æ”¯æŒã€‚

+   [é®è”½è¯­è¨€å»ºæ¨¡](https://huggingface.co/course/chapter7/3?fw=pt)ç« èŠ‚æ¥è‡ªğŸ¤— Hugging Face è¯¾ç¨‹ã€‚

+   é®è”½è¯­è¨€å»ºæ¨¡

é—®ç­”

+   XLMRobertaForQuestionAnswering ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)æ”¯æŒã€‚

+   TFXLMRobertaForQuestionAnswering ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)æ”¯æŒã€‚

+   FlaxXLMRobertaForQuestionAnswering ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)æ”¯æŒã€‚

+   [é—®ç­”](https://huggingface.co/course/chapter7/7?fw=pt)ç« èŠ‚æ¥è‡ªğŸ¤— Hugging Face è¯¾ç¨‹ã€‚

+   é—®ç­”ä»»åŠ¡æŒ‡å—

**å¤šé¡¹é€‰æ‹©**

+   XLMRobertaForMultipleChoice ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)æ”¯æŒã€‚

+   TFXLMRobertaForMultipleChoice ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)æ”¯æŒã€‚

+   å¤šé¡¹é€‰æ‹©ä»»åŠ¡æŒ‡å—

ğŸš€ éƒ¨ç½²

+   å…³äºå¦‚ä½•åœ¨ AWS Lambda ä¸Šéƒ¨ç½²æ— æœåŠ¡å™¨ XLM RoBERTa çš„åšå®¢æ–‡ç« [Deploy Serverless XLM RoBERTa on AWS Lambda](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface)ã€‚

è¿™ä¸ªå®ç°ä¸ RoBERTa ç›¸åŒã€‚è¯·å‚è€ƒ RoBERTa çš„æ–‡æ¡£ä»¥è·å–ç”¨æ³•ç¤ºä¾‹ä»¥åŠä¸è¾“å…¥å’Œè¾“å‡ºç›¸å…³çš„ä¿¡æ¯ã€‚

## XLMRobertaConfig

### `class transformers.XLMRobertaConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/configuration_xlm_roberta.py#L45)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True classifier_dropout = None **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, é»˜è®¤ä¸º 30522) â€” XLM-RoBERTa æ¨¡å‹çš„è¯æ±‡å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ XLMRobertaModel æˆ– TFXLMRobertaModel æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, é»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `intermediate_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` æˆ– `Callable`, *å¯é€‰*, é»˜è®¤ä¸º `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`ã€`"relu"`ã€`"silu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `max_position_embeddings` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512 æˆ– 1024 æˆ– 2048ï¼‰ã€‚

+   `type_vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2) â€” åœ¨è°ƒç”¨ XLMRobertaModel æˆ– TFXLMRobertaModel æ—¶ä¼ é€’çš„ `token_type_ids` çš„è¯æ±‡é‡ã€‚

+   `initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `position_embedding_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"absolute"`) â€” ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚é€‰æ‹© `"absolute"`ã€`"relative_key"`ã€`"relative_key_query"` ä¸­çš„ä¸€ä¸ªã€‚å¯¹äºä½ç½®åµŒå…¥ï¼Œè¯·ä½¿ç”¨ `"absolute"`ã€‚æœ‰å…³ `"relative_key"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[ä½¿ç”¨ç›¸å¯¹ä½ç½®è¡¨ç¤ºçš„è‡ªæ³¨æ„åŠ›ï¼ˆShaw ç­‰äººï¼‰](https://arxiv.org/abs/1803.02155)ã€‚æœ‰å…³ `"relative_key_query"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[ä½¿ç”¨æ›´å¥½çš„ç›¸å¯¹ä½ç½®åµŒå…¥æ”¹è¿› Transformer æ¨¡å‹ï¼ˆHuang ç­‰äººï¼‰](https://arxiv.org/abs/2009.13658) ä¸­çš„ *æ–¹æ³• 4*ã€‚

+   `is_decoder` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ¨¡å‹æ˜¯å¦ç”¨ä½œè§£ç å™¨ã€‚å¦‚æœä¸º `False`ï¼Œåˆ™æ¨¡å‹ç”¨ä½œç¼–ç å™¨ã€‚

+   `use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨ `config.is_decoder=True` æ—¶ç›¸å…³ã€‚

+   `classifier_dropout` (`float`, *å¯é€‰*) â€” åˆ†ç±»å¤´çš„ä¸¢å¤±æ¯”ç‡ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ XLMRobertaModel æˆ– TFXLMRobertaModel é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª XLM-RoBERTa æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿä¸ XLMRoBERTa [xlm-roberta-base](https://huggingface.co/xlm-roberta-base)æ¶æ„ç±»ä¼¼çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import XLMRobertaConfig, XLMRobertaModel

>>> # Initializing a XLM-RoBERTa xlm-roberta-base style configuration
>>> configuration = XLMRobertaConfig()

>>> # Initializing a model (with random weights) from the xlm-roberta-base style configuration
>>> model = XLMRobertaModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## XLMRobertaTokenizer

### `class transformers.XLMRobertaTokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py#L63)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' sp_model_kwargs: Optional = None **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `bos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚

    åœ¨ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—å¼€å§‹çš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯ `cls_token`ã€‚

+   `eos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

    åœ¨ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—ç»“æŸçš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯`sep_token`ã€‚

+   `sep_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” åˆ†éš”ç¬¦æ ‡è®°ï¼Œåœ¨ä»å¤šä¸ªåºåˆ—æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºæ–‡æœ¬å’Œé—®é¢˜çš„é—®é¢˜å›ç­”ã€‚ä¹Ÿç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚

+   `cls_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” åœ¨è¿›è¡Œåºåˆ—åˆ†ç±»ï¼ˆå¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œåˆ†ç±»è€Œä¸æ˜¯æ¯ä¸ªæ ‡è®°çš„åˆ†ç±»ï¼‰æ—¶ä½¿ç”¨çš„åˆ†ç±»å™¨æ ‡è®°ã€‚åœ¨æ„å»ºå¸¦æœ‰ç‰¹æ®Šæ ‡è®°çš„åºåˆ—æ—¶ï¼Œå®ƒæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚

+   `unk_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `pad_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<pad>"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `mask_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<mask>"`) â€” ç”¨äºæ©ç›–å€¼çš„æ ‡è®°ã€‚åœ¨ä½¿ç”¨æ©ç›–è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚

+   `sp_model_kwargs` (`dict`, *å¯é€‰*) â€” å°†ä¼ é€’ç»™ `SentencePieceProcessor.__init__()` æ–¹æ³•ã€‚[SentencePiece çš„ Python åŒ…è£…å™¨](https://github.com/google/sentencepiece/tree/master/python)å¯ç”¨äºè®¾ç½®ï¼š

    +   `enable_sampling`: å¯ç”¨å­è¯æ­£åˆ™åŒ–ã€‚

    +   `nbest_size`: unigram çš„é‡‡æ ·å‚æ•°ã€‚å¯¹äº BPE-Dropout æ— æ•ˆã€‚

        +   `nbest_size = {0,1}`: ä¸æ‰§è¡Œé‡‡æ ·ã€‚

        +   `nbest_size > 1`: ä» nbest_size ç»“æœä¸­è¿›è¡Œé‡‡æ ·ã€‚

        +   `nbest_size < 0`: å‡è®¾ nbest_size ä¸ºæ— é™ï¼Œå¹¶ä½¿ç”¨å‰å‘è¿‡æ»¤å’Œåå‘é‡‡æ ·ç®—æ³•ä»æ‰€æœ‰å‡è®¾ï¼ˆæ ¼ï¼‰ä¸­è¿›è¡Œé‡‡æ ·ã€‚

    +   `alpha`: ç”¨äº unigram é‡‡æ ·çš„å¹³æ»‘å‚æ•°ï¼Œä»¥åŠç”¨äº BPE-dropout çš„åˆå¹¶æ“ä½œçš„ dropout æ¦‚ç‡ã€‚

+   `sp_model` (`SentencePieceProcessor`) â€” ç”¨äºæ¯æ¬¡è½¬æ¢ï¼ˆå­—ç¬¦ä¸²ã€æ ‡è®°å’Œ IDï¼‰çš„*SentencePiece*å¤„ç†å™¨ã€‚

æ”¹ç¼–è‡ª RobertaTokenizer å’Œ XLNetTokenizerã€‚åŸºäº[SentencePiece](https://github.com/google/sentencepiece)ã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `build_inputs_with_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py#L200)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” å°†æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„ ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *å¯é€‰*) â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

å¸¦æœ‰é€‚å½“ç‰¹æ®Šæ ‡è®°çš„ input IDs åˆ—è¡¨ã€‚

é€šè¿‡è¿æ¥å’Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ä»åºåˆ—æˆ–åºåˆ—å¯¹æ„å»ºç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å…¥ã€‚XLM-RoBERTa åºåˆ—çš„æ ¼å¼å¦‚ä¸‹ï¼š

+   å•ä¸ªåºåˆ—: `<s> X </s>`

+   åºåˆ—å¯¹: `<s> A </s></s> B </s>`

#### `get_special_tokens_mask`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py#L226)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *å¯é€‰*) â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

+   `already_has_special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ ¼å¼åŒ–ä¸ºæ¨¡å‹ã€‚

è¿”å›

`List[int]`

ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º[0, 1]ï¼š1 è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0 è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ— idã€‚å½“ä½¿ç”¨ tokenizer çš„`prepare_for_model`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œå°†è°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py#L254)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *optional*) â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

é›¶åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚ XLM-RoBERTa ä¸ä½¿ç”¨æ ‡è®°ç±»å‹ idï¼Œå› æ­¤è¿”å›ä¸€ä¸ªé›¶åˆ—è¡¨ã€‚

#### `save_vocabulary`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py#L312)

```py
( save_directory: str filename_prefix: Optional = None )
```

## XLMRobertaTokenizerFast

### `class transformers.XLMRobertaTokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py#L82)

```py
( vocab_file = None tokenizer_file = None bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `bos_token` (`str`, *optional*, defaults to `"<s>"`) â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚

    åœ¨ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—å¼€å¤´çš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯`cls_token`ã€‚

+   `eos_token` (`str`, *optional*, defaults to `"</s>"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

    åœ¨ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—ç»“å°¾çš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯`sep_token`ã€‚

+   `sep_token` (`str`, *optional*, defaults to `"</s>"`) â€” åˆ†éš”ç¬¦æ ‡è®°ï¼Œåœ¨ä»å¤šä¸ªåºåˆ—æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºæ–‡æœ¬å’Œé—®é¢˜çš„é—®é¢˜å›ç­”ã€‚å®ƒè¿˜ç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚

+   `cls_token` (`str`, *optional*, defaults to `"<s>"`) â€” åœ¨è¿›è¡Œåºåˆ—åˆ†ç±»ï¼ˆæ•´ä¸ªåºåˆ—è€Œä¸æ˜¯æ¯ä¸ªæ ‡è®°çš„åˆ†ç±»ï¼‰æ—¶ä½¿ç”¨çš„åˆ†ç±»å™¨æ ‡è®°ã€‚å½“ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œå®ƒæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚

+   `unk_token` (`str`, *optional*, defaults to `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `pad_token` (`str`, *optional*, defaults to `"<pad>"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `mask_token` (`str`, *optional*, defaults to `"<mask>"`) â€” ç”¨äºå±è”½å€¼çš„æ ‡è®°ã€‚è¿™æ˜¯åœ¨ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚

+   `additional_special_tokens` (`List[str]`, *optional*, defaults to `["<s>NOTUSED", "</s>NOTUSED"]`) â€” åˆ†è¯å™¨ä½¿ç”¨çš„é¢å¤–ç‰¹æ®Šæ ‡è®°ã€‚

æ„å»ºä¸€ä¸ªâ€œå¿«é€Ÿâ€XLM-RoBERTa åˆ†è¯å™¨ï¼ˆç”± HuggingFace çš„*tokenizers*åº“æ”¯æŒï¼‰ã€‚æ”¹ç¼–è‡ª RobertaTokenizer å’Œ XLNetTokenizerã€‚åŸºäº[BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models)ã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerFastï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py#L174)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” å°†æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„ ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

å…·æœ‰é€‚å½“ç‰¹æ®Šæ ‡è®°çš„è¾“å…¥ ID åˆ—è¡¨ã€‚

é€šè¿‡è¿æ¥å’Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä»åºåˆ—æˆ–åºåˆ—å¯¹æ„å»ºç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å…¥ã€‚XLM-RoBERTa åºåˆ—çš„æ ¼å¼å¦‚ä¸‹ï¼š

+   å•ä¸ªåºåˆ—ï¼š`<s> X </s>`

+   åºåˆ—å¯¹ï¼š`<s> A </s></s> B </s>`

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py#L200)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

é›¶åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚XLM-RoBERTa ä¸ä½¿ç”¨æ ‡è®°ç±»å‹ IDï¼Œå› æ­¤è¿”å›ä¸€ä¸ªé›¶åˆ—è¡¨ã€‚

Pytorch éšè— Pytorch å†…å®¹

## XLMRobertaModel

### `class transformers.XLMRobertaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L679)

```py
( config add_pooling_layer = True )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ XLM-RoBERTa æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ä½¿ç”¨å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

è¯¥æ¨¡å‹å¯ä»¥ä½œä¸ºç¼–ç å™¨ï¼ˆä»…å…·æœ‰è‡ªæ³¨æ„åŠ›ï¼‰ä»¥åŠè§£ç å™¨ï¼Œæ­¤æ—¶åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¹‹é—´æ·»åŠ äº†ä¸€å±‚äº¤å‰æ³¨æ„åŠ›ï¼Œéµå¾ª Ashish Vaswaniã€Noam Shazeerã€Niki Parmarã€Jakob Uszkoreitã€Llion Jonesã€Aidan N. Gomezã€Lukasz Kaiser å’Œ Illia Polosukhin åœ¨*æ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„*ä¸­æè¿°çš„æ¶æ„ã€‚

ä¸ºäº†ä½œä¸ºè§£ç å™¨è¡Œä¸ºï¼Œæ¨¡å‹éœ€è¦ä½¿ç”¨é…ç½®ä¸­çš„`is_decoder`å‚æ•°åˆå§‹åŒ–ä¸º`True`ã€‚è¦åœ¨ Seq2Seq æ¨¡å‹ä¸­ä½¿ç”¨ï¼Œæ¨¡å‹éœ€è¦ä½¿ç”¨`is_decoder`å‚æ•°å’Œ`add_cross_attention`å‚æ•°éƒ½åˆå§‹åŒ–ä¸º`True`ï¼›ç„¶åæœŸæœ›ä¸€ä¸ª`encoder_hidden_states`ä½œä¸ºå‰å‘ä¼ é€’çš„è¾“å…¥ã€‚

.. _*æ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„*ï¼š[`arxiv.org/abs/1706.03762`](https://arxiv.org/abs/1706.03762)

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L727)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚ é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š

    +   1 ç”¨äºâ€œæœªå±è”½â€çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºâ€œå±è”½â€çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°çš„ä½ç½®çš„ç´¢å¼•åœ¨ä½ç½®åµŒå…¥ä¸­é€‰æ‹©ã€‚ åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚ é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªâ€œå±è”½â€ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«â€œå±è”½â€ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚ å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚ æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚ æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `encoder_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚ å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚

+   `encoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥çš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚ å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨æ­¤æ©ç ã€‚ é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š

    +   1 ç”¨äºâ€œæœªå±è”½â€çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºâ€œå±è”½â€çš„æ ‡è®°ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„ 4 ä¸ªå¼ é‡ï¼‰- åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚ å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„è¾“å…¥ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰ â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰ â€” ç»è¿‡è¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼Œåºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œå¯¹äº BERT ç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œ tanh æ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯å±‚æ¨¡å‹çš„è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ã€‚

+   `cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`å’Œ`config.add_cross_attention=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax åä½¿ç”¨ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œå¦‚æœ`config.is_encoder_decoder=True`è¿˜æœ‰ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„è®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼Œä»¥åŠå¯é€‰çš„`config.is_encoder_decoder=True`æ—¶çš„äº¤å‰æ³¨æ„åŠ›å—ï¼‰å¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

XLMRobertaModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, XLMRobertaModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = XLMRobertaModel.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## XLMRobertaForCausalLM

### `class transformers.XLMRobertaForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L865)

```py
( config )
```

å‚æ•°

+   `config` (XLMRobertaConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM-RoBERTa æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´éƒ¨ï¼Œç”¨äº CLM å¾®è°ƒã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L891)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None past_key_values: Tuple = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” åˆ†æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç `ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `encoder_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚

+   `encoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥çš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨æ­¤æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—ä»å·¦åˆ°å³çš„è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼‰çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size]`å†…ï¼ˆè¯·å‚é˜…`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆè¢«`masked`ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`å†…çš„æ ‡è®°ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„ 4 ä¸ªå¼ é‡ï¼‰â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©æ€§åœ°ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™ä¼šè¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.CausalLMOutputWithCrossAttentions æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.CausalLMOutputWithCrossAttentions æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„è¾“å‡º+æ¯å±‚è¾“å‡ºçš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–å½“`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`torch.FloatTensor`å…ƒç»„çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder = True`æ—¶ç›¸å…³ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰åŠ é€Ÿé¡ºåºè§£ç ã€‚

XLMRobertaForCausalLM çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, XLMRobertaForCausalLM, AutoConfig
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("roberta-base")
>>> config = AutoConfig.from_pretrained("roberta-base")
>>> config.is_decoder = True
>>> model = XLMRobertaForCausalLM.from_pretrained("roberta-base", config=config)

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> prediction_logits = outputs.logits
```

## XLMRobertaForMaskedLM

### `class transformers.XLMRobertaForMaskedLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1029)

```py
( config )
```

å‚æ•°

+   `config` (XLMRobertaConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åœ¨é¡¶éƒ¨å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´çš„ XLM-RoBERTa æ¨¡å‹ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1058)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢«`æ©ç `çš„æ ‡è®°ä¸º 1ã€‚

    +   å¯¹äºè¢«`æ©ç `çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ*å¯é€‰*ï¼Œå½“è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡æ—¶è¿”å›ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size]`å†…ï¼ˆè¯·å‚é˜…`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆè¢«é®è”½ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`å†…çš„æ ‡è®°ã€‚

+   `kwargs`ï¼ˆ`Dict[str, any]`ï¼Œå¯é€‰ï¼Œé»˜è®¤ä¸º*{}*ï¼‰â€” ç”¨äºéšè—å·²è¢«å¼ƒç”¨çš„æ—§å‚æ•°ã€‚

è¿”å›

transformers.modeling_outputs.MaskedLMOutput æˆ–`tuple(torch.FloatTensor)`

transformers.modeling_outputs.MaskedLMOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” `torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” `torch.FloatTensor`çš„å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

XLMRobertaForMaskedLM çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, XLMRobertaForMaskedLM
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = XLMRobertaForMaskedLM.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # retrieve index of <mask>
>>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

>>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
>>> tokenizer.decode(predicted_token_id)
' Paris'

>>> labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
>>> # mask labels of non-<mask> tokens
>>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

>>> outputs = model(**inputs, labels=labels)
>>> round(outputs.loss.item(), 2)
0.1
```

## XLMRobertaForSequenceClassification

### `class transformers.XLMRobertaForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1159)

```py
( config )
```

å‚æ•°

+   `config` (XLMRobertaConfig) â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM-RoBERTa æ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰åºåˆ—åˆ†ç±»/å›å½’å¤´ï¼ˆæ±‡æ€»è¾“å‡ºçš„é¡¶éƒ¨çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº GLUE ä»»åŠ¡ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1179)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`() ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºæœªè¢«é®è”½çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«é®è”½çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›é®ç½©ï¼Ÿ

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ®µè½æ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† `input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.SequenceClassifierOutput æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.SequenceClassifierOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡º+æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

XLMRobertaForSequenceClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

å•æ ‡ç­¾åˆ†ç±»çš„ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoTokenizer, XLMRobertaForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
>>> model = XLMRobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_id = logits.argmax().item()
>>> model.config.id2label[predicted_class_id]
'optimism'

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = XLMRobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion", num_labels=num_labels)

>>> labels = torch.tensor([1])
>>> loss = model(**inputs, labels=labels).loss
>>> round(loss.item(), 2)
0.08
```

å¤šæ ‡ç­¾åˆ†ç±»çš„ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoTokenizer, XLMRobertaForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
>>> model = XLMRobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion", problem_type="multi_label_classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = XLMRobertaForSequenceClassification.from_pretrained(
...     "cardiffnlp/twitter-roberta-base-emotion", num_labels=num_labels, problem_type="multi_label_classification"
... )

>>> labels = torch.sum(
...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1
... ).to(torch.float)
>>> loss = model(**inputs, labels=labels).loss
```

## XLMRobertaForMultipleChoice

### `class transformers.XLMRobertaForMultipleChoice`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1259)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰ â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åœ¨é¡¶éƒ¨å¸¦æœ‰å¤šé¡¹é€‰æ‹©åˆ†ç±»å¤´çš„ XLM-RoBERTa æ¨¡å‹ï¼ˆåœ¨æ±‡æ€»è¾“å‡ºçš„é¡¶éƒ¨å’Œ softmax ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº RocStories/SWAG ä»»åŠ¡ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1278)

```py
( input_ids: Optional = None token_type_ids: Optional = None attention_mask: Optional = None labels: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.MultipleChoiceModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°è¿”å› 1ã€‚

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°è¿”å› 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„ç‰¹å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—å¤šé¡¹é€‰æ‹©åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., num_choices-1]`èŒƒå›´å†…ï¼Œå…¶ä¸­`num_choices`æ˜¯è¾“å…¥å¼ é‡ç¬¬äºŒç»´çš„å¤§å°ã€‚ï¼ˆå‚è§ä¸Šé¢çš„`input_ids`ï¼‰

è¿”å›

transformers.modeling_outputs.MultipleChoiceModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.MultipleChoiceModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º*(1,)*çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€”åˆ†ç±»æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_choices)`çš„`torch.FloatTensor`ï¼‰â€”*num_choices*æ˜¯è¾“å…¥å¼ é‡çš„ç¬¬äºŒç»´ã€‚ï¼ˆå‚è§ä¸Šé¢çš„*input_ids*ï¼‰ã€‚

    åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”`torch.FloatTensor`çš„å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

XLMRobertaForMultipleChoice çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, XLMRobertaForMultipleChoice
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = XLMRobertaForMultipleChoice.from_pretrained("xlm-roberta-base")

>>> prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
>>> choice0 = "It is eaten with a fork and a knife."
>>> choice1 = "It is eaten while held in the hand."
>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

>>> # the linear classifier still needs to be trained
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## XLMRobertaForTokenClassification

### `class transformers.XLMRobertaForTokenClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1354)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfig](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM-RoBERTa æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªæ ‡è®°åˆ†ç±»å¤´ï¼ˆéšè—çŠ¶æ€è¾“å‡ºçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1377)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€”è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   å¯¹äº`æœªå±è”½`çš„æ ‡è®°ï¼Œ

    +   å¯¹äº`å±è”½`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ã€‚

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªå±è”½`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨`å±è”½`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨äºè®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`ä¸­ã€‚

è¿”å›

transformers.modeling_outputs.TokenClassifierOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.TokenClassifierOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`çš„`torch.FloatTensor`ï¼‰ â€” åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºä¸€ä¸ª + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

XLMRobertaForTokenClassification å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, XLMRobertaForTokenClassification
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("Jean-Baptiste/roberta-large-ner-english")
>>> model = XLMRobertaForTokenClassification.from_pretrained("Jean-Baptiste/roberta-large-ner-english")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
... )

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_token_class_ids = logits.argmax(-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
>>> predicted_tokens_classes
['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']

>>> labels = predicted_token_class_ids
>>> loss = model(**inputs, labels=labels).loss
>>> round(loss.item(), 2)
0.01
```

## XLMRobertaForQuestionAnswering

### `class transformers.XLMRobertaForQuestionAnswering`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1463)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM-RoBERTa æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰ä¸€ä¸ªè·¨åº¦åˆ†ç±»å¤´ï¼Œç”¨äºæå–å¼é—®ç­”ä»»åŠ¡ï¼Œå¦‚ SQuADï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„çº¿æ€§å±‚ä¸Šè®¡ç®—`è·¨åº¦å¼€å§‹å¯¹æ•°`å’Œ`è·¨åº¦ç»“æŸå¯¹æ•°`ï¼‰ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py#L1482)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 ç”¨äºæœªè¢«â€œæ©ç â€çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºè¢«â€œæ©ç â€çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—æ ‡è®°èŒƒå›´çš„å¼€å§‹ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦(`sequence_length`)ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…è®¡ç®—æŸå¤±ã€‚

+   `end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—æ ‡è®°èŒƒå›´çš„ç»“æŸä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦(`sequence_length`)ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…è®¡ç®—æŸå¤±ã€‚

è¿”å›

transformers.modeling_outputs.QuestionAnsweringModelOutput æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.QuestionAnsweringModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€” æ€»è·¨åº¦æå–æŸå¤±æ˜¯å¼€å§‹å’Œç»“æŸä½ç½®çš„äº¤å‰ç†µä¹‹å’Œã€‚

+   `start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) â€” è·¨åº¦å¼€å§‹å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) â€” è·¨åº¦ç»“æŸå¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

XLMRobertaForQuestionAnswering çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°ä¸­å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, XLMRobertaForQuestionAnswering
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
>>> model = XLMRobertaForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

>>> inputs = tokenizer(question, text, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()

>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)
' puppet'

>>> # target is "nice puppet"
>>> target_start_index = torch.tensor([14])
>>> target_end_index = torch.tensor([15])

>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
>>> loss = outputs.loss
>>> round(loss.item(), 2)
0.86
```

TensorFlow éšè— TensorFlow å†…å®¹

## TFXLMRobertaModel

### `class transformers.TFXLMRobertaModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L983)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config` (XLMRobertaConfig) â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ XLM RoBERTa æ¨¡å‹å˜å‹å™¨ï¼Œè¾“å‡ºåŸå§‹çš„éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸çš„ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ Keras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œåœ¨ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras `Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   ä¸€ä¸ªåªåŒ…å«`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦ä¸å®šçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šé¡ºåºçš„è¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])` æˆ– `model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šè¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L993)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None encoder_hidden_states: np.ndarray | tf.Tensor | None = None encoder_attention_mask: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 ç”¨äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºè¢«`masked`çš„æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹ï¼Œå°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹ï¼Œå°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯ä»¥åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹ï¼Œè¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—ï¼Œå¦‚ dropout æ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `encoder_hidden_states` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚

+   `encoder_attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨ç¼–ç å™¨è¾“å…¥çš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 ç”¨äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºè¢«`masked`çš„æ ‡è®°ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`Tuple[Tuple[tf.Tensor]]`ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰ â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚åœ¨è®­ç»ƒæœŸé—´è®¾ç½®ä¸º`False`ï¼Œåœ¨ç”ŸæˆæœŸé—´è®¾ç½®ä¸º`True`ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼‰ â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`tf.Tensor`ï¼‰ â€” åºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œè¿›ä¸€æ­¥ç”±çº¿æ€§å±‚å’Œ Tanh æ¿€æ´»å‡½æ•°å¤„ç†ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

    è¿™ä¸ªè¾“å‡ºé€šå¸¸*ä¸æ˜¯*è¾“å…¥è¯­ä¹‰å†…å®¹çš„å¥½æ‘˜è¦ï¼Œé€šå¸¸æœ€å¥½å¯¹æ•´ä¸ªè¾“å…¥åºåˆ—çš„éšè—çŠ¶æ€è¿›è¡Œå¹³å‡æˆ–æ± åŒ–ã€‚

+   `past_key_values`ï¼ˆ`List[tf.Tensor]`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚ 

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFXLMRobertaModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFXLMRobertaModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = TFXLMRobertaModel.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## TFXLMRobertaForCausalLM

### `class transformers.TFXLMRobertaForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1216)

```py
( config: XLMRobertaConfig *inputs **kwargs )
```

å‚æ•°

+   `config` (XLMRobertaConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM-RoBERTa æ¨¡å‹ï¼Œåœ¨é¡¶éƒ¨å¸¦æœ‰ç”¨äº CLM å¾®è°ƒçš„`è¯­è¨€å»ºæ¨¡`å¤´ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKeras æ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨è¯¸å¦‚`model.fit()`ä¹‹ç±»çš„æ–¹æ³•æ—¶ï¼Œæ‚¨åº”è¯¥å¯ä»¥â€œè½»æ¾åœ°â€ä½¿ç”¨ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨ Keras `Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   åªæœ‰ä¸€ä¸ªåŒ…å«`input_ids`çš„å¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦ä¸åŒçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1254)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None encoder_hidden_states: np.ndarray | tf.Tensor | None = None encoder_attention_mask: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy æ•°ç»„`æˆ–`tf.Tensor`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºæœªè¢«å±è”½çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«å±è”½çš„æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*optional*) â€” åˆ†æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ã€‚

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `inputs_embeds` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸­çš„`attentions`ã€‚è¿™ä¸ªå‚æ•°åªèƒ½åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸­çš„`hidden_states`ã€‚è¿™ä¸ªå‚æ•°åªèƒ½åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—ï¼Œå¦‚ dropout æ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `encoder_hidden_states` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚

+   `encoder_attention_mask` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*optional*) â€” ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥çš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºæœªè¢«å±è”½çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«å±è”½çš„æ ‡è®°ã€‚

+   `past_key_values` (`Tuple[Tuple[tf.Tensor]]`ï¼Œé•¿åº¦ä¸º `config.n_layers`) â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰çš„å½¢çŠ¶ä¸º `(batch_size, 1)`ï¼Œè€Œä¸æ˜¯æ‰€æœ‰`decoder_input_ids`çš„å½¢çŠ¶ä¸º `(batch_size, sequence_length)`ã€‚

+   `use_cache` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ™ä¼šè¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚åœ¨è®­ç»ƒæœŸé—´è®¾ç½®ä¸º`False`ï¼Œåœ¨ç”ŸæˆæœŸé—´è®¾ç½®ä¸º`True`ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—äº¤å‰ç†µåˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.vocab_size - 1]`èŒƒå›´å†…ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(n,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå…¶ä¸­ n æ˜¯éæ©ç æ ‡ç­¾çš„æ•°é‡ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`tf.Tensor`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values`ï¼ˆ`List[tf.Tensor]`ï¼Œ*å¯é€‰*ï¼Œåœ¨ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

TFXLMRobertaForCausalLM çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ä¾‹å¦‚ï¼š

```py
>>> from transformers import AutoTokenizer, TFXLMRobertaForCausalLM
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = TFXLMRobertaForCausalLM.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)
>>> logits = outputs.logits
```

## TFXLMRobertaForMaskedLM

### `class transformers.TFXLMRobertaForMaskedLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1126)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM RoBERTa æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

TensorFlow æ¨¡å‹å’Œå±‚åœ¨`transformers`ä¸­æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼ŒKeras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œåœ¨ä½¿ç”¨è¯¸å¦‚`model.fit()`ä¹‹ç±»çš„æ–¹æ³•æ—¶ï¼Œåº”è¯¥ä¼šâ€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä¼ é€’`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼çš„è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   åªæœ‰`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1145)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFMaskedLMOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   å¯¹äº`æœªè¢«æ©ç `çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äº`è¢«æ©ç `çš„æ ‡è®°ä¸º 0ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®çš„ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`Numpy æ•°ç»„` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨ `[0, 1]` ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢« `æ©ç `ã€‚

+   `inputs_embeds` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† `input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—ï¼Œå¦‚ä¸¢å¼ƒæ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[-100, 0, ..., config.vocab_size]` ä¸­ï¼ˆè¯·å‚é˜… `input_ids` æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º `-100` çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼Œä»…å¯¹æ ‡ç­¾åœ¨ `[0, ..., config.vocab_size]` ä¸­çš„æ ‡è®°è®¡ç®—æŸå¤±ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFMaskedLMOutput æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFMaskedLMOutput æˆ–ä¸€ä¸ª `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False` æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(n,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›ï¼Œå…¶ä¸­ n æ˜¯éå±è”½æ ‡ç­¾çš„æ•°é‡) â€” æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰æŸå¤±ã€‚

+   `logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´éƒ¨çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯å±‚æ¨¡å‹çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´éƒ¨ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFXLMRobertaForMaskedLM çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFXLMRobertaForMaskedLM
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = TFXLMRobertaForMaskedLM.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("The capital of France is <mask>.", return_tensors="tf")
>>> logits = model(**inputs).logits

>>> # retrieve index of <mask>
>>> mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
>>> selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

>>> predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
>>> tokenizer.decode(predicted_token_id)
' Paris'
```

```py
>>> labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
>>> # mask labels of non-<mask> tokens
>>> labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

>>> outputs = model(**inputs, labels=labels)
>>> round(float(outputs.loss), 2)
0.1
```

## TFXLMRobertaForSequenceClassification

### `class transformers.TFXLMRobertaForSequenceClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1395)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰é¡¶éƒ¨çš„åºåˆ—åˆ†ç±»/å›å½’å¤´ï¼ˆæ± åŒ–è¾“å‡ºé¡¶éƒ¨çš„çº¿æ€§å±‚ï¼‰çš„ XLM RoBERTa æ¨¡å‹å˜æ¢å™¨ï¼Œä¾‹å¦‚ç”¨äº GLUE ä»»åŠ¡ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ Keras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   åªæœ‰ä¸€ä¸ªå¸¦æœ‰`input_ids`çš„å¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„è¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå¸¦æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡çš„å­—å…¸ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1414)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š

    +   1 è¡¨ç¤ºæœªè¢«é®è”½çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«é®è”½çš„æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚ dropout æ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`ä¸­ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFSequenceClassifierOutput æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFSequenceClassifierOutput æˆ–ç”±å„ç§å…ƒç´ ç»„æˆçš„`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰å–å†³äºé…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`tf.Tensor`ï¼‰â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ - å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ - å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFXLMRobertaForSequenceClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFXLMRobertaForSequenceClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
>>> model = TFXLMRobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

>>> logits = model(**inputs).logits

>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
>>> model.config.id2label[predicted_class_id]
'optimism'
```

```py
>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = TFXLMRobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion", num_labels=num_labels)

>>> labels = tf.constant(1)
>>> loss = model(**inputs, labels=labels).loss
>>> round(float(loss), 2)
0.08
```

## TFXLMRobertaForMultipleChoice

### `class transformers.TFXLMRobertaForMultipleChoice`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1483)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰ - å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åœ¨é¡¶éƒ¨å…·æœ‰å¤šé€‰åˆ†ç±»å¤´çš„ XLM Roberta æ¨¡å‹ï¼ˆåœ¨æ± åŒ–è¾“å‡ºçš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚å’Œä¸€ä¸ª softmaxï¼‰ï¼Œä¾‹å¦‚ç”¨äº RocStories/SWAG ä»»åŠ¡ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

TensorFlow æ¨¡å‹å’Œ`transformers`ä¸­çš„å±‚æ¥å—ä¸¤ç§æ ¼å¼ä½œä¸ºè¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

ç¬¬äºŒç§æ ¼å¼å¾—åˆ°æ”¯æŒçš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKeras æ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨è¯¸å¦‚`model.fit()`ä¹‹ç±»çš„æ–¹æ³•æ—¶ï¼Œå¯¹æ‚¨æ¥è¯´åº”è¯¥â€œåªéœ€å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨ Keras`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   åªæœ‰ä¸€ä¸ª`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨ [å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›å†…å®¹ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1506)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids` (`Numpy æ•°ç»„` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_choices, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.`call`() å’Œ PreTrainedTokenizer.encode()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`Numpy æ•°ç»„` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_choices, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºè¢« `æœªæ©ç ` çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢« `æ©ç ` çš„æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`Numpy æ•°ç»„` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_choices, sequence_length)`ï¼Œ*å¯é€‰*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰å®šåœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   0 å¯¹åº”äºä¸€ä¸ª *å¥å­ A* çš„æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äºä¸€ä¸ª *å¥å­ B* çš„æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`Numpy æ•°ç»„` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_choices, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰å®šèŒƒå›´ä¸º `[0, config.max_position_embeddings - 1]`ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`Numpy æ•°ç»„` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æ˜¯ `æœªæ©ç çš„`,

    +   0 è¡¨ç¤ºå¤´éƒ¨æ˜¯ `æ©ç ` çš„ã€‚

+   `inputs_embeds` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_choices, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’ä¸€ä¸ªåµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°† `input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯ä»¥åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—ï¼Œå¦‚ dropout æ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—å¤šé¡¹é€‰æ‹©åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨ `[0, ..., num_choices]` èŒƒå›´å†…ï¼Œå…¶ä¸­ `num_choices` æ˜¯è¾“å…¥å¼ é‡ç¬¬äºŒç»´çš„å¤§å°ã€‚(å‚è§ä¸Šé¢çš„ `input_ids`)

è¿”å›å€¼

transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆ`tf.Tensor`å½¢çŠ¶ä¸º*(batch_size, )*ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_choices)`çš„`tf.Tensor`ï¼‰â€” *num_choices*æ˜¯è¾“å…¥å¼ é‡çš„ç¬¬äºŒç»´åº¦ã€‚ï¼ˆå‚è§ä¸Šé¢çš„*input_ids*ï¼‰ã€‚

    SoftMax ä¹‹å‰çš„åˆ†ç±»åˆ†æ•°ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFXLMRobertaForMultipleChoice çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFXLMRobertaForMultipleChoice
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = TFXLMRobertaForMultipleChoice.from_pretrained("xlm-roberta-base")

>>> prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
>>> choice0 = "It is eaten with a fork and a knife."
>>> choice1 = "It is eaten while held in the hand."

>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
>>> inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
>>> outputs = model(inputs)  # batch size is 1

>>> # the linear classifier still needs to be trained
>>> logits = outputs.logits
```

## TFXLMRobertaForTokenClassification

### `class transformers.TFXLMRobertaForTokenClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1588)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM RoBERTa æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªæ ‡è®°åˆ†ç±»å¤´ï¼ˆéšè—çŠ¶æ€è¾“å‡ºçš„é¡¶éƒ¨çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ Keras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨ `model.fit()` ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯¹æ‚¨â€œåªéœ€å·¥ä½œâ€ - åªéœ€ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ï¼Œä»¥ä»»ä½• `model.fit()` æ”¯æŒçš„æ ¼å¼ï¼ç„¶è€Œï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨ Keras `Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   åªæœ‰ä¸€ä¸ªåŒ…å« `input_ids` çš„å¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])` æˆ– `model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1615)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFTokenClassifierOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.`call`() å’Œ PreTrainedTokenizer.encode()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]`ï¼š

    +   1 è¡¨ç¤ºæœªè¢« `masked` çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºè¢« `masked` çš„æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•é€‰åœ¨ `[0, 1]`ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]`ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask` (`Numpy array` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(num_heads,)` æˆ– `(num_layers, num_heads)`ï¼Œ*å¯é€‰*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚

+   `inputs_embeds` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† `input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„ `attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—ï¼Œå¦‚ dropout æ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” ç”¨äºè®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚

è¿”å›å€¼

transformers.modeling_tf_outputs.TFTokenClassifierOutput æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFTokenClassifierOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(n,)`ï¼Œ*optional*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`) â€” åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFXLMRobertaForTokenClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFXLMRobertaForTokenClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("ydshieh/roberta-large-ner-english")
>>> model = TFXLMRobertaForTokenClassification.from_pretrained("ydshieh/roberta-large-ner-english")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
... )

>>> logits = model(**inputs).logits
>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
>>> predicted_tokens_classes
['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']
```

```py
>>> labels = predicted_token_class_ids
>>> loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
>>> round(float(loss), 2)
0.01
```

## TFXLMRobertaForQuestionAnswering

### `class transformers.TFXLMRobertaForQuestionAnswering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1684)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM RoBERTa æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªç”¨äºæå–å¼é—®ç­”ä»»åŠ¡ï¼ˆå¦‚ SQuADï¼‰çš„è·¨åº¦åˆ†ç±»å¤´ï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„çº¿æ€§å±‚ä¸Šè®¡ç®—`span start logits`å’Œ`span end logits`ï¼‰ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   åœ¨æ‰€æœ‰è¾“å…¥éƒ½ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆå¦‚ PyTorch æ¨¡å‹ï¼‰æ—¶ï¼Œæˆ–è€…

+   åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ Keras æ–¹æ³•åœ¨å‘æ¨¡å‹å’Œå±‚ä¼ é€’è¾“å…¥æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€åƒå¯¹å¾…`model.fit()`æ”¯æŒçš„ä»»ä½•å…¶ä»–æ ¼å¼ä¸€æ ·ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras `Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   ä¸€ä¸ªä»…åŒ…å«`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„è¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py#L1706)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None start_positions: np.ndarray | tf.Tensor | None = None end_positions: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`Numpy`æ•°ç»„æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚ dropout æ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `start_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦èµ·å§‹ä½ç½®çš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ï¼ˆ`sequence_length`ï¼‰ã€‚è¶…å‡ºåºåˆ—èŒƒå›´çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚

+   `end_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦ç»“æŸä½ç½®çš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ï¼Œä»¥è®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ï¼ˆ`sequence_length`ï¼‰ã€‚è¶…å‡ºåºåˆ—èŒƒå›´çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚

è¿”å›å€¼

transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`start_positions`å’Œ`end_positions`æ—¶è¿”å›ï¼‰- æ€»è·¨åº¦æå–æŸå¤±æ˜¯èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„äº¤å‰ç†µä¹‹å’Œã€‚

+   `start_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼‰- SoftMax ä¹‹å‰çš„è·¨åº¦èµ·å§‹åˆ†æ•°ã€‚

+   `end_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼‰- SoftMax ä¹‹å‰çš„è·¨åº¦ç»“æŸåˆ†æ•°ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› Softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFXLMRobertaForQuestionAnswering çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFXLMRobertaForQuestionAnswering
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("ydshieh/roberta-base-squad2")
>>> model = TFXLMRobertaForQuestionAnswering.from_pretrained("ydshieh/roberta-base-squad2")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

>>> inputs = tokenizer(question, text, return_tensors="tf")
>>> outputs = model(**inputs)

>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens)
' puppet'
```

```py
>>> # target is "nice puppet"
>>> target_start_index = tf.constant([14])
>>> target_end_index = tf.constant([15])

>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
>>> loss = tf.math.reduce_mean(outputs.loss)
>>> round(float(loss), 2)
0.86
```

JAXHide JAX å†…å®¹

## FlaxXLMRobertaModel

### `class transformers.FlaxXLMRobertaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L1000)

```py
( config: XLMRobertaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ XLM RoBERTa æ¨¡å‹å˜æ¢å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä» PyTorch æ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰

æ­¤æ¨¡å‹è¿˜æ˜¯[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax äºšéº»æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥äº†è§£ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„ JAX åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š

+   å³å°†ï¼ˆJITï¼‰ç¼–è¯‘

+   è‡ªåŠ¨å¾®åˆ†

+   çŸ¢é‡åŒ–

+   å¹¶è¡ŒåŒ–

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L830)

```py
( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—ä»¤ç‰Œçš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    è¾“å…¥ ID æ˜¯ä»€ä¹ˆï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤º`æœªå±è”½`çš„ä»¤ç‰Œï¼Œ

    +   0 è¡¨ç¤º`å±è”½`çš„ä»¤ç‰Œã€‚

    æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µä»¤ç‰Œç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*ä»¤ç‰Œï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*ä»¤ç‰Œã€‚

    ä»¤ç‰Œç±»å‹ ID æ˜¯ä»€ä¹ˆï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—ä»¤ç‰Œåœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

+   `head_mask` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œå¯é€‰) -- ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«æ©è”½ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®(XLMRobertaConfig)å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`) â€” åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œè¿›ä¸€æ­¥ç”±ä¸€ä¸ªçº¿æ€§å±‚å’Œä¸€ä¸ª Tanh æ¿€æ´»å‡½æ•°å¤„ç†ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxXLMRobertaPreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxXLMRobertaModel

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = FlaxXLMRobertaModel.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## FlaxXLMRobertaForCausalLM

### `class transformers.FlaxXLMRobertaForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L1462)

```py
( config: XLMRobertaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config` (XLMRobertaConfig) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM Roberta æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ï¼ˆéšè—çŠ¶æ€è¾“å‡ºçš„é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºè‡ªå›å½’ä»»åŠ¡ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä» PyTorch æ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax äºšéº»æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒ JAX çš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L830)

```py
( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer æ¥è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ`å¯é€‰`ï¼‰- ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`jnp.ndarray`ï¼‰- è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(jnp.ndarray))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰- é•¿åº¦ä¸º`config.n_layers`çš„`jnp.ndarray`å…ƒç»„çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder = True`æ—¶ç›¸å…³ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

`FlaxXLMRobertaPreTrainedModel`çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxXLMRobertaForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = FlaxXLMRobertaForCausalLM.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
>>> outputs = model(**inputs)

>>> # retrieve logts for next token
>>> next_token_logits = outputs.logits[:, -1]
```

## FlaxXLMRobertaForMaskedLM

### `class transformers.FlaxXLMRobertaForMaskedLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L1070)

```py
( config: XLMRobertaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰é¡¶éƒ¨`è¯­è¨€å»ºæ¨¡`å¤´çš„ XLM RoBERTa æ¨¡å‹ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä» PyTorch æ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax linen æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„ JAX ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L830)

```py
( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDsï¼Ÿ

+   `attention_mask` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äºä¸€ä¸ª*å¥å­ A*çš„æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äºä¸€ä¸ª*å¥å­ B*çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯ token type IDsï¼Ÿ

+   `position_ids` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

+   `head_mask` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ`å¯é€‰) -- ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç `ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling æˆ–è€…`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling æˆ–è€…ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–è€…`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`) â€” åºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œè¿›ä¸€æ­¥ç”±ä¸€ä¸ªçº¿æ€§å±‚å’Œä¸€ä¸ª Tanh æ¿€æ´»å‡½æ•°å¤„ç†ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–è€…`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚ 

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–è€…`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ã€‚

`FlaxXLMRobertaPreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxXLMRobertaForMaskedLM

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = FlaxXLMRobertaForMaskedLM.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

>>> outputs = model(**inputs)
>>> logits = outputs.logits
```

## FlaxXLMRobertaForSequenceClassification

### `class transformers.FlaxXLMRobertaForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L1137)

```py
( config: XLMRobertaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM Roberta æ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªåºåˆ—åˆ†ç±»/å›å½’å¤´ï¼ˆåœ¨æ±‡æ€»è¾“å‡ºçš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº GLUE ä»»åŠ¡ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½ã€ä¿å­˜å’Œä» PyTorch æ¨¡å‹è½¬æ¢æƒé‡ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax linen æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

æœ€åï¼Œè¿™ä¸ªæ¨¡å‹æ”¯æŒå†…ç½®çš„ JAX ç‰¹æ€§ï¼Œæ¯”å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [å‘é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L830)

```py
( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `numpy.ndarray`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`() ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 ç”¨äº `æœªè¢«æ©ç›–` çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äº `è¢«æ©ç›–` çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µæ ‡è®°ç´¢å¼•ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* çš„æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `numpy.ndarray`ï¼Œ`å¯é€‰ï¼‰-- ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨ `æœªè¢«æ©ç›–`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨ `è¢«æ©ç›–`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`jnp.ndarray`ï¼‰- åˆ†ç±»ï¼ˆå¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxXLMRobertaPreTrainedModel`çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxXLMRobertaForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = FlaxXLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

>>> outputs = model(**inputs)
>>> logits = outputs.logits
```

## FlaxXLMRobertaForMultipleChoice

### `class transformers.FlaxXLMRobertaForMultipleChoice`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L1218)

```py
( config: XLMRobertaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM Roberta æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰å¤šé€‰åˆ†ç±»å¤´ï¼ˆæ± åŒ–è¾“å‡ºé¡¶éƒ¨çš„çº¿æ€§å±‚å’Œ Softmaxï¼‰ï¼Œä¾‹å¦‚ç”¨äº RocStories/SWAG ä»»åŠ¡ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä» PyTorch æ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax linen æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥äº†è§£ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„ JAX ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L830)

```py
( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`numpy.ndarray` of shape `(batch_size, num_choices, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯ input IDs?

+   `attention_mask` (`numpy.ndarray` of shape `(batch_size, num_choices, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯ attention masks?

+   `token_type_ids` (`numpy.ndarray` of shape `(batch_size, num_choices, sequence_length)`, *optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *sentence A* tokenï¼Œ

    +   1 å¯¹åº”äº *sentence B* tokenã€‚

    ä»€ä¹ˆæ˜¯ token type IDs?

+   `position_ids` (`numpy.ndarray` of shape `(batch_size, num_choices, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

+   `head_mask` (`numpy.ndarray` of shape `(batch_size, num_choices, sequence_length)`, `optional) -- ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®(XLMRobertaConfig)å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `logits` (`jnp.ndarray` of shape `(batch_size, num_choices)`) â€” *num_choices* æ˜¯è¾“å…¥å¼ é‡çš„ç¬¬äºŒç»´ã€‚(å‚è§ä¸Šé¢çš„ *input_ids*)ã€‚

    åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯ä¸€å±‚æ¨¡å‹çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ã€‚

`FlaxXLMRobertaPreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxXLMRobertaForMultipleChoice

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = FlaxXLMRobertaForMultipleChoice.from_pretrained("xlm-roberta-base")

>>> prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
>>> choice0 = "It is eaten with a fork and a knife."
>>> choice1 = "It is eaten while held in the hand."

>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
>>> outputs = model(**{k: v[None, :] for k, v in encoding.items()})

>>> logits = outputs.logits
```

## FlaxXLMRobertaForTokenClassification

### `class transformers.FlaxXLMRobertaForTokenClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L1300)

```py
( config: XLMRobertaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config`ï¼ˆXLMRobertaConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM Roberta æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªæ ‡è®°åˆ†ç±»å¤´ï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä» PyTorch æ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax linen æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

æœ€åï¼Œè¿™ä¸ªæ¨¡å‹æ”¯æŒå†…ç½®çš„ JAX ç‰¹æ€§ï¼Œæ¯”å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L830)

```py
( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxTokenClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç›–`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   0 å¯¹åº”äºä¸€ä¸ª*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äºä¸€ä¸ª*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ`å¯é€‰`ï¼‰-- ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç›–`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç›–`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxTokenClassifierOutput æˆ–è€…`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxTokenClassifierOutput æˆ–è€…ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–è€…`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®(XLMRobertaConfig)å’Œè¾“å…¥ä¸åŒå…ƒç´ ã€‚

+   `logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`) â€” åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–è€…`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–è€…`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxXLMRobertaPreTrainedModel`çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxXLMRobertaForTokenClassification

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = FlaxXLMRobertaForTokenClassification.from_pretrained("xlm-roberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

>>> outputs = model(**inputs)
>>> logits = outputs.logits
```

## FlaxXLMRobertaForQuestionAnswering

### `class transformers.FlaxXLMRobertaForQuestionAnswering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L1377)

```py
( config: XLMRobertaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config` (XLMRobertaConfig) â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

XLM Roberta æ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªç”¨äºæå–å¼é—®ç­”ä»»åŠ¡ï¼ˆå¦‚ SQuADï¼‰çš„è·¨åº¦åˆ†ç±»å¤´ï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç”¨äºè®¡ç®—`è·¨åº¦èµ·å§‹ logits`å’Œ`è·¨åº¦ç»“æŸ logits`ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä» PyTorch æ¨¡å‹ä¸‹è½½ã€ä¿å­˜å’Œè½¬æ¢æƒé‡ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ Flax linen æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

æœ€åï¼Œè¿™ä¸ªæ¨¡å‹æ”¯æŒ JAX çš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py#L830)

```py
( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰åœ¨`[0, 1]`ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

+   `head_mask` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ`optional) -- ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆXLMRobertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `start_logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è·¨åº¦èµ·å§‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `end_logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è·¨åº¦ç»“æŸåˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºçš„éšè—çŠ¶æ€ã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxXLMRobertaPreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ æ’­çš„é…æ–¹éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxXLMRobertaForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
>>> model = FlaxXLMRobertaForQuestionAnswering.from_pretrained("xlm-roberta-base")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
>>> inputs = tokenizer(question, text, return_tensors="jax")

>>> outputs = model(**inputs)
>>> start_scores = outputs.start_logits
>>> end_scores = outputs.end_logits
```
