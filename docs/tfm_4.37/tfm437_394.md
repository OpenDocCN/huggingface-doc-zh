# Trainer 的实用程序

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/internal/trainer_utils`](https://huggingface.co/docs/transformers/v4.37.2/en/internal/trainer_utils)

此页面列出了由 Trainer 使用的所有实用函数。

这些大多数只有在研究库中 Trainer 的代码时才有用。

## 实用程序

### `class transformers.EvalPrediction`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L134)

```py
( predictions: Union label_ids: Union inputs: Union = None )
```

参数

+   `predictions` (`np.ndarray`) — 模型的预测。

+   `label_ids` (`np.ndarray`) — 要匹配的目标。

+   `inputs` (`np.ndarray`, *可选*) —

评估输出（始终包含标签），用于计算指标。

### `class transformers.IntervalStrategy`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L208)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

一个枚举。

#### `transformers.enable_full_determinism`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L58)

```py
( seed: int warn_only: bool = False )
```

用于在分布式训练期间获得可重复行为的辅助函数。参见

+   [`pytorch.org/docs/stable/notes/randomness.html`](https://pytorch.org/docs/stable/notes/randomness.html) 适用于 pytorch

+   [`www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism`](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) 适用于 tensorflow

#### `transformers.set_seed`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L85)

```py
( seed: int )
```

参数

+   `seed` (`int`) — 要设置的种子。

用于在`random`、`numpy`、`torch`和/或`tf`（如果已安装）中设置种子以获得可重复的行为的辅助函数。

#### `transformers.torch_distributed_zero_first`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L242)

```py
( local_rank: int )
```

参数

+   `local_rank` (`int`) — 本地进程的等级。

装饰器，使分布式训练中的所有进程等待每个本地主机执行某些操作。

## 回调内部

### `class transformers.trainer_callback.CallbackHandler`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L307)

```py
( callbacks model tokenizer optimizer lr_scheduler )
```

内部类，按顺序调用回调列表。

## 分布式评估

### `class transformers.trainer_pt_utils.DistributedTensorGatherer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L371)

```py
( world_size num_samples make_multiple_of = None padding_index = -100 )
```

参数

+   `world_size` (`int`) — 分布式训练中使用的进程数。

+   `num_samples` (`int`) — 数据集中的样本数量。

+   `make_multiple_of` (`int`, *可选*) — 如果传递，类会假定传递给每个进程的数据集是该参数的倍数（通过添加样本）。

+   `padding_index` (`int`, *可选*, 默认为-100) — 如果数组的序列长度不相同时要使用的填充索引。

一个负责通过块在 CPU 上正确聚合张量（或嵌套的列表/元组张量）的类。

如果我们的数据集有 16 个样本，每个进程上有 3 个进程的批量大小为 2，并且我们在每一步都进行聚合然后传输到 CPU，我们的采样器将生成以下索引：

`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1]`

为了获得大小为 3 的倍数的数据（以便每个进程获得相同的数据集长度）。然后进程 0、1 和 2 将负责为以下样本进行预测：

+   P0: `[0, 1, 2, 3, 4, 5]`

+   P1: `[6, 7, 8, 9, 10, 11]`

+   P2: `[12, 13, 14, 15, 0, 1]`

每个进程上处理的第一批将是

+   P0: `[0, 1]`

+   P1: `[6, 7]`

+   P2: `[12, 13]`

因此，如果我们在第一批结束时进行聚合，我们将获得一个张量（嵌套的列表/元组张量），对应以下索引：

`[0, 1, 6, 7, 12, 13]`

如果我们直接连接我们的结果而不采取任何预防措施，用户将在预测循环结束时按此顺序获得索引的预测：

`[0, 1, 6, 7, 12, 13, 2, 3, 8, 9, 14, 15, 4, 5, 10, 11, 0, 1]`

由于某种原因，这不会引起他们的兴趣。这个类就是为了解决这个问题。

#### `add_arrays`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L431)

```py
( arrays )
```

将`arrays`添加到内部存储，将在第一次传递数组时将存储初始化为完整大小，以便如果我们绑定要发生 OOM，它会在开始时发生。

#### `finalize`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L467)

```py
( )
```

返回正确收集的数组并截断到样本数量（因为采样器添加了一些额外的内容，以使每个进程的数据集长度相同）。

## Trainer 参数解析器

### `class transformers.HfArgumentParser`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L109)

```py
( dataclass_types: Union **kwargs )
```

这个`argparse.ArgumentParser`的子类使用数据类的类型提示生成参数。

该类设计用于与原生 argparse 很好地配合。特别是，在初始化后可以向解析器添加更多（非数据类支持的）参数，解析后将作为额外的命名空间返回。可选：要创建子参数组，请在数据类中使用`_argument_group_name`属性。

#### `parse_args_into_dataclasses`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L265)

```py
( args = None return_remaining_strings = False look_for_args_file = True args_filename = None args_file_flag = None ) → export const metadata = 'undefined';Tuple consisting of
```

返回

由以下内容组成的元组

+   以与它们传递给初始化程序时相同的顺序的数据类实例。abspath

+   如果适用，用于在初始化后向解析器添加更多（非数据类支持的）参数的额外命名空间。

+   剩余参数字符串的潜在列表。（与 argparse.ArgumentParser.parse_known_args 相同）

将命令行参数解析为指定数据类类型的实例。

这依赖于 argparse 的`ArgumentParser.parse_known_args`。请参阅文档：docs.python.org/3.7/library/argparse.html#argparse.ArgumentParser.parse_args

#### `parse_dict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L351)

```py
( args: Dict allow_extra_keys: bool = False ) → export const metadata = 'undefined';Tuple consisting of
```

参数

+   `args`（`dict`）—包含配置值的字典

+   `allow_extra_keys`（`bool`，*可选*，默认为`False`）—默认为 False。如果为 False，则如果字典包含未解析的键，则会引发异常。

返回

由以下内容组成的元组

+   以与它们传递给初始化程序时相同的顺序的数据类实例。

另一个不使用`argparse`的辅助方法，而是使用字典并填充数据类类型。

#### `parse_json_file`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L379)

```py
( json_file: str allow_extra_keys: bool = False ) → export const metadata = 'undefined';Tuple consisting of
```

参数

+   `json_file`（`str`或`os.PathLike`）—要解析的 json 文件的文件名

+   `allow_extra_keys`（`bool`，*可选*，默认为`False`）—默认为 False。如果为 False，则如果 json 文件包含未解析的键，则会引发异常。

返回

由以下内容组成的元组

+   以与它们传递给初始化程序时相同的顺序的数据类实例。

另一个不使用`argparse`的辅助方法，而是加载一个 json 文件并填充数据类类型。

#### `parse_yaml_file`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L401)

```py
( yaml_file: str allow_extra_keys: bool = False ) → export const metadata = 'undefined';Tuple consisting of
```

参数

+   `yaml_file`（`str`或`os.PathLike`）—要解析的 yaml 文件的文件名

+   `allow_extra_keys`（`bool`，*可选*，默认为`False`）—默认为 False。如果为 False，则如果 json 文件包含未解析的键，则会引发异常。

返回

由以下内容组成的元组

+   以与它们传递给初始化程序时相同的顺序的数据类实例。

另一种不使用`argparse`的辅助方法，而是加载一个 yaml 文件并填充数据类类型。

## 调试工具

### `class transformers.debug_utils.DebugUnderflowOverflow`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/debug_utils.py#L27)

```py
( model max_frames_to_save = 21 trace_batch_nums = [] abort_after_batch_num = None )
```

参数

+   `model` (`nn.Module`) — 要调试的模型。

+   `max_frames_to_save` (`int`，*可选*，默认为 21) — 记录多少帧以前

+   `trace_batch_nums(List[int],` *可选*，默认为`[]`) — 要跟踪的批次号（关闭检测）

+   `abort_after_batch_num` (`int`，*可选*) — 是否在某个批次号完成后中止

此调试类有助于检测和理解模型何时开始变得非常大或非常小，更重要的是`nan`或`inf`权重和激活元素。

有 2 种工作模式：

1.  下溢/溢出检测（默认）

1.  没有检测的特定批次绝对最小/最大跟踪

模式 1：下溢/溢出检测

激活下溢/溢出检测，使用以下模型初始化对象：

```py
debug_overflow = DebugUnderflowOverflow(model)
```

然后像平常一样运行训练，如果在权重、输入或输出元素中至少有一个检测到`nan`或`inf`，此模块将抛出异常，并打印导致此事件的`max_frames_to_save`帧，每帧报告

1.  完全限定的模块名称加上运行其`forward`的类名

1.  每个模块权重的所有元素的绝对最小值和最大值，以及输入和输出

例如，这是在 fp16 中运行的`google/mt5-small`的检测报告中的标题和最后几帧

混合精度：

```py
Detected inf/nan during batch_number=0
Last 21 forward frames:
abs min  abs max  metadata [...]
                  encoder.block.2.layer.1.DenseReluDense.wi_0 Linear
2.17e-07 4.50e+00 weight
1.79e-06 4.65e+00 input[0]
2.68e-06 3.70e+01 output
                  encoder.block.2.layer.1.DenseReluDense.wi_1 Linear
8.08e-07 2.66e+01 weight
1.79e-06 4.65e+00 input[0]
1.27e-04 2.37e+02 output
                  encoder.block.2.layer.1.DenseReluDense.wo Linear
1.01e-06 6.44e+00 weight
0.00e+00 9.74e+03 input[0]
3.18e-04 6.27e+04 output
                  encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense
1.79e-06 4.65e+00 input[0]
3.18e-04 6.27e+04 output
                  encoder.block.2.layer.1.dropout Dropout
3.18e-04 6.27e+04 input[0]
0.00e+00      inf output
```

您可以在这里看到，`T5DenseGatedGeluDense.forward` 的输出激活结果，其绝对最大值约为 62.7K，非常接近 fp16 的 64K 上限。在下一帧中，我们有`Dropout`，它重新规范化权重，在将一些元素归零后，将绝对最大值推到超过 64K，导致溢出。

正如您所看到的，当数字开始变得非常大时，我们需要查看之前的帧以了解 fp16 数字。

跟踪是在前向挂钩中完成的，该挂钩在`forward`完成后立即调用。

默认情况下，将打印最后 21 帧。您可以更改默认值以适应您的需求。例如：

```py
debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)
```

为了验证您已正确设置此调试功能，并且您打算在可能需要几个小时才能完成的训练中使用它，首先使用正常跟踪启用运行一些批次，如下一节所述。

模式 2. 没有检测的特定批次绝对最小/最大跟踪

第二种工作模式是批次跟踪，关闭下溢/溢出检测功能。

假设您想观察每个`forward`调用的所有成分的绝对最小值和最大值

给定批次，并且仅对第 1 和第 3 批次执行此操作。然后，实例化此类：

```py
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])
```

现在完整的第 1 和第 3 批次将使用与上述相同的格式进行跟踪。批次从 0 开始索引。

如果您知道程序在某个批次号之后开始表现不佳，那么您可以直接快进到该区域。

提前停止：

您还可以指定在哪个批次号之后停止训练，使用：

```py
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)
```

此功能主要在跟踪模式下有用，但您可以在任何模式下使用它。

**性能**：

由于此模块在每次前向传播时测量模型的每个权重的绝对`min`/`max`，因此会减慢训练速度。因此，请记住在满足调试需求后将其关闭。
