# FLAN-T5

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-t5`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-t5)

## 概述

FLAN-T5 发布在论文[扩展指令微调语言模型](https://arxiv.org/pdf/2210.11416.pdf)中 - 这是 T5 的增强版本，已在多种任务中进行微调。

可以直接使用 FLAN-T5 权重，无需微调模型：

```py
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
>>> tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

>>> inputs = tokenizer("A step by step recipe to make bolognese pasta:", return_tensors="pt")
>>> outputs = model.generate(**inputs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Pour a cup of bolognese into a large bowl and add the pasta']
```

FLAN-T5 包含与 T5 版本 1.1 相同的改进（有关模型改进的完整详情，请参见[此处](https://huggingface.co/docs/transformers/model_doc/t5v1.1)。）

Google 发布了以下变体：

+   [google/flan-t5-small](https://huggingface.co/google/flan-t5-small)

+   [google/flan-t5-base](https://huggingface.co/google/flan-t5-base)

+   [google/flan-t5-large](https://huggingface.co/google/flan-t5-large)

+   [google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl)

+   [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl)。

原始检查点可以在[此处](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)找到。

有关所有 API 参考、代码示例和笔记本，请参阅 T5 的文档页面。有关 FLAN-T5 的训练和评估的更多详细信息，请参考模型卡片。
