# DialoGPT

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt)

## 概述

DialoGPT 是由 Yizhe Zhang、Siqi Sun、Michel Galley、Yen-Chun Chen、Chris Brockett、Xiang Gao、Jianfeng Gao、Jingjing Liu、Bill Dolan 在[《DialoGPT:大规模生成式预训练用于对话回复生成》](https://arxiv.org/abs/1911.00536)中提出的。它是一个在 Reddit 上提取的 147M 对话式交流数据上训练的 GPT2 模型。

论文摘要如下：

*我们提出了一个大型、可调节的神经对话回复生成模型 DialoGPT（对话生成式预训练变压器）。在 Reddit 评论链中提取的 147M 对话式交流数据上训练，跨越 2005 年至 2017 年，DialoGPT 扩展了 Hugging Face PyTorch 变压器，以在单轮对话设置中实现接近人类的自动和人类评估性能。我们展示了利用 DialoGPT 的对话系统生成比强基线系统更相关、内容更丰富和上下文更一致的回复。预训练模型和训练流程已公开发布，以促进神经回复生成研究和更智能的开放领域对话系统的发展。*

原始代码可以在[这里](https://github.com/microsoft/DialoGPT)找到。

## 使用提示

+   DialoGPT 是一个带有绝对位置嵌入的模型，因此通常建议在右侧填充输入而不是左侧。

+   DialoGPT 在对话数据上使用因果语言建模（CLM）目标进行训练，因此在开放领域对话系统中的回复生成方面非常强大。

+   DialoGPT 使用户可以仅用 10 行代码创建一个聊天机器人，如[DialoGPT 的模型卡片](https://huggingface.co/microsoft/DialoGPT-medium)所示。

训练：

为了训练或微调 DialoGPT，可以使用因果语言建模训练。引用官方论文中的话：*我们遵循 OpenAI GPT-2，将多轮对话会话建模为长文本，并将生成任务构建为语言建模。我们首先将对话会话中的所有对话轮次连接成一个长文本 x_1,…, x_N（N 为序列长度），以结束文本标记结束。*更多信息请参考原始论文。

DialoGPT 的架构基于 GPT2 模型，请参考 GPT2 的文档页面获取 API 参考和示例。
