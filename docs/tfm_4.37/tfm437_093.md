# 哲学

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/philosophy`](https://huggingface.co/docs/transformers/v4.37.2/en/philosophy)

🤗 Transformers 是一个为以下人群构建的主观库：

+   希望使用、研究或扩展大规模 Transformer 模型的机器学习研究人员和教育工作者。

+   希望微调这些模型或在生产中使用它们的实践者。

+   只想下载预训练模型并用于解决特定机器学习任务的工程师。

该库设计时有两个强烈的目标：

1.  尽可能简单快捷地使用：

+   我们强烈限制了用户接触的抽象数量，事实上，几乎没有抽象，只需要三个标准类来使用每个模型：配置、模型和一个预处理类（NLP 的分词器、视觉的图像处理器、音频的特征提取器以及多模态输入的处理器）。

+   所有这些类都可以通过使用通用的`from_pretrained()`方法从预训练实例中简单统一地初始化，该方法会从[Hugging Face Hub](https://huggingface.co/models)提供的预训练检查点或您自己保存的检查点下载（如果需要），缓存并加载相关的类实例和相关数据（配置的超参数、分词器的词汇和模型的权重）。

+   除了这三个基本类之外，该库还提供两个 API：pipeline()用于快速在给定任务上使用模型进行推断，以及 Trainer 用于快速训练或微调 PyTorch 模型（所有 TensorFlow 模型都兼容`Keras.fit`）。

+   因此，该库不是神经网络的模块化工具箱。如果您想扩展或构建该库，只需使用常规的 Python、PyTorch、TensorFlow、Keras 模块，并从库的基类继承以重用模型加载和保存等功能。如果您想了解更多关于我们模型编码哲学的信息，请查看我们的[Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy)博客文章。

1.  提供性能尽可能接近原始模型的最新模型：

+   我们至少为每种架构提供一个示例，该示例重现了该架构的官方作者提供的结果。

+   代码通常尽可能接近原始代码库，这意味着一些 PyTorch 代码可能不像*pytorchic*那样，因为它可能是转换为 TensorFlow 代码，反之亦然。

另外几个目标：

+   尽可能一致地暴露模型的内部：

    +   我们使用单个 API 访问完整的隐藏状态和注意力权重。

    +   预处理类和基础模型 API 被标准化，以便轻松在模型之间切换。

+   整合一组有前途的工具，用于微调和研究这些模型：

    +   一种简单而一致的方法，用于向词汇表和嵌入中添加新的标记以进行微调。

    +   简单的方法来屏蔽和修剪 Transformer 头。

+   轻松在 PyTorch、TensorFlow 2.0 和 Flax 之间切换，允许使用一个框架进行训练，使用另一个框架进行推断。

## 主要概念

该库围绕每个模型的三种类型的类构建：

+   **模型类**可以是 PyTorch 模型（[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)）、Keras 模型（[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)）或 JAX/Flax 模型（[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)），可以使用库中提供的预训练权重。

+   **配置类**存储构建模型所需的超参数（例如层数和隐藏大小）。您不总是需要自己实例化这些。特别是，如果您使用预训练模型而没有任何修改，创建模型将自动处理实例化配置（这是模型的一部分）。

+   **预处理类**将原始数据转换为模型接受的格式。一个 tokenizer 存储每个模型的词汇表，并提供编码和解码字符串的方法，以便将其转换为要馈送给模型的标记嵌入索引列表。图像处理器预处理视觉输入，特征提取器预处理音频输入，以及一个处理器处理多模态输入。

所有这些类都可以从预训练实例实例化，保存在本地，并通过三种方法在 Hub 上共享：

+   `from_pretrained()`允许您从库本身提供的预训练版本（支持的模型可以在[Model Hub](https://huggingface.co/models)上找到）或用户本地（或服务器上）存储的模型、配置和预处理类实例化。

+   `save_pretrained()`允许您将模型、配置和预处理类保存在本地，以便可以使用`from_pretrained()`重新加载。

+   `push_to_hub()`允许您将模型、配置和预处理类共享到 Hub，以便所有人都可以轻松访问。
