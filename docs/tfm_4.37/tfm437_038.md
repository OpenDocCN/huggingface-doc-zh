# 计算机视觉知识蒸馏

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/tasks/knowledge_distillation_for_image_classification`](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/knowledge_distillation_for_image_classification)

知识蒸馏是一种技术，用于将知识从一个更大、更复杂的模型（教师）转移到一个更小、更简单的模型（学生）。为了从一个模型中提取知识到另一个模型，我们采用一个在特定任务上（本例中为图像分类）训练过的预训练教师模型，并随机初始化一个学生模型用于图像分类训练。接下来，我们训练学生模型以最小化其输出与教师输出之间的差异，从而使其模仿行为。这最初是由 [Hinton 等人在神经网络中提取知识](https://arxiv.org/abs/1503.02531) 中首次引入的。在这个指南中，我们将进行特定任务的知识蒸馏。我们将使用 [beans 数据集](https://huggingface.co/datasets/beans)。

这个指南演示了如何使用 🤗 Transformers 的 [Trainer API](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) 将一个 [fine-tuned ViT 模型](https://huggingface.co/merve/vit-mobilenet-beans-224)（教师模型）蒸馏到一个 [MobileNet](https://huggingface.co/google/mobilenet_v2_1.4_224)（学生模型）。

让我们安装进行蒸馏和评估过程所需的库。

```py
pip install transformers datasets accelerate tensorboard evaluate --upgrade
```

在这个例子中，我们使用 `merve/beans-vit-224` 模型作为教师模型。这是一个基于 `google/vit-base-patch16-224-in21k` 在 beans 数据集上微调的图像分类模型。我们将将这个模型蒸馏到一个随机初始化的 MobileNetV2。

我们现在将加载数据集。

```py
from datasets import load_dataset

dataset = load_dataset("beans")
```

我们可以从任一模型中使用图像处理器，因为在这种情况下它们返回相同分辨率的相同输出。我们将使用 `dataset` 的 `map()` 方法将预处理应用于数据集的每个拆分。

```py
from transformers import AutoImageProcessor
teacher_processor = AutoImageProcessor.from_pretrained("merve/beans-vit-224")

def process(examples):
    processed_inputs = teacher_processor(examples["image"])
    return processed_inputs

processed_datasets = dataset.map(process, batched=True)
```

基本上，我们希望学生模型（随机初始化的 MobileNet）模仿教师模型（微调的视觉变换器）。为了实现这一点，我们首先从教师和学生中获取 logits 输出。然后，我们将它们中的每一个除以控制每个软目标重要性的参数 `temperature`。一个称为 `lambda` 的参数权衡了蒸馏损失的重要性。在这个例子中，我们将使用 `temperature=5` 和 `lambda=0.5`。我们将使用 Kullback-Leibler 散度损失来计算学生和教师之间的差异。给定两个数据 P 和 Q，KL 散度解释了我们需要多少额外信息来用 Q 表示 P。如果两者相同，它们的 KL 散度为零，因为不需要其他信息来解释 P。因此，在知识蒸馏的背景下，KL 散度是有用的。

```py
from transformers import TrainingArguments, Trainer
import torch
import torch.nn as nn
import torch.nn.functional as F

class ImageDistilTrainer(Trainer):
    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):
        super().__init__(model=student_model, *args, **kwargs)
        self.teacher = teacher_model
        self.student = student_model
        self.loss_function = nn.KLDivLoss(reduction="batchmean")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.teacher.to(device)
        self.teacher.eval()
        self.temperature = temperature
        self.lambda_param = lambda_param

    def compute_loss(self, student, inputs, return_outputs=False):
        student_output = self.student(**inputs)

        with torch.no_grad():
          teacher_output = self.teacher(**inputs)

        # Compute soft targets for teacher and student
        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)

        # Compute the loss
        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)

        # Compute the true label loss
        student_target_loss = student_output.loss

        # Calculate final loss
        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss
        return (loss, student_output) if return_outputs else loss
```

我们现在将登录到 Hugging Face Hub，这样我们就可以通过 `Trainer` 将我们的模型推送到 Hugging Face Hub。

```py
from huggingface_hub import notebook_login

notebook_login()
```

让我们设置 `TrainingArguments`、教师模型和学生模型。

```py
from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification

training_args = TrainingArguments(
    output_dir="my-awesome-model",
    num_train_epochs=30,
    fp16=True,
    logging_dir=f"{repo_name}/logs",
    logging_strategy="epoch",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="tensorboard",
    push_to_hub=True,
    hub_strategy="every_save",
    hub_model_id=repo_name,
    )

num_labels = len(processed_datasets["train"].features["labels"].names)

# initialize models
teacher_model = AutoModelForImageClassification.from_pretrained(
    "merve/beans-vit-224",
    num_labels=num_labels,
    ignore_mismatched_sizes=True
)

# training MobileNetV2 from scratch
student_config = MobileNetV2Config()
student_config.num_labels = num_labels
student_model = MobileNetV2ForImageClassification(student_config)
```

我们可以使用 `compute_metrics` 函数在测试集上评估我们的模型。这个函数将在训练过程中用于计算我们模型的 `准确率` 和 `f1`。

```py
import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    acc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))
    return {"accuracy": acc["accuracy"]}
```

让我们使用我们定义的训练参数初始化 `Trainer`。我们还将初始化我们的数据收集器。

```py
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()
trainer = ImageDistilTrainer(
    student_model=student_model,
    teacher_model=teacher_model,
    training_args=training_args,
    train_dataset=processed_datasets["train"],
    eval_dataset=processed_datasets["validation"],
    data_collator=data_collator,
    tokenizer=teacher_processor,
    compute_metrics=compute_metrics,
    temperature=5,
    lambda_param=0.5
)
```

我们现在可以训练我们的模型。

```py
trainer.train()
```

我们可以在测试集上评估模型。

```py
trainer.evaluate(processed_datasets["test"])
```

在测试集上，我们的模型达到了 72％的准确率。为了对蒸馏效率进行合理性检查，我们还使用相同的超参数从头开始在豆类数据集上训练 MobileNet，并观察到测试集上的 63％准确率。我们邀请读者尝试不同的预训练教师模型、学生架构、蒸馏参数，并报告他们的发现。蒸馏模型的训练日志和检查点可以在[此存储库](https://huggingface.co/merve/vit-mobilenet-beans-224)中找到，从头开始训练的 MobileNetV2 可以在此[存储库](https://huggingface.co/merve/resnet-mobilenet-beans-5)中找到。
