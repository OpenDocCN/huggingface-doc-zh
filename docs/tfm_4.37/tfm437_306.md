# EnCodec

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/encodec`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/encodec)

## 概述

EnCodec 神经编解码器模型由 Alexandre Défossez、Jade Copet、Gabriel Synnaeve 和 Yossi Adi 在[High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438)中提出。

论文摘要如下：

*我们引入了一种基于神经网络的最先进的实时、高保真音频编解码器。它由一个流式编码器-解码器架构组成，具有量化的潜在空间，以端到端的方式进行训练。我们通过使用一个多尺度频谱对抗器简化和加速训练，有效减少了伪影并产生高质量样本。我们引入了一种新颖的损失平衡机制来稳定训练：损失的权重现在定义了它应该代表整体梯度的比例，从而将这个超参数的选择与典型损失的规模分离。最后，我们研究了轻量级 Transformer 模型如何进一步压缩获得的表示，最多可减少 40%，同时保持快于实时。我们提供了对所提出模型的关键设计选择的详细描述，包括：训练目标、架构变化以及各种感知损失函数的研究。我们进行了广泛的主观评估（MUSHRA 测试），并对一系列带宽和音频领域进行了消融研究，包括语音、嘈杂混响语音和音乐。我们的方法在所有评估设置中优于基线方法，考虑到 24 kHz 单声道和 48 kHz 立体声音频。*

该模型由[Matthijs](https://huggingface.co/Matthijs)、[Patrick Von Platen](https://huggingface.co/patrickvonplaten)和[Arthur Zucker](https://huggingface.co/ArthurZ)贡献。原始代码可在[此处](https://github.com/facebookresearch/encodec)找到。

## 用法示例

以下是使用该模型对音频进行编码和解码的快速示例：

```py
>>> from datasets import load_dataset, Audio
>>> from transformers import EncodecModel, AutoProcessor
>>> librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

>>> model = EncodecModel.from_pretrained("facebook/encodec_24khz")
>>> processor = AutoProcessor.from_pretrained("facebook/encodec_24khz")
>>> librispeech_dummy = librispeech_dummy.cast_column("audio", Audio(sampling_rate=processor.sampling_rate))
>>> audio_sample = librispeech_dummy[-1]["audio"]["array"]
>>> inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors="pt")

>>> encoder_outputs = model.encode(inputs["input_values"], inputs["padding_mask"])
>>> audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs["padding_mask"])[0]
>>> # or the equivalent with a forward pass
>>> audio_values = model(inputs["input_values"], inputs["padding_mask"]).audio_values
```

## EncodecConfig

### `class transformers.EncodecConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/configuration_encodec.py#L35)

```py
( target_bandwidths = [1.5, 3.0, 6.0, 12.0, 24.0] sampling_rate = 24000 audio_channels = 1 normalize = False chunk_length_s = None overlap = None hidden_size = 128 num_filters = 32 num_residual_layers = 1 upsampling_ratios = [8, 5, 4, 2] norm_type = 'weight_norm' kernel_size = 7 last_kernel_size = 7 residual_kernel_size = 3 dilation_growth_rate = 2 use_causal_conv = True pad_mode = 'reflect' compress = 2 num_lstm_layers = 2 trim_right_ratio = 1.0 codebook_size = 1024 codebook_dim = None use_conv_shortcut = True **kwargs )
```

参数

+   `target_bandwidths` (`List[float]`, *optional*, defaults to `[1.5, 3.0, 6.0, 12.0, 24.0]`) — 模型可以对音频进行编码的不同带宽范围。

+   `sampling_rate` (`int`, *optional*, defaults to 24000) — 音频波形应数字化的采样率，以赫兹（Hz）表示。

+   `audio_channels` (`int`, *optional*, defaults to 1) — 音频数据中的通道数。单声道为 1，立体声为 2。

+   `normalize` (`bool`, *optional*, defaults to `False`) — 传递音频时是否应该进行归一化。

+   `chunk_length_s` (`float`, *optional*) — 如果定义了，音频将被预处理成长度为`chunk_length_s`的块，然后进行编码。

+   `overlap` (`float`, *optional*) — 定义每个块之间的重叠。用于计算`chunk_stride`的公式为：`int((1.0 - self.overlap) * self.chunk_length)`。

+   `hidden_size` (`int`, *optional*, defaults to 128) — 中间表示维度。

+   `num_filters` (`int`, *optional*, defaults to 32) — 第一个`EncodecConv1d`下采样层的卷积核数量。

+   `num_residual_layers` (`int`, *optional*, defaults to 1) — 残差层的数量。

+   `upsampling_ratios` (`Sequence[int]` , *optional*, defaults to `[8, 5, 4, 2]`) — 核大小和步幅比例。编码器使用下采样比率而不是上采样比率，因此将使用与此处指定的相反顺序的比率，这些比率必须与解码器顺序匹配。

+   `norm_type` (`str`, *optional*, defaults to `"weight_norm"`) — 归一化方法。应为`["weight_norm", "time_group_norm"]`之一。

+   `kernel_size` (`int`, *optional*, defaults to 7) — 初始卷积的核大小。

+   `last_kernel_size` (`int`, *可选*, 默认为 7) — 最后一个卷积层的核大小。

+   `residual_kernel_size` (`int`, *可选*, 默认为 3) — 残差层的核大小。

+   `dilation_growth_rate` (`int`, *可选*, 默认为 2) — 每层增加膨胀的量。

+   `use_causal_conv` (`bool`, *可选*, 默认为`True`) — 是否使用完全因果卷积。

+   `pad_mode` (`str`, *可选*, 默认为`"reflect"`) — 卷积的填充模式。

+   `compress` (`int`, *可选*, 默认为 2) — 在残差分支中的降维度（来自 Demucs v3）。

+   `num_lstm_layers` (`int`, *可选*, 默认为 2) — 编码器末端的 LSTM 层数。

+   `trim_right_ratio` (`float`, *可选*, 默认为 1.0) — 在`use_causal_conv = True`设置下对转置卷积右侧进行修剪的比例。如果等于 1.0，则表示所有修剪都在右侧完成。

+   `codebook_size` (`int`, *可选*, 默认为 1024) — 组成 VQVAE 的离散代码的数量。

+   `codebook_dim` (`int`, *可选*) — 代码书向量的维度。如果未定义，则使用`hidden_size`。

+   `use_conv_shortcut` (`bool`, *可选*, 默认为`True`) — 是否在`EncodecResnetBlock`块中使用卷积层作为“跳过”连接。如果为 False，将使用一个恒等函数，提供一个通用的残差连接。

这是用于存储 EncodecModel 配置的配置类。它用于根据指定的参数实例化一个 Encodec 模型，定义模型架构。使用默认值实例化配置将产生类似于[facebook/encodec_24khz](https://huggingface.co/facebook/encodec_24khz)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import EncodecModel, EncodecConfig

>>> # Initializing a "facebook/encodec_24khz" style configuration
>>> configuration = EncodecConfig()

>>> # Initializing a model (with random weights) from the "facebook/encodec_24khz" style configuration
>>> model = EncodecModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## EncodecFeatureExtractor

### `class transformers.EncodecFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/feature_extraction_encodec.py#L29)

```py
( feature_size: int = 1 sampling_rate: int = 24000 padding_value: float = 0.0 chunk_length_s: float = None overlap: float = None **kwargs )
```

参数

+   `feature_size` (`int`, *可选*, 默认为 1) — 提取特征的特征维度。对于单声道使用 1，立体声使用 2。

+   `sampling_rate` (`int`, *可选*, 默认为 24000) — 音频波形应数字化的采样率，以赫兹（Hz）表示。

+   `padding_value` (`float`, *可选*, 默认为 0.0) — 用于填充值的值。

+   `chunk_length_s` (`float`, *可选*) — 如果定义了，则音频将被预处理成长度为`chunk_length_s`的块，然后进行编码。

+   `overlap` (`float`, *可选*) — 定义每个块之间的重叠。用于计算`chunk_stride`的公式为：`int((1.0 - self.overlap) * self.chunk_length)`。

构建一个 EnCodec 特征提取器。

这个特征提取器继承自 SequenceFeatureExtractor，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。

使用默认值实例化特征提取器将产生类似于[facebook/encodec_24khz](https://huggingface.co/facebook/encodec_24khz)架构的配置。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/feature_extraction_encodec.py#L84)

```py
( raw_audio: Union padding: Union = None truncation: Optional = False max_length: Optional = None return_tensors: Union = None sampling_rate: Optional = None )
```

参数

+   `raw_audio` (`np.ndarray`，`List[float]`，`List[np.ndarray]`，`List[List[float]]`) — 要处理的序列或序列批次。每个序列可以是一个 numpy 数组，一个浮点值列表，一个 numpy 数组列表或一个浮点值列表的列表。对于单声道音频（`feature_size = 1`），numpy 数组的形状必须为`(num_samples,)`，对于立体声音频（`feature_size = 2`），形状为`(2, num_samples)`。

+   `padding` (`bool`，`str`或 PaddingStrategy，*可选*，默认为`True`) — 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引）：

    +   `True`或`'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度（使用参数`max_length`）或模型的最大可接受输入长度（如果未提供该参数）。

    +   `False`或`'do_not_pad'`（默认）：无填充（即，可以输出长度不同的序列批次）。

+   `truncation` (`bool`，*可选*，默认为`False`) — 激活截断，将长于`max_length`的输入序列截断为`max_length`。

+   `max_length` (`int`, *可选*) — 返回列表的最大长度，可选填充长度（见上文）。

+   `return_tensors` (`str`或 TensorType，*可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`：返回 TensorFlow `tf.constant`对象。

    +   `'pt'`：返回 PyTorch `torch.Tensor`对象。

    +   `'np'`：返回 Numpy `np.ndarray`对象。

+   `sampling_rate` (`int`，*可选*) — 对`audio`输入进行采样的采样率。强烈建议在前向调用时传递`sampling_rate`以防止潜在错误。

对一个或多个序列进行特征化和准备模型的主要方法。

## EncodecModel

### `class transformers.EncodecModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L526)

```py
( config: EncodecConfig )
```

参数

+   `config`（EncodecConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

EnCodec 神经音频编解码器模型。该模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以了解所有与一般用法和行为相关的事项。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L707)

```py
( audio_codes: Tensor audio_scales: Tensor padding_mask: Optional = None return_dict: Optional = None )
```

参数

+   `audio_codes` (`torch.FloatTensor`，形状为`(batch_size, nb_chunks, chunk_length)`，*可选*) — 使用`model.encode`计算的离散码嵌入。

+   `audio_scales` (`torch.Tensor`，形状为`(batch_size, nb_chunks)`，*可选*) — 每个`audio_codes`输入的缩放因子。

+   `padding_mask` (`torch.Tensor`，形状为`(batch_size, channels, sequence_length)`） — 用于填充`input_values`的填充蒙版。

+   `return_dict` (`bool`，*可选*) — 是否返回 ModelOutput 而不是普通元组。

将给定的帧解码为输出音频波形。

请注意，输出可能比输入大一点。在这种情况下，可以裁剪末尾的任何额外步骤。

#### `encode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L579)

```py
( input_values: Tensor padding_mask: Tensor = None bandwidth: Optional = None return_dict: Optional = None )
```

参数

+   `input_values` (`torch.Tensor` of shape `(batch_size, channels, sequence_length)`) — 输入音频波形的浮点值。

+   `padding_mask` (`torch.Tensor` of shape `(batch_size, channels, sequence_length)`) — 用于填充`input_values`的填充掩码。

+   `bandwidth` (`float`, *optional*) — 目标带宽。必须是`config.target_bandwidths`之一。如果为`None`，则使用最小可能的带宽。带宽表示为其千分之一，例如 6kbps 带宽表示为`bandwidth == 6.0`

将输入音频波形编码为离散代码。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L755)

```py
( input_values: Tensor padding_mask: Optional = None bandwidth: Optional = None audio_codes: Optional = None audio_scales: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.encodec.modeling_encodec.EncodecOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`, *optional*) — 将原始音频输入转换为 Float 并填充到适当的长度，以便使用长度为 self.chunk_length 的块和`config.chunk_stride`的步长进行编码。

+   `padding_mask` (`torch.BoolTensor` of shape `(batch_size, channels, sequence_length)`, *optional*) — 用于避免在填充标记索引上计算缩放因子的掩码（我们可以避免在这些上计算卷积+）。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    应始终传递`padding_mask`，除非输入被截断或未填充。这是因为为了有效处理张量，输入音频应该被填充，以便`input_length % stride = step`，其中`step = chunk_length-stride`。这确保所有块具有相同的形状

+   `bandwidth` (`float`, *optional*) — 目标带宽。必须是`config.target_bandwidths`之一。如果为`None`，则使用最小可能的带宽。带宽表示为其千分之一，例如 6kbps 带宽表示为`bandwidth == 6.0`

+   `audio_codes` (`torch.FloatTensor` of shape `(batch_size, nb_chunks, chunk_length)`, *optional*) — 使用`model.encode`计算的离散代码嵌入。

+   `audio_scales` (`torch.Tensor` of shape `(batch_size, nb_chunks)`, *optional*) — 每个`audio_codes`输入的缩放因子。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.encodec.modeling_encodec.EncodecOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.encodec.modeling_encodec.EncodecOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（EncodecConfig）和输入的各种元素。

+   `audio_codes` (`torch.FloatTensor` of shape `(batch_size, nb_chunks, chunk_length)`, *optional*) — 使用`model.encode`计算的离散代码嵌入。

+   `audio_values` (`torch.FlaotTensor` of shape `(batch_size, sequence_length)`, *optional*) 解码音频值，使用 Encodec 的解码器部分获得。

EncodecModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoProcessor, EncodecModel

>>> dataset = load_dataset("ashraq/esc50")
>>> audio_sample = dataset["train"]["audio"][0]["array"]

>>> model_id = "facebook/encodec_24khz"
>>> model = EncodecModel.from_pretrained(model_id)
>>> processor = AutoProcessor.from_pretrained(model_id)

>>> inputs = processor(raw_audio=audio_sample, return_tensors="pt")

>>> outputs = model(**inputs)
>>> audio_codes = outputs.audio_codes
>>> audio_values = outputs.audio_values
```
