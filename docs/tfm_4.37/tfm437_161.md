# 编码器解码器模型

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/encoder-decoder`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/encoder-decoder)

## 概述

EncoderDecoderModel 可以用于初始化一个序列到序列模型，其中预训练的自编码模型作为编码器，预训练的自回归模型作为解码器。

在 [利用预训练检查点进行序列生成任务](https://arxiv.org/abs/1907.12461) 中展示了使用预训练检查点初始化序列到序列模型以进行序列生成任务的有效性，作者是 Sascha Rothe、Shashi Narayan 和 Aliaksei Severyn。

在训练/微调了这样一个 EncoderDecoderModel 之后，它可以像其他模型一样保存/加载（请参阅示例以获取更多信息）。

这种架构的一个应用可能是利用两个预训练的 BertModel 作为编码器和解码器，用于总结模型，就像在 Yang Liu 和 Mirella Lapata 的文章中展示的那样：[使用预训练编码器进行文本摘要](https://arxiv.org/abs/1908.08345)。

## 从模型配置随机初始化 EncoderDecoderModel。

EncoderDecoderModel 可以从编码器和解码器配置随机初始化。在下面的示例中，我们展示了如何使用默认的 BertModel 配置作为编码器和默认的 `BertForCausalLM` 配置作为解码器来做到这一点。

```py
>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

>>> config_encoder = BertConfig()
>>> config_decoder = BertConfig()

>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)
>>> model = EncoderDecoderModel(config=config)
```

## 从预训练的编码器和预训练的解码器初始化 EncoderDecoderModel。

EncoderDecoderModel 可以从预训练的编码器检查点和预训练的解码器检查点初始化。请注意，任何预训练的自编码模型，例如 BERT，都可以作为编码器，而预训练的自编码模型，例如 BERT，预训练的因果语言模型，例如 GPT2，以及序列到序列模型的预训练解码器部分，例如 BART 的解码器，都可以用作解码器。根据您选择的解码器架构，交叉注意力层可能会被随机初始化。从预训练的编码器和解码器检查点初始化 EncoderDecoderModel 需要对模型进行下游任务的微调，正如在 *Warm-starting-encoder-decoder 博客文章* 中所示。为此，`EncoderDecoderModel` 类提供了一个 EncoderDecoderModel.from_encoder_decoder_pretrained() 方法。

```py
>>> from transformers import EncoderDecoderModel, BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")
```

## 加载现有的 EncoderDecoderModel 检查点并执行推理。

要加载 `EncoderDecoderModel` 类的微调检查点，EncoderDecoderModel 提供了 `from_pretrained(...)` 方法，就像 Transformers 中的其他模型架构一样。

要执行推理，可以使用 `generate` 方法，该方法允许自回归生成文本。该方法支持各种解码形式，例如贪婪、束搜索和多项式采样。

```py
>>> from transformers import AutoTokenizer, EncoderDecoderModel

>>> # load a fine-tuned seq2seq model and corresponding tokenizer
>>> model = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert_cnn_daily_mail")
>>> tokenizer = AutoTokenizer.from_pretrained("patrickvonplaten/bert2bert_cnn_daily_mail")

>>> # let's perform inference on a long piece of text
>>> ARTICLE_TO_SUMMARIZE = (
...     "PG&E stated it scheduled the blackouts in response to forecasts for high winds "
...     "amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were "
...     "scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
... )
>>> input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors="pt").input_ids

>>> # autoregressively generate summary (uses greedy decoding by default)
>>> generated_ids = model.generate(input_ids)
>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_text)
nearly 800 thousand customers were affected by the shutoffs. the aim is to reduce the risk of wildfires. nearly 800, 000 customers were expected to be affected by high winds amid dry conditions. pg & e said it scheduled the blackouts to last through at least midday tomorrow.
```

## 将 PyTorch 检查点加载到 TFEncoderDecoderModel 中。

TFEncoderDecoderModel.from_pretrained()目前不支持从 pytorch 检查点初始化模型。将`from_pt=True`传递给此方法将引发异常。如果特定编码器解码器模型仅有 pytorch 检查点，可以使用以下解决方法：

```py
>>> # a workaround to load from pytorch checkpoint
>>> from transformers import EncoderDecoderModel, TFEncoderDecoderModel

>>> _model = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert-cnn_dailymail-fp16")

>>> _model.encoder.save_pretrained("./encoder")
>>> _model.decoder.save_pretrained("./decoder")

>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
... )
>>> # This is only for copying some specific attributes of this particular model.
>>> model.config = _model.config
```

## 训练

创建模型后，可以像对 BART、T5 或任何其他编码器解码器模型进行微调一样进行微调。如您所见，为了计算损失，模型只需要 2 个输入：`input_ids`（编码输入序列的`input_ids`）和`labels`（编码目标序列的`input_ids`）。

```py
>>> from transformers import BertTokenizer, EncoderDecoderModel

>>> tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")

>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id

>>> input_ids = tokenizer(
...     "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side.During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was  finished in 1930\. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft).Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
...     return_tensors="pt",
... ).input_ids

>>> labels = tokenizer(
...     "the eiffel tower surpassed the washington monument to become the tallest structure in the world. it was the first structure to reach a height of 300 metres in paris in 1930\. it is now taller than the chrysler building by 5\. 2 metres ( 17 ft ) and is the second tallest free - standing structure in paris.",
...     return_tensors="pt",
... ).input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
```

详细的[Colab](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=ZwQIEhKOrJpl)用于训练。

此模型由[thomwolf](https://github.com/thomwolf)贡献。此模型的 TensorFlow 和 Flax 版本由[ydshieh](https://github.com/ydshieh)贡献。

## EncoderDecoderConfig

### `class transformers.EncoderDecoderConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L25)

```py
( **kwargs )
```

参数

+   `kwargs` (*optional*) — 关键字参数的字典。特别是：

    +   `encoder` (PretrainedConfig, *optional*) — 定义编码器配置的配置对象实例。

    +   `decoder` (PretrainedConfig, *optional*) — 定义解码器配置的配置对象实例。

EncoderDecoderConfig 是用于存储 EncoderDecoderModel 配置的配置类。它用于根据指定的参数实例化编码器解码器模型，定义编码器和解码器配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

>>> # Initializing a BERT bert-base-uncased style configuration
>>> config_encoder = BertConfig()
>>> config_decoder = BertConfig()

>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

>>> # Initializing a Bert2Bert model (with random weights) from the bert-base-uncased style configurations
>>> model = EncoderDecoderModel(config=config)

>>> # Accessing the model configuration
>>> config_encoder = model.config.encoder
>>> config_decoder = model.config.decoder
>>> # set decoder config to causal lm
>>> config_decoder.is_decoder = True
>>> config_decoder.add_cross_attention = True

>>> # Saving the model, including its configuration
>>> model.save_pretrained("my-model")

>>> # loading model and config from pretrained folder
>>> encoder_decoder_config = EncoderDecoderConfig.from_pretrained("my-model")
>>> model = EncoderDecoderModel.from_pretrained("my-model", config=encoder_decoder_config)
```

#### `from_encoder_decoder_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L91)

```py
( encoder_config: PretrainedConfig decoder_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';EncoderDecoderConfig
```

返回值

EncoderDecoderConfig

配置对象的实例

从预训练的编码器模型配置和解码器模型配置实例化 EncoderDecoderConfig（或派生类）。

Pytorch 隐藏 Pytorch 内容

## EncoderDecoderModel

### `class transformers.EncoderDecoderModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L169)

```py
( config: Optional = None encoder: Optional = None decoder: Optional = None )
```

参数

+   `config` (EncoderDecoderConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

这个类可用于使用任何预训练的自编码模型作为编码器和任何预训练的自回归模型作为解码器初始化序列到序列模型。编码器通过 from_pretrained()函数加载，解码器通过 from_pretrained()函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如摘要）上进行微调。

在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu 的《利用预训练检查点进行序列生成任务》](https://arxiv.org/abs/1907.12461)中展示了使用预训练检查点初始化序列到序列模型进行序列生成任务的有效性。

一旦这样的编码器解码器模型已经训练/微调，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。

此模型继承自 PreTrainedModel。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

EncoderDecoderModel 是一个通用模型类，当使用:meth*~transformers.AutoModel.from_pretrained*类方法为编码器创建实例，并使用:meth*~transformers.AutoModelForCausalLM.from_pretrained*类方法为解码器创建实例时，将实例化为一个变压器架构。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L535)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Tuple = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 输入序列标记在词汇表中的索引。

    可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`之间：

    +   对于未被屏蔽的标记，为 1，

    +   对于被屏蔽的标记，为 0。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 解码器输入序列标记在词汇表中的索引。

    可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

    如果使用了`past_key_values`，则可能只需输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

    对于训练，模型会通过将`labels`向右移动，用`pad_token_id`替换-100，并在其前面加上`decoder_start_token_id`来自动创建`decoder_input_ids`。

+   `decoder_attention_mask` (`torch.BoolTensor`，形状为`(batch_size, target_sequence_length)`，*optional*) — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `encoder_outputs` (`tuple(torch.FloatTensor)`，*optional*) — 此元组必须包含（`last_hidden_state`，*optional*：`hidden_states`，*optional*：`attentions`）`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。

+   `past_key_values`（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组包含 4 个形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的张量） — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用了`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（这些`decoder_input_ids`没有给出其过去键值状态的模型），而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length, hidden_size)`，*optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果您想要更多控制权，以便将`decoder_input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于计算解码器的掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 如果设置为`True`，模型将返回`~utils.Seq2SeqLMOutput`而不是普通元组。

+   `kwargs`（*optional*） — 剩余的关键字参数字典。关键字参数有两种风格：

    +   不带前缀，将作为`**encoder_kwargs`输入到编码器前向函数中。

    +   带有*decoder_*前缀，将作为`**decoder_kwargs`输入到解码器前向函数中。

返回

transformers.modeling_outputs.Seq2SeqLMOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.Seq2SeqLMOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（EncoderDecoderConfig）和输入不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 语言建模损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。

    解码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。

    编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

EncoderDecoderModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import EncoderDecoderModel, BertTokenizer
>>> import torch

>>> tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(
...     "bert-base-uncased", "bert-base-uncased"
... )  # initialize Bert2Bert from pre-trained checkpoints

>>> # training
>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id
>>> model.config.vocab_size = model.config.decoder.vocab_size

>>> input_ids = tokenizer("This is a really long text", return_tensors="pt").input_ids
>>> labels = tokenizer("This is the corresponding summary", return_tensors="pt").input_ids
>>> outputs = model(input_ids=input_ids, labels=labels)
>>> loss, logits = outputs.loss, outputs.logits

>>> # save and load from pretrained
>>> model.save_pretrained("bert2bert")
>>> model = EncoderDecoderModel.from_pretrained("bert2bert")

>>> # generation
>>> generated = model.generate(input_ids)
```

#### `from_encoder_decoder_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L385)

```py
( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path: str = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path`（`str`，*可选*）— 初始化编码器所需的信息。可以是：

    +   一个字符串，预训练模型的*模型 id*，托管在 huggingface.co 上的模型仓库中。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。

    +   包含使用 save_pretrained()保存的模型权重的*目录*路径，例如，`./my_model_directory/`。

    +   指向*TensorFlow 索引检查点文件*的路径或 url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象作为`config`参数提供。使用此加载路径比使用提供的转换脚本将 TensorFlow 检查点转换为 PyTorch 模型并加载 PyTorch 模型要慢。

+   `decoder_pretrained_model_name_or_path`（`str`，*可选*，默认为`None`）— 初始化解码器所需的信息。可以是：

    +   一个字符串，托管在 huggingface.co 上的模型仓库中的*模型 id*。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。

    +   包含使用 save_pretrained()保存的模型权重的*目录*路径，例如，`./my_model_directory/`。

    +   指向*TensorFlow 索引检查点文件*的路径或 url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象作为`config`参数提供。使用此加载路径比使用提供的转换脚本将 TensorFlow 检查点转换为 PyTorch 模型并加载 PyTorch 模型要慢。

+   `model_args`（剩余的位置参数，*可选*）— 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数字典，*可选*）— 可用于更新配置对象（在加载后）并初始化模型（例如，`output_attentions=True`）。

    +   更新编码器配置时，对每个配置参数使用前缀*encoder_*。

    +   更新解码器配置时，对每个配置参数使用前缀*decoder_*。

    +   要更新父模型配置，请不要对每个配置参数使用前缀。

    根据是否提供`config`或自动加载而表现不同。

从预训练模型检查点中的一个或两个基类实例化一个编码器和一个解码器。

默认情况下使用`model.eval()`将模型设置为评估模式（Dropout 模块被停用）。要训练模型，需要首先使用`model.train()`将其设置回训练模式。

示例：

```py
>>> from transformers import EncoderDecoderModel

>>> # initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")
>>> # saving model after fine-tuning
>>> model.save_pretrained("./bert2bert")
>>> # load fine-tuned model
>>> model = EncoderDecoderModel.from_pretrained("./bert2bert")
```

TensorFlow 隐藏内容

## TFEncoderDecoderModel

### `class transformers.TFEncoderDecoderModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L192)

```py
( config: Optional[PretrainedConfig] = None encoder: Optional[TFPreTrainedModel] = None decoder: Optional[TFPreTrainedModel] = None )
```

参数

+   `config`（EncoderDecoderConfig） — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

这个类可以用来初始化一个序列到序列模型，其中编码器是任何预训练的自编码模型，解码器是任何预训练的自回归模型。编码器通过 from_pretrained()函数加载，解码器通过 from_pretrained()函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如摘要）上进行微调。

在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu 的《利用预训练检查点进行序列生成任务》](https://arxiv.org/abs/1907.12461)中展示了使用预训练检查点初始化序列到序列模型进行序列生成任务的有效性。

在训练/微调了这样的编码器解码器模型之后，它可以像其他模型一样保存/加载（查看示例以获取更多信息）。

这个模型继承自 TFPreTrainedModel。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取与一般使用和行为相关的所有内容。

TFEncoderDecoderModel 是一个通用模型类，当使用 from_pretrained()类方法为编码器创建一个库的基本模型类，并为解码器创建一个库的基本模型类时，将实例化为一个变压器架构。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L458)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False **kwargs ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, sequence_length)`） — 输入序列标记在词汇表中的索引。

    可以使用 PreTrainedTokenizer 获取索引。查看 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()获取详细信息。

    什么是输入 ID？

+   `attention_mask` (`np.ndarray`或`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `decoder_input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 词汇表中解码器输入序列标记的索引。

    可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

    如果使用了`past_key_values`，可以选择仅输入最后一个`decoder_input_ids`（请参阅`past_key_values`）。

    为解码器提供序列到序列训练。可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

+   `decoder_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — 这个元组必须包含 (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) 是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(tf.Tensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用了`past_key_values`，用户可以选择只输入最后一个`decoder_input_ids`（那些没有将过去的键值状态提供给该模型的）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这是有用的，而不是使用模型的内部嵌入查找矩阵。

+   `decoder_inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果您想要更多控制权来将`decoder_input_ids`索引转换为相关向量，这是有用的，而不是使用模型的内部嵌入查找矩阵。

+   `labels` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算解码器的掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 如果设置为`True`，模型将返回一个`~utils.Seq2SeqLMOutput`而不是一个普通的元组。

+   `training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

+   `kwargs` (*optional*) — 剩余的关键字参数字典。关键字参数有两种风格：

    +   没有前缀，将作为编码器前向函数的`**encoder_kwargs`输入。

    +   带有*decoder_*前缀，将作为解码器前向函数的`**decoder_kwargs`输入。

返回值

transformers.modeling_tf_outputs.TFSeq2SeqLMOutput 或`tuple(tf.Tensor)`

一个 transformers.modeling_tf_outputs.TFSeq2SeqLMOutput 或一个`tf.Tensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含各种元素，取决于配置（EncoderDecoderConfig）和输入。

+   `loss` (`tf.Tensor`，形状为`(n,)`，*optional*，其中 n 是未屏蔽标签的数量，当提供`labels`时返回） — 语言建模损失。

+   `logits` (`tf.Tensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`List[tf.Tensor]`，*optional*，当传递`use_cache=True`或当`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可以用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    解码器每一层的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    编码器每一层的隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

TFEncoderDecoderModel 前向方法，覆盖`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import TFEncoderDecoderModel, BertTokenizer

>>> # initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-cased", "gpt2")

>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

>>> # forward
>>> input_ids = tokenizer.encode(
...     "Hello, my dog is cute", add_special_tokens=True, return_tensors="tf"
... )  # Batch size 1
>>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)

>>> # training
>>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)
>>> loss, logits = outputs.loss, outputs.logits

>>> # save and load from pretrained
>>> model.save_pretrained("bert2gpt2")
>>> model = TFEncoderDecoderModel.from_pretrained("bert2gpt2")

>>> # generation
>>> generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.bos_token_id)
```

#### `from_encoder_decoder_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L311)

```py
( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path: str = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path`（`str`，*可选*）— 初始化编码器所需的信息。可以是：

    +   一个字符串，预训练模型的*模型 ID*，托管在 huggingface.co 上的模型存储库中。有效的模型 ID 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下进行命名空间，如`dbmdz/bert-base-german-cased`。

    +   一个包含使用 save_pretrained()保存的模型权重的*目录*的路径，例如`./my_model_directory/`。

    +   一个*pytorch 索引检查点文件*的路径或 URL（例如，`./pt_model/`）。在这种情况下，`encoder_from_pt`应设置为`True`。

+   `decoder_pretrained_model_name_or_path`（`str`，*可选*，默认为`None`）— 初始化解码器所需的信息。可以是：

    +   一个字符串，预训练模型的*模型 ID*，托管在 huggingface.co 上的模型存储库中。有效的模型 ID 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下进行命名空间，如`dbmdz/bert-base-german-cased`。

    +   一个包含使用 save_pretrained()保存的模型权重的*目录*的路径，例如`./my_model_directory/`。

    +   一个*pytorch 检查点文件*的路径或 URL（例如，`./pt_model/`）。在这种情况下，`decoder_from_pt`应设置为`True`。

+   `model_args`（剩余的位置参数，*可选*）— 所有剩余的位置参数将传递给基础模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数字典，*可选*）— 可用于更新配置对象（加载后）并初始化模型（例如，`output_attentions=True`）。

    +   要更新编码器配置，请为每个配置参数使用前缀*encoder_*。

    +   要更新解码器配置，请为每个配置参数使用前缀*decoder_*。

    +   要更新父模型配置，请不要为每个配置参数使用前缀。

    根据是否提供`config`或自动加载而表现不同。

从预训练模型检查点的一个或两个库基类实例化编码器和解码器。

示例：

```py
>>> from transformers import TFEncoderDecoderModel

>>> # initialize a bert2gpt2 from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized
>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "gpt2")
>>> # saving model after fine-tuning
>>> model.save_pretrained("./bert2gpt2")
>>> # load fine-tuned model
>>> model = TFEncoderDecoderModel.from_pretrained("./bert2gpt2")
```

JAXHide JAX 内容

## FlaxEncoderDecoderModel

### `class transformers.FlaxEncoderDecoderModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L301)

```py
( config: EncoderDecoderConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（EncoderDecoderConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

+   `dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`）— 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和`jax.numpy.bfloat16`（在 TPU 上）之一。

    这可以用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的`dtype`执行。

    “请注意，这仅指定计算的数据类型，不会影响模型参数的数据类型。”

    如果您希望更改模型参数的数据类型，请参阅 to_fp16()和 to_bf16()。

此类可用于使用任何预训练的自动编码模型作为编码器和任何预训练的自回归模型作为解码器初始化序列到序列模型。编码器通过 from_pretrained()函数加载，解码器通过 from_pretrained()函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如摘要）上进行微调。

在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu 的《利用预训练检查点进行序列生成任务》](https://arxiv.org/abs/1907.12461)中展示了使用预训练检查点初始化序列到序列模型进行序列生成任务的有效性。

在训练/微调了这样的编码器解码器模型之后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。

该模型继承自 FlaxPreTrainedModel。检查超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个 Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规 Flax 模块，并参考 Flax 文档以获取与一般用法和行为相关的所有事项。

FlaxEncoderDecoderModel 是一个通用的模型类，当使用:meth*~transformers.FlaxAutoModel.from_pretrained*类方法为编码器创建模块（flax.nn.Module）时，将实例化为一个变压器架构，并为解码器创建另一个模块，对于解码器，使用:meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained*类方法。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L628)

```py
( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `decoder_input_ids` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) — 词汇表中解码器输入序列标记的索引。

    可以使用 PreTrainedTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是解码器输入 ID？

    对于序列到序列训练，应提供`decoder_input_ids`。`decoder_input_ids`应在模型之外创建，通过将`labels`向右移动，用`pad_token_id`替换-100，并在其前面添加`decoder_start_token_id`。

+   `decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。默认情况下还将使用因果掩码。

+   `position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.encoder.max_position_embeddings - 1]`中选择。

+   `decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 每个解码器输入序列标记在位置嵌入中的位置索引。在范围`[0, config.decoder.max_position_embeddings - 1]`中选择。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 如果设置为`True`，模型将返回`~utils.FlaxSeq2SeqLMOutput`而不是普通元组。

返回

transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput 或 `tuple(torch.FloatTensor)`

transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput 或 `torch.FloatTensor` 元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（EncoderDecoderConfig）和输入。

+   `logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    解码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `decoder_attentions` (`tuple(jnp.ndarray)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(jnp.ndarray)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`jnp.ndarray`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    编码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `encoder_attentions` (`tuple(jnp.ndarray)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    编码器的注意力权重，在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

FlaxEncoderDecoderModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import FlaxEncoderDecoderModel, BertTokenizer, GPT2Tokenizer

>>> # load a fine-tuned bert2gpt2 model
>>> model = FlaxEncoderDecoderModel.from_pretrained("patrickvonplaten/bert2gpt2-cnn_dailymail-fp16")
>>> # load input & output tokenizer
>>> tokenizer_input = BertTokenizer.from_pretrained("bert-base-cased")
>>> tokenizer_output = GPT2Tokenizer.from_pretrained("gpt2")

>>> article = '''Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members
>>> singing a racist chant. SAE's national chapter suspended the students,
>>> but University of Oklahoma President David Boren took it a step further,
>>> saying the university's affiliation with the fraternity is permanently done.'''

>>> input_ids = tokenizer_input(article, add_special_tokens=True, return_tensors="np").input_ids

>>> # use GPT2's eos_token as the pad as well as eos token
>>> model.config.eos_token_id = model.config.decoder.eos_token_id
>>> model.config.pad_token_id = model.config.eos_token_id

>>> sequences = model.generate(input_ids, num_beams=4, max_length=12).sequences

>>> summary = tokenizer_output.batch_decode(sequences, skip_special_tokens=True)[0]
>>> assert summary == "SAS Alpha Epsilon suspended Sigma Alpha Epsilon members"
```

#### `from_encoder_decoder_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L760)

```py
( encoder_pretrained_model_name_or_path: Union = None decoder_pretrained_model_name_or_path: Union = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`，*optional*) — 初始化编码器所需的信息。可以是：

    +   字符串，huggingface.co 模型库中预训练模型的*模型 id*。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   指向包含使用 save_pretrained()保存的模型权重的*目录*的路径，例如，`./my_model_directory/`。

+   `decoder_pretrained_model_name_or_path`（`Union[str, os.PathLike]`，*可选*，默认为`None`）- 初始化解码器所需的信息。可以是：

    +   一个字符串，预训练模型的*模型 id*，托管在 huggingface.co 模型库中。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。

    +   指向包含使用 save_pretrained()保存的模型权重的*目录*的路径，例如，`./my_model_directory/`。

+   `model_args`（剩余的位置参数，*可选*）- 所有剩余的位置参数将传递给基础模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数，*可选*）- 可用于更新配置对象（加载后）并初始化模型（例如，`output_attentions=True`）。

    +   更新编码器配置时，对每个配置参数使用前缀*encoder_*。

    +   更新解码器配置时，对每个配置参数使用前缀*decoder_*。

    +   要更新父模型配置，请不要对每个配置参数使用前缀。

    根据是否提供或自动加载`config`而表现不同。

从预训练模型检查点实例化一个编码器和一个解码器，可以使用库的一个或两个基类。

示例：

```py
>>> from transformers import FlaxEncoderDecoderModel

>>> # initialize a bert2gpt2 from pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
>>> model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-cased", "gpt2")
>>> # saving model after fine-tuning
>>> model.save_pretrained("./bert2gpt2")
>>> # load fine-tuned model
>>> model = FlaxEncoderDecoderModel.from_pretrained("./bert2gpt2")
```
