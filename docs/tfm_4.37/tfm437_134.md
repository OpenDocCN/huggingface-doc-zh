# BARTpho

> 原始文本: [`huggingface.co/docs/transformers/v4.37.2/en/model_doc/bartpho`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bartpho)

## 概述

BARTpho 模型是由 Nguyen Luong Tran, Duong Minh Le 和 Dat Quoc Nguyen 在[《BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese》](https://arxiv.org/abs/2109.09701)中提出的。

论文摘要如下：

*我们提供了两个版本的 BARTpho — BARTpho_word 和 BARTpho_syllable — 这是为越南语预训练的首个公开大规模单语序列到序列模型。我们的 BARTpho 使用了 BART 序列到序列去噪模型的“large”架构和预训练方案，因此特别适用于生成式 NLP 任务。在越南语文本摘要的下游任务上的实验表明，在自动和人工评估中，我们的 BARTpho 优于强基线 mBART，并改进了最新技术。我们发布 BARTpho 以促进未来的生成式越南语 NLP 任务的研究和应用。*

这个模型是由[dqnguyen](https://huggingface.co/dqnguyen)贡献的。原始代码可以在[这里](https://github.com/VinAIResearch/BARTpho)找到。

## 用法示例

```py
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> bartpho = AutoModel.from_pretrained("vinai/bartpho-syllable")

>>> tokenizer = AutoTokenizer.from_pretrained("vinai/bartpho-syllable")

>>> line = "Chúng tôi là những nghiên cứu viên."

>>> input_ids = tokenizer(line, return_tensors="pt")

>>> with torch.no_grad():
...     features = bartpho(**input_ids)  # Models outputs are now tuples

>>> # With TensorFlow 2.0+:
>>> from transformers import TFAutoModel

>>> bartpho = TFAutoModel.from_pretrained("vinai/bartpho-syllable")
>>> input_ids = tokenizer(line, return_tensors="tf")
>>> features = bartpho(**input_ids)
```

## 用法提示

+   与 mBART 一样，BARTpho 使用 BART 的“large”架构，并在编码器和解码器的顶部增加了一个额外的层归一化层。因此，在 BART 文档中的用法示例，在适应 BARTpho 时，应通过用 mBART 专用类替换 BART 专用类来进行调整。例如：

```py
>>> from transformers import MBartForConditionalGeneration

>>> bartpho = MBartForConditionalGeneration.from_pretrained("vinai/bartpho-syllable")
>>> TXT = "Chúng tôi là <mask> nghiên cứu viên."
>>> input_ids = tokenizer([TXT], return_tensors="pt")["input_ids"]
>>> logits = bartpho(input_ids).logits
>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
>>> probs = logits[0, masked_index].softmax(dim=0)
>>> values, predictions = probs.topk(5)
>>> print(tokenizer.decode(predictions).split())
```

+   这个实现仅用于标记化：“monolingual_vocab_file”包含从多语言 XLM-RoBERTa 的预训练 SentencePiece 模型“vocab_file”中提取的越南语专用类型。其他语言，如果使用这个预训练的多语言 SentencePiece 模型“vocab_file”进行子词分割，可以重用 BartphoTokenizer 与自己的语言专用“monolingual_vocab_file”。

## BartphoTokenizer

### `class transformers.BartphoTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L46)

```py
( vocab_file monolingual_vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' sp_model_kwargs: Optional = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。这个词汇是来自多语言 XLM-RoBERTa 的预训练 SentencePiece 模型，也被 mBART 使用，包含 250K 种类型。

+   `monolingual_vocab_file` (`str`) — 单语词汇文件的路径。这个单语词汇包含从 250K 种类型的多语言词汇`vocab_file`中提取的越南语专用类型。

+   `bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

    在构建序列时使用特殊标记时，这不是用于序列开头的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。

    在构建序列时使用特殊标记时，这不是用于序列结尾的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *可选*, 默认为 `"<s>"`) — 用于进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。在使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇中不存在的标记无法转换为 ID，而是设置为此标记。

+   `pad_token`（`str`，*可选*，默认为`"<pad>"`）- 用于填充的标记，例如在批处理不同长度的序列时。

+   `mask_token`（`str`，*可选*，默认为`"<mask>"`）- 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `sp_model_kwargs`（`dict`，*可选*）- 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece 的 Python 包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：

    +   `enable_sampling`：启用子词正则化。

    +   `nbest_size`：unigram 的抽样参数。对于 BPE-Dropout 无效。

        +   `nbest_size = {0,1}`：不执行抽样。

        +   `nbest_size > 1`：从 nbest_size 结果中抽样。

        +   `nbest_size < 0`: 假设 nbest_size 是无限的，并使用前向过滤和后向抽样算法从所有假设（格子）中抽样。

    +   `alpha`：unigram 抽样的平滑参数，以及 BPE-dropout 合并操作的丢失概率。

+   `sp_model`（`SentencePieceProcessor`）- 用于每次转换（字符串、标记和 ID）的*SentencePiece*处理器。

改编自 XLMRobertaTokenizer。基于[SentencePiece](https://github.com/google/sentencepiece)。

此标记器继承自 PreTrainedTokenizer，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L191)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）- 将添加特殊标记的 ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）- 序列对的可选第二个 ID 列表。

返回

`List[int]`

具有适当特殊标记的 input IDs 列表。

通过连接和添加特殊标记构建用于序列分类任务的序列或序列对的模型输入。BARTPho 序列的格式如下：

+   单个序列：`<s> X </s>`

+   序列对：`<s> A </s></s> B </s>`

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L293)

```py
( tokens )
```

将一系列标记（子词的字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L245)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）- ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）- 序列对的可选第二个 ID 列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。BARTPho 不使用标记类型 ID，因此返回一个零的列表。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bartpho/tokenization_bartpho.py#L217)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）- ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）- 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`）- 标记列表是否已经格式化为模型的特殊标记。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 ID。在使用标记器`prepare_for_model`方法添加特殊标记时调用此方法。
