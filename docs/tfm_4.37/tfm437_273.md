# Mask2Former

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/mask2former`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mask2former)

## æ¦‚è§ˆ

Mask2Former æ¨¡å‹æ˜¯ç”± Bowen Chengã€Ishan Misraã€Alexander G. Schwingã€Alexander Kirillovã€Rohit Girdhar åœ¨[Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527)ä¸­æå‡ºçš„ã€‚Mask2Former æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å…¨æ™¯ã€å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œç›¸æ¯”äº MaskFormer å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½å’Œæ•ˆç‡æ”¹è¿›ã€‚

æ¥è‡ªè®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*å›¾åƒåˆ†å‰²å°†å…·æœ‰ä¸åŒè¯­ä¹‰çš„åƒç´ åˆ†ç»„ï¼Œä¾‹å¦‚ç±»åˆ«æˆ–å®ä¾‹æˆå‘˜èµ„æ ¼ã€‚æ¯ç§è¯­ä¹‰é€‰æ‹©å®šä¹‰äº†ä¸€ä¸ªä»»åŠ¡ã€‚è™½ç„¶æ¯ä¸ªä»»åŠ¡çš„è¯­ä¹‰ä¸åŒï¼Œä½†å½“å‰çš„ç ”ç©¶é‡ç‚¹æ˜¯ä¸ºæ¯ä¸ªä»»åŠ¡è®¾è®¡ä¸“é—¨çš„æ¶æ„ã€‚æˆ‘ä»¬æå‡ºäº† Masked-attention Mask Transformerï¼ˆMask2Formerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†ä»»ä½•å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼ˆå…¨æ™¯ã€å®ä¾‹æˆ–è¯­ä¹‰ï¼‰ã€‚å…¶å…³é”®ç»„ä»¶åŒ…æ‹¬æ©ç æ³¨æ„åŠ›ï¼Œé€šè¿‡é™åˆ¶åœ¨é¢„æµ‹æ©ç åŒºåŸŸå†…çš„äº¤å‰æ³¨æ„åŠ›æ¥æå–å±€éƒ¨ç‰¹å¾ã€‚é™¤äº†å°†ç ”ç©¶å·¥ä½œé‡è‡³å°‘å‡å°‘ä¸‰å€å¤–ï¼Œå®ƒåœ¨å››ä¸ªæµè¡Œæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜æ˜¾ä¼˜äºæœ€ä½³ä¸“é—¨æ¶æ„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMask2Former åœ¨å…¨æ™¯åˆ†å‰²ï¼ˆCOCO ä¸Šçš„ 57.8 PQï¼‰ã€å®ä¾‹åˆ†å‰²ï¼ˆCOCO ä¸Šçš„ 50.1 APï¼‰å’Œè¯­ä¹‰åˆ†å‰²ï¼ˆADE20K ä¸Šçš„ 57.7 mIoUï¼‰æ–¹é¢å–å¾—äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚*

![å›¾ç¤º](img/5ce83994aad79cfa0f1a7945fa6a424d.png) Mask2Former æ¶æ„ã€‚å–è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2112.01527)ã€‚

è¿™ä¸ªæ¨¡å‹ç”±[Shivalika Singh](https://huggingface.co/shivi)å’Œ[Alara Dirik](https://huggingface.co/adirik)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/Mask2Former)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   Mask2Former ä½¿ç”¨ä¸ MaskFormer ç›¸åŒçš„é¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ã€‚ä½¿ç”¨ Mask2FormerImageProcessor æˆ– AutoImageProcessor æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒå’Œå¯é€‰ç›®æ ‡ã€‚

+   è¦è·å¾—æœ€ç»ˆçš„åˆ†å‰²ç»“æœï¼Œå–å†³äºä»»åŠ¡ï¼Œæ‚¨å¯ä»¥è°ƒç”¨ post_process_semantic_segmentation()æˆ– post_process_instance_segmentation()æˆ– post_process_panoptic_segmentation()ã€‚æ‰€æœ‰è¿™ä¸‰ä¸ªä»»åŠ¡éƒ½å¯ä»¥ä½¿ç”¨ Mask2FormerForUniversalSegmentation çš„è¾“å‡ºæ¥è§£å†³ï¼Œå…¨æ™¯åˆ†å‰²æ¥å—ä¸€ä¸ªå¯é€‰çš„`label_ids_to_fuse`å‚æ•°ï¼Œä»¥å°†ç›®æ ‡å¯¹è±¡ï¼ˆä¾‹å¦‚å¤©ç©ºï¼‰çš„å®ä¾‹åˆå¹¶åœ¨ä¸€èµ·ã€‚

## èµ„æº

ä¸€ç³»åˆ—å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ Mask2Formerã€‚

+   å…³äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œæ¨ç†+å¾®è°ƒ Mask2Former çš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Mask2Former)æ‰¾åˆ°ã€‚

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨è¿™é‡Œï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ã€‚èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## Mask2FormerConfig

### `class transformers.Mask2FormerConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/configuration_mask2former.py#L33)

```py
( backbone_config: Optional = None feature_size: int = 256 mask_feature_size: int = 256 hidden_dim: int = 256 encoder_feedforward_dim: int = 1024 activation_function: str = 'relu' encoder_layers: int = 6 decoder_layers: int = 10 num_attention_heads: int = 8 dropout: float = 0.0 dim_feedforward: int = 2048 pre_norm: bool = False enforce_input_projection: bool = False common_stride: int = 4 ignore_value: int = 255 num_queries: int = 100 no_object_weight: float = 0.1 class_weight: float = 2.0 mask_weight: float = 5.0 dice_weight: float = 5.0 train_num_points: int = 12544 oversample_ratio: float = 3.0 importance_sample_ratio: float = 0.75 init_std: float = 0.02 init_xavier_std: float = 1.0 use_auxiliary_loss: bool = True feature_strides: List = [4, 8, 16, 32] output_auxiliary_logits: bool = None **kwargs )
```

å‚æ•°

+   `backbone_config` (`PretrainedConfig` or `dict`, *optional*, defaults to `SwinConfig()`) â€” ä¸»å¹²æ¨¡å‹çš„é…ç½®ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ä¸`swin-base-patch4-window12-384`å¯¹åº”çš„é…ç½®ã€‚

+   `feature_size` (`int`, *optional*, defaults to 256) â€” ç»“æœç‰¹å¾å›¾çš„ç‰¹å¾ï¼ˆé€šé“ï¼‰ã€‚

+   `mask_feature_size` (`int`, *optional*, defaults to 256) â€” æ©ç çš„ç‰¹å¾å¤§å°ï¼Œæ­¤å€¼è¿˜å°†ç”¨äºæŒ‡å®šç‰¹å¾é‡‘å­—å¡”ç½‘ç»œç‰¹å¾çš„å¤§å°ã€‚

+   `hidden_dim` (`int`, *optional*, defaults to 256) â€” ç¼–ç å™¨å±‚çš„ç»´åº¦ã€‚

+   `encoder_feedforward_dim` (`int`, *optional*, defaults to 1024) â€” ç”¨ä½œåƒç´ è§£ç å™¨ä¸€éƒ¨åˆ†çš„å¯å˜å½¢ detr ç¼–ç å™¨çš„å‰é¦ˆç½‘ç»œç»´åº¦ã€‚

+   `encoder_layers` (`int`, *optional*, defaults to 6) â€” ç”¨ä½œåƒç´ è§£ç å™¨ä¸€éƒ¨åˆ†çš„å¯å˜å½¢ detr ç¼–ç å™¨ä¸­çš„å±‚æ•°ã€‚

+   `decoder_layers` (`int`, *optional*, defaults to 10) â€” å˜å‹å™¨è§£ç å™¨ä¸­çš„å±‚æ•°ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 8) â€” æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚

+   `dim_feedforward` (`int`, *optional*, defaults to 2048) â€” å˜å‹å™¨è§£ç å™¨ä¸­å‰é¦ˆç½‘ç»œçš„ç‰¹å¾ç»´åº¦ã€‚

+   `pre_norm` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨å˜å‹å™¨è§£ç å™¨ä¸­ä½¿ç”¨é¢„ LayerNormã€‚

+   `enforce_input_projection` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨ Transformer è§£ç å™¨ä¸­æ·»åŠ ä¸€ä¸ªè¾“å…¥æŠ•å½± 1x1 å·ç§¯ï¼Œå³ä½¿è¾“å…¥é€šé“å’Œéšè—ç»´åº¦ç›¸åŒã€‚

+   `common_stride` (`int`, *optional*, defaults to 4) â€” ç”¨äºç¡®å®šä½œä¸ºåƒç´ è§£ç å™¨ä¸€éƒ¨åˆ†ä½¿ç”¨çš„ FPN çº§åˆ«æ•°çš„å‚æ•°ã€‚

+   `ignore_value` (`int`, *optional*, defaults to 255) â€” è®­ç»ƒè¿‡ç¨‹ä¸­è¦å¿½ç•¥çš„ç±»åˆ« IDã€‚

+   `num_queries` (`int`, *optional*, defaults to 100) â€” è§£ç å™¨çš„æŸ¥è¯¢æ¬¡æ•°ã€‚

+   `no_object_weight` (`int`, *optional*, defaults to 0.1) â€” ç”¨äºç©ºï¼ˆæ— å¯¹è±¡ï¼‰ç±»çš„æƒé‡ã€‚

+   `class_weight` (`int`, *optional*, defaults to 2.0) â€” äº¤å‰ç†µæŸå¤±çš„æƒé‡ã€‚

+   `mask_weight` (`int`, *optional*, defaults to 5.0) â€” æ©ç æŸå¤±çš„æƒé‡ã€‚

+   `dice_weight` (`int`, *optional*, defaults to 5.0) â€” dice æŸå¤±çš„æƒé‡ã€‚

+   `train_num_points` (`str` or `function`, *optional*, defaults to 12544) â€” åœ¨æŸå¤±è®¡ç®—è¿‡ç¨‹ä¸­ç”¨äºé‡‡æ ·çš„ç‚¹æ•°ã€‚

+   `oversample_ratio` (`float`, *optional*, defaults to 3.0) â€” ç”¨äºè®¡ç®—é‡‡æ ·ç‚¹æ•°çš„è¿‡é‡‡æ ·å‚æ•°ã€‚

+   `importance_sample_ratio` (`float`, *optional*, defaults to 0.75) â€” é€šè¿‡é‡è¦æ€§é‡‡æ ·æŠ½æ ·çš„ç‚¹æ¯”ä¾‹ã€‚

+   `init_std` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `init_xavier_std` (`float`, *optional*, defaults to 1.0) â€” ç”¨äº HM æ³¨æ„åŠ›å›¾æ¨¡å—ä¸­ Xavier åˆå§‹åŒ–å¢ç›Šçš„ç¼©æ”¾å› å­ã€‚

+   `use_auxiliary_loss` (`boolean``, *optional*, defaults to` True`) -- å¦‚æœ`True`ï¼Œ`Mask2FormerForUniversalSegmentationOutput`å°†åŒ…å«ä½¿ç”¨æ¯ä¸ªè§£ç å™¨é˜¶æ®µçš„ logits è®¡ç®—çš„è¾…åŠ©æŸå¤±ã€‚

+   `feature_strides` (`List[int]`, *optional*, defaults to `[4, 8, 16, 32]`) â€” ä¸ä¸»å¹²ç½‘ç»œç”Ÿæˆçš„ç‰¹å¾å¯¹åº”çš„ç‰¹å¾æ­¥å¹…ã€‚

+   `output_auxiliary_logits` (`bool`, *optional*) â€” æ¨¡å‹æ˜¯å¦è¾“å‡ºå…¶`auxiliary_logits`ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ Mask2FormerModel çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª Mask2Former æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Mask2Former [facebook/mask2former-swin-small-coco-instance](https://huggingface.co/facebook/mask2former-swin-small-coco-instance)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç›®å‰ï¼ŒMask2Former ä»…æ”¯æŒ Swin Transformer ä½œä¸ºéª¨å¹²ç½‘ç»œã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import Mask2FormerConfig, Mask2FormerModel

>>> # Initializing a Mask2Former facebook/mask2former-swin-small-coco-instance configuration
>>> configuration = Mask2FormerConfig()

>>> # Initializing a model (with random weights) from the facebook/mask2former-swin-small-coco-instance style configuration
>>> model = Mask2FormerModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

#### `from_backbone_config`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/configuration_mask2former.py#L218)

```py
( backbone_config: PretrainedConfig **kwargs ) â†’ export const metadata = 'undefined';Mask2FormerConfig
```

å‚æ•°

+   `backbone_config`ï¼ˆPretrainedConfigï¼‰ â€” éª¨å¹²é…ç½®ã€‚

è¿”å›

Mask2FormerConfig

ä¸€ä¸ªé…ç½®å¯¹è±¡çš„å®ä¾‹

ä»é¢„è®­ç»ƒçš„éª¨å¹²æ¨¡å‹é…ç½®å®ä¾‹åŒ–ä¸€ä¸ª Mask2FormerConfigï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚

## MaskFormer ç‰¹å®šè¾“å‡º

### `class transformers.models.mask2former.modeling_mask2former.Mask2FormerModelOutput`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/modeling_mask2former.py#L145)

```py
( encoder_last_hidden_state: FloatTensor = None pixel_decoder_last_hidden_state: FloatTensor = None transformer_decoder_last_hidden_state: FloatTensor = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_intermediate_states: Tuple = None masks_queries_logits: Tuple = None attentions: Optional = None )
```

å‚æ•°

+   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼Œ*optional*) â€” ç¼–ç å™¨æ¨¡å‹ï¼ˆéª¨å¹²ï¼‰æœ€åé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*) â€” ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰çš„å…ƒç»„ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `pixel_decoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼Œ*optional*) â€” åƒç´ è§£ç å™¨æ¨¡å‹æœ€åé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰çš„å…ƒç»„ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `transformer_decoder_last_hidden_state` (`tuple(torch.FloatTensor)`) â€” å˜å‹å™¨è§£ç å™¨çš„æœ€ç»ˆè¾“å‡º`(batch_size, sequence_length, hidden_size)`ã€‚

+   `transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*) â€” å˜å‹å™¨è§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰çš„å…ƒç»„ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `transformer_decoder_intermediate_states` (`tuple(torch.FloatTensor)`ï¼Œå½¢çŠ¶ä¸º`(num_queries, 1, hidden_size)`) â€” ä¸­é—´è§£ç å™¨æ¿€æ´»ï¼Œå³æ¯ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºï¼Œæ¯ä¸ªéƒ½ç»è¿‡äº† layernormã€‚

+   `masks_queries_logits` (`tuple(torch.FloatTensor)`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`) â€” transformer è§£ç å™¨ä¸­æ¯å±‚çš„æ©ç é¢„æµ‹ã€‚

+   `attentions` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„å…ƒç»„çš„å…ƒç»„ã€‚transformer è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›æƒé‡ã€‚

ç”¨äº Mask2FormerModel çš„è¾“å‡ºç±»ã€‚è¯¥ç±»è¿”å›è®¡ç®— logits æ‰€éœ€çš„æ‰€æœ‰éšè—çŠ¶æ€ã€‚

### `class transformers.models.mask2former.modeling_mask2former.Mask2FormerForUniversalSegmentationOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/modeling_mask2former.py#L191)

```py
( loss: Optional = None class_queries_logits: FloatTensor = None masks_queries_logits: FloatTensor = None auxiliary_logits: Optional = None encoder_last_hidden_state: FloatTensor = None pixel_decoder_last_hidden_state: FloatTensor = None transformer_decoder_last_hidden_state: FloatTensor = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None attentions: Optional = None )
```

å‚æ•°

+   `loss` (`torch.Tensor`ï¼Œ*å¯é€‰*) â€” è®¡ç®—å¾—åˆ°çš„æŸå¤±ï¼Œåœ¨å­˜åœ¨æ ‡ç­¾æ—¶è¿”å›ã€‚

+   `class_queries_logits` (`torch.FloatTensor`) â€” ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_queries, num_labels + 1)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®ç±»åˆ«ã€‚è¯·æ³¨æ„ï¼Œ`+ 1`æ˜¯å› ä¸ºæˆ‘ä»¬åŒ…å«äº†ç©ºç±»åˆ«ã€‚

+   `masks_queries_logits` (`torch.FloatTensor`) â€” ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®æ©ç ã€‚

+   `auxiliary_logits` (`List[Dict(str, torch.FloatTensor)]`ï¼Œ*å¯é€‰*) â€” transformer è§£ç å™¨æ¯å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” ç¼–ç å™¨æ¨¡å‹ï¼ˆéª¨å¹²ï¼‰æœ€åé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„å…ƒç»„çš„`torch.FloatTensor`ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ è§£ç å™¨æ¨¡å‹æœ€åé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„å…ƒç»„çš„`torch.FloatTensor`ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_last_hidden_state` (`tuple(torch.FloatTensor)`) â€” transformer è§£ç å™¨çš„æœ€ç»ˆè¾“å‡º`(batch_size, sequence_length, hidden_size)`ã€‚

+   `transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„å…ƒç»„çš„`torch.FloatTensor`ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚transformer è§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `attentions` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„å…ƒç»„çš„å…ƒç»„ã€‚æ¥è‡ª transformer è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚

`Mask2FormerForUniversalSegmentationOutput`çš„è¾“å‡ºç±»ã€‚

è¿™ä¸ªè¾“å‡ºå¯ä»¥ç›´æ¥ä¼ é€’ç»™ post_process_semantic_segmentation()æˆ– post_process_instance_segmentation()æˆ– post_process_panoptic_segmentation()æ¥è®¡ç®—æœ€ç»ˆçš„åˆ†å‰²åœ°å›¾ã€‚è¯·å‚é˜…[`~Mask2FormerImageProcessor]ä»¥è·å–æœ‰å…³ç”¨æ³•çš„è¯¦ç»†ä¿¡æ¯ã€‚

## Mask2FormerModel

### `class transformers.Mask2FormerModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/modeling_mask2former.py#L2189)

```py
( config: Mask2FormerConfig )
```

å‚æ•°

+   `config`ï¼ˆMask2FormerConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ Mask2Former æ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/modeling_mask2former.py#L2203)

```py
( pixel_values: Tensor pixel_mask: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.mask2former.modeling_mask2former.Mask2FormerModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`AutoImageProcessor.preprocess`ã€‚

+   `pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºçœŸå®åƒç´ ï¼ˆå³`æœªæ©ç `ï¼‰ï¼Œ

    +   0 è¡¨ç¤ºå¡«å……åƒç´ ï¼ˆå³`æ©ç `ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› Detr è§£ç å™¨æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›`~Mask2FormerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.models.mask2former.modeling_mask2former.Mask2FormerModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.models.mask2former.modeling_mask2former.Mask2FormerModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆMask2FormerConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç¼–ç å™¨æ¨¡å‹ï¼ˆéª¨å¹²ï¼‰æœ€åé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `pixel_decoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” åƒç´ è§£ç å™¨æ¨¡å‹æœ€åé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `transformer_decoder_last_hidden_state`ï¼ˆ`tuple(torch.FloatTensor)`ï¼‰â€” å˜å‹å™¨è§£ç å™¨çš„æœ€ç»ˆè¾“å‡º`(batch_size, sequence_length, hidden_size)`ã€‚

+   `transformer_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚å˜å‹å™¨è§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚å½“ä¼ é€’`output_hidden_states=True`æ—¶è¿”å›ã€‚

+   `transformer_decoder_intermediate_states`ï¼ˆå½¢çŠ¶ä¸º`(num_queries, 1, hidden_size)`çš„`tuple(torch.FloatTensor)`) â€” ä¸­é—´è§£ç å™¨æ¿€æ´»ï¼Œå³æ¯ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºï¼Œæ¯ä¸ªéƒ½ç»è¿‡äº†ä¸€ä¸ª layernormã€‚

+   `masks_queries_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`çš„`tuple(torch.FloatTensor)`) â€” å˜å‹å™¨è§£ç å™¨ä¸­æ¯ä¸ªå±‚çš„æ©ç é¢„æµ‹ã€‚

+   `attentions`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚æ¥è‡ªå˜å‹å™¨è§£ç å™¨çš„è‡ªæ³¨æ„æƒé‡ã€‚

`Mask2FormerModelOutput`

Mask2FormerModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoImageProcessor, Mask2FormerModel

>>> # load image
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # load image preprocessor and Mask2FormerModel trained on COCO instance segmentation dataset
>>> image_processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-small-coco-instance")
>>> model = Mask2FormerModel.from_pretrained("facebook/mask2former-swin-small-coco-instance")
>>> inputs = image_processor(image, return_tensors="pt")

>>> # forward pass
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # model outputs last hidden states of shape (batch_size, num_queries, hidden_size)
>>> print(outputs.transformer_decoder_last_hidden_state.shape)
torch.Size([1, 100, 256])
```

## Mask2FormerForUniversalSegmentation

### `class transformers.Mask2FormerForUniversalSegmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/modeling_mask2former.py#L2293)

```py
( config: Mask2FormerConfig )
```

å‚æ•°

+   `config`ï¼ˆMask2FormerConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Mask2Former æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰ç”¨äºå®ä¾‹/è¯­ä¹‰/å…¨æ™¯åˆ†å‰²çš„å¤´ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch çš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/modeling_mask2former.py#L2350)

```py
( pixel_values: Tensor mask_labels: Optional = None class_labels: Optional = None pixel_mask: Optional = None output_hidden_states: Optional = None output_auxiliary_logits: Optional = None output_attentions: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.mask2former.modeling_mask2former.Mask2FormerForUniversalSegmentationOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ AutoImageProcessor è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`AutoImageProcessor.preprocess`ã€‚

+   `pixel_mask` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, height, width)`ï¼Œ*optional*) â€” é¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºçœŸå®åƒç´ ä¸º 1ï¼ˆå³`not masked`ï¼‰ï¼Œ

    +   å¯¹äºå¡«å……åƒç´ ä¸º 0ï¼ˆå³`masked`ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› Detr è§£ç å™¨æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›`~Mask2FormerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `mask_labels` (`List[torch.Tensor]`, *optional*) â€” å½¢çŠ¶ä¸º`(num_labels, height, width)`çš„æ©ç æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºé¦ˆé€åˆ°æ¨¡å‹ã€‚

+   `class_labels` (`List[torch.LongTensor]`, *optional*) â€” å½¢çŠ¶ä¸º`(num_labels, height, width)`çš„ç›®æ ‡ç±»åˆ«æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºé¦ˆé€åˆ°æ¨¡å‹ã€‚å®ƒä»¬æ ‡è¯†`mask_labels`çš„æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ`class_labels[i][j]`çš„æ ‡ç­¾æ˜¯`mask_labels[i][j]`ã€‚ 

è¿”å›

transformers.models.mask2former.modeling_mask2former.Mask2FormerForUniversalSegmentationOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.models.mask2former.modeling_mask2former.Mask2FormerForUniversalSegmentationOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½®ï¼ˆMask2FormerConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.Tensor`, *optional*) â€” è®¡ç®—çš„æŸå¤±ï¼Œåœ¨å­˜åœ¨æ ‡ç­¾æ—¶è¿”å›ã€‚

+   `class_queries_logits` (`torch.FloatTensor`) â€” å½¢çŠ¶ä¸º`(batch_size, num_queries, num_labels + 1)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®ç±»åˆ«ã€‚è¯·æ³¨æ„ï¼Œ`+ 1`æ˜¯å› ä¸ºæˆ‘ä»¬åŒ…å«äº†ç©ºç±»ã€‚

+   `masks_queries_logits` (`torch.FloatTensor`) â€” å½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®æ©ç ã€‚

+   `auxiliary_logits` (`List[Dict(str, torch.FloatTensor)]`, *optional*) â€” æ¥è‡ªå˜å‹å™¨è§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” ç¼–ç å™¨æ¨¡å‹ï¼ˆéª¨å¹²ï¼‰æœ€åä¸€ä¸ªé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ è§£ç å™¨æ¨¡å‹æœ€åä¸€ä¸ªé˜¶æ®µçš„æœ€åéšè—çŠ¶æ€ï¼ˆæœ€ç»ˆç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_last_hidden_state` (`tuple(torch.FloatTensor)`) â€” å˜æ¢å™¨è§£ç å™¨çš„æœ€ç»ˆè¾“å‡º `(batch_size, sequence_length, hidden_size)`ã€‚

+   `transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚å˜æ¢å™¨è§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `attentions` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tuple(torch.FloatTensor)` å…ƒç»„ã€‚å˜æ¢å™¨è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚

`Mask2FormerUniversalSegmentationOutput`

Mask2FormerForUniversalSegmentation çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›– `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

å®ä¾‹åˆ†å‰²ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
>>> from PIL import Image
>>> import requests
>>> import torch

>>> # Load Mask2Former trained on COCO instance segmentation dataset
>>> image_processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-small-coco-instance")
>>> model = Mask2FormerForUniversalSegmentation.from_pretrained(
...     "facebook/mask2former-swin-small-coco-instance"
... )

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # Perform post-processing to get instance segmentation map
>>> pred_instance_map = image_processor.post_process_semantic_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]
>>> print(pred_instance_map.shape)
torch.Size([480, 640])
```

è¯­ä¹‰åˆ†å‰²ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
>>> from PIL import Image
>>> import requests
>>> import torch

>>> # Load Mask2Former trained on ADE20k semantic segmentation dataset
>>> image_processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-small-ade-semantic")
>>> model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-small-ade-semantic")

>>> url = (
...     "https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg"
... )
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # Perform post-processing to get semantic segmentation map
>>> pred_semantic_map = image_processor.post_process_semantic_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]
>>> print(pred_semantic_map.shape)
torch.Size([512, 683])
```

å…¨æ™¯åˆ†å‰²ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
>>> from PIL import Image
>>> import requests
>>> import torch

>>> # Load Mask2Former trained on CityScapes panoptic segmentation dataset
>>> image_processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-small-cityscapes-panoptic")
>>> model = Mask2FormerForUniversalSegmentation.from_pretrained(
...     "facebook/mask2former-swin-small-cityscapes-panoptic"
... )

>>> url = "https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # Perform post-processing to get panoptic segmentation map
>>> pred_panoptic_map = image_processor.post_process_panoptic_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]["segmentation"]
>>> print(pred_panoptic_map.shape)
torch.Size([338, 676])
```

## Mask2FormerImageProcessor

### `class transformers.Mask2FormerImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/image_processing_mask2former.py#L345)

```py
( do_resize: bool = True size: Dict = None size_divisor: int = 32 resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: float = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None ignore_index: Optional = None reduce_labels: bool = False **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†è¾“å…¥è°ƒæ•´å¤§å°åˆ°ç‰¹å®šçš„ `size`ã€‚

+   `size` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 800) â€” è°ƒæ•´è¾“å…¥å¤§å°ä¸ºç»™å®šå¤§å°ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚å¦‚æœ size æ˜¯ä¸€ä¸ªç±»ä¼¼ `(width, height)` çš„åºåˆ—ï¼Œè¾“å‡ºå¤§å°å°†åŒ¹é…åˆ°è¿™ä¸ªå¤§å°ã€‚å¦‚æœ size æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå›¾åƒçš„è¾ƒå°è¾¹å°†åŒ¹é…åˆ°è¿™ä¸ªæ•°å­—ã€‚å³ï¼Œå¦‚æœ `height > width`ï¼Œé‚£ä¹ˆå›¾åƒå°†è¢«é‡æ–°ç¼©æ”¾ä¸º `(size * height / width, size)`ã€‚

+   `size_divisor` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 32) â€” ä¸€äº›ä¸»å¹²ç½‘ç»œéœ€è¦å¯è¢«æŸä¸ªæ•°å­—æ•´é™¤çš„å›¾åƒã€‚å¦‚æœæœªä¼ é€’ï¼Œåˆ™é»˜è®¤ä¸º Swin Transformer ä¸­ä½¿ç”¨çš„å€¼ã€‚

+   `resample` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `Resampling.BILINEAR`) â€” å¯é€‰çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥æ˜¯ `PIL.Image.Resampling.NEAREST`ã€`PIL.Image.Resampling.BOX`ã€`PIL.Image.Resampling.BILINEAR`ã€`PIL.Image.Resampling.HAMMING`ã€`PIL.Image.Resampling.BICUBIC` æˆ– `PIL.Image.Resampling.LANCZOS` ä¸­çš„ä¸€ä¸ªã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_rescale` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†è¾“å…¥é‡æ–°ç¼©æ”¾åˆ°ç‰¹å®šçš„ `scale`ã€‚

+   `rescale_factor` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `1/255`) â€” é€šè¿‡ç»™å®šå› å­é‡æ–°ç¼©æ”¾è¾“å…¥ã€‚ä»…åœ¨ `do_rescale` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_normalize` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œå‡å€¼å’Œæ ‡å‡†å·®å½’ä¸€åŒ–ã€‚

+   `image_mean` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `[0.485, 0.456, 0.406]`) â€” æ¯ä¸ªé€šé“çš„å‡å€¼åºåˆ—ï¼Œç”¨äºå½’ä¸€åŒ–å›¾åƒã€‚é»˜è®¤ä¸º ImageNet å‡å€¼ã€‚

+   `image_std` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `[0.229, 0.224, 0.225]`) â€” æ¯ä¸ªé€šé“çš„æ ‡å‡†å·®åºåˆ—ï¼Œç”¨äºå½’ä¸€åŒ–å›¾åƒã€‚é»˜è®¤ä¸º ImageNet æ ‡å‡†å·®ã€‚

+   `ignore_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨åˆ†å‰²å›¾ä¸­ä¸ºèƒŒæ™¯åƒç´ åˆ†é…çš„æ ‡ç­¾ã€‚å¦‚æœæä¾›ï¼Œç”¨ 0ï¼ˆèƒŒæ™¯ï¼‰è¡¨ç¤ºçš„åˆ†å‰²å›¾åƒç´ å°†è¢«æ›¿æ¢ä¸º`ignore_index`ã€‚

+   `reduce_labels`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å°†æ‰€æœ‰åˆ†å‰²å›¾çš„æ ‡ç­¾å€¼å‡ 1ã€‚é€šå¸¸ç”¨äºæ•°æ®é›†ï¼Œå…¶ä¸­ 0 ç”¨äºèƒŒæ™¯ï¼Œå¹¶ä¸”èƒŒæ™¯æœ¬èº«ä¸åŒ…å«åœ¨æ•°æ®é›†çš„æ‰€æœ‰ç±»åˆ«ä¸­ï¼ˆä¾‹å¦‚ ADE20kï¼‰ã€‚èƒŒæ™¯æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸º`ignore_index`ã€‚

æ„å»ºä¸€ä¸ª Mask2Former å›¾åƒå¤„ç†å™¨ã€‚è¯¥å›¾åƒå¤„ç†å™¨å¯ç”¨äºä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒå’Œå¯é€‰ç›®æ ‡ã€‚

æ­¤å›¾åƒå¤„ç†å™¨ç»§æ‰¿è‡ª`BaseImageProcessor`ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `é¢„å¤„ç†`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/image_processing_mask2former.py#L670)

```py
( images: Union segmentation_maps: Union = None instance_id_to_semantic_id: Optional = None do_resize: Optional = None size: Optional = None size_divisor: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None ignore_index: Optional = None reduce_labels: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

#### `ç¼–ç è¾“å…¥`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/image_processing_mask2former.py#L858)

```py
( pixel_values_list: List segmentation_maps: Union = None instance_id_to_semantic_id: Union = None ignore_index: Optional = None reduce_labels: bool = False return_tensors: Union = None input_data_format: Union = None ) â†’ export const metadata = 'undefined';BatchFeature
```

å‚æ•°

+   `pixel_values_list`ï¼ˆ`List[ImageInput]`ï¼‰â€” è¦å¡«å……çš„å›¾åƒï¼ˆåƒç´ å€¼ï¼‰åˆ—è¡¨ã€‚æ¯ä¸ªå›¾åƒåº”è¯¥æ˜¯å½¢çŠ¶ä¸º`(channels, height, width)`çš„å¼ é‡ã€‚

+   `segmentation_maps`ï¼ˆ`ImageInput`ï¼Œ*å¯é€‰*ï¼‰â€” å…·æœ‰åƒç´ çº§æ³¨é‡Šçš„ç›¸åº”è¯­ä¹‰åˆ†å‰²å›¾ã€‚

    ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰ï¼šæ˜¯å¦å°†å›¾åƒå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€å¤§çš„å›¾åƒï¼Œå¹¶åˆ›å»ºåƒç´ æ©ç ã€‚

    å¦‚æœä¿ç•™é»˜è®¤è®¾ç½®ï¼Œå°†è¿”å›ä»¥ä¸‹åƒç´ æ©ç ï¼š

    +   1 è¡¨ç¤ºçœŸå®åƒç´ ï¼ˆå³`æœªæ©ç `ï¼‰ï¼Œ

    +   0 è¡¨ç¤ºå¡«å……åƒç´ ï¼ˆå³`masked`ï¼‰ã€‚

+   `instance_id_to_semantic_id`ï¼ˆ`List[Dict[int, int]]`æˆ–`Dict[int, int]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯¹è±¡å®ä¾‹ id å’Œç±»åˆ« id ä¹‹é—´çš„æ˜ å°„ã€‚å¦‚æœä¼ é€’ï¼Œ`segmentation_maps`å°†è¢«è§†ä¸ºå®ä¾‹åˆ†å‰²å›¾ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¡¨ç¤ºä¸€ä¸ªå®ä¾‹ idã€‚å¯ä»¥ä½œä¸ºä¸€ä¸ªå…¨å±€/æ•°æ®é›†çº§åˆ«æ˜ å°„çš„å•ä¸ªå­—å…¸æä¾›ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå­—å…¸åˆ—è¡¨ï¼ˆæ¯ä¸ªå›¾åƒä¸€ä¸ªï¼‰ï¼Œä»¥åˆ†åˆ«æ˜ å°„æ¯ä¸ªå›¾åƒä¸­çš„å®ä¾‹ idã€‚

+   `return_tensors`ï¼ˆ`str`æˆ– TensorTypeï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ NumPy æ•°ç»„ã€‚å¦‚æœè®¾ç½®ä¸º`'pt'`ï¼Œåˆ™è¿”å› PyTorch `torch.Tensor`å¯¹è±¡ã€‚

+   `input_data_format`ï¼ˆ`ChannelDimension`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œå°†è¢«æ¨æ–­ã€‚

è¿”å›

BatchFeature

ä¸€ä¸ª BatchFeatureï¼Œå…·æœ‰ä»¥ä¸‹å­—æ®µï¼š

+   `pixel_values` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„åƒç´ å€¼ã€‚

+   `pixel_mask` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„åƒç´ æ©ç ï¼ˆå½“`=True`æˆ–`pixel_mask`åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

+   `mask_labels` â€” å½¢çŠ¶ä¸º`(labels, height, width)`çš„å¯é€‰æ©ç æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºé¦ˆé€ç»™æ¨¡å‹ï¼ˆå½“æä¾›`annotations`æ—¶ï¼‰ã€‚

+   `class_labels` â€” å½¢çŠ¶ä¸º`(labels)`çš„å¯é€‰ç±»åˆ«æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºé¦ˆé€ç»™æ¨¡å‹ï¼ˆå½“æä¾›`annotations`æ—¶ï¼‰ã€‚å®ƒä»¬æ ‡è¯†`mask_labels`çš„æ ‡ç­¾ï¼Œä¾‹å¦‚å¦‚æœ`class_labels[i][j]`çš„æ ‡ç­¾æ˜¯`mask_labels[i][j]`ã€‚

å°†å›¾åƒå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€å¤§çš„å›¾åƒï¼Œå¹¶åˆ›å»ºç›¸åº”çš„`pixel_mask`ã€‚

Mask2Former ä½¿ç”¨æ©ç åˆ†ç±»èŒƒå¼å¤„ç†è¯­ä¹‰åˆ†å‰²ï¼Œå› æ­¤è¾“å…¥åˆ†å‰²å›¾å°†è¢«è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ©ç åˆ—è¡¨åŠå…¶ç›¸åº”çš„æ ‡ç­¾ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå‡è®¾`segmentation_maps = [[2,6,7,9]]`ï¼Œè¾“å‡ºå°†åŒ…å«`mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`ï¼ˆå››ä¸ªäºŒè¿›åˆ¶æ©ç ï¼‰å’Œ`class_labels = [2,6,7,9]`ï¼Œæ¯ä¸ªæ©ç çš„æ ‡ç­¾ã€‚

#### `åå¤„ç†è¯­ä¹‰åˆ†å‰²`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/image_processing_mask2former.py#L961)

```py
( outputs target_sizes: Optional = None ) â†’ export const metadata = 'undefined';List[torch.Tensor]
```

å‚æ•°

+   `outputs` (Mask2FormerForUniversalSegmentation) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes` (`List[Tuple[int, int]]`, *å¯é€‰*) â€” é•¿åº¦ä¸º(batch_size)çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹(`Tuple[int, int]]`)å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°(é«˜åº¦ï¼Œå®½åº¦)ã€‚å¦‚æœè®¾ç½®ä¸º Noneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚

è¿”å›å€¼

`List[torch.Tensor]`

ä¸€ä¸ªé•¿åº¦ä¸º`batch_size`çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹æ˜¯å½¢çŠ¶ä¸º(é«˜åº¦ï¼Œå®½åº¦)çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº target_sizes æ¡ç›®(å¦‚æœæŒ‡å®šäº†`target_sizes`)ã€‚æ¯ä¸ª`torch.Tensor`çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚

å°† Mask2FormerForUniversalSegmentation çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚

#### `post_process_instance_segmentation`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/image_processing_mask2former.py#L1016)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False return_binary_maps: Optional = False ) â†’ export const metadata = 'undefined';List[Dict]
```

å‚æ•°

+   `outputs` (Mask2FormerForUniversalSegmentation) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©ç çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚

+   `mask_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” åœ¨å°†é¢„æµ‹çš„æ©ç è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚

+   `overlap_mask_area_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.8) â€” åˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©ç ä¸­çš„å°ä¸è¿ç»­éƒ¨åˆ†çš„é‡å æ©ç åŒºåŸŸé˜ˆå€¼ã€‚

+   `target_sizes` (`List[Tuple]`, *å¯é€‰*) â€” é•¿åº¦ä¸º(batch_size)çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹(`Tuple[int, int]]`)å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°(é«˜åº¦ï¼Œå®½åº¦)ã€‚å¦‚æœè®¾ç½®ä¸º Noneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚

+   `return_coco_annotation` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™ä»¥ COCO è¿è¡Œé•¿åº¦ç¼–ç (RLE)æ ¼å¼è¿”å›åˆ†å‰²åœ°å›¾ã€‚ 

+   `return_binary_maps` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™å°†åˆ†å‰²åœ°å›¾ä½œä¸ºäºŒè¿›åˆ¶åˆ†å‰²åœ°å›¾çš„è¿æ¥å¼ é‡è¿”å›(æ¯ä¸ªæ£€æµ‹åˆ°çš„å®ä¾‹ä¸€ä¸ª)ã€‚

è¿”å›å€¼

`List[Dict]`

ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š

+   `segmentation` â€” å½¢çŠ¶ä¸º`(é«˜åº¦ï¼Œå®½åº¦)`çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¡¨ç¤ºä¸€ä¸ª`segment_id`æˆ–`List[List]`çš„è¿è¡Œé•¿åº¦ç¼–ç (RLE)çš„åˆ†å‰²åœ°å›¾ï¼Œå¦‚æœ return_coco_annotation è®¾ç½®ä¸º`True`ï¼Œåˆ™è®¾ç½®ä¸º`None`ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°é«˜äº`threshold`çš„æ©ç ã€‚

+   `segments_info` â€” åŒ…å«æ¯ä¸ªæ®µçš„é¢å¤–ä¿¡æ¯çš„å­—å…¸ã€‚

    +   `id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚

    +   `label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ« id çš„æ•´æ•°ã€‚

    +   `score` â€” å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚

å°†`Mask2FormerForUniversalSegmentationOutput`çš„è¾“å‡ºè½¬æ¢ä¸ºå®ä¾‹åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒ PyTorchã€‚

#### `post_process_panoptic_segmentation`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mask2former/image_processing_mask2former.py#L1135)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) â†’ export const metadata = 'undefined';List[Dict]
```

å‚æ•°

+   `outputs` (`Mask2FormerForUniversalSegmentationOutput`) â€” æ¥è‡ª Mask2FormerForUniversalSegmentation çš„è¾“å‡ºã€‚

+   `threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©ç çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚

+   `mask_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” åœ¨å°†é¢„æµ‹çš„æ©ç è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚

+   `overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) â€” é‡å æ©æ¨¡é¢ç§¯é˜ˆå€¼ï¼Œç”¨äºåˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©æ¨¡ä¸­çš„å°ä¸è¿ç»­éƒ¨åˆ†ã€‚

+   `label_ids_to_fuse` (`Set[int]`, *optional*) â€” æ­¤çŠ¶æ€ä¸­çš„æ ‡ç­¾å°†æ‰€æœ‰å®ä¾‹åˆå¹¶åœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ä¸€å¼ å›¾åƒä¸­åªèƒ½æœ‰ä¸€ä¸ªå¤©ç©ºï¼Œä½†å¯ä»¥æœ‰å‡ ä¸ªäººï¼Œå› æ­¤å¤©ç©ºçš„æ ‡ç­¾ ID å°†åœ¨è¯¥é›†åˆä¸­ï¼Œä½†äººçš„æ ‡ç­¾ ID ä¸åœ¨å…¶ä¸­ã€‚

+   `target_sizes` (`List[Tuple]`, *optional*) â€” é•¿åº¦ä¸º (batch_size) çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹ (`Tuple[int, int]]`) å¯¹åº”äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚çš„æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚å¦‚æœç•™ç©ºï¼Œåˆ™é¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚

è¿”å›å€¼

`List[Dict]`

ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š

+   `segmentation` â€” å½¢çŠ¶ä¸º `(height, width)` çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ª `segment_id`ï¼Œå¦‚æœæœªæ‰¾åˆ°æ©æ¨¡åˆ™è®¾ç½®ä¸º `None`ã€‚å¦‚æœæŒ‡å®šäº† `target_sizes`ï¼Œåˆ™å°†åˆ†å‰²è°ƒæ•´ä¸ºç›¸åº”çš„ `target_sizes` æ¡ç›®ã€‚

+   `segments_info` â€” ä¸€ä¸ªåŒ…å«æ¯ä¸ªæ®µçš„é™„åŠ ä¿¡æ¯çš„å­—å…¸ã€‚

    +   `id` â€” ä»£è¡¨ `segment_id` çš„æ•´æ•°ã€‚

    +   `label_id` â€” ä»£è¡¨ä¸ `segment_id` å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ« id çš„æ•´æ•°ã€‚

    +   `was_fused` â€” ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ `label_id` åœ¨ `label_ids_to_fuse` ä¸­åˆ™ä¸º `True`ï¼Œå¦åˆ™ä¸º `False`ã€‚ç›¸åŒç±»åˆ«/æ ‡ç­¾çš„å¤šä¸ªå®ä¾‹è¢«èåˆå¹¶åˆ†é…ä¸€ä¸ªå•ç‹¬çš„ `segment_id`ã€‚

    +   `score` â€” å¸¦æœ‰ `segment_id` çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚

å°† `Mask2FormerForUniversalSegmentationOutput` çš„è¾“å‡ºè½¬æ¢ä¸ºå›¾åƒå…¨æ™¯åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒ PyTorchã€‚
